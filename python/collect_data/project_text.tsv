project_url	file_text
https://github.com/97agupta/CourseProject	"Team 'Buddie' Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a.Aman Gupta, amang5 (Captain) b.Venkata Sandeep Chillara, vsc5 c.Katie Shin, ks56 2.Which paper have you chosen? Causal topic modeling Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3.Which programming language do you plan to use? Python 4.Can you obtain the datasets used in the paper for evaluation? a.Yes there are two datasets mentioned in the paper, a New York Times article dataset and a stock time series dataset. The NYT dataset is available here: http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2008T19 . A stock time series dataset can be requested and pulled from here: https://finance.yahoo.com/quote/AAPL/history/ 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? N/A 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A Team Buddie Final Documentation Each team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). main.py  compiles code above to automate the process of running PLSA without priors first and then with priors until we get the convergence we want (0.95+). This is to determine the correlation between US Stock data and New York Times articles per date. If the file structures match up, we can replace either the stock data or articles with other data sources to get the correlation between those two data sets also. This project is an attempt to reproduce the following paper: https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/2505515.2505612 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. -plsa_without_prior.py : initial run of PLSA without any priors -plsa_with_prior.py : subsequent PLSA runs with priors determined from Granger and Pearson tests -word_retriever.py : retrieves various info required such as word frequency per day -analysis.py : contains code for running the Granger and Pearson coefficient tests -analysis.ipynb : A jupyter notebook containing the same code in analysis.py allowing for more exploration and changes as needed. We utilized this notebook to explore our CSVs, view dataframes, and run our analysis. -main.py : 1. Retrieve and normalize stock data 2.Initially run PLSA without prior, and run the analysis (Granger and Pearson coefficient tests) to retrieve priors 3.Using the priors retrieved above, iterate with the PLSA with prior until the desired convergence is achieved. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. How to run the project: -Pull from github repo  https://github.com/97agupta/CourseProject -Install the necessary libraries, with the most notable one being -pip install plsa -Place NYT corpus in the correct folder format -Can be retrieved from  https://catalog.ldc.upenn.edu/LDC2008T19 -To run the iterative code, run 'python3 main.py' -To run sections of the iterative code, run 'python3 <filename>.py' Sidenote: -After running the granger test, the F values for different topics need to be manually inspected and relevant topics need to be parsed out. These are then used to pull top words which are used for a pearson test. -Running the PLSA as written can take multiple hours, so we have provided the end results for our 1st iteration in the /data folder for your viewing 4) Brief description of contribution of each team member in case of a multi-person team -Katie -Ran initial PLSA algorithm without any priors -With the relevant topics retrieved from Granger test, retrieved the top 20 relevant words per topic and their frequencies per date. -Aman: -Parsed Iowa Stock Exchange data -Used external time series & PLSA topics to write functions for a Granger test to find relevant topics -Used external time series & top words per topic to write functions for a Pearson test to create sub-topics from our topics with positively and negatively correlated words for use as a prior -Sandeep -The  PLSA library  we were using did not have a provision to take priors and use that as part of the algorithm. So extended the library to accept priors and use them as part of the M-step of PLSA using numpy einsum. -Read the priors as input from csv of the previous steps, filter only the words that exist in the particular day's corpus and feed them into the PLSA step and do multiple iterations. Team BuddieTopic: Reproducing a Paper (Casual Topic Mining)Katie Shin, Sandeep Venkata, Aman GuptaGeneral Idea*Can we combine probabilistic topic models and causal analysis with external time series to find topics in a corpus of documents that are both coherent semantically and correlated with the time series? *Reproduced this paper by utilizing two data sets from the paper: *Corpus: NYT Articles *Time Series: Iowa Presidential Stock Markets *Looking for topics that specifically caused support for Bush or Gore to change. Based on: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management (CIKM '13). Association for Computing Machinery, New York, NY, USA, 885-890. DOI:https://doi.org/10.1145/2505515.2505612 How to Use Software-Pull from github repo https://github.com/97agupta/CourseProject-Install the necessary libraries, with the most notable one being-pip install plsa-Place NYT corpus in the correct folder format-Can be retrieved from https://catalog.ldc.upenn.edu/LDC2008T19-Folder: CourseProject/data/<month>/<day>/<filename>.xml-To run the iterative code, run 'python3 main.py'-To run sections of the iterative code, run 'python3 <filename>.py'(Note: This takes hours to run, and therefore we have provided a sample run w/ end-results here) ImplementationPart 1Part 2Part 3Part 4Part 5ImplementationPart 1*For each group of xml files that belong to a single day (05/01/2000 - 10/31/2000), we ran and picked the best model out of 5 PLSA models.*From the top model, we extract the top 5 highest probability words for each date that will be considered the topic.*Library used: PLSAImplementation (Part 2)Part 2*Based on topics identified during PLSA, we identify related topics by running a Granger Test between the change in probability of topics and the normalized price of one candidate. *We use a lag of 5 days & create stationary time series for both of our Granger Test Variables. *Not every topic has enough data points for testing, so we skip over topics where the Granger Test fails*Based on the highest F-value for the each Granger Test, we determine if a topic is or isn't correlated with the external time series. Implementation (Parts 3 & 4)Part 3Part 4*From the best PLSA model (refer to Part1), we retrieved the top 20 relevant words (i.e. highest probability) per topic that were deemed related from the Granger test.*For each word series, we run a pearson coefficient test comparing the external time series and the frequency of top words of each topic. *We then segment these words into negative and positive correlations, returning words that when combined, meet our probability threshold (0.75) *These words and then used as a Prior for the PLSA algorithm. Implementation (Part 5)Part 5*We used the PyPI package for PLSA but it didn't offer support for including priors to guide the PLSA as per user inputs.*So we enlisted the source code and extended the PLSA algorithm to overwrite the M step.*The library essentially uses numpy 'einsum' to efficiently perform multi-dimensional matrix operations.*We modified the M-step of the algorithm to include the parameters (m) and the pseudo counts.*The key part was to look for only the words that occured in the day.Results (1st Iteration) PLSA without prior*After running our first PLSA on the corpus of NYT documents, we found 343 topics. Of these topics, only 4 topics showed causality with our external time series: bush, gore, campaign, and clinton.*For each of these topics, we extracted the top words and segmented each topic into two topics made up of positively and negatively influenced words. These new topics were used a prior for our second iteration. Here is an example of the Gore topic, being split into two: Results (2nd Iteration)*With a new iteration using PLSA with prior, we find the following relevant topics using our granger test: *Teacher, oil, drug, debate *This is a clear improvement over our original PLSA, as it delves deeper into the issues that moved the campaign likelihoods for each candidate *Bush, Gore, Campaign, Clinton-- This clearly shows that our model is finding topics that are more related to the campaign issues & with further iterations we expect this to further improve. Next Steps*We would want to run further iterations on these topics.*Additionally, instead of using the full NYT corpus we could identify paragraphs that mention Bush, Gore, or Presidential to create a smaller corpus from which to topic mine.*We would want to find a more robust method of comparing and understanding the different F-Scores from each lag of our Granger Test. Thanks!(This is Buddie) Team Buddie Progress Report Topic: Reproducing a Causal Topic Mining Paper Team Members: -Aman Gupta (@amang5) -Katie Shin (@ks56) -Venkata Sandeep Chillara (@vsc5) Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? -Implemented the initial PLSA algorithms using  https://github.com/yedivanseven/PLSA . Modified the source code to include the prior. (Our initial implementation for PSLA can be found in main.py) -Ingested and parsed stock data for the 2000 presidential election using daily markets in a Jupyter Notebook. Calculated normalized price for each candidate on a daily basis and stored this data in a data frame. This can be seen in (Stock Data Parsing & Normalization.ipynb with stock data in the stock_data folder) -Explored using https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.grangercausalitytests.html  for running granger tests to find casual topics based on a time series 2) Which tasks are pending? -Organize the PLSA results in a Date-Topic-Probability format so that it can be used in the Granger test Run granger tests for topics using results from first task with time series PLSA output -Understand and run pearson coefficients tests to find casual words from relevant documents -Use output from pearson coefficient tests as prior for PLSA. -Add README.md with steps to organize NYT corpus data in and run the code 3) Are you facing any challenges? -PLSA process time for the NYT corpus times out when run on a local machine. One option is to look into  https://colab.research.google.com/ -Since we need the Date-Topic-Probability, we don't have to run PLSA on the entire dataset -The version of PLSA with priors isn't readily available, we had to implement that on our own. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. NYT Corpus folder structure In order to run the PLSA algorithm, the NYT corpus structure has to be '../CourseProject/data//' Stock data folder structure We save the stock prices from May 2000 to Oct 2000 as htm files in the format of '_2000.htm' in a folder called 'stock_data'. The htm files can be saved from https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 Code content plsa_without_prior.py: initial run of PLSA without any priors plsa_with_prior.py: subsequent PLSA runs with priors determined from Granger and Pearson tests word_retriever.py: retrieves various info required such as word frequency per day analysis.py: contains code for running the Granger and Pearson coefficient tests main.py: Retrieve and normalize stock data Initially run PLSA without prior, and run the analysis (Granger and Pearson coefficient tests) to retrieve priors Using the priors retrieved above, iterate with the PLSA with prior until the desired convergence is achieved. How to run code Once the data files are saved in the structure defined above, you should be able to run python3 main.py which will converge after a desired convergence has been retrieved."
https://github.com/AnantSharma18/CourseProject	Page 1 of 2 Project Progress Report Anant Ashutosh Sharma anantas2@illinois.edu Individual Project | Free Topic Text and Tweet Classification using Machine Learning 1) Which tasks have been completed ? The following tasks have been successfully completed and corresponding code has been pushed on the GitHub repo: 1. Created a custom dataset from public sources for Political and Non Political Tweets. Total corpus of 6060 tweets, with 4088 labelled as Political and 1972 labelled as Not Political. 2. Cleaned the following datasets for classification a. Spam SMS Dataset b. Offensive Language Dataset c. Political Tweets Dataset 3. Performed the following Pre-Processing on the text data a. Removed Stop Words b. Removed Non Alphabetic Characters c. Performed Stemming 4. Performed the following types of feature extractions: a. Count Vectors b. Word level TF-IDF c. N-Gram level TF-IDF d. Character Level TF-IDF 5. Trained the following models: a. Na*ve Bayes b. Linear Classifier c. SVM d. Random Forest 6. Produced Classification reports for each of the following trained models Note : The tasks 3 to 6 were performed on all the three datasets. Each model has a separate notebook on GitHub under the Helper Notebooks Directory. In each notebook, the currentDF (i.e. the current Data Frame can be changed to choose one among the three datasets. Page 2 of 2 2) Which tasks are pending ? Except CNN, all the models have been trained and tested on the datasets. They are showing good accuracy levels. The following tasks are still pending 1. Train a CNN Model to perform text classification 2. Analyse the accuracies of different model on different datasets with different features by creating a comparison table. 3. Train and Save the most accurate model for classifying political tweets (Indian Context). 4. Fetch Tweets from twitter using the twitter API and classify them as Political or Not Political using the saved model. 5. Create a comprehensive jupyter Notebook to cover all the task performed (for the purposed of presentation). Create readme file on GitHub (documentation with instructions) and Video Presentation for submission. 3) Are you facing any challenges ? Having no experience with sklean python library, it was initially a bit challenging to get things done. But as I progressed, things became much more clear. Implementing a CNN to classify texts also seemed a bit challenging at the beginning. However, I am confident that I will be able to accomplish the task. Also, looking forward to use the tweepy python library to extract tweets automatically. Page 1 of 5 Individual Project D Free Topic Anant Ashutosh Sharma anantas2@illinois.edu Topic Text and Tweet Classification using Machine Learning What is the task ? The task involves classifying texts into their relevant categories using different machine learning techniques. Text classification is one of the standard applications in text mining. . The objective of our text classification task is to find appropriate labels for previously unlabelled data from a predictive model which has been trained on a pre-labelled dataset. A series of necessary subtasks are performed to identify and extract relevant features from a given text, which can be further applied to train a predictive model The text to be classified can either be a sentence or a group of sentences i.e. a paragraph. Depending on the labels of the dataset, the text can be classified in binary labels or multiple labels. For instance, on training the models on SMS spam dataset, it will be able to accurately classify texts as Spam (spam) or Not Spam (ham), whereas on training the models on the Hate-Speech and Offensive Language Data, it will achieve the task of classifying text into hate speech, offensive language or neither As of now, I plan to train models to 1. Classify emails in to Spam or Not Spam 2. Classify text into different hate-speech, offensive language categories 3. Classify political tweets using a newly created dataset Why is it important or interesting ? The internet is a hub of textual information. Users are mostly overwhelmed with the amount of information that they have to go through every day. Classifying the text which users encounters into different buckets can give a boost to their efficiency as well as understanding. For instance, classification of news into different topic allows the users to only focus on the topics relevant to their interest. The core importance of textual classification lies in finding an appropriate representation of text data where interesting metrics (as measurements) can be used in order to compare different text data in accordance to their similarity to extract insights. Page 2 of 5 Further, classification of different texts from public social media platform can be tremendously useful in identifying the nature of a post / tweet. For instance, classifying tweets into different hate-speech categories can be used to automatically remove the tweet from the userOs handle. The opportunities of text classification are endless. The interesting thing not only lies in just classifying texts into different buckets, but the analysis which can be on the basis of classification obtained. For instance, using the classifier to classify Political tweets can be used to identify politically vocal users and the models can further be improvised to provide analytical parameters for their political inclinations. What is your planned approach ? I plan to implement multiple models in order to classify textual data from publicly available datasets initially. A comparison study between the different models will allow to identify the most accurate model for a particular dataset. As of now, I plan to follow the following pipeline to categorize textual data 1. Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters 2. Extract features from the pre-processed data I plan to implement and compare the following vector representations a. Count Vectors b. TF-IDF Vectors i. Bag of words (word level) ii. Bag of n-grams (n-gram level) iii. Character level 3. Training Model D Learning a. Na*ve Bayes b. Linear Classification c. SVM d. Random Forest e. Convolutional Neural Network 4. Classification of text using the trained model 5. Evaluation and Comparison of different models Page 3 of 5 I plan to use the above approach for different datasets, namely, SMS Spam and hate-speech. Further, I will using the above approach on my newly generate dataset to classify political tweet in the Indian context. What tools, systems or datasets are involved? I will primarily be using Python. Within python, I plan to use the following libraries sklearn, seasborn, pandas, numpy, Spacy, nltk for getting stopwords corpus, tweepy etc. Datasets Involved Y= SMS Spam Collection http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam. Labels : spam / ham Y= Hate-speech and Offensive Language Dataset https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns: count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF). hate_speech = number of CF users who judged the tweet to be hate speech. offensive_language = number of CF users who judged the tweet to be offensive. neither = number of CF users who judged the tweet to be neither offensive nor non-class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 D neither Labels : hate speech / offensive language / neither Y= Political Tweets Dataset D Custom (India) I plan to obtain the relevant political tweet (Indian context) from the publicly available dataset on Kaggle https://www.kaggle.com/codesagar/indian-political-tweets-2019-feb-to-may-sample Page 4 of 5 I will label them as political. Further, I will collect non-political tweets from publicly available twitter datasets and label them as non-political. What is the expected outcome? I expect to accurately classify the textual data in their respective categories using the trained models. Further, I intend to write a script which can automatically collect tweets based on some parameters and accurately classify them into political and non-political tweets. Inputs : Textual data from the respective datasets Expected outcomes as per the models trained on different datasets SMS Spam Data : spam / ham (Not spam) Hate Speech and Offensive Data : hate speech / offensive language / neither Political Tweets Dataset : political / non political How are you going to evaluate your work? I intend to evaluate the models by comparing their predictions using parameters like precision, recall, f1 scores etc. This evaluation will help identify the most suitable model for textual classification, depending on the datasets. At the time of training, the dataset will be divided into testing, training and validation sets. Which programming language do you plan to use? I plan to implement this project in python. Further, I am also planning to use Jupiter notebook for making the code more presentable and easy to demonstrate during the project presentation. Justification of Work Load S. No. Task Estimated Hours 1. Import and Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters 2 hrs 2. Extract features from the pre-processed data a. Count Vectors (0.5 hrs) b. TF-IDF Vectors (1.5 hrs) i. Bag of words (word level) 1.5 hrs Page 5 of 5 ii. Bag of n-grams (n-gram level) iii. Character level 3. Training Model D Learning a. Na*ve Bayes (0.5 hrs) b. Linear Classification (0.5) c. SVM (1 hr) d. Random Forest (1 hr) e. Convolutional Neural Network (4 hrs) 7 hrs 4. Classification of text using the trained model 2 hrs 5. Evaluation and Comparison of different models 4 hrs 6. Create new data set for classifying political tweets in India 3 hrs 7. Fetch tweets using different parameters and classify as Political / Non Political 3 hrs This above table only gives a rough time estimate of the tasks which will be involved in completing the project. It fulfils the 20+ hours workload as mentioned in the requirements. CourseProject | Text and Tweet Classification using Machine Learning This project is focussed at classifying texts into their relevant categories using different machine learning techniques. Text classification is one of the standard applications in text mining. . The objective of our text classification task is to find appropriate labels for previously unlabelled data from a predictive model which has been trained on a pre-labelled dataset. A series of necessary subtasks are performed to identify and extract relevant features from a given text, which can be further applied to train a predictive model. The following Classifications have been accomplished in this project: 1. Classify SMS into Spam or Not Spam 2. Classify text into different hate-speech and offensive language category 3. Classify political tweets (Indian context) using a custom dataset Note : The purpose of this project is to present the different machine learning implementations to classify textual data. While the accuracies of the different models are mentioned in the result section, the project focussed more on the different ways machine learning can be used to perform text classification. Contribution Project by Anant Ashutosh Sharma Free Topic : Text and Tweet Classification using Machine Learning Course : CS 410 NetID : anantas2 The following documentation and demo is submitted to the GitHub Repo 1. Project Proposal 2. Project Progress Report 3. Self-Evaluation Report 4. Demo (Video) Demo Video : https://uillinoisedu-my.sharepoint.com/:v:/g/personal/anantas2_illinois_edu/ETcTrozhFLVFjqRJjMrkPTkBk-iMH54QljVBzl-KbDYWaA?e=7r6QDU Datasets The details for the datasets is as follows: - SMS Spam Collection Source: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam. Labels : spam / ham Hate-speech and Offensive Language Source: Dataset https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns: count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF). hate_speech = number of CF users who judged the tweet to be hate speech. offensive_language = number of CF users who judged the tweet to be offensive. neither = number of CF users who judged the tweet to be neither offensive nor non-class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither Labels : hate speech / offensive language / neither Political Tweets Dataset - Custom | Indian Context The political tweets have been taken from a GitHub repo for Sentiment Analysis on Indian Political fetched tweets. Source: https://github.com/rohitgupta42/polity_senti Further, a set of General tweets was taken from the The Twitter Political Corpus Source: https://www.usna.edu/Users/cs/nchamber/data/twitter/ Both of these datasets were combined and cleaned to obtain the required datasets. Labels : POL / NOTPOL Methodology In this project, I have implemented multiple models in order to classify textual data from publicly available datasets initially. Each model has been implemented in a seperate Jupyter Notebook for clear understanding. A comparison study is also presented between the various models in the result section of this documentation. Pre-processing Cleaning of text Stop Words removal Stemming Removal of non-alphabetic characters Feature Extraction Count Vectors b. TF-IDF Vectors Bag of words (word level) Bag of n-grams (n-gram level) Character level Implemented Models Naive Bayes Linear Classification SVM Random Forest Convolutional Neural Network At the end of each model notebook, an accuracy report is present. The classification accuracy of different models is summarised in the results section below. Results The following table contain the F1 Scores for each of the predicted categories: Naive Bayes | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.88 | 0.96 | 0.97 | | Word TF-IDF | 0.83 | 0.93 | 0.92 | | N-Gram TF-IDF | 0.60 | 0.91 | 0.80 | | Char TF-IDF | 0.71 | 0.94 | 0.91 | Linear Classification | Feature / Cagtegory | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.91 | 0.97 | 0.96 | | Word TF-IDF | 0.81 | 0.97 | 0.97 | | N-Gram TF-IDF | 0.18 | 0.91 | 0.82 | | Char TF-IDF | 0.84 | 0.96 | 0.95 | SVM | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.89 | 0.97 | 0.95 | | Word TF-IDF | 0.92 | 0.97 | 0.96 | | N-Gram TF-IDF | 0.79 | 0.92 | 0.81 | | Char TF-IDF | 0.88 | 0.97 | 0.94 | Random Forest | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:---------------:| | Count | 0.86 | 0.97 | 0.95 | | Word TF-IDF | 0.85 | 0.97 | 0.96 | | N-Gram TF-IDF | 0.74 | 0.92 | 0.81 | | Char TF-IDF | 0.83 | 0.96 | 0.94 | Convolutional Neural Network (CNN) | CNN / Category | Spam SMS | Offensive Lang | Political Tweet | |---------------------|:--------:|:--------------:|:---------------:| | F1 Score | 0.91 | 0.97 | 0.95 | | Accuracy | 0.98 | 0.95 | 0.94 | Summarized Results The best performing features have the following F1 Scores in each of the models: | Model / Category | Spam SMS | Offensive Lang | Political Tweet | |-----------------------|:--------:|:--------------:|:---------------:| | Naive Bayes | 0.88 | 0.96 | 0.95 | | Linear Classification | 0.90 | 0.97 | 0.96 | | SVM | 0.92 | 0.97 | 0.96 | | Random Forest | 0.86 | 0.97 | 0.96 | | CNN | 0.91 | 0.96 | 0.95 | Political Tweets Prediction The Political Tweet Prediction notebook can be used to fetch and predict tweets into Political and Non Political Categories. The tweets are fetched using the tweepy package which uses the Twitter API to stream tweets pertaining to a certain tracking list. The most accurate model for political tweet prediction i.e. Linear Classification with Word Level TF-IDF features has been saved as a pickle model is used to categorise the tweets in pol / notpol categories. Installation Guide The project makes use of Jupyter Notebook to implement the different models. The following steps to install and run the code on your local machine. Running the Helper Notebooks Clone this repo onto your local machine. You will need the following pre-requisites installed in your local environment: python3, jupyter notebook, numpy, pandas, keras, tensorflow, sklearn, spacy, ntlk, string, pickle, twitter Once you have made sure that the above mentioned packages are installed in you environment, you can go ahead and launch jupyter notebook Each Helper Notebook can be executed seperately to get results for different models. In each of the Helper Notebook, the variable currentDF can be changed to either OffensiveLangDF | spamSmsDF | politicalDF to run the model on different datasets Note : You might have to download the spacy 'en' model seperately in order to run the notebooks Predicting Political Tweets The pre-trained model to predict political tweets is already saved in the folder 'Saved Model' under the name LR_Pol.plk In order to run the political tweet prediction notebook, you will have to first obtain credentials from Twitter API. The consumer_key, consumer_secret, access_token and access_token_secret variables need to be replaced with your unique Twitter API credentials. Guide to obtain Twitter API credentials can be found under references. Once you have replaced the XXXX with your Twitter API credentials, you can execute the notebook to obtain 4 different tweets using the tracking list and classfiy them into pol / notpol using the pre-trained model The trackingList1 and trackingList2 lists can be edited to stream different tweets. The n_tweets variable can be changed to the number of tweets you wish to obtain. Limitations The datasets being used to train the model can be further improved. They currently have very niche examples of the catergories. For instance, the political tweets dataset only contains political tweets pertaining to the Indian Context. Further, the number of training records are less. A higher number of traininf records may allow the models to perform better. The Convulutional Neural Network (CNN) implemented in this project is a simple and generic one. A much more complex and accurate CNN can be designed and fine tuned as per the requirements of each of the datasets. These limitations are present in this project since the purpose of this project is not to present any accurate model to classify text objects, but to present the different methods and ways in which machine learning models can be used to classify textual data. References https://rapidapi.com/blog/how-to-use-the-twitter-api/ http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data https://github.com/rohitgupta42/polity_senti https://www.usna.edu/Users/cs/nchamber/data/twitter/ scionoftech GitHub Repo for helping in eaxtracting different features for models Page 1 of 2 Self Evaluation Report Anant Ashutosh Sharma anantas2@illinois.edu Individual Project | Free Topic Text and Tweet Classification using Machine Learning 1) Have you completed what you have planned ? Yes, I have successfully completed what I planned and presented in the Project Proposal as well as the Progress Report. The following table mentions the tasks which have been successfully completed in this project along with the checklist. (These were the same tasks as mentioned in the Project Proposal) S. No. Task Completion 1. Import and Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters Completed 2. Extract features from the pre-processed data a. Count Vectors b. TF-IDF Vectors i. Bag of words (word level) ii. Bag of n-grams (n-gram level) iii. Character level Completed 3. Training Model D Learning a. Na*ve Bayes b. Linear Classification c. SVM d. Random Forest e. Convolutional Neural Network Completed 1. Classification of text using the trained model Completed 2. Evaluation and Comparison of different models Completed 3. Create new data set for classifying political tweets in India Completed 4. Fetch tweets using different parameters and classify as Political / Non Political Completed Page 2 of 2 2) Have you got the expected outcome ? Yes, I have got the expected outcome from my proposed project. All the machine learning models successfully classify the textual data from three different datasets into the respective categories with good accuracies and F1 scores. The purpose of this project was to present the different machine learning models which can be used to perform text classification instead of comparing and competing them against one another. Further, as expected, the tweets are automatically streamed using the Twitter API and are classified into political and not political as expected. All the results and outcomes have been successfully presented in the GitHub repo (readme.md) as well as in the various Jupiter Notebooks.
https://github.com/AnirudhaPatil/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Arnavj3/CourseProject	"CS 410 Project Proposal Song Retrieval using Sentiment Analysis The Group The group consists of Arnav Jain (Arnavj3) and Nikhil Sahni (Sahni4). The captain of the group will be Arnav Jain. Our Free Topic We plan on scraping the web for song lyrics and building an informational retrieval system. We will conduct a sentiment analysis of these lyrics and classify the songs. The user will then be able to query our tool with a combination of parameters like song sentiment, artist, genre, etc. We think this is incredibly interesting as it introduces a novel and useful way of searching for music. An example query for our tool would be ""I want a happy country song by ____ artist"" Our planned approach is to first gather the data and clean it into a processable format. We will then work on building a program to conduct sentiment analysis on the gathered lyrics. Once we have a feature set for each song's lyrics, we can start building our search engine that will allow the user to use our tool. We plan on using NLTK for sentiment analysis and text processing tasks. We will scrape and gather data using python. We are still unsure of the data storage tools we will use and plan on making that decision once we start the process of collecting data. This way we will be able to make a more informed decision based on the nature of the data we will work with. The expected outcome is a working multi-feature search engine for songs. To evaluate our search performance, two useful metrics will be precision and recall. We think a combination of analytical and human oversight will help us evaluate our tool. Programming Language We will primarily be using Python for this project. We will be using multiple packages such as NLTK, MeTAPy and Selenium to perform the sentiment analysis, create the search engine and scrape the web Workload We expect this project to take us anywhere from 50-60 hours to complete. The division of time is expected to be as follows: Phase 1: The first phase of this project is to scrape the web to create a dataset of as many songs and lyrics as we can. This would take us 10-15 hours in total. Phase 2: The second phase consists of creating the search engine to be able to give accurate recommendations of songs only based on the lyrics searched. This would take us 15 - 20 hours Phase 3: The third phase consists of adding sentiment analysis to the search engine so that users could search for genres and moods and accordingly get song recommendations. This would take us 15-20 hours. Project Progress Report Progress So far, we have managed to set-up our data retrieval pipeline. We first mine billboard's website to scrape song names from their top 100 charts for a range of years. We use these song names and query Genius.com's API to obtain the lyrics for the song. We then cleaned these lyrics for processing by removing stop words and non-useful information. This way the data is in a consistent universal format throughout. We've combined all the meta-data we have on these songs with the lyrics and consolidated it into a dataframe. We are currently working on our sentiment analysis model to assign each song a sentiment value. Remaining Tasks We have part of the sentiment analysis on the lyrics remaining. We will then add this sentiment value as an additional feature in our data frame for each song. Once we are done with this, we need to create the search/recommendation engine to recommend songs based on the user's query. Challenges Faced One of the biggest challenges we faced while collecting the lyrics was that we weren't able to find any ready datasets online for lyrics. Most were bag-of-word representations or very limited. We decided to scrape the top 100 billboard website to get all top 100 songs and then we used those song names to query the ""Genius"" API to retrieve lyrics for each of the songs. This way we achieved more flexibility and customization in our retrieval. SmartLyrics Song Search System Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. To run the code follow the following steps: - Unzip the provided CSV file and upload it to your google drive - open the notebook in google drive and run the cell which reads the CSV (This will require you to mount your drive and should not take more than a few seconds) - run the cell that reads the unnormalized dataset and run all the cells that follow - the last cell will have the following search fields: artist, album, year, sentiment, profanity and lyric. You can use these search fields to find a song you're looking for. The following video explains all the different steps taken to create this project - https://www.youtube.com/watch?v=briL4ZidYhE"
https://github.com/Asciotti/CourseProject	"Progress Report Andrew Sciotti, sciotti2 Which tasks have been completed: The following tasks have been completed: -Setup environment (for python 2.7) -Debugged code so that it was working in original state -Identified the location where to hook into the EductionalWeb system to apply auto-text summarization to the returned results -Implemented two text summarization prototypes, extractive using shallow NLP & abstractive using deep NLP Which tasks are pending: -Implement summarizers into rest of the code base (currently sitting separate using an example text paragraph -Fully hook in the summarizer to display on the EductionalWeb browser (see ""challenges"") -Explore improvements to the shallow NLP extractive text summarization (stemming, stop word removal, word embedding, etc) Are you facing any challenges: 1)Errors during running I cannot get the full workflow to work. The scoring function inside the ""get_explanation"" function of model.py freezes during computation. See https://github.com/Asciotti/CourseProject/blob/master/model.py#L311 However, this only occurs when I run it via the app.py, if I were to manually run the scorer, though ipython/command line, it has no issues. Because of this, I cannot fully close the loop. 2)The data (para_idx_data) is not representative of what exists on the publicly hosted website The sourced ""explanations"" from the  http://timan102.cs.illinois.edu/explanation/  website seem to be derived from the full text of the textbook, but the data provided in the git repo appears to only be snippets/headers of the text. Team APS Members: Andrew Sciotti (sciotti2), sciotti2@illinois.edu 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Member/Captain: Andrew Sciotti, sciotti2 2.What system have you chosen? Which subtopic(s) under the system? I have chosen to improve the EducationalWeb System, specifically by providing more context and improved reading comprehension to the explanations in the form of summarization. For instance, the explanation of ""PLSA"" is (roughly) 63 sentences long! 3.Briefly describe the datasets, algorithms or techniques you plan to use The goal is to implement extractive summarization on the retrieved relevant explanations provided by EducationalWeb System. The dataset will be provided via the textbook, ""Text data management and analysis: a practical introduction to information retrieval and text mining"". The fundamentals of the extractive summarization are based on PageRank, but for text, coined (not so creatively), TextRank (reference: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf ). This is an unsupervised algorithm. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? The most straightforward method of evaluation is to manually obtain a few examples of explanations that are lengthy and evaluate the effective conciseness provided by the function. Quantitative measures are unlikely to be evaluated, so the results will be qualitatively evaluated. Ideally, if there were sufficient users, something like A/B testing would be implemented, but that is entirely out of scope. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly The plan is to identify where the explanation is retrieved and forwarded to the webpage, and intercept that function call to be routed through the extractive summarizer. 6.Which programming language do you plan to use? Python 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Project Plan: -Setup environment,  2 hours -Familiarization with EducationalWeb System,  5 hours -Progress Report,  1 hour -Deep dive into the retrieval/explanation system,  3 hours -Implement extractive summarization,  3 hours -Debug & evaluated extractive summarization,  3 hours -Document code & prepare tutorial,  3 hours EducationalWeb TUTORIAL FOUND HERE: https://mediaspace.illinois.edu/media/1_5ohs6cp4 If you are not familiar with the EducationalWeb, it is a resource provided that contains a multitude of features related to the lectures slides including: search term throughout slides/audio Downloading slides Providing reference to slides that are related ""Explain"" highlighted words on the slides Setup The following instructions have been tested with Python3.7.9 on Windows. You should have ElasticSearch installed and running Ubuntu - https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Windows - https://www.elastic.co/guide/en/elasticsearch/reference/current/windows.html MacOs - https://www.elastic.co/guide/en/elasticsearch/reference/current/brew.html Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ Download lemur-stopwords.txt from here -- https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt lemur-stopwords.txt needs to be placed under EducationalWeb/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal, setup your python environment (requires python 3.7.9 - I recommend making a clean venv/conda environment) by running pip install -r requirements.txt. If using conda, the following is a useful command to make the correct environment conda create --name uiuc3.7 python=3.7.9 Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/ Run through the examples (at the end of the video tutorial) to see the usage and benefits of the summarization. Motivation: The ""Explanation"" feature available in the EducationalWeb system can be used to provide an explanation of the highlighted words on a given slide. The issue is the returned explanations can be too lengthy and not all that specific to the query word. One could improve these explanations by providing a more targeted, concise summary of the raw explanation return. This can be done in a few manners, I chose to look at extractive methods (using shallow NLP) and abstractive methods (using deep pre-trained NN based language models). Extractive summarization can be thought of as restating the most useful or ""main"" points in a document. Abstract summarization can be thought of as paraphrasing the document itself in a (typically) more concise way [1]. Concepts of explanation ""finding"" The general process to discover explanations is as follows - this is essentially a document retrieval problem given a query. 1. Preprocess corpus for future ranking on query words 2. Extract the query word from the webpage 3. With query, corpus (processed), and ranking algo (BM25 in our case), determine most likely documents (In our case documents were provided in the repo already, they appear to be headers of chapters) 4. Take top-k (1 in our case) most likely documents 5. Given the best matched document, then run summarization on the content 6. Return the summarized ""explanation"" to the user Extractive Summary Process (ref [2]) Provided into the extractive summary process is a preprocessed document that contains a list of lists for each sentene then each word in said sentence. In the extractive summary process we utilize a vector space method ""encoding"" of each sentence. Each element of the vector is the count of a single unique ""token"". We use the nltk python language package for tokenization. The similarity measure between each sentence with every other sentence is computed via cosine similarity. This gives us an MxM matrix (M=# sentences), the similarity matrix. The pagerank algorithm [5] can be used to return probabilities of ""landing"" on each page (sentence in our case). The sentences with the highest probabilities are most likely to occur, and are thus the most ""useful"". We add some additional weighting to sentences which contain the query word itself - to try to improve the likelihood of seeing the word in the explanations given. You can run an example of abstractive summarization by running the following: python extractive_summarizer.py It will run a summary of the below input data (selected document) which resulted from the query annotations. In this section, we're going to continue our discussion of web search, particularly focusing on how to utilize links between pages to improve search. In the previous section, we talked about how to create a large index on using MapReduce on GFS. Now that we have our index, we want to see how we can improve ranking of pages on the web. Of course, standard IR models can be applied here; in fact, they are important building blocks for supporting web search, but they aren't sufficient for the following reasons. First, on the web we tend to have very different information needs. For example, people might search for a web page or entry page-this is different from the traditional library search where people are primarily interested in collecting literature information. These types of queries are often called navigational queries, where the purpose is to navigate into a particular targeted page. For such queries, we might benefit from using link information. For example, navigational queries could be facebook or yahoo finance. The user is simply trying to get to those pages without explicitly typing in the URL in the address bar of the browser. Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. These features provide an opportunity to use extra context information of the document to improve scoring. Finally, information quality greatly varies. All this means we have to consider many factors to improve the standard ranking algorithm, giving us a more robust way to rank the pages and making it more difficult for spammers to manipulate one signal to improve a single page's ranking. You should see: Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. First, on the web we tend to have very different information needs. In this section, we're going to continue our discussion of web search, particularly focusing on how to utilize links between pages to improve search. One might notice that without any postprocessing, the resultant summary can be devoid of any context or fluidity. Abstractive Summary Process (ref [3]) Abstractive summary utilizes a pre-trained deep NN from HuggingFace [4]. We utilize the T5 language network to perform summarization. The T5 network has an accompanying tokenizer that ingests the raw document (sentences). We have picked the T5-small model (and tokenizer) rather than the medium/large models due to computation time (we are running locally on CPUs). The larger models 1) are more accurate and 2) have the ability to process longer documents. The small is limited to 512 tokens, which roughly translates to 500 words or ~20-30 sentences. Without going into the details ([3] can provide more information) of the generational configuration, we choose to utilize beam search to improve the summary in addition to constraining the length of the summary returned to [50,150] tokens. You can run the following to see an example of an abstract summary (note the output might be slightly different from yours). Input text same as for extractive summarization example. python abstract_summarizer.py we're going to continue our discussion of web search. in the previous section, we talked about how to create a large index on using MapReduce on gfs. we want to see how we can improve ranking of pages on the web. the standard IR models can be applied here, but they aren't sufficient for the following reasons. Given the network we used, it was not possible to add any weighting/favoring towards generating sentences with the query word. Overview of selected pieces of code Below will highlight a fews places in the code that are specific to the augmentation of the Explanation functionality. This code base is large but most of it is related to the web based portion whereas a select few python files are used to actually perform much of the backend functionality. app.py : Contains the flask server code, handles various API calls (unmodified) model.py : Handles the model that performs document-query retreival. get_explanation is the function that returns the Explanation results. Inside this function is where the main ""injection"" occurrs. After we retrieve the normal explanation (aka the highest scoring document), we feed that result directly into the summarizer functions. ranker.py : Retrieves the highest ranking documents given the query word and corpus. Replaced metapy ranker w/ rank_bm25 3rd party package due to bugs. extractive_summarizer.py : Contains extractive summary code, if run alone via command line, will summarize an example text. abstract_summarizer.py : Contains abstract summary code, if run alone via command line, will summarize an example text. Demo The demo is provided via the video tutorial found here: https://mediaspace.illinois.edu/media/1_5ohs6cp4 The slide used to demo can be found here (given the app is running and you can connnect to it: http://localhost:8096/next_slide/cs-410/86/cs-410----13_week-12----02_week-12-lessons----05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt----slide2.pdf The word example we will go through is for Coverage in the above slides. If you don't want to go through the video. Go through the installation above such that you can see the slides. Go to those specific set of slides (click on link). Highlight ""coverage"" in the bottom right corner and hit the box with the graduation cap on it in the top right. If you hover over the boxes it shoulds say ""Explain selected text"" You should see the unmodified ""Explanation"" Now go to your text editor and open app.py. Modify SUMMARIZER="""" to SUMMARIZER=""EXTRACT"" located at the top of the file. Hit save file. Wait a few seconds then reload the page. Highlight ""coverage"" again and hit the explain button. See the extractive summarized ""Explanation"". Now go to your text editor and open app.py. Modify SUMMARIZER=""EXTRACT"" to SUMMARIZER=""ABSTRACT"" located at the top of the file. Hit save file. Wait a few seconds then reload the page. Highlight ""coverage"" again and hit the explain button. See the abstract summarized ""Explanation"". References [1] https://www.quora.com/Natural-Language-Processing-What-is-the-difference-between-extractive-and-abstractive-summarization [2] https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70 [3] https://huggingface.co/blog/how-to-generate [4] https://huggingface.co/ [5] https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"
https://github.com/BhuvaneswariPeriasamy/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/CapnDanger/CourseProject	"Bryan Holcomb CS 410: Text Information Systems Final Project Progress Report Currently, I have the majority of the sentiment analysis code written and I am in the process of testing on small, manually-copied articles. My next step is to complete testing, which I plan to do this week. The next steps after testing is complete are to build a webscraper to build up a corpus of news articles from various sources. I foresee one challenge being to find an easily-accessible API that allows scraping of web pages without ads or irrelevant content. Bryan Holcomb CS 410: Text Information Systems Project Proposal Team Member: Bryan Holcomb (bryanph2) as individual Topic: Sentiment Analysis of News Articles Around the 2020 US Presidential Election For my course project, I plan to do a sentiment analysis of news articles around the 2020 US Presidential Election. There are a number of applications for which we can use this sentiment analysis, including an evaluation/comparison of biases between different news sources, as well as measure the general opinion of the candidates in different audiences and demographics. Additionally, we can evaluate how sentiment towards each candidate has changed over time. For this analysis, I plan to build a web crawler to obtain a large sample of articles from various websites, including large national news outlets (CNN, NBC, ABC, Fox News), at least one international source (i.e. BBC), and various newspapers from major cities around the country. For each article, I will track the date and source, and then parse each article and perform a sentiment analysis. I plan to use a number of packages to do the work, including MeTa, NLTK, Beautiful Soup, and IBM Watson. The vast majority, if not 100% of the project, will be done in Python. Hours Breakdown: Web Crawler design and setup: ~5 hours - This could take longer depending on how much customization is needed for each source I use, and depending on my ability to re-use the same procedures and my ability to run my scrapers behind paywalls. Parsing and database setup: ~5 hours - Once I have the articles saved as documents, I believe parsing should be very straightforward, as done in lecture and the MPs. Design of Sentiment Analysis code: ~7 hours - This will take the bulk of my effort. I believe the most challenging part will be to divide articles that discuss both candidates and group each sentiment with the correct candidate. Statistical analysis of results: ~3 hours Documentation Python Files cnn_scraper.py: Used to scrape CNN articles. Accesses the publicly available CNN API. fox_scraper.py: Used to scrape Fox News articles. Accesses the back-end API to scrape article information, then uses those links to access articles. reuters_scraper.py: Used to scrape Reuters articles. Scrapes list of articles and then uses those links to access articles. ibm_sentiment.py: Uses the IBM Watson Natural Language Understanding API to analyze the sentiment of each corpus of articles. Targets are set to analyze sentiment for keywords ""Trump"" and ""Biden"". See inline comments for detailed descriptions of code functions. Note that the scrapers take a long time to run (sometimes upwards of 1-2 hours), while the sentiment analysis takes ~20-30 minutes to run. Output Files XXX_urls.csv: contains data on each corpus. All files contain the publish date, headline, url, and trump/biden sentiment scores. Depending on the source, may also include authors or category. XXX_scores.csv: output of ibm_sentiment for each file. This can be manually copy/pasted to the respective XXX_urls file to populate those columns. XXX_body.txt: output of XXX_scraper scripts. Each file contains one article per line, in the same order as the respective XXX_urls file. Other File chromedriver: necessary for selenium package to operate the scraper API Documentation The IBM Watson Natural Language Understanding API documentation can be found at https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#sentiment Other attributions Select portions of the scraper codes have been adapted from MP2.1. Video Demonstration https://mediaspace.illinois.edu/media/t/1_ksj5ytyq"
https://github.com/Clara9/LARA_Reproduce_410	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/CoreyShih/CourseProject	"Progress Report: Text Classification Competition Corey Shih (coreys2@illinois.edu) Progress: So far, the data preprocessing pipeline and model definition have been completed. I decided to try using the recently developed BERT transformer encoder architecture rather than an LSTM as I originally planned for this project. Most of my time thus far has been dedicated to preprocessing the data and learning how to use BERT in Tensorflow. At present, I decided to use only the most recent context element in conjunction with the response text in the model for simplicity and to cut down on computation. Should the results of such a model not prove sufficient I will look into adding the rest of the context back into the model input. Remaining Tasks: Model training/testing and hyperparameter tuning still has yet to be done. Following that, the source code submission/documentation and the video demo will have to be completed. Challenges/Issues: At present, I am simply concatenating the response tweet with the context to feed into the BERT model as a single input. Ideally, I would like to have multiple inputs to the model, i.e. one input for the response text and one input for the context text, but I have been running into issues getting that to work in Tensorflow. I am still working on this aspect of the model. Project Proposal: Text Classification Competition Corey Shih (coreys2@illinois.edu) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This will be an individual project; as such, I will be the team captain. My name and NetID are at the top of this proposal. 2. Which competition do you plan to join? I plan on joining the text classification competition. 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I am prepared to learn deep learning techniques for state-of-the-art text classification. I am familiar with Keras and TensorFlow, having used the former for time series forecasting with recurrent neural networks and the latter for various deep learning tasks. I have previously utilized TensorFlow for projects involving audio classification and computer vision. I will likely implement an RNN or LSTM using one of these frameworks for this project. 4. Which programming language do you plan to use? I plan on using Python for this project, as both Keras and TensorFlow are Python libraries. Text Classification Competition: Twitter Sarcasm Detection This course project is an entry into the CS 410 Fall 2020 text classification competition. More details about the competition and the relevant datasets can be found at https://github.com/CS410Fall2020/ClassificationCompetition. Implementation details This project utilizes a BERT (Bidirectional Encoder Representations from Transformers) model to classify the data. BERT is a recent NLP language model developed by Google in 2018. The specific BERT model fine-tuned in this project can be found here. The classifier model constructed in this project consists of a BERT preprocessing layer, followed by the BERT model, a Dropout layer, and a Dense layer. It uses the response tweet and last context tweet as inputs. This was done for simplicity and also because, intuitively, the tweet that the response tweet is a direct response to is the most relevant context for determining sarcasm. No preprocessing was performed on the text data itself prior to input to the BERT preprocessing model (e.g. stop word removal, stemming, etc.). The model was trained using a sparse categorical cross-entropy loss function and an AdamW optimizer. This model proved sufficient to beat the baseline provided by the competition, so no other models or methods were tried. Minimal hyperparameter tuning was performed, with only the number of training epochs being adjusted for best results and to avoid overfitting. The code for this project predominantly follows the TensorFlow tutorial found here for using BERT on TPU in Google Colab, with modifications made to accomodate for the structure of the Twitter dataset. Utility testing methods Several utility methods are included in the code to assist with running the model on the test dataset and viewing the results. ```python def preprocess_dataset(dataset_path, split): """"""Processes Twitter sarcasm data into tf.data.Dataset. Args: dataset_path: str path of jsonl dataset. split: str designating train or test dataset, either 'train' or 'test'. Returns: A tf.data.Dataset of the Twitter sarcasm data, retaining only the response tweet, the last context tweet, and the label if present. """""" ``` ```python def prepare(record): """"""Prepares records from processed dataset for prediction. Args: record: dict of str Tensors. Returns: A list of lists of str Tensors. """""" ``` ```python def get_result(test_row, model): """"""Predicts whether a Twitter sarcasm test example is sarcasm or not sarcasm. Args: test_row: list of str Tensors. model: TensorFlow SavedModel for the sarcasm classifier. Returns: A str, either 'SARCASM' or 'NOT_SARCASM', corresponding to the predicted result. """""" ``` ```python def print_result(test_row, model): """"""Prints out the context, response, and predicted label for a Twitter sarcasm test example. Args: test_row: list of str Tensors. model: TensorFlow SavedModel for the sarcasm classifier. Returns: None. """""" ``` Usage examples The following code snippets provide a few examples for running the trained classifier on the Twitter sarcasm test set. Loading the trained model python load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost') reloaded_model = tf.saved_model.load(saved_model_path, options=load_options) Downloading and preprocessing the test dataset ```python test_url = 'https://raw.githubusercontent.com/CS410Fall2020/ClassificationCompetition/main/data/test.jsonl' test_path = tf.keras.utils.get_file('test.jsonl', test_url) test_dataset = preprocess_dataset(test_path, 'test') ``` Printing some classifier results on the test set python for test_row in test_dataset.shuffle(1800).map(prepare).take(5): print_result(test_row, reloaded_model) ``` context: [b'@USER @USER @USER It \xe2\x80\x99 s obvious I \xe2\x80\x99 m dealing with a double digit IQ . Have a good life .'] response: [b'@USER @USER @USER Hahahahahah What a chump . No testicular fortitude at all . It \xe2\x80\x99 s unsurprising that liberals lose with people like this . '] prediction: SARCASM context: [b'@USER @USER asked me to respond to @USER . See attached . Thanks for the opportunity . #KXL '] response: [b'@USER @USER @USER Imagine that . A politician making baseless accusations . Because has * never * done that before .'] prediction: SARCASM context: [b'@USER @USER By all means you should initiate another failed impeachment , causing further embarrassment ( if that were even possible ) to your party , then go tear up some official documents like a toddler .'] response: [b'@USER @USER @USER Yet you have no shame in supporting the biggest criminal in White House history .'] prediction: SARCASM context: [b'@USER @USER Aaaayyyyyeeee I \xe2\x80\x99 m the Hypemobile humie . I gatchu ! ! ! \xf0\x9f\x98\x9c \xf0\x9f\x91\x8c \xf0\x9f\x8f\xbc'] response: [b'@USER @USER Oh ... OHHHH ! That \xe2\x80\x99 s how it \xe2\x80\x99 s gonna be ? My two bestfriends just gon \xe2\x80\x99 team up on me ? \xf0\x9f\x98\xa1'] prediction: NOT_SARCASM context: [b""@USER @USER @USER Thank You so much , Diablo , My Dearest Friend ! I am grateful every day , as I am happy every day . I make those choices every day for me ! It doesn't matter what is going on , my choices stick . I give the gift of positivity to myself , to everyone I touch ! LOVE U XOXO ""] response: [b'@USER @USER @USER All good things are possible when the #heart illuminates the mind #ThinkBIGSundayWithMarsha #InspireThemRetweetTuesday '] prediction: NOT_SARCASM ```"
https://github.com/Diegoma89/CS410_CourseProject_DM	"ACross-CollectionMixtureModelforComparativeTextMiningChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbanaChampaignAtulyaVelivelliDepartmentofElectricalandComputerEngineeringUniversityofIllinoisatUrbanaChampaignBeiYuGraduateSchoolofLibraryandInformationScienceUniversityofIllinoisatUrbanaChampaignABSTRACTInthispaper,wede neandstudyanoveltextminingproblem,whichwerefertoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcom-monthemesacrossallcollectionsaswellassummarizethesimilarityanddi erencesofthesecollectionsalongeachcom-montheme.Thisgeneralproblemsubsumesmanyinterest-ingapplications,includingbusinessintelligenceandopinionsummarization.Weproposeagenerativeprobabilisticmix-turemodelforcomparativetextmining.Themodelsimul-taneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.ThemodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algo-rithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.CategoriesandSubjectDescriptors:H.3.3[Informa-tionSearchandRetrieval]:TextMiningGeneralTerms:AlgorithmsKeywords:Comparativetextmining,mixturemodels,clus-tering1.INTRODUCTIONTextminingisconcernedwithextractingknowledgeandpatternsfromtext[5,6].Whiletherehasbeenmuchre-searchintextmining,mostexistingresearchisfocusedononesinglecollectionoftext.Thegoalsareoftentoextractbasicsemanticunitssuchasnamedentities,toextractrela-tionsbetweeninformationunits,ortoextracttopicthemes.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.KDD'04,August22-25,2004,Seattle,Washington,USA.Copyright2004ACM1-58113-888-1/04/0008...$5.00.Inthispaper,westudyanovelproblemoftextminingre-ferredtoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcommonthemesacrossallcollectionsaswellassummarizethesimilarityanddi er-encesofthesecollectionsalongeachcommontheme.Specif-ically,thetaskinvolves:(1)discoveringthedi erentcom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Theneedforcomparativetextminingexistsinmanydi erentapplica-tions,includingbusinessintelligence,summarizingreviewsofsimilarproducts,andcomparingdi erentopinionsaboutacommontopicingeneral.Inthispaper,westudytheCTMproblemandproposeagenerativeprobabilisticmixturemodelforCTM.Themodelsimultaneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.Themixturemodelisbasedoncomponentmultinomialdistributionmodels,eachcharacterizingadi erenttheme.Thecommonthemesandcollection-speci cthemesareexplicitlymodeled.Thepro-posedmodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algorithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.Therestofthepaperisorganizedasfollows.InSection2,webrie yintroducetheproblemofCTM.Wethenpresentabaselinesimplemixturemodelandanewcross-collectionmixturemodelinSection3andSection4.WediscusstheexperimentresultsinSection5.2.COMPARATIVETEXTMINING2.1AmotivatingexampleWiththepopularityofe-commerce,onlinecustomereval-uationsarebecomingwidelyprovidedbyonlinestoresandthird-partywebsites.Pioneerslikeamazon.comandepin-ions.comhaveaccumulatedlargeamountsofcustomerinputincludingreviews,comments,recommendationsandadvice,etc.Forexample,thenumberofreviewsinepinions.comismorethanonemillion[4].Givenaproduct,therecouldbeuptohundredsofreviews,whichisimpossibleforthereaderstogothrough.Itisthusdesirabletosummarizeacollectionofreviewsforacertaintypeofproductsinordertoprovidethereadersthemostsalientfeedbacksfromthepeers.Forreviewsummarization,themostimportanttaskistoidentifydi erentsemanticaspectsofaproductthatthereviewersmentionedandtogrouptheopinionsaccord-ingtotheseaspectstoshowsimilaritiesanddi erencesintheopinions.Forexample,supposewehavereviewsofthreedi erentbrandsoflaptops(Dell,IBM,andApple),andwewanttosummarizethereviews.Ausefulsummarywouldbeatab-ularrepresentationoftheopinionsasshowninTable1,inwhicheachrowrepresentsoneaspect(subtopic)anddi er-entcolumnscorrespondtodi erentopinions.Table1:AtabularsummarySubtopicsDellIBMAppleBatterylifelongenoughshortshortMemorygoodbadgoodSpeedslowfastfastItis,ofcourse,verydicult,ifnotimpossibletopro-ducesuchatablecompletelyautomatically.However,wecanachievealessambitiousgoal{identifyingthesemanticaspectsandidentifyingthecommonandspeci ccharacter-isticsofeachproductinanunsupervisedway.Thisisaconcreteexampleofcomparativetextmining.2.2ThegeneralproblemTheexampleaboveisonlyoneofthemanypossibleappli-cationsofcomparativetextmining.Ingeneral,thetaskofcomparativetextmininginvolves:(1)discoveringthecom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Itisveryhardtopreciselyde newhatathemeis,butitcorrespondsroughlytoatopicorsubtopic.Thegranularityofthemesisapplication-speci c.CTMisafundamentaltaskinex-ploratorytextanalysis.Inadditiontoopinioncomparisonandsummarization,ithasmanyotherapplications,suchasbusinessintelligence(comparingdi erentcompanies),cus-tomerrelationshipmanagement(comparingdi erentgroupsofcustomers),andsemanticintegrationoftext(comparingcomponenttextcollections).CTMischallenginginseveralways:(1)Itisacompletelyunsupervisedlearningtask;notrainingdataisavailable.(ItisforthesamereasonthatCTMcanbeveryusefulformanydi erentpurposes{itmakesminimumassumptionsaboutthecollectionsandinprinciplewecancompareanyarbitrarypartitionoftext.)(2)Weneedtoidentifythemesacrossdi erentcollections,whichismorechallengingthanidentifyingtopicthemesinonesinglecollection.(3)Thetaskinvolvesadiscriminationcomponent{foreachdiscov-eredtheme,wealsowanttoidentifytheuniqueinformationspeci ctoeachcollection.Suchadiscriminationtaskisdif- cultgiventhatwedonothavetrainingdata.Inaway,CTMgoesbeyondtheregularone-collectiontextminingbyrequiringan\alignment""ofmultiplecollectionsbasedoncommonthemes.Sincenotrainingdataisavailable,ingeneral,wemustrelyonunsupervisedlearningmethods,suchasclustering,toperformCTM.Inthispaper,westudyhowtouseprob-abilisticmixturemodelstoperformCTM.Belowwe rstdescribeasimplemixturemodelforclustering,whichrepre-sentsastraightforwardapplicationofanexistingtextmin-ingmethod,andthenpresentamoresophisticatedmixturemodelspeci callydesignedforCTM.3.CLUSTERINGWITHASIMPLEMIXTUREMODELqqqqqFigure1:TheSimpleMixtureModelAnaivesolutiontoCTMistotreatthemultiplecollec-tionsasonesinglecollectionandperformclustering.Ourhopeisthatsomeclusterswouldrepresentthecommonthemesacrossthecollections,whilesomeotherswouldrep-resentthemesspeci ctoonecollection(seeFigure1).Wenowpresentasimplemultinomialmixturemodelforclus-teringanarbitrarycollectionofdocuments,inwhichweassumethereareklatentcommonthemesinallcollections,andeachischaracterizedbyamultinomialworddistribu-tion(alsocalledaunigramlanguagemodel).Adocumentisregardedasasampleofamixturemodelwiththesethememodelsascomponents.We tsuchamixturemodeltotheunionofallthetextcollectionswehave,andtheobtainedcomponentmultinomialmodelscanbeusedtoanalyzethecommonthemesanddi erencesamongthecollections.Formally,letC=fC1;C2;:::;Cmgbemcomparablecol-lectionsofdocuments.Let1;:::;kbekthemeunigramlanguagemodelsandBbethebackgroundmodelforallthecollections.Adocumentdisregardedasasampleofthefollowingmixturemodel(basedonwordgeneration).pd(w)=Bp(wjB)+(1 Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Documentation Reproducing a Paper: A Cross-Collection Mixture Model for Comparative Text Mining Introduction My project selection is the reproduction of the paper titled ""A Cross-Collection Mixture Model for Comparative Text Mining"" cited at the end of this document. The idea is to propose an improvement over simple mixture model for comparative text mining by explicitly distinguish the collections being compared. This allows to generate topic language models that are specific to each collection along with a common topic language model that describes all collections for each cluster (See Figure 2). The model In a simple mixture model, the probability of a word is given by the sum of the portion of probability of being generated by a background model plus the probability of being generated by a topic model. Notice lB below is acting as a parameter that tunes the weight being applied to the background model. The parameter p is present on both models and simply describes the mix of topics of each document. In the proposed cross-collection mixture model, the number of topic language models grows from k in the simple mixture mode to k + mk, being k the number of topics, V the vocabulary size and m the number of collections. We also add another tunable parameter lC that acts as the weight we give to the common topic models. So, in simple term, the probability of a word in a given collection is the Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC sum of it being generated by the background model plus the probability of being generated by either the common topic model or the collection specific topic model. Implementation For the implementation I used as base the PLSA programming assignment. In the assignment we did not have a background model so a function ""build_background_model"" was implemented using the whole corpus as follows: The functions ""build_corpus"", ""build_vocabulary"", ""build_term_doc_matrix"" and ""initialize_randomly"" were mostly kept the same as they are also necessary for this model. They were only modified to explicitly separate the different m collections. For example, instead of relying in the ""self.documents"" matrix that contained all the documents, ""self.documents_collections"" was created which is a list of arrays that contain the documents of length m. A similar approach was used on the other mentioned functions. The EM algorithm was heavily modified as the number of parameters grew considerably compared to the PLSA assignment. The following formulas were used to update the E and M step: Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC However, I identified a mistake in the last formula that suggests a sum across collections to calculate the specific topic model probabilities which would end up removing the distinctions between collections and also removing one dimension of parameters. This was confirmed by the TAs as a mistake. Finally, to monitor the performance of the algorithm, a log-likelihood function essentially the same as the one on PLSA assignment but with the additional parameters was implemented following the given formula on the paper: I also made a simple mixture model implementation to compare the results, the EM updating formulas and log-likelihood estimation con be found on the referenced paper. Experiments The model was tested with 2 data sets, one with a collection of 59 news articles of the Iraq and Afghanistan war (26 from Afghanistan and 33 from Iraq war) from 2 different sources (BBC and CNN). The second data set are laptop reviews of 3 different models (a MacBook, a Dell XPS and a Lenovo Yoga) with a total of 250 reviews obtained from Amazon.com. lC and lB have to be tuned by the user. We can think of lB as how verbose the text is, the more it is, the more non-significant words we will find which in turn means we want to select a higher value. Generally, something higher than 0.9 is recommended. For lC, as we go higher, we enrich the common topic models but hurt the specific, and vice versa, so it should be set very carefully based on the data and where we want the most emphasis. The data sets used in the paper are not explicitly referenced so it was impossible to obtain the same exact data, which means I'm unable to reproduce the exact same response as in the paper and also have to tune my own l parameters. Here are my results: Table 1. Simple mixture model war news WordkitwolfowitzclothingnbcdesertbuyequipmentlogisticsProb0.037323960.033748320.020939880.019280820.019280820.014560990.01345110.0128544WordghraibabusymbolprisonguardsphotodemolishedusaProb0.064275160.047861240.020227670.019261760.015060550.011559320.01155930.0092431WordsarinshellnerveneillbowdenfilledagentoProb0.03780520.030472890.025144730.020773320.016898140.01603160.0145250.013795WordtalibanalladenbinalliancenorthernhehusseinProb0.015496120.009558440.007857280.007693620.007314750.006608030.00560290.0055208WordetapropagandablacktrainslegalityundermineplantingoptionsProb0.033312620.024646710.017162140.01665880.011297540.011297540.01129750.0112975Theme 1Theme 2Theme 3Theme 4Theme 5Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC While clustered words talk about certain topics common between both wars, it is not possible to make a distinction of which words belong to which collection. Table 2. Cross-collection mixture model war news In the cross-collection model we can for example see cluster 2, where the common theme talks about prisoners, prison, detainees, detention, guards, while in the corresponding collection specific cluster we can see the location were these prisoners were being held. The Guantanamo bay detention camp is referenced in the Afghanistan cluster, which is relevant, while the Iraq war references Abu Ghraib, which is also a prison relevant to Iraq war. The output of the model is a text file named either ""SimpMix_output.txt"" for the simple model and ""CCMix_output.txt"" for the cross-collection model. Also, due the nature of randomly initializing parameters, runs can vary so I included a few other runs in the ""test"" folder. In the same folder, output for the laptop reviews can be found that were omitted in this documentation . Challenges and opportunities for improvement During the implementation I encountered 2 issues that persisted and was not able to figure out. The first one is related to my log-likelihood estimation which as stated in the lectures, the EM algorithm monotonically increases the likelihood. I ran into the problem that at some point relatively early in the WordbinladensaudijihadvideoarabiaplotfamilyfatherauthoritiesProb0.189393290.09815520.035643890.032392760.031453530.026996850.01898020.01435030.013579550.01339957WordladensaudirecordedsudanyemenProb0.10280030.029140180.023326540.020354730.02033913WorddataritterbowdenmidubiousProb0.038068870.033829540.030007920.025357480.02112141WordprisonersprisondetaineesdetentionghraibguardsexecutedrightsamnestygenevaProb0.128743430.080340440.038784310.023954340.021159450.020676630.016515960.016429860.015953760.01563221WordtreatedguantanamocubabasebayProb0.039369060.036589370.036259780.027691930.02742886WordghraibabusymbolusaphotoProb0.079081940.075833430.021504380.013376950.01223926WordblastsspanishaznarsevenetakingpoliciescondolencesmadridbombsProb0.030224170.029323210.026623270.021310740.014819730.014304790.014007540.01219930.012110670.01150711WordcachebankdemonstratorsimfsoutheasternProb0.031692610.027304450.021309790.015980930.01584349WordetatrainsspainexpresskarbalaProb0.033059060.01984230.019364810.010580350.01055226WordpentagoncheneywolfowitzlibyaeuropeabroadjapancontroversialvicerumsfeldProb0.049137610.038560550.029938840.027173690.022786140.022178820.018653510.017191550.015985770.01430421WordfrenchkarachipropagandablackbusProb0.067937510.055029650.036755010.032300340.02751049WordwolfowitzlibyajapanesegadhafidispatchProb0.060584560.054050230.023174380.022277230.01039494WordtheoftothatandiniswasaweProb0.079755470.039632660.032142860.021823070.01874330.018015430.016765570.015571040.015376070.01396474WordtalibanafghanistanalliancenorthernkandaharProb0.084057060.034872290.034653080.031160970.02699336WordiraqsaddamiraqihusseiniProb0.043809530.02992540.020655320.012546490.00693652Cluster 5 - Collection Afghanistan warCluster 1 - Collection Iraq warCluster 2 - Collection Iraq warCluster 3 - Collection Iraq warCluster 4 - Collection Iraq warCluster 5 - Collection Iraq warCommon theme cluster 1Common theme cluster 2Common theme cluster 3Common theme cluster 4Common theme cluster 5Cluster 1 - Collection Afghanistan warCluster 2 - Collection Afghanistan warCluster 3 - Collection Afghanistan warCluster 4 - Collection Afghanistan warDiego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC iterations, the likelihood decreases for a few iterations and then goes back to improve without interruption until convergence. I was unable to find any issues with my EM algorithm or the calculation of log-likelihood. I'm inclined to believe this could be an underflow issue during the E-step. I tried implementing the normalization to avoid underflow technique in lecture 10.4 but it did not make a difference, so the cause remains unknown. The second problem was undefined divisions (division by 0) which caused the algorithm to crash. I was able to patch this issue by implementing pseudo counts in the M-step by adding a uniformly distributed prior with a parameter u = 1 as seen in lecture 9.9. This essentially guarantees each word ""appears"" at least once and avoids divisions by 0. I don't believe this diverges too much from reality so I thought it was a reasonable fix. Would be interested if the authors ran into a similar issue or if this is an implementation problem on my side. Conclusion The cross-collection mixture model can be a powerful tool when tuned optimally to create more meaningfully comparisons than the simple mixture model. In the experiments, for example the laptop reviews output, can be clearly used by manufactures to identify the weaknesses and strengths of their product while also learning valuable information of their competitors as analyzing reviews individually or pooled can easily misdirect us from what is really important. But clearly we can extract insights from any comparable collections, maybe m different medical treatments response based on testimony from patients, political ideologies on social media and where they intersect or differ, etc. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Reference ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Progress Report Reproducing a Paper: A Cross-Collection Mixture Model for Comparative Text Mining 1) Which tasks have been completed? * Similar test data has been compiled for both experiments described in the paper. I'm using about ~30 news articles per event (Afghanistan and Iraq war) from BBC and CNN news and a combined total of 250 reviews of 3 recent model of laptops (Lenovo, Dell and Apple) from Amazon.com. * Data has been roughly curated. Major spelling errors were removed. * I'm using the MP3 skeleton code as base since it is very similar to the paper structure-wise and can help me and reviewers follow along the code. * The following functions have been added or implemented: o Init variables - Added needed variables o Build corpus - Builds whole corpus and individual collections. Cleans data of punctuation and digits o Build vocabulary - Same as MP3 o Build term matrix - Added term matrix per individual collection o Build background model - This is a new function o Random initialization of parameters with normalization o Expectation step - First implementation o Maximization step - First implementation * Currently writing the clustered words to 2 different text files, one for the top ten words per topic on ""common.txt"" and top 10 words per collection per topic on ""specific.txt"" 2) Which tasks are pending? * Implement log likelihood function * Check for errors in algorithm (see question 3, challenges faced) * Interpret and report results in a friendly manner * Tune Lambda B and C parameters (background and Collections ""weights"") * Clean code 3) Are you facing any challenges? In the EM updating formulas presented on the paper, I have not figured out one operation circled in the image below. The formula states to sum the terms across all d's in Ci across all Cm (collections). If my understanding is correct, this is wrong as that would pool all the documents in the corpus together and lose the focus to a specific collection ending up with only ""k"" themes. My current implementation only sums across all d's in Ci and normalizes based on this sum across words. This results in ""k"" times ""number of collections"" specific theme models where all probabilities sum to 1 within each of them. Will reach out to TA if I get stuck debugging. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Proposal Reproducing a Paper: Contextual text mining 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Will work individually on the project. Captain: Diego Millan NetID: diegom3 2.Which paper have you chosen? The paper selected is one of the contextual text mining subtopic options, specifically the following: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150. 3.Which programming language do you plan to use? I plan on using Python. 4.Can you obtain the datasets used in the paper for evaluation? The paper references 2 datasets, the first one is about war news that compares the Iraq and Afghanistan war. The second dataset compares 3 laptop model reviews. I am not able to obtain the exact same datasets used on the paper. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? The paper references the BBC and CNN websites as the source of the news articles, also mentioning how many articles from each were selected and the time span (30 articles starting 1 year before the paper publication for the Iraq war and 26 articles on a 1 year span starting November 2001 for the Afghanistan war). Unfortunately, there is no way to know the exact articles used but a similar sample can be obtained from the same sources (will be using Google news to obtain random articles on the specified time span). Regarding the laptop dataset, the review source website (epinions.com) no longer exists which makes it impossible to get the same dataset. Since laptop reviews are readily available in numerous other sites (amazon.com could be a good replacement), my plan is to use more recent data to replace this dataset. I will be comparing a newer model of each of the 3 laptop brands referenced on the paper (Apple, Dell, and IBM). 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? Even though the exact same datasets cannot be obtained, the alternatives should be close enough to get comparable results to the ones concluded on the paper. CourseProject Link to Demo Video https://youtu.be/O_R6IzCWMGM This is the course project for CS410 Fall 2020 at UIUC. The project intent is to reproduce a paper on contextual text mining, specifically the refence below. ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 Please see Project Documentation.pdf for full explanation"
https://github.com/DiptamGit/CourseProject	Offensive Language Detection - Project progress report For our project we have decided to divide our entire project work into four basic modules, - View - Extractor - Analyzer - Repository View For view module we have decided on using react / dash combination, we may switch to static html webpage based on our needs, view will be used for two main reason, one from ui we will input what hashtags we want to search, that will be fed into our extractor system. Also, our ui will have separate dashboard, where will have visual elements of different datasets, like tweet counts, their overall sentiment, etc. Extractor This module will be used to get the tweets for a list of hashtags, we have decided to use Java for this module, we have signed up for Twitter developer account, we are using twitter official hbc api for getting our tweets, twitter/hbc: A Java HTTP client for consuming Twitter's realtime Streaming API (github.com) We have been able to complete the coding of this module, and we were able to get tweets from api successfully for particular hashtags, please find code snippets below, PAGE 1 And we are getting outputs like this, Now we are working on cleaning the tweets, so that we can use them directly to our analyzer module without any deformed text Analyzer We have decided on using Python for this module, there will be python script running in background, where we will feed our cleaned tweets, and we will scan for offensive words in that text and mark that tweet accordingly, also we are planning for doing sentiment analysis of the tweets, we are still deciding on that topic. Repository We plan on storing all our analyzed tweets on mongoDB on cloud, so that we use data from our UI component. Sentiment Analyis and Offensive Language Detection By: Riya Gupta, Chitra Uppalapati, and Diptam Sarkar What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - riyag3 (captain), chitrau2, diptams2 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic is conducting sentiment analysis as well as polite and impolite language detection on a set of aroundtop tweets from the daily trending hashtags, dynamically. We are trying to categorize the most popular tweets as positive, negative, depending on the topic. This is an interesting project because it allows us to sense what the overall attitude and feeling is towards certain ideas when examining the most relevant tweets per hashtag. This can allow us to find certain patterns and sentiments in the trending hashtags, which can help us identify how people feel about popular discussions and products as well as how polarized specific topics on Twitter are. This project will be most helpful for identifying sentiment towards political discussions as well as new products. We plan to use Twitter HBC, the Java HTTP client for accessing the Twitter API, to fetch our 1,000 tweets. Then, we will build a system that classifies the tweets. When analyzing our data, we will experiment with different classifiers and evaluate our system using the standard classification evaluations metrics (Precision, Recall, and F-score). The expected outcome is to display our results on a web app that we will create using React. Which programming language do you plan to use? - Python, Java, Javascript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: - Dynamically get tweets from daily trending hashtags (20 hours) - Input hashtags from our web app - Clean our data - store in local - Perform sentiment analysis/language detection (positive, negative,) (20 hours) - Feed tweets into modules for language analysis - Update tweets with language analysis information - Output results on a web app (20 hours) - Create a good UI - Add information on how to use the tool/its purpose About the project There is a growing trend amongst enterprises now a days to do market study using sentiment analysis and profanity checking, when any company releases a new product for announce a merger, they normally introduce a hashtags or campaign slogan, and leveraging that they try to collect user data from social media to get an overview. Our goal pf this project was to provide a unified solution, which anyone can use for researching a keyword or hashtags and do sentiment analysis on that dataset with a view. We tried to make our entire app dynamic and, so that it will readily deployable and open to any further enhancement. Installation Instructions To run the code in your local system you will need four things, assuming you already have git installed Java 8 Maven 3+ Node js 12 Python 3 Installation instructions which I found helpful: - For Java you can use this link, we used grallvm for dev use, but any jdk would work as long as its 8, please refrain from using 11, it may not run spring modules. - Node Js use this link - Python 3, we used Anaconda distribution but technically any 3 distribution should work - Maven 3.6 is being used in this project but you can use any 3 above version as well Once you install and everything is setup, go your command prompt and check if they are in your path and setup was correct. sh $ java -version $ node -v $ python -version $ maven -v If all of them responded correctly, please proceed to the next section How to run the application Clone the application and you should see three main folder - data analyzer - python web and sentiment analysis module - file-read-api - node api for monitoring - twitter-api - spring web module for two twitter api and web views Open three command prompt, and cd into three separte folder run below from twitter api sh mvn -v spring-boot:run run below from data analyzer sh pip install flask pip install nltk pip install preprocessor pip install profanity-check python api.py run below from file-read-api sh node app.js if eveything is running and you can see no error in terminal, open any web browser(except older ie) and type (http://localhost:8080/search.html) ### Tools/Languages - Intellij IDEA - VS Code - Java - Spring - HTML/JS/CSS - Jquery - Bootstrap - NodeJs - Python Team Diptam Sarkar Riya Gupta Chitra Uppalapati ### Project Demo Presentation Credits We referred ideas and codes for inspiration from Spring official documentation, NLTK sentiment analyis examples, Bootstrap 5 docs, freecodecamp sentiment analysis youtube videos
https://github.com/DrLucky2/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Ehaly/CourseProject	"Project Documentation Lin Yutong Yutong21@illinois.edu 1. Overview This software is designed to complete the text mining task by a contextual generative model, which is an extension of the PLSA generative model. Several texts from different collections, several parameters (lambda c, lambda b, number of clusters etc.) will be passed in, and the software will produce the matrix of clusters and top k words in each language model (common model, collection-specific models) in the form similar to the original paper shown in Figure 1. The idea is that a word could be generated from a common theme model with lambda c probability, but also has (1 - lambda c) probability to be generated from a collection specific theme model. The probability of a word given the collection is shown in Figure 2, and the mixture model is illustrated in Figure 3. Figure 1: example output Figure 2: Pd(w|Ci) Figure 3: the cross-collection mixture model 2. Implementation Details The class Corpus consists of the following functions: __init__(document_path): initialize a Corpus object. build_corpus(): read the document in document path, store the collection number and the document in self.collection and self.documents. build_vocabulary(): read the documents and build the vocabulary for the whole dataset. build_background_model(): build the background model from the whole dataset. build_term_doc_matrix(): Construct the term-document matrix where each row represents a document, each column represents a vocabulary term.self.term_doc_matrix[i][j] is the count of term j in document i. initialize(self, number_of_collections, number_of_clusters, random=True): initialize the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob. expectation_step(number_of_collections,number_of_clusters,lambda_b, lambda_c): the E-step updates the P(zd ci w = j), i.e. the topic_prob matrix, p(zd,Ci,w = B) i.e. the bg_prob matrix, and p(zd,Ci ,j,w = C ), i.e. the common_topic_prob matrix. maximization_step(number_of_collections, number_of_clusters ): the M-step updates the , and . calculate_likelihood(number_of_collections,lambda_b, lambda_c): Calculate the current log-likelihood of the model using the model's updated probability matrices. ccmm(number_of_collections, number_of_clusters, max_iter, lambda_b, lambda_c, epsilon): execute the text mining on the document passed in in max_iter times of iteration. In each iteration, execute the E-step and the M-step, calculate the likelihood. Stop when the likelihood converges and print the topic models. The function ccmm(*) is the core function of the project. The update of each matrix and the calculation of likelihood are based on the following equations. Figure 4: log-likelihood calculation Figure 5: E-step and M-step updates 3. Usage Documentation The project uses two examples to test the text mining performance, both are similar from the example in the original paper. The data in the first example was scraped from BBC and CNN websites. The news URLs were selected by the author, so the contents are different from the news used in the original paper. The data in the second example was scraped from BestBuy.com, which are customer reviews on three kinds of laptop (Macbook-air-13-3-laptop, Dell-g5-15-6-fhd-gaming-laptop, Lenovo-yoga-c940-2-in-1-14-touch-screen-laptop). The scraper code and the scraped texts could be found in the project folder. To run the scraper code, run ""jupyter notebook"". The code could be run by command ""python model.py -h"". To run the first example, ""python model.py --document wars_news.txt --clusterNumber 5 --collectionNumber 2 --c 0.25 --b 0.91"" To run the second example, ""python model.py --document laptop_reviews.txt --clusterNumber 4 --collectionNumber 3 --c 0.7 --b 0.96"". Notice here we set a smaller cluster number than the original paper, due to the content difference and the worse performance with 8 clusters in experiment. The result will be saved in results.txt. An example output of the first example text mining. The collection 0 is the Iraq-theme model, and the collection 1 is the Afghanistan-theme model. 4. Work Distribution This is an individual project. All work was done by the author. Project Proposal Yutong21 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. It's an individual project. Yutong21. Which paper have you chosen? A Cross-Collection Mixture Model for Comparative Text Mining. Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes. The data is from CNN news and from epinions.com. Though we don't know which slice exactly, I could download similar data from those sources. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N.A Project Progress Report Yutong Lin NetId: yutong21 Project Introduction The project is to reproduce the paper ""A Cross-Collection Mixture Model for Comparative Text Mining"". The project is in its final phase so far. The program is improved based on the PLSA model, which includes an expectation algorithm step, a maximization algorithm step and a likelihood calculation step. The new mixture model has more matrix to update. In the PLSA model, there is a document topic coverage matrix and a topic-specific language model matrix. In the new CCMM (Cross-Collection Mixture Model) has a collection-specific topic language model. It is designed as a 3D array. I finished the code and tried running data to test its performance. The original paper used two datasets for experiments. The first is a news dataset including about 30 articles on Iraq War and 30 articles on Afghanistan War from BBC and CNN between 2003 to 2004. I wrote a scraper to scrape from BBC and CNN websites (scraper code in news_scraper.ipynb, scraped data in wars_news.txt ). The second dataset is customer reviews on three types of laptop from Apple, Dell and IBM from epinion.com. However, the epinion.com website has been shut down, so I scrape 84 customer reviews from BestBuy.com on the M1 chip Macbook Air, Dell g5 fhd gaming laptop and Lenovo Yoga c940 (scraper code in reviews_scraper.ipynb, scraped data in laptop_reviews.txt). The model performs well and could print the common theme model and the collection-specific theme model matrix in terminal by command ""python model.py"". The format is the same as the original paper. The column stands for different clusters, and the probability of the word decreases by row. Remaining Challenge The remaining challenge is to find a better set of parameters to pass in, that could make each cluster more distinct make the top 5 words more representative. At present, the clusters seem to be too much for the laptop review dataset, so each cluster is not very distinctive. Also, there remains a question in the maximization step, I shall contact the professor to check my implementation. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Please read Project_Documentation for more details. The video could be found at : https://mediaspace.illinois.edu/media/t/1_n26fno8k To run the scraper code, run ""jupyter notebook"". The code could be run by command ""python model.py -h"". To run the first example, ""python model.py --document wars_news.txt --clusterNumber 5 --collectionNumber 2 --c 0.25 --b 0.91"" To run the second example, ""python model.py --document laptop_reviews.txt --clusterNumber 4 --collectionNumber 3 --c 0.7 --b 0.96"" The class Corpus consists of the following functions: init(document_path): initialize a Corpus object. build_corpus(): read the document in document path, store the collection number and the document in self.collection and self.documents. build_vocabulary(): read the documents and build the vocabulary for the whole dataset. build_background_model(): build the background model from the whole dataset. build_term_doc_matrix(): Construct the term-document matrix where each row represents a document, each column represents a vocabulary term.self.term_doc_matrix[i][j] is the count of term j in document i. initialize(self, number_of_collections, number_of_clusters, random=True): initialize the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob. expectation_step(number_of_collections,number_of_clusters,lambda_b, lambda_c): the E-step updates the P(zd ci w = j), i.e. the topic_prob matrix, p(zd,Ci,w = B) i.e. the bg_prob matrix, and p(zd,Ci ,j,w = C ), i.e. the common_topic_prob matrix. maximization_step(number_of_collections, number_of_clusters ): the M-step updates the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob calculate_likelihood(number_of_collections,lambda_b, lambda_c): Calculate the current log-likelihood of the model using the model's updated probability matrices. ccmm(number_of_collections, number_of_clusters, max_iter, lambda_b, lambda_c, epsilon): execute the text mining on the document passed in in max_iter times of iteration."
https://github.com/ElizWang/CourseProject	"1) An overview of the function of the code (i.e., what it does and what it can be used for). Our code scrapes authors and paper titles from the DBLP dataset. We extract patterns using a Python wrapper for the SPMF library. From these extracted patterns, our code then removes redundant patterns -- we implemented both hierarchical microclustering and one-pass microclustering. Finally, we extract strongest context indicators, representative transactions, and semantically similar patterns. We can use this code to reproduce the DBLP experiment as described in section 5.1 of the paper. That is, we can take in an author or title pattern and find its strongest context indicators, most representative titles and authors / co-authors, and the most semantically similar title / author patterns. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Prerequisites: the libraries mentioned below (see answer #3), Python3, Java, shell Web Scraping (utils/build_data_from_web.py) *Build a CSV file called data/data.csv where each line is a list of comma-separated authors and a single title (aka all the information needed from a single paper). *These papers are pulled from the following text mining/database-related conferences: (aciids, icdm, sdm, dba, balt, dbsec, dbcrowd, pkdd, kdd, trec, cikm, sigir). *For each conference, we pull all papers from the most recent 10 events (workshops, talks, submissions, etc) *For each paper, we stem the titles using the nltk.stem.porter stemmer, remove commas and periods (because we are writing the data as a .csv file), and make all author names lowercase in order to avoid interpreting the same author names with different capitalizations as different authors. We also remove all spaces within a single author name and replace them with underscores (firstname_lastname) *Usage: python3 utils/build_data_from_web.py. Note that you must run this from CourseProject/, the data/ directory must already exist, and you must be using Python3. Utility file: Python wrapper for spmf.jar (utils/frequent_pattern_mining/spmf_python_wrapper.py) *Motivation: spmf is a Java library. To make everything run smoothly and to minimize the amount of work the user has to do to run the pattern mining algorithms and set everything up, we wrote a Python wrapper for spmf that runs Java as a subprocess. *Java -jar libs/spmf.jar run [alg_name] [input_file] [output_file] Frequent pattern mining: (utils/frequent_pattern_mining/build_frequent_patterns.py) *Builds frequent pattern files for authors and title terms and caches them to a file. *Maps all unique author names to non-negative integers and all unique title terms to non-negative integer IDs as well -- this is because of 2 reasons: first of all, it's more efficient to work with numbers rather than strings of a theoretically arbitrary length; also, spmf is a Java library that works with numbers rather than strings. Note that the two mappings are completely disjoint; in other words, an author name with an ID of 0 has nothing to do with a title term with an ID of 0. *Runs our Python wrapper for spmf.jar for FP-Close (to mine frequent patterns from author terms) and Clo-Span (to mine sequential patterns from title terms) to generate intermediate patterns we would then parse. *Files generated *data/frequent_author_patterns.txt: Frequent author patterns (via FP-Close), where each line is a pattern. Format: 11391 11393 11392 14928 #SUP: 9 *data/frequent_title_term_patterns.txt: Frequent title term patterns (via Clo-Span), where each line is a pattern. Format: 4 -1 226 -1 240 -1 #SUP: 32 *data/author_id_mappings.txt: Mapping from author ID to author name. Format: 0 helun_bu *data/title_term_id_mappings.txt: Mapping from title term ID to title term. Format: 0 toward Redundant pattern removal: (utils/remove_redundant_patterns.py) *Two utility methods for removing redundant patterns: *Eliminate redundancy using one-pass microclustering *Eliminate redundancy using hierarchical microclustering *The main method is currently written to support one-pass microclustering -- a threshold can be passed in or the default threshold can be used. The cleaned title term patterns are then written to a file. *Usage: *title_patterns = parse_author_file_into_patterns(FrequentPatternBuilder.TITLE_TERMS_OUTPUT_FILE_PATH) *minimal_patterns = find_one_pass_microclustering_patterns(title_patterns, 0.6) # To use one-pass microclustering *minimal_patterns = find_hierarchical_microclustering_patterns(title_patterns, 0.6) # To use hierarchical microclustering *write_patterns_to_file(MINIMAL_TITLE_TERMS_FILENAME, minimal_patterns) Utility file: Cosine similarity (utils/cosine_similarity.py) *Compute cosine similarity given two context vectors of the same length. Used as a utility method *Usage: consine_sim = compute_cosine_similarity(context_vec_1, context_vec_2) Utility file: Mutual information computation/caching manager (utils/mutual_information_manager.py) *Computes mutual information for two given ordered collections of patterns (which can be author-author patterns, author-title patterns, or title-title patterns) and caches the mutual information values to a file for each (pattern 1 ID, pattern 2 ID) pair, where each pattern ID just corresponds to its index in the ordered collection. *Each file begins with a value [0, 3], which corresponds to a mapping: 0 = author-author, 1 = author-title, 2 = title-author, 3 = title-title *Example file format: #2 #0 0 0.000016 #0 1 0.000016 #0 2 0.000016 #0 3 0.000016 #0 4 0.000016 #0 5 0.000016 #0 6 0.000016 #0 7 0.000016 #0 8 0.000016 #0 9 0.000016 *Files generated #data/author_author_mutual_info_patterns.txt = Mutual information values for each author-author pattern pair, where index_1 <= index_2 #data/author_title_mutual_info_patterns.txt = Mutual information values for each author-title/title-author pattern pair #data/title_title_mutual_info_patterns.txt = Mutual information values for each title-title pattern pair, where index_1 <= index_2 *Usage: #transactions = TransactionsManager(""data/data.csv"", ""data/author_id_mappings.txt"", ""data/title_term_id_mappings.txt"") #mutual_info = MutualInformationManager(transactions, True) #mutual_info.compute_mutual_information(author_patterns) *Reads in the mutual information from one of the files described above, depending on what the type of manager (author-author, author-title, or title-title manager), and caches all the mutual information values in a large matrix -- which is triangular for the author-author/title-title cases and rectangular for the author-title case *Usage: *mutual_info = MutualInformationManager(MutualInformationManager.PatternType.X) *mutual_info.read_mutual_information_from_file() *mutual_info.get_mutual_information(1, 2) # to get mutual info for patterns 1 and 2 *Used to compute mutual information one time (which is expensive) and to abstract how mutual information is stored and managed into a class Utility file: Transaction/paper data parser/manager (utils/transactions_manager.py) *Parses and stores author-id mappings, title-term-id mappings, and a list of all papers, which are basically (author set, title-term sequence patterns) *Utility methods for *Compute context models for each paper's title terms given a list of frequent title patterns *Compute context models for each paper's authors given a list of frequent author patterns *Find all transaction IDs, which are represented as paper indices from the list of all papers parsed, that have a title pattern as a sequential subset *Find all transaction IDs, which are represented as paper indices from the list of all papers parsed, that have an author pattern as a non-sequential subset *Get author name from author ID, title term from author ID, get a paper's author from the paper ID, get a paper's title terms from the paper ID, and get the number of transactions (number of papers) *Usage: transactions = TransactionsManager(""data/data.csv"", ""data/author_id_mappings.txt"", ""data/title_term_id_mappings.txt"") Pattern annotator: Representative transaction extraction (pattern_annotators/representative_transaction_extractor.py) *Extracts representative transactions given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern *Usage: python pattern_annotators/representative_transaction_extractor.py [target_id] [k] [is author experiment] *Example: python pattern_annotators/representative_transaction_extractor.py 0 3 True *For an author experiment, the top k most representative titles from the transaction manager are displayed *For a title experiment, the top k most representative titles and the top k most representative authors from the transaction manager are displayed Pattern annotator: Semantically similar pattern extraction (pattern_annotators/semantically_similar_pattern_extractor.py) *Extracts semantically similar patterns given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern *Usage: python pattern_annotators/semantically_similar_pattern_extractor.py [target_id] [k] [is author experiment] *Example: python pattern_annotators/representative_transaction_extractor.py 1 10 False *For an author experiment, the top k most representative author patterns are displayed *For a title experiment, the top k most representative title term patterns are displayed Pattern annotator: Strongest context indicator extraction (pattern_annotators/strongest_context_indicator_extractor.py) *Extracts strongest context indicators given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern *Usage: python pattern_annotators/strongest_context_indicator_extractor.py [target_id] [k] [is author experiment] *Example: python pattern_annotators/strongest_context_indicator_extractor.py 2 15 True *For an author experiment, the given pattern is annotated with the top k strongest context indicators -- the top k author patterns and the top k title patterns *For a title experiment, the given pattern is annotated with the top k strongest context indicators -- the top k title patterns and the top k author patterns 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Libraries Used *urlllib (web scraping from DBLP) *bs4 (parsing scraped data from DBLP) *nltk (stemming) *spmf (Java library to mine frequent patterns -- will be installed as a part of the setup step, see the next section) *Installation: pip install urllib/bs4/nltk Setup and Installation NOTE: Our data files are quite large; thus, we put the data/ directory in our .gitignore and we're subsequently not committing any of our data files or our library files to our repository, which is a standard in good software design. *git clone  https://github.com/ElizWang/CourseProject.git  # Clones repository *cd CourseProject # Navigate to repository *sh setup.sh # Sets up the data/ and libs/ directories and runs all scripts from the preprocessing step. Detailed description below: *Creates a data/ directory if one does not already exist *Builds a csv file called data/data.csv by scraping DBLP paper submissions from the web *Creates a libs/ directory if one does not already exist and pulls spmf as libs/spmf.jar *Mines frequent author patterns using FP-Close *Mines sequential title term patterns using Clo-Span *Removes redundancies from sequential title term patterns using microclustering *Computes and caches all mutual-information values between (author pattern, author pattern), (title term pattern, author pattern), and (title term pattern, title term pattern) 4) Brief description of contribution of each team member in case of a multi-person team. We worked together in understanding the paper and formulating pseudocode that we used as our overarching plan. We split the paper into two steps -- preprocessing (which includes data scraping, pattern mining, and similar pattern pruning) and pattern annotation. We worked as a team by calling each other on Zoom and pair programming. Elizabeth typed, while Steven frequently took remote control in implementing preprocessing, extracting representative transactions, and the extraction of strongest context indicators and semantically similar patterns. 5) Software demo link:   https://www.youtube.com/watch?v=3v8M0sW3xHc We have completed the implementation part of our project (i.e. the coding portion of the project is completed). In particular, we've mined author and title patterns using FP-Close and CloSpan respectively and implemented redundancy removal using one-pass microclustering, extracting strongest context indicators / representative transactions / semantically similar patterns, and used these to reproduce the DBLP experiment. We may need to touch up some documentation, and we need to record the tutorial presentation. Aside from potential technical issues with recording the tutorial (which would most likely be minor), we do not anticipate any challenges at this time. 1.What are the names and NetIDs of all your team members? Who is the captain? Elizabeth Wang, eyw3, Captain Steven Pan, stevenp6 2.Which paper have you chosen? We have chosen the following paper: ""Generating semantic annotations for frequent patterns with context analysis"" 3.Which programming language do you plan to use? Python 4.Can you obtain the datasets used in the paper for evaluation? No 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, a more recent version of the dataset that derives from the dataset used in the paper can be found here:  https://dblp.org/faq/1474681.html 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?  N/A CourseProject Authors Elizabeth Wang Steven Pan Proposal NOTE: We've uploaded a PDF (Proposal.pdf) with the same information as we hae below: What are the names and NetIDs of all your team members? Who is the captain? Elizabeth Wang, eyw3, Captain Steven Pan, stevenp6 Which paper have you chosen? We have chosen the following paper: ""Generating semantic annotations for frequent patterns with context analysis"" Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, a more recent version of the dataset that derives from the dataset used in the paper can be found here: https://dblp.org/faq/1474681.html If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A Demo https://www.youtube.com/watch?v=3v8M0sW3xHc Setup Install bs4, urllib, and nltk (if they're not already installed) Run setup.sh (sh setup.sh) from CourseProject/ to Build the csv file containing all (author list, title) entries. The code that builds this data file is here: utils/build_data_from_web.py. This script will create a directory called data/ and create a csv file called data.csv within that directory -- CSV file format: author1, author2, author3, ... etc, Title (where each line in the CSV file corresponds to a single paper) Create the libs/ directory and download spmf.jar, which is a JAR file for the SPMF library (download link is also here: http://www.philippe-fournier-viger.com/spmf/index.php?link=download.php) Builds frequent patterns for authors and title terms -- data/frequent_author_patterns.txt and data/frequent_title_term_patterns.txt, where all words are mapped to unique ids and the id mapping is cached in these 2 files respectively: data/author_id_mappings.txt and data/title_term_id_mappings.txt. The code that builds these files is here: utils/frequent_pattern_mining/build_frequent_patterns.py Removes redundancies from sequential frequent title patterns (data/title_term_id_mappings.txt) and creates a new file called data/minimal_title_term_patterns.txt containing these minimal patterns Builds files to cache all mutual information values between pairs of author patterns, between pairs of author-title patterns, and between pairs of title patterns RELEVANT OUTPUT FILES FOR NEXT STAGE: * data/frequent_author_patterns.txt (ID mappings: data/author_id_mappings.txt) * data/minimal_title_term_patterns.txt (ID mappings: data/title_term_id_mappings.txt) * data/author_author_mutual_info_patterns.txt * data/author_title_mutual_info_patterns.txt * data/title_title_mutual_info_patterns.txt NOTE: utils/parse_patterns.py contains utility methods to parse patterns into data structures and write them to files, you may find these methods useful"
https://github.com/EsportsNoEyes/CourseProject	"CS 410 Project Proposal Group: LiveDataLab Admins Team Members *Yanbo Chen, ychen380 *Linfei Jing, ljing2 *Huaminghui Ding, hding14 *Captain: Yanbo Chen Project Topic Text classification competition We choose the classification competition and are fully prepared to learn state-of-the-art neural network classifiers. We have learnt about some classical machine learning algorithms such as Naive Bayes, Decision Tree. We also have basic knowledge about pattern mining from text files using  Apriori algorithm and FP-Growth.  Our team member used to write those algorithms from scratch without using predefined libraries to classify animals in a zoo's data-set. Those will be helpful for text classification algorithm design in the project. Programming Language Python. Problem to solve Given a training set containing tweets from Twitter, each with a label ""SARCASM"" or ""NOT_SARCASM"", train a classifier to predict the label for each tweet in the test set. Methods to use 1) Naive Bayes Classifier. Before we dive into neural network classifiers, we decide to apply Naive Bayes Classifier first and see if it performs well on the problem. 2) Neural Network Classifiers If Naive Bayes Classifier does not work well on predicting the label, we are prepared to learn some neural network classifiers such as LSTM and BERT. 3)Deep Learning Frameworks We plan to use some popular frameworks such as Keras library in Tensorflow. CS 410 Project Progress Report Group: LiveDataLab Admins Team Members *Yanbo Chen, ychen380 *Linfei Jing, ljing2 *Huaminghui Ding, hding14 *Captain: Yanbo Chen Progress Made Based on the plan in our project proposal, we designed and implemented a Naive Bayes Classifier to classify the given data into two categories ""SARCASM"" and ""NOT_SARCASM"". The performance of the current classifier has already beat the baseline. Remaining Tasks Some further tuning to make the Naive Bayes algorithm even more accurate. Challenges/Issues Being Faced What method to use for tuning the algorithm and we are considering if some weighted method could be used over Naive Bayes to make it more precise specifically for this task. CS410 Classification Competition Usage First clone the source code to local, and use this command to install all the dependencies pip3 install -r requirements.txt, assuming Python3 is used. Use command python3 prediction.py to generate the prediction results which is stored in answer.txt If there is any errors with nltk package when running the code, please try to install suggested additional dependencies to solve the issue. We also welcome our reviewers to schedule a live demo. Description of Algorithm As we planned in the project proposal, the first method we tried is classifier that based on Naive Bayes. The equation to compute the probability is We used Laplace smoothing when we calculate the probability of every single key in the ""SARCASM"" and ""NOT_SARCASM"" dictionary. The equation for calculating the probability is and . UNK stands for the words that we have not seen in the training data. D stands for the dictionary we used when we calculate the probability of its words. It can be either ""SARCASM"" dictionary or ""NOT_SARCASM"" dictionary. Alpha stnads for the laplace smoothing parameter we set before training, default to be 1.0. Count(W) stands for the number of times a specific word W appeared in the training data. V stands for the size of the corresponding dictionary. Overview of Functions & Implementation Details reader.py, provide helpers to load the datasets into proper data structures to be used by the algorithm loadFile(name,stemming,sarcasm,training): The helper function to load training data and test data. The parameter ""name"" indicates the directory path to the file of data. The parameter ""sarcasm"", a boolean variable, indicates whether the training data is labelled as ""SARCASM"" or ""NOT_SARCASM"". The parameter ""training"", a boolean variable, indicates whether the input data file is training or test. ""stemming"" is provided as an optional parameter to enable stemming. It returns a list containing the tweets. load_dataset(train_dir,dev_dir,stemming): It loads data and form structures that can be used by naive bayes algorithm using loadFile(). It returns lists indicating the label of each data entry. prediction.py, the wrapper file that is called by the user to run the our Naive Bayes Classifier main(args): It is a wrapper function that extracts data, run naive bayes algorithm and output prediction results using the functions provided by reader.py and naive_bayes.py naive_bayes.py, implementation of our Naive Bayes Classifier naiveBayes(train_set, train_labels, dev_set, smoothing_parameter): The wrapper function of our implemented Naive Bayes algorithm. do_unigram(train_set, trai_labels, dev_set, smoothing_parameter): It applies unigram model and predict the labels of tweets. get_probability(tweet,p_dict): It calculates the sum of the probability of a word in a dictionary. get_probability_dict(some_dict,word_count,smoothing_parameter): It calculates the probability of every single key in the ""SARCASM"" and ""NOT_SARCASM"" dictionary, including the words that we have not seen in the training data. We used the equations mentioned in the above section to calculate the probability. get_dicts(train_set, train_labels): It creates ""SARCASM"" dictionary and ""NOT_SARCASM"" dictionary respectively and store the count of the occurrences of all the ""SARCARSM"" words and all the ""NOT_SARCASM"" words Based on the training data given, we first tried to predict the labels using the response tweets without context tweets as our dictionary. The accuracy was around 0.69. Then we included the context tweets into our dictionary. This time the accuracy beat the baseline. We tried to tune the laplace smoothing parameter and it turned out that the default one gave the highest. We also tried with stemming using the PorterStemmer of nltk package but it does not improve the accuracy considerably. Contribution of team members We went through design of algorithms, coding, and documentation together."
https://github.com/Finkoy/CourseProject	CS 410 Progress Report 1. Which tasks have been completed? I have completed the task of preprocessing the data. I have also completed the task of implementing different models for the Text Classification Competition. I have tried using SVMCNN, CNN, and LSTM. I have also experimented a bit with logistic regression, RBF kernel SVM, and different combinations of those models. The baseline score has not been beat yet. 2. Which tasks are pending? Currently I am trying to fine-tune a BERT model on my preprocessed data. We will see if this beats the baseline or not. 3. Are you facing any challenges? Only with beating the baseline. The best F1 score achieved so far came from feature selection from a CNN trained on context data and TFIDF vectorization of the response data with LASSO to determine useful words. These features were combined and fed into a linear SVM. CS410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This team only contains one person, Brian Vien. The NetID is bvien2 and by default, he is the captain. 2. Which competition do you plan to join? I plan on joining the text classification competition. 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods I am prepared to learn state-of-the-art neural network classifiers. I have multilayer perceptron, recurrent neural networks, and convolutional neural networks. I have heard of TensorFlow, Keras, and PyTorch. I have a little bit of prior experience with multilayer perceptron from coursework and have seen demos of TensorFlow. I hope to learn more through this competition. 4. Which programming language do you plan to use? I am planning on using Python3.8. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Guanhuali2/CourseProject	Documentation: Functions of our code: *Given a root website (University's Web Page URL) and its name (e.g. UIUC, UCB ....), the program will automatically find its Faculty directory page. *After getting the Faculty directory page, the program will automatically find all of its faculty homepages. *Then according to the user's instructions, the program will output a given number of Faculty Information including their Phone, Email, past College, personal bio, Research Area, and earned Awards. *This program can be used for CS students to search desired colleges' professors, and get their information in a convenient way (the program will filter useful information for them). How code is implemented: *For the first part, we use Selenium to find and interact with College's search bar in order to find candidates for the Faculty directory page url (directory.py). Since some college's search system is not perfect, we combine search results from google to make sure our candidate list includes the true Faculty directory page url. Then we build Random Forest Classifier (rfclassifier1.py, one model - rfclassifier1) with manually collected training data to classify the correct directory page url.(getfeature.py) *Second part, we keep using Selenium and Requests/Bs4 to find candidates' url list of Faculty Homepage. Since there is too much candidate url, we use some conditions to filter out as much as possible url. Then building another Random Forest Classifier (rfclassifier2.py, one model - rfclassifier2) to find corrected Faculty Homepages. (getfeature.py) *Inside each Faculty Homepage, we first get the html file and filter out useless information (dataprocess.py), and make all useful information into a list. Then we build a tfidf vectorizer as the feature to train our text classifier (Random Forest.py, two models - Vectorizer and text_classifier). Then we use the classifier to classify our desired information (bio,education,awards,research interests). How to run the code: *All the main code is inside directory.py. In order to run it, just need to run ./directory.py and make sure the computer has the correct version of browser to fit with Selenium. And then input College Url, College Name, and desired number of output. *Environment requirement: beautifulsoup4, selenium, numpy, pandas, seaborn, scikit-learn, matplotlib Contribution of Team member: *Guanhua Li: Implementing the Selenium/Web crawler part and building models for classifiers. *Ruoyu Zhu: Collect dataset and find features for models. *Ziqi Li: Find features for models , scrape features from webpages and function1 coding. Team Name: LZL Team Member: Guanhua Li( guanhua2@illinois.edu ),Ruoyu Zhu( ruoyuz3@illinois.edu ),Ziqi Li( ziqili3@illinois.edu ) Team Project: Improving ExpertSearch System Primary Language: Python Team Leader: Ruoyu Zhu *What is the function of the tool? Given a university website as the root URL, our tool is able to classify the correct faculty directory pages. Then we are able to classify the correct faculty home webpage according to the directory pages. Thus, our tool can further extract useful information from faculty webpages(Bios, Awards, Teaching Courses, Email...) *Who will benefit from such a tool? College students that want to get detailed information about certain professors. *Does this kind of tools already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? Our tool will try to achieve higher accuracy and efficiency. *What existing resources can you use? In order to build classifier, Python packages such as scikit-learn and Pytorch In order to extract useful features and information from given URL website, Python packages such as Selenium and Requests/Bs4 *What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) Using Web Crawler technique to get some features (or URL features) for building and training classifiers (Decision Trees - Random Forest Classifier). Classify the correct Faculty Directory pages and Faculty Home Webpages. Using Text Retrieval Technique learned from CS410 courses to get relevant useful text from Faculty Home Webpages. *How will you demonstrate the usefulness of your tool? It can efficiently get detailed and correct faculty information. *A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) By November, Finish building the basic classifier for Faculty Directory pages and the training process for it, and starting to evaluate the classifier for further improvement. By 11/10, Finish building the Basic classifier for Faculty Home Webpages, starting the evaluation process, at the same time, starting building proper text retrieval algorithm for extracting useful info from Faculty Homepages. By 11/15 Finish all the classifiers, and make sure the high accuracy of each classifier. By December, Finish the whole project 1) Progress made thus far *Given the correct specific department of University, we are able to recognize correct faculty directory pages using classifiers. *Extract faculty web pages according to different directory pages. 2) Remaining tasks *Given correct Faculty pages, we still need to get useful information using some textual information retrieved methods. *Integrate all the code for different parts *Preparing a short presentation for the overall project 3) Any challenges/issues being faced *Since faculty directory pages are different among different departments, do we need to identify all departments' faculty directory pages given a University website, or particular department website will be given. *Since we are using classifiers to classify the correct website pages, we cannot ensure 100% accuracy on all the University. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team Name : LZL Team Member: Guanhua Li, Ruoyu Zhu, Ziqi Li Team Leader: Guanhua Li, Ruoyu Zhu directory.py is the main program that needs to run. Documentation is inside the code dumentation.pdf file Demo video is inside the Project Video Link file In order to run the program, you need to allow browser remote automation, Safari menu->Developer->allow remote automation
https://github.com/IanG89998/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/JackDeDobb/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Jagaskak/CourseProject	"Causal Topic Modeling with Time Series Feedback Akshaya Jagannadharao (akshaya2) Heidi Toussaint (heidist2) Hari Venkitaraman (hv4) Pre-setup -- Data This project requires access to New York Times data. There are two methods to collect articles published by New York Times from May 2000-Oct 2000. Gain access to the LDC corpus. You should parse the documents and only collect relevant articles (i.e. keep paragraphs that contain ""Bush"" or ""Gore). You can also collect the data by scraping the New York Times website. Details on how to do so can be found in the README file in the Data folder of the project repo. Ensure the data is stored locally Pre-setup -- Jupyter Notebook For both methods below, you will need to have a version of the data stored locally. We strongly suggest using Google Colab as it reduces the overhead of installing packages. To run on Google Colab: Click on link at top of Jupyter Notebook that says ""Open in Colab"" Make sure that the notebook is not in your Shared Drive Edit the paths to the New York Times and IEM datasets in the 3rd cell To run locally: Clone GitHub Open up Jupyter Notebook locally and open file ("""") How to install the software In Google Colab: Run cell block two (everything is pretty much already installed) Locally: Install the following libraries using pip (if you have not done so already) Gensim Spacy pyLDAvis Scipy Statsmodels Numpy Pandas Nltk For stopwords, run python -m nltk.downloader stopwords 2. How to use the software Run each cell block in the notebook. If you'd like to try out different parameters (i.e. change number of topics, decay param, etc.) you can edit the block and rerun it. 3. Example use case Improve topic modeling by using a causal reference (e.g. analyzing topics with external time series variables such as stock prices) Video Link: CS410_TeamEastToWest_FinalProject - Illinois Media Space Team EastToWest Akshaya Jagannadharao (akshaya2) | Heidi Touissant (heidist2) | Hari Venkitaraman (hv4) 1) An overview of the function of the code (i.e., what it does and what it can be used for). We are reproducing a paper in this project: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback. We are only reproducing the first experiment where the authors examine the 2000 U.S. Presidential Election. The code we implement here can be used to find causal relationships between two different datasets. To be more specific, we are trying to find a causal relationship between new articles and stock market prices. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Data: New York Times: There are two methods to collect articles published by New York Times from May-Oct 2000. One is to gain access to the LDC corpus. You should parse the documents and only collect relevant articles (i.e. keep paragraphs that contain ""Bush"" or ""Gore). If the corpus is unavailable to you, it is still possible to collect the data by scraping the New York Times website. Details on how to do so can be found in the README file in the Data folder. Iowa Electronic Market (IEM) Time Series: The procedure to collect the IEM data is detailed in the README file in the Data folder. Code: We are running everything inside a Jupyter Notebook (with Python version 3.7.5) so that we can easily create and display visualizations. The code can be broken down into 3 sections (we have taken the initiative to section it off into three cells). The first cell contains all the import statements. We have detailed the usage of software in section 3 of this document for further perusal. The second cell loads the New York Times dataset and the Iowa Electronic Market Time Series Data into data tables and creates BOW (bag of words) representations by article and date. You should only have to run this cell once unless you want to change the BOW representation. The third cell contains the code to run the topic modeling with time series feedback. There is an example use case documented in the README file as well as an example all in the jupyter notebooks. Challenges + Improvements + Differences between our results and theirs: *Delay in obtaining dataset from NYT corpus *We were unsure if we were able to obtain the dataset because of copyright permissions. Due to this, we were unable to start our project until 1 month before the deadline (and one week was technically break). *No efficient way to write code as a group remotely *We struggled to find a collaborative Jupyter Notebook environment. Jupyter Notebook is an important tool for our use to easily visualize the different outputs and data structures we were generating throughout the code (i.e. visualization for LDA model, visualization of results). *Missing dates in Time Series *There are multiple ways to handle missing dates in time-series data. Namely, take the last price, take the logistic regression, take an average, drop the missing date. #We took the easiest route and simply used the previous day's prices for the missing dates as there were only two (6/7 & 6/8) *Calculating mu remained unclear after referring to the referenced paper, attending multiple office hours, and consulting with other classmates. *We decided to go with a similar parameter already integrated with Gensim's LDA model, decay. *Some material was outside the scope of the course. We needed to understand how Granger Causality worked and the constraints of a time series feedback (namely the data should be stationary). *A typographical error in the paper was discovered while attempting to reproduce Table 1 (p. 3, table 1) *This caused confusion amongst the group and we spent some time trying to understand it. After attending office hours, the professor, a co-author of the paper, ultimately decided this was a typographical error *Our results do not replicate the results achieved in the paper. There is a clear difference in the progression of confidence and purity levels through multiple iterations. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Additional Instructions can be found in the README files in the GitHub repository. You can find a short demo on Mediaspace on how to set up the code:  CS410_TeamEastToWest_FinalProject - Illinois Media Space . To view a run of our code with the graphs generated at the end, visit CS410_TeamEastToWest_FinalProject . If you would like to see the code in action, please reach out to the contributors (Akshaya, Heidi, or Hari) for a demo/tutorial. Due to copyright restrictions on the data, it is not possible to run this code yourself. We use the following package and import statements: pip install pyLDAvis import re import numpy as np import numpy.linalg as la import pandas as pd from pprint import pprint import datetime # NLTK import nltk nltk.download('stopwords') from nltk.corpus import stopwords from nltk import ngrams # Gensim import gensim from gensim import models import gensim.corpora as corpora from gensim.utils import simple_preprocess from gensim.models import CoherenceModel from gensim.models import Phrases # TODO: to create bigrams with # spacy for lemmatization import spacy # Plotting tools import pyLDAvis import pyLDAvis.gensim # don't skip this import matplotlib.pyplot as plt import statsmodels from statsmodels.tsa.stattools import grangercausalitytests import warnings warnings.filterwarnings('ignore') References: Mei, Q, Ling, X, Wondra, M, Su H., and Zhai, C. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171-180, New York, NY, USA, 2007. ACM. Mupfururirwa , W. (2019, September 19). Retrieved from https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis-on-a-large-gdp-data-with Prabhakaran , S.   Time Series Analysis in Python , Retrieved from https://www.machinelearningplus.com/time-series/time-series-analysis-python Sarit, M. (2019, October 7)  Time Series Forecasting using Granger's Causality and Vector Auto-regressive Model , Retrieved from https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6 4) Brief description of the contribution of each team member in case of a multi-person team. We initially tried to code in real-time as a group across various notebook environments but could not find an efficient solution that would have allowed us to access the data at the same time (we could have potentially hosted the data on the cloud but that would have required additional charges). We ultimately decided that Akshaya would share her screen via Zoom video call while she coded and Hari and Heidi worked through understanding the steps of the paper together as well as communicated with TAs and Prof during office hours. Essentially pair programming with one person as driver and two people as navigators. During the week, Heidi and Hari would go through the paper and figure out the next steps. Akshaya would code during the week. During the meeting, we would review what Akshaya did and find any gaps in logic or deviations from the paper/catch for mistakes. After reviewing the code, Hari and Heidi would go through the next steps that Akshaya would code up later in the week. We split up the work like this because the LDA model would take a significant time to run during the meeting. If there was time remaining in our meeting, we would take some time to code as a team. If there was a report due, we would complete it as a team during the meeting. To make the best use of our time since the algorithm takes a significant amount of time to run, we decided on this strategy. When issues came up that we could not agree on a solution as a team, Hari and Heidi would take the time during the week to talk with the TAs during office hours. A good example of this situation in our group would be that Hari found a typo in the paper that caused a lot of confusion. Heidi and Hari went to office hours to clarify with the TA. And when there was no resolution, Heidi and Akshaya spoke with the professor during office hours and finally received an answer. Team East to West Reproduce results from paper:  Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 1.Which tasks have been completed? Implemented one iteration of the paper. 2.Which tasks are pending? *Need to understand how the Mu weight works *Implement feedback loop *Calculate purity *Use purity as stopping criteria 3.Are you facing any challenges? a.Calculating/implementing mu? b.Granger Causality covariance warning i.Calculating the word stream leads does not make a full rank matrix. We think this makes sense that this happens, but the paper doesn't mention what they do 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. *Akshaya Jagannadharao, akshaya2 [Captain] *Heidi Toussaint, heidist2 *Hariharan Venkitaraman, hv4 2.Which paper have you chosen? Causal topic modeling Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3.Which programming language do you plan to use? Python 4.Can you obtain the datasets used in the paper for evaluation? Sort of. New York Times Dataset (preferred dataset if we get access): *https://catalog.ldc.upenn.edu/LDC2008T19 Iowa Electronic Markets (IEM) 2000 Presidential Winner-TakesAll Market: *https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? New York Times Articles *https://spiderbites.nytimes.com/2000/ *It has all the articles written in 2000. We just need to scrape the page ourselves and aggregate the useful text (which we would have to do using the LDC corpus anyway). 6.If you answer ""no"" to Questions 4 & 5, how are you gohttps://spiderbites.nytimes.com/2000/ing to demonstrate that you have successfully reproduced the method introduced in the paper? Our results shouldn't differ from the method in the paper because we are using the same exact data. The paper didn't use any of the other meta information from the LDC corpus, only the date (which we can find in the article) and the text (which is available). Since we have both pieces of information, we should be able to reproduce the method introduced in the paper. Overview This project attempts to reproduce the paper Mining causal topics in text data: iterative topic modeling with time series feedback. We are only reproducing the first experiment where the authors examine the 2000 U.S. Presidential Election. The code we implement here can be used to find causal relationships between a textual dataset and non-textual dataset. We prove that by using the idea of causality, we can acheive better topic mining results than the baseline model (i.e. without time series feedback). Each folder in this repo contains a README file that will explain more about the contents of the directory. Repository Structure Parent Directory The top level directory contains the jupyter notebooks with our code. There is further documentation in the jupyter notebook as well. To view a demo on how to setup the notebook, visit Example Run for Causal Topic Modeling with Time Series Feedback. To view a run of our code with the graphs generated at the end, visit CS410_TeamEastToWest_FinalProject. Data Folder The Data folder provides an overview/methodology of how to access the data and structure it to run our code. Results Folder The Results folder contains images of our results as well as interactive visualizations in the form of html files. We also provide a README file in this directory that analyzes the differences between our results and the papers and why this may be so. Run the code We strongly suggest using Google Colab as it reduced the overhead of installing packages. The other option is to download the Jupyter Notebook and set it up on your local machine. Make sure the data is stored locally. To run on GoogleColab (will save a copy in your Google Drive) Open the Jupyter notebook titled ""ITMTF_GoogleColab.ibynb"" At the top of the notebook there is a link that says ""Open in Colab"" Click the link and follow the instructions to give permissions to GoogleColab The notebook should be located in My Drive. Edit the paths in the first cell to point to your data on your Google Drive Run the cells To run locally Clone this reproduce Open the Juptyer notebook titled ""ITMTF.ipynb"" Run the cells Code Examples There is documentation for the main function (ITMTF()) which implements the topic modeling. An example of how to call it is below as well as in the notebook. ITMTF Function Parameters: paramCtrl -- str (either 'k' or 'decay'), tells function to test various k or decay values params -- list of integers, each val in list is either a k or decay values to test. For k values, the last k value will be used to test a variable number of topics starting at k topics. In every iteration of ITMTF, k will increase by the number of causal topics found + some constant decay -- (0.5, 1] decay parameter to use when running lda (effective only when paramCtrl='k') k -- Interger representing number of topics to use when running lda (effective only when paramCtrl='decay') iterations -- Integer number of iterations to run ITMTF const_k_increase -- Integer number of topics to increase (in addition to number of causal topics found) each iteration when running varying number of topics (effective only when paramCtrl='k') Default is 0 verbose -- True/False Print out purity and confidence every iteration Returns: k_lda_model -- The final lda_model k_avg_purities -- Average purity of all causal topics in each iteration k_avg_conf -- Average confidence of all causal topics in each iteration Example Calls: k_lda_model, k_avg_purities, k_avg_conf = ITMTF(""k"", params=[10, 20, 30, 40, 10]) mu_lda_model, mu_avg_purities, mu_avg_conf = ITMTF(""decay"", params=[0.51, 0.6, 0.7, 0.8, 0.9, 1]) Standard Graph Visualizations These visualizations are built using matplotlib.plotly. The function show_plot() is a wrapper for the plotly function plot(). ``` k_labels = [""10"", ""20"", ""30"", ""40"", ""Varying Number of Topics""] show_plot(range(1, 6), k_avg_purities, ""Iteration"", ""Average Purity"", k_labels, xticks=range(1, 6), yaxisrange=[0, 120], title=""Average Purity for Different Number of Topics"", legend_title=""Number of Topics"", saveAs=""purity_k.png"") show_plot(range(1, 6), k_avg_conf, ""Iteration"", ""Average Confidence"", k_labels, xticks=range(1, 6), yaxisrange=[95.5, 100], title=""Average Confidence for Different Number of Topics"", legend_title=""Number of Topics"", saveAs=""confidence_k.png"") mu_labels = [""0.51"", ""0.6"", ""0.7"", ""0.8"", ""0.9"", ""1""] show_plot(range(1, 6), mu_avg_purities, ""Iteration"", ""Average Purity"", mu_labels, xticks=range(1, 6), yaxisrange=[0, 120], title=""Average Purity for Different Decay Values"", legend_title=""Decay Values"", saveAs=""purity_mu.png"") show_plot(range(1, 6), mu_avg_conf, ""Iteration"", ""Average Confidence"", mu_labels, xticks=range(1, 6), yaxisrange=[95.5, 100], title=""Average Confidence for Different Decay Values"", legend_title=""Decay Values"", saveAs=""confidence_mu.png"") ``` Interactive Visualizations These visualizations are built using pyLDAvis. The below example also includes a line showing how to save the visualization. ``` pyLDAvis.enable_notebook() vis = pyLDAvis.gensim.prepare(k_lda_model, corpus, id2word) pyLDAvis.save_html(vis, 'LDAvis_k.html') pyLDAvis.enable_notebook() vis = pyLDAvis.gensim.prepare(mu_lda_model, corpus, id2word) pyLDAvis.save_html(vis, 'LDAvis_mu.html') ```"
https://github.com/JamesFCoffey/CourseProject	"Progress Report CS 410 Text Information Systems - Fall 2020 Team: The Electric Moccasins James Coffey, NetID: jamesfc2 - Captain and Praveen Bhushan, NetID: bhushan6 Goal The function of the tool being implemented is to ""combine probabilistic topic modeling with time series causal analysis to uncover topics that are both coherent semantically and correlated with time series data."" (Kim et al., DOI=10.1145/2505515.2505612) This tool is being implemented in Python instead of the R implementation done in the paper to make it easier for software deployment. The techniques/algorithms being used are PLSA topic model and Granger testing. Progress made thus far * Implementation in Python Jupyter Notebook o Acquired needed datasets: New York Times Annotated Corpus (NYTAC), IEM 2000 U.S. Presidential Election ticker, and stock tickers for AAPL and AAMRQ o Wrote script for determining significant Granger causality at different lag values o Wrote function for calculating impact value using Granger causality o Wrote function for calculating topic purity o Wrote script to trim and organize NYTAC to data subset needed o Imported PLSA class from MP 3 and modified the build corpus to use NLTK and multiprocessing. o Wrote function for topic level causality. Remaining tasks * Compare results of Python implementation with reference paper. * Write command line interface for tool. * Write documentation and record tutorial presentation. Challenges/issues being faced * Even with multiprocessing, building corpus takes a while on 4 core Intel i7 CPU * EM algorithm takes a while. Could this be sped up by using CuPy instead of NumPy so that it uses the GPU? * Not sure if it is one iteration of EM for each update of the topic prior or if it is multiple EM iterations for one topic prior update. * Not sure how to incorporate the topic prior into the PLSA algorithm. Project Proposal CS 410 Text Information Systems - Fall 2020 Team: The Electric Moccasins James Coffey, NetID: jamesfc2 - Captain and Praveen Bhushan, NetID: bhushan6 We have chosen to reproduce a listed paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The function of the tool that we will implement from this paper is to ""combine probabilistic topic modeling with time series causal analysis to uncover topics that are both coherent semantically and correlated with time series data."" (Kim et al.) This tool can be used to find positive and negative correlation of topics in textual data with time series data. This could be useful for example in predicting stock prices by a real user such as an investment bank. Our tool will be different in that we plan to use Python for implementation instead of the R implementation done in the paper. This will be impactful as Python is a general-purpose programming language and would make it easier to deploy in more programs. Lastly, the techniques/algorithms we will use are those in the paper: PLSA topic model, Pearson correlation coefficients, and Granger testing. We can obtain the datasets used in the paper. We can get the New York Times Annotated Corpus from https://catalog.ldc.upenn.edu/LDC2008T19, IEM 2000 U.S. Presidential Election: Winner-Takes-All Market from https://iemweb.biz.uiowa.edu/closed/pres00_WTA.html, and historical stock prices for AAPL and AAMRQ from https://finance.yahoo.com/ and https://thestockmarketwatch.com/. Using this data, we can demonstrate the usefulness of our tool be replicating the results published in the original paper on the same datasets. Our rough timeline for the proposed project is as follows: * Nov. 1st - Implement paper in Python. * Nov. 8th - Verify performance of against paper and compare results. * Nov. 15th - Write interface for using tool on new data to generate results * Nov. 22nd - Troubleshoot and bug fix. * Nov. 29th - Submit progress report. * Dec. 6th - Finish documentation and record presentation. * Dec. 9th - Submit code with documentation and tutorial presentation. CS 410: Text Information Systems Course Project Documentation Tutorial presentation video Microsoft Hosting https://web.microsoftstream.com/video/625f5bff-8451-45b6-881b-207b9590da96 YouTube Hosting https://youtu.be/DQfXFzNvuAc Overview of the function of the code This code implements the following paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The code takes times series data and a corpus of text documents both with time stamps from the same period as input and outputs a set of causal topics with the lag that correlates them to the time series data. It does this through the iterative algorithm given below: 1. Apply probabilistic latent semantic analysis (PLSA) to the document corpus to generate a preselected number (input parameter) of topics topics. 2. Use the Granger causality test correlate topics with the time series data and output correlations with a significance (one minus p-value) above a cutoff value (0.95 default) to a list of (topic, lag) tuples. 3. For each topic in the list of (topic, lag) tuples, use the Granger causality test to find causal words with above signifcance above the cutoff among top words. By default the top words are the those with the highest probability words whose summed probability does not surpass 50%. 4. Separate positive and negative impact words of each topic into individual topics and ignore the lesser topic if it is below a ratio of the major topic (by default, less than 0.1). 5. With the newly separated topics from step 4, define prior word probabilities according the significances found in step 3. 6. Apply PLSA to the document corpus using the prior word probabilities incorporated into the M-step of the EM algoritm. 7. Repeat 2-6 until a spectified number of iterations is performed. This code can be used to text mining as to find causal topics correlated with external time series data. For example, it can be used to find topics correlated with the movement of stock prices. It can also be used to find topics correlated with the outcome of an election. Implementation The code is implemented in an Python class called ITMTF in a module called causal_topic_mining.py. A portion of the code was adapted from CS 410 MP3. The class is composed of the following: Class Variables self.documents: The text document corpus. self.doc_timestamps: The timestamps of the documents in the corpus. self.vocabulary: The vocabulary. self.likelihoods: The log likelihoods. self.documents_path: The path to the text document corpus. self.term_doc_matrix: The term document matrix. self.document_topic_prob: P(z | d) self.topic_word_prob: P(w | z) self.topic_prob: P(z | d, w) self.number_of_documents: The number of documents. self.vocabulary_size: The size of the vocabulary. self.lag (default, 5): The maximum lag to which evaluate Granger causality. self.sig_cutoff (default, 0.95): The significance cutoff. self.probM (default, 0.5): The maximum sum of the probabilities of the top words for a topic. self.min_impact_ratio (default, 0.1): The ratio below which to discard minority impact words in a topic. self.topic_prior: The prior word probabilities. self.mu: The strength of the prior in each iteration. self.ct: The list of (topic, lag) tuples for causal topics discovered by the algorithm. self.average_entropy: The average entropy of the topics for each iteration of the algorithm. self.average_topic_purity: The average topic purity of the topics for each iteration of the algorithm. self.average_causality_confidence: The average causality confidence of the topics for each iteration of the algorithm. self.time_series: The time series data saved on intialization of the class. Class Functions normalize(input_matrix): Normalizes rows two-dimensional matrix to sum to one. token_pipeline(line): Tokenizes line of input text. build_corpus(): Reads documents from the corpus, tokenizes them, and stores them. It also stores the timestamps of the documents and records the number of documents. build_vocabulary(): Constructs the vocabulary and records the size of the vocabulary. build_term_doc_matrix():Constructs the term document matrix. initialize_randomly(number_of_topics): Randomly initializes the probability distributions for P(z | d) and P(w | z). It also intializes the topic prior. initialize_uniformly(number_of_topics): Uniformly initializes the probability distributions for P(z | d) and P(w | z). It also intializes the topic prior. initialize(number_of_topics, random=False): Calls an intialization function. expectation_step(): The E-step of the EM algorithm for PLSA where it updates P(z | w, d). maximization_step(number_of_topics): The of the EM algorithm for PLSA where it M-step updates P(w | z) and P(z | d). process(number_of_topics, max_plsa_iter, epsilon, mu, itmtf_iter): The master control loop for the iterative topic modeling with time series feedback algorithm. impact_value(data, lag): Calculates the impact value. build_TS(): Builds a topic stream. topic_level_causality(): Finds the causal topics from the topic streams and time series data. top_words(topic): Finds the top words in a topic. build_WS(tw): Builds a word stream. word_level_causality(): Finds the significant words and their impact value for each topic using the word streams and time series data. generate_topic_prior(wc): Generates the topic prior. create_eval_df(): Creates a dataframe from which to evaluate metrics. metrics(): Evaluates the average entropy, average topic purity, and average causality confidence of the topics. Usage of the software Installation Download the module causal_topic_mining.py to your project folder. The module uses Python 3.7 and requires the following libraries: - numpy - pandas - statsmodels - nltk - normalise - tqdm demonstration_notebook.ipynb requires the additional library: - matplotlib Demonstration A demonstration of the use of the class ITMTF is given in the Jupyter notebook demonstration_notebook.ipynb. Import the module. from causal_topic_mining import ITMTF Create an object from ITMTF class itmtf = ITMTF(documents_path, time_series) The path is to the text corpus of text files named YYYY-MM-DD.txt (YYYY = 4 digit year, MM = 2 digit month, DD = 2 digit day) and each line in the text file is a separate document. The time series is a Pandas series where the indicies are the dates in pd.dateime format and the time series is a stationary series. Tokenize the text corpus and record document timestamps itmtf.build_corpus() Build vocabulary itmtf.build_vocabulary() Run ITMTF algorithm itmtf.process(number_of_topics = 30, max_plsa_iter = 1, epsilon = 0.001, mu = 1000, itmtf_iter = 5) Access the results by calling the class object parameters desired itmtf.average_topic_purity for example. Team Contributions James Coffey (Captain) Wrote Proposal.pdf Wrote Progress Report.pdf Wrote causal_topic_mining.py Wrote demonstration_notebook.ipynb Wrote README.md documentation Gave tutorial presentation Praveen Bhushan Gave tutorial presentation"
https://github.com/JiajunWuEdu/CourseProject	Proposal of the text classification competition What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. My group has only one member. Jiajun Wu, jiajunw6. So I will be the captain myself. Which competition do you plan to join? I would like to join a text classification competition. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Though I am not very familiar with most classifiers, I decide to learn some of these and I hope they will be useful in the competition. While my interest is classification and prediction of utterances in classroom contexts, I heard of some neural network classifiers and deep learning frameworks which were employed in this area. For instance, Zhao (2016) developed a sequence predictor using three neural network classifiers (i.e. MLP, CNN, and LSTM). Donnelly et al. (2016) used the Naive Bayes classifier using the WEKA machine learning toolbox identified five key instructional segments (Question & Answer, Procedures and Directions, Supervised Seatwork, Small Group Work, and Lecture) in the classroom audio data with F1 scores ranging from 0.64 to 0.78. Suresh et al. (2019) trained a bi-LSTM network to classify the transcript of teacher-student dialogues sentence by sentence in to 6 talk moves (e.g. Restating, revoicing, pressing for reasoning, getting students to relate to another's ideas, etc.) with an F1 measure of 65%. Reference Donnelly, P.J., Blanchard, N., Samei, B., Olney, A.M., Sun, X., Ward, B., Kelly, S., Nystran, M. and D'Mello, S.K., 2016, July. Automatic teacher modeling from live classroom audio. In Proceedings of the 2016 conference on user modeling adaptation and personalization (pp. 45-53). Suresh, A., Sumner, T., Jacobs, J., Foland, B. and Ward, W., 2019, July. Automating Analysis and Feedback to Improve Mathematics Teachers' Classroom Discourse. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 9721-9728). Zhao, Y., Chu, S., Zhou, Y. and Tu, K., 2017, January. Sequence Prediction Using Neural Network Classiers. In International conference on grammatical inference (pp. 164-169). Which programming language do you plan to use? I plan to use Python, and C++ if it is necessary. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Jon-LaFlamme/CourseProject	"CS410 Project Presentation: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAMTeam Members: Jon LaFlamme (captain) jml11@illinois.edu Pradeep Sakhamoori ps44@illinois.edu Rahul Sharma rahul9@illinois.edu Project Overview: This application is a vertical search engine for food recipes. Special features include auto-complete and auto-suggestion in the search bar achieved with character and word n-grams. Search results are rendered on the same page as the search bar. Our system is configured with and run on Elasticsearch. Included with the project but not integrated with the search application is a content-based recommender system. Project Review Roadmap Special Consideration: Our team originally consisted of four members through the formation, proposal, development and progress report phases of the project, but one of our team members dropped this course prior to providing assigned deliverables. Per the TA, we were instructed to notify our reviewers to take this development into consideration in the evaluation of our final submission, as it has necessitated some adjustments to the scope of this project. Application/Software Functionality: This application is a food recipe search engine with a dataset of over 65,000 food recipes. Word and character-based autosuggest and autocomplete are supported dynamically in the search bar. Search results are dynamically displayed on the same page as the search bar, with matched query terms RecommendedSupplementaryProject overview video: https://youtu.be/8MrQOFWIooo Installation instructions: README.md Video install guides in README.md Recommender system video demo: https://youtu.be/pEuqzOdScEQ Recommender system writeup: Recommender_System.pdf Search engine and project architecture: https://youtu.be/8rgRQOhtluQhighlighted in the title field of each recipe that is rendered in the search results. Additionally, a content-based recommender system was developed and validated on the authorOs local machine. It is not integrated with our search application and is thus considered an OextraO in terms of our project submission. To review the recommender module, simply watch the demo video and read the pdf. Application/Software Implementation: Software architecture: batch -> data ingestion modules; recommender module dataset -> food recipe datasets in JSON format es-setup -> Elasticsearch configuration files search-engine-webapp -> app -> application driver and API, browser files and templates search_engine_egg.info -> Application setup files user-profile -> our beta-recommender with user-profile as JSON AUTHORS.txt -> source code attributions README.md -> Installation instructions Dockerfile.amd -> Docker build script pdfs (proposal, progress report, final documentation, recommender system) Description of Team Member Contributions: All members: Weekly team meetings, topic exploration, identification of datasets, learning Elasticsearch and Kibana, shared contributions to proposal, progress report and final submission documents. Jon: Administrative tasks of coordinating, authoring or editing and submitting project deliverables. Assisted with troubleshooting and documenting Docker installation requirements on Mac. Spent several hours familiarizing myself Elasticsearch Assisted with automation of Elasticsearch data ingestion with data_loader.py. Assisted with user interface development (parsing JSON search results, and displaying search results as hyperlink recipe titles on same page as search bar). Assisted with bug fixes recommender API. Generated ~1500 test queries by collecting most popular OrecipeO queries using Google Trends (not implemented in final submission). Developed algorithm for sorting recipes search results based on complexity/difficulty (not implemented in final submission). Developed content-based recommender system (included in project but not integrated with our final application.) Pradeep: Started with spending time brainstorming project architecture, design discussions, planning, task allocation, internal milestones and deadlines (We used Microsoft teams to collaborate, drive weekly tasks and discussions). Initially started with contributions to look for available open source food recipe datasets, found couple of them which are publicly available (from Kaggle) and analyzing its format and its usability. We used Elasticsearch (ES) engine, python and flask interface for programming D Initial challenge is to get the environment ready to unblock others with a base line application (by pulling initially committed Skelton changes from team). Then spent time containerizing the complete application. Spent time understanding on launching elastic server as backend process from Docker with Ubuntu 16.04 environment D figuring out missing components in terms of launching web app and tunneling ES web server to localhost machine web-server D Debugging, fixing bug and released initial docker containerized app which was quite challenging. Developed/Coded basic/initial pipeline for recommender system (contextual based) by pre-populating UserProfile (static) with specific food interest and which will be used for querying from ES(which is current a standalone module D which user can trigger from docker bash). Spent time supporting team with guidance on fixing docker issues on Mac. Rest of the time spent in testing application on Ubuntu. Rahul: Assisted with identification of dataset, set up project collaboration space on Trello. Built project architecture template for search application with Flask. Designed and authored Elasticsearch index configuration and ingestion scripts. Authored setup instructions and scripts for application build without Docker. Designed and authored user interface for search application. Designed and authored autosuggest and autocorrect features with Elasticsearch and Flask. Methodology and analysis: Please see included supplementary videos and documents for a more in-depth coverage of design choices, implementation details and results analysis. CS410 Project Progress Report: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAM Team Members: Jonathan LaFlamme(Captain)jml11@illinois.edu Pradeep Sakhamoorips44@illinois.edu Rahul Sharmarahul9@illinois.edu Rohan Khatukhatu2@illinois.edu Project Summary: Our original project proposal described a vertical search engine, specializing in Indian Cuisine that supports both queries and a recommender system. Our project scope has since narrowed due to time constraints. Our current project expectation is a working search engine that supports full text search with auto-suggestion as well some advanced filtering/query options. The application is run on Elasticsearch and containerized on Docker to run locally with minimal installation steps. Progress made thus far: *Identified our food recipe dataset *Developed search engine application using Elasticsearch and Kibana (for index file generation) *Dockerizing (container) to launch Elasticsearch server in the background and tunneling food recipe application to host machine on 127.0.0.1:5000 port using Flask *Validating and debugging container on Ubuntu and MacOS *Additional configurations and installations required for Mac users to get the tunneling of webserver onto localhost *Python notebook to injest data *Preprocessing and inverted index configuration tested on Kibana *Limited autosuggest and autocomplete features are deployed *Github project repo with build and deploy instructions Remaining Tasks: *Formatting search results page (html) *Organizing recipe links with single click URL *Displaying results on the same page as the query *Improving and enhancing the autocomplete and autosuggest features *Python script automating the preprocessing and inverted index generation *Optional inclusion of images with search engine, if possible Challenges and Issues: *Bug fixing in terms of adding missing python packages for deployment of automation pipeline with container *Running elasticsearch in background with Flask *Identifying search and autosuggest algorithm (eg. n-gram, character n-gram, stemming, lemmitization) *Team members learning and deploying Elasticsearch and Kibana toolkits. *Enabling Docker on Ubuntu Linux, specifically identifying the correct docker run options for routing Docker network traffic to localhost with '--network host' option. *Enabling Docker on Mac OS, specifically since docker runs as a virtual machine on Mac, additional installs and procedures are required for docker container to have access to mac host as a localhost *Integrating python Flask app with Elasticsearch and front-end *Identifying correct method for building Elasticsearch index CS410 Project Proposal: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAM 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team member s Team Members: Jonathan LaFlamme (Captain)jml11@illinois.edu Pradeep Sakhamoori ps44@illinois.edu Rahul Sharmarahul9@illinois.edu Rohan Khatukhatu2@illinois.edu Project Overview ( What is the function of the tool?) : 2.What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? What is your free topic? Please give a detailed description. What is the task? Our project proposal is a vertical search engine that specializes in retrieving Indian Cuisine food recipes in a collection aggregated from multiple sources. Our goal is to support both push and pull retrieval models. We think that the specialization of this search tool is what will allow us to return more relevant documents to our users compared with existing food recipe search platforms that are broader in scope. For instance, we can build a user profile (essentially a customized background language model) with a brief initialization phase (ask the user a series of questions when they set up their profile) that should improve accuracy with each subsequent query (since we will gain a better understanding of our user's palette in the form of language modeling with user feedback/relevance judgments). We may also include sentiment analysis of recipe reviews as part of our document ranking algorithm at a later stage, but we are not yet committed to that. It will depend on available time. This project proposal is being submitted under the ""Free Topics"" category. Why is it important or interesting? This project holds tremendous academic value for our group in the context of this course as it requires both a high level understanding information retrieval as well as many opportunities to wrestle with challenging design questions specific to our context and application, such as language modeling, feedback mechanisms, ranking functions, classification, recommender systems and evaluation techniques. In a broader sense, we think that our application should outperform more generic food recipe search engines because we will be able to tune our search results based on a narrower context. But more specifically, we think that in the context of food recipes this will be especially advantageous due to the lower amount of noise present in our user's background language model. For instance, a user who routinely searches for American, Indian and Italian cuisine would add a lot of misdirection to their background language model compared with a user who only ever searched for Indian cuisine. Our system would begin to develop a more sophisticated picture of a user's Indian cuisine preferences and consequently retrieve more relevant recipes. Target User  ( Who will benefit from such a tool?) :  We think this application would appeal to at-home cooks and chefs who are interested in Indian Cuisine. Originality of Tool   ( Does this kind of tool already exist? If similar tools exist, how is your tool different from them?  Would people care about the difference? ) : We could not find a major food recipe search engine that specifically focuses only on Indian Cuisine. However, many of the most popular food recipe websites do provide search filters for querying or browsing Indian Cuisine. Yet, as mentioned in an earlier section, we think that our limited scope of topic coverage will allow us to tune our retrieval models to a greater degree of accuracy than would be possible on a platform that supports all types of cuisine. Another advantage of our tool is that it aggregates recipes in this one category across multiple sources. This adds a level of convenience and efficiency for our users who can query or browse a much larger collection of recipes from a single source that would otherwise be a tedious process working across multiple websites with different types of interfaces and retrieval models. Resource Utilization  ( What existing resources can you use? What techniques/algorithms will you use to develop the tool?) : We think that we can use the META toolkit for many of the preprocessing tasks involved, including web scraping, tokenizing, POS tagging, indexing and ranking. The BM25F ranking algorithm is an obvious candidate for our ranking algorithm because recipes naturally translate well into structured documents (eg. list of ingredients; title; description; cook time; etc.). As alluded to earlier, our document collection will be aggregated from multiple sources. One obvious early source will be a Kaggle dataset of 250,000 food recipes, but we hope to find additional recipes through other sources or from scraping food recipe websites. We hope to dynamically update our background language model in some fashion based on user feedback, but the exact formula/algorithm we will employ is still a matter under consideration. For our push recommendation system, we hope to test and compare both content and collaborative models of filtering. We will implement the better performing model for our final submission or consider implementing a combination of the two models. Validation  ( How will you demonstrate the usefulness of your tool?) For the validation phase, we will solicit 10 volunteers to generate and run their own set of 10 queries and make explicit relevance judgments about the top 10 ranked results for each query. Five of the queries will be fed to competing search engines with top ten results returned and judged by our users. Ideally, this will be done at random so as to obfuscate to the user which search engine they are using at any given time. We will be using statistical methods to evaluate our search engine and compare with competing engines (using precision and recall as the primary metrics). Once the queries and judgments are complete, we will feed our participant's user data into our recommendation system and send an email to each of the participants with five recipe recommendations. This email will ask for relevance judgments from each of the five recipes. We will also send another set of emails based on a collaborative model once all participants have completed their queries and associated relevance judgments. 3.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Project Timeline  ( A very rough timeline to show when you expect to finish what.): Complete Proposal (By Oct 25) Architecture document with API level information (Python) and Test Scenarios  (By Nov 1, 12 hours expected) Architecture block diagram showing end2end pipeline (Interface to search results representation) High level Input and Output data for each function block Sample TestCase (""Queries"") and TestResults (""Expected"") Document collection formation: tokenizing, POS tagging, indexing, etc. (By Nov 1) ( 20 total hours expected to aggregate our document collection for Indian Cuisine recipes and associated review/comments ) Associated preprocessing tasks  ( 2 hours expected to tokenize, POS tag, build out the document collection, and build an inverted index ) Recipe Classification (By Nov 20) C lassify recipes by cuisine style, meal type, dish type, etc.  (20 hours) This process will help provide filters for advanced search functionality, and could also be integrated into our background language models. Develop test queries and test/select optimal scoring function (By Nov 8) Develop test queries  ( 4 hours expected ) Test/select/optimize scoring function  ( 3 hours expected ) Develop user profile and feedback model (By Nov 15) Research, build and test/select optimal user feedback model for recommendation system  ( 15 hours expected ) Working user interface (By Nov 22) Locally hosted Web Interface/Python GUI with search button and results window  ( 20 hours expected ) Trial Phase (Nov 22-Nov 29) Build automated system to pull query results from competing search engines  (5 hours expected) Solicit volunteers for our application   (4 hours expected to solicit volunteers, explain the tool and the participation requirements ) Evaluate and document/model query relevance judgments  (1 hour expected) Evaluate push recommendation relevance judgments and adjust recommendation model  (2 hours expected) Progress report completed (By Nov 29) Final software code with documentation and software presentation (By Dec 9) Debug/test source code  (2 hours expected) Construct Readme  (2 hours expected) Create presentation  (4 hours expected) Total Hours expected: 116 hours Total Hours required: 80 hours 4.Which programming language do you plan to use ? Primarily Python Additional Details and Expected Outcome : We expect that our tool will outperform competing search engines within our specified topic/domain of Indian food recipes. FOOD RECIPE SEARCH ENGINE USING ELASTICSEARCH, PYTHON AND FLASK Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Installation guide for Ubuntu and Mac Users as given below (make sure you are not behind any proxy/firewall to test this project) $git clone https://github.com/Jon-LaFlamme/CourseProject.git Video Tutorial (Ubuntu): Build Docker Image, Generate Index file and running food recipe search engine web app https://www.youtube.com/watch?v=FooQHpTv63I Docker installation Follow instructions from https://docs.docker.com/get-docker/ select download based on your host machine OS Check if docker daemon is running on host machine (Ex: Output from Ubuntu 16.04 host machine): $docker # docker Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files (default ""/home/psakamoori/.docker"") -c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with ""docker context use"") -D, --debug Enable debug mode -H, --host list Daemon socket(s) to connect to -l, --log-level string Set the logging level (""debug""|""info""|""warn""|""error""|""fatal"") (default ""info"") --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default ""/home/psakamoori/.docker/ca.pem"") --tlscert string Path to TLS certificate file (default ""/home/psakamoori/.docker/cert.pem"") --tlskey string Path to TLS key file (default ""/home/psakamoori/.docker/key.pem"") --tlsverify Use TLS and verify the remote -v, --version Print version information and quit (Linux) Instruction to Build docker image with ElasticSearch and application After successful installation of Docker, next step is to build the docker container image with ElasticSearch and application Command to build the docker image (makesure you are in same path as Dockerfile.amd64 file) $ docker build -f Dockerfile.amd64 -t food_recipe_se . [you can choose your own name instead of ""food_recipe_se""] It will take some time to build the image. Once done you can check your image on host machine with below command $ docker images Should see something like below REPOSITORY TAG IMAGE ID CREATED SIZE food_recipe_se latest 72ab77c2a9b3 35 minutes ago 2.13GB Troubleshooting ""docker build"" failure: - Change Folder & File access permissions of project recipes-search-engine$ chmod -R 777 * - Check if docker daemon is running https://docs.docker.com/config/daemon/ ### (Linux) Run the docker container to trigger search engine $docker run --net=host -p 5000:5000 food_recipe_se Expected output: Starting Elastic Server * Serving Flask app ""app"" (lazy loading) * Environment: development * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 271-999-248 **IMP NOTE**: Before launching search engine web app and start searching for food recipes user needs to follow below steps to generate index file (Linux) Launch ingest script: Open a new terminal window $docker ps -a (need sudo acccess) Identify and copy <container ID> for food_recipe_se, then run $ docker run -it --net=host -p 5000:5000 <container image name ex: food_recipe_se used to build the docker image> bash Output docker shell will look like below WARNING: Published ports are discarded when using host network mode UIUC@node01:/app$ In the container shell run: UIUC@node01:/app$ python batch/es_data_loader.py NOTE: If you receive connection error, retry again in 30-60 seconds Expected output (may take several minutes to complete ingestion): ****Load begins**** **Loading now: /app/dataset/recipes_raw_nosource_ar.json * 1000 documents successfully indexed in recipes_id index. ... ... * 65000 documents successfully indexed in recipes_idx index. (Linux) On your host machine open: http://0.0.0.0:5000 Paste url into browser to begin testing the application (Mac) ideo Tutorial Build Docker Image, Run Docker Image, Generate Index file https://youtu.be/pEuqzOdScEQ (Mac) Docker Desktop for Mac download instructions and configurations Install the latest edge release from Docker (v2.5.1.0 or later): https://docs.docker.com/docker-for-mac/edge-release-notes/ Install jq. Options: $brew install jq $port install jq Or: https://stedolan.github.io/jq/download/ Setup jq to work with Docker SOCKS server: $cd ~/Library/Group\ Containers/group.com.docker/ $mv settings.json settings.json.backup $cat settings.json.backup | jq '.[""socksProxyPort""]=8888' > settings.json Enable SOCKS proxy: System Preferences -> Network -> Advanced -> Proxies Select the box next to 'SOCKS proxy' Ensure SOCKS Proxy server reads 'local host:8888' Ensure Bypass settings reads '*.local, 169.254.0.0/16, *.io' If Unselected, Select the box next to 'Use Passive FTP Mode' Select ""OK"" then ""APPLY"" NOTE: Internet access may be disabled while you running your proxy server. To restore your settings after testing the application: * Deselect the box next to 'SOCKS Proxy' in your Network settings * Select ""OK"" and ""Apply"" Start Docker Desktop for Mac application: 'commmand' + 'space' and type 'Docker.app' then launch The blue whale Docker icon should appear in the apple menu bar * Select the icon and ensure green status: 'Docker Desktop is Running' Verify Expiramental features in Docker Desktop for Mac: Docker.app -> Preferences -> Expiramental Features * Ensure all features are enabled (Mac) Build the Docker image Navigate to project directory 'recipes-search-engine' in terminal. Enter command to build the Docker image: $ docker build -f Dockerfile.amd64 -t food_recipe_se . Expected output (may take a few minutes to complete): Building ... ... ... naming to docker.io/library/food_recipe_se (Mac) Run the Docker image Run the Docker image: $docker run -p 5000:5000 food_recipe_se Expected output: Starting Elastic Server * Serving Flask app ""app"" (lazy loading) * Environment: development * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 544-260-012 (Mac) Launch ingest script: Docker.app -> Dashboard -> Containers/Apps Next to the running food_recipe_se image, select ""CLI"" icon. This will open a terminal window fromn inside the Docker image. Recommend closing unused applications to speed up ingestion In the container shell: $python /app/batch/es_data_loader.py NOTE: If you receive connection error, retry again in 30-60 seconds Expected output (may take several minutes to complete ingestion): ****Load begins**** **Loading now: /app/dataset/recipes_raw_nosource_ar.json * 1000 documents successfully indexed in recipes_id index. ... * 65000 documents indexed. 65125 documents successfully indexed in recipes_idx index. (Mac) On your host machine open: http://0.0.0.0:5000/ Paste url into browser to begin testing the application (Linux/Mac) Exit condition: Kill the (app) docker container How to kill the container: - Open new command terminal - $docker ps -a (need sudo acccess) - Look for docker container with name ""food_recipe_se"" copy CONTAINER ID) - $docker stop <CONTAINER ID> - $docker rm <CONTAINER ID> Recommender System Food Recipe Search Engine This is a brief overview of the recommender system we developed for our recipe search engine. It was not integrated into the app due to time constraints and some of the difficulties of working in the containerized environment. Overview We implemented a content-based recommender system for our food recipes. Essentially this is a cosine similarity scheme in which unseen recipes that are closest to the recipes in the userOs proThle would be recommended up to the user. Preprocessing steps and design choices For each recipe, the title and ingredient Thelds were concatenated into a single string, then preprocessed to lowercase and remove non-alphabetical characters. The rationale here is that title and ingredients would provide the best sense of the similarity between recipes and culinary preferences more generally. Since the ingredients Theld contains a lot of noise and would also have been much more expensive in terms of compute and space overhead, this was a relatively obvious design choice for us. Following concatenation of the title and ingredients Theld, we tokenized each word, and performed stop-word removal. The stop-words were adapted from NLTKOs stop word list, which covers very frequent English words. We added 32 words that we found commonly in our dataset that were obviously non-informative for distinguishing food recipes. These include words like, OtablespoonO, OcupsO,OadvertisementO, etc. The next step was forming a corpus vocabulary from the preprocessed recipe documents, where every unique word in the vocabulary was stored in an index with the associated number of documents in which each word occurs. Concurrent with this step, each recipe document was converted from a list of words to an index of unique words with an associated word count for each unique word in the document. The average document length of the corpus was also computed during the vocabulary build process. The Thnal step was performing a sub-linear transformation on the document vectors, converting the unique word counts in each document into a score for each term that captured the TF-IDF weighting for each term in each document. The BM25 formula was a natural Tht for this transformation. To save on compute resources, this collection preprocessing step was done once with the results written to Thle in JSON format for easy retrieval during the recommendation step. Recommendation step For the recommendation step, a collection of recipe IDs are provided as the user proThle input as well as the number of recommendations that the user would like returned. Then, the corpus is split into OseenO and OunseenO documents, where the OseenO documents are the list of recipes in the user proThle. For each seen document, the dot product of the weighted recipe term vectors is computed for that document with each document in the unseen documents collection. The documents that return the overall k-highest scores are stored and returned as the algorithms output. Test ProThle 'California Club Turkey Sandwich' ""Slammin' Salmon"" 'Molasses Cookies' 'Asian Chicken Salad' 'Holiday Chicken Salad' 'Jamaican Turkey Sandwich' 'Perfect Pumpkin Pie' 'Best Guacamole' 'American Lasagna' 'Cooky CookiesO Test Recommendations 'Easiest, Amazing Guacamole' 'Party Guacamole' 'Simple Pumpkin Pie' 'California Guacamole Hummus' 'Baked Pumpkin Custard O Analysis These results are somewhat to be expected, since guacamole and avocado and pumpkin are relatively uncommon ingredients and would score quite highly, particularly in if they were short recipes. We suspect that several performance improvements could be achieved with some BM25 parameter turning, expansion of the stop-words list and perhaps even introducing smoothing into the scoring algorithm. Overall, though, we were pleased to complete a working recommender that returned reasonable results. Further use and testing As we did not design this recommender system to be integrated with our application, this was included as an OextraO. if you wish to explore this module on your own, you will need to make some minor changes to the code. They are as follows: 1) You need to make copies of the datasets with the O.O Removed from each recipe ID Theld. This is because the raw dataset we used is fed into Elasticsearch with the periods removed dynamically during ingest without updating our datasets. Batch -> Oes_data_loader.pyO should give you a nice template for how to complete this step. 2) Update Thlepaths: batch -> recommend_preprocessor.py ln 27 ln 125 batch -> term_vector_rec.py ln 87 ln 115 ln 116 3) Run recommend_preprocessor.py 4) Use test recipes for identical output 5) For a customized output, Thnd the recipe IDs associated with your search results in the output of your Docker shell window. Just copy and paste the recipe IDs that you want to test into the term_vector_rec.py (lines 69-78) 6) Run term_vector_rec.py 7) Steps 1-3 are one-time activities, repeat steps 5 and 6 to run more tests with different IDs Installation guide Go to search-engine-webapp directory Onetime setup-- Begin setup flaskr virtual environment & install it. python3 -m venv venv pip install -e . Download Elasticsearch/Kibana servers https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html https://www.elastic.co/guide/en/kibana/current/install.html Paste elasticsearch.yml from es-setup directory to <ELASTICSEARCH-HOME>/conf Start Elasticsearch and Kibana servers. Fix if there're any errors. Next step is launch the kibana UI and create index. This index will be used to store all documents and provide search features Kibana UI- localhost:5601 Go to management-> Dev Tools Next copy the create index command from /es-setup/es-notes.txt and run from the dev tools Onetime setup-- End Activate virtual environment . venv/bin/activate Launch webapp export FLASK_APP=app export FLASK_ENV=development flask run Fix if there are any missing libraries python -m pip install json2html TODO: set application port from config file Move to docker so same command works for everyone. Automate entire installation process"
https://github.com/Junting98/CourseProject	1. Done data preprocessing and have implemented CNN and bidirectional LSTM. Cannot beat the baseline but have come pretty close. The general pipeline of the classification model is completed. Might need some fine-tuning. 2. Beat the benchmark result, submit the code with documentation, and prepare for the presentation. 3. Mainly have concerns with data processing. Not sure what kind of words to filter and what preprocessing will have the best result. 1 Captain: Junting Wang(junting3) Member: Tianwei Zhang(tianwei4) 2 Text classification competition 3 We are prepared to learn state-of-the-art neural network classifiers. I know pytorch and tensorflow. I have completed several research projects using pytorch and tensorflow. 4 I plan to use python Documentation In the following, we will describe how the text classification model is build and its major components. Project Overview In this project, our team is doing the text classification competition. The specific task is sarcasm detection, given a specific Twitter response, we are trying to tell if the response is sarcasm. Each data point in the given dataset consist of the response tweet and also the context of the response. The goal of this project is to develop a robust model that can perform well in telling whether the given Twitter response is sarcasm or not. DEMO Link https://mediaspace.illinois.edu/media/1_pa3h5d8y Classification Model We have experimented with many different models. In the end, we reached the competition benchmark score by fine-tuning a pre-trained BERT, distilled BERT in specific. Data Preprocessing For data cleaning, we remove punctuations and any other special characters in the Twitter response. Also, we expand abbreviations such as can't and won't into can not and will not respectively. We also remove the heading of each response. After cleaning the data, we use a pre-trained DistilBertTokenizer to tokenize the cleaned response data. Then, the tokenized responses are used as the input to our model. Model Architecture The general idea of our model is to fine-tune the pre-trained distilled BERT for text classification. We achieved this by adding two extra fully connected linear layers to the BERT output and fixing the parameters for the BERT model. The general pipeline of the model is that given tokenized responses as input, we first put the inputs to the distilled BERT base model to get high-dimensional representations of the responses. Then the response representations are input to the two linear layers to get the final prediction of whether the response is sarcasm. Between the linear layers, we used ReLU as activation. We also applied dropout to both the output of the base model and the output of the linear layers. Model Training Given raw Twitter response, we first preprocessing it following the data preprocessing steps to get tokenized responses. Then, the tokenized responses are used as the input to the BERT base model, which will give high-dimensional representations of the responses. Then, those representations are put into several linear layers to generate the final prediction. The loss function we use is a NLL loss. After getting the training prediction, together with the ground truth labels that indicate whether the response is sarcasm or not, we put them to the NLLLoss and performs back propagation on the computed loss. Evaluation During training, we split the train dataset into two subset, one for actual training, one for validation. The percentage of the validation set is 20% of the data point in the original training dataset. In each epoch, we evaluate the F1 score of the model on the validation set save the model having the best F1 score. For the actual prediction task on the test set, we use the saved model during training for the actual prediction. Previous Attempts We have come a long way to the model we have right now. We first thought of models based on CNN and RNN. But after actually implemented them, those models did not give results good enough to beat the competition benchmark. Apart from distilled BERT, we have also experimented with the full BERT, which gives decent result, but it tends to overfit and takes a lot more time to run. We have also tried to vary the number of linear layers and the dimension of those layers used to fine-tune the model, we have tried to add 3 or 4 linear layers and many other different combinations of dimension, but we finalized to 2 linear layers, which are of size(768, 256) and (256, 2). In terms of different activation function, we have tried Tanh, PReLU, and LeakyReLU. Though they all give very similar results, we choose ReLU in the end. We have also experimented the dropout ratio in the range [0, 0.5]. We observed that with 0.5 dropout, the model reaches best performance on the test set. We also tried to tune the learning rate in the range[0.00001, 0.01]. For data preprocessing, we found that removing stopwords and stemming the words has negatively affected the performance of our model. Expanding abbreviations seems to have improve the performance of the model by reducing overfitting. Removing punctuations and special characters generally gives cleaner data for the tokenizer. Therefore, it helps both with the model training and model testing. Dependencies Python Json PyTorch Skit-Learn Transformers To install dependencies, you can use the included environment.ysml in the code directory to create a virtual environment with Anaconda. Installation reference can be found here. A detailed tutorial is also included in the DEMO. Contributions Junting Wang: Team Leader. Implemented the model and written up the code documentation. Tianwei Zhang: Helped with experiments, model testing and project DEMO.
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	"Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Matthew Walowski  mlw6@illinois.edu  Captain Zoheb Satta  satta2@illinois.edu Jiacheng Guo  jg17@illinois.edu 2.What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic (Free Topic): Our application will have the user rank a set of movies/tv shows in their preferred order (favorite to least favorite). We will then use sentiment analysis on publicly available review data for the provided movies/tv shows to predict how the average person would rank these movies. From this, we can inform the user how unique their movie tastes are. I.E. we can tell the user if they ranked the movies similarly to the public. Interest: Making an analysis of the sentiment of these movie reviews could help people know the general category of these movies in advance. It will help people to judge if they would like to watch this movie and choose their own favorite movie. Approach\Tools: We will utilize The Movie Database API( https://developers.themoviedb.org/ ) to get review information about each of the movies. To perform sentiment analysis we will use an existing python library (NTLK  https://www.nltk.org/howto/sentiment.html ). We will experiment with different modes of feedback for the user (how do we express the similarity between their ranking and the general ranking). 3.Which programming language do you plan to use? Python backend to do all the data processing, javascript/html (react or angular) for frontend. 4.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Workload: Scraping API data - 10 hours -Learn specifics about API usage -Get movie ID from movie title -Get reviews from movie ID -Download and parse movie reviews Sentiment Analysis - 15 hours -Given a list of reviews, perform sentiment analysis on all -Aggregate results -Output ranking of movies Similarity Measure - 10 hours -Take a list of user rankings and list of generated rankings -Output a numerical representation of similarity -This requires a bit of experimenting so it will take extra time, we test multiple techniques to find a suitable solution: -Ex: Try MAP, gMAP, simple difference, VSM,... Web Interface - 30 hours -Build a web interface to interface with the user -Problems to solve: -How to make it intuitive for users? -How to communicate between python backend and javascript frontend Total Hours: 65 hours Overview We are creating a web app that allows users to rank their favorite movies. We will then do sentiment analysis on reviews of those movies to show the user how similar their ranking is to how the general public would rank those movies. There are 4 components to this project * Frontend * Backend REST Endpoints * Review Scraping * Sentiment Analysis Frontend The frontend is finished. It's also been integrated with a mock endpoint until we create the real endpoint. See image below for screenshot of frontend. Backend REST Endpoints The web frontend will do the sentiment analysis by sending the user data to the backend which will run our python scripts. Right now, the frontend is communicating with a backend endpoint that returns fake data. We just need to drop in the real script once it's done. Review Scraping The scraping of data was done using BeautifulSoup and Python Requests. We scrape the Metacritic website for user reviews as well as critic reviews then combine them into one single array containing all the arrays as strings. Sentiment Analysis Sentiment analysis (also known as opinion mining is a text analysis technique that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph, sentence, or clause. Understanding people's emotions is essential for businesses since customers express their thoughts and feelings more openly than ever before. In sentiment analysis, we use some packages like nltk.classify, nltk.corpus, nltk.sentiment, nltk.sentiment.util. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings, like the following example (['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj'). We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets like the following train_subj_docs = subj_docs[:80]; test_subj_docs = subj_docs[80:100]. Then, we use simple unigram word features, handling negation: sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats) and apply features to obtain a feature-value representation of our datasets: training_set = sentim_analyzer.apply_features(training_docs); test_set = sentim_analyzer.apply_features(testing_docs). We can now train our classifier on the training set, and subsequently output the evaluation results, which is shown as the follows: Accuracy: 0.8, F-measure [obj]: 0.8, F-measure [subj]: 0.8, Precision [obj]: 0.8, Precision [subj]: 0.8, Recall [obj]: 0.8, Recall [subj]: 0.8 Challenges The biggest challenge is to familiar myself with the NLTK library during our sentiment analysis. The other challenge is due to the lack of api, I have to create a function that will translate the input movie title, into a url-appropriate title. For example: ""The Matrix"" -> ""the-matrix"" Future Steps * We need to decide how we want to display the result to users * There may be some small changes to the frontend for showing results Overview We are creating a web app that allows users to rank their favorite movies. We will then do sentiment analysis on reviews of those movies to show the user how similar their ranking is to how the general public would rank those movies. There are 4 components to this project - Frontend - Backend REST Endpoints - Review Scraping - Sentiment Analysis Frontend The frontend is finished. It's also been integrated with a mock endpoint until we create the real endpoint. See image below for screenshot of frontend. Backend REST Endpoints The web frontend will do the sentiment analysis by sending the user data to the backend which will run our python scripts. Right now, the frontend is communicating with a backend endpoint that returns fake data. We just need to drop in the real script once it's done. Review Scraping The scraping of data was done using BeautifulSoup and Python Requests. We scrape the Metacritic website for user reviews as well as critic reviews then combine them into one single array containing all the arrays as strings. Sentiment Analysis Sentiment analysis (also known as opinion mining is a text analysis technique that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph, sentence, or clause. Understanding people's emotions is essential for businesses since customers express their thoughts and feelings more openly than ever before. In sentiment analysis, we use some packages like nltk.classify, nltk.corpus, nltk.sentiment, nltk.sentiment.util. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings, like the following example (['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj'). We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets like the following train_subj_docs = subj_docs[:80]; test_subj_docs = subj_docs[80:100]. Then, we use simple unigram word features, handling negation: sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats) and apply features to obtain a feature-value representation of our datasets: training_set = sentim_analyzer.apply_features(training_docs); test_set = sentim_analyzer.apply_features(testing_docs). We can now train our classifier on the training set, and subsequently output the evaluation results, which is shown as the follows: Accuracy: 0.8, F-measure [obj]: 0.8, F-measure [subj]: 0.8, Precision [obj]: 0.8, Precision [subj]: 0.8, Recall [obj]: 0.8, Recall [subj]: 0.8 Challenges The biggest challenge is to familiar myself with the NLTK library during our sentiment analysis. The other challenge is due to the lack of api, I have to create a function that will translate the input movie title, into a url-appropriate title. For example: ""The Matrix"" -> ""the-matrix"" Future Steps We need to decide how we want to display the result to users There may be some small changes to the frontend for showing results Project Overview Technologies Backend Use Python 3.7 for all of backend (scraping data, sentiment analysis, similarity measure). Flask for the backend. Frontend NodeJS 14.15.0 with ReactJS for the frontend Yarn for package manager Scraping API data - 10 hours (Zoheb) Learn specifics about API usage Get movie ID from movie title Get reviews from movie ID Download and parse movie reviews Data from https://developers.themoviedb.org/ Sentiment Analysis - 15 hours (Leo) Given a list of reviews, perform sentiment analysis on all Aggregate results Output ranking of movies Analyzer from https://www.nltk.org/howto/sentiment.html Similarity Measure - 10 hours (Zoheb) Take a list of user rankings and list of generated rankings Output a numerical representation of similarity This requires a bit of experimenting so it will take extra time, we test multiple techniques to find a suitable solution: Ex: Try MAP, gMAP, simple difference, VSM,... Web Interface - 30 hours (Matthew) Build a web interface to interface with the user Problems to solve: How to make it intuitive for users? How to communicate between python backend and JavaScript frontend Timeline Nov 29 - Progress Report Dec 1 - Goal Finish Date (for code) Dec 13 - Project Due CourseProject Overview of the project This repository consists of two applications -- a web frontend and a python backend. Given a list of movies from the user on the frontend, it will be sent to the backend for processing. The backend will scrape reviews of the movies, do sentiment analysis, and compose a ranking of the movies based on this analysis. Then the user can compare their ranking of movies with the sentiment analyzer's rankings. Presentation See the presentation here! If you have any issues or want a live demo, email mlw6@illinois.edu Member Contributions See the Proposal.md file in the root of the repository for a breakdown of what each member contributed. Implementation Details about how the software work and its implementation can be found in Proposal.md and PROGRESS_UPDATE.md Running the project Prereqs Must have the following software installed: * Python 3.7 * Pip 3 * NodeJS 14.15.0 * Flask (See below) * Yarn (See below) Installing Flask and Yarn From anywhere in your command line, to install flask (and other dependencies), run pip install flask pip install flask_cors pip install requests pip install bs4 pip install html5lib pip install nltk And to install yarn, run npm install -g yarn Running the Backend Navigate to the backend/ directory and run the following command: flask run This will start the backend on port 5000 Running the Frontend Navigate to the frontend/ directory and run the following two commands in order: yarn yarn start This will start the frontend on port 3000."
https://github.com/LifeBringer/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Ludy11xc/CS-410-CourseProject	"CS 410 Progress Report 11/29/20 ludy2 Topic: Reproducing a paper- Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Progress Made: All progress thus far has been related to planning and researching. Collection of data from NY Times articles for evaluating the proposed algorithm has been completed. I have also done some research and thinking on how I will go about implementing the proposed algorithms (PLSA implemented based on the Lemur information retrieval toolkit) to run on the data. Remaining Tasks: Implement the proposed algorithms in python, run code on dataset with and without guidance from the time series, as well as with tweaked parameters (number of topics and strength of the prior), evaluate and analyze results and compare with results from the published paper. To do the data analysis, I will evaluate performance using Grainger tests. I will then be able to determine if I have successfully reproduced the results found in the paper, that ""ITMTF finds topics that are both more pure and more highly correlated with the external time series than typical topic modeling, especially with a strong feedback loop."" Adam Ludy Ludy2 CS 410 Project Proposal Reproducing a paper: Causal topic modeling 1.I am working as an individual. My name is Adam Ludy, my netid is ludy2. 2.I chose the paper about casual topic modeling. Mining causal topics in text data. 3.I plan on using python. 4.I believe I will be able to obtain the dataset used in the paper. I can access the NY Times articles from 2000 from these links:  https://spiderbites.nytimes.com/2000/ , https://spiderbites.nytimes.com/2001/ 5.N/A 6.N/A CS-410-CourseProject Presentation https://mediaspace.illinois.edu/media/1_8aiq79tk Objective Reproducing results from a paper: ""Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback"" Problems Data: The first problem encountered was obtaining the data in order used to produce the results. Unfortunately, I missed the piazza post to request access to the NY times dataset. So, I wrote a webscraper (data_scraper.py) to scrape articles from NY Times, which I was able to save in the data folder. I only saved results which contained ""bush"" or ""gore"" so it was a manageable size to be uploaded. For the Market data, I manually copied the data into a csv. Scope: The second problem encountered was the scope of the problem. After reading through the paper and doing a couple of hours of research, I discovered this would be a very complex result to produce, especially working as an individual, and factoring in my own gaps of knowledge involving the algorithm to be implemented. I was unable to completely recreate the algorithm using time stamp series feedback. Results I used an LDA model to find the top 3 words from topics discovered from the relevant documents. These are the results. | Top 3 Words in Significant Topics | | ---------- | | party nader vote | | tax plan social | | oil price juniper | | street music sunday | | company court death | | debate right candidate | | city game old | | clinton cheney know | | school test student | | clinton lazio mr_clinton | Looking at these words, we can definitely see some topics that were very relevant to the presidential election. Many of these words were also mined from the algorithm used in the paper. Setup/Run Code * If desired, set up virtual env. Then, to set up the code: * ``` Clone repo, could take a couple seconds-minutes depending on download speed because of data (20-30 MB) git clone https://github.com/Ludy11xc/CS-410-CourseProject.git Navigate into repo cd CS-410-CourseProject Install dependencies. If EnvironmentError is encountered, rerun with --user pip install -r requirements.txt ``` If you would like to run the websraper, you can with python data_scrapper.py However, know that it will most likely take multiple hours to complete, and the data has already been scraped and is present in the data folder. Then to run the code, ``` Run this to get results from current model python lda.py OR, run this to train a new model and get new results python lda.py train ```"
https://github.com/MLwithSandy/CourseProject	"Project Topic: Stock Recommender System[ ] [ ] [ ] [ ] [ ] [ Team MembersBackgroundProposalMarket researchHigh-level System designProject ] [ ]timelineFAQsTeam MembersNameNetIDEmail IDEzra Schroederezras2ezras2@illinois.eduRasbihari Palpal9pal9@illinois.eduSandeep Kumarkumar64kumar64@illinois.eduTeam captain marked in BOLD.BackgroundIn the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investor of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors.ProposalWe would like to propose a solution ""Stock recommender system"" to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combinesstock rating data from various market analyst, market sentiments and company profile for determination of the cumulated rating and curation of the recommendation list. Market researchFor the viability of the stock recommender system, we researched features offered by existing tools (freely available tools) such as Yahoo Finance, eToro platform etc. Each of these tools offers all the information, required for decision making, in isolation and relies on the users to combine them for their investment decisions. As an example, above screenshots shows a recommender from Yahoo Finance which cumulates ratings from various market analysts but does not include current market sentiments and does not recommend similar stocks.Based on our analysis, we have not come across any product which provides sentiment weighted cumulated stock rating and recommendation for similar stocks which can be used by retail investors for faster and informed decision making.High-level System designThere might be several possible approaches to solve the problem - to allow easy access to information for informed investment decision, we would like to propose following solution design, based on our learnings from the UIUC course ""CS 410 - Text Information System"". The solution design leverages following aspects of the CS410 course to build the Stock recommender system.Scraping of webpages for relevant information and collating the dataSentiment analysis based on user textsRecommender system based on content similarityProject timelineThe implementation of the project is planned in seven phases. First phase is focused on project planning and task distribution. Subsequent four phases are aimed to development of major software components such as Ratings system, Sentiment Analysis etc. The last two phase focuses on UI and documentation which will be executed in parallel to component development. FAQsWhat is the function of the tool?The stock recommender system helps customers, who would like to invest in equity market, to identify best value stock for investment based on consolidated rating of various market analyst, who rates the stock after in-depth research, and based on current market sentiment, judged via twitter feeds.Who will benefit from such a tool?Retail investors, financial institution etc.Does this kind of tools already exist? If similar tools exist, how is your tool different from them? Would people care about the difference?Based on our research, there are various tools in the market offering various features such as cumulated ratings. However, we have not come across any product which provides sentiment weighted average stock ratings and recommendation for similar stocks which can be used by retail investors for faster and informed decision making.What existing resources that we use?Sourcing of Market Analysts ratings by web scraping (e.g. Python Beautiful Soup, etc.):Stock Target Advisor https://www.stocktargetadvisor.com/Market Beat https://www.marketbeat.com/Market SentimentsTwitter FeedsWe intend to use the Twitter dev API https://developer.twitter.com/en to ingest tweets for related companies to analyze user sentiments. This will help us create a sentiment index to obtain a positive / negative pulse on a given stock ticker symbol (e.g. AAPL) based on public opinion of users interested in a given stock. What techniques/algorithms will you use to develop the tool?Refer to Proposal and High-level System design to understand the approach we are planning to use for development of the tool.How will you demonstrate the usefulness of your tool?As part of this exercise, we may take feedback from our class about the usefulness of the tool and any improvement recommendation.A very rough timeline to show when you expect to finish what.Refer to Project Timeline for details Project Report Stock Recommender System CS 410: Text Information System University of Illinois, Urbana-Champaign Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. [ Team Members ] [ Project Overview ] [ Abstract ] [ Motivation ] [ Introduction ] [ High Level Design ] [ Component View (Deployment Perspective) ] [ SRE Front-end (UI) ] [ SRE Back-end (Rest API) ] [ Implementation details ] [ Rating System ] [ Sentiment Analysis ] [ Recommender System ] [ Libraries and Datasets ] [ Verification ] [ Conclusion and Future Work ] [ References ] Project Overview Abstract Our project is a novel text-mining application which combines 3 different functionalities around rating stocks into one application. Namely, it consists of a base analysis which is a two-tiered summary of sentiment of stock analyst ratings who are well known in the industry and rate stock ticker symbols, a twitter sentiment analysis aspect which scrapes tweets off of Twitter and analyzes them for sentiment about a particular stock symbol, and a recommendation engine which recommends stock symbols similar to a user-provided query stock symbol. There are many conceivable use-cases for an app such as ours upon further embellishment and improvements, such as individual investors in the stock market who want automated and instantaneous advice and suggestions that incorporates both analyst ratings from well-known analysts across the internet, public sentiment as embodied by tweets, and that produces not only concise summaries of these analyst ratings and Twitter sentiment but recommends similar stocks to their stock symbol (e.g. AAPL) of interest. Imaginably there may well also be corporate interest in incorporating an application such as ours into a larger pipeline which could be serving a huge myriad of purposes but that leverages actionable knowledge about the stock market into a larger purpose. Although our application is alpha version, it is not inconceivable that it could springboard academic research into these areas. Motivation In the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investors of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors. Introduction We propose ""Stock recommender system"" as a solution to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combines stock rating data from various market analyst, market sentiments and company profile for determination of the cumulated rating and curation of the recommendation list. High Level Design Component View (Deployment Perspective) Stock Recommendation System consists of two components - SRE Front-end and SRE Back-end. The SRE Front-end is the UI component for user interaction, developed using Angular. The Front-end relies on SRE Back-end for all its data needs for providing various user centric functions. The SRE Back-end is the main component, which implements all the necessary algorithm and business rules and finally exposes the data related to ratings, recommendations and tweets via Rest APIs. SRE Front-end (UI) The SRE Front-end is developed in Angular framework and is a single page application. It consists of the following components: A. app-container The main component to render the Stock Recommendation System web page B. app-navbar The component responsible for top navigation bar C. app-routing The component dealing with url based routing and takes care of routing to the home page or canonical ""page-not-found"" page. D. home.component This is the main component which takes user input, calls SRE backend services to fetch relevant data. E. p404.component This is the component for handling ""page not found"" scenario in case user enters incorrect URL not supported by SRE Front-end. SRE Back-end (Rest API) The SRE Back-end is a Flask based app, developed using Python and TinyDB which exposes various Rest APIs for the SRE Front-end. List of Rest APIs: Rest API Sample Response Get list of all stocks in the corpus (output abridged for succinctness) GET /stock/all [ { ""Symbol"":""AAPL"", ""Security Name"": ""Apple Inc"", ""Market"": ""NASDAQ"", ""Sector"": ""Technology"" } ] Get ratings for a given stock from a given market GET /stock/ratings/<market>/<stock_symbol> [ { ""stockSymbol"": ""AAPL"", ""marketPlace"": ""NASDAQ"", ""refreshData"": ""2020-11-25"", ""overallRating"": ""HOLD"", ""analystsRatings"": [ { ""level_0"": 0, ""index"": 1, ""ratingDate"": ""2020-11-19"", ""ratingAgency"": ""The Goldman Sachs Group"", ""ratingAssigned"": ""Sell"", ""newRatings"": -1, ""scaledRatings"": ""SELL"" } ] } ] Get a list of recommended stocks, similar to given stock GET /stock/recommendation/<stock_symbol> [ { ""seq"": 1, ""stockSymbol"": ""HAFC"", ""stockName"": ""Hanmi Financial Corporation"", ""sector"": ""Finance"", ""rating"": ""SELL"" }, ] Get Twitter sentiment for given stock 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Get list of user request log GET /requests/all [ { ""datetime"": ""2020-11-25T21:55:52.924706"", ""symbol"": ""TSLA"", ""market"": ""NASDAQ"" } ] List of the Rest APIs exposed out of the backend can be found on GitHub project repository: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/StockRatingsSystem The SRE Back-end is comprised of the following components and/or modules: A. Flask app This is the main component which exposes various Rest APIs for external consumers - in our case SRE Front-end. It integrates all other components such as ratings_system, recommender_system etc. to provide the necessary services. In addition, it also logs all requests to requestLogDB. B. ratings_system The ratings_system is the component which calculates the overall rating for the user selected (searched) stock, by scraping the required ratings data from a variety of market analyst websites and aggregating them. It also provides the list of our selected latest ten ratings from an assortment of various market analysts as a reference. C. recommender_system The recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected (searched) stock and a pre-defined similarity function. D. TwitterSentimentAnalysis TwitterSentimentAnalysis component fetches latest tweets for the user selected (searched) stock from Twitter which by making use of the Twitter API, performs sentiment analysis and returns overall sentiment for the given stock. It also provides the latest five tweets as a reference. E. nasdaq The nasdaq component is primarily used for preparation of data. It combines the nasdaq listed stocks with the dataset we used for identifying various feature parameters of a stock. By cleaning and combining our data sources, we were able to create a final dataset of 1462 stocks for use across our systems in our app (analyst ratings, twitter sentiment, recommender system). F. batchRatingProcess The batchRatingProcess is used to update ratings of all 1462 stocks in background. This is to ensure that the user does not face any significant delay in fetching the data from third party websites while interacting with our SRE app's UI. In addition, the ratings of a stock is not updated frequently and batch processing fits well from a requirements and solution design perspective. G. batchRecoDataPreparation The batchRecoDataPreparation is another batch process to prepare the data for the recommendation system as the recommendation engine relies on an assortment of analyst ratings of stocks as a feature in the similarity function. Implementation details Rating System A. Approach To calculate the ratings, the following algorithm is used: First of all, rating for the selected(searched) stock is searched in the ratings database. Case 1: ratings data is already available in the ratings database. In such a case, ratings data is fetched from the database. Case 2: ratings data is not available in ratings database. In this case, following step is executed to get the ratings. Read the website marketbeat.com which lists the ratings from various market analysts and scrape the page section containing ratings relevant date Data is cleansed, validated and structured as per requirement In case of more than 10 ratings, only latest 10 ratings are selected. Initially data from only current and previous months were considered but to increase the data quantity, this restriction is switched off in final product Final list of ratings is stored in the database Once the rating data is available, ratings from various market analysts are scaled to following ratings scale: {-1: SELL, 0: HOLD, 1: BUY} Finally, overall rating is calculated based on the selected scaled ratings with possible outcome as SELL, HOLD or BUY. B. Webpage Scraper Webpage scraper was developed using the learnings from MP 2.1 of the course CS 410: Text Information System. Python library BeautifulSoup and Chrome driver were primarily used to fetch the web document. Following URL builder code were used to get URL for various stocks: # MarketBeat URL builder def get_mb_url(market, stock_symbol): base_url = 'https://www.marketbeat.com/stocks/{market}/{stock_symbol}/price-target/' final_url = base_url.format(market=market, stock_symbol=stock_symbol) return final_url e.g. https://www.marketbeat.com/stocks/NASDAQ/AAPL/price-target/ As part of the exercise, following additional aspects were required to be taken care: 1. the time delay in reading the complete web document, and 2. the URL redirects - in case a particular stock was not found on the website, website would redirect the URL request to a default page. # create a webdriver object and set options for headless browsing def load_webdriver(): if str(filePath).find(""CS410"") == -1: options = webdriver.ChromeOptions() options.add_argument('--no-sandbox') options.add_argument('--headless') options.add_argument(""--disable-dev-shm-usage"") driver = webdriver.Chrome(chrome_options=options) else: options = Options() options.headless = True driver = webdriver.Chrome(filePath / 'chromedriver', options=options) return driver # read web document using beutifulsoup def get_js_soup(url_web, driver): driver.get(url_web) time.sleep(5) if url_web != redirected_url: print(""redirected URL : "" + redirected_url) new_url = redirected_url + ""price-target/"" print(""new URL for Analysts rating : "" + new_url) if new_url.find('NASDAQ/price-target/') == -1: driver.get(url_web) else: return Using the html div class identifier, relevant data were extracted from the webpage data and subsequently processed in panda dataframe. table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-left-column fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table""}) if table is None: print(stock_symbol + "": No table found for market analyst rating"") else: table_body = table.find('tbody') rows = table_body.find_all('tr') for row in rows: cols = row.find_all('td') cols = [ele.text.strip() for ele in cols] data.append([ele.split("""")[-1].strip() for ele in cols if ele]) C. Uniform Scaling of Ratings Uniform scaling of ratings was required to unify the ratings from various market analysts and ensure that the calculation of overall rating is without any bias and is not affected by some higher or lower ratings from some market analysts, only due to the fact that they use different scales for rating a stock. Various ratings were mapped to {-1: SELL, 0: HOLD, 1: BUY} using following mapping dictionary is used. ratings_dict = { ""SELL"": -1, ""STRONG SELL"": -1, ""BEARISH"": -1, ""UNDERPERFORM"": -1, ""SECTOR UNDERPERFORM"": -1, ""MODERATE SELL"": -1, ""WEAK HOLD"": -1, ""UNDERWEIGHT"": -1, ""REDUCE"": -1, ""HOLD"": 0, ""NEUTRAL"": 0, ""AVERAGE"": 0, ""MARKET PERFORM"": 0, ""SECTOR PERFORM"": 0, ""SECTOR WEIGHT"": 0, ""PEER PERFORM"": 0, ""EQUAL WEIGHT"": 0, ""IN-LINE"": 0, ""MARKET OUTPERFORM"": 1, ""OUTPERFORM"": 1, ""MODERATE BUY"": 1, ""ACCUMULATE"": 1, ""OVER-WEIGHT"": 1, ""OVERWEIGHT"": 1, ""STRONG-BUY"": 1, ""ADD"": 1, ""BULLISH"": 1, ""BUY"": 1, ""POSITIVE"": 1, ""STRONG BUY"": 1, ""TOP PICK"": 1, ""CONVICTION-BUY"": 1, ""OUTPERFORMER"": 1 } There were other options also considered e.g., scaling of all ratings on a scale of 1-5 or 1-3, weighted scale to reflect strong ratings e.g., strong buy or strong sell. However, based on our need to combine the analyst rating with twitter sentiment, scaling to {-1: SELL, 0: HOLD, 1: BUY} were selected for this exercise. D. Overall Ratings Calculation First of all, aggregated ratings of all ratings from various market analyst is calculated based on arithmetic mean of all selected scaled ratings. e.g., aggregated rating for following five scaled ratings {-1: SELL, 1: BUY, 0: HOLD, 1: BUY, -1: SELL} from various market analyst will be {0: HOLD}. Once the aggregated rating is calculated, it is combined with Twitter Sentiment analysis result as per below table for Overall ratings. Aggregated Rating Twitter Sentiment Overall Ratings -1: SELL POSITIVE HOLD -1: SELL NEUTRAL SELL -1: SELL NEGATIVE SELL 0: HOLD POSITIVE BUY 0: HOLD NEUTRAL HOLD 0: HOLD NEGATIVE SELL 1: BUY POSITIVE BUY 1: BUY NEUTRAL BUY 1: BUY NEGATIVE HOLD Sentiment Analysis Sentiment Analysis, also known as Opinion Mining, refers to the use of Natural Language Processing to computationally determine opinions and emotions of an opinion holder for an opinion target. A common use for this technology is to discover how people feel about certain topics, particularly through users' textual posts in Social Media space. To perform Sentiment Analysis to provide stock recommendation, Twitter has been considered as the Social Media space where users post their opinion as their tweets. As most of the elements in the opinion representation such as the opinion holder (Twitter users in this case) and opinion target (Stock to be recommended in this case) and the content and the context of the opinion (financial context) are already known, the main task is to decide opinion sentiment. So, this is a case of just using sentiment classification for understanding opinion where the input is opinionated text object, the output is a sentiment label i.e., polarity analysis with predefined categories such as positive, negative, or neutral. For this project, the sentiment analysis has been done through the process outlined below. Process Description A. Preparing the Data Set To build the model, training and testing data set is needed. Ideally for optimal performance the financial tweets need to be downloaded from tweeter and should be human evaluated to create the labels which can be used for training the model and later for testing the model. However, one needs to spend considerable amount of manual effort to build such data. To optimize time and resource for this project, a readily available downloadable training set (polarity dataset from Cornell university -) has been used. The data set contain 2000 processed down-cased text files used in Pang/Lee ACL 2004 [1]; the names of the two subdirectories in that folder, ""pos"" and ""neg"", indicate the true classification (sentiment) of the component files according to the automatic rating classifier the tweets of the data set have been all labeled as positive or negative, depending on the content. The data set have been persisted into pickle file (a binary representation of python structure) to optimize performance of the subsequent run to build the classifier. import pickle from sklearn.datasets import load_files #nltk.download('stopwords') #Import Dataset -> generate two classes one each for each sub directorties dataset = load_files('txt_sentoken/') X,y = dataset.data, dataset.target #store as pickle file, these are byte type file with open('X.pickle', 'wb') as f: pickle.dump(X,f) with open('y.pickle', 'wb') as f: pickle.dump(y,f) The same data has been split into training and testing data set following a 80-20 rule where 80% of the downloaded pre-labeled data has been used as training data set and 20% of the same downloaded data has been used as testing data set. B. Preprocessing the Data Set The downloaded data set has been preprocessed before feeding into the program to create the Classifier to remove all the non-word characters, to convert into lower case, to remove single characters (e.g. i, a etc.). import re import pickle with open('X.pickle', 'rb') as f: X=pickle.load(f) with open('y.pickle', 'rb') as f: y=pickle.load(f) corpus = [] for i in range(0,len(X)): data = re.sub(r'\W', ' ', str(X[i])) data = data.lower() data = re.sub(r'\s+[a-z]\s+', ' ', data) data = re.sub(r'^[a-z]\s+', ' ', data) data = re.sub(r'\s+', ' ', data) corpus.append(data) C. Building the BOW, TF-IDF and Logistic Regression Classifier Scikit-learn library (a free machine learning library for Python) has been used to create the Classifier. At first, the bag of words model has been created and later the bag of words model would be converted into TF-IDF model. To covert the data into bag of words model, classes from Scikit-learn has been used. First, a max feature has been set to 2000 which means 2000 most frequent words would be used as features. The min document frequency would ensure to exclude a word to be considered as 2000 features which appear 3 or less documents (to prevent a word to become a feature which is rare into the set but very popular within a certain document) and the max document frequency would ensure to exclude a word to be considered as 2000 features which appear 60% or more documents (to exclude the most common words like the, an etc.). Then Stop words has been removed that is defined in nltk corpus. Now to form the Bag of Words model the corpus from above steps has been used. from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer from nltk.corpus import stopwords nltk.download('stopwords') vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X = vectorizer.fit_transform(corpus).toarray()#will generate 2D array [len(X),len(X)], total number of docs = number of features = len(x) Finally, TfidfTransformer from sklearn is used to create TF-IDF model from Bag of Words model created earlier. from sklearn.feature_extraction.text import TfidfTransformer # convert BOW to TF-IDF transformer = TfidfTransformer() X = transformer.fit_transform(X).toarray() Logistic Regression (a Discriminative Classifier) is a classification algorithm (learning algorithm) which is used for binary classification problem. The task of sentiment analysis for this project can be thought as a binary classification problem where the goal is to predict positive or negative sentiment from a given tweets (a sentence in particular). Hence for the purpose of this project, a logistics regression classifier has been built where negative and positive sentiments have been denoted as 0 and 1 respectively. So, the Binary Response Variable Y Ie {0,1} needs to be calculated from the predictor X where X = { x1, x2 ..... x2000} (all the 2000 features) Hence, for Logistics Regression can be represented as below: So essentially using training data T, , parameters (M=2000) needs to be estimated. Hence the conditional likelihood estimate is: and The goal of Logistic Regression algorithm is to optimize the parameters using training data set. Once the optimal values of the parameters are found by the algorithm, y can be calculated for any unknown sentiment of a new sentence. If y>=0.5, then that sentiment is positive sentiment and if y<0.5, then that sentiment is negative sentiment. For this project, LogistricRegression from sklearn has been used to build the classifier. First the input data set has been split as - 80% of available data has been considered as training data and 20% of the available data has been considered as test data from sklearn.model_selection import train_test_split text_train,text_test,sentiment_train,sentiment_test = train_test_split(X,y,test_size=0.2,random_state=0)#80% training and 20% testing data Then classifier is built: from sklearn.linear_model import LogisticRegression # create the classifier using logistic regression classifier = LogisticRegression() classifier.fit(text_train,sentiment_train) Just to showcase the model performance, confusion_matrix class from sklearn has been used. With the input data, close to 85% accuracy has been achieved. from sklearn.metrics import confusion_matrix #testing model performance sentiment_prediction = classifier.predict(text_test) cm = confusion_matrix(sentiment_test,sentiment_prediction)# [[predicted as 0 and actually 0, predicted as 0 and actually 1], #[predicted as 1 and actually 0, predicted as 1 and actually 1]] print(""accuracy : "", (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,""%"") ''' [model predicted 0and actual 0 model predicted 1 nand actual 0 model predicted 1 and actual 0 model predicted 1 and actual 1] ''' Finally, the model and vectorizer have been stored as pickle file (binary representation of python objects) so that while calculating the real time tweets, the saved model can be used as it is. from sklearn.feature_extraction.text import TfidfVectorizer #store the classifier ....pickle file with open('classifier.pickle', 'wb') as f: pickle.dump(classifier,f) #store the TFIDF vectorizer vectorizerTFIDF = TfidfVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X_TDIDF = vectorizerTFIDF.fit_transform(corpus).toarray() with open('vectorizerTFIDF.pickle', 'wb') as f: pickle.dump(vectorizerTFIDF,f) D. Fetching the real time tweets A developer app has been created in Twitter for the purpose of this project. The OAuth2 mechanism has been used to make API calls to Twitter API for fetching the recent tweets. ConsumerKey and ConsumerSecret from the created app are used to generate bearer token in the runtime API call using https://api.twitter.com/oauth2/token Finally, the bearer token is used to call API to perform recent search (returns Tweets from the last 7 days that match a search query) using ""recent search"" API. https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent For this project, Max result of 100 has been set to perform recent search. This is to ensure throttling of number of results fetched (as for the basic access, twitter account has 500000 search results/month limitation) E. Performing Sentiment analysis of the fetched tweets To perform sentiment analysis in real time, firstly the Logistic Regression Classifier model and TF-IDF vectorizer is loaded from the saved pickle file. import pickle import re with open('classifier.pickle', 'rb') as f: clf=pickle.load(f) with open('vectorizerTFIDF.pickle', 'rb') as f: vectorizer=pickle.load(f) For a given stock the ""recent search"" API is used to search recent tweets. After fetching related tweets (max = 100), each tweet is preprocessed to create bag of words and to be represented as TF-IDF vectorizer. Then, prebuilt Logistic Regression Classifier is used to predict the sentiment of each tweets. for t in tweets_fetched: t = re.sub(r'^https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*$',' ',t) t = t.lower() t = re.sub(r""that's"",'that is',t) t = re.sub(r""there's"",'there is',t) t = re.sub(r""what's"",'what is',t) t = re.sub(r""where's"",'where is',t) t = re.sub(r""it's"",'it is',t) t = re.sub(r""who's"",'who is',t) t = re.sub(r""i'm"",'i am',t) t = re.sub(r""she's"",'she is',t) t = re.sub(r""he's"",'he is',t) t = re.sub(r""they're"",'they are',t) t = re.sub(r""who're"",'who are',t) t = re.sub(r""shouldn't"",'should not',t) t = re.sub(r""wouldn't"",'would not',t) t = re.sub(r""couldn't"",'could not',t) t = re.sub(r""can't"",'can not',t) t = re.sub(r""won't"",'will not',t) t = re.sub(r'\W', ' ', t) t = re.sub(r'\d', ' ', t) t = re.sub(r'\s+[a-z]\s+', ' ', t) t = re.sub(r'\s+[a-z]$', ' ', t) t = re.sub(r'^[a-z]\s+', ' ', t) t = re.sub(r'\s+', ' ', t) sentiment = clf.predict(vectorizer.transform([t]).toarray()) Finally, total positive and negative sentiments are calculated for total tweets fetched in runtime and the final sentiment of a particular stock has been calculated as positive = 1 (if the positive percentage > 65%), negative = -1 (if the positive percentage < 35%) or neutral = 0 (if the positive percentage is in between 35% and 65%). if (tot_positive+tot_negetive)>0 : positive_percentage = tot_positive/(tot_positive+tot_negetive) print(""positive_percentage :"",positive_percentage*100, ""%"") if positive_percentage>0.65 : print(""stock is buy"") return 1 elif positive_percentage<0.35 and positive_percentage>0: print(""stock is sell"") return -1 else: print(""stock is neutral"") return 0 Following end point has been built to provide twitter sentiment analysis result to a stock. Definition Get Twitter sentiment for given stock - 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> e.g. http://localhost:5000/stock/sentiments/NASDAQ/AAPL Response 200 OK on success json { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Sample run: Recommender System Approach The basic approach to the recommendation engine is to use a similarity function to compare pre-identified features of all stocks in our corpus against user-provided stock symbol features. While doing so, we are implementing one of the methods for building recommendation engine - Content based recommendation or Item similarity, which was part of Week 6 lecture of course CS 410 Text Information System. Recommender System recommends to the user the top 5 stock symbols which are most similar to the user provided stock symbol in terms of those underlying features and the similarity function. For the initial, Cosine similarity is used as similarity function and the following financial/economic features were selected as underlying features for similarity calculation. company is in S&P 500 company profitability over last three years revenue growth for last three years current market analyst ratings sector gross profit per market cap Analyst ratings are calculated by rating_system and same is reused also for recommendation system. Other features for the stock are calculated or derived using the financial data of the companies from year 2018. Source: Kaggle dataset available at https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018. As part of the exercise, significant amount of effort was used to identify a single data source for financial or economic data and also to understand various financial terms to find the right feature. Based on our current understanding of a company performance and its relation to stock price, we used above listed features. The features might be further optimized with right guidance from a financial analyst or person with insight of stock market and factors influencing the investor decisions. Libraries and Datasets Python Libraries Flask flask_cors markdown beautifulsoup4 selenium pandas pymongo tinydb misaka nltk requests sklearn flask-jsonpify Datasets https://www.marketbeat.com/ https://www.cs.cornell.edu/people/pabo/movie-review-data/ https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018?select=2018_Financial_Data.csv https://developer.twitter.com Verification The result of the recommender system was verified based on manual verification process by selecting some stocks at random and checking the result for individual section e.g., ratings, sentiments and overall ratings. For example, Overall rating for Facebook (FB) stock is BUY. Looking at individual ratings component, the result seems reasonable. Analyst Ratings: BUY Verification: In the list of market analyst ratings, all 10 analysts have rated it as BUY. Therefore, the aggregated rating is BUY. Tweet Sentiments: Neutral Verification: From the latest 100 tweets fetched (as shown in below screenshot), it seems reasonable to have a NEUTRAL sentiment Overall Rating: BUY Verification: Combining Analyst Ratings ""Buy"" with Twitter Sentiments ""NEUTRAL"", as per listed decision table, the result is BUY. Conclusion and Future Work Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating (e.g., buy, hold, sell) for the user entered Stock symbol and recommending five similar stocks. The result has been validated by some popular stocks. One of key challenges faced in building the system is the access on right financial data as well as good training and testing set to train and test the model for sentiment analysis. The choice we had to manually create the required data. However, to optimize the time and resource available, it has been decided to use readily available data in the web which might have not yielded the perfect recommendation. There are several interesting directions for the future version of recommender system. First, the overall functionality can be improved by considering user input of sectors and providing recommendation and trends specific to that sector. A second direction involves defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market and use the same to train and test the model. Given the current trend of machine learning algorithms, a third interesting research direction is to explore the timeseries data for stock adjacency. Finally, overall score can be improved further to consider more analysts' reports and microblogging websites and come up with more recommender categories i.e., Strong Buy, Buy, Hold, Sell, Strong Sell etc. At the end, it has been a great journey of ideation and learning in a collaborative manner to design and develop the stock recommender system. We thank different analysts' websites to make their ratings available publicly and twitter to grant access of real time tweets through their public API and last but not the least, we thank professor ChengXiang Zhai and all our TAs and all the reviewers to give the direction needed and to provide the valuable feedback. References [1] Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL '04). Association for Computational Linguistics, USA, 271-es. DOI:https://doi.org/10.3115/1218955.1218990 [2] Carbone, N. (2020, January 18). 200+ Financial Indicators of US stocks (2014-2018). Retrieved December 11, 2020, from https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018 Project Report Stock Recommender System CS 410: Text Information System University of Illinois, Urbana-Champaign Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. [ Team Members ] [ Project Overview ] [ Abstract ] [ Motivation ] [ Introduction ] [ High Level Design ] [ Component View (Deployment Perspective) ] [ SRE Front-end (UI) ] [ SRE Back-end (Rest API) ] [ Implementation details ] [ Rating System ] [ Sentiment Analysis ] [ Recommender System ] [ Libraries and Datasets ] [ Verification ] [ Conclusion and Future Work ] [ References ] Project Overview Abstract Our project is a novel text-mining application which combines 3 different functionalities around rating stocks into one application. Namely, it consists of a base analysis which is a two-tiered summary of sentiment of stock analyst ratings who are well known in the industry and rate stock ticker symbols, a twitter sentiment analysis aspect which scrapes tweets off of Twitter and analyzes them for sentiment about a particular stock symbol, and a recommendation engine which recommends stock symbols similar to a user-provided query stock symbol. There are many conceivable use-cases for an app such as ours upon further embellishment and improvements, such as individual investors in the stock market who want automated and instantaneous advice and suggestions that incorporates both analyst ratings from well-known analysts across the internet, public sentiment as embodied by tweets, and that produces not only concise summaries of these analyst ratings and Twitter sentiment but recommends similar stocks to their stock symbol (e.g. AAPL) of interest. Imaginably there may well also be corporate interest in incorporating an application such as ours into a larger pipeline which could be serving a huge myriad of purposes but that leverages actionable knowledge about the stock market into a larger purpose. Although our application is alpha version, it is not inconceivable that it could springboard academic research into these areas. Motivation In the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investors of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors. Introduction We propose ""Stock recommender system"" as a solution to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combines * stock rating data from various market analyst, * market sentiments and * company profile for determination of the cumulated rating and curation of the recommendation list. High Level Design Component View (Deployment Perspective) Stock Recommendation System consists of two components - SRE Front-end and SRE Back-end. * The SRE Front-end is the UI component for user interaction, developed using Angular. The Front-end relies on SRE Back-end for all its data needs for providing various user centric functions. * The SRE Back-end is the main component, which implements all the necessary algorithm and business rules and finally exposes the data related to ratings, recommendations and tweets via Rest APIs. SRE Front-end (UI) The SRE Front-end is developed in Angular framework and is a single page application. It consists of the following components: A. app-container The main component to render the Stock Recommendation System web page B. app-navbar The component responsible for top navigation bar C. app-routing The component dealing with url based routing and takes care of routing to the home page or canonical ""page-not-found"" page. D. home.component This is the main component which takes user input, calls SRE backend services to fetch relevant data. E. p404.component This is the component for handling ""page not found"" scenario in case user enters incorrect URL not supported by SRE Front-end. SRE Back-end (Rest API) The SRE Back-end is a Flask based app, developed using Python and TinyDB which exposes various Rest APIs for the SRE Front-end. List of Rest APIs: Rest API Sample Response Get list of all stocks in the corpus (output abridged for succinctness) GET /stock/all [ { ""Symbol"":""AAPL"", ""Security Name"": ""Apple Inc"", ""Market"": ""NASDAQ"", ""Sector"": ""Technology"" } ] Get ratings for a given stock from a given market GET /stock/ratings/<market>/<stock_symbol> [ { ""stockSymbol"": ""AAPL"", ""marketPlace"": ""NASDAQ"", ""refreshData"": ""2020-11-25"", ""overallRating"": ""HOLD"", ""analystsRatings"": [ { ""level_0"": 0, ""index"": 1, ""ratingDate"": ""2020-11-19"", ""ratingAgency"": ""The Goldman Sachs Group"", ""ratingAssigned"": ""Sell"", ""newRatings"": -1, ""scaledRatings"": ""SELL"" } ] } ] Get a list of recommended stocks, similar to given stock GET /stock/recommendation/<stock_symbol> [ { ""seq"": 1, ""stockSymbol"": ""HAFC"", ""stockName"": ""Hanmi Financial Corporation"", ""sector"": ""Finance"", ""rating"": ""SELL"" }, ] Get Twitter sentiment for given stock 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Get list of user request log GET /requests/all [ { ""datetime"": ""2020-11-25T21:55:52.924706"", ""symbol"": ""TSLA"", ""market"": ""NASDAQ"" } ] List of the Rest APIs exposed out of the backend can be found on GitHub project repository: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/StockRatingsSystem The SRE Back-end is comprised of the following components and/or modules: A. Flask app This is the main component which exposes various Rest APIs for external consumers - in our case SRE Front-end. It integrates all other components such as ratings_system, recommender_system etc. to provide the necessary services. In addition, it also logs all requests to requestLogDB. B. ratings_system The ratings_system is the component which calculates the overall rating for the user selected (searched) stock, by scraping the required ratings data from a variety of market analyst websites and aggregating them. It also provides the list of our selected latest ten ratings from an assortment of various market analysts as a reference. C. recommender_system The recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected (searched) stock and a pre-defined similarity function. D. TwitterSentimentAnalysis TwitterSentimentAnalysis component fetches latest tweets for the user selected (searched) stock from Twitter which by making use of the Twitter API, performs sentiment analysis and returns overall sentiment for the given stock. It also provides the latest five tweets as a reference. E. nasdaq The nasdaq component is primarily used for preparation of data. It combines the nasdaq listed stocks with the dataset we used for identifying various feature parameters of a stock. By cleaning and combining our data sources, we were able to create a final dataset of 1462 stocks for use across our systems in our app (analyst ratings, twitter sentiment, recommender system). F. batchRatingProcess The batchRatingProcess is used to update ratings of all 1462 stocks in background. This is to ensure that the user does not face any significant delay in fetching the data from third party websites while interacting with our SRE app's UI. In addition, the ratings of a stock is not updated frequently and batch processing fits well from a requirements and solution design perspective. G. batchRecoDataPreparation The batchRecoDataPreparation is another batch process to prepare the data for the recommendation system as the recommendation engine relies on an assortment of analyst ratings of stocks as a feature in the similarity function. Implementation details Rating System A. Approach To calculate the ratings, the following algorithm is used: * First of all, rating for the selected(searched) stock is searched in the ratings database. o Case 1: ratings data is already available in the ratings database. In such a case, ratings data is fetched from the database. o Case 2: ratings data is not available in ratings database. In this case, following step is executed to get the ratings. # Read the website marketbeat.com which lists the ratings from various market analysts and scrape the page section containing ratings relevant date # Data is cleansed, validated and structured as per requirement # In case of more than 10 ratings, only latest 10 ratings are selected. # Initially data from only current and previous months were considered but to increase the data quantity, this restriction is switched off in final product # Final list of ratings is stored in the database * Once the rating data is available, ratings from various market analysts are scaled to following ratings scale: {-1: SELL, 0: HOLD, 1: BUY} * Finally, overall rating is calculated based on the selected scaled ratings with possible outcome as SELL, HOLD or BUY. B. Webpage Scraper Webpage scraper was developed using the learnings from MP 2.1 of the course CS 410: Text Information System. Python library BeautifulSoup and Chrome driver were primarily used to fetch the web document. Following URL builder code were used to get URL for various stocks: # MarketBeat URL builder def get_mb_url(market, stock_symbol): base_url = 'https://www.marketbeat.com/stocks/{market}/{stock_symbol}/price-target/' final_url = base_url.format(market=market, stock_symbol=stock_symbol) return final_url e.g. https://www.marketbeat.com/stocks/NASDAQ/AAPL/price-target/ As part of the exercise, following additional aspects were required to be taken care: 1. the time delay in reading the complete web document, and 2. the URL redirects - in case a particular stock was not found on the website, website would redirect the URL request to a default page. # create a webdriver object and set options for headless browsing def load_webdriver(): if str(filePath).find(""CS410"") == -1: options = webdriver.ChromeOptions() options.add_argument('--no-sandbox') options.add_argument('--headless') options.add_argument(""--disable-dev-shm-usage"") driver = webdriver.Chrome(chrome_options=options) else: options = Options() options.headless = True driver = webdriver.Chrome(filePath / 'chromedriver', options=options) return driver # read web document using beutifulsoup def get_js_soup(url_web, driver): driver.get(url_web) time.sleep(5) if url_web != redirected_url: print(""redirected URL : "" + redirected_url) new_url = redirected_url + ""price-target/"" print(""new URL for Analysts rating : "" + new_url) if new_url.find('NASDAQ/price-target/') == -1: driver.get(url_web) else: return Using the html div class identifier, relevant data were extracted from the webpage data and subsequently processed in panda dataframe. table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-left-column fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table""}) if table is None: print(stock_symbol + "": No table found for market analyst rating"") else: table_body = table.find('tbody') rows = table_body.find_all('tr') for row in rows: cols = row.find_all('td') cols = [ele.text.strip() for ele in cols] data.append([ele.split("""")[-1].strip() for ele in cols if ele]) C. Uniform Scaling of Ratings Uniform scaling of ratings was required to unify the ratings from various market analysts and ensure that the calculation of overall rating is without any bias and is not affected by some higher or lower ratings from some market analysts, only due to the fact that they use different scales for rating a stock. Various ratings were mapped to {-1: SELL, 0: HOLD, 1: BUY} using following mapping dictionary is used. ratings_dict = { ""SELL"": -1, ""STRONG SELL"": -1, ""BEARISH"": -1, ""UNDERPERFORM"": -1, ""SECTOR UNDERPERFORM"": -1, ""MODERATE SELL"": -1, ""WEAK HOLD"": -1, ""UNDERWEIGHT"": -1, ""REDUCE"": -1, ""HOLD"": 0, ""NEUTRAL"": 0, ""AVERAGE"": 0, ""MARKET PERFORM"": 0, ""SECTOR PERFORM"": 0, ""SECTOR WEIGHT"": 0, ""PEER PERFORM"": 0, ""EQUAL WEIGHT"": 0, ""IN-LINE"": 0, ""MARKET OUTPERFORM"": 1, ""OUTPERFORM"": 1, ""MODERATE BUY"": 1, ""ACCUMULATE"": 1, ""OVER-WEIGHT"": 1, ""OVERWEIGHT"": 1, ""STRONG-BUY"": 1, ""ADD"": 1, ""BULLISH"": 1, ""BUY"": 1, ""POSITIVE"": 1, ""STRONG BUY"": 1, ""TOP PICK"": 1, ""CONVICTION-BUY"": 1, ""OUTPERFORMER"": 1 } There were other options also considered e.g., scaling of all ratings on a scale of 1-5 or 1-3, weighted scale to reflect strong ratings e.g., strong buy or strong sell. However, based on our need to combine the analyst rating with twitter sentiment, scaling to {-1: SELL, 0: HOLD, 1: BUY} were selected for this exercise. D. Overall Ratings Calculation First of all, aggregated ratings of all ratings from various market analyst is calculated based on arithmetic mean of all selected scaled ratings. e.g., aggregated rating for following five scaled ratings {-1: SELL, 1: BUY, 0: HOLD, 1: BUY, -1: SELL} from various market analyst will be {0: HOLD}. Once the aggregated rating is calculated, it is combined with Twitter Sentiment analysis result as per below table for Overall ratings. Aggregated Rating Twitter Sentiment Overall Ratings -1: SELL POSITIVE HOLD -1: SELL NEUTRAL SELL -1: SELL NEGATIVE SELL 0: HOLD POSITIVE BUY 0: HOLD NEUTRAL HOLD 0: HOLD NEGATIVE SELL 1: BUY POSITIVE BUY 1: BUY NEUTRAL BUY 1: BUY NEGATIVE HOLD Sentiment Analysis Sentiment Analysis, also known as Opinion Mining, refers to the use of Natural Language Processing to computationally determine opinions and emotions of an opinion holder for an opinion target. A common use for this technology is to discover how people feel about certain topics, particularly through users' textual posts in Social Media space. To perform Sentiment Analysis to provide stock recommendation, Twitter has been considered as the Social Media space where users post their opinion as their tweets. As most of the elements in the opinion representation such as the opinion holder (Twitter users in this case) and opinion target (Stock to be recommended in this case) and the content and the context of the opinion (financial context) are already known, the main task is to decide opinion sentiment. So, this is a case of just using sentiment classification for understanding opinion where the input is opinionated text object, the output is a sentiment label i.e., polarity analysis with predefined categories such as positive, negative, or neutral. For this project, the sentiment analysis has been done through the process outlined below. Process Description A. Preparing the Data Set To build the model, training and testing data set is needed. Ideally for optimal performance the financial tweets need to be downloaded from tweeter and should be human evaluated to create the labels which can be used for training the model and later for testing the model. However, one needs to spend considerable amount of manual effort to build such data. To optimize time and resource for this project, a readily available downloadable training set (polarity dataset from Cornell university -) has been used. The data set contain 2000 processed down-cased text files used in Pang/Lee ACL 2004 [1]; the names of the two subdirectories in that folder, ""pos"" and ""neg"", indicate the true classification (sentiment) of the component files according to the automatic rating classifier the tweets of the data set have been all labeled as positive or negative, depending on the content. The data set have been persisted into pickle file (a binary representation of python structure) to optimize performance of the subsequent run to build the classifier. import pickle from sklearn.datasets import load_files #nltk.download('stopwords') #Import Dataset -> generate two classes one each for each sub directorties dataset = load_files('txt_sentoken/') X,y = dataset.data, dataset.target #store as pickle file, these are byte type file with open('X.pickle', 'wb') as f: pickle.dump(X,f) with open('y.pickle', 'wb') as f: pickle.dump(y,f) The same data has been split into training and testing data set following a 80-20 rule where 80% of the downloaded pre-labeled data has been used as training data set and 20% of the same downloaded data has been used as testing data set. B. Preprocessing the Data Set The downloaded data set has been preprocessed before feeding into the program to create the Classifier to remove all the non-word characters, to convert into lower case, to remove single characters (e.g. i, a etc.). import re import pickle with open('X.pickle', 'rb') as f: X=pickle.load(f) with open('y.pickle', 'rb') as f: y=pickle.load(f) corpus = [] for i in range(0,len(X)): data = re.sub(r'\W', ' ', str(X[i])) data = data.lower() data = re.sub(r'\s+[a-z]\s+', ' ', data) data = re.sub(r'^[a-z]\s+', ' ', data) data = re.sub(r'\s+', ' ', data) corpus.append(data) C. Building the BOW, TF-IDF and Logistic Regression Classifier Scikit-learn library (a free machine learning library for Python) has been used to create the Classifier. At first, the bag of words model has been created and later the bag of words model would be converted into TF-IDF model. To covert the data into bag of words model, classes from Scikit-learn has been used. First, a max feature has been set to 2000 which means 2000 most frequent words would be used as features. The min document frequency would ensure to exclude a word to be considered as 2000 features which appear 3 or less documents (to prevent a word to become a feature which is rare into the set but very popular within a certain document) and the max document frequency would ensure to exclude a word to be considered as 2000 features which appear 60% or more documents (to exclude the most common words like the, an etc.). Then Stop words has been removed that is defined in nltk corpus. Now to form the Bag of Words model the corpus from above steps has been used. from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer from nltk.corpus import stopwords nltk.download('stopwords') vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X = vectorizer.fit_transform(corpus).toarray()#will generate 2D array [len(X),len(X)], total number of docs = number of features = len(x) Finally, TfidfTransformer from sklearn is used to create TF-IDF model from Bag of Words model created earlier. from sklearn.feature_extraction.text import TfidfTransformer # convert BOW to TF-IDF transformer = TfidfTransformer() X = transformer.fit_transform(X).toarray() Logistic Regression (a Discriminative Classifier) is a classification algorithm (learning algorithm) which is used for binary classification problem. The task of sentiment analysis for this project can be thought as a binary classification problem where the goal is to predict positive or negative sentiment from a given tweets (a sentence in particular). Hence for the purpose of this project, a logistics regression classifier has been built where negative and positive sentiments have been denoted as 0 and 1 respectively. So, the Binary Response Variable Y Ie {0,1} needs to be calculated from the predictor X where X = { x1, x2 ..... x2000} (all the 2000 features) Hence, for Logistics Regression can be represented as below: So essentially using training data T, , parameters (M=2000) needs to be estimated. Hence the conditional likelihood estimate is: and The goal of Logistic Regression algorithm is to optimize the parameters using training data set. Once the optimal values of the parameters are found by the algorithm, y can be calculated for any unknown sentiment of a new sentence. If y>=0.5, then that sentiment is positive sentiment and if y<0.5, then that sentiment is negative sentiment. For this project, LogistricRegression from sklearn has been used to build the classifier. First the input data set has been split as - 80% of available data has been considered as training data and 20% of the available data has been considered as test data from sklearn.model_selection import train_test_split text_train,text_test,sentiment_train,sentiment_test = train_test_split(X,y,test_size=0.2,random_state=0)#80% training and 20% testing data Then classifier is built: from sklearn.linear_model import LogisticRegression # create the classifier using logistic regression classifier = LogisticRegression() classifier.fit(text_train,sentiment_train) Just to showcase the model performance, confusion_matrix class from sklearn has been used. With the input data, close to 85% accuracy has been achieved. from sklearn.metrics import confusion_matrix #testing model performance sentiment_prediction = classifier.predict(text_test) cm = confusion_matrix(sentiment_test,sentiment_prediction)# [[predicted as 0 and actually 0, predicted as 0 and actually 1], #[predicted as 1 and actually 0, predicted as 1 and actually 1]] print(""accuracy : "", (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,""%"") ''' [model predicted 0and actual 0 model predicted 1 nand actual 0 model predicted 1 and actual 0 model predicted 1 and actual 1] ''' Finally, the model and vectorizer have been stored as pickle file (binary representation of python objects) so that while calculating the real time tweets, the saved model can be used as it is. from sklearn.feature_extraction.text import TfidfVectorizer #store the classifier ....pickle file with open('classifier.pickle', 'wb') as f: pickle.dump(classifier,f) #store the TFIDF vectorizer vectorizerTFIDF = TfidfVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X_TDIDF = vectorizerTFIDF.fit_transform(corpus).toarray() with open('vectorizerTFIDF.pickle', 'wb') as f: pickle.dump(vectorizerTFIDF,f) D. Fetching the real time tweets A developer app has been created in Twitter for the purpose of this project. The OAuth2 mechanism has been used to make API calls to Twitter API for fetching the recent tweets. ConsumerKey and ConsumerSecret from the created app are used to generate bearer token in the runtime API call using https://api.twitter.com/oauth2/token Finally, the bearer token is used to call API to perform recent search (returns Tweets from the last 7 days that match a search query) using ""recent search"" API. https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent For this project, Max result of 100 has been set to perform recent search. This is to ensure throttling of number of results fetched (as for the basic access, twitter account has 500000 search results/month limitation) E. Performing Sentiment analysis of the fetched tweets To perform sentiment analysis in real time, firstly the Logistic Regression Classifier model and TF-IDF vectorizer is loaded from the saved pickle file. import pickle import re with open('classifier.pickle', 'rb') as f: clf=pickle.load(f) with open('vectorizerTFIDF.pickle', 'rb') as f: vectorizer=pickle.load(f) For a given stock the ""recent search"" API is used to search recent tweets. After fetching related tweets (max = 100), each tweet is preprocessed to create bag of words and to be represented as TF-IDF vectorizer. Then, prebuilt Logistic Regression Classifier is used to predict the sentiment of each tweets. for t in tweets_fetched: t = re.sub(r'^https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*$',' ',t) t = t.lower() t = re.sub(r""that's"",'that is',t) t = re.sub(r""there's"",'there is',t) t = re.sub(r""what's"",'what is',t) t = re.sub(r""where's"",'where is',t) t = re.sub(r""it's"",'it is',t) t = re.sub(r""who's"",'who is',t) t = re.sub(r""i'm"",'i am',t) t = re.sub(r""she's"",'she is',t) t = re.sub(r""he's"",'he is',t) t = re.sub(r""they're"",'they are',t) t = re.sub(r""who're"",'who are',t) t = re.sub(r""shouldn't"",'should not',t) t = re.sub(r""wouldn't"",'would not',t) t = re.sub(r""couldn't"",'could not',t) t = re.sub(r""can't"",'can not',t) t = re.sub(r""won't"",'will not',t) t = re.sub(r'\W', ' ', t) t = re.sub(r'\d', ' ', t) t = re.sub(r'\s+[a-z]\s+', ' ', t) t = re.sub(r'\s+[a-z]$', ' ', t) t = re.sub(r'^[a-z]\s+', ' ', t) t = re.sub(r'\s+', ' ', t) sentiment = clf.predict(vectorizer.transform([t]).toarray()) Finally, total positive and negative sentiments are calculated for total tweets fetched in runtime and the final sentiment of a particular stock has been calculated as positive = 1 (if the positive percentage > 65%), negative = -1 (if the positive percentage < 35%) or neutral = 0 (if the positive percentage is in between 35% and 65%). if (tot_positive+tot_negetive)>0 : positive_percentage = tot_positive/(tot_positive+tot_negetive) print(""positive_percentage :"",positive_percentage*100, ""%"") if positive_percentage>0.65 : print(""stock is buy"") return 1 elif positive_percentage<0.35 and positive_percentage>0: print(""stock is sell"") return -1 else: print(""stock is neutral"") return 0 Following end point has been built to provide twitter sentiment analysis result to a stock. Definition * Get Twitter sentiment for given stock - 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> e.g. http://localhost:5000/stock/sentiments/NASDAQ/AAPL Response * 200 OK on success json { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Sample run: Recommender System Approach The basic approach to the recommendation engine is to use a similarity function to compare pre-identified features of all stocks in our corpus against user-provided stock symbol features. While doing so, we are implementing one of the methods for building recommendation engine - Content based recommendation or Item similarity, which was part of Week 6 lecture of course CS 410 Text Information System. Recommender System recommends to the user the top 5 stock symbols which are most similar to the user provided stock symbol in terms of those underlying features and the similarity function. For the initial, Cosine similarity is used as similarity function and the following financial/economic features were selected as underlying features for similarity calculation. 1. company is in S&P 500 2. company profitability over last three years 3. revenue growth for last three years 4. current market analyst ratings 5. sector 6. gross profit per market cap Analyst ratings are calculated by rating_system and same is reused also for recommendation system. Other features for the stock are calculated or derived using the financial data of the companies from year 2018. Source: Kaggle dataset available at https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018. As part of the exercise, significant amount of effort was used to identify a single data source for financial or economic data and also to understand various financial terms to find the right feature. Based on our current understanding of a company performance and its relation to stock price, we used above listed features. The features might be further optimized with right guidance from a financial analyst or person with insight of stock market and factors influencing the investor decisions. Libraries and Datasets Python Libraries Flask flask_cors markdown beautifulsoup4 selenium pandas pymongo tinydb misaka nltk requests sklearn flask-jsonpify Datasets * https://www.marketbeat.com/ * https://www.cs.cornell.edu/people/pabo/movie-review-data/ * https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018?select=2018_Financial_Data.csv * https://developer.twitter.com Verification The result of the recommender system was verified based on manual verification process by selecting some stocks at random and checking the result for individual section e.g., ratings, sentiments and overall ratings. For example, Overall rating for Facebook (FB) stock is BUY. Looking at individual ratings component, the result seems reasonable. Analyst Ratings: BUY Verification: In the list of market analyst ratings, all 10 analysts have rated it as BUY. Therefore, the aggregated rating is BUY. Tweet Sentiments: Neutral Verification: From the latest 100 tweets fetched (as shown in below screenshot), it seems reasonable to have a NEUTRAL sentiment Overall Rating: BUY Verification: Combining Analyst Ratings ""Buy"" with Twitter Sentiments ""NEUTRAL"", as per listed decision table, the result is BUY. Conclusion and Future Work Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating (e.g., buy, hold, sell) for the user entered Stock symbol and recommending five similar stocks. The result has been validated by some popular stocks. One of key challenges faced in building the system is the access on right financial data as well as good training and testing set to train and test the model for sentiment analysis. The choice we had to manually create the required data. However, to optimize the time and resource available, it has been decided to use readily available data in the web which might have not yielded the perfect recommendation. There are several interesting directions for the future version of recommender system. First, the overall functionality can be improved by considering user input of sectors and providing recommendation and trends specific to that sector. A second direction involves defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market and use the same to train and test the model. Given the current trend of machine learning algorithms, a third interesting research direction is to explore the timeseries data for stock adjacency. Finally, overall score can be improved further to consider more analysts' reports and microblogging websites and come up with more recommender categories i.e., Strong Buy, Buy, Hold, Sell, Strong Sell etc. At the end, it has been a great journey of ideation and learning in a collaborative manner to design and develop the stock recommender system. We thank different analysts' websites to make their ratings available publicly and twitter to grant access of real time tweets through their public API and last but not the least, we thank professor ChengXiang Zhai and all our TAs and all the reviewers to give the direction needed and to provide the valuable feedback. References [1] Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL '04). Association for Computational Linguistics, USA, 271-es. DOI:https://doi.org/10.3115/1218955.1218990 [2] Carbone, N. (2020, January 18). 200+ Financial Indicators of US stocks (2014-2018). Retrieved December 11, 2020, from https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018 Project Status Report: Stock Recommender System[ ] [ ] [ ] [ ] [ ] [ Team MembersProject OverviewProject StatusProject ChallengesProposed resolutionNext ]StepsTeam MembersNameNetIDEmail IDEzra Schroederezras2ezras2@illinois.eduRasbihari Palpal9pal9@illinois.eduSandeep Kumarkumar64kumar64@illinois.eduTeam captain marked in BOLD.Project OverviewWe are building a product ""Stock recommender system"", which will enable retail investors to make informed decision about their investment choices, based on market analysts ratings and market/investors sentiments at the moment. Based on user's preference of a stock, the product recommends 5 stocks which closely matches the user's preferred stock and ratings for them.The recommender system combinesrating data from various market analyst, market sentiments using microblogging data (using twitter) and company profile for determination of the overall rating and curation of the recommendation list. Project StatusThe team has been making steadfast progress in various topics in order to deliver the project on time by the final project submission deadline. Progress on each individual topic can be seen in below actualized project planning chart.Percentage completion of the tasks under individual topics is mentioned in brackets () next to each topic name in above planning chart.Project ChallengesAt the conception of the project, we had foreseen some challenges but during the execution, we faced several challenges, which we had either not foreseen or assumed as minor challenges. Below, we list of the challenges which required significant time, effort and energy from all of us.Working with Twitter API and gathering training and test data for the model we prepared for sentiment analysisCollecting good quality financial data of various companiesScraping of market analysts websitesTechnical limitation to process the dataAbove challenges are in addition to a major hurdle related to working in a team with members sitting across Atlantic Ocean and having a time difference of 7 hours. Thanks to team members flexibility, team agility and technology, this challenge did not pose a big threat to our continuous progress.Proposed resolutionWe, as a team, invested significant amount of time working on above listed challenges. Some of the challenges we could overcome with sustained effort but for some of the challenges we had to find a middle ground to meet the project objective of delivering a 'reasonably' working solution and hoping to improvise the solution over time with larger effort.Working with Twitter API and gather training and test data for the classifierIt required significant effort and multiple round of communication with Twitter team to get access to the Twitter API - an important part of our proposed solution for performing the market sentiment analysis for a given stock. Eventually we received the access but with restriction on the volume of tweets we can pull in a month.For training and test data, we used public data available in internet. We used the following polarity data set:https://www.cs.cornell.edu/people/pabo/movie-review-data/This helped us to avoid the significant manual effort for downloading the twitter data and possibly human labeled them.Collecting good quality financial data of various companiesFor building a good recommendation engine, access to financial data of various companies is required. We could not find a single source with latest financial data for the listed companies. To avoid the trap of spending significant time and energy to resolve the issue, we eventually settled with a kaggle data source having financial data from 2018. But the solution is being designed in a way that we can update the data any time to reflect current reality.Scraping of market analysts websitesOur initial idea was to scrape websites and reports from various market analysts for their ratings to various stocks. After collating the information for couple of stocks, we realized the vastness of this topic. For each stock, we needed to scrape data from various market analysts and each report following free text structure. For our project, we needed access to the assigned rating from various market analysts and we settlement with an established website, which collects and publishes such ratings in structured format.Technical limitation to process the dataAt the moment we have collected financial data for more that 1800 companies. To scrape the analyst ratings and get twitter feed for all these companies and then processing them for final result would require significant time and processing infrastructure. As a realistic approach, we have decided to limit the scope of the project to a limited list of stocks. This limit is only for practical purpose only. However, the solution is capable to manage more volume of data with parallel and batch processing.Next StepsAt the moment, we do not see any major technical challenge in our way in coming days. We are planning to complete the remaining tasks from all topics and focus on final project delivery on time. CourseProject: CS410 Text Information System, UIUC Project Topic: Stock Recommender System Team Members |--|---------------|----------|---------------------| | # | Name | Net ID | Email Id | | --- | --------------- | ---------- | --------------------- | | 1. | Ezra Schroeder | ezras2 | ezras2@illinois.edu | | 2. | Rasbihari Pal | pal9 | pal9@illinois.edu | | 3. | Sandeep Kumar | kumar64 | kumar64@illinois.edu | | -- | --------------- | ---------- | --------------------- | Project Documentation Project Final Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectReport_StockRecommenderSystem.pdf Software usage tutorial presentation video https://mediaspace.illinois.edu/media/1_zbfjmw9g Project Presentation - Powerpoint https://github.com/MLwithSandy/CourseProject/blob/main/StockRecommendationEngine_V1.3.pptx Technical Set-up Guide https://github.com/MLwithSandy/CourseProject/blob/main/Technical_set-up_guide.pdf User Guide https://github.com/MLwithSandy/CourseProject/blob/main/UserGuide.pdf Project Status Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectStatusReport_StockRecommenderSystem.pdf Project Proposal https://github.com/MLwithSandy/CourseProject/blob/main/ProjectProposal_StockRecommenderSystem.pdf Stock Recommendation Engine - A consolidated recommender Bringing market within the reach of commons 1 PROBLEM Lack of consolidated recommender More retail investors due to availability and easiness of trading platform The retail investor of today lacks time for in-depth research Investors lacks necessary knowledge to analyze the financial standing of a company. Too many analysis/reports in the web Stream of information in modern era of social computing 12.08.2020 2 12.08.2020 3 SOLUTION One stop window to provide stock recommendation Enable retail investors easy access to information to make informed investment decision The recommender system combines stock rating data from various market analyst, market sentiments in social media (Twitter) company profile and financial attributes 12.08.2020 4 System Design PRODUCT Analyst reports ratings_system is the component which calculates the overall rating for the user selected(searched) stock, by scraping the required ratings data from market analyst website and aggregating them. Market Sentiments TwitterSentimentAnalysis component fetches latest tweets for the user selected(searched) stock from Twitter used Twitter API, performs sentiment analysis and return overall sentiment for the given stock. Financial attributes of the company recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected(searched) stock and a pre-defined similarity function. 12.08.2020 5 Three aspects of parameters 12.08.2020 6 Project Documents User Guide https://github.com/MLwithSandy/CourseProject/blob/main/UserGuide.pdf Technical Set-up Guide https://github.com/MLwithSandy/CourseProject/blob/main/Technical_set-up_guide.pdf Project Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectReport_StockRecommenderSystem.pdf 12.08.2020 7 Installation & Setup Install Docker on your local machine https://docs.docker.com/get-docker/ Download the project from github https://github.com/MLwithSandy/CourseProject Go to the folder SRE: CourseProject > ProjectCode > SRE To start the Stock Recommendation System, run the start scripts start.bat for windows start.sh for linux To stop, run the stop scripts stop.bat for windows stop.sh for linux 12.08.2020 8 How to Use Open browser, preferably Chrome or FireFox Go to http://localhost:8080/home to access Stock Recommendation System UI Enter any stock symbols e.g. AAPL (NASDAQ Market) in the search box and click ""Search"" Stock Recommendation Engine will show Analyst ratings Twitter Sentiment Ratings Overall ratings Most recent five tweets Similar stock recommendation Use http://localhost:5000/ to see list of Rest API end points the product 12.08.2020 9 Live Demo - Use Case Data Used - Acknowledgements 10 https://www.marketbeat.com/ https://www.cs.cornell.edu/people/pabo/movie-review-data/ https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018?select=2018_Financial_Data.csv https://developer.twitter.com OUR TEAM Sandeep Kumar Rasbihari Pal pal9@illinois.edu Ezra Schroeder ezras2@illinois.edu 12.08.2020 11 kumar64@illinois.edu Conclusion Future Work Considering user input of sectors and providing recommendation and trends specific to that sector Defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market Explore the timeseries data for stock adjacency. Combine more analyst report and microblogging sites 12.08.2020 12 Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating for the user entered Stock symbol and recommending five similar stocks. THANK YOU! 13 Technical set-up guide Stock Recommender System is packaged as docker images to ensure that the all its component can run on any machine, independent of OS e.g. Windows or Linux and without any need for special configuration and additional software dependencies. Listed below are the only two pre-requisites in order to set-up and run Stock Recommender System on your local machine. 1. Access to internet 2. Docker CLI or Docker Desktop installed on the local machine In case, you do not have Docker CLI or Docker Desktop installed on your local machine, you may install Docker Desktop from docker website: https://www.docker.com/products/docker-desktop Once above listed pre-requisites are fulfilled, you can proceed with following five steps in same sequence. Step 1: Download relevant scripts from project github Please download either the complete project or only the SRE directory from the GitHub repo: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode SRE folder consists of following files / scripts, which are relevant to set-up and run the Stock Recommendation System: You may follow readme.md at GitHub repo additional details. https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/SRE/readme.md Step 2: Run start script Depending on your operating system, please run start script in SRE directory as mentioned below: Windows: run start.bat in command prompt Mac OS, Linux: run start.sh in terminal The start script downloads docker images for Frontend and Backend components of Stock Recommender System from Docker Hub. Docker images are published by Stock Recommender System project team as a public repository. Size of docker images: sre-backend: 700 MB (approx.) sre-frontend: 18 MB (approx.) Depending on the speed of internet, it may take a while to download the two images. Once the two images are downloaded, the start script will run the docker images as docker container on your local machine. Step 3: Access the Stock Recommender System UI To access the recommender system UI, please open following link in your internet browser, preferable Chrome or Firefox. http://localhost:8080/home For details of the UI, please refer to User Guide. Step 4: Access the Stock Recommender System Rest APIs This is an optional step and required only when you are interested in using the Rest services. Please follow below link to access the list of Rest APIs exposed from sre backend. http://localhost:5000/ Step 5: Stop the Stock Recommender System components In order to stop the backend and frontend components of Stock Recommender System, please run stop script in SRE directory, corresponding to your operating system. User Guide Welcome to the user guide of our Stock Recommender System. Before you start, please ensure that following pre-requisites are fulfilled: 1. Stock recommender backend component is running either on a server or your local machine 2. Stock recommender frontend component is running either on a server or your local machine User Screen When you go to the URL of Stock recommender system (e.g. http://localhost:4200/home), you will see a page such as below. Stock recommender system shows you overall rating and recommendation for Apple Inc (Stock symbol: AAPL) by default. You may enter a stock of your choice in given search area and click on Search. Stock recommender system will provide you all necessary information about searched stock and also recommend stocks similar to the one you are interested in. Various sections of the screen are described below: Overall Rating Overall Rating consists of three parts. Y= Analyst Rating Analyst rating is calculated based on the ratings of various market analysts, who publishes such ratings e.g. buy, sell, hold etc. for various listed companies. The recommender system aggregates last 10 ratings from various analysts to calculated weighted analyst rating. Y= Twitter Sentiments Current market sentiment is determined based on the Twitter trends. Recommender system fetches recent tweets from Twitter and perform sentiment analysis to provide current market sentiment for the given stock. Y= Overall Rating Recommender system combines analyst rating and twitter sentiment to provide you with an overall ratings of the stock you are interested in. Based on the recent market trend, both the parameters have been given equal weightage for calculation of overall ratings. Please refer to below table to understand the calculation of overall ratings. Aggregate rating based on various market analysts Market Sentiment Overall rating Buy Neutral Buy Buy Negative Hold Buy Positive Buy Hold Neutral Hold Hold Negative Sell Hold Positive Buy Sell Neutral Sell Sell Negative Sell Sell Positive Hold Analyst ratings & latest Tweets Analyst ratings & latest Tweets consists of following three section. Y= Rating chart Rating chart shows the distribution of ratings (buy, sell, hold) among the market analysts based on recently published ratings Y= Analysts rating Analysts rating shows the details - ratings (buy, sell, hold), rating agency (market analyst) and date of publication of the rating Y= Tweets List of latest 5 tweets (indicative only). Market sentiment is calculated on many more tweets. Similar stocks Similar stocks show list of 5 stocks which are similar to the one you are interested in. Similar stocks are identified using a similarity function based on following parameters. 1. company is in S&P 500 2. company profitability over last three years 3. revenue growth for last three years 4. current market analyst ratings 5. sector 6. gross profit per market cap"
https://github.com/MM026184/CourseProject	"Michael McClanahan NetID: mjm31 CS410: Text Information Systems Project Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Michael McClanahan (NetID: mjm31) This project will be completed individually. Which paper have you chosen? Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Which programming language do you plan to use? Python (3.6.1 via Anaconda 4.4.0) Can you obtain the datasets used in the paper for evaluation? No. The dataset from the LDC requires a license for access: https://catalog.ldc.upenn.edu/LDC2008T19 If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? I could build a scraping engine to capture all of the text from the same articles here: https://spiderbites.nytimes.com/2000/ Ideally though, I would already have access to the data in XML file format from the LDC. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A CS 410 Course Project Reproducing paper: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Michael McClanahan Online MCSDS NetID: mjm31 Overview Project Objectives Installation and Use Example Results Discussion Project Objectives Implement the Iterative Topic Modeling framework with Time Series Feedback (ITMTF) outlined by Kim et al 2013: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Specifically, this is a Python3 implementation that attempts to use the LDA topic modeling implementation from Gensim Perform the paper's 2000 Presidential Election experiment Attempt to replicate results from Table 2 and Figure 2(b) The experiment for Figure 2(a) was not attempted because Gensim's LDA model implementation does not have a u parameter Installation and Use If neccessary, install Python3 Then install the following project dependencies using pip pandas - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html gensim - https://radimrehurek.com/gensim/ nltk - https://www.nltk.org/install.html scipy - https://www.scipy.org/install.html numpy - https://numpy.org/install/ Clone the project repository, making a local copy of all files. This is also the working directory for the code, all of which is implemented in ITMTFPresidential.py. All neccessary objects to reproduce my results are in this repository and automatically consumed by the script. Navigate to your local directory to which the project was cloned and run the script with the following command: python ITMTFPresidential.py Installation and Use Analyze resulting csv files causal_topic_words.csv - contains the top five words for causal topics identified during each IMTMF iteration for each of 5 different IMTMF runs , each with a different initial number of topics identified for LDA itmtf_stats.csv - contains neccessary average causality confidence and purity statistics for each iteration of the 5 different IMTMF runs Example Results Example Results Discussion In general poorer this implementation's results were poorer than what was noted in the paper Improvements in causality confidence and purity were not observed with more iterations Top words for causal topics seemed applicable, but not all that compelling or different from one another Poorer results are likely due to differences in this implementation: Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results. CS 410 Course Project Reproducing paper: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Michael McClanahan Online MCSDS NetID: mjm31 Overview Project Objectives Installation and Use Example Results Discussion Project Objectives Implement the Iterative Topic Modeling framework with Time Series Feedback (ITMTF) outlined by Kim et al 2013: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Specifically, this is a Python3 implementation that attempts to use the LDA topic modeling implementation from Gensim Perform the paper's 2000 Presidential Election experiment Attempt to replicate results from Table 2 and Figure 2(b) The experiment for Figure 2(a) was not attempted because Gensim's LDA model implementation does not have a u parameter Installation and Use If neccessary, install Python3 Then install the following project dependencies using pip pandas - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html gensim - https://radimrehurek.com/gensim/ nltk - https://www.nltk.org/install.html scipy - https://www.scipy.org/install.html numpy - https://numpy.org/install/ Clone the project repository, making a local copy of all files. This is also the working directory for the code, all of which is implemented in ITMTFPresidential.py. All neccessary objects to reproduce my results are in this repository and automatically consumed by the script. Navigate to your local directory to which the project was cloned and run the script with the following command: python ITMTFPresidential.py Installation and Use Analyze resulting csv files causal_topic_words.csv - contains the top five words for causal topics identified during each IMTMF iteration for each of 5 different IMTMF runs , each with a different initial number of topics identified for LDA itmtf_stats.csv - contains neccessary average causality confidence and purity statistics for each iteration of the 5 different IMTMF runs Example Results Example Results Discussion In general poorer this implementation's results were poorer than what was noted in the paper Improvements in causality confidence and purity were not observed with more iterations Top words for causal topics seemed applicable, but not all that compelling or different from one another Poorer results are likely due to differences in this implementation: Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results. Michael McClanahan CS 410 - Text Mining University of Illinois at Urbana Champaign (Online) MCS-DS NetID: mjm31 Final Project Progress Report Reproducing a paper Mining causal topics in text data: Iterative topic modeling with time series feedback. (Zhai et al 2013) Progress made thus far Read the paper (above) Obtained access to the New York Times Annotated Corpus from the Linguistic Data Consortium Downloaded the NYT annotated corpus Read through the overview document for the NYT corpus Remaining Tasks Loop through XML data files and create functions for text parser Parse XML data for the framework inputs: a time series dataset (list of timestamps) and a corpus consisting of a list of (Document,timestamp) tuples Select a topic modeling method (M) and implement using a standard library Implement the iterative topic modeling framework with time series feedback Train against the NYT dataset Implement the experiment in the paper and produce the same set of sample results Challenges Corpus size is significant (3.06 GB), but it should fit in memory on my laptop (which has 32 GB RAM). This will be a significant undertaking to perform myself, but I'm taking the final early to give myself more time to get this done. CS 410 Course Project - Michael McClanahan (mjm31) All project deliverables (source code, documentation, and presentation) were completed by me as I was not the member of a team. Reproduction of Paper Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et. al. 2013) Link to paper Overview of Implementation This project attempts to reproduce the 2020 Presidential Election experiment outlined in the paper above. All programming was done in Python 3.6 (specifically Anaconda distribution 4.4.0). A requirements.txt file is provided outlining all non-standard libraries utilized and their associated versions. pandas==1.1.5 gensim==3.8.0 nltk==3.4.4 scipy==1.5.4 numpy==1.19.4 All source code is contained within ITMTFPresidential.py as a series of functions. For convenience, the following objects have been serialized to files (specifically .pkl files) for easy re-use: president_norm_stock_ts : This is the non-text time series containing normalized presidential stock prices (May through October 2020 for the Bush-Gore 2000 presidential race. gore_bush_nyt_ts : This is the document time series containing NY Times articles from May through October 2020 mentioning either Bush or Gore. cleaned_doc_list: This the document collection to be analyzed (ie: a list of documents represented each as a list of tokens or words). gensim_dictionary: This is the gensim dictionary object to be analyzed (built from gore_bush_doc_list). gensim_corpus: This is the gensim corpus to be analyzed (ie: a list of documents represented each as a list of wordIDs and their counts in the document) word_impact_dict: This is a dictionary of corpus {wordID:(impact score,p-value)}. It represents the result of section 4.2.2's Word-level Causality analysis. At runtime, if the script's reload_data variable is set to False, the script will reload president_norm_stock_ts and gore_bush_nyt_ts from disk in O(1) time. If set to True, functions build_datasets() and parse_nyt_corpus_for_gore_bush() will get called and rebuild these datasets from a .csv file and the NYT corpus for XML documents, respectively. Since the NYT dataset was too large, it was not uploaded to this repository. Therefore, setting this variable to True is not recommended. Additionally, if the script's build_new_corpus variable is set to False, it will reload all of the other remaining objects from disk in O(1) time. If set to True, it will rebuild all of the other objects by rebuilding the collection, the gensim dictionary, and the gensim corpus (which is the object passed to the LdaModel() for its corpus parameter. It will then reperform the Word-level Causality Analysis from 4.2.2, storing the result per gensim dictionary word ID in a dictionary for quick lookup during ITMTF iterations. The following 4 parameters are then set and ITMTF iterations are started by calling the ITMTF() recursive function. min_significance_value = 0.8 min_topic_prob = 0.001 iterations = 5 number_of_topics = 10 causal_topics = ITMTF(gore_bush_gensim_corpus,gore_bush_gensim_dictionary,number_of_topics,number_of_topics,word_impact_dict,gore_bush_nyt_ts,president_norm_stock_ts,ts_tsID_map,min_significance_value,min_topic_prob,iterations) The ITMTF function will call itself for the number of iterations specified, each time building an LdaModel() object, passing in a 2D matrix (num_topics,num_unique_terms) matrix of re calculated prior topic word probability distributions into its eta parameter. To re-calculate the prior, causal topics are first extracted during the topic-level causality analysis (see section 4.2.1 in the paper), which is performed using a Pearson correlation against the reference Topic Stream and the presidential stock price time series. The topic-word probability prior is then calculated for top words in causal topics in the build_prior_matrix() function. This function makes use of the word_impact_dict object above for each causal topic. With each iteration, .csv files causal_topic_words.csv and itmtf_stats.csv in the working directory are appended with a list of signficant topics and their top 5 words as well as that iteration's average causality confidence and average purity, respectively. Goal The project aims to reproduce the paper's results documented in Table 2 and Figure 2(b). You will note that the u parameter from the paper is not defined prior to calling the ITMTF function above, primarily because it is not an parameter for the LdaModel() class provided by Gensim. It is for this reason that Figure 2(a) from the paper was not reproduced. How to Use Install the most recent versions of the above non-standard libraries using pip in a Python3 environment. Ex: pip install pandas Clone the repository, which contains the working directory and all dependent objects. Navigate to the working directory and run the script with python ITMTFPresidential.py Presentation Files Presentation slides Voiced Presentation - To listen, open the .pptx document in Powerpoint, then navigate to the Slide Show tab and hit the From Beginning button. The presentation should start from there. Voiced Presentation Video - If unable to listen directly in Powerpoint, you can view it as a video in the Illinois Media Space. Video Demo'ing Code Results In general this implementation's results were poorer than what was noted in the paper. Improvements in causality confidence and purity were not observed with more iterations. Top words for causal topics seemed applicable, but not all that compelling or different from one another. Poorer results are likely due to the following differences with this implementation: - Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). - Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). - Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results."
https://github.com/Madokami/CourseProject	CS410 Project Proposal In your project proposal, please answer the following questions: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. xuzhizs2@illinois.edu  (individual project) 2.Which competition do you plan to join? Text classification competition 3.If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I'm planning to learn about using tensor flow to build neural network classifiers. I've had some experience before doing image classification with OpenCV. 4.Which programming language do you plan to use? Python Text Classification Competition Project demo youtube link In case the youtube video is unavailable, the video can also be found in the github repo, named project_demo.wmv Environment setup Requires python3.8 and tensorflow already setup. One easy way is to directly use the tensorflow docker container which already has python and tensorflow installed: tensorflow/tensorflow:latest Running the code python3 train.py // this will generate the ML model used to make classifications python3 evaluate.py // evaluates the test dataset and output results How the training works The model trained is a RNN (recurrent neural network). For each line of input, the response and the entire context, are read and tokenized as a text vector. Specifically, a vocabulary is created from all the words observed, so each word could be represented using an integer between 0 and the maximum vocabulary size. Each tokenized line along with its corresponding tag are then fed into the model for training. Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? 1.I set up a docker container to train a tensorflow recurrent neural network for the text classification competition. So far the best tf1 I got is around 0.69, which is about 0.03 away from the baseline. 2.Need to work on improving the model to beat the baseline and create a video and documentation on running the model. 3.Improving the model is easy, and takes a lot of trial and error. CourseProject The topic of this course project is Text Classification Competition. Project demo video link: https://youtu.be/44ZIyAuVs78
https://github.com/NK10/CourseProject	"1) An overview of the function of the code (i.e., what it does and what it can be used for). As part of this project, I am predicting if a given text (tweet) be SARCASM and NOT_SARCASM based on the response text. This can be extend to other text based application like sentiment analysis where we can find if a text is sarcasam or not and based on that mark the text as positive or negative sentiment or retrain the model for sentiment analysis. If we have to repurpose the code, we do have to train it on the text data so that the model learn the underlying details for better prediction.*2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. The code is implemented in python and using various library like Pandas, Numpy, transformer, sklearn. Below is the snapshot of the versions of different packages*About Project*About data:*Importing libraries and loading data*Encoding the data to prepend and append the sentence with CLS and SEP token which is needed by BERT#padding data : To keep the fixed length for each row of the text data in train and test.#creating attention mask : Creating a attention mask for train and test data which is needed for the model#Converting data to torch tensor#Creating dataloader#Pre Processing of text data :To remove the noise from the text data.*Loading BertForSequenceClassification model : Loading pretrain BERT model.*Creating Scheduler : Needed for BERT*Training and validation of the model : Where I am training the train data and doing the testing on validation test*Prediction on testing data : Predicting the label for the test data.*Generating output file*References*I have trained the model on the Google Colab using the GPU option. Without the GPU the training was taking 15 hours -20 hours. With GPU I was able to trained the model in 5 min -10 mins. I have added the comments on the code file for better readability and explaining the steps. Below are the high levelsteps.*3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Go to 'https://colab.research.google.com/notebooks/intro.ipynb'*Click on file, upload notebook and sign in with google account ( if you don't have an account, if possible create one. If it's not possible, please run it on any machine (preferred with GPU if not then CPU) with has the packages mentioned above.**Browse the file 'Text Classification Competition.ipynb'*In order to run the code, we need an environment with the above packages. I would recommend to run it on Google Colab with GPU by following the below steps.*Once the 'Text Classification Competition.ipynb' is imported, please upload the test and train files by click on the arrow highlighted red below. If the arrow is not visible click on the folder icon on the left and then select the arrow:*Final ReportSaturday, December 12, 202012:31 AM CS 410 Text Analysis Page 1 https://margaretmz.medium.com/running-jupyter-notebook-with-colab-f4a29a9c7156#Please follow below link on how to import the file and run it on Google Colab GPU.*Once GPU is selected on Google Colab Runtime. click on Runtime on the menu and select ""Run all"" option as shown below**Same has been explained in the video attached on the github.4)Things I triedI have tried various method as mentioned below:i.I have done the preprocessing of the data like, removing stopwords, any character which is not alphanumeric, remove punctation. However with BERT only preprocessing I did was remove the following tokens ('@user','..','<url>').ii.I have tried various model, Navies Bayes (various variation like, MultinomialNB,ComplementNB,BernoulliNB), Logistic Regression, SVM, however with all of these I was not able to beat the baseline. I was revolving around .66 -.70. Out of all these BernoulliNB was able to provide better result.iii.I have tried Neural networks as well but still not able to beat the baseline. I was still revolving around .66 -.70iv.I have tried CNN with various filters and kernel but still not able to beat the baseline. I was still revolving around .66 -.70v.I have tried LSTM with various combinations like, Bidirectional, different units (64,128,256) and also regularization but still it revolve around .68 -.71.vi.In the end I have tried pretrained uncase base BERT model and was able to beat the baseline.vii.1)Smoothing parameter for Navies Bayes2)Tried TFIDF with various ngram_range3)Regularization L1 and L2 on deep neural network as well as other machine learning model.4)Used the word embedding 'glove.twitter.27B.100d.txt' as well as 'glove.twitter.27B.200d.txt' in neural network model.5)Tried adding dropouts, multilayer network in LSTM/BiDirectional LSTM.I have tried various hyperparameter tuning for these models as well like: CS 410 Text Analysis Page 2 As part of the final project, I have opted for the classification problem. Below is the status for my progress:I have done the preprocessing of the data like, removing stopwords, any character which is not alphanumeric, remove punctation.a.I have tried various model, Navies Bayes, Logistic Regression, SVM, however with all of these I was not able to beat the baseline. I am still revolving around .66 -.70b.I have tried Neural networks as well but still not able to beat the baseline. I am still revolving around .66 -.70c.I have tried CNN with various filters and kernel but still not able to beat the baseline. I am still revolving around .66 -.70d.I have tried LSTM with various combinations like, Bidirectional, different units (64,128,256) and also regularization but still it revolve around .68 -.71e.Progress made thus far:1)a.I am still working on improving my vocabulary as well as will try BERT to see if I can beat the baseline.Remaining tasks:2)a.I am still trying to find on how to improve the model F1 score to beat the baseline. I think improving the vocabulary will be helpful but not sure what other things should I try.Any challenges/issues being faced:3)Progress ReportSunday, November 29, 202010:45 AM CS 410 Text Analysis Page 1 CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team: 1) Nitin Kumar (UIN : 656280346). I am working as individual. Option selected : classification competition, Yes, i am prepared to learn state-of-the-art neural network classifiers Some of the Neural classifiers and deep learning frameworks that you may have heard of. LSTM, RNN,GRU Describe any relevant prior experience with such methods I have used some of these techniques for a proof of concept in one of my MOOC courses. Which programming language do you plan to use? I will be using the python and use Keras library and other libraries like Spacy, Gensim or NLTK CourseProject Following is the short description of the files: 1) Final Report.pdf : This contain the answers for the questions. 2) Project_Code_walk_Through.mp4 : This is the video that shows walk thorugh of the code 3) Text_Classifocation_Competition.ipynb : This is the code for the classifciation competition 4) answer.txt : This will contains the outcome of the prediction of 1800 records given as test data. 5) test.jsonl : This contains the 1800 test records 6) train.jsonl : This contains the 5000 train records. 7) Progress Report.pdf : This is old file which i submitted for progress report task. 8) Proposal.md : This is initial proposal document i submitted at the begining of the project."
https://github.com/NikhilDIL/CourseProject	"Team TextMiningMasters Members:  ndeena2, ajaw2, pwasal3, jiahuah2 Project:  Reproducing a Paper, Mining Casual Topics in Text Data 1)Progress made thus far: We have gathered and prepared the necessary data in order to perform one of the experiments stated in the paper. More specifically, we have parsed the NYT corpus and time series data from Iowa Electronic Markets 2000 Presidential Winner-Takes-All market using Python and stored the data into a data structure. 2)Remaining tasks: Now that we've gathered and prepared the data the next step is to implement the iterative topic modeling with time series feedback algorithm stated in the paper, and feed in the data that we have prepared into that algorithm. Then the next step is to perform some data visualization to understand our results. 3)Any challenges/issues being faced: Some parts of the algorithm are unclear to us. More specifically, steps 2 and 3. We are unsure on how to compute  sig (C, X, T) in step 2, and how to apply granger tests to each of the candidate topics for step 3. Project Presentation Team Members: Nikhil Deenamsetty (ndeena2) Peter Wasala (pwasal3) Angela Jaw(ajaw2) Jiahua He (jiahuah2) Step 1: Parsing Data For Time Series Prices data: We converted the data that we found online into a CSV file which we read using pandas. `` df = pd.read_csv(""timeSeriesPrices.csv"") print(df) `` Website link: https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 For the NYT_Corpus data: we loop through all the folders in order to reach the XML files and search for all the paragraphs that include the words ""Gore"" and ""Bush"" and use that to filter out the non-relevant documents. Libraries used: os, tarfile, pandas, xml.etree. Step 2: Applying Latent Dirichlet Analysis We get the topics by applying the topic modeling method (Latent Dirichlet Analysis) to the set of documents with time stamps, let's call this set D. **The original experiment used PLSA, but according to lecture, PLSA and LDA perform the same. We wanted to use LDA since the gensim library has an LDA function, and our PLSA implementations take a very long time and are possibly incorrect.** Step 3: Get Candidate Causal Topics with lags We use the Granger Tests to find topics with significant values greater than 1 - the output of the Granger Tests. We can get the set of candidate causal topics with lags, let's call this set CT. Step 4: Find the Most Significant Causal Words We apply the Granger Tests for each candidate topic in CT in order to find the most significant causal words. Once we find those values, we record them. Step 5: Define a prior on the Topic Model Parameters We need to separate the positive valued terms from the negative valued terms and ignore terms with values less than 10%. we can assign the prior probability proportions according to the significance levels. Step 6: Apply LDA to D We use the prior from step 5. We use the feedback signals to guide LDA to form topics that better correlate with the time series. Step 7: Repeat step 2 to step 5 We repeat steps 2 through 5 until the stopping criteria. Once we reach the stopping criteria, the process stops and the function outputs CT, which is the output causal topic list. Libraries we used in the project: For parsing, we use the libraries: os, tarfile, pandas, xml.etree. These libraries should already be included. For the Iterative Topic Modeling Framework with Time Series Feedback function, we use the libraries: gensim, nltk, re, pprint, spacy. To install them, the commands are listed as below: conda install gensim conda install nltk conda install nltk conda install re conda install pprint conda install -c conda-forge spacy THANK YOU SO MUCH! 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a.Nikhil Deenamsetty ndeena2 (captain) b.Peter John Wasala pwasal3 c.Jiahua He jiahuah2 d.Angela Carol Jaw ajaw2 2.Which paper have you chosen? a.Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 3.Which programming language do you plan to use a.Python 4.Can you obtain the datasets used in the paper for evaluation? a.Yes. 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? a.N/A 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? a.N/A *What is the function of the tool? *To analyze textual topics in conjunction with external time series variables. *Who will benefit from such a tool? *Anyone that needs to analyze text along with time series variables. For example, stock traders can benefit from this tool because this tool could help a stock trader decide what stocks to buy, hold, or trade since news articles can tell more information about the future of a stocks' value. *Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? *Yes, this kind of tool already exists. The difference between the similar tool that we found and ours is that the similar tool that we found displays geometric properties and our tool displays prior distributions on parameters. People would probably prefer our tool more since it provides feedback at each iteration. *What existing resources can you use? *https://catalog.ldc.upenn.edu/LDC2008T19  (Dataset) *https://spiderbites.nytimes.com/2000/  (NYT Articles Backup) *https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29  (Iowa Electronic Markets (IEM) Time Series Data) *https://finance.yahoo.com/quote/AAL/history?period1=946684800&period2=978220800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true (American Airlines Stock Data) *https://finance.yahoo.com/quote/AAPL/history?period1=946684800&period2=978220800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true (Apple Stock Data) *MeTA toolkit for topic modeling algorithms *What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) *PLSA or Latent Dirichlet Analysis (LDA) as Topic Model *Granger causality measure testing *How will you demonstrate the usefulness of your tool? *Give a presentation on how you could use this tool to help a user decide what stocks to buy, hold, or trade and show how news articles can influence stock prices. *A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) *Gather data and necessary resources to complete project (10/25) *Finish Iterative Topic Modeling Algorithm (11/15) *Create Documentation (11/22) *Create Presentation (11/29) CS 410 CourseProject Table of Contents 1. Project Video 2. An overview of the function of the code 3. Documentation of how the software is implemented 4. Documentation of the usage of the software 5. Brief description of the contribution of each team member 6. References Project Video Project Video 1. An overview of the function of the code (i.e., what it does and what it can be used for) We chose to reproduce the first experiement of the paper, Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback, which is where the authors of the paper examine the 2000 U.S. Presidential Election. We do this by: First, we parse through both the Time Series Prices data, which was from the Iowa Electronic Markets(IRM) Time Series Data, as well as the NYT_Corpus data. Then we start implementing the Iterative Topic Modeling Framework with Time Series Feedback as explained in the paper. Using this data we are able to implement a general text mining framework for discovering causal topics from text by combining the probabilistic topic model with time series causal analysis to discover topics that are semantically coherent as well as correlated with the time series data. By iterating through the data, we can refine the topics which increase the correlation of the topics with the time series. We can use this function in order to analyze textual topics in conjunction with external time series variables such as stocks What we implemented can be used to find the causal relationships between text data and non-text data, between media coverage and public opinion. Thus, our code can potentially be modified in order to identify target paragraphs for topics relating to not just the 2000 U.S. Presidential Election but instead can be used for other things such as measuring the relationship between the public's response to topics such as climate change, corona virus, as well as other issues. 2. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement For the Time Series Prices data, we converted the data that we found online on this site into a CSV file which we read using pandas. For the NYT_Corpus data, we loop through all the folders in order to reach the XML files and search for all the paragraphs that include the words ""Gore"" and ""Bush"" and use that to filter out the non-relevant documents. The Iterative Topic Modeling Framework with Time Series Feedback function can be broken down into the following six parts First, we get the topics by applying the topic modeling method (Latent Dirichlet Analysis) to the set of documents with time stamps, let's call this set D The original experiment used PLSA, but according to lecture, PLSA and LDA perform the same. We wanted to use LDA since the gensim library has an LDA function, and our PLSA implementations take a very long time and are possibly incorrect. Second, we use the Granger Tests to find topics with significant values greater than 1 - the output of the Granger Tests. Then we can get the set of candidate causal topics with lag, let's call this set CT. Third, we apply the Granger Tests for each candidate topic in CT in order to find the most significant causal words. Once we find those values, we record them. Fourth, we define a prior on the topic model parameters using significant terms and their values First, we need to separate the positive valued terms from the negative valued terms. We can ignore terms with values less than 10% Then, we can assign the prior probability proportions according to the significance levels Fifth, we use the prior from step 4 to apply LDA to D This is the part that uses the feedback signals and guides LDA to form topics that better correlate with the time series Sixth, we repeat steps two through five until the stopping criteria. Once we reach the stopping criteria, the process stops and the function outputs CT, which is the output causal topic list 3. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run the software, whichever is applicable This project was run using python version 3.8. Make sure you have access to Jupyter Notebook either by installing Jupyter by following the directions at this link or by installing Anaconda by following the directions at this link For parsing, we use the libraries: os, tarfile, xml.dom.minidom, pandas, xml.etree. These libraries should already be included in python. For the Iterative Topic Modeling Framework with Time Series Feedback function, we use the libraries: gensim, nltk, re, pprint, spacy. To use these libraries, you need to install them which you can do by doing the following commands in the command prompt (the one we used was the Anaconda Prompt): - conda install gensim - conda install nltk - conda install re - conda install pprint - conda install -c conda-forage spacy After installing all the libraries, once you launch the notebook, you should be able to run each cell in the notebook by pressting the ""Restart and Run All"" button or you can run each cell one at a time by pressing the ""Run"" button. 4. Brief description of the contribution of each team member in case of a multi-person team In general, we worked together as a group on a call and gave each other advice and helped when possible whether it be by looking at the current issue, clarify what the Iterative Topic Modeling Framework with Time Series Feedback function is doing, or Googling resources such as libraries to use. Down below is what we were tasked with, but we worked on things outside of what we were in charge of. Nikhil Deenamsetty (ndeena2) In charge of the Iterative Topic Modeling Framework with Time Series Feedback function parts 2 and 3 Peter Wasala (pwasal3) In charge of the Iterative Topic Modeling Framework with Time Series Feedback function parts 1, 4, 5 Angela Jaw (ajaw2) In charge of parsing data, wrote up the documentation, helped when asked, did tasks assigned to me Jiahua He (jiahuah2) In charge of parsing data, worked on the presentation, helped when asked, did tasks assigned to me 5. References Topic Modeling with Gensim (Python) Topic Distribution statsmodel documentation"
https://github.com/PSUlion16/CourseProject	"CS 410 Project Topics Overview We will use  Microsoft CMT  to manage course project grading. Each student should please create an account there using their illinois email ids. After deciding your topics, each student should please enter their details in  this sign-up sheet . Carefully enter the information used while registering in CMT into the first few columns (this information will be used for grading, so please be careful! ). Then, enter your group name (could be anything) and project topic. Only the group leader needs to enter the project topic. However, every student needs to enter all other details. This needs to be completed before the proposal submission, i.e. before Oct, 25 to facilitate grading. Multiple groups can choose the same topic.  Feel free to coordinate with other groups working on the same topic. For example, different groups can work on separate sub-tasks to increase the project-scope and overall contribution. For the course project topics, we provide five broad categories of options for you: 1.You can choose to  reproduce the model and results in a published paper . We provide some papers below. If you choose one of those papers, your project proposal is almost certain to get ""approved""*. 2.You can choose to  improve over a current system by adding a function that is relevant to this course . We provide some systems and candidate functions to add below. If you choose among those systems and functions, your project proposal is almost certain to get ""approved""*. 3.It is possible that you  work on other papers or systems  that are not listed by us. 4.You can choose to  join a text classification competition, or an information retrieval competition . If you choose this option, your project proposal is almost certain to get ""approved""*. 5.You can  freely propose a topic relevant to this course . * For all the categories, the instructors will carefully review your project proposals and provide feedback. If we find your project topic/plan has some limitations, we will provide suggestions to improve it or suggest you to pick one of the sample topics. You're allowed to change topics after the proposal stage based on our feedback. More detailed information about each option is given below. Option 1: Reproducing A Paper You can choose to reproduce one of the following papers from one of the following subtopics: *Subtopic: Latent aspect rating analysis *Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 *Subtopic: Pattern annotation *Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 *Subtopic: Contextual text mining *ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 *Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture model for contextual text mining. In Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 649-655. DOI=10.1145/1150402.1150482 *Qiaozhu Mei, Chao Liu, Hang Su, and ChengXiang Zhai. 2006. A probabilistic approach to spatiotemporal theme pattern mining on weblogs. In Proceedings of the 15th international conference on World Wide Web (WWW 2006). ACM, New York, NY, USA, 533-542. DOI=10.1145/1135777.1135857 *Subtopic: Causal topic modeling *Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 All these papers are discussed in the lectures of Week 12. Once you have chosen one of these papers, please provide clear answers to the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.Which paper have you chosen? 3.Which programming language do you plan to use? 4.Can you obtain the datasets used in the paper for evaluation? 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? At the final stage of your project, you need to deliver the following: *Your documented source code and main results. *A demo that shows your code can actually run on the test dataset and generate the desired results. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. *Discuss how your results match or mismatch those reported in the original paper. Your results should cover all the main aspects and datasets discussed in the paper. *If some of your results do not match the paper, discuss possible reasons and solutions. Option 2: Improving A System You may choose to improve a system or service. We provide some candidates below. Depending on your group size and the complexity of the techniques you develop, you may choose to work on one or several sub-topics provided per system.  Again, it should take you at least 20*N hours, where N is the total number of students in your team. 2.1 MeTA Toolkit By now, you should all be familiar with the  MeTA toolkit  and its Python library  Metapy . You can also refer to the  publication . Choose this option if you wish to contribute to it as stated below. *Enhance MeTA and Metapy usability Many of you have experienced difficulties while using Metapy in assignments this semester. You now have a chance to improve it so that future students and researchers can use this useful resource easily! Some ideas for improvement are given below: *Make Metapy compatible with the latest Python versions and different OS systems *Integrate it with existing popular toolkits e.g. NLTK, gensim *Enhance available tutorials for installing and using the tool on different platforms *Add text mining functions to the MeTA toolkit The aim of this subtopic is to add some existing text mining algorithms to MeTA. Note that the papers mentioned below are all discussed in the lectures of Week 12 but cannot go to Option 1 because there are online GitHub repositories that implement them. If you want to borrow some code snippets from some repositories, make sure the licenses of those repositories allow you to do so, and you should follow the instructions in the licenses. If a repository does not have a license file, according to  GitHub , the default copyright laws apply, meaning that the authors retain all rights to their source code and no one may reproduce, distribute, or create derivative works from their work. That means you CANNOT use codes from GitHub repositories without a proper license unless you obtain explicit written permission from the authors. *Latent aspect rating analysis, given by the following paper #Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903 *Topic modeling with network regularization, given by the following paper #Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengXiang Zhai. 2008. Topic modeling with network regularization. In Proceedings of the 17th international conference on World Wide Web (WWW 2008). ACM, New York, NY, USA, 101-110. DOI=10.1145/1367497.1367512 2.2 ExpertSearch System TheExpertSearchsystem( http://timan102.cs.illinois.edu/expertsearch// )wasdevelopedby somepreviousCS410studentsaspartoftheircourseproject!Thesystemaimstofindfaculty specializinginthegivenresearchareas.Theunderlyingdataandrankercurrentlycomesfrom theMP2submissionsofthepreviouscourseoffering.Youcanreadmoreaboutit here (Sections3.6and4:Projectareespeciallyrelevant).Thecodeisavailable here .Beloware someideastoimproveandexpandthissystem.Youmaychoosetointegrateyourcodewiththe existingsystem,orborrowsomeideasfromit,orbuildyourownsystems/algorithmsfrom scratch. *Automatically crawling faculty webpages Recall that you developed scrapers for faculty web-pages in MP2.1, which, in general, can be a time-consuming task. So, the question is can we automate this process? Some challenges include: *Identifying faculty directory pages:  First, we need to identify the pages from where faculty web-pages can be mined. In MP2.1, we used faculty directory pages as the starting point to find faculty webpages. So, given a university website, can we automatically identify the directory pages? This can be posed as a classification task, i.e. classify a URL into a directory page vs. non-directory page. We have a huge resource of directory page URLs available in the  sign-up sheet . These can be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs(e.g. other URLs on the university websites, product websites, news sites,etc.). These would be the ""negative"" examples. *Identifying faculty webpage URLs:  Next, we need to extract the faculty webpages from the directory pages. This can again be posed as a classification task. Given a URL, can we identify whether it is a faculty webpage or not? We have a huge resource of faculty webpage URLs (available under MP2.3 on Coursera). These would be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs (e.g. other URLs on the university websites, product websites, news sites, etc.) to get the ""negative"" labels. *Extracting relevant information from faculty bios: The problem here is to convert the unstructured text in faculty webpages into more  structured text . Such structured information would enhance the utility of the system. For example, in the ExpertSearch system, emails and faculty names extracted from bios are shown in the search results. Users can click on the ""mail"" button to directly mail the faculty. Extraction is done using regex-based techniques and Named Entity Recognition (NER) that don't always work well. Can you improve those existing techniques? You can also develop techniques for extracting other information, e.g. faculty research interests. For example, you may perform  topic mining  on the bios available under MP2.3 on Coursera. The top-keywords per topic could be the common research areas. You might also perform  keyword extraction  from faculty bios, research papers, etc. 2.3 EducationalWeb System The EducationalWeb system ( http://timan102.cs.illinois.edu/explanation//slide/cs-410/0 ) is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and recommend relevant slides for each slide. You can read more about this in the following papers  Web of Slides ,  WOS Demo .; 2) Find an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""cap/scholar"" button on the top-right of a slide. It will try to retrieve a relevant section from the Professor's textbook that contains an explanation of the selected phrase. You can read more about the underlying algorithm  here . The code for the system is available  here . Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms. *Improving the usability and reach of the existing system Some of you might have used the system and identified potential areas of improvement. The aim of this subtopic is to refine the current version of EducationWeb. Some specific ideas include (many are borrowed from  this Piazza post ): 1.  Scale up the current system . Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. Again, it might be useful to think about automatic crawling similar to the subtopic in 2.2 above. It would be very interesting to see the interaction between slides/textbooks at a large scale!! 2.  Improve the performance of the system . Currently, loading each slide takes time. 3.  Allow downloading slides in bulk.  Currently, we can only download one slide at a time. 4.  Add more context to the explanations  (e.g. link to the specific page in the textbook) 5.  Allow adding additional courses/lectures directly from the web interface . This would also involve dynamically identifying the recommended/relevant slides for a new slide. Currently, a static file is used which contains pre-computed recommendations for each slide. 6.  Integrate the tool with Piazza/Coursera , i.e. maybe link Piazza/Coursera to the tool or vice-versa. Alternatively, add discussion forum and video capabilities to the tool so that it serves as a one-stop-shop for all users' educational needs. 7.  Link to latest related research articles : In this way, the lecture content can be automatically updated 8. You could also work on  improving  the current recommendation, search and explanation mining  algorithms  (described in the papers at the beginning of this section 2.3) *Automatically creating teaching material for in-demand skills This subtopic is an extended version of the existing EducationWeb system. There is an increasing demand for skilled workers in the industry. Quality education is not easily accessible to everyone due to barriers such as high cost, geographical and language barriers, etc. Also, instructors cannot be available 24*7 to provide personalized support to all learners. In this subtopic, the overarching aim is to tackle some of these issues. In particular, the following tasks might be good starting points. *Identifying in-demand skills:  You can crawl and analyze relevant sections of job boards, news articles, scientific articles, social media, etc. to automatically identify the  emerging keywords /topics. For this, you may refer to some papers on contextual text mining (mentioned in Option 1 of this document). *Creating lectures and tutorials for those skills:  For this, you may consider lecture slides (e.g. from Coursera courses) as the basic units of knowledge. Then, the task could be to find the most relevant slides or clusters of slides (could be across multiple courses/lectures) for a given skill (topic). You may borrow some ideas from the EducationWeb system for this. You may also use the slides in existing lectures on some topics as the ""relevant slides"" for those topics. In this way, you can automatically generate training data for supervised learning. You could also combine knowledge from multiple sources (e.g. textbook sections, slides, videos, blogs, codebases) for creating more comprehensive tutorials. A more challenging task would be to automatically  generate  the lectures/tutorials using techniques from natural language generation and abstractive text summarization. Another interesting idea is to automatically  generate agents , e.g. using Virtual Agent Interaction Framework (VAIF). This goes beyond the material covered in class but could lead to some highly innovative and state-of-the-art projects! If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What system have you chosen? Which subtopic(s) under the system? 3.Briefly describe the datasets, algorithms or techniques you plan to use 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 6.Which programming language do you plan to use? 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code. *A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Option 3: Working on Other Papers or Systems It is also possible that you work on other papers or systems that are not listed by us. Because we do not list them, we will need more information from your proposal to decide whether you have a good topic. You may find more guidelines below. 3.1 Reproducing an unlisted paper If you choose to reproduce a paper not listed under Option 1, please make sure that your chosen paper satisfies the following criteria: 1.The paper should solve one of the research challenges introduced in lectures. 2.The paper should be published at a trustable venue (i.e. conference or journal). Some examples include ACM SIGIR, KDD, EMNLP, ACL, Learning @ Scale, EDM, etc. If you can find Cheng's paper(s) at some venue, then that venue is likely to be trustable. 3.There should be NO publicly available implementation for this paper. For example, if the main method in a paper is already released on GitHub or built into a library, you cannot choose that paper. 4.The main method in the paper should be advanced enough so that the work of reproducing it likely takes at least 20*N hours, where N is the total number of students in your team. You may justify this by listing the main tasks to be completed and the estimated time cost for each task. In your proposal, please explain how your chosen paper satisfies the above criteria, and also answer the questions listed under Option 1. Other requirements are the same as Option 1. 3.2 Improving over a paper You can choose to improve over a paper that is relevant to one of the tasks introduced in the lectures. If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What paper have you chosen? 3.What is your idea for improving the paper/system? Why do you think your idea will hopefully work better? 4.How are you going to evaluate your idea? What are the datasets and baseline methods? 5.Which programming language do you plan to use? 6.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code and main results. *A demo that shows your code can actually run on the test dataset and generate the desired results. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. *If your idea works, discuss what advantages it has over the original paper, and what possible limitations are still there in your method. If your idea does not work, don't worry - just discuss possible causes and potential solutions, and you will get your credits as long as your study is solid and your discussion is thorough. 3.3 Adding an unlisted function to a listed system You can choose to improve one of the systems listed under Option 2 by adding a function that is not listed there but relevant to the course content. If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What system have you chosen? What function are you adding? How will the new function benefit the users? 3.How will you demonstrate that the new function works as expected? 4.How will your code communicate with or utilize the system? 5.Which programming language do you plan to use? 6.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code. Explain how your code communicates with or utilize the system. *A demo that shows your implementation actually works. 3.4 Improving over an unlisted system You can choose to improve a system or service that is relevant to the course content but not listed under Option 2. You may either add a new function to the system or improve the performance of an existing function. If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What system have you chosen? Are you adding a function or improving a function? What function? 3.If you are adding a function, why is the new function important or interesting? How will it benefit the users? If you are improving a function, what are the main limitations of the current function? How are you going to improve it? How will your improvements benefit the users? 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5.How will your code communicate with or utilize the system? 6.Which programming language do you plan to use? 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code. Explain how your code communicates with or utilize the system. *A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Option 4: Competitions This option may fit you well if you would like to gain some experience in state-of-the-art text classification or information retrieval practices . You will need to research by yourselves some cutting-edge models that are more recent than those introduced in the lectures. Of course your TAs will be there for you when you need help. Performance is the most important factor. Once you achieve the state-of-the-art performance, we give you a bonus (10% extra credit) that can be used to cover any loss of points in the project caused by small mistakes. We hope you can have fun in learning and trying recent methods in this option. We are thinking of hosting an  information retrieval (IR) competition  and a  text classification competition . You will have access to the text and labels of a training dataset and the text of the test dataset, but you cannot see the labels of the test set. You will then develop a text classifier or a document ranker and submit your test set predictions. We will automatically evaluate your results on the test dataset and release the test set performance of all participating teams on a leaderboard (the setup would be similar to MP2.4). The text classification competition is available  here . It is also available on LiveDataLab along with the baseline scores. The IR competition is available  here . It is also available on LiveDataLab along with the baseline scores. You're allowed to use pre-built machine learning packages. However, if you find someone's solutions (source code) to similar competitions online, you may not use them directly. You're free to borrow some of their ideas with proper citations and credit attribution. You can also use publicly available external datasets but please make sure they don't overlap with the test sets. In each competition, you will compete with a competitive baseline and your classmates. Your grade will largely depend on your test set performance. Recall that in your project grading, there is  45% on ""source code submission"" . (You can find the project grade composition in Week 1.) Assuming that there is no issue with your submitted source code, that 45% will be graded with the following criteria based on your test set performance on the leaderboard: *If you outperform the baseline, you get all 45%, plus 10% extra credit  to make it up for you if you lose points in other parts of the project. The extra credit will not make you earn more than 100% for your project, and cannot be applied to other parts of this course. *If you do not outperform the baseline but make a valid submission, you get 15% + 30% * (1 - (r-1)/N), where r is your rank on the leaderboard and N is the total number of teams that have chosen this option. If there is an issue with your source code (e.g. using others' code without properly handled copyright, obvious bugs, etc.), it is possible that you get a lower score than those listed above. In your project proposal, please answer the following questions: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.Which competition do you plan to join? 3.If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods 4.Which programming language do you plan to use? At the final stage of your project, you need to deliver the following: *Your documented source code and test set predictions. *Explain your model, and how you perform the training. Describe your experiments with other methods that you may have tried and any hyperparameter tuning. *A demo that shows your code can actually run on the test set and generate your submitted predictions. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. Option 5: Free Topics You may freely propose a topic that is not listed in this document but relevant to this course. In your proposal, please answer the following questions: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? 3.Which programming language do you plan to use? 4.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code and main results. *Self-evaluation. Have you completed what you have planned? Have you got the expected outcome? If not, discuss why. *A demo that shows your code can actually run and generate the desired results. If there is a training process involved, you don't need to show that process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. CS410 Project Proposal Name: Chris Toombs NetID: ctoombs2@illinois.edu Names and NetIDs of Members: For the scope of this project, I will be completing this text classification competition individually. My name and NetID is listed at the top of this page for reference. Intended Competition: I intend to join the text classification competition, which is based around a sarcasm detector based on twitter posts. Details of project: For the scope of this competition, I intend to complete my code using Python, as I mainly focus on Python at my current workplace and due purely to the ease of coding with Python. I am very interested in this competition as there are direct parallels with my job at General Motors in the field of text classification. In my current role, we have a need for classification of Vehicle Repair Verbatems, so that we can more easily determine root cause in an automated fashion. This will be a related project which I can use in my day-to-day job. I am excited to use Neural Network classification techniques, and will be focusing mainly on Word2vec and Glove (although I am not sure which I will use yet). A few of my coworkers at work have used these, and python has a few libraries created to leverage, such as gensim. I will make it clear which approach I will take in my project documentation. I do not have prior experience with these frameworks however there is a robust set of documentation online which I can leverage. CS410 Project Status Report Name: Chris Toombs NetID: ctoombs2@illinois.edu Current Progress: For this project, I am working on the twitter sarcasm detector as part of the text classification project. I have successfully created a python project with git integration to my project repository. I have verified that the repository connects successfully to Live Data Lab. In terms of implementation of the project, I was going to use word2vec, but I am looking at utilizing gensim's doc2vec along with scikit-learn for classification. The reason for this is that doc2vec will allow me to retain semantics, whereas word2vec does not. I have successfully loaded in both the train and test data sets in my programs and have preprocessed the words using the NLTK package and it's associated stop words. I am not going to perform stemming at this time, but I am adding some custom stop words (such as @USER) to clean up the documents (i.e. tweets) Remaining Tasks: I will need to make documentation on how to run the program, as well as fully comment my code. To be completed, I need to instantiate my documents with word2vec and implement the logistic classifier. I also want to clean up my pre-processing step, so I will be working on that in the next couple of days. Challenges: I've never used Gensim before, so I will need to read up on that, but there seems to be a lot of content online regarding. I will be getting married this week, so I will not be completing this project (most likely) until next week, so I am going to try and get as much work done as possible from 11/29-12/1. So far, nothing blocking me from finishing this on time. CTOOMBS CS410 Course Project Documentation Description of this repository: answer.txt - Classified answers based off the test.jsonl document provided to team classify.py - final code for the Doc2Vec Twitter Sarcasm model CS410_Classification_Demo LINK!!! - 20 Minute Demonstration of project, code, issues - https://mediaspace.illinois.edu/media/t/1_fltka8gr project_proposal_ctoombs2.pdf - proposal for project project_status_report_chris_toombs.pdf - Status report from November in regards to project progress README.md - Full description and documentation for code TwitterSarcasmModel.d2v - Doc2Vec model from successful submission of project To Run the classifier: FROM PYCHARM: - You can load in the Project Repo AS-IS and run the code directly without modification - Most environments will already have NLTK and GENSIM included. If you get an error, just import those libraries to your environment FROM CLI: - Navigate to local folder for course project repo - type classifier.py NOTE: THIS ASSUMES YOU HAVE PYTHON INSTALLED ON YOUR COMPUTER Description of Code My classify utilizes both GENSIM and NLTK to classify the test data into SARCASTIC or NON_SARCASTIC tweets. I was able to use the TwitterTokenizer to parse the REPONSE fields of the data into Tokenized lists, with Handles removed, words moved to lowercase, and repeated characters shortened to one. A brief description of the code methods: read_inputs() -- Reads in both TRAIN and TEST Files and tokenizes into lists initialize_doc2vec() -- Initializes the DOC2VEC Model based off the TaggedDocument objects in the TRAIN list get_labels() -- Determines the labels for the TEST dataset based off the DOC2VEC model saved in initialize_doc2vec() This method uses the INFER_VECTOR method of doc2vec to vectorize the test tweets output_results() -- Outputs the data to answer.txt Difficulties The most difficult part of this project was the hyperparameter tuning. Much of the parameters had vague documentation and it seemed like use cases for parameter values varied greatlywith different users online. I probably spent 10-12 hours or so toying around with the parameters adding / removing items and observing behavior before landing on the set that worked for me. In the future, it may be more useful to find a more recent model with a larger user base, as it seems doc2vec is not as widely used (the git repo has not had a commit since 2018). I was confortable with the implementation of this as I have used word2vec in previous projects. References I saw a ton of forums for where people were discussing use cases, but the main sources I pulled from were the following: stackoverflow.com https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/doc2vec.py https://www.nltk.org/ https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
https://github.com/Parkkeo1/CourseProject	"CS 410 FA20 - Project Progress Report 2.2 ExpertSearch System -  Extracting relevant information from faculty bios Keon Park,  keonp2@illinois.edu Background / Overview My project is a system based on  ExpertSearch  that, when finished, will automatically scrape and parse UIUC faculty pages from user-provided URLs to extract key information about faculty members. My original proposal is available here:  https://github.com/Parkkeo1/CourseProject . Progress So Far My work on the project so far has focused on text data retrieval, processing, and some keyword extraction and topic mining. My code is still in a WIP Jupyter Notebook and has not yet been integrated into a full application. To get started, I am currently using my dataset of UIUC CHBE faculty text data from my MP2.1. Using this dataset, I have built a web scraper that scrapes text data from HTML tags whose CSS classes match common faculty profile sections (i.e. scrape only relevant content from the page). Because faculty pages for a given department have the same or very similar format, I have implemented the ability to customize the web scraper's list of key CSS classes to match for each department at UIUC (currently only have one, for CHBE). Using the basic CHBE faculty data, I have also developed working implementations of the following three key components in my application: 1.Tokenizer, using spaCy, with stop words + punctuation filtering and lemmatization. 2.TF-IDF weighting, using scikit-learn; the trained vocabulary/index is able to be saved and loaded for use on new faculty data. 3.LDA, using scikit-learn, the topic distributions are calculated using the training data, and the topic coverage for a new doc can be calculated too. With these parts working, I am able to scrape relevant text data from a faculty member page URL not in the training data, tokenize/clean it, and add it to the existing TF-IDF index/vocabulary to calculate its top weighted keywords and its topic coverage using LDA. Remaining Tasks 1.Refactor the existing components: scraper, tokenizer, TF-IDF, and LDA to work in a smooth pipeline for both training data and new user-input data such that: a.Refine the scraper's ability to match relevant HTML tags for more different formats of faculty pages than just CHBE's. b.For new faculty's text data, use spaCy to find named entities and search for email and phone number tokens during the tokenization step, 2.Test out the above functionality with a much bigger dataset (UIUC faculty bios from MP2.3) and refine/fix things as needed (the final application's TF-IDF weights should be based on this as it should be more comprehensive over more different departments. 3.Build out final backend and frontend to package work into a full application. Challenges The main challenge I foresee in finishing this project is the frontend/UI and how I should approach designing and implementing the data visualizations for the faculty text mining results. Given that I have already put in a lot of focus and effort on the text retrieval and mining components of the project, I may have settle for more basic data visualizations in the frontend than I originally planned, in order to keep my total project development time reasonable. CS 410 FA20 - Project Proposal 2.2 ExpertSearch System -  Extracting relevant information from faculty bios Keon Park,  keonp2@illinois.edu  (Individual; Captain) Overview For my project, I plan to build upon ExpertSearch's extraction features and develop a new system that, given a faculty webpage's URL, extracts not only names and emails but also other relevant information such as research and instruction areas by implementing keyword extraction on the text data. These extracted keywords will then function as ""tags"" that indicate faculty members' top topics/research areas. Once finished, these features could be integrated into ExpertSearch to enhance its existing extraction utility. However, for now, my project will be a standalone system that will serve as a prototype for the new keyword extraction functionality. Impact My system will tackle the problem of converting unstructured text into structured data by automatically and quickly summarizing faculty text data into topics/keywords for users to more easily understand. This will be better than what ExpertSearch currently does; apart from the faculty name/email extraction, it has users manually visit and read the faculty webpages if they desire to know more information, which may be burdensome as faculty webpages are often varied in format and content. Thus, my system will allow users to more easily learn about faculty members with less manual effort. To demonstrate my system's usefulness, I will compare its results (containing structured information and topics) with ExpertSearch's results (basic word matches) for a given set of UIUC faculty. Architecture My new system will be implemented as a web application, with a React.js frontend and Python-Flask backend. Through the frontend, the user will be able to enter a URL of a faculty webpage, which the backend will then scrape and extract information from. Although my system will be separate from the existing ExpertSearch system, I plan to use its faculty name/email extraction code as a starting point/reference for my own implementation. Data / Techniques I plan to mainly use UIUC faculty and their webpages during development to test my system. For scraping text from faculty webpages, I will use my work from MP2.1 as a reference. To help implement my extraction techniques, I plan to use existing libraries such as NLTK and TextBlob. Key Tasks + Timeline 1.Build web scraper with ability to detect and extract basic information from faculty profile sections (name, title, research, bio, contact, etc), using NER and HTML tags - 5 hours 2.Implement keyword extraction to tag faculty to relevant topics (research areas, areas of expertise, etc) - 4 hours 3.Test and refine web scraper and extraction functionality - 2 hours 4.Incorporate code from steps 1-3 into Flask backend - 2 hours 5.Implement frontend for faculty webpage URL queries and displaying results - 6 hours 6.Integrate backend with frontend and finalize system - 3 hours Keon Park - CS 410 CourseProject Extracting relevant information from faculty bios (2.2) This repository contains the source code, data, and documentation for Keon (Isaac) Park's final project for CS 410 Fall 2020 at UIUC. This project is an extension/spin-off of the ExpertSearch system that seeks to build upon ExpertSearch's NLP features by extracting not only names and emails from faculty pages but also keywords, named entities, and topics in order to provide users with a more comprehensive overview without having to manually visit the page. This project was an individual effort by Keon (Isaac) Park. Software Usage Tutorial Video The link to the tutorial video for this project is here. The video is a brief explanation of how to locally install/run and use the project code, including a example use case. The rest of this README document also provides details regarding this software's functionality and implementation. Overview As previously mentioned, this project is a standalone extension of the ExpertSearch system that provides improved NLP features to better analyze text in faculty web pages. In its current implementation, ExpertSearch only extracts the name and email of the faculty member from the page, limiting its use as a tool for users seeking more in-depth overviews of faculty members and their biographies. As a result, this project was developed with more advanced text retrieval and mining features to automatically provide users with a useful ""snapshot"" of faculty page content. It accomplishes this by scraping, processing, and analyzing text data from faculty pages via URLs entered by the user using BeautifulSoup, spaCy, scikit-learn, and pre-trained TF-IDF and LDA models (available as .pkl files in the server/data directory) to automatically extract/calculate relevant keywords, named entities, and topics. For example, here is a screenshot of the system's results for UIUC Professor Tarek Abdelzaher: As shown above, this software, like ExpertSearch includes the likely name and emails of the faculty member. It also provides detailed overviews of the system's calculated keywords, named entities, and topics with specific numbers to provide users with an effective ""snapshot"" of the page without visting it manually. For example, the user can deduce that Professor Abdelzaher is likely to be an engineering (likely CS or CompE, due to keywords like system, transactions, and data) research professor involved/related with the Institute of Electrical and Electronics Engineers (IEEE). Pipeline The current pipeline of this sytem for retrieving, processing, and analyzing text data via user-provided URLs is as follows: User enters a URL of a faculty web page Web-scrape the URL and retrieve HTML source of the page Retrieve text from ""relevant"" HTML tags containing useful information (match CSS classes of HTML tags; nlp/html_parser.py) Perform NER to find names/organizations and find emails/links from text data using spaCy Tokenize text data into unigrams using spaCy, keeping only alphabetic, non-stopword, noun/verb/adj tokens Use tokenized documents for pre-trained scikit-learn TF-IDF vectorizer (trained on ~600 UIUC faculty bios) to find keywords Use TF-IDF weights of document to calculate its topic coverage using pre-trained scikit-learn LDA model and topic distributions Compile results into dataframes, which are then converted into html and rendered in the results page. The code for this pipeline can be roughly traced in the query() route handler function in app.py. Implementation This project/system is implemented as a Flask web application with a Python backend and HTML/CSS frontend. The code is organized like a standard Flask application, in the following directory structure (not all files shown): CourseProject/ | README.md | .gitignore | ... +---server/ | app.py | train.py | config.py | requirements.txt +---data/ | tfidf.pkl | uiuc_bios.txt | ... +---nlp/ | scraper.py | html_parser.py | tfidf_lda.py | ... +---static/ | main.css +---templates/ base.html ... app.py houses the Python code directly responsible for running the Flask web application, including route handlers for the home and results pages. When the user enters and submits a URL through the form in the home page, the Flask app redirects the user to the /query endpoint with the provided URL to invoke the corresponding route handler query() that is responsible for running the text retrieval and mining pipeline of the application. Thus, most future changes/tweaks to this system's NLP pipeline will originate in this function (query()). train.py contains the Python code necessary to train and save TF-IDF and LDA models using the UIUC faculty bios text data in the data directory; the model files generated by train.py are used by the Flask application. config.py contains all of the constants used by both the Flask application and train.py as configuration for various parameters during text retrieval and mining. For example, this file can be customized to tweak max_df and min_df used by the TF-IDF vectorizer. The nlp directory contains all of the NLP-related code of the system, divided into individual modules/files that is used to perform scraping, tokenization, NER, keyword extraction, and topic mining. The functions in this package implement and use various NLP and data libraries, such as: BeautifulSoup (for web-scraping), spaCy (for tokenization and NER), scikit-learn (for TF-IDF and LDA), and pandas (for compiling results into dataframes). Thus, most future changes to the specific NLP techniques used by this project will originate in this package and its specific modules. The data directory contains the trained TF-IDF vectorizer and LDA models as pickle files that are loaded in by the Flask application when calculating TF-IDF weights of keywords and topic coverages for the faculty page provided by the user. This directory also contains text files of UIUC faculty bios and urls compiled from the MP2.3 dataset; these data files are used by train.py to generate the .pkl files for the trained models. The static and template directories contain HTML/CSS code for the Flask frontend. Flask uses Jinja2 templates. Local Setup These instructions assume the user is already knowledgeable of git and Python environments and has Python 3 (note: 3.7 was used during development) with pip installed. Clone this repository bash git clone https://github.com/Parkkeo1/CourseProject.git cd CourseProject Create a new Python venv and install dependencies bash python -m venv venv source venv/Scripts/activate pip install -r server/requirements.txt python -m spacy download en_core_web_lg Launch the software. bash cd server python train.py // if you want to to newly train and save a TF-IDF and LDA model based on data/uiuc_bios.txt to be later used by the Flask app. python app.py // if you want to launch the project's main application, the Flask app. If you launched the Flask app, navigate to localhost:5000 in your web browser to view and use the Flask app."
https://github.com/Redstone-WB/CS410-guava	<CS410 : Project Progress> Project Topic  Project topic : (Option 1) Reproducing a paper: Latent Aspect Rating Analysis Project Schedule (Table 1) Date TODO 11.20 ~ 11.28 Paper review 11.29 Progress report 11.30 ~ 12.01 Source review 12.02 ~ 12.08 Source implementation 12.09 ~ 12.11 Result analysis & Documentation 12.12 Source code & Documentation submission Team members 1. Hongseok ha (netID : hh23), with administrative duties 2. Changsoo Kim (netID : ck37) Progress Report 1. Which tasks have been completed? (Progress made thus far) A. Paper review i. We went through the paper to get a holistic understanding and found that this paper suggests a unified framework LARAM, which is improves LARA and enables aspect rating analysis without knowledge of the target domain. ii. LARAM has two components: 1) an aspect modeling module, and 2) a rating analysis module similar to the Latent Rating Regression Model (LRR) used in LARA. iii. Once we infer the latent aspect assignment z and aspect weight a with the given model Th = (e, g, b, m, S, 2), we can estimate corpus-level parameters using the Expectation Maximization algorithm. B. Find materials that help understand the paper i. We found the author's original source from his homepage. This would be very helpful to understand the process more precisely. However, it is implemented in Java, so we need to convert it to Python. ii. URL: http://sifaka.cs.uiuc.edu/~wang296/ 2. Which tasks are pending? (Remaining tasks) A. Python code implementation i. As there are two components and we have two members, each member will be in charge of one module. 1. Hongseok Ha: Aspect modeling module 2. Changsoo Kim: Rating analysis module B. An overall schedule is shown in Table 1. 3. Which tasks have been completed? (Any challenges/issues being faced) A. Ambiguity of aspect modeling part: The aspect modeling module behaves similarly as LDA or sLDA, but it has different assumptions. We cannot spot the big difference so far, so we sent an email to the TA and we are going to ask on Piazza as well. If there are only subtle difference, we are going to use LDA instead and see the difference between the paper's result and ours. B. Python portability: We will convert the author's original code in Java to Python. These differences can lead to slight differences in results. C. Experiment results: In the paper, there are several comparisons, such as LDA vs sLDA vs LARAM, LDA+LRR vs sLDA+LRR vs LARAM, Bootstrap+LRR, LARAM, and so on. If we were to compare the number of all cases, it would take a lot of time. We will only compare the paper's suggesting result and our result. <Execution Guidelines> CS410 Final Project, 2020, Fall Author : Changsoo Kim, Hongseok Ha Prerequisites Python : version over 3.5 Python packages : numpy, lda If you want to use np.load to load .npy files, please use numpy version 1.16.1 Data Default data(small, to upload git repository) directory : './data/yelp_sanitation_data/' To try another dataset, please change line 16 of 'Main.py' file. Output Example LDA output (5 topics) LRR output Execution Steps Clone the repository Execute the Main.py, using command 'python Main.py' If you want to change the LDA topic parameter (k), please modify line 24 of 'Main.py' file. (n_topics) <CS410 : Project Proposal> Team name : guava Team members : Hongseok ha (netID : hh23), with administrative duties Changsoo Kim (netID : ck37) Project topic : (Option 1) Reproducing a paper: Latent Aspect Rating Analysis Programming language that will be used : Python Dataset Availability Two datasets (the hotel review dataset and the Amazon Mp3 review dataset) that are used in the paper are both available in the link below. Dataset Link : http://sifaka.cs.uiuc.edu/~wang296/Data/index.html However, for the Amazon MP3 review dataset, the number of reviews are different in the paper and the actual dataset. (the paper : #16,680, the dataset : #55,740) We will see if we can get results similar to the paper. The paper (Latent Aspect Rating Analysis) deals with the methodology of mining latent topical aspects, without pre-specification of aspect keywords (hashtags, ... ). LARAM would be helpful to cluster user reviews with various aspects, making it easy for other users to retrieve topics that they want. Also, reproducing this paper would be helpful for us to understand overall contents of CS410. <Project Review> CS410, 2020, Fall Authors : Changsoo Kim, Hongseok Ha Overall LARAM(2011) improved LARA(2010) in that it allows finding latent topics without specifying the seed words of the topics. LARAM can be divided into two modules, which are 1) finding latent topic aspects (aspect modeling module) and 2) finding ratings on each identified aspect (rating analysis module). Although we tried to understand the details of LARAM based on the original author's source code and python code from other sources, it was difficult for us to figure out all the details of the paper. Thus, as we mentioned in our progress report, we tried to implement the paper by replacing the aspect modeling module with LDA, and the rating analysis module with LRR (Latent Rating Regression). However, it was difficult for us to link the LDA results to LRR. Also, although we converted the java code of the original author to implement a python-version of LRR (which we could not find other references), there were subtle numeric differences in our intermediate results. We could not be sure whether the previous mismatch was the fundamental cause of following failures, but many unknown causes leaded us to a problem in which LBFGS (which was used as an optimizer, to minimize beta parameters) fails in line search, so that the beta parameters could not converge. In conclusion, the beta parameters could not be updated, so that the EM algorithm could not worked properly, resulting the maximization of log-likelihood (that we expected) could not happen. We found that we should try to understand the paper more sufficiently, before starting the code implementation. Also, to save time, utilizing libraries and packages would be very helpful. With this project, we were able to empathize with researchers who were making great efforts to advance their research in their fields. Difficulties There were ambiguous words, that we could not be sure whether they should be included in stopwords. For instance, the phrase 'n't' was not included in stopwords of the nltk package. Although it seems to mean 'not' with some verbs, it would mean nothing, without any verbs. Although the python package 'scipy' provides L-BFG-S option in an optimizer, structures of parameters were different, so that using the function imported from scipy was difficult. That's why we chose to convert the original author's java code of LRR. However, it resulted in the numerical mismatch in intermediate results. In LARAM, the Z values from aspect segmentation should be utilized in LRR, to improve the overall performance, but we could not implement that part, for lack of understanding of the paper and limit of time. We found that it is a tough task to implement a paper to well-structured codes only by looking at the formula and diagram in the paper. Additional resources (such as the author's presentation materials, that provides details and easier explanation) seems to be helpful to implement the paper in detail. CS410 : CourseProject TERM INFO : UIUC, CS410, 2020, FALL Authors : Changsoo kim, Hongseok Ha Guideline for the execution : Execution Guidelines.docx Review for the whole project : Project Review.docx Execution Video : https://uillinoisedu-my.sharepoint.com/:v:/g/personal/ck37_illinois_edu/EbHMSN_G_cJFrKHp7k164TwBjtK73jVoCEMs8Bs7yL_DYw?e=j3w4Eq References LARAM (java) : http://sifaka.cs.uiuc.edu/~wang296/ Preprocessing : https://github.com/tonyzhang1231/LARA_Python ETC : https://github.com/ericcds/LARAM_Python
https://github.com/Reynold-Chan/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/RyoTakaki/CS410_TIS_CourseProject	"CS410 Text Information Systems (Fall 2020) Project Progress Report Text Classification Competition: Twitter Sarcasm Detection Ryo Takaki 1.Progress made thus far 1.1 Project outline: The goal of this project is to classify tweets into two labels(""SARCASM"" and ""NOT_SACASM""). A performance of the classified label will be evaluated by using F1 score, and will be compared to a baseline performance generated by a state-of-the-art model. 1.2 Progress: I decided to use a BERT(Bidirectional Encoder Representations from Transformers) which is known to have a good performance with text classification problems. Fortunately, an implementation of the BERT model was easy and a performance for our task(twitter sarcasm detection) was good enough to beat the baseline. Specifically, although it was the first try to train the model, the F1 score was 0.744 (Baseline: 0.723). 2.Remaining tasks 2.1 Model update: As I have already outperformed the baseline performance, additional parameters tuning for the model training is not necessary. 2.2 Presentation preparation: For the final presentation, remaining tasks below will be done. *Add comments to the code *Make a project presentation video 3.Any challenges/issues being faced Currently, I do not have any issues. Therefore, no additional support is needed. 1 CS410 Text Information Systems (Fall 2020) Project Final Report Text Classification Competition: Twitter Sarcasm Detection Ryo Takaki 1.Overview of functions 1.1 Twitter Sarcasm Detection The main function of this system is to predict a label(SARCASM or NOT_SARCASM) of input tweets. The input tweets include response tweets and its context tweets. We are given 5,000 training data sets and 1,800 test data sets. The prediction performance will be evaluated using F1 score and compared to the baseline score. Input : Pairs of the tweets(response and context tweets) Output : Predicted labels of the response tweets(SARCASM or NOT_SARCASM) Process : Pre-trained BERT model + single layer classifier 1.2 Code outline The code of the twitter sarcasm detection system mainly consists of the components below. Detailed instructions of how to implement those components are explained in the next section. 1 No Component Function outline 1 Library installation Install the Hugging Face Library for BERT model 2 Context selection Derive the latest tweet from context tweets 3 Removing @USER Remove unnecessary words from the input tweets 4 Tokenization Convert the input text to the tokenized IDs 5 Formatting Reshape the input IDs size with fixed length 6 Data split Split the data into training and validation data set 7 Model training Build and train the BERT model with recommended parameters 8 Label prediction Predict the labels using test set data CS410 Text Information Systems (Fall 2020) 2.Implementation 2.1 Library Installation BERT pre-trained model and its library can be used by just installing the library. https://huggingface.co/transformers/ 2.2 Context selection As you can see that some contexts have several context tweets(e.g. 20), however the majority have only two to five context tweets. Although we can see that at least two tweets are included in the context information, I decided to use only one latest context tweet as a first step. In the actual code below, the derived latest context tweet is stored as a new column(pre_comment). 2 CS410 Text Information Systems (Fall 2020) 2.3 Removing @USER As you can see, the original response and context include unnecessary words like ""@USER"". The unnecessary word(@USER) is removed by this function. 2.4 Tokenization In order for the BERT model to handle the input tweets, the original tweets sentences will be converted to tokenized words and its IDs. 2.5 Formatting In order to define the BERT model's input dimensions, we need to identify the maximum length of the input tweets(response + context). Thanks to the library, what we need to do to create the fixed size input data is to set the maximum input length when we encode the data. Even if the actual input data length is shorter than the maximum input length, remaining parts will be automatically filled with zero. 3 CS410 Text Information Systems (Fall 2020) 2.6 Data split 5,000 Data sets are divided into 90% of training sets and 10% of validation sets randomly(although it is random selection, it is reproducible as the seed value is specified). 4 CS410 Text Information Systems (Fall 2020) 2.7 Model training 2.7.1 Model definition The model here consists of mainly two parts. One is the pre-trained BERT model and the other is single layer linear classification for this tweet classification application. In order to build the model, we only need to import ""BertForSequenceClassification"" from transformers like below, and specify some parameters. In our case, num_lebels is 2 as we want to classify the tweets into SARCASM or not. 5 CS410 Text Information Systems (Fall 2020) 2.7.2 Trainable parameters setting Finally, I decided not to change the trainable parameters of the BERT model as the performance with fixing the BERT model was not good enough. However, those parameters can be changed by enabling the cell below if needed for other purposes. 2.7.3 Model training with recommended parameters The batch size, the learning rate, the epsilon, and the number of epochs were determined based on the recommendation of the author of the BERT papar. Although I have not tried to change the parameters, It worked well at the first trial. 6 CS410 Text Information Systems (Fall 2020) Although I only tried to use the pre-trained BERT model + single linear classifier with recommended parameters, the training time was faster than expected and also the performance was good enough to beat the baseline(See 2.9). 7 CS410 Text Information Systems (Fall 2020) 2.8 Label prediction on the test data By applying the same preprocessing as for the training/validation data set, the test data can also be inputted to the trained model. And finally, we can get the predicted labels by applying the softmax function to predicted values. 2.9 Competition result The F1 score (0.737) beat the baseline score(0.723) as below. 2.10 Reproducing the same prediction After re-running all the code, a reproducibility test will be done automatically as below by downloading the actual submitted prediction file and comparing them to the prediction labels generated in your environment. 8 CS410 Text Information Systems (Fall 2020) 2.11 Reference The project code was inspired by a UIUC's tech review and a public website below. *https://github.com/zen030/tech_review/blob/master/techreview.pdf *https://mccormickml.com/2019/07/22/BERT-fine-tuning/ 3.Usage 3.1 Open ""SarcasmClassification.ipynb"" 9 CS410 Text Information Systems (Fall 2020) 3.2 Follow the setup instruction 1.1 and 1.2 below. 3.3 Run the code After setting up the google colab GPU setting above, you can run all the code by clicking the ""Runtime -> Run all"" below, or simply run all the cells one by one. 10 CS410 Text Information Systems (Fall 2020) 4.Contribution As my team member is only me, everything is done by myself. 11 CS410 Text Information Systems (Fall 2020) Project Proposal and Team Formation Submission for Grading Ryo Takaki In your project proposal, please answer the following questions: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Ryo Takaki (Captain) NetID:rtakaki2 2.Which competition do you plan to join? Text classification competition 3.If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods No. Methods Experience 1 CNN Through Udacity's Self-Driving Car Nanodegree and Computer Vision Nanodegree course, I have learned the basics of CNN, RNN and how to use PyTorch. Computer Vision Nanodegree course syllabus Automatic Image Captioning: Combine CNN and RNN knowledge to build a deep learning model that produces captions given an input image. Image captioning requires that you create a complex deep learning model with two components: a CNN that transforms an input image into a set of features, and an RNN that turns those features into rich, descriptive language. In this project, you will implement these cutting-edge deep learning architectures. 2 RNN 3 LSTM 4 PyTorch 5 BERT No actual experience with this method, but I would like to try this first just because the BERT is well-known. If the BERT is not good enough, then I will try other methods. 4.Which programming language do you plan to use? Python CourseProject Classification Competition Twitter Sarcasm Detection Project proposal Project proposal Project progress report Project progress report Project final report Project final report Code Prediction results Answer.txt Tutorial movie UIUC media space"
https://github.com/Sembian2-CS410Fall2020/CourseProject	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Project Documentation: https://github.com/Sembian2-CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_documentation.pdf CS410 - Course Final Project Project Documentation Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) Project Installation guide The project code is completely executed in a google colab environment, please download the ipynb file and upload to google, you can also make a copy directly from the google colab link https://colab.research.google.com/drive/1gzwQJeSNKXulIjOX34z-quQePByhPt75?usp=sharing Download the ipynb and load into google colab ( https://colab.research.google.com/) and enable the GPU runtime **Project Documentaiton:** https://github.com/Sembian2-CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_documentation.pdf **Project Progress Report:** https://github.com/Sembian2-CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_progress_report.pdf **Project Proposal Document:** https://github.com/Sembian2-CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_proposal.pdf **Project Presentation Video** (YouTube Link) : https://www.youtube.com/watch?v=cH2tZB5n_8Y Motivation & Dataset: The text classification competition involves a binary classification of tweets with a balanced training including labels indicating SARCASM(0), NOT_SARCASM(1), I used the state-of-the-art Transformers, pytorch libraries with BERT Embeddings. The training dataset had 5000 labelled samples with balanced label distribution of 50 % and the test dataset had 1800 rows with additional id field. Approach: The training dataset is loaded into pandas DataFrame and the response column was pre-processed with text cleaning including removing @USER and @URL, expanding shortwords, expanding emoji's, and removing any stopwords using nltk library, removing special characters and punctuations. After pre-processing the response tweets column and label column is split into training and validation I used a .33% validation and .77% training data set. Classification Methods: The First approach is to use the Multinomial Naive Bayes by applying TF-IDF and got a baseline AUC score of .8118 the accuracy was around 72% I used this as a baseline and tried improving the baseline using BERT embeddings and a feed forward neural network. Text Classification with Transformers in PyTorch: BERT The transformer-based LM(Language models) has shown promising progress on number of NLP benchmarks. By combining transfer learning methods with large-scale transformer language model is becoming a standard in modern NLP compared to traditional classification approaches. In this final approach to improve the baseline score of 72.24% from the MultinomialNB approach we will attempt to increase the accuracy score by implementing a transformer architecture and fine-tuning of the pre-trained BERT model for classification. The two important complimentary concepts in Natural Language Processing: * Word embeddings * Language Model Transformers are used to build the language model and embeddings can be retrieved as the by-product of pretraining. Transformers architecture implements so-called attention mechanism to include an entire sequence as a whole enabling training in parallel when compared to traditional LSTM approaches. The huggingface transformers library has a huge collection of the language models and embeddings and makes it easier for implementing using pytorch in python. BERT BERT( Bidirectional Encoder Representations from Transformers) is a mothod of pretraining language representation. BERT does not have a decoder but stacks 24 layer encoders for bert-uncased-large) #Sample code showing the import and instantiation of BERT Model from transformers. import torch import torch.nn as nn from transformers import BertModel # Instantiate BERT model self.bert = BertModel.from_pretrained('bert-large-uncased') BERT Tokenizer and Netowrk Architecture The important limitation of BERT is that the maximum sequence length is 512 tokens, the shorter sentences are added with [PAD] and there is also a [CLS] token for indicating beginning of the sentence and [SEP] token at the end of sentence the tokenized sentence is then encoded using BERT Embeddings the bert-large has 1024 embeddings While there are multiple approaches I used a custom BertClassifier with a single feedforward neural network with # Specify hidden size of BERT, hidden size of our classifier, and number of labels # BERT-Large, Uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters D_in, H, D_out = 1024, 50, 2 # Instantiate an one-layer feed-forward classifier self.classifier = nn.Sequential( nn.Linear(D_in, H), #https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear nn.ReLU(), #https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU nn.Linear(H, D_out), #https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear ) The final layer out put is passed thru a ReLU activation layer and output dimensions of 2 indicating the 2 labels[SARCASM-0, NOT_SARCASM-1] , the BERT tokenizer is applied on all responses of the training data and map tokens into WordPiece embeddings using the encode_plus function, the following parameters were used for training. LearningRate 5e-5 Max Sequence Length 89 Batch Size 32 No. of Epochs 4 The model is then trained for 4 epochs and achieved a score of 81.17% on the training set that is almost 10 point increase from the baseline MultiNomialNB model. For HyperParameter tuning I used wandb.com ( weights and Biases) to report out the various runs and compared the best score and the run named revivedpthunder-388 scored the highest and achieved a 81.17 validation accuracy and 75.19 Test Accuracy in the leaderboard Report Link to wandb Tables Training Accuracy TF-IDF Vectorizer and Multinomial Naive Bayes 72.24% Transformers with BERT_large_uncased embeddings 81.17% Test Accuracy of 75.18% - Position 10 on Leaderboard as of 12.02.2020 Using the Transformers, Pytorch and BERT Classification model I was able to beat the baseline score on the leaderboard and improved the score by repeating the training with Hyper Parameter tuning and text pre-processing techniques and achieved a score of 75.18% Test Accuracy, and have no challenges. References: * Images for illustration are taken from the original BERT paper (Devlin et al. 2018). * Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,Bert: Pre-training of deep bidirectional transformers for languageunderstanding, 2018. * Weights and Biases: https://wandb.ai/cayush/bert-finetuning/reports/Sentence-Classification-With-Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA * The Role of Conversation Context for Sarcasm Detection in Online Interactions: https://arxiv.org/pdf/1707.06226.pdf * Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. ""What does bert look at? an analysis of bert's attention."" arXiv preprint arXiv:1906.04341 (2019). * Sileo, Damien. ""Understanding bert transformer: Attention isnt all you need."" Towards Data Science (2019). * Weiss, Karl, Taghi M. Khoshgoftaar, and DingDing Wang. ""A survey of transfer learning."" Journal of Big data 3, no. 1 (2016): 9. * Biewald, Lukas. ""Experiment Tracking with Weights and Biases."" (2020). * Text Tweet Pre processing - https://github.com/digitalepidemiologylab * The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) - https://jalammar.github.io/illustrated-bert/ * Pytorch.org - https://pytorch.org/docs/stable/index.html * BERT Fine-Tuning Tutorial with PyTorch for Text Classification: https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1 * Huggingface transformers library - https://github.com/huggingface/transformers * BERT for Advance NLP with Transformers in Pytorch - https://www.linkedin.com/pulse/part1-bert-advance-nlp-transformers-pytorch-aniruddha-choudhury/ * Attention Is All You Need; Vaswani et al., 2017. - https://arxiv.org/pdf/1706.03762.pdf * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding; Devlin et al., 2018. - https://arxiv.org/pdf/1810.04805.pdf * Encoder-Decoder Architecture& Bert Paperon the full research. - https://arxiv.org/pdf/1810.04805.pdf * Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). [Tackling the poor assumptions of naive bayes text classifiers. In ICML](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) (Vol. 3, pp. 616-623). * Text classification with transformers in Tensorflow 2: BERT, XLNet - https://atheros.ai/blog/text-classification-with-transformers-in-tensorflow-2 CS410 - Course Final Project Project Progress Report Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) Which tasks have been completed? The Final project was planned and split into 18 Tasks and over six 2 week sprints, below are the tasks and child issues completed with Sprint reports For HyperParameter tuning I used wandb.com ( weights and Biases) to report out the various runs and compared the best score and the run named revivedpthunder-388 scored the highest and achieved a 81.17 validation accuracy and 75.19 Test Accuracy in the leaderboard Report Link to wandb Test Accuracy of 75.19% - Position 6 on Leaderboard as of 11.26.2020 Which tasks are pending? The following tasks are in progress and I am on track for completing the project code documentation and presentation. Are you facing any challenges? Using the Transformers, Pytorch and BERT Classification model I was able to beat the baseline score on the leaderboard and improved the score by repeating the training with Hyper Parameter tuning and text pre-processing techniques and achieved a score of 75.19% Test Accuracy, and have no challenges. CS410 - Course Final Project Project Proposal Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Sembian Balasubramanian NetID: sembian2 I will doing this project as individual, I will ensure all administrative tasks are completed and will be submitting the deliverables as per schedule. Which competition do you plan to join? I am really interested in the text classification competition, this would allow me to learn and build, train a state of the art classifier model using industry leading libraries implementing methods like LSTM and Convolutional Neural networks using TensorFlow-Keras / Pytorch transformers and using word embedding techniques - I will also experiment and train the model using Activations like sigmoid, ReLU, softmax and exploring evaluations of accuracy and loss of training and validation , performing parameter tuning updates and using optimizers While the technology is complex, after watching the coursera lectures on text classification, I am comfortable in text classification methods, also at my I am leading the multi-channel measurement strategy and would apply my learning from this project to propose state of the art classification models and hyperparameter tuning for sentiment, sarcasm & emotion detection Which programming language do you plan to use? Python /Jupyter Notebook / Google Colab with cuda for model training"
https://github.com/ShyamShah11/CourseProject	"Which tasks have been completed? I have determined the three techniques I wanted to implement (naive bayes, knn, and max entropy classifiers). I have generated my datasets, and have generated the feature vector representation of each URL. I'm still experimenting with different representations but have most of the code ready for me to easily make changes to it. I originally tried using tokens from the URL but have moved to using English words in the URL. I have also created the first draft of my naive bayes and knn classifiers. They both currently classify about 80% of URLs (n=500) correctly. Which tasks are pending? I need to implement the max entropy classifier and tune my models. I also need to prepare the documentation and potentially add some files to be ran specifically for demoing. Are you facing any challenges? Progress has been limited at times because of hardware limitations (I considered moving to cloud but wanted to keep it simple/local for demo purposes). So I haven't been able to use my entire dataset yet which may skew the classifiers performance. Topic Specific Questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will be doing this project by myself so I will be the captain. My name is Shyam Shah and my NetID is shyam3. 2. What system have you chosen? Which subtopic(s) under the system? The system I chose is ExpertSearch and the subtopic I will work on is ""Automatically crawling faculty webpages"" -> ""Identifying webpage faculty URLs"" 3. Briefly describe the datasets, algorithms or techniques you plan to use I will be working primarily with the faculty webpage dataset (from MP2.3) for positive examples. I intend on using the links on the sign-up sheet from MP2.1 (https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVxixU5vvMuUaM/edit#gid=0) to also scrape some negative examples. My plan is to approach this as a classification problem, as suggested. I intend on using different classification algorithms/techniques (such as logistic regression, k-nearest neighbors, and neural networks) and want to treat part of this as a research opportunity to teach myself some different kinds of classification algorithms that are effective with text. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? I am going to treat this as adding a function to classify URLs on a given webpage. I can demonstrate that this works by splitting my dataset into a training and validation dataset and measuring precision/recall on the validation dataset. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly I'll start by forking the current project on Github and adding the functionality to it while developing it. There are instructions in the repo to run the application locally so I would not need to build anything else on my own. Ultimately, I would like to have it merged with the current project but that may be something I look into outside of this course. 6. Which programming language do you plan to use? I will be using Python 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 3h Research algorithms 2h Prepare data 10h Implement different algorithms 5h Testing and tuning parameters Other General Questions: 1. What is the function of the project? The function is to add the ability for ExpertSearch to automatically identify links within a page as relevant without requiring human intervention. 2. Who will benefit from this project? The users and developers of ExpertSearch 3. What existing resources can you use? I can use existing Python libraries like tensorflow and sklearn to implement the models, most of the work will probably be identifying good features to use for them 4. A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) November 1 Gather information on all of the models I want to implement November 22 Finish implementing first version of models November 29 Finish tuning and testing models December 7 Finish creating documentation and demonstration ExpertSearch: Identifying Faculty Webpage URLs Proposal The project proposal is in the file called proposal.pdf. Progress Report The progress report is in the file called progress_report.pdf Demonstration For my reviewers, the setup instructions and demo are documented in the README file of the code folder. Details about my implementation and decisions made are below. Implementation I decided that I did not need to use the ExpertSearch code for this but I did use some of the ExperSearch data. I can isolate the task of classifying URLs from the rest of the functionality provided by ExpertSearch so this project will be me working on it seperately (for now). Everything implementation related is in this folder. Preparing data In the data folder, I have a file for positive training data and negative training data. The positive training data has examples of actual faculty URLs and is taken directly from here. The negative data has examples of non faculty URLs from university pages as well as all of the directory URLs themselves from the MP2.1 signup spreadsheet. The code for generating these is in scraper.py. Note: the scraper uses selenium which requires chromedriver.exe to either be on your PATH, or in the same directory as scraper.py. I have over 15,000 positive and negative URLs but only ended up using 500 of each of them. You won't need to run scraper.py unless you intend on using more than 15,000 URLs to retrain the models. Data as feature vectors For features, I used tokens from the URL (ie. everything seperated by a non alphabetic characters) first. After a few tests, I decided to use only the tokens that were also English words since the tokens that aren't English words are usually people's names or university name acronyms which I felt did not add enough value for the number of features they added. preparedata.py converts the datasets into feature vectors which are used for the different algorithms, and it counts the term frequency over all URLs. I have only uploaded feature vectors for a small subset of my data to avoid file size constraints. Classifiers The three classifiers I experimented with were Naive-Bayes, maximum entropy, and k-nearest neighbors. I also tuned my models based on a small sample size of data (1000 total URLs) because of time and hardware constraints. If the models are retrained with more data, it's likely that they will need to be tuned again. Naive Bayes I experimented with 4 kinds of Naive Bayes classifiers: Gaussian, Bernoulli, complement, and multinomial. Over 400 test points, these were the results (accuracy is the number of correctly labelled URLs). | Classifier | Accuracy | | :-------------|:------------- | | Gaussian | 83% | | Bernoulli | 87% | | Complement | 79% | | Multinomial | 49% | Based on this, I decided to go with a Bernoulli Naive Bayes classifier. One of the benefits of a Bayes classifier is that you get to use a prior. However since I had an even split of positive and negative samples in the data I used, the prior I chose to use was a uniform prior over the two labels. A possible improvement would be to modify the scraper to calculate a more realistic prior by calculating the amount of URLs on a university webpage that are faculty URLs. K-nearest neighbors For the k-nearest neighbors classifier, I experimented with using uniform weights and distance based weights. These were the results for uniform weights. | k | Accuracy | | :----|:------------- | | 2 | 71% | | 3 | 84% | | 4 | 83% | | 5 | 84% | | 6 | 84% | | 7 | 83% | | 8 | 85% | | 9 | 84% | | 10 | 84% | | 11 | 85% | | 12 | 84% | | 13 | 83% | | 14 | 82% | | 15 | 82% | | 16 | 79% | And these were the results for distance based weights. | k | Accuracy | | :----|:------------- | | 2 | 71% | | 3 | 84% | | 4 | 84% | | 5 | 85% | | 6 | 85% | | 7 | 84% | | 8 | 86% | | 9 | 86% | | 10 | 86% | | 11 | 88% | | 12 | 86% | | 13 | 86% | | 14 | 86% | | 15 | 85% | | 16 | 85% | Based on these results I went with k=11 and using distance based weights. Maximum entropy For the maximum entropy classifier, I first had to make changes to how my data was being represented. The nltk MaxEntClassifier requires a dictionary instead of an array to be trained over. So instead of using the same feature vectors, I converted them back to dictionaries of tokens. I experimented with the GIS and the IIS algorithms. With 100 iterations, the GIS algorithm classified 92% of the test points correctly, and the IIS algorithm had 91% accuracy. The accuracy for both algorithms went down when I tried again with 500 iterations so I stuck with the GIS algorithm and 100 iterations. I used this paper to help come up with some of the ideas used for this project."
https://github.com/SphtKr/MeTAPyquod	"Matt Williamson N mdw8 (lead/captain) CS410 November 27, 2020 MeTAPyquod Progress Report As of the end of November, the metapyquod-dev container image is essentially complete and workingNat least for x86_64 platforms. Work on metapyquod-server is in its early stages. metapyquod-devInitial creation of the container deThnition went smoothly and operates as expected on macOSNin fact, a surprising result was observed that running a working implementation of MP2.4 natively on a macOS 11 host and then in the metapyquod-dev container resulted in a consistently faster execution time in the container than natively: Two adjustments from the initial proposal are likely necessary for the metapyquod-dev portion of the project: 1.A desired goal was to include a GUI IDE for Python development in the containerNhowever, this was seen as high-complexity and added risk, especially on non-Linux platforms. However, in the process of researching options the web-pdb Python module was discovered, METAPYQUOD - PROGRESS REPORT1which provides a full debugging environment within a web browser. The need for a robust debugger was the primary motivator for providing a GUI IDE, and this module provides the visual debugging capability with far lower complexity. Therefore, the Thnal solution will map the host working directory into the container to allow any editor (PyCharm, VSCode, Spyder, etc.) to work directly with the Thles on the host, but provide debugging via web-pdb. 2.Another desired goal was to support armv7l/armhf and armv8/aarch64 systems, mainly because of the popularity of Raspberry Pi and similar SBC hardware. However, attempts to do so have been beset with build problems, despite the fact that the base ofThcial Python 3.6 image being used supports those architectures. First, scipy/numpy and related dependencies are not well-supported on these systems, though this may be surmountableNand if so, the beneTht may be signiThcant because the time required to build these dependencies on these platforms is a large investment, and a prebuilt container image would eliminate this cost. Second, and more difThcult, the MeTA build system itself fails on aarch64 (at least) when trying to download and build icu4cNthe same problem encountered by many users on many different platformsNand the solution to this problem (or even itOs exact root cause) remains elusive. This may be resolvable but would be likely to consume more time than is available within the project scope. METAPYQUOD - PROGRESS REPORT2It may be noted thatNin this studentOs opinionNthe present state of support of MeTA/metapy jeopardizes the viability of continuing to base the course content on these tools. It appears that no updates have been made to the tool in two years, and as Python and other dependencies continue to evolve, the surface area of supported conThgurations will continue to shrink. Additionally, it may be particularly noteworthy that the build problems described above with 64-bit ARM systems are likely to also occur with new Apple ARM processor (M1) systems, which will eventually include all new Apple systems going forward. metapyquod-serverWork on metapyquod-server is just beginning, but will beneTht both from work already done on metapyquod-dev and the tech review completed on AWS LambdaNwhich shares some implementation details including the OpenAPI speciThcation. Given difThculties with metapyquod-dev, it is likely that some descoping may occur here as well, for instance the feedback or metrics endpoints. Remaining technical unknowns have to do with the behavior of MeTA inverted disk indexes being updatedNe.g. it is unclear whether a total re-index will be necessary when starting the container or whether the existing index can be loaded and new documents added. Based on experience with metapyquod-dev, the metapyquod-server container will only target x86_64 platforms, though this is less of an issue for its use case.METAPYQUOD - PROGRESS REPORT3 Matt Williamson N mdw8 (lead/captain) CS410 October 25, 2020 MeTAPyquod Containerized metapy Appliances The object of this project is to simplify the use of MeTA and the metapy bindings by creating pre-packaged docker containers ready to use with all dependencies for different use cases. The result will be at least the following two reusable containers: metapyquod-devThis container will contain the metapy libraries and a functional development environment including a known good version of Python, metapy dependencies and useful related libraries (e.g. scipy, numpy), Git version control binaries, and a Linux shell, suitable for experimentation and basic development (i.e., all the CS410 programming assignments should be able to be completed with only the tools in the container). This container (or a derivative) may also include a GUI IDE such as Spyder and its dependencies to better facilitate development work. A particular goal of this container is to simplify use of the toolchain on Windows with Docker (and/or Windows Subsystem for Linux), and the hope is that with DockerOs new Omulti-archO features that this and running on ARM devices (e.g. Raspberry Pi) will be possible. If this goal is achieved, such users will be able to instantiate a full working environment for metapy development with a single ""docker run ..."" command. metapyquod-serverThis container will provide a simple search engine core as a microservice behind a REST API. The goal is for this container to be capable enough to serve in basic production use as a simple search appliance, for instance in an intranet search scenario. It will also facilitate experimentation with different metapy components in a real world use case. The search service will be intended for use with English language corpora. METAPYQUOD1This container will expose a Volume where it expects to Thnd a web mirror Thlesystem structure according to the conventions produced by the wget spider process. This will allow the user to use the basic but ssexible wget tool (likely in another container) to crawl and download web content for ingest by the metapyquod-server appliance. Since wget supports timestamping of downloaded Thles (and requery culling based on Last-Modified headers, the appliance will monitor the Volume for newly modiThed Thles, gaining some efThciencies. In addition to providing sane defaults, this container will also expose a volume with conThguration control data (e.g. a config.toml or other relevant Thles) to facilitate customization of or experimentation with the search appliance. The search service itself will be written in Python (probably with an existing HTTP framework like Django) and will expose at least the following three capabilities as HTTP REST services: 1.A search service, with a text string parameter and pagination parameters. This service will return JSON-formatted search results including the URL, and will include a query identiTher for potential use with a feedback mechanism. 2.A feedback service for registering click-throughs for implicit feedback. The viewed URL and the query identiTher from the search service will be received as inputs. This service will likely have no effect in the initial iteration, but is established for future implementation of feedback incorporation. 3.A telemetry service providing basic information about the search index for diagnostic or engagement purposes (e.g. last 10 URLs indexed, total documents/terms in index, etc.) A Swagger/OpenAPI speciThcation will be provided such that a compliant client should be able to interact with the serviceNit should be possible to test the service using a generic Swagger/OpenAPI client. If time permits, a simple example web frontend demonstrator for the service may be provided. Estimates metapyquod-dev: Design and build container speciThcation with dependencies, deploy and test on all target environments, create documentation: 4-6 hours. metapyquod-server: Develop and implement REST services: 10-15 hours. Package and test containerization, create documentation: 4-8 hours. METAPYQUOD2 MeTAPyquod Proposal PDF Progress Report PDF Containerized metapy Appliances The object of this project is to simplify the use of MeTA and the metapy bindings by creating pre-packaged docker containers ready to use with all dependencies for different use cases. The result is three reusable containers: metapyquod-dev With this container users can instantiate a full working environment for metapy development with a single ""docker run ..."" command, instead of fighting python versions and build issues. This container includes the metapy libraries and a functional development environment including a known good version of Python, metapy dependencies and useful related libraries (e.g. scipy, numpy), Git version control binaries, and a Linux shell, suitable for experimentation and basic development (e.g., all the UIUC CS410 programming assignments should be able to be completed with only the tools in this container). This container builds on multiple architectures including i386, amd64, armv7l and aarch64--so it will run on Windows, macOS, Linux, and Raspberry Pi 2, 3, and 4, for 32-bit and 64-bit OS's! (It's also believed to be ready for Windows ARM64 and MacOS M1 ARM64 processors when Docker supports them!) metapyquod-server This container provides a simple search engine core as a microservice behind a REST API. This server is almost capable enough to serve in basic production use as a simple search appliance in an intranet search scenario, but lacks some robustness. Mainly it can facilitate experimentation with different metapy components in a real world use case. The search service is intended for use with English language corpora. metapyquod-indexer This container expedites the creation of MeTA indexes from a directory structure like that produced by wget --recursive. It captures a few useful fields in the index metadata including the original URL, the title from HTML documents, and the modification time of the file (which can be made to correspond to the modification date of the web page when retrieving with wget). This is intended to be used in conjunction with metapyquod-server, but may be useful in other contexts. What Docker/Why Docker? Docker is perhaps many things, but you can view it as yet another attempt to deliver ""Write Once Run Anywhere"" capability for software. Java did this by abstracting away the operating system and hardware and presenting a new, ""virtual machine"" (the Java VM runtime) that never existed before in concrete form. Hardware virtualization (e.g. VMWare, VirtualBox, KVM) does this by presenting a software-based abstraction of existing physical hardware, upon which you can run any whole OS and software. Docker does something in between, by creating a virtual OS running inside a ""container"" within another running OS (usually Linux) that to your software looks like a dedicated instance of an OS, and--crucially--provides efficient mechanisms to package that virtual OS and all of the dependencies necessary to run your software. In this way, software can be packaged, delivered, and used in a way that is predictable and repeatable regardless of the system on which it is run. With the above explanation, it may be clear why Docker can be useful for distributing and using software like MeTA/metapy, especially for learning or research: it has dependencies or proclivities for certain versions of python, certain operating systems (i.e. Windows) present problems, and gathering and/or building the software and its dependencies can be tedious and error prone and may require a level of proficiency in a number of different technical areas. For the containers in this project, one essentially needs only to install Docker and run a single command to get started. Helpful Background While it is not necessary to have a thorough understanding of Docker to use these containers (especially metapyquod-dev), the following references are helpful in understanding key concepts: Docker Simplified: A Hands-On Guide for Absolute Beginners (text) Learn Docker in 12 Minutes (video) Other References Install Docker Desktop on Windows (Windows Home specific instructions) Install Docker Desktop on Mac Install Docker Engine on Ubuntu (other distros also described on docs.docker.com, and pay special notice to the Optional post-installation steps) Installing Docker on the Raspberry Pi How were these images built? The Dockerfile within each directory defines the build steps necessary for each container, and you should be able to reproduce the build by using (or reading) these Dockerfiles. Note, however, that images for each container have been pushed to Docker Hub, so you should not have to build them yourself (e.g. docker build . -t metapyquod-indexer) unless you want to--you should be able to simply run them (e.g. docker run --rm sphtkr/metapyquod-indexer --help). The images on Docker Hub were built as ""multiarch"" images using buildx/BuildKit, using an amd64 host, a Raspberry Pi 3 running Raspbian Buster as an arm7l (32-bit ARM) build host and a Raspbery Pi 4 running Ubuntu as an aarch64 (64-bit ARM) build host. The result is that you should be able to use a single tag to pull/run any of these images from any x86_64 Linux Docker host (including macOS's HypervisorKit driver and Windows' WSL driver) or any ARMv7 or ARMv8 system supported by Docker (e.g. Raspberry Pi 2, 3, and 4 systems). As the above link points out, this is probably also crucial to support use on Windows and Mac systems with ARM processors, though Docker is not yet supported on these systems. The base image for all containers are the [official Python images], which are (thankfully) already multiarch images, simplifying the build process. Since the MeTAPyquod containers rely on the build system components being in the base image, the ""full"" official images are used and not the slim or alpine variants, which does result in a larger image/download size. (This could be improved for -server and -indexer in the future with a multi-stage build.) One notable aspect of all three Dockerfiles is that they build metapy from source with a small patch (instead of using pip install metapy). In short, all current branches of MeTA include a specified URL for downloading the ICU library, but the ICU project has changed all their download links to a new location on GitHub. This problem should be avoidable by installing libicu and libicu-dev via the package manager (apt), but on some architectures the Cmake build scripts for MeTA force a static build of ICU (for reasons that are not entirely clear), which forces the use of the download URL. Furthermore, the metapy in pip and at the head of the master branch in GitHub builds from a specific commit of MeTA that is no longer at the head of any branch, so to fix the problem both repositories would have to be updated. Therefore, to simplify the cross-architecture build process, we simply checkout the metapy source from GitHub recursively (including the specific commit of MeTA), patch the broken URL (the MD5 hash still matches) and build from that patched source. What's With the Name? The Pequod was the ship sailed by Captain Ahab in Moby Dick. The Whale mascot in the Docker Logo is named Moby Dock. So, ba dum tss (or womp womp)."
https://github.com/Sushanta77/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/TomTJarosz/CourseProject	Thomas Jarosz NetId: Tjarosz2 UIN: 669902044 CS410 Final Project Proposal For my final project, I have chosen Option 1: Reproducing A Paper. Questions: 1a) What are the names and NetIDs of all your team members? Thomas Jarosz: Tjarosz2 1b) Who is the captain? Tjarosz2 2) Which paper have you chosen? I have chosen to reproduce Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505. 3) Which programming language do you plan to use? I plan to use Python. 4) Can you obtain the datasets used in the paper for evaluation? Yes; the authors of this paper provide a link to the datasets. Thomas Jarosz Progress Report 1) Which tasks have been completed? I have read through the paper. 2) Which tasks are pending? Implementing the paper. 3) Are you facing any challenges? Yes; understanding the paper is difficult. CourseProject This repo contains the work for Team Foo's CS410 final project. Proposal Contained within the file CS410Proposal.pdf is the team Foo's project proposal.
https://github.com/VisualPracticeRank/CourseProject	Visual Practice Rank Goal The goal of this project is to create a visual representation of a supplied ranking function in a form that is comparable to modern search engines with the display of additional data that would be only in the background Project Presentation https://mediaspace.illinois.edu/media/t/1_3s2zdgys Getting Started Installation Create a virtual environment using conda: conda create -n [your-env-name] python=3.6 e.g. conda create -n vpr python=3.6 anaconda Activate the virtual environment conda activate [your-env-name] e.g. conda activate vpr Download the repository git clone https://github.com/VisualPracticeRank/CourseProject.git Install the packages in requirements.txt cd CourseProject pip install -r requirements.txt Running the Django server While in the CourseProject folder, cd into VPR, list of the files and folders and you will see a file called manage.py. Run the following command: python3 manage.py makemigrations python3 manage.py migrate --run-snycdb python3 manage.py runserver A browser with the application will pop-up, or you can head to 127.0.0.1:8000 If you are running this on a VM, instead of python3 manage.py runserver, you can run the following command: python3 manage.py runserver 0.0.0.0:8000 Then, head over to [your ip]:8000, e.g. 18.219.133.210:8000 Shutting down the Django server If you want to shutdown the Django server, you can do ctrl+c in the terminal to shut down the server. You can also deactivate the virtual environment with this command: conda deactivate Features Dataset You can upload your dataset by selecting Dataset on the homepage and fill in and upload the appropriate files needed: 1. Name 2. Description 3. Data 4. Qrels 5. Queries and select Upload. Model In addition to the default models available ('OkapiBM25', 'PivotedLength', 'AbsoluteDiscount', 'JelinekMercer', 'DirichletPrior'), you can upload your own custom models. You can upload your own model (ranker) by selecting Model on the homepage and fill in the textboxes: 1. Name 2. Description 3. Model and select Add. Query After selecting the dataset and model that you want to use, you can specify a query in the textbox and select Search. You will get the top 10 documents with the highest score in your model, displayed in descending order. Step Through This functionality allows you to step through the queries.txt file you specified when uploading the dataset and observe the changes in the ndcg score and other various stats. After selecting the dataset and model that you want to use, you can select Step Through. You can use the '>>' and '<<' buttons to step through and step out. How It Works Overview This program is implemented using Django (Python, HTML, SQLite3) as the frontend and metapy (and Python) as the backend. Frontend The dataset details are stored in the webui_dataset. The dataset, qrels, and queries files are stored in a folder, with a unique name generated by the system, under the dataset folder. The webui_dataset has the following fields: id (primary key), name, description, data, qrels, queries. When a dataset is being uploaded, the documents in the dataset is loaded into webui_document with the following fields: id (primary key), document_id, body, dataset_id (foreign key to webui_dataset.id). When a model is being uploaded, the model is loaded into webui_model with the following fields: id (primary key), description, model, name. Before storing the actual model, it will be encoded to base64. After you can select a dataset, model, and query, you click on Search. Then frontend (in view.py) will call the backend (search_eval.py) and pass the following variables: folder (folder where the dataset is stored), model, query. For the iteration feature, the frontend will send these information to the backend: folder (path to dataset), model. After receiving the response from the backend, the frontend will display the following information: query (as specified in the queries.txt), NDCG, a table of the following information: the score of the document, size of the document, unique terms in the document, and snippit of the body. Backend The backend (search_eval.py) will these variables from the frontend: folder, model, query. It will first change its directory to [folder_to_dataset]/datasets and build an inverted index. Then, it will determine if the model is one of the defaults ('OkapiBM25', 'PivotedLength', 'AbsoluteDiscount', 'JelinekMercer', 'DirichletPrior'). If it's not, then it will decode the string (using base64) and build the ranker. Finally, it will run the ranker with the inverted index, query for the top 10 documents and return the top 10 documents, and their score. For the iteration feature, the backend will utilize the qrels.txt and queries.txt that were uploaded when uploading the dataset. It will return a list of list: 1. results[0] = list of top k articles 2. results[1] = list of ndcg 3. results[2] = list of running avg ndcg 4. results[3] = list of queries Reference Chase Geiglem, 2017. [2-search-and-ir-eval.ipynb] (https://github.com/meta-toolkit/metapy/blob/master/tutorials/2-search-and-ir-eval.ipynb).
https://github.com/Wenfan1993/CourseProject	CS410 Text Information Systems - Team: Fan & Jain Rhymes Project Proposal Team Members: Member Name NetID Email Wenxi Fan (Captain) wenxif2 wenxif2@illinois.edu Abhishek Jain aj26 aj26@illinois.edu Competition to join: Text Classification Language to use: Python Our team's key motivation to join the Text Classification Competition is to go beyond what has been taught in the course, study state-of-the-art NLP tools and apply it on a real-world problem. This competition gives us such an opportunity where we cannot just learn the latest tools by solving a real-world problem but also it challenges us to raise our bars to implement optimal solutions to meet the baseline scores. We are extremely excited to learn, participate and give our best performance. In our current professional engagements, both of us are part of AI teams respectively where machine learning tools & techniques are used for problem solving. Neural networks being the most advanced & extensively used methodologies in our respective enterprises, we both have got a chance to work and closely observe best practices & implementation styles using it. We have high level understanding and hands on experience of ANN, RNN, CNN & LSTM architecture styles and its realization using TensorFlow Keras & PyTorch based implementation. For text related problems we have awareness on usage of word embeddings viz. Word2Vec & GloVe. With the latest developments in NLP, we are aware of the release of newer advanced tools like BERT, OpenAI GPT1/2/3, HuggingFace Transformers which we have not yet explored. These tools are also in our consideration list for approaching the problem solution. Wenxi was engaged in building RNN models for contract terms cleaning and classification, and in building the Seq2Seq model for document cleaning and document state mapping. Abhishek has experience of working closely with data science teams which utilizes neural networks-based framework implementation to solve several problems like task sequence prediction, call conversation compliance, sales forecasting, customer look-a-likes and segmentation in customer relationship management domain. For this project, we will explore the sequential text features, and learn the associations between words in the short text. We will plan to explore the models that combine the text ranking algorithms and machine learning models (e.g. TF-IDF and SVM). Then, we will plan to build models with Recurrent Neural Networks, or Convolutional Neural Networks or a combination of both and learn to improve the model performance with different architectures. We also plan to explore attention-based models and leverage attention models/hierarchical attention models to explore the performance impact. We will compare the different models and select the model with the best performance. CS410 Text Information Systems - Team: Fan & Jain Rhymes Progress report Task completed: Below are the tasks we have completed so far: 1. Solution Architecture Design & Approach Finalization: Team planned to use several state-of-art architectural styles to experiment & validate the results & compare different approaches results. Approach is to start from conventional styles and gradually progress to use advance methodologies. a. Conventional ML methodologies b. Deep Learning based implementations c. Attention & Transformers based implementations. 2. Environment setup a. Analyzed different environment viz. local desktop, Google Colab and Cloud to setup experimentation playground. Cost effectiveness & high processing needs were the key parameters considered. b. Colab Pro environment was preferred over others which enabled us using High Memory & GPU/TPU based processing for Deep Learning based implementations. 3. ML Pipeline setup: a. Data Import: Training & Test data was imported to google drive & authentication setup was done to access it. b. Data Preprocessing: Data clean step is performed to address words spelling error, repetition, signs & emoji. c. Feature Engineering: Several features were constructed to support solution approach as multi sentence sequence & single document classification problem. d. Model Training/Fine Tuning: Select, train, and evaluate the model. For pre-trained models fine tuning step was performed considering different solution classification styles. e. Prediction: Output the predictions. 4. ML Models: Below are several modelling strategies which team has evaluated for given Classification Problem. a. TF-IDF + dimensionality reduction (via SVD) + Tree (Random forest). b. General LSTM model (with one or multiple LSTM layers followed by fully connected layers). c. Transformer (attention-based) models leveraging hugging face pre-trained models: Roberta, BERT, XLM, based on which we fine-tune for our task. 5. Observations: We have compared all the approaches & results are per our expectation as below: - Conventional ML (TFIDF/SVD/Random Forest) based implementation was not able to beat the baseline score. - Deep Learning (LSTM based) implementation - 2-3 LSTM layers followed by 2-5 fully connected layers, which implementation was not able to beat the baseline score. - Transformer based approach: CS410 Text Information Systems - Team: Fan & Jain Rhymes o Bert base: Just at par with Baseline results but ranked intermediate on leader board. o Roberta base: Performed very well with highest accuracy, precision & F1 values (achieved 72.3% accuracy, 80.4% recall, 76.2% F1) Task pending: Below are the tasks we plan to complete before the final submission: 1. We are planning to further explore other models for our task, that includes: GPT-2, GPT-3. 2. We are planning to complete the documentation of our pipeline to be prepared for the final submission. Challenges faced: Below are the challenges we faced: 1. Noises in the data: The input data appears to have a lot of noises - words spelling error, repetition, words/signs/emoji that appear to occur very few times. In addition, there are many words/signs in test sets that are not included in the training set. These create challenges in extracting useful information out from the inputs and challenges in generalizing the model to the data not included in the training process. 2. The way to leverage context/response to engineer effective input features: Currently we use the context/response by inputting them as two separate features or concatenating as one (and/or reversing orders). Although we leveraged the transformer models that generate attention mechanism(both on words and position of words), we have not yet been able to explore a way ourselves to engineer features that might be more effective as inputs than pure words and sentences. The example of such inputs could be sentiment of sentences, the hierarchical attention from character-level to sentence level. 3. Hyper-parameter tuning We found challenges in hyper-parameter tuning, especially when the parameter space is large. We used 'Trainer' (utility from transformers) that helped automate the parameter search, but still the parameter space is large, and getting the best model may take time. 4. Large Model We are unable to tune large transformer models like ROBERTA_LARGE & BERT_LARGE models due to GPU memory issues. 5. Select the performance criteria We found challenges in selecting models that generates well on test data - although we used validation set in addition to test set (which sets are separated out from training data), the model that test well (good F1) on validation and test set may not generalize well to the other data not seen in the training process. Instead of just using F1 score, the one datapoint, as the criteria to judge the performance, the below steps were performed that achieved better performance criteria (which gives better picture on the performance of model): CS410 Text Information Systems - Team: Fan & Jain Rhymes a. Evaluate the distribution of scores on multiple batches of the validation set, to analyze the level and consistency of the performance. b. In addition to evaluate precision/recall/f1 the level of performance, we could also evaluate from a different perspective, such as analyzing the correlation between prediction and ground truth. Colab Notebook Final version of our project code is available as Google Colab Notebook which is shared and available at SequenceClassification. Through this code, we trained and fine-tuned the RoBERTa based transformer model for the text classification competition (identifying if the tweet is SARCASM or NOT_SARCASM), and we are able to achive f1 score higher than the base-line. Below are the several steps detailed to be execute to run the code end to end. Since modelling methodology is Transformer based it would be recommended to use GPU for processing. Google Colab is the preferred environment to run the end to end process and generate the results. Use of our model We have created the checkpoint our model, available to download from the below path. For use of our model, user can just just load the checkpoint to the API 'AutoModelForSequenceClassification'. (See https://huggingface.co/transformers/model_doc/auto.html for more details) For the use case of our text competition (and as an example of using our model), please refer to colab notebook RoBERTa_Model_Test fpr using the checkpoint of our model and reproducing answer.txt. Tutorial: Reproduce_Answers_with_Checkpoint Checkpoint available to Download from: Checkpoint See slide 11 for detailed step-by-step guidance Presentation Presentation and Model Testing Please see Presentation for the slides we created for the presentation. Please see video Tutorial: Reproduce_Answers_with_Checkpoint for step by step guidance how to re-run and test our model Contribution of Team Members Each of us collaborated very closely in each step of reseach, experiment, and improvise. We have touch point scheduled on a weekly basis, where we shared the learning and resources, discuss our approach, and walk-through our codes. With that, each of us contributed 100% effort in each step of the project process. Our team members are: Wenxi Fan (NetID: wenxif2; Email:wenxif2@illinois.edu) Abhishek Jain (NetID: aj26; Email:aj26@illinois.edu) Below are the Steps in our Model Training and Output Generation (as in SequenceClassification.) Environment Setup First setup the environment, we will do the following steps here. - Transformers Model Installation - Hyper Parameter Tuning Library Installation - Colab Setup You will be required to authorize the code using your google account. Copy the authorization code generated and pass it in the notebook in the input box provided when you run mount drive code. Below is the reference: ```py Colab setup from google.colab import drive drive.mount('/content/drive', force_remount=True) ``` Also please copy the train & test JSONL files provided in your google drive required for training and testing the models further. Tutorial: Environment Setup Data Load Next step is to load the Training & Test Datasets as Pandas dataframe. Please update correct data path where training and test dataset is copied in your google drive. py datapath = r'/content/drive/My Drive/mcsds/cs-410-text-mining/project/ClassificationCompetition/data' train_pddf = pd.read_json(datapath+'/train.jsonl', lines=True) test_pddf = pd.read_json(datapath+'/test.jsonl', lines=True) Above example suggests my train & test jsonl files are copied in my drive at '/mcsds/cs-410-text-mining/project/ClassificationCompetition/data' location. Further run the data load section. Reference: Data Load & Preprocessing Data Preprocessing Next step is to run the data preprocessing steps. Below are the different components of it: Feature Engineering Create new features: * Last Response: Extract the last response from the context since the current response was generated on Last this can be separately treated. * Context Reversed: Reverse the context before feeding to transformers so that latest tweets are given more attention and incase if context is too big latest shall be considered. * Combine all into a single * Combine Current & Last Response into Single Sequence Structuring Define how do we want to structure the different tweets, basically two approaches are followed: * Combine into single: Last response only, Combine all tweets togeather or current and last. * Two Sentence: (Current, Last Response) or (Current, Context Reversed). Transform to Datasets Translate preprocessed dataframes to Transformer Datasets. This step is required to make our dataset translated into Transformer datasets construct. Reference: Data Load & Preprocessing Model Configurtion Configure which model strategy to select, train test valid splits, performance metrics, training batch sizes etc. Below are the details: 1. model_checkpoint: which model to use for text sequence classification. Roberta models are observed to give the maximum performance. 2. task: specify how to structure the sequences as described in sequence structuring step. We have observed the maximum performance with 'response_context_rev_sep' structure. This format structures input as two sequence where response is last tweet to be classified, and context tweets are the previous tweets in an reversed order of occurance. 3. metric_name: metric to be optimized while training. We have configured it to accuracy. 4. num_labels: 2, number of classes Sarcasm, Not Sarcasm 5. batch_size: 16 for roberta, 64 for bert otherwise we face out of memory issues. 6. train_test_split: to divide training data into train and test datasets. 7. test_valid_split: to divide test dataset into test and validation set. 8. epoch: number of epochs to train model on. 9. weight_decay: determines how much an updating step influences the current value of the weights 10. learning_rate: weight update rule that causes the weights to exponentially decay to zero Reference: Model Config Tokenization This step translates words to context tokens. Transformers Tokenizer tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires. Reference: Tokenization & Single Model Fine Tuning Single Model Fine Tuning Download the pretrained model and fine tune the selected model with arguments configured in the previous step. Reference: Tokenization & Single Model Fine Tuning Reference: Training Results Test Validation Validate the results on test data and compute the metrics. Reference: Validation Results Hyper Parameter Tuning Could be only run with HIGH GPU environment Using Transformer Trainer utility which supports hyperparameter search using optuna or Ray Tune libraries which we have installed in our previous step. During hyperparameter tuning step, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized and the hyperparameters it used for that run. Reference: Hyperparameter Tuning Best Run Selection & Training Could be only run with HIGH GPU environment To reproduce the best training run from our previous hyper parameter train setp we will set the best hyperparameters TrainingArgument before training the model again. Reference: Hyperparameter Tuning Text Classification Competition - Attention-based Transformers CS410: Text Information System Team: Fan & Jain Rhymes - Wenxi Fan (NetID: wenxif2; Email:wenxif2@illinois.edu) Abhishek Jain (NetID: aj26; Email:aj26@illinois.edu) Agenda: Project Goal and Our Approach Code Walkthrough Run the Code Test Our Results Project Goal Project Goal: Implement text classifier to be able to identify if the twit is sarcasm or not. Inputs: text (tweets) - Response : the tweet response to be classified Context : the conversation context of the response Labels: SARCASM or NOT_SARCASM (two classes) Goal: text classifier implemented to achieve f1 score >70% Motivation: Research, learn and implement best-in-class next generation techniques to approach text-based classification problems. Expand our knowledge & understanding in NLP space beyond what is taught in the course. Approach - Overview Research Goal Research available best-in-class NLP techniques Learn & understand the application of each techniques Outcome Attention-based Approach to Text Classification https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/AttentionBased%20Approach%20to%20Text%20Classification.pdf The Application of Attention Models in Text Classification https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/Application%20of%20Attention%20to%20Classification.pdf Experiment Goal Evaluate different NLP techniques & short list for final implementation Outcome Initial experiments conducted for baseline results TFIDF/SVM Bi-directional LSTM with embedding Transformer based Pre-Trained Model) Improvise Goal Optimize the best technique researched & tune it for result generation. Outcome Transformer based implementation were finalized Tuned with different input structures Used different pre-trained models like Bert, Roberta, XLM etc. Approach - Research Research Focus: Attention Mechanism, a breakthrough principle which has revolutionized building state-of-the-art NLP solutions. Detailed overview on different solution architectures styles which can be used to solve text classification problem. See below reference to our research documents: https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/AttentionBased%20Approach%20to%20Text%20Classification.pdf https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/Application%20of%20Attention%20to%20Classification.pdf Approach - Experiment Approach - Improvise (1) Feature Engineering & Problem Structuring Input sequence structured as: Single Document Classification All tweets combined as single document for classification Response and Last tweet combined as single document for classification Sequence Classification Tweets modeled as two text sequence for classification Response, Last Tweet Response, Context Response, Context reversed Best results observed when problem modelled as Sequence Classification setup with text sequence as Response and Context reversed. Approach - Improvise (2) BERT and RoBERTa BERT Model - Bidirectional Encoder Representations from Transformer Designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers; Has certain limitations, e.g. BERT is limited to a particular input length for long document is split into segments of a fixed size with overlap. RoBERTa - Robustly optimized BERT approach RoBERTa is the enhanced transformer based on BERT approach RoBERTa is trained with dynamic masking, FULL-SENTENCES without NSPloss, large mini-batches, and a larger byte-level BPE. (2) RoBERTa overcomes several limitations of BERT including the one mentioned above. Approach - Improvise (3) Model Overview Roberta Embeddings: Word embeddings Position embeddings Token-type embeddings Roberta Encoder (12 Roberta Layers) Each layer includes: Roberta Self-attention based on query, key, value Roberta Self-output with dense layer, layer norm and dropout Roberta Intermediate with dense/linear layer Roberta Classifier: with dense/liner layers For more information, please refer to reference (1) Code Walk-through Steps: Environment Setup (Transformers model installation, Hyper-parameter tuning Library installation, Colab set-up) Data load and preprocessing Features: we explored using the response, latest context, response and latest context concatenated, context reversed, and the combination of such, and turns out using the below features gave the best performance Response Context reversed (starting from latest dialog) Label: 1 for SARCASM; 0 for Non-SARCASM Model Configuration Choose roberta-base model Set up the evaluation metrics, batch-size, train-test split ratio, number of epochs Tokenization Use API AutoTokenizer Single model finetuning Set AutoModelForSequenceClassification, TrainingArguments wrapped in Trainer object Hyperparameter tuning Use hyperparameter_search method of the trainer object Selected the best run Select the best hyper-parameter that maximize the metrics (accuracy) Test and compute the accuracy, precision, recall and f1 of the best model Generate the outputs for submission See Colab file 'SequenceClassifier.ipynb' for the script that includes these steps https://colab.research.google.com/drive/1nhsCc1krBzPR6LKg3Qfwq_cxHv4sr_Ib?usp=sharing Run the Code Test Steps: Tutorial available at https://drive.google.com/file/d/1aa75KqPg4qnEJ5HYpZrW51ETR_UA0IDN/view?usp=sharing Open Colab notebook 'RoBERTa_Model_Test' at the below link: https://colab.research.google.com/drive/1S9g8dD7JmuT6JsJo1ysAa4e3nTCNakxk?usp=sharing Also uploaded to GitHub Repo GitHub Repo path: https://github.com/Wenfan1993/CourseProject Load the below items from the GitHub Repo to Colab environment at '\Content\' test.jsonl (Uploaded to GitHub Repo) https://github.com/Wenfan1993/CourseProject/tree/main/Test_Source_Data check_point.pth (the checkpoint where we stored our trained model) https://drive.google.com/file/d/1z1IIeU1e7DgqtAyyPWE66QyAG7h1D-sT/view?usp=sharing Run the Colab notebook The output 'Answer.txt' will be output and stored at Collab '\Content\' Our Results We passed the base-line! As of 12/11, 11:00PM CST, we ranked No.5 at the leaderboard, We achieved precision 72.3%, recall 80.4% and f1 76.2%. We are here Appendix References: (1) https://huggingface.co/roberta-base (2) RoBERTa: A Robustly Optimized BERT Pretraining Approach, by Linhan Liu et. al. https://arxiv.org/abs/1907.11692 GitHub Repo path: https://github.com/Wenfan1993/CourseProject Thank You! For any question, please feel free to contact our team members: Wenxi Fan Email: wenfan1993@gmail.com/Wenxif2@illinois.edu Abhishek Jain Email:aj26@illinois.edu
https://github.com/Xinpeij/CourseProject	CS410 Project Proposal Team Members: Y= Xinpei Jiang, xinpeij2 (individual team) Topic: Y= Text Classification Competition l I plan to learn and use state-of-the-art neural network classifiers in the competition. l Such state-of-the-art methods might include deep neural networks like CNN, RNN, LSTM, or the NLP transformer models like Google's BERT, l I also plan to learn and use deep learning frameworks like TensorFlow in which I might apply libraries like Keras. l I plan to use Python for this project. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Xinyihe123/LARA	1) Progress made thus far: Our group is working on reproducing a paper of Latent Aspect Rating Analysis. The progress we made thus far includes the followings. All of our group members watched the lecture videos and read through the paper in order to understand the concepts of LARA and its fundamental difference with LRR. We have found sample Java code online implementing LRR and will start from here, write our own Python code implementing LARA. We have collected data sets and finished the preprocessing: remove the reviews with missing aspect rating or document length less than 50 words, convert to lower cases, removing punctuations and stop words. 2) Remain task: We still need to read paper and understand the equations, and to finish our implement of the aspect modeling module. The model also needs validation, and we need to use it to produce results similar to the paper. 3) Any challenges/issues being faced The equations provided in the paper are complex, it takes lots of time to try to understand them. Besides, we also need to read the paper described LRR model to implement LARA. Since the functions are complex, this task requires heavily coding and debugging. Course Project Proposal Topic: Reproduce A Paper 1. Team member: a. Xinyi He, netid: xinyihe4; Dingsen Shi, netid: dingsen2; Qunyu Shen, netid: qunyus2; b. Captain: Xinyi He 2. The paper we choose is : Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3. We plan to use Java, since the model we need to reproduce according to the paper is based on LRR, whose source codes language is Java. 4. We find the datasets used in the paper from http://times.cs.uiuc.edu/~wang296/Data/ CS410 CourseProject -- Reproduce a paper Documentation: Team LARA Datasets from http://times.cs.uiuc.edu/~wang296/Data/ References: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. The codes in LRR are downloaded from Internet. These are references. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. Presentation: https://mediaspace.illinois.edu/media/t/1_fo2gtfej files: clean.py: data preprocess: First we remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects). Then we convert all the words into lower cases and remove punctuations and stop words. In vocab.txt we write vocabulary appearance based on reviews. If a word appears in several times in the same review, it would only be counted as once. We then filtered out words that have less than ten occurences. load.py: build matrix for reviews and generate results. Load data for testing. lara.py: The LARA model, mainly the aspect modeling part, using EM algorithm.In this program, we implemented function such as update_mu, update_beta, E_step, M_step etc. Gererated the alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector. The overall rating for the review can be drawn from the Gaussian distribution with mean alpah.T dot product s, and variance delta. Data: The test data we use, download from http://times.cs.uiuc.edu/~wang296/Data/: TripAdvisor Data Set: JSON Results: High rating words collections and the aspect rating weight for each reviews represented as matrix. LRR : Downloaded from Internet. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. The LRR model implemented by Hongning Wang using Java We used some codes in LRR, since the LARA we built is a generative model of LRR. We used some parameter initilizing codes in LRR. Required packages in Python3: numpy scipy nltk Project Members: Xinyi He Weijiang Li Dingsen Shi Qunyu Shen Ziyuan Wei We decided to work with another team to build this model since the challenge we were facing when trying to understand the methods and alogrithm are extremely hard. Implementation of Model: The inputs of this model for each review d are: (epsilon, gamma, beta, mu, sigma, delta), with hidden parameters: sigma_inverse, alpha, sigma_square, eta, phi. Epsilon, gamma, beta, mu, sigma, delta, phi, alpha, eta and the rating vector r are used in calculate log-likelihood. The outputs are alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector, used for gererate the final overall ratings of review d. The detail of this model can be found in the paper. This model involves more than ten parameters, some of them are generated by mutivariate Gaussian distribution, variational distribution, Dirichlet distribution, and multinomial distribution, and they are updated using gradient based method, which are hard to implement and transfer the complex math equations into codes. We spend most of our time on reading and understanding the paper and the math methods in the paper. Thus although we spent more than 20*5 hours on this project, we can only produce a simple and crude model and test one dataset in the paper. Team contribution: Since each steps and parameters in the EM algorithm are closely related to each other, we usually coded and debuged together through zoom, all of our teammates contributed their 100% effort on this project.
https://github.com/ZhengyuLi97/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/acscharf/CourseProject	"Alexander Scharf CS410 (Fall 2020) Course Project Background : After learners complete an online business course, they are prompted to enter a ""reflection"" on how they can apply the knowledge from the course to their job or daily life. These reflections are shared with other learners so they can deepen their understanding, learning how others applied their learning. This project aims to: 1.) to analyze ""useful"" and ""not useful"" reflections, finding syntactic elements that make up each 2.) gather user input for a user reflection and predict whether that reflection is ""useful"" or ""not useful"" via a web application The project uses a real data set from an online learning service with reflections in both English and Japanese language. 1.) Progress made thus far -The data set has been extracted for both English and Japanese text -The labeling of the data set (""useful"" or ""not useful"" is being done manually and took longer than expected just for English, but is nearly complete -Decided to use spAcy as framework and researched its usage as part of tech review -Ran preliminary analysis on data set (number of words, parts of speech, common words) for both ""useful"" and ""not useful"" reflections -Coding for training model complete by referencing spAcy sample code -Preliminary web app created and hosted on personal server (take a look at http://alexscharf.com/) 2.) Remaining tasks -Complete labeling the English data set -Begin labeling the Japanese data set -Make adjustments to the code as needed for the Japanese data set -Make the web app slightly more user friendly -Conduct user tests -Update documentation and clean up code to remove debugging 3.) Any challenges/issues being faced -I thought I could outsource labeling the training data with Amazon Mechanical Turk, but labeling it required too much domain knowledge - taking more time than I thought -Unrelated to the direct goal of this project, but fiddling with the public facing web server took longer than expected since I don't have experience in this area. Ended up switching from Apache to Nginx -Labeling the Japanese data set will take more time than expected. I plan labeling a train data size of 1000 reflections for English, I may do half of that for Japanese -Still a bit unknown how much rewriting the original application will be necessary for Japanese or any other strange bugs like character encoding, especially with web app Alex ScharfProject Proposal, CS410Free Topic - English and Japanese Course Reflection Analysis 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members.Individual project, Alex Scharf (acscharf2)2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work?I am the product owner of GLOBIS Unlimited (GLOBIS Manabihodai) in Japanese, an online course platform focused on business content with currently over 200,000 learners. After learners complete a course on the platform, they are prompted to enter an optional reflection about how they can apply the learning from the course to their job or life:Some of the responses are quite good and well-thought out (OI am an engineer, and I can use logic trees to help me create test casesO) and others are not as insightful (OLogic trees are goodO). After learners post their reflections, they can view the reflections of other learners and OLikeO ones they found useful. As learners see the reflections of others, it is important that the reflections be of high quality, and hence useful to a wider audience.Because the service is in English and Japanese, data exists for both Japanese and English reflections.I would like to do the following:A.) Identify language and parts of speech that are common in reflections for both English and JapaneseB.) Create an application that predicts whether an input reflection is Ohigh qualityO or Olow qualityO for English onlyC.) Host that application on a public web serverThis task is important and interesting for the following reasons:- Real-world challenge based upon actual data set- Creates a proof of concept for a feature that would give learners feedback about the quality of their course reflection, leading to great personalization in online learning- Creating a user-facing application that potential users could interact with- Working across two language (Japanese and English) that are very differentMy current plan is as follows:1.) Extract data set for both English and Japanese2.) Choose the most appropriate way to score the data set (manual approach or use OlikesO)3.) Find tool that can analyze and make predictions for both English and Japanese4.) Run analysis of common words in reflections to satisfy point A5.) Train predictive model on current data set6.) Create application that allows user input and compares against predictive model7.) Host application on a publicly accessible web server8.) Run user test with 4 people to see if their experience is in line with expectationsI expect that the application should be able to identify OgoodO and ObadO reflections to a certain degree. I plan on evaluating my work if the test users enter a OgoodO reflection and a ObadO reflection and the application is in line with their expectations.Here is what I imagine the application flow to look like:1.) User is prompted for reflection input and submits2.) System checks user input against trained data and decides whether it is Ohigh qualityO or Olow qualityO3.) Feedback is sent back to the user3. Which programming language do you plan to use?I plan on using Python4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task.- Research tool that is appropriate for both Japanese and English (2 hours)- Extract data set and prepare in proper format (3 hours)- Score the data set in appropriate format (3 hours)- Create application to run semantic analysis of reflections (5 hours)- Create application to train predictive model (5 hours)- Create front-end application that allows users to input data against predictive model (5 hours)- Host front-end application on publicly accessible web server (2 hours)- Prepare for user test (1 hour)- 4 user tests x 30 minutes (2 hours)- Create full documentation (2 hours)- Summarize findings (2 hours)Total: 32 hours English and Japanese Course Reflection Analysis and Prediction About After learners complete a video-based online business course, they are prompted to enter a ""reflection"" on how they can apply the knowledge from the course to their job or daily life. These reflections are shared with other learners so they can deepen their understanding, learning how others applied their learning. This project aims to: 1.) to analyze ""useful"" and ""not useful"" reflections, finding syntactic elements that make up each, and create a trained model (train.py) 2.) gather user input for a user reflection and predict whether that reflection is ""useful"" or ""not useful"" based upon trained model (webapp.py) Both 1.) and 2.) are done for both English and Japanese language. Project Presentation https://www.youtube.com/watch?v=JN-Gm5Pj-hs Try It A live version of the software is hosted below, complete with sample videos. Try a ""useful"" and ""not useful"" reflection and see if it matches your expectations. The sample reflections will give you an idea of what might be considered ""useful"" and ""not useful."" English http://alexscharf.com/ Japanese http://alexscharf.com/ja train.py (Training Application) Overview Reads labeled CSV for reflection data, analyzes reflections, trains a model, and saves that model to disk. Analysis looks at parts of speech (by percentage), common words, and average word counts for both ""useful"" and ""not useful"" reflections. Implementation The application has three key functions, explained below: read_csv(filename, rows) Opens a csv with the name of ""filename"" and reads the first number rows specified by ""rows."" The CSV should have two columns, the first with a label of '1' if the reflection is 'useful' or '0' if it is ""not useful."" Strips whitespace and returns a pandas DataFrame. analyze_reflections(reflections, nlp, language) Analyze reflections when provided with a pandas DataFrame, spaCy NLP object, a string to display for output. Iterates over each unigram for both useful and not useful reflections, counting parts of speech, common words, and average length while ignoring whitespace. Outputs the result using the print function. Example output: https://github.com/acscharf/CourseProject/blob/main/example_output.txt train_reflections(reflections, nlp, n_iter, n_texts): Trains model based upon label reflections data with a pandas DataFrame, spaCy NLP object, and the number of iterations and items in the reflection data. Holds 20% of the labeled data for evaluation, training off of the remaining 80%. Prints loss, recall, precision, and f-score for each training iteration. Currently build using the ""simple_cnn"" architecture provided by spaCy. Usage The application requires the spaCy and pandas libaries as well as the ""en_core_web_sm"" and ""ja_core_news_sm"" spaCy models. Additionally, the software needs the english.csv and japanese.csv labeled reflection datasets in a ""data"" subfolder. These data sets were labeled ""useful"" or ""not useful"" by me and reflect actual user output. The data set for this project can be found in the below repository: English https://github.com/acscharf/CourseProject/blob/main/data/english.csv Japanese https://github.com/acscharf/CourseProject/blob/main/data/japanese.csv After completion, the program saves a model to disk in the ""english_model"" and ""japanese_model"" subfolders. Assuming the provided csv files are included, the program can be run as-is with no additional parameters. webapp.py (User-facing Web Application) Overview Flask-based web application that loads training model and gathers uset input to predict usefulness of reflection. English version can be accessed at the main directory (/), while Japanese version can be accessed via a subdirectory (ja). Implementation The program is implemented with Flask, mixing Python and HTML. There are two pages, and submission page and a results page, both in English and Japanese. The submission page is pure HTML and Javascript. The results page takes the submission from the previous page a parameters, loads a spaCy model created by the train.py application, and runs the user submission against the trained text classifier to guess whether the submission is ""useful"" or ""not useful."" This output is displayed to the user, along with some generic hints for a useful reflection inferreed from analyzing reflections through the training application. Usage The application requires the spaCy and flask libraries as well as a training modeled generated by train.py. The app can be launched with the following commands: export FLASK_APP=webapp.py flask run This will start a development server on http://127.0.0.1:5000/. A working version can be found at http://alexscharf.com/ Other files Proposal.pdf Project proposal Progress report.pdf Mid-term progress report example_output.txt Example output of train.py analysis waitress_server.py Configuration file for production web server Self-evaluation Have you completed what you have planned? I was able to complete complete all the planned outcomes as mentioned in the original project proposal. In fact, I went beyond the project proposal by including generating a training model and a web front-end for Japanese as well as English. I initially did not include this in the original proposal because I was not sure of my ability to correctly label the Japanese data, but I found a subset of the data (for an accounting course) that allowed me to do so, and hence exceeded the original project proposal. Have you got the expected outcome? The outcome for the reflection predicter is as expected. As originally proposed, I conducted user tests to see if the program functions to their expectations. Their feedback was as follows: - The application is very good at filtering out obviously bad reflections (""The course was interesting""). This is very useful, as these low-quality responses have the largest user impact - The application can still be ""tricked"" by writing grammatically correct and keyword packed sentences that ultimately have little meaning (""I love studying business and applying business for my presentations. It helps me succeed at work with my boss and also with my coworkers."" gets a perfect score). This is not intended to be a grading mechanism, however, so tackling these it outside the scope of the project. The outcome for the reflection analyzer was also insightful, but not as much as expected. As expected, better reflections tend to have more words (around 22 on average, compared with 9 for not useful ones). However, the parts of speech and common words were quite similar for ""useful"" and ""not useful"" reflections, suggesting that to do a heuristic analysis of reflections, much deeper insight is needed and training a model is a much more effective approach, justifying the original project."
https://github.com/adeetikaushal/CourseProject	"Team: Voltron Topic: Book Recommendation System 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name NetIds Captain Adeeti Kaushal Adeetik2 Yes Vivek Bansal Vivekb3 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Book recommendation system. Description: In the course, we discussed about Recommendation systems and Search. There are two main recommendation system approaches that were discussed further, Content-based filtering and Collaborative based filtering. In this project, we are focusing on simple content-based book recommendation system. The content will be downloaded from public sites (Project Gutenberg). Tasks involved: The tasks involve loading the content of book into python after downloading it. Search for relevant content/words in the loaded data. Tokenize the corpus and perform stemming on the tokenized corpus. Next step would involve building bag of words model and find stop words, build term frequency-inverted document frequent model and show the results of tf-idf model. Compute the distance between texts. Look into search criteria and find similar books matching the content using Cosine Similarity. Important and Interesting: This topic is important and interesting because big corporations like Facebook, Amazon, Apple, Netflix, Google (FAANG) and others big companies use recommendation system to show/target the content based on similarities in content. This is important for business to present or build the resources that users are searching and show them similarities between other books. Planned Approach: The approach that we plan to take involves collecting the books from Project Gutenberg which offers free books and that will be used as our dataset. We will find a topic/search term or title that will be used to create the model and then then find similarities in other books. Python libraries will be used to perform various tasks like stemming, tokenizing etc. The outcome would involve books with similarities. Tools, Systems, Dataset: Python Libraries that can be used for NLP, tokenization, stemming, parsing, classification etc. which are important tasks. Jupyter Notebook will be used to read the dataset and execute the python program. From Project Gutenberg, free books would be downloaded to build the dataset. Expected outcome: Being able to retrieve/recommend similar books based on the content. Evaluation: Results retrieved from the program should give recommendation of books that are matching or similar in some way or form. For example, a crime mystery novel should not return any match with children book. 3. Which programming language do you plan to use? We are planning to use Python. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. We are planning to cover the following topics in the project. Tasks Estimated Time (hrs Learning Python 8* Research/Build dataset 5 Code for tokenization, Stemming on corpus 4 Build bag of words, find stop words 2 Build tf-idf 4 Build Similarity matrix 2 Full end to end integration, tuning 6 Testing 5 Visual representation of results (matplotlib) 6 Other use case - Content based Similarity 6 Report 4 Total 52 N=2 20*2 hours = 40 hrs project Estimated hours = 52 * Since, both need to learn Python, so we have added 8 hrs for learning but without that the efforts required for project are above the required hours. CS-410 Text Information System Book Recommendation System Team: Voltron Team Member Email Captain Adeeti Kaushal adeetik2@illinois.edu Yes Vivek Bansal vivekb3@illinois.edu Table of Contents INTRODUCTION ............................................................................................................................................................ 3 FLOW DIAGRAM .......................................................................................................................................................... 3 OVERVIEW OF TASKS .................................................................................................................................................... 4 ABOUT THE DATASET ................................................................................................................................................... 4 CODE .......................................................................................................................................................................... 4 SETUP ....................................................................................................................................................................... 4 Step by Step Code details ......................................................................................................................................... 5 SET SEARCH CRITERIA: .................................................................................................................................................. 5 LOAD DATASET AND SORT ............................................................................................................................................ 5 LOAD TITLES AND TEXT IN OBJECTS ................................................................................................................................. 6 LOAD INDEXES ............................................................................................................................................................. 7 TOKENIZE THE CORPUS ................................................................................................................................................. 7 PICKLING THE TOKENIZED CORPUS ................................................................................................................................. 8 STEMMING OF TOKENIZED CORPUS ............................................................................................................................... 9 BUILDING A BAG-OF-WORDS MODEL .............................................................................................................................. 9 VISUALIZE THE MOST COMMON WORDS ....................................................................................................................... 10 BUILD A TF-IDF MODEL ............................................................................................................................................... 11 RESULTS OF THE TF-IDF MODEL .................................................................................................................................... 12 COMPUTE DISTANCE BETWEEN TEXTS ........................................................................................................................... 13 SIMILAR BOOKS ......................................................................................................................................................... 14 BOOKS WITH SIMILAR CONTENT .................................................................................................................................. 15 INTRODUCTION The Book recommendation system aims to provide a selection to user based on the user's taste. Generally, a system would rely on user's metadata (for ex. Author of the book, theme etc.) to determine which book would user enjoy the most. However, when we take the same approach with full text search or full content, it becomes a very heavy dataset. In this project, we will try to demonstrate the same by providing recommendation based on the content. FLOW DIAGRAM Above picture shows the entire flow of the project. In dataset layer, we first downloaded the books (public) in text form from glutenberg.org. For our sample, we took 25 books and processed. In the data processing layer, we loaded the books as text and titles and performed clean up. We went through removal of stop words, grouped them together (created stem). Using the stem words and dictionary, BagofWords were created for each book. Finally, in data processing layer, we built the inverted Index model. In the end, we did the analysis using similarity calculations by creating the matrix and using the matlib, created the visualization and horizontal dendrogram. OVERVIEW OF TASKS Following tasks were performed to complete this project. Research/Build dataset Code for tokenization, Stemming on corpus Build bag of words, find stop words Build tf-idf Build Similarity matrix Full end to end integration, tuning Testing Visual representation of results (matplotlib) Other use case - Content based Similarity Report In this report, we will walk through the code step by step and also showcase the output of each step. ABOUT THE DATASET The dataset is collection of books that is manually downloaded from Project Gutenberg. For this project, around 20 books were downloaded and used to find the content by searching the Text using similarity matrix. CODE The code is written in Jupyter notebook and python3. Following are the 2 components: Recommender.ipynb - The ipynb is iPythonNotebook file (Jupyter file) that contains the code. In the step 1, we will start with setting up the search criteria. The criteria for searching book of interest is set here. Library - Downloaded data from Project Gutenberg SETUP Install the following tools/lang: Y= Jupyter Y= Python 3 Install following python/machine learning libraries Y= Glob: This is used fir filename and pattern matching. Y= Re: This is used for regular expression matching. Y= Nltk: Natural Language toolkit Y= Os: IT consists of functions interacting with operating system Y= Genism: It is the natural language processing library used for unsupervised topic modeling Y= Pandas: most important library used by data scientist for data analysis. Y= Matplotlib: Used for visualization Y= Scipy: used for numerical integration and optimization. Step by Step Code details SET SEARCH CRITERIA: bookInterstedIn = RelativityandGravitation LOAD DATASET AND SORT Y= Download the dataset and store it in a folder ""library"". Y= Read the .txt files from the library folder and load it into the memory using glob library. Y= Sort the files using sort(). Following Output was obtained after reading the files from the ""library"" folder ['library/Relativity.txt', 'library/ExperimentalMechanics.txt', 'library/ThePoetryofScience.txt', 'library/TheEinsteinTheoryofRelativityAConciseStatement.txt', 'library/TheGravityBusiness.txt', 'library/FromNewtontoEinstein.txt', 'library/The BoyPlaybookofScience.txt', 'library/SidelightsonRelativity.txt', 'library/RelativityTheSpecialandGeneralTheory.txt', 'library/TheEinsteinSeeSaw.txt', 'library/AetherandGravitation.txt', 'library/The EarthBeginning.txt', 'library/specialtheoryRelativity.txt', # The folder created below folder = ""library/"" # List all the .txt files files = glob.glob(folder + ""*.txt"") 'library/TheTheoriesof DarwinandTheirRelationtoPhilosophyReligionandMorality.txt', 'library/RelativityandGravitation.txt', 'library/ThoughtsonArt.txt', 'library/EinsteinTheoriesofRelativityandGravitation.txt', 'library/TheJuniorClassics.txt'] LOAD TITLES AND TEXT IN OBJECTS Y= Next step requires converting the data as information instead of string. For that purpose, open the files and encode them with UTF-8 signature (utg-8-sig). When reading the file using utf-8-sig, it will treat BOM as file info. Y= Further clean up the file and remove the non-alphanumeric characters. Y= After reading the files, store the text and tiles of the books in two lists and save them as titles and txts. Y= To remove the folder name and .txt extension from the file name, use the os.path.basename() and replace() functions. #define objects to hold text and titles content_txts = [] book_titles = [] #loop through each, read, encode, remove txt extension for n in files: f = open(n, encoding='utf-8-sig') val = re.sub('[\W_]+', ' ', f.read()) content_txts.append(val) book_titles.append(os.path.basename(n).replace("".txt"", """")) [len(t) for t in content_txts] Here is the Output of above code: [24297, 519663, 875023, 59083, 56364, 169246, 953601, 69853, 24297, 69011, 941247, 600351, 197572, 669416, 548935, 273437, 548935, 716050] LOAD INDEXES Y= In the next step, we need to store the index of the interested title from the ""titles"" list to a variable ""typeofBook"" Y= To verify, print the content of the ""typeofBook"" variable. # the list contains all the book titles for i in range(len(book_titles)): if(book_titles[i]==bookInterstedIn): typeOfBook = i print(str(typeOfBook)) Output: 14 TOKENIZE THE CORPUS Now that the information has been collected, we will tokenize the corpus and transform into list of individual words. This is important step as we will now perform following steps in this: Y= To filter our words for processing, we need to define the stop words Y= Use lower() method to convert the contents in ""content_txts"" Y= Breakdown the lower case text into individual words and python provides a method ""split()"" for the same. Y= Store the split word into another variable ""txts_split"". Y= Remote the list of stop words in ""stoplist"". Y= Store the resulting list into another variable ""texts"" Y= Print first few tokens for the searched books. # Define a list of stop words stoplist = set('for w a of the and to in to be which some is at that we i who whom show via may my our might as well project by gutenberg ebook'.split()) # Convert the text to lower case Y= The output containing 20 tokens is below: PICKLING THE TOKENIZED CORPUS Y= In this step, we will generate a stem for each token. Y= Further, we used the pickle library of python for serializing. Python pickle module is used for serializing and de-serializing a Python object structure. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. txts_lower_case = [txt.lower() for txt in content_txts] # Transform the text into tokens txts_split = [txt.split() for txt in txts_lower_case] # Remove tokens which are part of the list of stop words texts = [[word for word in txt if word not in stoplist] for txt in txts_split] # Print the first 20 tokens texts[typeOfBook][0:20] ['einstein', 'theories', 'relativity', 'gravitation', 'malcolm', 'united', 'states', 'other', 'parts', 'world', 'cost', 'restrictions', 'whatsoever', 'away', 'or', 're', 'under', 'terms', 'license', 'included'] # Create an instance of a PorterStemmer object porter = PorterStemmer() # For each token of each text, we generated its stem texts_stem = [[porter.stem(token) for token in text] for text in texts] # Save to pickle file pickle.dump( texts_stem, open( ""library/porterstem.p"", ""wb"" ) ) STEMMING OF TOKENIZED CORPUS Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Generally, it is also a part of queries and Internet search engines. In our use case, the words related to the concept of selection would be gathered under the select stem. As we are analyzing 25 full books, the stemming algorithm can take several minutes to run. We will then load the final results from a pickle file and review the method used to generate it. Output: BUILDING A BAG-OF-WORDS MODEL Now, we need to build the model using the stemmed tokens, it will be used by algorithms in next part. Y= We created a dictionary that contains universe of all words in our corpus of books. Y= Then, using the stemmed tokens and the dictionary, we will create bag-of-words models (BoW) of each of our texts. Y= The BoW models will represent our books as a list of all unique tokens they contain associated with their respective number of occurrences # Load the stemmed tokens list from the pre generated pickle file texts_stem = pickle.load(open(""library/porterstem.p"", ""rb"" ) ) # Print the 20 first stemmed tokens from texts_stem[typeOfBook][0:20] ['einstein', 'theori', 'rel', 'gravit', 'malcolm', 'unit', 'state', 'other', 'part', 'world', 'cost', 'restrict', 'whatsoev', 'away', 'or', 're', 'under', 'term', 'licens', 'includ'] # Create a dictionary from the stemmed tokens Output: VISUALIZE THE MOST COMMON WORDS For better understanding and interpret the results returned by BoW model, there is a need for visualization. This will help understand which stemmed tokens are present in given book and how many occurrences are found. To visualize the content, we need to transform the content into DataFrame using the libraries and display 10 most common stems for the book as searched book Osearch criteriaO. dictionary = corpora.Dictionary(texts_stem) # Create a bag-of-words model for each book, using the previously generated dictionary bows = [dictionary.doc2bow(text) for text in texts_stem] # Print the first five elements using BoW model bows[typeOfBook][0:5] [(0, 1), (1, 1), (5, 51), (6, 5), (13, 3)] # Convert the BoW model into a DataFrame df_bow_origin = pd.DataFrame(bows[typeOfBook]) # Add the column names to the DataFrame df_bow_origin.columns = [""index"", ""occurrences""] # Add a column containing the token corresponding to the dictionary index df_bow_origin[""token""] = [dictionary[index] for index in df_bow_origin[""index""]] # Sort the DataFrame by descending number of occurrences and print the first 10 values df_bow_origin.sort_values(by=""occurrences"", ascending=False).head(10) Output: BUILD A TF-IDF MODEL Next, we need to generate the TF-IDF (term frequencyDinverse document frequency) model from BoW model using the library function gensim's (TfidfModel()). TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This model defines the importance of each word depending on how frequent it is in this text and how infrequent it is in all the other documents. As a result, a high tf-idf score for a word will indicate that this word is specific to this text. We will compute the score and print the results from model. # Generate the tf-idf model model = TfidfModel(bows) # Print the model model[bows[typeOfBook]] Output: RESULTS OF THE TF-IDF MODEL In order to interpret the results of TF-IDF model, we will display the 10 most specific words for a book. In our case, we used ORelativity"" book. # Convert the tf-idf model into a DataFrame df_tfidf = pd.DataFrame(model[bows[typeOfBook]]) # Name the columns of the DataFrame id and score df_tfidf.columns=[""id"", ""score""] # Add the tokens corresponding to the numerical indices for better readability df_tfidf['token'] = [dictionary[i] for i in list(df_tfidf[""id""])] # Sort the DataFrame by descending tf-idf score and print the first 10 rows. df_tfidf.sort_values(by=""score"", ascending=False).head(10) Output: COMPUTE DISTANCE BETWEEN TEXTS Stemmed token that are specific to each book are returned by TF-IDF model. The topics defined on the book ""Relativity"" can be seen now (like, gravitation etc). With this, we have a model associating tokens to how specific they are to each book, we can measure how related to books are between each other. #Compute similarity matrix sims = similarities.MatrixSimilarity(model[bows]) # Transform results to DF sim_df = pd.DataFrame(list(sims)) # Add book_titles of the books as columns and index of DF sim_df.columns = book_titles sim_df.index = book_titles # Print matrix sim_df Output: SIMILAR BOOKS The output we have contains the matrix containing all the similarity measures between any pair of books from the library. This matrix will be useful to quickly extract the information like distance between two books or more. This is needed to display plots in a notebook %matplotlib inline # Select the column corresponding to v = sim_df[bookInterstedIn] # Sort by ascending scores v_sorted = v.sort_values(ascending=True) # Plot this data has a horizontal bar plot v_sorted.plot.barh(x='lab', y='val', rot=0).plot() # Modify the axes labels and plot title for better readability plt.xlabel(""Cosine distance"") plt.ylabel("""") plt.title(""Most similar books to ""+ bookInterstedIn) Output BOOKS WITH SIMILAR CONTENT This project/approach is a good fit and of use, if we want to determine similar books that match userOs interest. For example, if user picked up the book ""Relativity,"" user can read books discussing similar concepts such as ""Special Theory of Relativity"" or ""Relativity and Gravitation If you are familiar with EinsteinOs work, these suggestions will likely seem natural to you. However, we now want to have a better understanding of the big picture and see how EinsteinOs books are generally related to each other (in terms of topics discussed). To this purpose, we will represent the whole similarity matrix as a dendrogram, which is a standard tool to display such data. This last approach will display all the information about book similarities at once. For example, we can find a book's closest relative but, also, we can visualize which groups of books have similar topics # Compute the similarity matrix the WVMA (Ward variance minimization algorithm) Z = hierarchy.linkage(sim_df, 'ward') # Display the results in horizontal dendrogram a = hierarchy.dendrogram(Z, leaf_font_size=10, labels=sim_df.index, orientation=""right"") Output: CS-410 Text Information System Fall 2020 Book Recommendation System Progress Report Team: Voltron Team Member Email Captain Adeeti Kaushal adeetik2@illinois.edu Yes Vivek Bansal vivekb3@illinois.edu Progress Report 1) Which tasks have been completed? Y= All tasks are complete, and report and presentation is submitted for review. 2) Which tasks are pending? Y= No task pending from the list and if any feedback is received, we will make changes accordingly. 3) Are you facing any challenges? Y= No. Tasks Status Learning Python Completed Research/Build dataset Completed Code for tokenization, Stemming on corpus Completed Build bag of words, find stop words Completed Build tf-idf Completed Build Similarity matrix Completed Full end to end integration, tuning Completed Testing Completed Visual representation of results (matplotlib) Completed Other use case - Content based Similarity Completed Report Completed - Submitted for Review Presentation Completed - Link checked-in to repo Total Completed Project Code: https://github.com/adeetikaushal/CourseProject/tree/main/code Project Report: https://github.com/vivekb3Illinois/CourseProject/blob/main/CS410_Voltron_Final_Project_Report.pdf Project Progress Report: https://github.com/vivekb3Illinois/CourseProject/blob/main/CS410_Voltron_Process_Report.pdf Project Video: https://mediaspace.illinois.edu/media/t/1_h258rjku"
https://github.com/akhilbhamidipati5/CourseProject	"Progress Report Our group has made some reasonable progress on our project, which is reproducing the paper outlined in  Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback.  Right now, we have our response data: our time series data which we arbitrarily chose to be the closing price of Facebook, Apple, Microsoft, Tesla, and American Airlines stock from 1/2/2018 to 10/30/2020. We have also decided what we will be using as our text data: the most popular tweets surrounding a ticker symbol tag on twitter. In our case, $FB, $AAPL, $MSFT, $TSLA, and $AAL. To get our document collections, we have begun using Tweepy, a twitter API, to create a collection of top tweets containing each ticker symbol along with the day they were tweeted. We have also written out the pseudocode for the Iterative Topic Modeling with Time Series Feedback algorithm so that we can begin with our model soon. The major tasks that we still have to carry out are finalizing the document collection, writing the code for our topic modeling of the tweets, deciding what our causality measure will be and which testing strategy (Granger or Pearson) we will use to evaluate significance, deciding how strong of an effect we want our prior to have, and writing code to perform sentiment analysis on tweets and words. One challenge we are working through is people who tag several ticker symbols in their tweet to try to make it more popular. These tweets often are not focused on the stock we are trying to observe and will create unnecessary noise. Another challenge is getting a complete understanding of how to use the prior in the iterative topic modeling and writing out the code for this process. A third problem we are facing comes in the presence of pictures. Oftentimes people will tweet a picture of a stock chart or the picture will contain essential information without which the tweets itself may seem out of context or to be missing information. Therefore, we are trying to think about what the best way to handle pictures will be. Right now we are thinking about filtering images out and ignoring all non-text data, but should we find that these tweets contain crucial text data, we may try to include them somehow. Paper we will be reproducing: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Project Proposal Our team is StonksOnlyGoUp and Akhil Bhamidipati (akhilsb2) will be the team captain along with team members Angeeras Ramanath (ar13) and Josh Perakis (perakis2). We will be reproducing an algorithm outlined in the paper  Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback  to observe the impact of news and tweets on the stock prices of Facebook, Apple, Microsoft, Tesla, and American Airlines. These specific stocks have been chosen for certain reasons: Apple and American Airlines stock prices were used as response data in the original paper and using this same data again after several years will likely yield quite interesting results; Facebook, Tesla and Microsoft have been extremely popular stocks over the past few years and analyzing the prices of those stocks will best serve the needs of our ideal users. The goal of our project will be to implement the ITMTF algorithm to determine words from our sources linked causally to stock price changes--relevant positive words which are correlated with increasing stock price and relevant negative words which are correlated with decreasing stock price. The important concept is that the time-series data, stock prices in this case, has to change after a certain time delay after the relevant data has been observed. Once we have our topic mining done, we will evaluate the effectiveness of our model on our time-series stock market data by using a significance test to compare the model to the actual prices of the respective stocks during those time periods. To carry out this project, we will use Python and several of its libraries for the development of the model and then use R for parts of our statistical analysis process when needed. For our data, we will also use Tweepy (a twitter API to get our Twitter input data from select accounts), web scrape news headlines and rumors from select pages, and use Finnhub to get our time series stock price data. Once we are finished, we will demonstrate the usefulness of our model by trying to evaluate it over a future series after the algorithm is developed and using a significance test to estimate its effectiveness in that window. This test should give us a baseline on whether our model is good enough to ""put our money on"" or not. The people who will benefit most from our model will be common Robinhood investors, investment bankers, and traders who can try to capitalize on market volatility induced by news or tweets. While similar tools do already exist, there are not many which show a causal relationship between news and actual changes in price which makes our tool unique in that sense. Also, because our tool is focused on a relatively simple set of inputs and response data, it will provide starting investors with a comprehensible algorithm which they can use to judge investor sentiment and make informed trades. Topic Modeling and Causality Evaluation An evaluation of how coverage of topics in tweets with cashtags ($TSLA, $PLTR, $NFLX) are causally linked to the price of the respective stock. Team Members: Captain -> Akhil Bhamidipati (akhilsb2), Angeeras Ramanath (ar13), Joshua Perakis (perakis2) Introduction We proposed to implement the paper titled Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback. In this paper they used AAL and AAPL stock as well as Presidential Probability odds as time series data and New York Times text data. The topic modeling is generic, however, in the paper they only implement the PLSA (Probabilistic Semantic Analysis) method. The paper uses both Pearson correlation coefficient and Granger causality to quantitatively evaluate the correlations. Our project was to implement the iterative topic modeling with time series feedback (ITMTF) algorithm to identify which words from tweets are linked causally to stock price changes. Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 ITMTF Algorithm Below is a general summarization / pseudocode of the ITMTF algorithm which helped us understand the paper: Identify our time series response data (X = x1, ..., xn) with timestamp t1, ..., tn 1a. Stock data Identify collection of documents form same time period D = {(d1,td1), ..., (dm, tdm)} 2a. Twitter tweets Use a topic modeling method to generate topics for each doc T1, .., Ttn 3a. This topic modeling method is M 3b. Going to apply M to D, Find topics with high causality For each topic apply C to find most significant causal words in the top words of the topic and get the impact of these significant words Separate positive impact terms and negative impact terms Assign prior probabilities according to significance level Use the prior to repeat until we reach good topic quality Purpose The goal of this project was to find relevant words which where causally linked with price movements so that we could use the document collection in the future to predict trends. Ultimately, what our code does is evaluate which words' coverages over time are most strongly causally linked to changed in price within a 5-day lag. Further improvements on our project will be able to more effectively find significantly causal words which are linked to movements in price and more accurately predict changes in stock prices based on the coverage of topics in tweets with the respective cashtag. Implementation We begin by using Tweepy, a twitter querying API to retreive tweets with the cashtags $TSLA, $PLTR, and $NFLX and format them into files with their date and the tweet. Parse the files from step 1 to make document collections Create corpuses to maintain a vocabulary and calculate word coverage over time in the time series Use Granger Testing to test for a causality relationship where the coverage of a word over time in the given corpus ""Granger causes"" the change in the stock's price within a lag of 5 days. Evaluate words which are significantly causal and possible implications/inferences. A Walk-Through of our Project Take a look at our demo video on Youtube: https://youtu.be/yu4mr-RQW80 In the file twitter_stock.py you will find the first steps of our project which involved retrieval of tweets. We then generated the tweets we retreived into the files tweet_data_tsla.txt, tweet_data_pltr.txt, and tweet_data_nflx.txt. At this point we were ready for the main portion of our project which can be found in doc_collection_topic_modeling.html or doc_collection_topic_modeling.ipynb. In this step, we first parsed the data from the previous files to create comprehensive document collections and then went ahead and intialized corpuses for all of these document collections while ignoring stopwords. After intializing these corpuses, we performed topic modeling calculations (which are further documented in doc_collection_topic_modeling.html) to understand the coverage of the most highly covered words at any point in our time series over time. Once we had narrowed down our list of words for every corpus along with their coverage over time, we converted these data frames into csv's so that we could import them in R and perform Granger tests. The final step of our project can be observed in grangertesting.html or grangertesting.Rmd and what it essentially comprises of is hand-selecting topics from the top 200 topics that we had filtered for in the previous step, and then performing a Granger test for causality from the coverage of that topic in the time series to movements in that stock's price within 5 days. After completing that last step, we were able to find a few words who's coverage over time was causally linked to changes in the stock's price. A lot of our documentation was best suited to be in docstring format within our files. Please take a look at the specified files for more details. Some Additional Information We decided to use the closing prices of stocks $TSLA, $PLTR, and $NFLX as our time series data. A reason we decided to choose these stocks was due to their liquidity, popularity, and unique ticker symbols which allowed for easy parsing. For our document collection we scraped twitter using Tweepy between 11/20/2020 and 12/11/2020 as there was increased volatility due to the US Presidential election and easy scrapers to aid in building the document collection. After creating the document collection, we created parsers using the PSLA algorithm to determine the probabilities of words in different ranges of vocabulary. The probabilities we calculated include a word in a tweet, a word in a tweet within a day, etc. We used stopwords as well to minimize the noise in our document collection. Additionally, our parser identified non-english tweets and scrapped them from the document collection. We used the same R library used by the paper used for determining Granger causality with the top 200 words for each of our three stock tickers. We experimented with different time lags between the range of 3-6 days to find the best results. Contributions of Each Member The project began with data collection by Joshua. The team decided that Twitter is a good platform to retrieve data from. After setting up a developer account, he began to pull tweets that contained PLTR, NFLX, and TSLA. 100 tweets were pulled from every day for the last month all using Twitter's API. These tweets were written into a respective .txt file and each tweet was treated as a document. Angeeras led the algorithms for topic modeling. Akhil contributed to the topic modeling as well. On top of that, he implemented Granger tests in R. The dataset for these algorithms to be run on came from the tweets that Joshua provided as well as the stock data that Akhil provided from Yahoo Finance. Overall the project was split very well. The contributions made by all members were all equally important in completing the project and also a great learning experience in applying class material to real world analysis. Conclusions & Further Research Things we learned: Creating a Twitter Document collection was quite difficult Excess noise in tweets makes it hard to scrape a good document collection Iterative topic modeling is complicated Pictures are very difficult to model Creating topic models for short documents is tricky Potential Future Extensions of this Project: Try to model more stocks and see what words we find as causally linked for these tickers Implement the feedback loop to allow our causal topics/words to guide our model and make it much more oaccurate Use the data from the project to created predictive models based on sentiment and topic coverage"
https://github.com/alany9552/CourseProject	"Reproducing Paper: Latent Aspect Rating Analysis Chengmin Huang Ge Yu Xuehao Wang Introduction and Overview In this paper, we will introduce the steps of building Latent Aspect Rating Analysis(LARA) in order to mining the opinion ratings on topical aspects of given certain services type(Hotel reviews, restaurant reviews etc.). Specifically, it will infer the opinion ratings and relative weights focusing on the different aspects based on the reviews from the website such as Yelps, Trip Advisor, Amazon. We'll be using Trip Advisor as our dataset. The basic model of LARA will be based on the pre-defined aspect keywords, whereas the advanced LARA model won't need the supervision of predefined aspect keywords. Since it's the time where the data is everywhere , LARA is helpful for users to digest a larger amount of the online reviews about a specific entity of tropic. In our project, we use the hotel reviews dataset provided by the Trip Advisor. Nowadays, most websites already decompose the overall rating into different specific aspects. For example, the hotel reviews might have such values, rooms, cleanliness and other categories. Since different users emphasize different aspects, it might still not be informative. Our LARA model can infer the relative emphasis placed by a reviewer on different aspects by digging into their specific reviews. LARA takes review texts about an entity as an input, and will produce output as 1) the ratings on a set of predefined aspects 2) the relative weights that the user placed based on their review texts. For our implementation, we divide the LARA model into three stages: 1) Data Processing to process the raw data into the format for further processing 2) A Bootstrap algorithm to identify the aspects and segment of the processed review content 3) A Latent Rating Regression model to infer the aspect rating and weights in a review. The specific implementation will be introduced in the next section. Implementation and Documentation Data Reading and Processing In order to process the data, we developed several functions. First, we read the initialized aspect words and stop words. We downloaded the stop words from the nltk library. Then we read the reviews from the json file downloaded from the database, and call the stemming Stop Removal() function to 1)tokenize the reviews into sentences and words 2) Remove the stop words to improve the accuracy of the model 3)Add words to the vocabulary list 4) Make the sentence objects and corresponding review objects. Also, since there are words that have less frequency but could be affecting the overall results as outliers, we developed a function to remove the words that have the frequency less than 5. After the processing step for the data, we call the functions in BootStrap.py to generate the processed word lists as local files. Bootstrap Algorithm As mentioned in the paper, the main usage of bootstrapping algorithms is to identify the aspects and segment the review content. We use a bootstrapping algorithm to generate the keywords. This is the code that we used from others and changed some details in order to make it satisfy our own goal . The assignAspect function: this is basically just assigning aspects to sentences. The chiSq and calcChiSq: these two functions are used to generate the chi-square value which is used to tell you how much difference exists between the observed date and the data you would expect to get. populateLists: this function is used to generate the word list. The bootStrap function: it is used to execute the algorithm, it basically implements all the functions I mentioned above. And saveToFile function is just simply saving the file. All the files are saved in the modelData folder. wList.json is a list of words and their frequency matrix, ratingsList.json is list of ratings dictionary belong to review class, reviewIdList.json is list of review IDs, vocab.json is the list of all the vocabularies that being selected and the aspectKeywords.json is the file that contains the keywords that we obtain using the bootstrapping algorithm. Then we applied a linear rating regression model with these keywords. Linear Rating Regression After identification of aspects and segments in review content, the authors applied the Latent Rating Regression(LRR) model to complete the prediction. The LRR model mainly consists of two steps. The input of the LRR model is a list of words and their frequency in the review content. At the beginning of LRR model, the word list is separated into two subset i.e. training set and testing set. The original code separated 75% word frequency data into training set and the rest of data into testing data. In the E-step of training step, the model constrained posterior inference. To be specific, it estimated the updated states using the current parameters. In the M-step, it updated the parameter estimation and maximized the log-likelihood of the whole corpus. The model is using the ""Overall"" rating of each review as the true value and calculating the likelihood between these values and prediction values. However, review text might not be directly related to the overall rating values. One possible improvement is to replace the ""Overall"" rating with the average of all aspects numbers such as ""Service"", ""Cleanliness"", and ""Location"". Another possible improvement is that since the original model didn't use the validation dataset during the training step while the validation set could help with the optimization and convergence of parameters. Further improvement The results of this model are promising and meaningful. Even though there are some large numbers in the prediction values, the overall trend and relevant values are almost consistent with the actual values. It can be told that the prediction numbers of actual 5.0 rating is greater than the prediction numbers of actual 3.0 rating in the order of magnitudes. We think the possible reason for these large numbers might come from the bag-of-words assumption. This assumption limited the model's aspect segmentation capabilities. At the beginning of E-step and M-step, the calculation parameters Mu and Sigma are set to the large range of numbers which leads prediction values to increase cumulatively. Even after many iterations steps, it would be difficult to lower these parameters down to the reasonable range. Further improvement can focus on the normalization of these prediction values to the [0, 5] range. In this way, it would be more clear to compare these two kinds of values and evaluate the performance of this model. Furthermore, the initialization of these parameters might also help with the improvement of accuracy. Also, like mentioned in the paper, we successfully implemented the proposed method of using LRR as the model with the aspect keywords as supervision. However, we fail to implement the advanced model which has a better performance without using predefined aspect keywords as supervision. In the future, we will implement the improved model and compared with our model using bootstrap algorithm and LRR model. Usage of software This paper proposed a generative LARA model and used the model to infer the opinion rating on topic aspects. Also it improves the model by eliminating the use of pre-defined aspects of keywords. The software can be downloaded from  https://github.com/alany9552/CourseProject . To run the software, users need to make sure they have installed the Python3 environment on their device. Also, the software uses  nltk  stopwords so users should use  import nltk, nltk.download('stopwords'), and nltk.download('punkt') to download the necessary dictionaries. After the completion of installation, users can run the software using python3 ReadData.py, python3 BootStrap.py, and python3 LRR.py sequentially. Then, the running results would show up. The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively. Also, there is a simple classification at the end of prediction that the review would be positive when the ""Predicted OverallRating"" is greater than 3.0 or negative when it is smaller than 3.0. The software running can also be customized by users in terms of ratio of training dataset and testing dataset. In the line 46 LRR.py file, the users can change the percentage of the training set. Currently, the training dataset and testing dataset are in 3:1 ratio. In addition to the training ratio, users can also specify the maximum interaction steps and coverage threshold in line 370. Moreover, if they want to change the maximum interaction steps much lower, the changing of line 339 is also needed. This model is applied to predict the review score of hotels and restaurants based on the review text. Therefore, it can be generalized to the prediction of most opinion tasks. Elimination of predefined aspects of keywords enables this model to be applied in various areas. For example, as mentioned in the paper, it can be applied to reviewer behavior analysis, topic opinion prediction, and personalization recommendations. Contribution of each team member in case It is hard to say who contributes to which part of the project since we are basically all doing work that overlapped. We first all read and understand the paper by ourselves, then we gathered and shared our understanding together. After uniting the idea, the team leader (GeYu) gave each of us different work. To be more specific, Chengmin Huang and Ge Yu contributed more in coding, and Xuehao Wang contributed more in testing and processing the data. Citations and Contributors Original Implementation of the authors: http://www.cs.virginia.edu/~hw5x/Codes/LARA.zip Since the implementation of the LRR and bootstrap algorithm, we extend existing models from the web as our model per the instructor's instructions and directions, and did some changes to fit our models better: https://github.com/redris96/LARA https://github.com/seanliu96/LARA https://github.com/biubiutang/LARA-1 Data Sources: http://times.cs.uiuc.edu/~wang296/Data/ Overall implementation and Ideas: ""Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach"" , http://sifaka.cs.uiuc.edu/~wang296/paper/rp166f-wang.pdf ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"", http://sifaka.cs.uiuc.edu/~wang296/paper/p618.pdf CS410 Project Progress Report Ge Yu - gey2 Chengmin Huang - ch61 Xuehao Wang - xuehaow2 *Progress made: 1.Understand the advantages of Latent Aspect Rating Analysis Model compared to previous two-step solution 2.Data from TripAdvisor has been processed into SQL database *Remaining tasks: 1.Try to pull more datas from the website to make the model more meaningful 2.Algorithms to calculate the K-means from different features 3.Still try to implement algorithms described in the paper *Challenges: 1.Fit the model with the bag-of-word assumption 2.How to determine the aspect segments and their weights 3.Acceleration on the training process for the test dataset 4.Dimensionality reduction to solve the data sparsity 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Xuehao wang: xuehaow2 GeYu: gey2 Chengmin Huang: ch61 GeYu is the captain 2. Which paper have you chosen? We choose the first one Subtopic: Latent aspect rating analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3. Which programming language do you plan to use? We plan to use python 4. Can you obtain the datasets used in the paper for evaluation? Yes, we can obtain from this website http://times.cs.uiuc.edu/~wang296/Data/ CourseProject for CS410 at University of Illinois at Urbana-Champaign Presentation Videos: https://mediaspace.illinois.edu/media/t/1_jj5lzils https://www.youtube.com/watch?v=o0bTraiYkNM&feature=youtu.be Citations and resources: The python implementation is following the instructions from research papers: ""Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach"", http://sifaka.cs.uiuc.edu/~wang296/paper/rp166f-wang.pdf ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"", http://sifaka.cs.uiuc.edu/~wang296/paper/p618.pdf This implementation uses the orignal implementation given by the author (http://www.cs.virginia.edu/~hw5x/Codes/LARA.zip). Also, the Latent Rating Regression model and Bootstrap use the implementation of existing models from: https://github.com/redris96/LARA https://github.com/seanliu96/LARA https://github.com/biubiutang/LARA-1 Contributors: Chengmin Huang - ch61@illinois.edu Xuehao Wang - xuehaow2@illinois.edu Ge Yu - gey2@illinois.edu Organization of the implemenataion: src/hotelRivews: This directory includes the hotel reviews download from the database http://times.cs.uiuc.edu/~wang296/Data/ from TripAdvisor. For testing, we include only one review file. You are free to download the whole dataset to do a better training, which might take up to 30 minutes to run 10 json files. src/Settings: This is the directory to store the pre-difined laten words and stopwords downloaded from nltk libaray. src/modelData: This is the directory to store the data after processed by readData.py, the file includes the ratings, reviewID, each word's frequencyy and the aspectKeywords after reading thereviews. src This is the directory to store all of the files including three main class: readData.py that includes the data proessig methods, BootStrap.py that contains the boot strapping algorithms and LRR.py that implemented the Linear Rating Regression model. How to run the model: To run the software, users need to make sure they have installed the Python3 environment on their device. Also, the software uses nltk stopwords so users should use import nltk, nltk.download('stopwords'), and nltk.download('punkt') to download the necessary dictionaries. Step-by-step tutorial: https://mediaspace.illinois.edu/media/1_s4x9i7wo https://www.youtube.com/watch?v=sbRppuoFsgY&feature=youtu.be Background: Python 3.7 Required Packages: NLTK, numpy, panda, pandas, matplotlib Specific Step: After the completion of installation, users can run the software using python3 ReadData.py, python3 BootStrap.py, and python3 LRR.py sequentially. Then, the running results would show up. The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively in your terminal window. Customize: The software running can also be customized by users in terms of ratio of training dataset and testing dataset. In the line 46 LRR.py file, the users can change the percentage of the training set. Currently, the training dataset and testing dataset are in 3:1 ratio. In addition to the training ratio, users can also specify the maximum interaction steps and coverage threshold in line 370. Moreover, if they want to change the maximum interaction steps much lower, the changing of line 339 is also needed. Reading Results: The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively. Also, there is a simple classification at the end of prediction that the review would be positive when the ""Predicted OverallRating"" is greater than 3.0 or negative when it is smaller than 3.0."
https://github.com/alex-pi/CourseProject	CourseProject Automatic Crawler of Faculty Pages Tool is live on this URL: https://sxxgrc.github.io/faculty-scraper/ Demo video under: doc/UsageDemo.mp4 Directory structure faculty_scraper_ui.py >> flask client for GUI cli.py >> command line utility to use the faculty scraper globals.py >> common python definitions README.md +---data finalized_model.sav >> svm model data output.zip >> output example TrainingDataSetTest.csv TrainingTestingDataSet.csv +---output >> csv files will be generated here, one for positive cases, another for all URLs tested .gitignore README.md +---doc Automatic crawler of faculty pages - Project Proposal.pdf ProgressReport.pdf FinalReport.pdf UsageDemo.mp4 +---spiderbot scrapy_spider.py +---ui >> web content for GUI such as JS, html and css +---public +---src faculty_scraper.js >> Main search page for UI, handles accessing the flask client and polling results.js >> Main results page which displays the JSON result from the Flask client +---urlclassification url_classification.py >> model training and classification
https://github.com/alex6499cat/CourseProject	"CS 410 - JAWs Project Progress Report Sentiment Analysis of Customer Support Tweets 1) Which tasks have been completed? *Replacing emojis with descriptions of emojis *General data cleaning to remove urls, unnecessary line endings, etc *Running topic analysis on Amazon, Apple, and Uber Tweets *Creating a parallel database that links all the tweets in the same thread. *Sentiment analysis of the first and last user tweet of each thread to determine change in sentiment due to the interaction with the customer service team *Summarize the average initial sentiment of the customer, and the average improvement of sentiment by company. *Visualize how the sentiment changes over time for one company. 2) Which tasks are pending? *Sentiment analysis between successive customer tweets of a thread *Generating a language model representing ""successful"" customer service tweets *Visualizing/Summarizing data *Continue topic analysis on airline companies trying different topic counts *Incorporating topic analysis into sentiment analysis of companies *Visualize how the company compares to others in the same industry regarding the effectiveness of their team in improving customer sentiment. 3) Are you facing any challenges? *General runtime of scripts *Choosing what parts of our analyses are most interesting to summarize and present *Finding coherent topics of tweets programmatically Team name:  JAWs Team members: Joan Ball - joan2 Alex Ginglen - aging2 Walter Griebenow - wfg2 - Team Captain CS - 410 Project Proposal:    Sentiment Analysis of Customer Support Tweets 1. Project objective / Hypotheses to test What is the function of the tool? Our tool analyzes customer support tweets generated by both customers and call center personnel and performs several textual analyses: *Determines the sentiment of the initial tweet, response tweet and subsequent interactions. *Determines the topics/categories that exist in order to categorize tweets. *Relating to complaint, such as bad service, faulty product, etc *Relating to industry, such as software, food service, etc *Searches in the response tweets word unigrams and bigrams that partially characterize the content of the response. *Mines possible associations of those uni or bi-grams with the change in the sentiment between the initial customer tweet and the tweet after the interaction with the call center employee (to estimate ""success"" of the customer service) *Uses non textual variables to control for variations due to company / industry and topic (maybe timeliness of response too). For further exploration: *Find correlations between inbound and outbound topic which would be the initial step for predicting the answer from the question the customer asks Who will benefit from such a tool? The idea behind this project is to find associations between the language used and the effectiveness of the interaction with the customer, that may lead to a focused A/B testing which in turn would be used to establish best practices for the service personnel. Does this kind of tools already exist? If similar tools exist, how is your tool different from them? We can assume that technology savvy companies already have discovered the best language to approach angry or dissatisfied customers and have clear guidelines for different types of interactions. But probably smaller companies do not have the resources to perform this type of analysis and rely on common sense and the experience of their employees. Would people care about the difference? We do not know yet what we will find but this path could be much faster than discovering language patterns only with the experience. Also, this would help to flatten the learning curve of new employees by informing of empirical best practices. How are you going to evaluate your work? Initially, we will evaluate our work by observing that our tool produces accurate results when analyzing the sentiments of a small amount of data. A larger scale test could be running our tool on a large, similar dataset that has already had a sentiment analysis done to it and compare the results. We will also visualize our output and analyzed input data to better understand it. 2. Course topics covered by the project / Tools to be used in analysis and display of results What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We will apply sentiment analysis, topic mining, and general CS techniques like parsing text to remove useless words, database wrangling to put together related tweets and some type of regression analysis. We intend to do the development work in Python. 3. Data description What existing resources can you use? Our group has identified a suitable dataset in Kaggle that can be accessed in this site: https://www.kaggle.com/thoughtvector/customer-support-on-twitter This dataset is a large corpus of over 3 million tweets and replies from big brands like Apple, Amazon, Delta and T-mobile. This dataset is organized in one row per tweet in 7 fields per row that include the links to previous and next (if applicable) tweets. 4. Other How will you demonstrate the usefulness of your tool? We will run the analysis using a Jupyter Notebook that can be run by the TA. This notebook will include graphs and tables that show the uncovered relationships (hopefully some with statistical significance) A very rough timeline to show when you expect to finish what 1.Data exploration and data wrangling - 11-05-2020 - 3 hours per person 2.Sentiment analysis and topic mining by type (customer or employee) by 11-15-2020 - 5 hours per person 3.Word unigrams and bigrams in the response and their association with the change in sentiment - 11-25-2020 - 5 hours per person 4.Project Presentation Made - 11-30-2020 - 3 hours person 5.Optional: prediction of response topic from inbound topic by - TBD Sentiment Analysis of Customer Service Tweets Presentation: https://youtu.be/wcCyMLzrOu8 Our goals Note: it was not our intention to create any library or software, but instead to do a deep dive into a data set and present our findings. Because of this, our presentation is more about our findings instead of usage, and our ""testable"" code is more of a playground with limited functionality, running all code that we used to generate our findings could take days. As discussed in our proposal, we set out to analyze 1 million customer service tweets. This analysis includes: - Analysis of the overall sentiment change of a thread. That is, the difference in sentiment between the first and last tweet of one customer in a thread. This data was stratified by time and company to further compare and analyze how well the top companies stack up against each other. - Analysis of common topics by company. With tweets being short and complaints not being too broad within the scope of one company, we found that a smaller amount of topics (k) tends to find more distinct topics. While these topics aren't given any human readable title, you can infer what a topic might be about based on its unigram language model. - Analysis of successive sentiment. That is, the difference in sentiment of customer tweets that ""sandwich"" a customer service tweet. As opposed to the earlier mentioned sentiment analysis, this had the intention of finding successful language, instead of comparing companies against each other or themselves over time. But again with tweets being short and customer service responses tending to be formulaic, there wasn't much difference in language between successful and unsuccessful customer service responses. Testing/""Playground"" Along with python notebooks, we also have a well put together excel spreadsheet that includes data from our first goal and that also has several sheets you can interact with. That spreadsheet is found at output/Sentiment summary by company and month.xlsx We have provided several playground files, or files meant for our tester to ""test"" our code with since they are more lightweight. - Overall sentiment change - tester files: Jupyter Notebooks/FromAdjacencyToDataframe.ipynb Jupyter Notebooks/Sentiment improvement by company and month.ipynb Jupyter Notebooks/SentimentAnalysis.ipynb - dependencies: pandas, datetime, stanza CoreNLP (java & pytorch needed for stanza) - Topic analysis: - tester files: Jupyter Notebooks/DeltaTopicsDetermination.ipynb Jupyter Notebooks/Sentiment of topics.ipynb - dependencies: gensim, nltk, collections, pandas - Consecutive sentiment change (developed on python 3.7.3) - tester files: Jupyter Notebooks/con_sent_tester.ipynb - dependencies: pandas, gensim Sentiment Analysis of Customer service tweets JAWs Team - CS410 Joan Ball, Alex Ginglen, Walter Griebenow Objective Monitor effectiveness of the service center in improving customers satisfaction and compare to industry benchmarks Find the topics most asked for in the tweets and their typical sentiment Portray the characteristics of a successful interaction with the customer Process followed Text cleaning Convert emoticons to words Associated tweets into Threads Customer Sentiment: initial & final Compare by industry / time Topic analysis Relate to sentiment Sentiment change after interaction Language of effective interactions 1 2 3 4 1. Pre processing Cleaning Remove unneeded Twitter Handles Remove urls and &amp; Emoji to text Replace emoticons with corresponding english words (grin face, happy face) Weave tweets into Threads Gather related tweets in a sequence to build a conversation 1. Translate Emojis Script EmojiTranslate.py Process Remove Emojis for topic analysis Translate emojis to text description for sentiment analysis Smile Face 2. Effectiveness of customer service Initial sentiment of customer tweets is improved by interaction with the customer service team. How do we compare? (Tech industry) Spotify not only has better initial sentiment but also they are better in improving the final sentiment. Microsoft lags behind. 2. Monitoring the sentiment in time Timely detect changes in sentiment of customer tweets. Delta Airlines does better than the average of its industry, but there is always space for improvement. Did something different happen in June 2017? 3. Topic Analysis Process Remove Stopwords Incorporate Bigrams Component Incorporate TF-IDF Component Create model using Latent Dirichlet Allocation Execute model on list of tweets Aggregate list of Tweets with assigned topics 3.Topic Analysis Results Generated topics did not directly match single, clear topics identifiable by humans. Topics tended to match some specific areas Topic 2 tended to match tweets about gates. Topic 5 tended to match tweets requesting help or being on hold. In order to identify coherent and granular topics consistently, our algorithm would need human intervention such as in text categorization or by providing a pre-classified list of tweets 3.Topic to Sentiment Some topics tended to have higher average sentiments than others Topic 0 tended to match tweets praising customer service and had a higher average sentiment starting a tweet thread Topic 5 tended to match tweets requesting help and being on hold and had the lowest average sentiment starting a tweet thread The more positively associated topics tended to improve less in sentiment Airlines may need to pay special attention to topics that have a low starting sentiment and a low sentiment improvement 4. Successive Sentiment More granular than overall sentiment Instead of evaluating company success, evaluating what successful or helpful speech looks like Sentiment analysis isn't perfect, makes for some interesting results! 4. Successive Sentiment: ""Best"" Results Analysis not always perfect, but it's not hard to see where things were interpreted wrong Correct strong positive change, but the CS tweet did not assist in any way Might have interpreted ""wining"" as ""winning"". ""good luck"" also added to evaluating as positive sentiment Good feedback! 4. Successive Sentiment: ""Worst"" Results Lots of these were genuinely confusing Customer response not related to CS response Not sure how to interpret this as a human Good interpretation! 4. Successive Sentiment: Unigrams ""Best"" tweets have sentiment change +3 ""Good"" tweets have sentiment change greater than or equal to +2 4. Successive Sentiment: Unigrams ""Worst"" tweets have sentiment change -3 ""Bad"" tweets have sentiment change less than or equal to -2 Future improvements Enhance Topic detection by initializing topics with keywords: Airlines: flight delay, bag claim, phone long, cancel refund, etc. Technology: battery life, screen size, game start, etc. Model the Quality of employee response as a Function of Language used: Q=f(L) Naive Bayes logit regression: TF best CS tweets TF worst CS tweets Add Bigrams as language features, here we analyzed unigrams only Industry, Topic, Time as context"
https://github.com/anilkpalli/CourseProject	Project: Text classification competition Progress: I was able to achieve an accuracy of 0.77 with my initial implementation which includes pre-trained language model along with pytorch neural network framework for fine-tuning the model to fit to the sarcastic dataset. However I forgot to set the seed values due to which I couldn't reproduce the results although the output predictions were saved. With a specific seed set, I am able to reproduce the output predictions with accuracy ~0.765, almost close to my max. I have used Google-Colab free version for training the model so it comes with its own limitations. I am kind of struck on achieving maximum accuracy of 0.77 (currently (as on 25-Nov) best on the leaderboard) on the hidden dataset, although the models performance is good enough (~0.86) on the hold-out validation set created from the overall training dataset. I will try to use few more derived heuristic variables along with pre-trained models to see if that helps in improving the performance a bit more. Remaining tasks:  Need to do a thorough documentation of the code  Create tutorial presentation as part of project deliverables Challenges/Issues:  Limited GPU memory is allocated in Google-Colab free version. Layering the pre-trained models with any more slightly complex or deep neural layers is resulting in out of memory issues. Had to settle with shallow layers.  Struck within a performance range of 0.7 to 0.77 with the approach listed above. Need to come up with a different approach to improve the performance while working with limited memory Name: ANIL KUMAR PALLI Net ID: anilkp2 Email: anilkp2@illinois.edu Project Topic: Text Classification Competition Programming Language: Python Prior Experience: Most of my real-time work experience has been on classification problems but never got to work with text classification. This would be a great opportunity and learning experience while researching and participating in the competition. In this process I would to explore more on the neural network classifiers based on Convolution and Recurrent Neural Networks preferably using Keras framework. Although my main interest lies in exploring the latest and leading language models like BERT, Open AI's GPT etc... that are being discussed a lot in the NLP community recently. Objective: The objective of this task is to detect sarcasm in tweets. Datasets: Train: 5000 records (Columns: response, context, and label) Test: 1000 records (Columns: response, context) Tools Used: Google colab notebook (free version) Language: Python Libraries: torch, sklearn, transformers (ver=3.0.0), numpy, pandas, json, time Approach: RoBERTa (developed as an extension of BERT) pre-trained model is used as the starting point. Further the model is fine-tuned using Sarcasm train dataset and the final optimal model is used for predicting the labels on Sarcasm test dataset. The code is split into 6 sections. In order to get the final predictions the code needs to be just executed in sequence as described below. Section: 1 - Read input data The train and test json files are stored in google drive and are directly imported into google colab by providing appropriate credentials. Refer to this short tutorial for importing data into colab Section: 2 - Preprocess data The input data (train) consists of 3 columns: response, context and label. In this step the response and context are combined into one single sentence in the order of last to first conversation sequence. Further analysis is done using this single sentence approach which captures both response and context as one complete conversation. Note: * Tried out using response alone without context and it didn't perform better than using response and context together * Tried out data cleaning steps like removing stop words, special characters, urls etc.. but they didn't prove to be any useful in improving the accuracy while using this approach so removed those steps from final code Section: 3 - Prepare data for modeling Split the data into training and validation: 80% of the data from train set is used for training the model and 20% of train dataset is hold-out for validation purpose. Tokenize and encode sequences of both training and validation sets: The conversations are of varying lengths and therefore have to be truncated and padded to equal lengths. Based on the distribution of lengths, max length is selected as 200 since most of the conversations are covered within this range. Any higher number could possibly cause model to train slower or run out of memory due to colab limitations in its free version Create train and validation tensor datasets: Tensor datasets are created to work efficiently with torch framework while building the model Section: 4 - Define model build functions Define functions to initialize the pre-trained roberta-base model and fine-tune the parameters as needed to fit the data in hand. In-line documentation of these functions is available in the code. Note: * For the per-trained model, tried out bert-base-uncased, bert-large-uncased, roberta-base, roberta-large. Among these the large variants ran out of memory quickly and among the base variants roberta-base performed better, so retained it in the final submission * Make sure to set appropriate seed values to reproduce the results Section: 5 - Build the model Define model parameters: Includes defining optimizer, loss functions, number of epochs and batch size. Used Adam optimizer and a suitable learning rate to tune roberta-base for 10 epochs. Negative Log-likelihood loss (alternatively cross-entropy loss) is used as the loss function. Run the model and store the best model: The model is iterated for each epoch while optimizing the parameters. During training, the model parameters are evaluated against the validation set. Saved the model each time the validation accuracy increases so that the model with the highest validation accuracy is identified and stored. The train and validation metrics (loss and accuracy) are captured and stored for all the epochs. Section: 6 - Test predictions Load the best model - Load the model having the highest validation accuracy that is stored in prior step for predicting the test set Prepare and manipulate test data - The test set should undergo same data preprocessing and preparation steps as the training which includes: combining response and context into one sentence, tokenize and encode test conversations Split the test dataset into 2 parts to overcome limited space issue in colab - There are 1800 records in test set. Due to the space limitations in colab where majority was already utilized for loading and tuning pre-trained RoBERTa, the test set was split into 2 sets of 1000 and 800 records respectively. Get the predictions for the 2 test sets and combine into one final dataframe - The test sets are scored using the best model and the predictions are combined into one single dataframe and then to csv to submit the result in appropriate format
https://github.com/antonioalfonso/CourseProject	"CS 410 - Text Information Systems Antonio Gomez Lopez Progress Report: Project: Text Classification Competition (Sarcasm on Twitter) 1) Which tasks have been completed? So far, as part of the project I have completed the following tasks: 1. Take training data into pandas (Python) 2. Processing of the data which includes: a. Removal of @USER references b. Removal of HMTL references c. Removal of hashtag symbols d. Removal of links e. Removal of punctuation f. Removal of stopwords g. Removal / Substitution of emojis by text (testing these two scenarios). h. Removal of other special symbols. 3. I have created a classifier using Neural Networks, specifically using LSTM with an Embeddings Layer (activation function being sigmoid, which I find appropriate for a classifier). 4. I have tested three approaches / packages: a. Word2Vec b. Glove c. FastText So far, I have gotten my best results (69.5% F1 Score) with Fast Text under specific conditions (see below). I was expecting much better results with Glove but that turned out not to be the case. What am I currently working on? 5. With the NN as described above, I am currently testing different embedding dimensions (25, 50, 100, 200) for the embeddings layer. I have no conclusion yet, but it seems like I am getting better results with Embedding dimension = 100. 6. Currently testing two optimizers: a. Started with Adam at learning rate 0.01, results were somewhat overwhelming (F1 score ~55%) b. I found better results by using Adamax (which is recommended for NN using embeddings), and at learning rate 0.001. Because of the smaller learning rate, I need to run more epochs (hence is slower to learn, which is totally expected), but I managed to improve to my best result using that. c. I am using cross-binary entropy as loss function, which again I find pertinent for the problem I am trying to optimize for (classification issue). d. It seems, however, that I am facing a barrier as I cannot move past 70% with my current efforts (see below on what's pending and issues facing). CS 410 - Text Information Systems Antonio Gomez Lopez 2) Which tasks are pending? I have a functioning NN model that is able to assess the tweets as required by the assignment. So in that sense I have something to show . Having said that, and as mentioned before, I am not achieving the desired result (beating the baseline, currently at above 72.3% F1 Score. Things I still have pending to do: * Further research for models that can take into advantage context information (which I am currently not using), such as replies to tweets, which could shed more light on the intention of the original tweet that is subject to sentiment analysis. * Check other potential approaches to the problem that do not include training a Neural Network (although I am not sure they could be better). 3) Are you facing any challenges? Yes. I have a working model, but I am not hitting high numbers to beat the baseline, and I would like to do so. Going further I would like to have a high F1 score (likely by having a high precision score, but open to less accuracy for a higher recall, if that gets me above the F1 score). With my current approach I am unable go above 69.5% F1 Score (with various numbers in precision ranging from 55% to 63%). My other challenges are: * I am currently not using the context information for tweets given in the training and tests set, and I think I am missing out on important information to improve my algorithms / models. I am not using this information mostly because I do not know how to embed or incorporate that information into the NN to improve the predictions (or how to use it at all even if it were not a NN). * I am currently converting emojis to text, but I am not sure if this is making any difference (versus, say leaving the emojis untouched and let the NN process them as they are represented). * I am not sure I am using the best optimizer and loss function (I think I am, but I don't know what else is out there). So, if you have any guidance, to a paper, specific model in Kaggle, whatever that can be of use for me, please feel free to share that information with me as part of the feedback. I am working in this project alone. CS410: Project Proposal Antonio Gomez Lopez 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I am doing this project individually. So, I am the captain . Full name: Antonio Gomez Lopez, NETID: aag8 2. Which competition do you plan to join? Text Classification (Sarcasm detection on Twitter). 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods: I am prepared to learn state-of-the-art neural network classifiers. I am interested in this challenge because I can see the difficulty for a non-human to discern from the literal to intended meaning of the words. Prior Experience: I have taken two Machine Learning courses (one in UT Austin, the other one currently in UIUC), in which I have implemented versions of the following (as part of projects): a. Perceptron Algorithm b. Logistic Regression (which is not quite useful for the classification issue but still). c. Support Vector Machine d. Naive Bayes I also studied the theory for K-Nearest neighbor, but never implemented it. In additionto this, I took a Reinforcement Learning class at UT Austin, and in it, as part of our practical experience, we had the choice to work with either TensorFlow or PyTorch in order to implement policy evaluation using Neural Networks, as well as the REINFORCE algorithm. I implemented both successfully using TensorFlow (which is the one framework with which I have experience). 4. Which programming language do you plan to use? Python Text Classification Competition: Twitter Sarcasm Detection Submission by Antonio Gomez Lopez This model was built using Neural Networks, as a consequence the training time can vary significantly depending on whether you train in a local environment (e.g. your computer), or GPUs (e.g. Google Colab) IMPORTANT: You can find the tutorial here (I show it running Colab and beat the baseline live :-)) https://www.youtube.com/watch?v=p1T-ekliduA If you want to run it using Colab, there is a read-only version of the notebook available on the link below: https://colab.research.google.com/drive/1kEjXNHX-tHkjG38BJtw9ol2QH67lGXaG?usp=sharing Otherwise, use the ClassificationCompetition_BERT_AntonioGomez.ipynb and run it locally. I recommend using Jupyter, but I assume any good Python editor would do the work. Check 2) to make sure you have both train.jsonl and test.jsonl files in the right directory. 1) What's included in this submission? 1.1) This file includes details on what needs to be setup. 1.2) The reference to Google Colab (above) as well as the .ipynb file contain the commented source code. 1.3) answer.txt contains the test set predictions (as required in https://docs.google.com/document/d/13ANy7FHYovh_2JL3gVrVvzXScDh5ol5l5XS2Nlp4DN4/edit) 2) What is required to run this model? In both cases you need to have the files train.jsonl, and test.jsonl ready to be loaded. 2.1) If you want to use the Google Colab instance, you should: 2.1.1) mount your google drive (assumes you have a gmail or otherwise google-compatible account) 2.1.2) upload the train.jsonl and test.jsonl at the root of your drive As you run the code, you might get the following message: ""Warning: This notebook was not authored by Google. This notebook was authored by aalfonzo@gmail.com. It may request access to your data stored with Google, or read data and credentials from other sessions. Please review the source code before executing this notebook. Please contact the creator of this notebook at aalfonzo@gmail.com with any additional questions."" 2.2)If you want to use the local run instance, you should: 2.2.1) From the folder you are running the .ipynb file, you should have a /data folder, in which you will place both the train.jsonl and the test.jsonl files. 3) About the workbooks: 3.1) Both workbooks (ipynb) have cell-by-cell runs with my comments on the code explaning either what the code does or why did I choose specific parameter values. -Remember that the local run instance will train much more slowly (it can take you many hours to get to the trained model). For instance, in my laptop it took roughly 35 minutes per epoch, so 35x3=105 minutes, a little bit less than two hours to have the model trained. 4) Please note that: 4.1) The final design, fine-tuning and training of the network, including number of nodes, size of input, batch-size, dropout rates, which optimizer and learning rate to use were all of my authorship after several tests being performed. 4.2) Having said that, when it comes to the Neural Network solution I am presenting, I got inspired by two other projects I found in Kaggle, and that I am referencing here: [A] Sarcasm detection using BERT (92% accuracy), by Raghav Khemka. https://www.kaggle.com/raghavkhemka/sarcasm-detection-using-bert-92-accuracy, [B] Sarcasm detection with Bert (val accuracy 98.5%), by Carmen SanDiego (clearly a pseudonym). https://www.kaggle.com/carmensandiego/sarcasm-detection-with-bert-val-accuracy-98-5 [C] https://arxiv.org/pdf/1810.04805.pdf To get information about best optimizers and learning rates to use for BERT 4.3) How did I arrive to the succesful model? Background: Important to note, in the course of the project I have worked with different pre-defined packages (and data transformations), that I am not including here for the sake of focus on what's really the working solution. Having said that, however, the exploring of different alternatives was what had made my views evolve (looking for higher F1 scores). Below is a summary of what I worked with and the average F1 scores I got in each case. Technology F1 Score - MetaPy (no features, Naive Bayes) ~57% - Embeddings and GloVe ~66% - Embeddings and Doc2Vec ~68% - Embeddings and FastText (by Facebook) ~70% - Embeddings and BERT ~73.1% Note that the setup of each model was dependent on the pre-defined package I was using. Explanation of the model: It is based on a Neural Network with 5 layers (including the input and output layers, and including one layer with embeddings provided by BERT). I included a chance to Dropout on the basis of potential overfitting (having a too big network for the task at hand). In addition, Optimizer, learning rate, batch size, and loss functions were ALL parameters up for fine-tuning. There is some pre-processing of the data: Mostly removed punctuation, special symbols and stopwords. Having said that, I obtained best results when I did not removed @USER, references or emoticons. Some encoding / padding was included to keep consistent the size of the input for tweets either in training or testing. How did the training happen? For all the methods used (except Metapy - Naive Bayes), all of my training and initial fine-tuning happened first with the optimizer, learning rate, and ""size"" of embeddings (this last one was not the case for BERT). I also experimented with pre-processing of data, but as I mentioned earlier, I seem to obtained better results when I did not remove as many references that I originally thought would be mostly noise (e.g. @USER, ). In the specific case of BERT I performed the following fine-tuning (not necessarily in this order): Optimizer: Started with Adam, then moved to AdamWeightDecay, and finally got best results with Adamax. As per the documentation in Tensorflow, Adamax is recommended when the model includes an embeddings layer. Learning rate and number of epochs: I used 2e-5 as direct recommendation of the BERT paper: https://arxiv.org/pdf/1810.04805.pdf number of epochs is also recommended in this paper. I experimented however with large numbers (large as 10), but it was very clear that I was overfitting the model with the training data. I found that a number of epochs between 2-4 would work best, but needed additional fine-tuning. Dropout rate: From the referenced model, they use a dropout rate of 20%. I was still overfitting in many instances after the third epoch, so ended up increasing the droupout to 40%, and with that I could mitigate the problem of overfitting the model with the training model. Pre-processing data: I coded different functions to remove different parts of the tweets that I thought would constitute noise, but ended commenting most of them and leaving the functions that would remove just the stopwords and special punctuation symbols. Batch-size: I experimented with different batch sizes making sure that I was maximizing the maximum input from BERT (512 bytes). I ended up with an input of 64 and batch size of 8, but I equally beat the baseline with input size of 128 and batch size of 4. 5) Other important details about this implementation: Throughout the workbook you will find comments that seek to justify my choices many of the parameters and fine-tuning completed. The most important things to know are: 5.1) The model worked best when running only for 3 epochs, given my selection of optimizer and learning rate. 5.2) I had to dropout roughly 40% of the nodes (according to literature I consulted it is considered normal to do dropouts between 20% and 50%. This was the case because I was getting overfitting on the training data already in the third epoch (which indicates that maybe my network was too big to begin with...). 5.3) I tried multiple times to provide a saved model so that you could run the network the same way I did, however for unknown reasons, even though I was able to succesfully save my models, I was not able to load them back again. 5.4) As a consequence of 5.3), it is likely that a run of the model as it is might not render results that beat the baseline, (because is dependent on the training), so take that into consideration as you do the peer-review or assessment. 4.5) Having said that, I included my best run as part of the project submission (also as it was required as per homework guidelines (see https://docs.google.com/document/d/13ANy7FHYovh_2JL3gVrVvzXScDh5ol5l5XS2Nlp4DN4/edit)"
https://github.com/armhoeft/CS410Fall2020-CourseProject	"CourseProject Enhance MeTA and Metapy Usability: Python 3.9 For required project submissions, see project-artifacts. Final Submission Details The first body of interesting code lies in an adjacent repo (https://github.com/armhoeft/metapy-container) which I built strictly for this project to separate my deliverables from an artifact that can live on beyond the scope of this course. The second body of interesting code lies in the sample-assignment directory, where I've provided an example of how to use this container for course assignments. The final (and perhaps most important) is artifact is a functioning Docker container (armhoeft/metapy-container) that can be found here (https://hub.docker.com/r/armhoeft/metapy-container) or by executing the following command. This container contains a Python 3.9 runtime environment and a compiled version of MeTA and metapy. {bash} docker pull armhoeft/metapy-container:0.2.13 Recommendations for a Reviewer Watch the video: https://drive.google.com/file/d/19wAVeWPICQYA1XFTOhIhv-6bQbbniu69/view?usp=sharing Look around this repo and: https://github.com/armhoeft/metapy-container. Read: https://github.com/armhoeft/metapy-container/blob/main/README.md Read: https://github.com/armhoeft/CS410Fall2020-CourseProject/blob/main/sample-assignment/README.md Download: https://hub.docker.com/r/armhoeft/metapy-container Have fun and be generous in your grading/feedback! Closing Thoughts After many hours of fighting with MeTA and metapy, I realized that literally building the project on Python 3.9 wasn't especially remarkable. You struggle through a series of errors (most of which were the product of dead links or the repository falling into relative disrepair as the software world keeps chugging along), search the web for ways to resolve them, and try building again until you hit the end. This realization caused me to revector my efforts after succeeding in my initial task and incorporate some of my ""stretch goals"": namely, building a Docker container and making this class-ready. In my view, the biggest outcomes of this assignment are: A better working knowledge of how MeTA and metapy are constructed. A documented process for using Docker to make standardized build and execution environments for all students (which should help with those who have difficulty getting environment consistency between their local setup and the autograder). A published Docker container on DockerHub! A path to replacing the typical autograders with Kubernetes (since k8s likes containers)! I don't pretend to know how our autograders work, but I do know from personal and professional experience that elastically scaling Kubernetes pods are the future and could lead to substantial cost savings if not already implemented by the school."
https://github.com/ashpradhan01123/CourseProject	"CS-410 Text Information Systems Final Project Text Miners Ashish Kumar Pradhan (apradh6@illinois.edu) Kirti Magadum (magadum2@illinois.edu) Bhuvaneswari Periasamy (bp14@illinois.edu) Improving EducationalWeb System 1. Introduction The EducationalWeb System is the term given to the Web of Slides (WOS) created at the University of Illinois at Urbana Champaign by Sahiti Labhishetty, Bhavya, Kevin Pei, Assma Boughoula and Prof. Chengxiang Zhai. It aimed to link all the lecture slides of a course which would be easily navigable and searchable. Not only that, with Machine Learning, it is also able to provide a list of related slides which has some similarity to the current slide displayed. It is created as a Python Flask Web App with an interface which is similar to opening up a PowerPoint presentation. Navigation can be primarily done by clicking through the previous and next buttons in the interface as well as from the list of similar slides provided by the system. A separate system allows the search ability through which the navigation can be done as well. This project consisted of adding a few features which would enhance the System so that it can be used in a more widespread manner. Keeping true to the vision which was to create a system which would interact with each of the slides in a way that the current internet interacts, the modifications are aimed that doing just that. 2. Background The existing code consists of two major components: a. The related slides code which computes the similarity between slides using Deep Learning https://github.com/Bhaavya/mooc-web-of-slides b. The structural part of the application which consists of the rest of the Python Flask Web App. https://github.com/CS410Fall2020/EducationalWeb Initially, we thought of incorporating both parts of the code into one and then including the modifications but due to the computationally heavy part of the Deep Learning component, we decided to exclude this part completely and focus on adding the new components to the Web App. 3. Code Explanation Before incorporating the modifications, installing the existing code from the repository was necessary. We have detailed the following steps taken for installing the code in our Windows Machines. a. Download the Download the code from the github page ( https://github.com/CS410Fall2020/EducationalWeb) to a folder EducationalWeb in your local machine b. Elastic Search Elastic Search was installed by downloading the zip file from the Elastic Search Website. https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-windows-x86_64.zip. Unzip the folder in a folder ElasticSearch and then run the following code in the command prompt after changing the directory to that location. .\bin\elasticsearch.bat This will start the elastic search in your local machine. Fig 1 Fig 2 c. Create index (need to be done only once) Run the following script after navigating to the EducationalWeb folder in the command prompt python create_es_index.py Fig 3 d. Download tfidf_outputs.zip folder Download it from the following link, unzip it and save it in EducationalWeb/static https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing e. Download the lecture slides Use the following link to download the lecture slides and then unzip it to EducationalWeb/pdf.js/static/slides/ https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing f. Install gulp server Follow the steps in this page ( https://gulpjs.com/docs/en/getting-started/quick-start/) to install gulp server g. Run Gulp Server Open a separate terminal window and navigate to EducationalWeb/pdf.js/build/generic/web and execute the following command to run gulp server gulp server Fig 4 h. Run the application In another terminal window, navigate to the EducationalWeb folder and run the following script to start the code. Make sure that all the python libraries present in the code are installed before the script is run. python app.py i. Launch the application The application should open in http://localhost:8096/ 4. Modifications to the code Our goal is to add additional courses to the existing code so that users can navigate between courses and utilize the features available to maximize the learning ability from those courses. In order to achieve that, we have made two additions to the code, namely: a. Created a web crawler which would take as input the url of the webpage which contains all the lecture content and create a list of urls which contains the pdf links b. Download the pdfs, and subsequently create a repository like the existing one for CS410, containing folders for each lecture pdfs which will contain single page pdfs split automatically by the code. c. These courses are then incorporated into the existing code, bringing in all the course slides into the EducationalWeb which can then be navigated like the existing CS410 course Web Crawler An automatic Web Crawler is essential to any web application which needs to add new content from the web. A web crawler essentially follows all the links from a starting page and then retrieves information from the pages as desired. Our web crawler recursively follows all the links which are contained from a starting page. If it finds a dead end in any of the pages, i.e. there are no additional pages to scrape from that page, it will just go one step up and then continue the scraping of the pages from there on. The creation of the web crawler was done by using the very useful Beautiful Soup library which parses content from HTML pages using a tree structure. It is very useful in navigating, searching and modifying a parse tree of an html page, all very essential parts of web scraping. Download the lecture content After the web crawler scrapes through all the URLs containing lecture slides in each webpage, we need to download them and structure them in such a way so that it fits the educational web code. The existing code handles only single page PDFs which are then navigated and searched by the system. To achieve that for the newly added courses, these are the steps that were taken: 1. Download the pdfs from the URLs scraped by the web crawler. 2. Scrape the lecture title from the webpage 3. Create a folder structure like the existing 410 slides. There needs to be one folder for every Lecture pdf 4. Split the pdfs into single page pdfs and then add them to that Lecture's folder. For our project, we have included 3 additional University of Illinois courses whose lecture slides are publicly available. Those are: 1. ECE 313 (https://courses.engr.illinois.edu/ece313/sp2013/Slides.html) 2. CS 425 (https://courses.engr.illinois.edu/cs425/fa2019/lectures.html) 3. CS 554 (https://solomonik.cs.illinois.edu/teaching/cs554/index.html) Add the slides to the existing code After the slides have been created according to the configuration of the code, modifications had to be made to add them under the drop downs under the ""Courses"" section. We specifically parsed texts from these webpages to add the details to that course section in the system. The details were present in various tags which needed to be extracted so that we get the relevant titles for the slides. Running the Modified Code The code has been integrated in the existing system in such a way that it does not need any additional input from the user. Changes were made inside the model.py and app.py scripts with the additional functions which were defined being referenced from within the existing functions, thus reducing the need for any user input specifically. To run the modified code, all that's required is to run the app.py function again which will start the web scraping from the webpages and then create the folder structures for each of the subject and then integrate them into the existing EducationalWeb System. Fig 5 Fig 6 Fig 7 Fig 8 5. Limitations and Scope for Further Work 1. Due to an absence of a training dataset, a classification model could not be created to correctly classify a webpage as one which contains lecture slides. Due to this we had to explicitly input the starting URL for web scraping. Creating that model required much more time than what could be allotted to this project. 2. The related slides section which uses Deep Learning to find out the similarity between the slides could not be incorporated. Additional work needs to be done to integrate it into the existing code 3. Documentation of the code and how to navigate between the scripts would have been helpful to understand the code in the initial stages and not spend a lot of time in that. 6. Contribution of Team Members Team Member Contribution Ashish Pradhan (Captain) Web Crawler, Documentation Kirti Magadum PDF splitting code, integration to the existing code Bhuvaneswari Periasamy Modeling code, Text parsing code 7. Conclusion The EducationalWeb is a powerful tool which can assist the learners from learning in an optimized manner without spending a lot of time in searching and navigating through slides across different topics. There is tremendous potential in this application and the scope of incorporating a lot of features is immense. The modifications which we added to the existing code paves the way for additional work which would improve the system. As stated in the scope above, creating a classification model and adding it to our web scraper would make it largely automated and be able to scrape the pdf urls from anywhere in the web CourseProject Please install the Educational Web as mentioned below from the CS410 Repository and then overwrite the two python scripts (app.py and model.py) from our project repository. The git push from the original repo was showing an error when we were trying to bring in the entire code. We have tested the code using Python 3.6. Please let us know if you need a demo or require any assistance installing the code in your local machine for testing. EducationalWeb Installation Steps The following instructions have been tested with Python2.7 on Linux and MacOS 1. Download EducationalWeb code from https://github.com/CS410Fall2020/EducationalWeb You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server Please install the libraries mentioned in app.py before you run the code. In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/ Text Miners Team Project 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team member NetId Ashish Pradhan [Captain] apradh6@illinois.edu Kirti Magadum magadum2@illinois.edu Bhuvaneswari Periasamy bp14@illinois.edu 2.What system have you chosen? Which subtopic(s) under the system? We have chosen to enhance the Educational Web System and intend to add new features, focusing on the following subtopics: *Scale up the current system Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. We intend to create an automatic crawler which could classify a webpage containing slides correctly and subsequently download them. *Allow downloading slides in bulk: Downloading the entire collection of slides for a particular course or interest. Our goal is to primarily focus on creating a successful crawler and then aim for creating the ability to allow slides in bulk after that. 3.Briefly describe the datasets, algorithms or techniques you plan to use Dataset Create a dataset of all the UIUC pages which has slides available for download and also some random pages which are ""negative"" examples. Algorithms Dirichlet prior, EM Algorithm, All algorithms currently in use. For classification, we intend to use all the standard available ones such as SVM, Clustering, Logistic Regression to achieve maximum accuracy. Techniques Automatic web crawler. All techniques currently in use. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? -Scale up the current system We are adding an automatic web crawler. We will crawl data related to selected courses from the mentioned dataset. Users should be able to select newly added courses and our updated code should be able to display relevant data to users. Also upon selection of a newly added course, options related to course should get updated. #Courses [Newly added courses] #Recently Visited slides #Lectures #Search result. *Allow downloading slides in bulk We will enhance the existing functionality from downloading a single slide into downloading multiple slides that fits under the same course/slide heading. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will aim to automate the crawling to feed slides directly into the system so that users will not have to create a zip file and then manually upload it into the system. For the second part, we will aim to provide the user two options: to download the slide for a particular lecture or the whole course (as a zip file). 6.Which programming language do you plan to use? We will be writing our code predominantly in Python but might implement Javascript, CSS and HTML if needed. 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. *Scale up the current system Task Hours ( hours * N team member) Detailed understanding code for existing system 6 (2 * 3 ) User Interface changes 3 (1 * 3 ) Automatic Web crawling (Including creation of training dataset and a classification model) 33 (11* 3) Deployment and testing 12 (4 * 3) Bug Fix 3 ( 1 * 3) Performance Test 3 (1 * 3) Total 60 Hours 8.Allow downloading slides in bulk NOTE : Any remaining time from the above section will be utilized to complete this task. Text Miners Team Project: Progress made thus far: Scale up the current system: POC to read the pdf file(slides) from UIUC url and download it in the required format is complete. Implemented the changes to show the newly added courses for users to select on test data. Remaining tasks: Creation of Automatic Web Crawler by leveraging the POC that we completed now. Implementing the code changes for search result and recently visited slides. Unit Testing and Integration Testing Any challenges/issues being faced: Determining the reference link on Course era was not straight forward as the links contained sections which could not be parsed via code. We need the slides in single-page pdf as input to the educational web search program. Also, it requires additional authentication, ultimately downloading the pdf from cloudfront.net. As we don't have access to cloudfront, we decided to refer slides from UIUC site as input data to our program."
https://github.com/avinashnathan1/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/azk0019/CourseProject	"Akash Singla 11/29/2020 CS 410 Progress Report - FALL 2020 1) I have implemented LARA using python. The data set used is Yelp Hotel Review corpus. Latent Aspects are calculated and new aspects from the reviews are generated. The mode was fined tuned empirically to get more than 25 aspects from every review. Every aspect was provided a rating. 2) Submit complete code and instructions (tutorial and documentation) for presentation. 3) The training phase on more than 4000 reviews took time. I would wish to use a GPU for this purpose. 1. I implemented a Latent Aspect Rating Analysis from Hongning Wang, Yue Lu, Chengxiang Zhai in python. The code is provided in a zip file. The dataset used is Yelp Restaurant Review corpus. Latent Aspects are calculated and new aspects from the reviews is generated. The model was fine tuned empirically to get more than 25 aspects from every review. Every aspect was provided a rating. 2. The training phase on more than 4000 reviews took time. I would wish to use a better GPU for this purpose. 3. The dataset is freely available from https://www.yelp.com/dataset 3. CODE: Dependencies It requires the nltk dataset and additional packages for running. Use nltk.download() and download all the given packages. Also required is the vader package from nltk. It requires python3. Running Code The code can be run manually by using main.py The newly acquired aspects are stored in the ""output"" folder.""final_aspect_words.txt "" text file holds the newly acquired aspects mined. Corresponding ratings are allocated to every aspect mined and stored in review_data.txt. 4. Theory and Observations: In this paper, authors identified and analysed a new problem of opinionated text data analysis called Latent Aspect Rating Analysis (LARA), which aims to analyse opinions expressed about an object in an online review at the level of topical aspects to discover the latent opinion of each individual reviewer on each aspect as well as the relative focus on various aspects when forming the overall judgement To solve this new text mining issue in a general manner, they proposed a novel probabilistic rating regression model. Empirical studies on a data set for a hotel review show that the proposed latent rating regression model can effectively solve the LARA problem and that a thorough analysis of opinions at the level of topical aspects allowed by the proposed model can help a broad range of application tasks, such as overview of aspect opinion, ranking of individuals based on aspect ratings, and reviewer analysis. In the code I went through the following steps to replicate the paper: I. Create Vocabulary from the dataset. II. I used porter stemmer on the dataset to create a vocabulary (stemmed) corpus of the reviews. This stemmed corpus will then be used for aspect mining and rating. III. Initially defined aspects (10 aspects) are read from the file init_aspect_word.txt"" IV. The aspects are then mined using LARAM bootstraping , which uses the following two classes for Restaurant reviews: Y= class Review: Y= class Restaurant V. The the w matrix is calculated using ci-square metrics to calculate the ratings per mined aspect. VI. The results are the saved in output folder. NOTE: The original paper used regression to calculate weight for each aspect. But I also used sentiment analysis module from nltk to rate individual aspects. The Regression.py does not work, I just left it there for reference. Use mainpy to run the actual code. Project Proposal Report 1) Akash Kumar- Net ID: akashk5. I am working this project individually and I will be the captain and have all the administrative duties. 2) I have chosen Latent Aspect Rating Analysis as my research paper to reproduce. 3) I will be using python to reproduce this paper. 4) No. 5) Yes, I can obtain similar dataset. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview."
https://github.com/bachman5/CS410-BiasDetector	"CS410 Course Project Proposal US News Political Bias Detector Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Sentiment Analysis Description: Our free topic is to design a system that can examine mainstream news headlines and determine if they contain any political bias. The bias would be determined using sentiment analysis and NLP techniques. Important or Interesting: The political tension between the Democrat and Republican parties in the US has never seemed higher. Mainstream news media reports on events with a palpable bias that slants heavily toward one party or the other. Many Americans want unbiased, factual news reports to avoid being manipulated. An example visualization of this political bias from AllSides.com is shown in Figure 1. These visualizations are created from community feedback data which can also be bias / not normalized. https://www.allsides.com/media-bias/media-bias-ratings Figure 1 Media Bias Visual Planned Approach: Our approach will be to text mine news headlines from 2020 to create a data set or find an existing dataset that has already been labeled. We will then build or modify an existing Sentiment Analysis Model and tune it for political bias. Other models that we might try in case we don't get good results are clustering based on latent Dirichlet allocation (LDA), logistic regression and deep learning. We then plan to visualize the results to see how various media outlets rank across a bias spectrum. Left and Right will be the sentiment classes. We might add Lean Left and Lean Right classes if deemed appropriate. Tools, systems, datasets: NLTK is a suite of python libraries that can be used for classification, tokenization, stemming, tagging, parsing and NLP. Google's Named Entities Sentiment Beta API for entity sentiment. Might use TensorFlow if we tried Deep Learning model. Gensim for LDA. We will high likely use Hybrid sentimental analysis algorithms which include stemming, tokenization, lexicon along with some automatic approaches available for public use like BERT. The final report might be displayed in form of BI visualization with Tableau or D3.js. Expected outcome: * Input * A New Article Headline * A fixed set of classes C = {c1,c2,..,cn} * Output o A predicted class c  C Project Timeline: Milestone Due Date Submit Proposal Oct 25th Working Prototype Nov 22th Project Progress Report Nov 29th Project Completion and Submission Dec 13th Programing Language: Python and NLTK (http://www.nltk.org/) Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Web scrapping for News Headlines 5 Collect and Label Test / Training Datasets (Corpus) 5 Build / Tune Sentiment Analysis Model 20 Train Sentiment Analysis Model (Iterative) 15 Test Sentiment Analysis Model 15 Visualization to display results (Tableau or Website) 8 Develop Software Documentation 2 Create Video Demonstration 2 Total 72 CS410 Course Project Final Report US News Political Bias Detector December 13th, 2020 Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Text Classification and Sentiment Analysis Public Repository: https://github.com/bachman5/CS410-BiasDetector Project Timeline: Milestone Due Date Submit Proposal Oct 25th - Complete Working Prototypes Nov 22th - Complete Code Video Demo Nov 28th - Complete Project Progress Report Submission Nov 29th - Complete 2 x Initial Peer Reviews Dec 2nd - Complete Project Completion and Submission Dec 13th - Complete 2 x Final Peer Reviews Dec 16th Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Clean News Headlines 4 hours - Complete Collect and Label Test / Training Datasets (Corpus) 10 hours - Complete Build / Tune Sentiment Analysis Technique 12 hours - Complete Build / Train / Tune Logistic & SVM Model 12 hours - Complete Build / Train / Tune Deep Learning Technique 12 hours - Complete Develop Software Documentation 2 hours - Complete Team Meetings 8 hours - Complete Create Video Demonstrations 5 hours - Complete Total 65 hours Project Motivation The political tension between the Democrat and Republican parties in the US has never seemed higher. Mainstream news media reports on events with a palpable bias that slants heavily toward one party or the other. Many Americans want unbiased, factual news reports to avoid being manipulated. Our free topic was to design a system that can examine mainstream news headlines and determine if they contain any political bias. The bias was determined using various sentiment analysis, shallow NLP and Machine Learning techniques. We then compared the results of three specific approaches by using all the knowledge gained from the CS 410 course. Collect and Label Datasets (Corpus) Raw Dataset: https://www.kaggle.com/snapcrack/all-the-news Our Corpus was taken from ~200,000 News Articles published in the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The bulk of headlines were taken from 2015-2017 during a heated US presidential election. Data Model Figure 1: Data collection, filtering and labeling workflow AWS SageMaker With AWS SageMaker, large labeling jobs can be broken up and assigned to public or private workforces. SageMaker increased the speed and accuracy of our labeling process. The jobs were broken up and assigned across all three team members. Each team member then recruited politically independent 3rd party members to assist with the large labeling task. We specifically tried to get labeling assistance from friends and family members that don't have strong political ties to either the Republican or Democratic ideologies. If the labeler has a strong political bias, then the resulting models could also reflect that bias. AWS GroundTruth AWS GroundTruth is the portal that private team members used to label our data. A simple dashboard was created for the labeler to view a single news headline at a time with no additional information like publisher that could introduce additional bias. The GroundTruth user had options to label the headline as either having Right Bias, Left Bias or Neutral. Final Labeled Test / Training Dataset: https://github.com/bachman5/CS410-BiasDetector/tree/main/data_labeled Our completed training data contained over 4,000 filtered, cleaned and labeled headlines from the workflow. News headlines were labeled as either Right Wing bias, Left Wing Political bias or Neutral using AWS Sagemaker and GroundTruth. Here is an example record correctly labeled with Left Wing bias: Lessons Learned AWS Ground Truth provides a great way to explain to the labeler exactly what you are looking for. You can even provide them examples of correct and incorrect label. These are great features, but they come at a high cost. Our team was unaware that AWS charges .08 cents per label. There is no warning or advertised cost until you get your monthly bill. 4,057 objects x .08 = $323.56. Category Classification + Sentiment Analysis Approach Language: Python Dependencies: nltk, nltk.vader, numpy, pandas Public Repository: https://github.com/bachman5/CS410-BiasDetector/blob/main/sentiment_test.py Primary Team Member: Nick Bachman Overview: VADER (Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is included in the NLTK package and can be applied directly to unlabeled text data. It can also be customized for specific domains and use cases. Our first approach was to see if we could design a system that combines topic category and sentiment to accurately predict political bias. Technical Approach: The goal of this model is to correctly determine if a text headline includes Left or Right political bias. In the headline ""Daily News mourns the death of the Republican Party, killed by epidemic of Trump"", the subject of the news article is the Daily News. The Reuters writer Lucas Jackson used the terms mourns, death, killed and epidemic to communicate significantly negative sentiment about the Republican Party and Donald Trump. Most people would agree that this headline supports a Left-Wing political position and thus includes a Left bias. {""source"":""69446,""Daily News mourns the death of the Republican Party, killed by epidemic of Trump'"",""class-name"":""Left""} As a group, we spent time thinking about the definition of political bias and how to design a labeling and detection system. The chart below summarizes our design decision. Sentiment alone is not enough to determine political bias. You also need a way to determine the category that sentiment is being directed toward. Topic Category Sentiment Bias Republican Negative :( Left Republican Positive :) Right Republican Neutral :| Neutral Democrat Negative :( Right Democrat Positive :) Left Democrat Neutral :| Neutral Table 1: Combining Category and Sentiment to determine Bias Figure 2: Combining Category and Sentiment to determine Bias Technical Challenges: Updating the category lexicon with descriptive terms and updating the VADER sentiment lexicon with popular political terms increased the accuracy by about 8-10%. Tunning the values for how much negative or positive sentiment yields a neutral response also helped because ""Neutral"" was the label with the most entropy. Example: ""Game of Thrones: Republicans hate it, Democrats love it"". GoT is the subject in this example. If the reader likes the HBO show Game of Thrones, it changes how they will label it. Both Republicans and Democrats are mentioned so there are multiple categories to attach the sentiment. Both Negative (hate) and Positive (Love) sentiment are expressed. The labeler may choose Neutral because they are unsure or do not know what Game of Thrones is in relation to this political topic. Complex sentiment and ambiguity make it very difficult to accurately label bias and accurately predict bias. Lessons Learned One additional feature that I added was putting a heavier weight on terms that occurred first in a headline. In English, most subjects occur to the left of the predicate. When both Democratic and Republican terms are mentioned, I subtracted the weights from each other. If the difference was minor, I added a category for ""Both"" that covers when Democrats and Republicans are both the subject (left of the predicate) and close together. This does not cover the minority cases when the subject is to the right of the predicate. This happens with questions, sentences that begin with ""Here"" or ""There"" and sentences beginning with one or more prepositional phrases. Additional rules could be added to check for these three exceptions to change the weighing when the subject occurs after the predicate. I tried using NLP external libraries such as Spacy but they also struggled with identifying the subject of many headlines. I think this is because news headlines are not using natural language. In other words, humans don't usually talk or write in the same way that news headlines are written to capture your attention. Example: ""Hillary Clinton: Republicans dishonor constitution by vowing to block Scalia replacementO Hillary Clinton (Dem) is saying that Republicans are dishonoring the constitution. The colon is acting like the verb ""says"". My code correctly identifies Hillary Clinton (Left) as the subject and attaches the negative sentiment because of the word dishonor and determines that it's right bias. However, the reader sees that the negative sentiment is attached to Republicans and labels it as left bias. In this case the sentiment was not attached to the subject but rather what the subject said about another subject. This is just one example of the challenges faced by trying to attach sentiment to a subject. Results: biasdetective1 biasdetective2 biasdetective3 Right Precision .580 .487 .414 Right Recall .593 .633 .708 Right F1 .586 .550 .523 Left Precision .450 .575 .557 Left Recall .565 .542 .578 Left F1 .503 .558 .568 Overall Accuracy .521 .528 .483 Support Vector Machine Approach Language: Python Public Repository: https://github.com/bachman5/CS410-BiasDetector/tree/main/SVM_Model Dependencies: numpy, panda, glove, spacy, sklearn Primary Team Member: Anh Nguyen Overview: Support Vector Machine (SVM), a Discriminative Classification introduced in one of our lectures in Text Categorization, is another method that we decided to implement in our model. The method is mainly included in sklearn package. To improve the dataset and support model's accuracy, couple of NLP methods and Vector Embedding are also utilized and can be explain further in the technical approach Figure 3: SVM explanation Technical Approach: Figure 4: Overall Diagram for SVM Model 1. Preprocessing Data (NLP): (Using Spacy) The main goal of this step is using some NLP techniques to conduct meaningful patterns and themes for the text data using spacy package: tokenization (breaking news titles into token), stopword (removing common stop works in sentence), punctuation (clean sentence better by ignoring punctuation like marks and spaces) Before and after Word Tokenized Before and after Removing stopwords Word Tokenization appeared to perform accurately with the purpose of data cleaning. However, removing stopwords tend to remove all the context of the sentence. 2. Word embedding & Magnitude (Using Glove) Building Corpus After preprocessing data, we then store available data into three different text files: train.txt, text.txt and corpus.txt. Ideally Corpus should be a dictionary including all political words in bigger spectrum than only words in train & text data, however due to the limitation of the data our Corpus only built upon our available scope of data. Words in Corpus then mapped to corresponding vectors (Word Embedding/word Vectorization) with the hope of capturing the meaning of potential relationship of word in term of similar contexts/syntax/spelling/co-occurrence. Corpus (Dictionary) with vectorized words Testing word's distance: Result: Another method that we were using (using Glove package) was store our corpus to a magnitude file. Normally a corpus can be text-formatted but storing dictionary in magnitude file helps improving the processing time. Vectorizing titles Using MeanEmbeddingVectorizer & TF-IDF Embedding 3. Train/Tune Model (Using sklearn) Current Result TF_IDF (Preprocessing) Mean Embedding Vector (preprocessing) Accuracy 0.514379622021364 0.5308134757600658 Precision 0.5306949522525308 0.4542249353200501 Recall 0.514379622021364 0.5308134757600658 F1 Score 0.4751926328128259 0.48172315509176467 Technical Challenge/ Lesson Learn: The current accuracy is still in range 50% with TF-IDF embedding preprocessing as well as Vector preprocessing. While there is not much success in improving the accuracy levels. We started to doubt the truthfulness of our labelled data. To assert the validity of our model pipeline, we switched to benchmarking using a different dataset while keeping everything else the same. The dataset is ATIS Airline Spoken Language Intent (train / test) which classifies a passenger's inquiry into 1 of the possible 18 intents: flight, flight time, meal, etc. The data size is about 5000 records and 18 classes (intents). The performance came out significantly better than what observed from our dataset: Accuracy : 0.9113495200451722 Precision: 0.9107883118388134 Recall : 0.9113495200451722 F1 score : 0.9011684778567066 One more example of our doubt in data integrity is that that we discovered the differences labels in one title. However, we do not complete blame on this issue as in the real world many opinions can appear for the same information. 100300,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral 113763,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Left 100300,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral 113763,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral Deep Learning: Recurrent Neural Network Classification Approach Language: Python Dependencies: TensorFlow Primary Team Member: Muhammad Rafay Overview: The approach uses a recurrent neural network (RNN) to classify news headlines as Left or Right biased. Technical Approach of RNN: Unlike the feed forward neural network we used an RNN for classifying news headlines. The difference in RNN verses feed forward is that output of an RNN is not just influenced by the input we just fed but it is influenced by the history of inputs we have fed earlier. An example is given below: So, it means RNN is suitable for classifying sequence data and news headline is a sequence. Since, neural network processes numbers we had to convert text to a sequence of token indices using an encoder. After the encoding is the embedding layer, this layer stores one vector per word after training words with similar meanings have similar vectors. Next comes the RNN with a stack of hidden layers, it processes the sequence of input by iterating through the elements. At each step output from the previous step is passed with the input of the next step. The sequence is converted to a single vector which is further converted to a single logit as the classification output. The architecture of our RNN is like the one given below: Lesson Learned: Our current classification accuracy is 53%. Not much different from other models. Since we are classifying based on political bias and not sentiments, the two classes: Left and Right has a lot of common vocabulary. Both text of both classes contains hate speech and words ""democrats"" and ""republican"" so in short, the vocabulary is not a very distinguishing feature for the two classes. The sentiment of bias is sometimes hidden behind the meaning of the sentence and it is not obvious e.g. ""Are Republicans more likely to prefer pulp in their orange juice?"" It is only obvious when something positive or negative is being said about one of the parties, and that involves recognizing the sentiment of the entity but not every headline is like that e.g. ""Republicans' patience with Trump may be running out"" We also suspected that the poor results are due to erroneous labeling of dataset. Hence, we explored and found another similar dataset Ideological Books Corpus (IBC). We trained/tested RNN on IBC dataset and found that might not me the case as the performance metrics for this dataset were like ours rather a bit poor So, we need a model that would learn the hidden meaning behind the headlines. On our own dataset Accuracy: 0.528 Precision: 0.507 Recall: 0.558 F1 score: 0.531 On our own dataset Accuracy: 0.504 Precision: 0.454 Recall: 0.432 F1 score: 0.442 -------------------------------------------- Deep Learning: Classification using BERT Language: Python Dependencies: TensorFlow Primary Team Member: Muhammad Rafay Overview: BERT is recent breakthrough in NLP. It makes use of a transformer mechanism that learns contextual relations between words. It has achieved state-of-the-art performance on several NLP tasks such as GLUE. As we looked forward to improving performance our next choice of model was BERT Technical Approach: As compared to an RNN based LSTMS which we implemented earlier BERT incorporates a non-directional encoder unlike LSTMS which read the test in sequence (left-to-right or right-to-left) BERT reads the entire sequence at once. This allows BERT to learn the context of a word based on all its surroundings. BERT models are pre-trained on a large corpus of text. They then fine-tuned for specific tasks Like the last model BERT also uses vector space representations of natural language to learn and predict. Lessons Learned: This approach validates our hypothesis for this kind of problem we need a model that better grasps the context of the sentence, rather than just learning from the word sentiment. BERT proved to be a significantly better classifier than the other three, due to its bidirectional training. But still, not good enough to be used in a real application. Current Result: On our own dataset Accuracy: 0.594 Precision: 0.568 Recall: 0.630 F1 score: 0.599 On IBC Dataset: Accuracy: 0.598 Precision: 0.558 Recall: 0.568 F1 score: 0.563 Video Demonstration Category + Sentiment Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing SVM Model Demo https://drive.google.com/file/d/1rGzV26i5q7GmMNR9MwLthTDqPrfe5H-4/view?usp=sharing Conclusion: We learned that getting a correctly labeled dataset is extremely important when training any ML model. Inaccuracy, duplication of records, ambiguity and bias can all negatively affect the outcomes. We also learned that how you create the labels are important. We chose three labels to categorize headlines but there were five or more separate clusters of word distributions that overlapped with each other. Discriminately classifying them and defining similarity was a significant challenge. The category/sentiment and SVM approach produced slightly lower accuracy than LSTM and BERT. Correctly identify the subject of the headline was difficult with shallow NLP which caused the sentiment to attach with the wrong subject and decreased the accuracy. One approach that we could have done is doing text categorization and sentimental analysis as two different steps of this project. This might be a future improvement that we can look into. Even though we didn't get to achieve the accuracy that we were hoping for, this project has brought much knowledge. It helps tie up all materials in class together from the way we do NLP, using TF-IDF to generative and classifier models like Naive Bayes, Logistic Regression, SVM, etc. It's like a perfect summary that we need to put everything we learnt in materials into practice. To conclude, we learned that bias detection is a harder text categorization problem than sentiment analysis because bias categories are weakly corelated to the surface features of the text such the sentiments of words in a sentence are not enough to predict bias. This hypothesis is validated by the result from BERT model. Since BERT learns contextual relationship between words it has performed better, and the accuracy has increased. Probably more fine-tuning can help it improve more. The other bottleneck includes amount of training data. Since we couldn't find the dataset, we were looking for we had to hand label it our self. Which bring us to another problem, we later found instances where our labeling was not accurate. So better training data set in terms of amount of data and better labeling would help improve bias detection as well. Category + Sentiment SVM LSTM BERT Overall Precision 0.512 0.530 0.507 0.568 Overall Recall 0.601 0.514 0.558 0.630 Overall F1 0.550 0.475 0.531 0.599 Overall Accuracy 0.521 0.514 0.528 0.594 References Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014. https://www.tensorflow.org/tutorials/text/classify_text_with_bert https://www.tensorflow.org/tutorials/text/text_classification_rnn https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/ https://aws.amazon.com/sagemaker/groundtruth/ https://spacy.io/usage/spacy-101 CS410 Course Project Progress Report US News Political Bias Detector Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Text Classification and Sentiment Analysis Public Repository: https://github.com/bachman5/CS410-BiasDetector Project Timeline: Milestone Due Date Submit Proposal Oct 25th - Complete Working Prototypes Nov 22th - Complete Code Video Demo Nov 28th - Complete Project Progress Report Submission Nov 29th - Complete 2 x Initial Peer Reviews Dec 2nd Project Completion and Submission Dec 13th 2 x Final Peer Reviews Dec 16th Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Clean News Headlines 4 hours - Complete Collect and Label Test / Training Datasets (Corpus) 10 hours - Complete Build / Tune Sentiment Analysis Technique 12 hours - Complete Build / Train / Tune Logistic & SVM Model 12 hours - Complete Build / Train / Tune Deep Learning Technique 14 hours - In progress Visualization to display results (Tableau or Website) ? Develop Software Documentation 2 Team Meetings 5 hours Create Video Demonstrations 4 hours Total Project Motivation Collect and Label Datasets (Corpus) Raw Dataset: https://www.kaggle.com/snapcrack/all-the-news Description: Our Corpus was taken from ~200,000 News Articles published in the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The bulk of headlines were taken from 2015-2017 during a heated US presidential election. Data Model Figure 1: Data collection, filtering and labeling workflow AWS SageMaker With AWS SageMaker, large labeling jobs can be broken up and assigned to public or private workforces. SageMaker increased the speed and accuracy of our labeling process. The jobs were broken up and assigned across all three team members. Each team member then recruited politically independent 3rd party members to assist with the large labeling task. We specifically tried to get labeling assistance from friends and family members that don't have strong political ties to either the Republican or Democratic ideologies. If the labeler has a strong political bias, then the resulting models could also reflect that bias. AWS GroundTruth AWS GroundTruth is the portal that private team members used to label our data. A simple dashboard was created for the labeler to view a single news headline at a time with no additional information like publisher that could introduce additional bias. The GroundTruth user had options to label the headline as either having Right Bias, Left Bias or Neutral. Final Labeled Test / Training Dataset: https://github.com/bachman5/CS410-BiasDetector/tree/main/data_labeled Description: ~4,000 filtered, cleaned and labeled headlines were created from the workflow. News headlines were labeled as either Right Wing bias, Left Wing Political bias or Neutral using AWS Sagemaker and GroundTruth. Here is an example record correctly labeled with Left Wing bias: Category + Sentiment Analysis Approach Language: Python Dependencies: nltk, nltk.vader, numpy, pandas Public Repository: https://github.com/bachman5/CS410-BiasDetector/blob/main/sentiment_test.py Primary Team Member: Nick Bachman Overview: VADER (Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is included in the NLTK package and can be applied directly to unlabeled text data. It can also be customized for specific domains and use cases. Our first approach was to see if we could design a system that combines topic category and sentiment to accurately predict political bias. Technical Approach: The goal of this model is to correctly determine if a text headline includes Left or Right political bias. In the headline ""Daily News mourns the death of the Republican Party, killed by epidemic of Trump"", the subject of the news article is the Republican party. The Reuters writer Lucas Jackson used the terms mourn, death, killed and epidemic to communicate significantly negative sentiment about the Republican Party and Donald Trump. Most people would agree that this headline supports a Left-Wing political position and thus includes a Left bias. {""source"":""69446,""Daily News mourns the death of the Republican Party, killed by epidemic of Trump'"",""class-name"":""Left""} As a group, we spent time thinking about the definition of political bias and how to design a labeling and detection system. The chart below summarizes our design decision. Sentiment alone is not enough to determine political bias. You also need a way to determine the category that sentiment is being directed toward. Topic Category Sentiment Bias Republican Negative :( Left Republican Positive :) Right Republican Neutral :| Neutral Democrat Negative :( Right Democrat Positive :) Left Democrat Neutral :| Neutral Table 1: Combining Category and Sentiment to determine Bias Figure 2: Combining Category and Sentiment to determine Bias Technical Challenges: Updating the category lexicon with descriptive terms and updating the VADER sentiment lexicon with popular political terms increased the accuracy by about 8-10%. Tunning the values for how much negative or positive sentiment yields a neutral response also helped because ""Neutral"" was the label with the most entropy. For example: ""Game of Thrones: Republicans hate it, Democrats love it"". If the reader likes the HBO show Game of Thrones, it changes how they will label it. Both Republicans and Democrats are mentioned so there are multiple subject categories. Both Negative (hate) and Positive (Love) sentiment are expressed. The labeler may choose Neutral because they are unsure or do not know what Game of Thrones is in relation to this political topic. Complex sentiment and ambiguity make it difficult to accurately label bias and accurately predict bias. Current Results: Test Dataset: biasdetective1.csv Classification Accuracy: 44% Test Dataset: biasdetective2.csv Classification Accuracy: 42% Test Dataset: biasdetective3.csv Classification Accuracy: 47% More advanced supervised and deep learning models discussed later in the report yield more accurate results. -------------------------------------------- Support Vector Machine Approach Language: Python Dependencies: numpy, panda, glove, spacy, sklearn Primary Team Member: Anh Nguyen Overview: Support Vector Machine (SVM) is a Discriminative Classification introduced in one of our lectures in Text Categorization. SVM uses classification algo to separate classes of data point then find the maximum margin of hyperplane between data points in their associated classes. The method below also includes some NLP methods + Vector Embedding to improve dataset and support model's accuracy Technical Approach: Follow by diagram below Overall Diagram 1. Prepare Data (NLP): (Using Spacy) Goal of this is to conduct a more meaningful patterns and themes for the text data using spacy. The main method for this data processing is tokenization was breaking news titles into token, clean sentence better by ignoring punctuation like marks and spaces. Before and after Word Tokenized Another method that I have tried is removing stopwords from sentences. However, it brings me to uncompleted sentences that are difficult to translate and understand. Before and after Removing stopwords OutPut: X : word in titles, Y : labels 2. Word embedding & Magnitude (Using Glove) Building Corpus After preprocessing data, I have built three different text files: train.txt, text.txt and corpus.txt. Ideally Corpus should be a dictionary including all political words in bigger spectrum than only words in train & text data, however due to the limitation of the data our Corpus only built upon our available scope of data. Words in Corpus then mapped to corresponding vectors (Word Embedding/word Vectorization) with the hope of capturing the meaning of potential relationship of word in term of similar contexts/syntax/spelling/co-occurrence. Corpus (Dictionary) with vectorized words Testing word's distance: Result: Another method that I'm using in this is store our corpus to a magnitude file. Normally a corpus can be text-formatted but storing dictionary in magnitude file helps improving the processing time. Vectorizing titles Using MeanEmbeddingVectorizer & TF-IDF Embedding 3. Train/Tune Model (Using sklearn) Current Result TF-IDF for preprocess + SVM classification Accuracy : 0.514379622021364 Precision: 0.5306949522525308 Recall : 0.514379622021364 F1 score : 0.4751926328128259 Mean Embedding Vector preprocess + SVM classification Accuracy : 0.5308134757600658 Precision: 0.4542249353200501 Recall : 0.5308134757600658 F1 score : 0.48172315509176467 The current accuracy is still in range 50% with TF-IDF embedding preprocessing as well as Vector preprocessing. While there is not much success in improving the accuracy levels. I started to doubt the truthfulness of our labelled data. To assert the validity of our model pipeline, I switched to benchmarking using a different dataset while keeping everything else the same. The dataset is ATIS Airline Spoken Language Intent (train / test) which classifies a passenger's inquiry into 1 of the possible 18 intents: flight, flight time, meal, etc. The data size is about 5000 records and 18 classes (intents). The performance came out significantly better than what observed from our dataset: Accuracy : 0.9113495200451722 Precision: 0.9107883118388134 Recall : 0.9113495200451722 F1 score : 0.9011684778567066 Challenges: NLP: Still facing challenging with precise deep Semantic Analysis & Bias in Labelled Data: -------------------------------------------- Deep Learning: Recurrent Neural Network Classification Approach Language: Python Dependencies: TensorFlow Primary Team Member: Rafay, Muhammad Overview: The approach uses a recurrent neural network (RNN) to classify news headlines as Left or Right biased. Technical Approach: Unlike the feed forward neural network we used an RNN for classifying news headlines. The difference in RNN verses feed forward is that output of an RNN is not just influenced by the input we just fed but it is influenced by the history of inputs we have fed earlier. An example is given below: So, it means RNN is suitable for classifying sequence data and news headline is a sequence. Since, neural network processes numbers we had to convert text to a sequence of token indices using an encoder. After the encoding is the embedding layer, this layer stores one vector per word after training words with similar meanings have similar vectors. Next comes the RNN with a stack of hidden layers, it processes the sequence of input by iterating through the elements. At each step output from the previous step is passed with the input of the next step. The sequence is converted to a single vector which is further converted to a single logit as the classification output. The architecture of our RNN is similar to the one given below: Current Results: Our current classification accuracy is 60% with the vanilla RNN and improved to 62% when we added two LSTM (Long Short-Term Memory) layers. Although the accuracy is not good enough for practical usage but this is the best, we have got among the models we tried. Challenges: Since we are classifying based on political bias and not sentiments, the two classes: Left and Right has a lot of common vocabulary. Both text of both classes contains hate speech and words ""democrats"" and ""republican"" so in short, the vocabulary is not a very distinguishing feature for the two classes. The sentiment of bias is sometimes hidden behind the meaning of the sentence and it is not obvious e.g. ""Are Republicans more likely to prefer pulp in their orange juice?"" It is only obvious when something positive or negative is being said about one of the parties, and that involves recognizing the sentiment of the entity but not every headline is like that e.g. ""Republicans' patience with Trump may be running out"" So, we need to involve feature engineering that would help the model learn the hidden meaning behind the headlines. Video Demonstration Rules + Sentiment Model Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing SVM Model Demo https://drive.google.com/file/d/16FyQPFDD6CV-WdYkEM-gWNPKaQADNHBd/view?usp=sharing Instruction for Testing SVM: $make install - installing all necessary packages in local dir $make data - build corpus $make train - train model $make test - test result Remaining Tasks In the next two weeks, we will continue to improve our accuracy. We look forward to use the BERT model a recent break thorough in machine learning. The BERT is a non-directional model instead of reading the words in a sequence it processes each token in context of all token before and after. Secondly, another bottle neck in improving accuracy is the accuracy of the data labeling. We suspect that some of the data has been incorrectly labeled as well that is confusing our classifier. Thirdly, we have about 4000 instances in our training data increasing the amount of training data might improve the results further. So, we are exploring new datasets to use with our model. References Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014. https://github.com/plasticityai/magnitude#file-format-and-converter CourseProject CS 410 Group Project- Bias Detectives We have divided our work into three different technical approaches to determine which is the most accurate for our use case. If you would like to test the code, clone this repo and follow the instructions below. You can also download a video demo of Rules and SVM. Rules_Model Video Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing Instructions for testing Rules_Model: $pip3 install nltk $pip3 install numpy $pip3 install pandas $cd Rules_Model $python3 sentiment_test.py SVM_Model Video Demo https://drive.google.com/file/d/16FyQPFDD6CV-WdYkEM-gWNPKaQADNHBd/view?usp=sharing Instruction for testing SVM_Model: $make install - installing all necessary packages in local dir $make data - build corpus $make train - train model $make test - test result Deep_Learning_Models Video Demo - https://drive.google.com/file/d/1TRKzGCTPX2U6qpORR2SQ3iWYDhAv7TuA/view Instruction for testing Deep_Learning_Models: Install Jupyter Notebook: ""pip install notebook"" Go to notebook directory in terminal Install TensorFlow: ""pip install tensorflow==2.3"" Run ""jupyter notebook"""
https://github.com/bashirpartovi/CourseProject	"ExpertSearch Improvements Progress Report Topic Modeler Assignee: Karthik Rajagopal ( kr22@illinois.edu ) Updates: Challenges: Running topic modeler for all bios is computationally extensive. We are incrementally optimizing the algorithm to achieve a better performance but this has proven to be challenging. Bio Page Classifier Assignee: Bashir Partovi ( partovi2@illinois.edu ) Updates: Task Progress Implemented topic modeling using spaCy python library and it accurately predicts topics for individual bio pages Completed Integrating topic modeler in ExpertSearch system to display top 5 topics for each search result In Progress Task Progress Implemented a URL crawler using Scrapy to crawl Carnegie Mellon University and University of Maryland in order to generate Completed Challenges: It was difficult to understand how Keras deep neural network layers work, especially for someone who has never worked with the library before. In addition, transforming the text data into a feature vector in order to fit the model was very challenging. Automatic URL Crawler Assignee: Mohana Venkata Kalyan Cheerla ( cheerla3@illinois.edu ) Updates: negative labels for classifier Using Keras, implemented a deep learning layer and trained it with the compiled bios from class project and the URLs that were crawled by the Scrapy spider, achieving 99% accuracy on the test data Completed Write a wrapper to load the model and use it with the URL crawler in order to identify bio pages In Progress Task Progress Code development to dynamically route from University home page to all its subsequent web pages and scraping their content has been completed. This scraping activity covers the extraction of the entire text information of those pages along with some important metadata that can feed additional information to ""Faculty Bio Page Classifier"" beyond what it requires today to help further improvements in future. Completed Adding additional filtering criteria to the crawler to eliminate crawling of uninteresting URLs. Configuration of Crawler to be more dynamic In Progress Challenges: Crawler runs for a long time and fetches over 40K+ web pages per university. Identifying the filter criteria to reduce the false positives is challenging. is also pending along with integrating it with the classifier CourseProject Overview As indicated in the project proposal, we improved upon Expert Search system. The Automated Crawler requires classification model which can be generated by running the classifier Source Code and Documentation Each component includes instructions on how to setup and run * Bio Page Classifier * Automated Crawler * Topic Modeler Presentation and Demo You can find our recorded presentation here as well as our presentation slides"
https://github.com/bearnomore/CourseProject	"CS 410 Project Report Reproduction of Paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" Team TR Squirrels: Ye Xu, Weidi Ouyang, Raj Datta 1 INTRODUCTION Our project focused on reproducing the paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" by ChengXiang Zhai et al (see reference 1.) We were interested in this paper since it has broad applicability covering a variety of data types. We were also enthused by the fact that the paper used multiple data mining techniques such as frequent patterns, clustering, and similarity functions. We believed that it would allow us to solidify our knowledge based on class materials while learning and exploring new areas as well (e.g. frequent patterns.) The paper uses frequent pattern mining as a fundamental building block. The frequent patterns that occur in datasets, however, have to be interpreted to understand their relevance and semantic applicability. The goal of the paper is to create a way to attach (annotate) meaningful information to frequent patterns that lets us understand the frequent pattern better. The analogy of a dictionary is taken whereby words that are looked up, have a description, but also have examples and other related words presented to fully understand the looked-up word. The equivalent in the frequent pattern universe would then be to identify a) the definition of the frequent pattern by its context indicators, b) representative transactions with the frequent pattern, and c) semantically similar patterns. These specifics are extracted through a series of data mining steps and algorithms which are detailed in the paper. We undertook the effort by first understanding the specifics of the paper, finalizing project scope, selecting appropriate tools and libraries, and implementing the desired result. PROJECT SCOPE When we evaluated the specifics of what the paper had accomplished, we realized that the full scope of the paper would require much more effort than what was expected for the team project and endeavored to refine the scope more clearly and realistically. In consultation and agreement with the lead TA, Bhavya, we decided on the following: * To use only one dataset, not three as in the paper * To implement only one of the two clustering algorithms for removal of title pattern redundancy. * To implement the entire sequence of steps/algorithms necessary to extract the context indicators and define the given frequent pattern in the context units space (section 4.1 of paper). * To leave extraction of representative transactions (section 4.2 of paper) and semantically similar patterns (section 4.3 of paper) as optional, to be implemented only if time allowed * We would then match our results against the paper's results for the appropriate database we decide to use (either section 5.1, 5.2, or 5.3 of the paper.) 2 IMPLEMENTATION We decided to choose the DBLP dataset for our implementation. Specifically, we started with DBLP50000 (see reference 2.) As in the paper, we decided to focus our efforts on the author data attributes for itemset frequent pattern mining, and on title data attributes for sequential frequent pattern mining. We also decided to implement the agglomerative hierarchical micro-clustering algorithm (mentioned as Algorithm 1 in the paper.) We wrote our scripts in Python and chose NLTK for stemming and stop-words removal for titles, MLXend library for frequent pattern mining of authors and PySpark library for sequential frequent pattern mining of titles, as they are more modern Python-based libraries with good adoption in the industry. These are not the same libraries used when the paper was written (which may be somewhat outdated), but using the same algorithms. For the author pattern mining, we used the FPGrowth algorithm with some code sample adjustment for finding closed frequent patterns (see reference 3). For title pattern mining we used the PrefixSpan algorithm. For stemming, we used the Porter stemmer (as opposed to Krovertz stemmer used in the paper) due to our familiarity with NLTK and possibility of higher false negative rates in Krovertz stemmer. We used the default stopword removal (for English and German languages) in NLTK (the paper only mentions the fact that 12 words were removed without mentioning which words.) Data acquisition and pre-processing After we acquired the DBLP50000 raw dataset (with 50000 transactions), we parsed it and removed transactions that had no author names, which resulted in 49233 rows. We found that some of our computations (e.g. the hierarchical clustering algorithm is not scaled up well for large data such as the gigantic distance matrix) were taking too long due to the size of the dataset. We decided to take a smaller slice of raw data only from the year 2000 (based on recommendation from Bhavya, lead TA.) This reduced our dataset size to a manageable 4004 rows of transactions. Obtaining Closed Frequent Patterns After obtaining the clean dataset, the next step is to get the closed frequent patterns for authors and titles which become the fundamental building blocks of the rest of the paper's approach. Given that we had a smaller dataset, we decided to lower the support of author count to 4 (as opposed to 10 as in the paper) and obtained 14 such closed frequent patterns. And using the same support count (4) as in paper for title frequent patterns, we obtained 1912 frequent patterns (each title has multiple word sequences and hence a higher likelihood for patterns.) As part of defining the building blocks, we also wrote scripts to find transactions related to author and title frequent patterns by building a reverse index of the transactions, which in turn helps downstream 3 computations related to clustering and building weight vectors of context units. Clustering We implemented the agglomerative hierarchical clustering algorithm with complete linkage, outlined as Algorithm 1 in the paper. We implemented the Jaccard distance measure as defined in the paper (Definition 9) for the purpose of clustering. After the visualization of the resulting dendrogram (Figure 1) and the elbow analysis of clustering iterations, we chose the threshold/cutoff of maximum depth at 0.01 to give 166 clusters. We tested a range of cutoff thresholds that gave different numbers of clusters around the elbow of the velocity curve where the clustering of branches slows down and examined the clustered words at these cutoff points. We felt that anything more than 166 clusters would have some redundancy, while less than 166 clusters would lack specificity and lose some information. Therefore, we chose 166 as our heuristic best. Figure 1: Dendrogram of Hierarchical Clustering of the 1912 sequential patterns of titles. Weighting and Context Indicators Weighting and the choice of context indicators is core to the context modeling part of the paper. For this, we chose the full set of our closed frequent patterns by combining both author and title frequent patterns into one pool of context indicators of 180 patterns. We then computed the mutual information between pairwise patterns as defined by paper and generated a 180 x 180 weight matrix of context indicators. Optional Features 4 Having achieved suitable implementation covering 4.1 of the paper, we decided to implement the optional parts of identifying example transactions (section 4.2 of paper), as well as finding semantically similar patterns (section 4.3 of paper.) After implementation, we created one example of context annotation for a given author pattern, and one example of context annotation for a given title pattern. In each example, the given pattern had top 5 weighted context indicators as its definition, 5 most representative titles from transactions and 5 most semantically similar titles or authors as synonyms to verify the soundness of the implementation. RESULTS ANALYSIS & CONCLUSION Since we had chosen the DBLP dataset for processing, our target for results comparison became what is presented in section 5.1 of the paper, which simply presents examples of the patterns and its contextual definition, representative transactions, and some semantically similar patterns (SSP's). Since we had taken a data slice from only one year (the year 2000), we knew the results wouldn't be exactly the same as in the paper, but should be indicative of the power of the approach mentioned in the paper. The following are the two examples generated from our project. As shown below, they are defining what the paper intended as contextual definition, representative examples, and similarly related patterns quite well. Having accomplished all 3 aspects (sections 4.1, 4.2, 4.3 of the paper), makes this more than what we had targeted in the scope of the project (since 4.2 and 4.3 were decided as optional.) 5 Results Example1: Context Annotation of An Author ""Ralf Steinmetz"". Author Definition Representative Titles (top 5) Synonym Authors (top5) Synonym Titles (top5) Ralf Steinmetz Ralf Steinmetz Domain Name Based Visualization of Web Histories in a Zoomable User Interface. Sanjay Kumar Madria user interfac virtuellen Intelligent graphical user interface design utilizing multiple fuzzy agents. Roberto Gorrieri process descript Sanjay Kumar Madria Realistic Force Feedback for Virtual Reality Based Diagnostic Surgery Simulators. Thomas S. Huang virtuellen workbench Techniques for simulating difficult queueing problems: adaptive importance sampling simulation of queueing networks. Edwin R. Hancock summari Edwin R. Hancock Blackboard Segmentation Using Video Image of Lecture and Its Applications. Gerald Sommer high speed 6 Results Example2: Context Annotation of A Title Title Definition Representative Titles (top5) Synonym Titles (top5) Synonym Authors (top5) virtual realiti Sanjay Kumar Madria Domain Name Based Visualization of Web Histories in a Zoomable User Interface. receiv Roberto Gorrieri Edwin R. Hancock Realistic Force Feedback for Virtual Reality Based Diagnostic Surgery Simulators. diagnost Sanjay Kumar Madria Roberto Gorrieri Blackboard Segmentation Using Video Image of Lecture and Its Applications. debug program Thomas S. Huang analysi access Techniques for simulating difficult queueing problems: adaptive importance sampling simulation of queueing networks. versu,Bharat K Bhargava Thomas S. Huang An approximate model for the computation of blocking probabilities in cellular networks with repeated calls. analysi access Bill Hancock Overall, we feel that the results are very useful and are convinced that this can help in building intelligence in applications reliant on various types of data. What we have built can easily be scaled up for larger datasets (by providing more computing power), and tuned to the specifics of the application (by modifying a variety of parameters in the algorithms implemented.) This was a useful exercise for the team to understand the importance of the mining approaches we learnt in the course. 7 REFERENCES 1. Q. Mei, D. Xin, H. Cheng, J. Han, and C. Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proc. of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06, ACM, pp. 337-346. 2. Source of DBLP50000: https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html Citation: Frequency-aware Similarity Measures.Lange, Dustin; Naumann, Felix (2011). 243--248. 3. How to Find Closed and Maximal Frequent Itemsets from FP-Growth | by Andrewngai | Towards Data Science 4. Project Github site: https://github.com/bearnomore/CourseProject 5. Presentation developed as Tutorial of Paper available on Github site: https://github.com/bearnomore/CourseProject/blob/main/Guidance%20of%20Reproducing%20the%20paper.pptx 6. Video of paper tutorial presentation and walkthrough of implementation details: https://mediaspace.illinois.edu/media/t/1_2uzja14v 8 APPENDIX: Setup Instructions Instruction for Reproducing paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" (Instructions are also available as a README file in the project github site (see reference 4.) 1. Overview of the github CourseProject There are three folders, Datasets, PythonCodes and JupyterNoteBookDemo, in the repository. The PythonCodes and the JupyterNotebookDemo contain the same scripts but different file formats (py file and ipynb file). Within each of these two script folders, there are 6 folders giving the execution order for reproducing the paper using the DBLP dataset (paper section 5.1). All datasets imported and generated using the scripts are located in the Datasets Folder. The input and output paths of these Datasets files need to be changed if downloaded to your local computer. Except for the raw dataset ""dblp50000.xml"", all other datasets are generated by the scripts. In addition, the final report, the link (https://mediaspace.illinois.edu/media/t/1_2uzja14v) to the video demo and the powerpoint slides of paper review and project introduction are also in the CourseProject repository. 2. Python Library and Packages numpy, scipy, pandas, nltk, csv, os, mlxend and pyspark are libraries needed for running the srcipts. Except for pyspark, all libraries can be downloaded and installed through pip or conda, depending on your preference and execution environment. The installation of pyspark (and Spark) is a bit complicated and requires some environmental configuration and functional java of version 8.0 or above. Here is the link of the tutorial how to install pyspark/Spark on Windows system: https://www.datacamp.com/community/tutorials/installation-of-pyspark 3. Script running instruction 3.1. Parse the raw data (Folder 1. RawDataParsing) Download ""dblp50000.xml"" and run script ""DBLP_raw_data_parsing.py"" or ""DBLP_raw_data_parsing.ipynb"". This generates the dataset ""DBLP2000.csv"". 3.2. Build the Context Units Space (Folder 2. ContextModeling) 3.2.1. Find closed Frequent Pattern (FP) for Authors using FPgrowth algorithm in MLXtend Lib. Run script ""Author_FP_mining.py"" or ""Author_FP_mining.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""authorsFP2000_with_index.csv"", which contains 14 closed FPs of authors and their transaction index in ""DBLP2000.csv"" (e.g. author ""Edwin R. Hancock"" is a closed FP and it showed in the 9 839th, 1119th, 1127th, 1204th and 1576th row of DBLP2000, its transaction index list is [839, 1119, 1127, 1204, 1576] ). 3.2.2. Preprocess DBLP titles Run script ""DBPL_preprocessing_titles.py"" or ""DBPL_preprocessing_titles.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""DBLP2000_preprocessed_titles.txt"". In this step, stop words are removed and the titles are stemmed. 3.2.3. Find Title sequential Pattern using PrefixSpan algorithm in PySpark Run ""titles_seqPattern_mining.py"" to import ""DBLP2000_preprocessed_titles.txt"" and to find closed sequential frequent patterns from titles of DBLP2000. I had issue with configuration of Spark in Jupyter Notebook environment therefore no corresponding script in ""ipynb"" format was put in the ""JupyterNoteBookDemo\ContextModeling"" directory . However, the python script was executed successfully in the windows cmd of my laptop. The script ""titles_seqPattern_mining.py"" generates an output folder containing the pattern file ""part-00000"". Set the ""part-00000"" file to txt format. Run ""Title_sequentialFP_processing.py"" or ""Title_sequentialFP_processing.ipynb"" to import ""part-00000.txt"" and generate the cleaned dataset ""titlesFP2000.csv"". 3.2.4. Find transaction index of title sequential patterns Run ""Find_transaction_index_of_title_FPs.py"" or ""Find_transaction_index_of_title_FPs.ipynb"" to import ""titlesFP2000.csv""and generate ""titlesFP2000_with_index.csv"", which adds the list of transaction index to each title pattern. 3.2.5. Reduce title FP redundancy by microclustering (hierarchical clustering) Run ""Hierarchical_clustering_titleFPs2000.py"" or ""Hierarchical_clustering_titleFPs2000.ipynb"" to import ""titlesFP2000_with_index.csv"" and generate ""titlesFP2000_final.csv"". This script applies the hierarchical clustering with Jaccard Distance defined per paper and clusters 1912 title sequential patterns into 166 clusters. It chooses the most frequent pattern in each cluster as the ""centroid"" pattern to further build the context unit space. 3.2.6. Combine author FPs and title FPs to build the context units space Run ""DBLP2000_context_units_with_transaction_index.py"" or ""DBLP2000_context_units_with_transaction_index.ipynb"" to import 'authorsFP2000_with_index.csv' and 'titlesFP2000_final.csv' and to generate the final context units dataset ""DBLP2000_context_units.csv"". 3.3. Define given frequent patterns using context units defined above (Folder 3. PatternDefinition) 3.3.1. Build weight vectors of FPs in the context unit space Run ""Weighting_function.py"" or ""Weighting_function.ipynb"" to import """"DBPL2000_context_units.csv"" and ""DBLP2000.csv"" and generate ""Context_units_weights.csv"". This script generates context vectors for all context units defined in 2.2 and builds a weight matrix between the pairwised context FPs. Each element of the matrix is the Mutual Information score between the context unit pair per definition in 10 the paper. 3.3.2. Annotate the given FP (e.g. an author) by context units with highest weights Run ""Defining_pattern_with_context_units.py"" or ""Defining_pattern_with_context_units.ipynb"" to import ""Context_units_weights.csv"". In this step, we first pick an author from the author FPs and rank the weights of its context vector. The context units with top 5 weights are selected as the definition of this author and are saved as ""author_annotation_example1.csv"". Similarly, we pick a title from the title FPs and rank the weights of its context vector, and save the context units with top 5 weights as ""title_annotation_example1.csv"". 3.4. Find representative titles of the given pattern (Folder 4. RepresentativeTitles2Pattern) Run script ""Find_representative_titles_to_pattern.py"" or ""Find_representative_titles_to_pattern.ipynb"" to import ""DBLP2000_context_units.csv"", ""DBLP2000.csv"" and ""Context_units_weights.csv"". This script first generates the weight matrix of transactions (titles) in the context units space as the dataset ""transaction_weights.csv"", and then computes the cosine similarity between the transaction weight vectors and the given pattern weight vector (e.g. the same author and title chosen in 2.3.2). The similarity matrix of transaction to author FPs is saved as ""similarity_scores_of_transaction_to_author.csv"", and the similarity matrix of transaction to title FPs is saved as ""similarity_scores_of_transaction_to_title.csv"". This script then generates the top 5 representative titles with highest similarity scores to the given author and to the given title pattern as dataset ""rep_titles_author_example1.csv"" and ""rep_titles_title_example1.csv"", respectively. 3.5. Find synonyms of the given pattern (Folder 5. Synonyms2Pattern) Run ""Find_synonyms_of_pattern.py"" or ""Find_synonyms_of_pattern.ipynb"" to import ""Context_units_weights.csv"" and to compute the cosine similarity between the candidate patterns of similarity (e.g. all closed frequent patterns of authors) and the given pattern (e.g. the same author and title chosen in 2.3.2). Select the authors with the highest 5 similarity scores as the synonyms of the given author or title other than the author or title itself. This script generates 2 datasets for synonyms of author pattern: ""coauthor_to_author_example1.csv"", ""syn_titles_to_author_example1.csv"", and 2 datasets for synonyms of title pattern: ""syn_titles_to_title_example1.csv"" and ""syn_authors_to_title_example1.csv"". 3.6. A final display of the context annotation of the given pattern (Folder 6. ContextAnnotation) Finally, Run ""Author_context_annotation_example1.py"" (or ""Author_context_annotation_example1.ipynb"") and ""Title_context_annotation_example1.py"" (or ""Title_context_annotation_example1.ipnb"") to combine the output datasets generated in step 2.4, 2.5 and 2.6. This script builds the two examples of context annotation for the given author pattern and the given title pattern respectivley and fullfills the two experiments in paper section 5.1. Progress Report for Reproducing the Paper ""Generating Semantic Annotations for Frequent Patterns with Context "" Raj Datta, Weidi Ouyang, Ye Xu Team TR Squirrels 1) Which tasks have been completed? *Thorough Study of Paper and understanding of options *Presentation draft prepared covering overview of paper *Tightly defined scope, in discussion with Bhavya, lead TA *To choose only one dataset (chosen to be DBLP) *To choose and use only one clustering algorithm *To target completing the context modeling and frequent pattern mining steps through part 4.1 of the paper *Consider 4.2 and 4.3 to be out of scope unless time allows after completing 4.1 *Obtained clarification on various open questions and direction from Bhavya, lead TA *Obtained the raw Dataset (DBPL.xml) *Cleaned & Parsed the Dataset into csv format with ""author"" and ""title"" columns *Decided the proper algorithm and libraries for ""Closed Frequent Pattern"" mining and Redundancy reduction (test pre-processings and pattern clustering). *Using UIUC paper authors, created a toy dataset to test the concepts, algorithms and libraries. *Initial trial on the DBLP dataset to generate author itemset frequent patterns 2) Which tasks are pending? *Finalize selection of toolset/library for closed frequent patterns mining on author list and title list *Generate formal itemsets and sequential frequent patterns for clean data set *Using hierarchical clustering algorithm, remove Redundancy from the initial closed frequent patterns *Finalize the context modeling by implementing the weighting function on context indicator and pattern pairs *Analysis of Results *Finalize presentation and report based on implementation and results 3) Are you facing any challenges? *Understanding the concepts and the algorithms in the paper *Paper was written for many general scenarios (e.g. graphs/subgraphs) which aren't necessarily applicable to our implementation; trying to make it more generic made it more difficult to understand the applicability and our relevant extracts *Some of the concepts weren't covered in depth in class (e.g. closed frequent patterns, maximal frequent patterns.) *Some ambiguity (e.g. stop word removal, specifics of laplace smoothing) *Since the paper is dated, we needed to find/explore some of our own more recent libraries and tools, hoping there would be minimal impact on the end result *Implementing weighting functions (Mutual Information) to build Context indicator vectors *Timeline for completing all remaining optional parts of the paper. CS410 Project Proposal Team TR Squirrels 20-Oct-2020 Team TR Squirrels has three members, Captain Ye Xu (Net ID: yex2), member Weidi Ouyang (Net ID: wonyan2) and Raj Datta (Net ID: datta7). We will work on reproducing the listed paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" using Python as the primary coding language. This proposal addressed all related questions from week1 guideline and topic instruction. Answers to these questions were highlighted. 1. Background of the paper This paper proposed a novel approach to generate semantic annotation for frequent patterns that can better interpret the in depth and hidden meaning of the pattern. One meaningful application of this algorithm/procedure is in the field of computational biology. With tremendous sequencing data of genes and their transcripts (e.g. mRNA sequence and protein sequence), annotation of functionality is much needed but barely and poorly supported by the lab-work evidence. Most annotations in our biological databases are counted on computational work that link the known or predicted functions to frequent patterns observed in sequence data. One example is to connect the structural motif of a peptide that describe the connectivity of secondary structural element (e.g. ""helix-turn-helix"" or ""Zinc finger"" ) to a potential function of a protein or part of the protein (e.g. DNA binding domain of a transcription regulator). Biologists, either working on ""omics"" (e.g. genomics, transcriptomics, etc.) or focusing on a particular gene cassette or metabolic pathway, all benefit from such application. A well tagged dataset leads to proficient and precise findings and this is true for applications in areas other than biological science. However, annotation of patterns is challenging and can be very labor intensive. Taking genome annotation for example, in addition to manual annotation (curation), a large part of annotation work is completed by automatic annotation tools based on different algorithms. The most basic one is the homology search tool ""BLAST"" though structural and functional annotation are usually needed to identify and tag the biological information to the genomic elements. This whole process often involves both biological experiment (lab evidence) and ""in silico"" bioinformatic analysis. This paper, however, provided a novel approach that completely based on the Text information retrieval and mining to interpret the discovered patterns and tested the procedure on three different datasets including a gene ontology annotation dataset. 2. Resources and Technique to reproduce the paper We will use the same or similar datasets to reproduce the results. The first dataset is DBLP dataset that provides bibliographical information about computer science journals and proceedings and is available for download (https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html). The second dataset is no longer available from the paper link but similar datasets that include the Gene Ontology terms and Motif for Drosophila are accessible from Gene Ontology Website (Annotation database). The third dataset is provided by BioCreAtIvE Task 1B and is also no longer available. However, we can apply the same approach by crawling abstracts from the MEDLINE database with query keyword ""Drosophila"" and recreate the dataset. We will apply the same technique used in the paper which include the two toolkits ""FP-Close"" and ""CloSpan"" to generate ""Closed Frequent Itemset"" and ""Krovertz stemmer"" to stem the title words. Same or similar clustering algorithms, either Hierarchical Clustering or One-Pass Clustering, will be applied for redundancy reduction. Python libraries and packages, such as ""Scipy"" and ""Scikit-Learn"" provide convenient built-in functions to implement these algorithms too. 3. Brief Timeline of the Project Course Week Dataset Acquisition Week 10 Dataset Processing based on understanding of the pattern context modeling Week 10 - 12 DataSet Oriented modeling and semantic analysis Week 13 - 14 Coding and Presentation finalization Week 15 - 16 The detailed workload and distribution will evolve as the project goes. Guidance of Reproducing the paper: Generating Semantic Annotations for Frequent Patterns with Context Analysis CS 410 Course Project Ye Xu, Weidi Ouyang, Raj Datta Overview of the paper Focus of the paper Not about Frequent Pattern Discovery, but about ""Interpreting the frequent pattern with semantic annotations by Constructing the context model of the frequent pattern Selecting context indicators Extracting representative transactions and semantically similar patterns Input: Frequent Pattern Output: Semantic Annotation of that pattern What is the semantic annotation of the frequent pattern 1. Semantic definition of the pattern can be inferred by its context and words sharing the similar context : Context indicator. 2. Extract the data transactions that best represent the meanings of the pattern. 3. Extract semantically similar patterns (SSPs) of the given pattern, i.e., patterns with similar contexts as the original pattern. What is the semantic annotation of the frequent pattern Definition Example sentences Synonyms or Thesaurus Definitions and Problems The Problem formulation section Some concept Transaction: A collection of itemsets, sequences, graphs... A pattern: A item, a subsequence, a subgraph... Support: Absolute number of transactions that contain the pattern The proportion of transactions containing the pattern in the entire transaction dataset Some Concepts Frequent Pattern: A pattern with support equal or larger than the specified threshold Context Unit: An object from the transaction set that carries semantic information and co-occurs with at least one pattern in the pattern set and in at least one transaction of the transaction set. Some Concepts Context indicator: A select Context Unit of a Frequent Pattern. Each of such units co-occur with the Frequent pattern and associate with a weight to measure its strength of semantic indication of the pattern Major Task Define context units to form the context vector space; Design a strength weight for each unit to model the contexts of frequent patterns in order to extract the most significant context indicators as the ""definition"" for the given pattern unit. Design similarity measures between a transaction and a pattern context and between the contexts of two patterns in order to extract representative transactions and semantically similar patterns to finalize the structured annotation of a given frequent pattern. Context Modeling Vector Space Model for Context modeling A transaction and the context of a frequent pattern both are represented as vectors of context units. Context unit selection Very flexible: Any object in the database that carries semantic information or serves to discriminate patterns semantically can be a context unit. single items, transactions, Patterns any group of items/patterns In the paper, Context unit = Pattern Redundancy Reduction  Context unit == Closed Frequent Pattern Closed Frequent Pattern: A frequent pattern is closed if there exists no super-pattern that has the same support count as this original pattern. How to Find Closed and Maximal Frequent Itemsets from FP-Growth | by Andrewngai | Towards Data Science Further Redundancy removal by micro-clustering (Both are agglomerative clustering with complete link) Hierarchical clustering One pass clustering Strength Weighting for Context Unit Intuitively, the strongest context indicators for a pattern should be those units that frequently co-occur with this pattern but infrequently co-occur with others. Extract Strongest Context Indicator As the Definition With the weighting function for each context unit (ui) of a pattern, compute wi = w(ui, pa), rank ui  U with wi in descending order and select the top k units. Semantic Analysis and Pattern Annotation Semantic Similarity For Pattern pa, its context vector is a vector of unit weighting functions c(a) = [w(u1, pa), w(u2, pa), ..., w(uk, pa)], where ui is the select context unit (closed frequent pattern). Cosine distance is used to compute the similarity between two context vectors. Extracting Representative Transactions Represent a transaction as a vector in the same vector space as the context model of the frequent pattern pa, i.e., c(t) = w1, w2, ..., wk. Compute Cosine Similarity between the transaction and the pattern: s(c(t), c(pa )). Rank the similarity in descending order and select top k transactions. Extracting Semantically Similar Patterns let Pc = {c(p1), ..., c(pc)} be the context vectors for {p1, ..., pc} which are believed to be good candidates for annotating the semantics of pa. Compute sim(c(pi), c(pa)) for each pi  Pc, rank them in descending order, and select the top k pi's. Pc can be very flexible (e.g. the whole frequent pattern set). Workflow of the DBLP dataset Parsing Raw Data Find closed FPs of Authors as Context Units Find closed Sequential FPs of Titles as Context Units Preprocessing titles FP mining Processing title FPs Combine FPs of Authors and Titles to form Context Units Space Weight Matrix of Context Units Annotation of Given Pattern by Context Units with Top Weights Find representative titles of given pattern Find synonyms of given pattern Instruction for Reproducing paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" 1. Overview of the github CourseProject There are three folders, Datasets, PythonCodes and JupyterNoteBookDemo, in the repository. The PythonCodes and the JupyterNotebookDemo contain the same scripts but different file formats (py file and ipynb file). Within each of these two script folders, there are 6 folders giving the execution order for reproducing the paper using the DBLP dataset (paper section 5.1). All datasets imported and generated using the scripts are located in the Datasets Folder. The input and output paths of these Datasets files need to be changed if downloaded to your local computer. Except for the raw dataset ""dblp50000.xml"", all other datasets are generated by the scripts. In addition, the final report, the link (https://mediaspace.illinois.edu/media/t/1_2uzja14v) to the video demo and the powerpoint slides of paper review and project introduction are also in the CourseProject repository. 2. Python Library and Packages numpy, scipy, pandas, nltk, csv, os, mlxend and pyspark are libraries needed for running the srcipts. Except for pyspark, all libraries can be downloaded and installed through pip or conda, depending on your preference and execution environment. The installation of pyspark (and Spark) is a bit complicated and requires some environmental configuration and functional java of version 8.0 or above. Here is the link of the tutorial how to install pyspark/Spark on Windows system: https://www.datacamp.com/community/tutorials/installation-of-pyspark 3. Script running instruction 3.1. Parse the raw data (Folder 1. RawDataParsing) Download ""dblp50000.xml"" and run script ""DBLP_raw_data_parsing.py"" or ""DBLP_raw_data_parsing.ipynb"". This generates the dataset ""DBLP2000.csv"". 3.2. Build the Context Units Space (Folder 2. ContextModeling) 3.2.1. Find closed Frequent Pattern (FP) for Authors using FPgrowth algorithm in MLXtend Lib. Run script ""Author_FP_mining.py"" or ""Author_FP_mining.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""authorsFP2000_with_index.csv"", which contains 14 closed FPs of authors and their transaction index in ""DBLP2000.csv"" (e.g. author ""Edwin R. Hancock"" is a closed FP and it showed in the 839th, 1119th, 1127th, 1204th and 1576th row of DBLP2000, its transaction index list is [839, 1119, 1127, 1204, 1576] ). 3.2.2. Preprocess DBLP titles Run script ""DBPL_preprocessing_titles.py"" or ""DBPL_preprocessing_titles.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""DBLP2000_preprocessed_titles.txt"". In this step, stop words are removed and the titles are stemmed. 3.2.3. Find Title sequential Pattern using PrefixSpan algorithm in PySpark Run ""titles_seqPattern_mining.py"" to import ""DBLP2000_preprocessed_titles.txt"" and to find closed sequential frequent patterns from titles of DBLP2000. I had issue with configuration of Spark in Jupyter Notebook environment therefore no corresponding script in ""ipynb"" format was put in the ""JupyterNoteBookDemo\ContextModeling"" directory . However, the python script was executed successfully in the windows cmd of my laptop. The script ""titles_seqPattern_mining.py"" generates an output folder containing the pattern file ""part-00000"". Set the ""part-00000"" file to txt format. Run ""Title_sequentialFP_processing.py"" or ""Title_sequentialFP_processing.ipynb"" to import ""part-00000.txt"" and generate the cleaned dataset ""titlesFP2000.csv"". 3.2.4. Find transaction index of title sequential patterns Run ""Find_transaction_index_of_title_FPs.py"" or ""Find_transaction_index_of_title_FPs.ipynb"" to import ""titlesFP2000.csv"" and generate ""titlesFP2000_with_index.csv"", which adds the list of transaction index to each title pattern. 3.2.5. Reduce title FP redundancy by microclustering (hierarchical clustering) Run ""Hierarchical_clustering_titleFPs2000.py"" or ""Hierarchical_clustering_titleFPs2000.ipynb"" to import ""titlesFP2000_with_index.csv"" and generate ""titlesFP2000_final.csv"". This script apply the hierarchical clustering with Jaccard Distance defined per paper and clusters 1912 title sequential patterns into 166 clusters. It chooses the most frequent pattern in each cluster as the ""centroid"" pattern to further build the context unit space. 3.2.6. Combine author FPs and title FPs to build the context units space Run ""DBLP2000_context_units_with_transaction_index.py"" or ""DBLP2000_context_units_with_transaction_index.ipynb"" to import 'authorsFP2000_with_index.csv' and 'titlesFP2000_final.csv' and to generate the final context units dataset ""DBLP2000_context_units.csv"". 3.3. Define given frequent patterns using context units defined above (Folder 3. PatternDefinition) 3.3.1. Build weight vectors of FPs in the context unit space Run ""Weighting_function.py"" or ""Weighting_function.ipynb"" to import """"DBPL2000_context_units.csv"" and ""DBLP2000.csv"" and generate ""Context_units_weights.csv"". This script generates context vectors for all context units defined in 2.2 and builds a weight matrix between the pairwised context FPs. Each element of the matrix is the Mutual Information score between the context unit pair per definition in the paper. 3.3.2. Annotate the given FP (e.g. an author) by context units with highest weights Run ""Defining_pattern_with_context_units.py"" or ""Defining_pattern_with_context_units.ipynb"" to import ""Context_units_weights.csv"". In this step, we first pick an author from the author FPs and rank the weights of its context vector. The context units with top 5 weights are selected as the definition of this author and are saved as ""author_annotation_example1.csv"". Similarly, we pick a title from the title FPs and rank the weights of its context vector, and save the context units with top 5 weights as ""title_annotation_example1.csv"". 3.4. Find representative titles of the given pattern (Folder 4. RepresentativeTitles2Pattern) Run script ""Find_representative_titles_to_pattern.py"" or ""Find_representative_titles_to_pattern.ipynb"" to import ""DBLP2000_context_units.csv"", ""DBLP2000.csv"" and ""Context_units_weights.csv"". This script first generates the weight matrix of transactions (titles) in the context units space as the dataset ""transaction_weights.csv"", and then computes the cosine similarity between the transaction weight vectors and the given pattern weight vector (e.g. the same author and title chosen in 2.3.2). The similarity matrix of transaction to author FPs is saved as ""similarity_scores_of_transaction_to_author.csv"", and the similarity matrix of transaction to title FPs is saved as ""similarity_scores_of_transaction_to_title.csv"". This script then generates the top 5 representative titles with highest similarity scores to the given author and to the given title pattern as dataset ""rep_titles_author_example1.csv"" and ""rep_titles_title_example1.csv"", respectively. 3.5. Find synonyms of the given pattern (Folder 5. Synonyms2Pattern) Run ""Find_synonyms_of_pattern.py"" or ""Find_synonyms_of_pattern.ipynb"" to import ""Context_units_weights.csv"" and to compute the cosine similarity between the candidate patterns of similarity (e.g. all closed frequent patterns of authors) and the given pattern (e.g. the same author and title chosen in 2.3.2). Select the authors with the highest 5 similarity scores as the synonyms of the given author or title other than the author or title itself. This script generates 2 datasets for synonyms of author pattern: ""coauthor_to_author_example1.csv"", ""syn_titles_to_author_example1.csv"", and 2 datasets for synonyms of title pattern: ""syn_titles_to_title_example1.csv"" and ""syn_authors_to_title_example1.csv"". 3.6. A final display of the context annotation of the given pattern (Folder 6. ContextAnnotation) Finally, Run ""Author_context_annotation_example1.py"" (or ""Author_context_annotation_example1.ipynb"") and ""Title_context_annotation_example1.py"" (or ""Title_context_annotation_example1.ipnb"") to combine the output datasets generated in step 2.4, 2.5 and 2.6. This script builds the two examples of context annotation for the given author pattern and the given title pattern respectivley and fullfills the two experiments in paper section 5.1."
https://github.com/blake-wright/CourseProject	"Course Project Proposal 1. I, Blake Wright, will be working on this project individually. NetID: blakekw2 2. I will join the text classification competition. 3. I am ready to learn state-of-the-art neural network classifiers. I am in constant search for classifiers or frameworks that I will be using. BERT seems to be a very popular framework. However, I did also find XLNet which can empirically outperform BERT on some instances and RoBERTa which also outperforms BERT on the GLUE benchmark. I am interested in some of the later listed models that are based off BERT as I find it interesting that they tried to address key problems they found lacking in BERT. Experience: I have no experience in any of these methods. I have used Tensorflow very mildly to try to build a prediction model for stock and cryptocurrency prices. However, no experience in using text classification. I hope this project will further my interest in using these frameworks. 4. I plan on using Python as my programming language. Blake Wright Nov 29, 2020 Progress Report - Text Classification 1. I have done more research and determined to use BERT (from Google) in order to complete my project. I have also done research on how to use BERT and some tutorials to familiarize myself with the libraries. I have also got my environment, libraries, and data set as needed and ran a short test to ensure I had everything. 2. The main pending task is the implementation of the coding and further documentation on my overall project. 3. The primary challenge was time. I have been busier than expected with my job the past couple weeks and haven't been able to really get to focus on the project. However, the next couple weeks I will be able to focus in and complete the project. I do feel like the implementation and producing a reasonable score will be the most difficult part. I am sure I will run into roadblocks during the coding. Classification Competition - Analyzing Twitter Tweets Video link to presentation: https://drive.google.com/file/d/17dMSY3kKD93lZayn_cvysVckzSeQaHD7/view?usp=sharing In the video I go over most of this README besides the setup. I do have another video below that is my own video of the environment setup. Video link to environment setup (farther down you will see the videos from Jeff Heaton where I sourced this information): https://drive.google.com/file/d/1hQFQuth2hXUuuUFBRt_a4RNttmnmoQ8x/view?usp=sharing conda env create -v -f tensorflow.yml python -m ipykernel install --user --name tensorflow --display-name ""Python 3.7 (tensorflow)"" Setting up your environment You will need the following libraries to successfully run my project: Library Version Used Pip install cmd Tensorflow 2.3.1 pip install tensorflow Sklearn-learn 0.23 pip install sklearn Transformers 3.5.1 pip install transformers Pandas 1.1.3 pip install pandas Numpy 1.18.5 pip install numpy Torch 1.7.1 pip install torch The following videos can be used as a reference on how to setup a miniconda python environment if you don't have any of the libraries and want some setup automatically. However, for the tensorflow.yml file you will want to update the tensorflow=2.0 to tensorflow=2.3.1. Or just copy the below. ``` name: tensorflow dependencies: - python=3.7 - pip>=19.0 - jupyter - tensorflow=2.3.1 - scikit-learn - scipy - pandas - pandas-datareader - matplotlib - pillow - tqdm - requests - h5py - pyyaml - flask - boto3 - pip: - bayesian-optimization - gym - kaggle ``` For Windows: https://www.youtube.com/watch?v=RgO8BBNGB8w For MacOS: https://www.youtube.com/watch?v=MpUvdLD932c&t=372s Running the project Before trying to run the project note that this is a very resource demanding program. I have tested it on the following pieces of hardware. Desktop: CPU: 3.9 GHz Ryzen 7 3800X RAM: 32GB DISK: < 10GB available Laptop: CPU: 3.1 GHz i5 RAM: 16GB DISK: < 200GB available If your hardware isn't able to run please contact me and you can use my system. I am working on additional ways to test on lower RAM devices. To run this project: If you have jupyter notebook you can locate the file, select file, and choose the Cell tab and then select Run All. Structure of the project I first imported the data that was given by using the pandas library. ``` read in train & test data trainData = pd.read_json('train.jsonl', lines = True) testData = pd.read_json('test.jsonl', lines = True) ``` Next I converted the labels (""SARCASM"" and ""NOT_SARCASM"") to binary values. This was done because the model requires a binary label to respond to. for i in range(len(trainData)): if trainData['label'][i] == ""SARCASM"": trainData['label'][i] = 1 else: trainData['label'][i] = 0 From here I set my modelId, tokenizer, and model. I choose to include case and I felt that sometimes when people are sending out sarcastic tweets they may often use letter case to further voice their sarcasm. As you can see I used the AutoTokenizer from the transformers library as I would not have to switch it when using different models. I initiailly wrote this project using DistilBert, which is far faster than Bert and is almost as accurate. I also did try using BERT, I did get better results (~2% accuracy) but when I uploaded them to the leaderboard they were slightly worse. I also tried XLNet and RoBERTa but with my code they were performing up to 10% less accurate than BERT. I believe this was because I was not able to configure them as precisely. ``` setting modelId, tokenizer, and model modelId = ""distilbert-base-cased"" tokenizer = AutoTokenizer.from_pretrained(modelId) model = DistilBertModel.from_pretrained(modelId) ``` Next up was getting all of the data ready for the model. Tokenizing the data, padding the lengths, and masking so the padding was not used. ``` tokenized = trainData['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens = True))) tokenized_2 = testData['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens = True))) trainPad = pad_sequences(tokenized, maxlen = 100, padding='post') testPad = pad_sequences(tokenized_2, maxlen = 100, padding='post') trainMask = np.where(trainPad != 0,1,0) testMask = np.where(testPad != 0,1,0) converting to int64 trainInput = torch.tensor(trainPad).to(torch.int64) testInput = torch.tensor(testPad).to(torch.int64) converting to tensor type trainMask = torch.tensor(trainMask) testMask = torch.tensor(testMask) ``` I concurrently modeled both the training set and the testing set. I have done these concurrently because upon prediction the input will have to match what it was trained against. with torch.no_grad(): output = model(trainInput, attention_mask = trainMask) with torch.no_grad(): outputTest = model(testInput, attention_mask = testMask) Here I prepared features and labels that will be used to train and test. trainFeats = output[0][:,0,:].numpy() testFeats = outputTest[0][:,0,:].numpy() labels = trainData['label'] trainFeats, valFeats, train_labels, test_labels = train_test_split(trainFeats, labels) I tried many, many different classifiers and RandomForestClassifier teneded to fair the best at ~77%. 77% accuracy was lower than expected and desired. I will expand on improvements in the future in the 'Improvement' section. classifier = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=8, random_state=2) classifier.fit(trainFeats, train_labels) classifier.score(valFeats, test_labels) For the rest of the program I am just creating the answer file and writing to it. There is a small bug where it sometimes does not write the entire file. I am troubleshooting this problem. I included a print screen so you could see the results being written. ``` results = classifier.predict(testFeats) f = open(""answer.txt"", ""w"") print(results[1799]) for i in range(len(results)): id = str(testData['id'][i]) answer = str(results[i]) f.write(id) f.write(',') if answer == ""1"": f.write(""SARCASM"") else: f.write(""NOT_SARCASM"") f.write('\n') print(id, answer) ``` Improvements I spent a large (maybe too much) time trying to train the model instead of using a pretrained model for BERT. I was not very successful in this and wish I could have had more time to expand on it as I feel like this would have greatly increased results. I also considered manipulating the tweets. Some things I considered were taking out common words that would be used in a sarcastic or not sarcastic and throwing them out as my model may have been able to better train on relevant information. There was also consideration on furthering expanding weighting on the hashtags found in tweets as many of them seemed to correlate strongly from my point of view. However, I was not sure how to go about this. I did not use the context as part of the analysis as well. I feel like this could have been a big improvement without too much more additional code. However, time was a factor in this project and I was not able to complete this task. Credit/Documentation Huggingface's website was a great help. The transformers library is maintained by them which was used in this project. They also provide ample of documentation on how to use them. I also found their examples extremely useful in understanding the flow of the program. I have included links to both the home page and to the example. https://huggingface.co/ https://huggingface.co/transformers/model_doc/distilbert.html (Documentation on library) https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb Jay Alammar was also a great help to this project. His visual guide helped fill in knowledge gaps of how each part of the model worked and which types it needed. (http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)"
https://github.com/bo8b/CourseProject	"CS 410: Text Information Systems Group Project Documentation Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Project Installation: The following instructions have been tested with Python2.7 on Linux and MacOS Source code (including Team Green Koala enhancements) can be found at https://github.com/CS410Fall2020/CourseProject You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web, run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should then be available at http://localhost:8096/ How to use: The EducationalWeb system was developed by some of the students in Prof. Zhai's research group for navigating through course slides. At present, it only contains the slides for CS410, but could be expanded in the future to include other courses. Note that if more courses are added in the future, the names for their directories and slides must follow the same naming conventions as used for CS410: Directory name can really be anything, but it may be helpful if it is a descriptive name of the lecture series. For instance, one CS410 directory was named ""06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt"". Using a name this long is not required, nor is it even recommended, as it causes problems for Windows users. Then within that directory, the slide names must be sequentially numbered, starting at zero (""0"") in this format: class name (""cs-410"") followed by four dashes (""----"") followed by directory name followed by four dashes (""----"") followed by ""slide"" + number + "".pdf"" Below are some of the features of the EducationalWeb tool: Choosing a lecture using the drop-down list in the navigation bar Sequentially navigating through the lectures/slides using the Next and Prev buttons at the bottom of a slide Searching for relevant slides using the search bar at the bottom of a slide Navigating to a related/recommend slide from the column on the right Finding an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""Explain selected text"" button on the top-right of a slide. It will try to retrieve a relevant section from Professor's textbook that contains an explanation of the selected phrase. Downloading the current slide being viewed by clicking on the ""Download"" button, to the right of the ""Explain selected text"" button. Downloading all slides for the lecture for the current slide being viewed by clicking on the ""Download All"" button, to the right of the ""Download"" button. (***NOTE: this is the function that was added by Team Green Koalas***) Download Download Explain selected text Explain selected text Download All Download All Implementation: First, in the /build/generic/web/viewer.js file, we included some code that allows us to see the url for the slide currently being viewed. Then, in the /build/generic/web/viewer.html file, we updated some code and wrote several new functions: alertAll() is used to get the current-slide URL we provided in viewer.js (above) and convert it to a usable format for our purposes. getURLs() converts the formatted current-slide URL into a list of potential slides that may be contained within that file's folder. zipItUp() takes that list of potential file names and adds any existing files to a zip file (""slide-series.zip"") and then provides the opportunity for a user to save that zip file through their browser. We also added a new button ""Download All"" to initiate this function. Finally, in the /build/generic/web/viewer.css file, we made changes necessary to format the ""Download All"" button referenced above. We also fixed the graphic for the existing button for the ""Explain Selected Text"" button to allow it to fit properly. Justification: Our intention was to also take on one additional improvement (performance, enabling new courses, etc.), but ran into several impediments that affected our ability to do so. First, setup of the EducationalWeb environment took much, MUCH longer than anticipated. One team member was tracking his time and can show (in Timesheet.xlxs) that he spent over 10 hours just trying to configure the system. Most of the problems were caused either by attempting to set up the tool in a Windows environment or using a newer version of Python. Another team member finally had success on a Mac, so the third team member borrowed a Mac computer, which allowed him to finish setup relatively quickly after that. Also, we did not realize when taking on this project that the work that needed to be done was almost entirely in Javascript. One team member has no experience in Javascript, and the other two have only slight knowledge, so it took a while just to get up-to-speed with this technology. If we had known more, we would not have been so cavalier in just assuming we'd be able to zip an entire folder at once or else get a list of files from a server folder on a client Javascript - which we now know we cannot do. MUCH time was spent researching ways to do this, including Node.js, JQuery, PHP, Ajax, and Kintone. We finally reverted to just using the file naming convention in our favor and assuming there would be fewer than 99 files in any lecture directory. According to the Timesheet.xlsx file, one team member spent over 30 hours on this project, which exceeds the requirements for this assignment, so we did not address any issue beyond the multiple-slide download enhancement. CS 410: Text Information Systems Group Project Progress Report Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Project Progress Report: 1) Which tasks have been completed? Two of the three team members have successfully installed the EducationalWeb tool in our local environments. 2) Which tasks are pending? Work has begun on enabling downloads for multiple slides at a time, but it is not completed yet. We need to complete that and (possibly) work on a performance improvement. We then need to produce the video talk. 3) Are you facing any challenges? Getting the EducationalWeb system up and running in local proved to be more difficult than expected, particularly in Windows. One team member is still struggling with this, but we are helping him through the last of his issues. Also, none of us are Javascript experts, so it is taking more time than we originally thought even to do a relatively simple enhancement. We are committed to completing on time, though. CS 410: Text Information Systems Group Project Proposal Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Proposal: For this group project, Team Green Koalas will be improving the Educational Web System. The Educational Web System is a tool recently developed by some of the students in Professor Zhai's research group that allows students to view all lecture slides from course CS 410 (Text Information Systems). This tool supports choosing a specific lecture's worth of lecture slides, sequential navigation through individual slides, searching for relevant slides, navigating to other recommended slides, and finding term and phrase definitions from the textbook. Specifically, we will develop the ability to allow for bulk downloading of slides. This simple enhancement would provide concrete value to future students, instructors, and other users of this system for years to come. Understanding that the expectation is for each group member to spend around 20 hours of effort on this assignment, if we are able to complete the bulk download in less than 60 hours (which seems likely), we will then move onto another topic, such as improving performance or expanding the content to include slides and textbooks from other courses. This could potentially expand the user base to students and instructors of other courses as well. To the best of our knowledge, no other tool of this type exists, at least specific to the lecture data for which we are using it. In terms of existing resources, we would obviously be modifying the source code for the tool. If we do take on the task of expanding to include other courses, we will require access to the lecture slides and possibly the textbooks for any additional classes. The first step would have to be securing permission from the owner of the lectures and textbooks to use these resources, to ensure we do not violate Intellectual Property rights. For downloading slides in batches, it should be easy enough to do through stitching individual slides into a single PDF or some other method. For improving performance, we would investigate buffering next slide and previous slide to speed up that portion of the application. If we expand to new courses, then no technology would change, we would merely be making sure the application is scalable enough to handle the new data. It should be a trivial effort to show the usefulness of bulk downloading or improved performance through a recorded demonstration comparing the current state with the new functionality. If we expand to include other classes, perhaps we could include a student from the other class or another student from our class (if the new course is not currently in session) and record a quick usability study. We expect to have a general understanding of the existing application by week 10. We will take weeks 11 and 12 to enable bulk download. Weeks 13 and 14 can then be used for one additional enhancement, followed by Week 15, which will be set aside for the recording the demonstration. CS 410: Text Information Systems Group Project Proposal Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Tutorial Video Link: https://youtu.be/geXK8Ohcg3c Team Green Koalas CS410 Group Project For this group project, Team Green Koalas will be improving the Educational Web System. The Educational Web System is a tool recently developed by some of the students in Professor Zhai's research group that allows students to view all lecture slides from course CS 410 (Text Information Systems). This tool supports choosing a specific lecture's worth of lecture slides, sequential navigation through individual slides, searching for relevant slides, navigating to other recommended slides, and finding term and phrase definitions from the textbook. Specifically, we will develop the ability to allow for bulk downloading of slides. This simple enhancement would provide concrete value to future students, instructors, and other users of this system for years to come. This enhancement required each group member to spend around 20 hours of effort on this assignment, so we were not able to move onto any another topic. Team members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Submission Files: Project Proposal - ""Green Koalas Project Proposal.docx"" Project Progress Report - ""Green Koalas Project Progress Report.docx"" Project Documentation - ""Green Koalas Project Documentation.docx"" Project Tutorial Video"
https://github.com/bojiang3/CourseProject	"INTEROFFICE MEMORANDUM To: Ms. Beth Springer, VP John Repogle, CEO Subject: Burt's Bees Date: 8/19/2019 After reviewing this case, I have concluded that Clorox will not become eco-friendly by only purchasing Burt's Bees. However, with Clorox's purchase of Burt's Bees and the Clorox's new initiative to ""think about the Greater Good,"" this acquisition can provide the Clorox with insight on how a successful eco-friendly business operates. Implementing these practices will give Clorox an advantage over their competitors who are trying to enter the eco-friendly market as well. The Burt's Bees brand will see some negative impact because many of their fans are environmentally conscious, and Clorox's reputation when it comes to being environmentally friendly is not well perceived. With this acquisition, Clorox will see more of a benefit than Burt's Bees. With Clorox's purchase of Burt's Bees, they now have an insight into the eco-friendly consumer market that not many of their competitors do not have. According to research done by Clorox, ""53% of consumers"" plan on purchasing more eco-friendly products within the next year. These consumers are also willing to pay more for those eco-friendly products. The information provided tells us there is now a need for more eco-friendly products in the consumer market. Clorox will be able to charge more for these ""luxury"" products, which can be up to 57% more than standard (not eco-friendly) products. Clorox will now be in the position to set the standard of what natural cleaning products should be and will be making a profit from their efforts. Another benefit for Clorox in this acquisition is Burt's Bees research lab; this lab is full of competitor's products that Burt's Bees have been testing. These products are being tested to see if they can be considered a natural product. This testing will give insight into what companies have tried and failed when qualifying for the Natural Products Association label. It will lead Clorox's brand of natural products to become the standard for what natural cleaning products should be. Clorox needs to implement its initiative for ""thinking about the Greater Good"" and inform the market that they are serious about producing more natural and eco-friendly products. Burt's Bees will not see such a positive impact from this acquisition. Although their product does not bare the Clorox name, they are now associated with the company, and many consumers think of bleach when seeing or hearing Clorox. Burt's Bees will need to focus on informing their consumers they still maintain their values of being eco-friendly. There is a need for eco-friendly products in the consumer market, and they will continue to make a profit if they retain those eco-friendly values. Clorox will achieve profitability more so than Burt's Bees in this acquisition. When Clorox implements Burt's Bees business practices to their company and especially to their line of eco-friendly products, it will have a positive impact. There is a need for more eco-friendly products in the consumer market, and consumers are willing to pay for those products. Implementing the eco-friendly products will ultimately lead to a profit even though the cost for producing these products will be more. Clorox should then advertise the fact that they are the standard when it comes to reliable, eco-friendly products. Although Burt's Bees will not be as profitable as Clorox, there is still a need for their eco-friendly products in the consumer market. Group SN #196 CS 410 Final Project Report Group SN # 196: Bojiang Li, David Ye, Yunfei Ma Sentiment Analysis Tool (Option 5: Free topic) This project aims to help students improve paper-reading and -writing ability by providing sentimental and keyword analysis. Midway Topic Change: Since during our original planned project implementation - EducationalWeb, we encountered too many unforeseen obstacles, we midway decided to switch something new - this sentimental analysis project. We have completed the project for giving feedback on the sentiment and giving scores for the sentimentals. We may keep working on the frontend later to to improve the user experience. Technology Used Programming language: Python Microsoft Azure Azure.ai.textanalytics library Azure.core.credentials library Azure.cognitiveservices library Msrest.authentication library Sample Sample Input Text/Paper Group SN #196 Sample Output Result Outcomes and Results -In the rendered, according to every sentence, it will give a score to the phrase. -Sentimental analysis will be given to each one of the sentences. -The high scores on both positive and negative will be noticed because it does not follow the required neutral tone of the essay. 050B.docx  is the sample file we test for the sentimental analysis. For  grammercheck.py , it is the code for checking the right wording for the whole paper. Main.py  stores the user side code for this project. Video Presentation https://mediaspace.illinois.edu/media/1_h6ncumvp Project Proposal - Group SN 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team members: Bojiang Li (bojiang3), Zhanyuan Ye(zye19), Yunfei Ma(yunfeim2). Captain: Bojiang Li 2.What system have you chosen? Which subtopic(s) under the system? We have chosen EducationalWeb system. The subtopic under the system is "" Improving the usability and reach of the existing system "". 3.Briefly describe the datasets, algorithms or techniques you plan to use We will use  Elasticsearch to implement searching functionality. We plan to use Python and JavaScript to build the frontend and backend. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We will have a user interface to test if our function can have the expected performance like observing if the searching results are related to the search keys. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will just implement some additional functions based on the finished system. 6.Which programming language do you plan to use? We plan to use Python and JavaScript. 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. *Project Setup - 15 hours *Implement Function 1 - 15 hours *Implement Function 2 - 15 hours *Implement Function 3 - 15 hours *Test and optimize - 10 hours CS 410 Final Project For the specific information, sample text input and output results, please see ""CS 410 Final Project Report"" in the main menu. Sentiment Analysis Tool (Option 5: Free topic) Group SN (#196) Bojiang Li, Yunfei Ma, David Ye This project aims to help students improve paper-reading and -writing ability by providing sentimental and keyword analysis. The source code is provided in the main folder. Software installation First install the azure coginitive services package service. pip3 install azure3 pip3 install cognitiveservices pip3 install msrest pip3 install docx2txt Then direct run the file with Python. - Python3 works python text_analytics_bing_search_key_phase.py The result will be rendered in textanalyticresult.docx Discussion & Midway Topic Change Since during our original planned project implementation - EducationalWeb, we encountered too many unforeseen obstacles, we decided to switch something new - this sentimental analysis project. We have completed the project for giving feedbacks on the sentiment and giving scores for the sentimentals. We may keep working on the frontend later to to improve the user experience. Result and Outcome In the rendered, according to every sentence, it will give a score to the phrase. Sentimental analysis will be given to each one of sentences. The high scores on both positive and negetive will be noticed because it does not follow the require neutral tone of the essay. 050B.docx is the sample file we test for the sentimental analysis. For grammercheck.py, it is the code for checking the right wording for the whole paper. Main.py will be the user side code for this project Video presentation https://mediaspace.illinois.edu/media/1_h6ncumvp References/Sources https://docs.microsoft.com/en-us/office/troubleshoot/word/spelling-grammar-checker-underline-color https://realpython.com/sentiment-analysis-python/ https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/ Progress Report Paper ID 196 Progress Made: Our team had 3 zoom meetings to discuss and understand the EducationWeb System and we made discussions about the potential bullet points and the potential improvements for the system. The small improvements we are interested in are scaling up the current system, Allowing downloading slides in bulk, adding more context to explanation, and integrating the tool with piazza. We may finally abandon some of the our choices based on the difficulty and time. Remaining tasks:We will push the remaining code to git, and we have finished 1 one the bullet point we mentioned above, and we are still working on scale up the system and allowing downloading slides in bulk this week, we may finish our code production on Dec,5 and have a zoom meeting to test and make improvements of the code. In addition, we may make a Youtube video to present our results to the peer reviewer. Challenges/Issues: If encountered we will attend office hour."
https://github.com/bzhao10/CourseProject	"CS410 CourseProject(Text Classification Competition): Twitter Sarcasm Detection Table of Contents Background Install Usage Presentation Results Contributing Citation Background This is a course project for CS410 Text Information Systems and is a part of a text classification competition, which involves twitter sarcasm detection. In this competition, it is required that you classify a twitter response in a conversation into either 'sarcasm' or 'not sarcasm'. Data The dataset is comprised of two jsonl files, including a train.jsonl for data training and a text.jsonl for text classification. Each line of the training dataset includes the following fileds: - response : the Tweet to be classified - context : the conversation context of the response - label : SARCASM or NOT_SARCASM - id: String identifier for sample. The testing dataset(text.jsonl) differs from the training dataset only in that the each line of the dataset lacks the label. The size of training dataset is 5000 and the size of the testing dataset is 1800. Output The output of the project is an anwser.txt document, each line of which includes both id of test sample and a label of either SARCASM or NOT_SARCASM predicted by the model. Install You can run the whole project on Google Colab. You don't have to install anything locally. Usage Each of the ipynb file in the Code folder represents one solution in the competition. Step 1 Download two jsonl files, including one training data file and one testing data file, from dataset. Step 2 Download one of these ipynb files and open it on Google Colab. Step 3 Upload the two jsonl files to Google Colab under the dataset folder in the project that you have opened in Step 2. Step 4 Run the code line by line using Google Colab. By following all the steps mentioned above, you will get an answer.txt file. Please refer to the presentation video for detailed instructions. Presentation Following is our presentation demo link: https://mediaspace.illinois.edu/media/1_ef6myyuo Results The following table records the best performance achieved (highest F1 score) by using each model: The score marked in bold are passing the baseline scores. | Model| Precision | Recall| F1 | |-------|-------|-------|-------| | ALBERT | 0.65814 |0.77222 | 0.71063 | | ALBERT V2 | 0.64377 |0.56222 | 0.60024 | | BERT | 0.62681 | 0.86778 | 0.72787 | | BERT(RSUP) | 0.64717 | 0.76222 | 0.7 | | BERT(R_context) | 0.66042 | 0.70444 | 0.68172 | | RoBERTa | 0.63974 | 0.89777 | 0.74711 | | RoBERTa-large | 0.54708 | 0.98778 | 0.70416 | | RoBERTa(R_context) | 0.67836 | 0.77333 | 0.72274 | | SqueezeBERT | 0.61348 | 0.91 | 0.73289 | | XML-RoBERTa | 0.61864 | 0.89222 | 0.73066 | RSUP: Remove stopwords and unnecessary puncuations R_context: Reverse Context Contributing Contributors Team Name: GFZ Team Member: Bei Zhao - beizhao3 (Captain) beizhao3@illinois.edu Ryan Fraser - rfraser3 rfraser3@illinois.edu Yiming Gu - yimingg7 yimingg7@illinois.edu Citation Tutorial: Fine-Tuning BERT for Spam Classification Text Classification Competition Proposal Team GFZ 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Bei Zhao - beizhao3 (Captain) Ryan Fraser - rfraser3 Yiming Gu - yimingg7 2.Which competition do you plan to join? Text Classification Competition 3.If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Here are some neural classifiers and deep learning frameworks that we are going to learn and try: *Word2Vec and GloVe Word2Vec and GloVe are word-based models that can map the vocabularies and phrases into vectors of numbers, which can help us to separate the phrases and words. For the context of meaningful classification, they need to combine with neural networks to work out, such as convolutional neural networks (CNN), recurrent neural networks (RNN), artificial neural networks (ANN), etc. *BERT BERT is a context-based model, which can map the vocabularies and phrases based on the sentence position. However, this model is limited to the size of the corpus it could train compared to Word2Vec and GloVe. But it abandons the traditional RNN and CNN formula, therefore, this may perform much faster than Word2vec and GloVe implementations. *Other Models We will try other useful classifiers and frameworks in the competition procedure continuously until we find the best fit one. Here are the relevant experience of each team members: *Bei: Don't have any Machine Learning experience but will try to learn as much as possible in this competition. *Ryan: Completed Data Science Certificate from UCLA for-credit, included the ""Machine Learning Using R"" course which went through the ""Introduction to Statistical Learning"" book. I will need to take some time to get up to speed on these methods but anticipate it should not take long and am prepared to learn these frameworks. I have made progress in the fast.ai tutorials in an effort to learn more about these methods, and have heard about using TensorFlow/Keras/PyTorch to complete these types of tasks. *Yiming: Don't have any Machine Learning experience but will try to learn it in this competition. 4.Which programming language do you plan to use? The main programming language that we are going to use is Python. 5.Competition Milestones: *Text /Data processing In this step, we will prepare and process the raw data. For example, text may contain numbers, special characters, etc. We need to transform the raw data into something that could be used in the project. *Feature extracting We will extract features from the texts in order to classify them. This is important because only when the features are extracted precisely, the text could then be rightly classified. We will evaluate different models in this step as well. *Train and evaluate models We will train different models on a dataset in order to investigate their actual effectiveness. It is essential that we optimize the models according to the results. *Deploy the models in the competition *Revise and keep optimizing if necessary *Document source code and test set predictions *Document the model and the test set predictions, experiments with other methods, hyperparameter tuning, etc. *Create a demo that shows code can run on the test set and generate submitted predictions, etc Team GFZ Final Report Bei Zhao(Captain)  beizhao3@illinois.edu Ryan Fraser  rfraser3@illinois.edu Yiming Gu  yimingg7@illinois.edu Table of Contents: Explain what problem we are trying to solve2 Explain our model3 Explain how we perform the training5 Explain any other methods tried and hyperparameter tuning6 Results from testing various pre-trained model and hyperparameter combinations7 Team Member Contributions9 References10 1 Explain what problem we are trying to solve This code attempts to detect sarcasm in tweets. This is a particularly tricky problem in Natural Language Processing (NLP) due to the nature of sarcasm itself. Sarcasm is highly dependent on the author, the specific topic in question, cultural attitudes about the topic, current events, tone, and much more. The difficulty is further compounded by the fact that sarcasm is (usually) intentionally designed to not sound sarcastic. In a simple example, a person asking his friend if an idea is good may hear that his idea is ""a really good idea,"" when in fact the friend means the opposite. In particular, this code was built for the Text Classification competition for CS 410 at the University of Illinois at Urbana-Champaign. In this competition, 2 sets of tweets are provided. The training set is pre-labeled and identifies if example tweets are sarcastic or not. The test set is not labeled. Both sets also contain the context of the tweets, which in this context means the parent tweets that the tweets in question are responding to. The goals of the competition are twofold. First, teams must outperform the baseline, which means realizing an F1 score of about 0.723 (based on current results on LiveDataLab). Second, teams must try to rank highest out of all other teams. 2 Explain our model We achieved our best results for this task by using a pre-trained deep learning model called RoBERTa. This model builds off of another well-known deep learning model called BERT, which stands for ""Bidirectional Encoder Representations from Transformers."" A few key differences between BERT and RoBERTa are outlined below: As one can see, RoBERTa takes longer to train because it has much more data. It also does not use Next Sentence Prediction, which is explained further below. At its core, BERT uses Transformers. These are a deep learning construct that ""learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms - an encoder that reads the text input and a decoder that produces a prediction for the task."" A key feature of BERT is that it is bidirectional. This means it reads all of the words at once and uses all context available. Prior models used left, right, left-right, right-left context or some combination of those. This means that BERT obtains a more holistic understanding of context versus prior methods, usually leading to better results. In very general terms, BERT uses 2 strategies to train on the data. First is Masked LM and second is Next Sentence Prediction. 3 In Masked LM, BERT hides a number of words in each sequence before training and then attempts to predict the hidden words. The loss function only considers the prediction of those hidden values. Because of this, the model is a bit slower to converge than prior models that did not make use of the full context. In Next Sentence Prediction, the BERT model also ""receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence."" Note that RoBERTa does not use Next Sentence Prediction. Both strategies are used when training the model and the goal is to balance and minimize the loss of the combined strategies. We believe that RoBERTa outperforms BERT for sarcasm detection in tweets because tweets are very short and tend to have less overall context. Therefore, it makes sense that a scoring mechanism that does not focus as much on Next Sentence Prediction will be more fit for purpose for this competition. 4 Explain how we perform the training Our very general outline of steps taken is below. To keep things concise, only high level descriptions are given. Our code is somewhat short and more detailed documentation is available in the code comments. Basic Steps: *Convert JSON data to CSV format while cleaning the labels *Create k-fold cross-validation training sets *Create a list of dictionaries for our training and test data *Create CSVs with the context data, the response data, and the labels *Convert CSVs into pandas dataframes *Split data into respective training/test sets *Select model and import into the environment *Encode the text data using the Tokenizer library *Convert data into Tensors using PyTorch *Set up model parameters *Batch Size *Wrap Tensors *Create sampler for sampling data during training *Create dataLoader for training set *Freeze parameters *Create model class and initialization logic *Initialize model *Import model optimizer from HuggingFace Transformers library *Compute starting class weights *Convert class weights to Tensors *Define number of training epochs *Define model training function *Define model evaluation function *Run and train the model 5 Explain any other methods tried and hyperparameter tuning The team spent time on a number of different tasks outside of just building the model: 1.We spent a few weeks researching the competition and educating ourselves on deep learning models. When we first started, we believed we'd use word2vec and/or sentence2vec, two approaches for NLP. However, as we did more research, we found that there were better models we could use. 2.We spent time trying various deep learning models to see how each performed. We used a library from HuggingFace that allowed us to easily swap out pretrained NLP deep learning models. Below we've outlined the performance of various pretrained model/hyperparameter combinations, which is how we ultimately decided to use RoBERTa. 3.We also spent time trying to identify which data to train the model on. The data includes both target tweets and the context surrounding them, therefore, there were 3 intuitive combinations we felt were worth trying: response-only, context-only, and a combination of both. We found that the combination of both works best. This made sense to us given the very nature of sarcasm. In fact, sarcasm is almost always an extremely context dependent response to a topic that is oftentimes serious, or not inherently sarcastic. By its nature, sarcasm taken out of context sounds like any other normal statement. (Just think of the phrase, ""good work,"" which can take on many different meanings depending on a huge number of factors. Most of the time, this should be taken as a friendly, encouraging, and positive statement. However, depending on the context, it could also be a sarcastic statement.) 6 Results from testing various pre-trained model and hyperparameter combinations Following are the different testing results we've tried so far (We only include the best scores of different setup models). Passing the baseline F1 scores are highlighted in green color. 7 Model precision recall f1 Model Description ALBERT BS=25 0.65294 0.74000 0.69375 ALBERT model with Batch Size 25 ALBERT BS=25 MSL= 300 0.64178 0.70667 0.67266 ALBERT model with Batch Size 25 max_seq_len=300 ALBERT BS=26 0.64400 0.75778 0.69627 ALBERT model with Batch Size 26 ALBERT BS=27 0.63944 0.75667 0.69313 ALBERT model with Batch Size 27 ALBERT BS=28 0.63520 0.77000 0.69613 ALBERT model with Batch Size 28 ALBERT BS=29 0.62868 0.76000 0.68813 ALBERT model with Batch Size 29 ALBERT BS=30 0.62960 0.79889 0.70421 ALBERT model with Batch Size 30 ALBERT BS=31 0.65814 0.77222 0.71063 ALBERT model with Batch Size 31 ALBERT BS=32 0.65145 0.69778 0.67382 ALBERT model with Batch Size 32 ALBERT BS=33 0.64371 0.77889 0.70488 ALBERT model with Batch Size 33 ALBERT BS=34 0.61558 0.82556 0.70527 ALBERT model with Batch Size 34 ALBERT BS=35 0.65490 0.74222 0.69583 ALBERT model with Batch Size 35 ALBERT V2 BS=31 0.64377 0.56222 0.60024 ALBERT model v2 with Batch Size 35 BERT BS=25 0.68477 0.71444 0.69929 BERT model with Batch Size 25 BERT BS=26 RS&UP 0.63957 0.72556 0.67985 BERT model with Batch Size 26 remove stopword and unnecessary punctuations BERT BS=27 0.65652 0.76667 0.70733 BERT model with Batch Size 27 BERT BS=27 RS&UP 0.64245 0.75667 0.6949 BERT model with Batch Size 27 remove stopword and unnecessary punctuations BERT BS=28 0.62681 0.86778 0.72787 BERT model with Batch Size 28 BERT BS=28 R_context 0.66042 0.70444 0.68172 BERT model with Batch Size 28 and Reverse Context BERT BS=28 RS&UP 0.64717 0.76222 0.7 BERT model with Batch Size 28 remove stopword and unnecessary punctuations BERT BS=29 0.6519 0.80111 0.71884 BERT model with Batch Size 29 BERT BS=29 R_context 0.67768 0.57000 0.61919 BERT model with Batch Size 29 and Reverse Context BERT BS=29 RS&UP 0.64358 0.70222 0.67163 BERT model with Batch Size 29 remove stopword and unnecessary punctuations 8 BERT BS=30 0.68614 0.66556 0.67569 BERT model with Batch Size 30 RoBERTa BS=28 0.66057 0.80222 0.72453 RoBERTa model with Batch Size 28 RoBERTa BS=29 0.64521 0.86888 0.74053 RoBERTa model with Batch Size 29 RoBERTa BS=29 R_context 0.67836 0.77333 0.72274 RoBERTa model with Batch Size 29 and Reverse Context RoBERTa BS=30 0.66324 0.78778 0.72016 RoBERTa model with Batch Size 30 RoBERTa BS=31 0.63974 0.89777 0.74711 RoBERTa-large model with Batch Size 31 RoBERTa-large BS=27 0.64571 0.62778 0.63662 RoBERTa-large model with Batch Size 27 RoBERTa-large BS=28 0.54708 0.98778 0.70416 RoBERTa-large model with Batch Size 28 RoBERTa-large BS=29 0.71308 0.37556 0.49199 RoBERTa-large model with Batch Size 29 SqueezeBERT BS=28 0.6506 0.72 0.68354 SqueezeBERT model with Batch Size 28 SqueezeBERT BS=29 0.6537 0.74667 0.6971 SqueezeBERT model with Batch Size 29 SqueezeBERT BS=30 0.65345 0.76889 0.70648 SqueezeBERT model with Batch Size 30 SqueezeBERT BS=32 0.63973 0.84444 0.72797 SqueezeBERT model with Batch Size 32 SqueezeBERT BS=33 0.61348 0.91 0.73289 SqueezeBERT model with Batch Size 33 SqueezeBERT BS=34 0.63851 0.84 0.72553 SqueezeBERT model with Batch Size 33 XML-RoBERTa BS=27 0.70183 0.34 0.45808 XML-RoBERTa model with Batch Size 27 XML-RoBERTa BS=28 0.6059 0.91222 0.72816 XML-RoBERTa model with Batch Size 28 XML-RoBERTa BS=29 0.61864 0.89222 0.73066 XML-RoBERTa model with Batch Size 29 XML-RoBERTa BS=29 MSL=150 0.64249 0.82667 0.72303 XML-RoBERTa model with Batch Size 29, max_seq_len = 150 XML-RoBERTa BS=30 0.60588 0.91556 0.72920 XML-RoBERTa model with Batch Size 30 Team Member Contributions -Bei Zhao (Team Captain):  Bei served as our team captain for this project. Bei helped on all facets of the project. She wrote our initial proposal and coordinated all meeting times. Bei also spent a lot of time working to try various model/hyperparameter combinations to get the best results. -Yiming Gu:  Yiming was instrumental in getting our model up and running. He was quickly and efficiently able to get us a very solid foundation to work off of, such that we could spend time focusing on the best model and hyperparameters. Yiming also spent a lot of time trying various model and hyperparameter combinations to find the best results and wrote the README for the project. -Ryan Fraser:  Ryan helped the team strategize about how to approach the competition. He spent time trying to get the model to run on a GPU for faster training times, but ultimately the team decided to use CPUs. Ryan also wrote the project progress report, the documentation, and the final report. 9 References Amardeep Kumar and Vivek Anand 2020. Transformers on Sarcasm Detection with Context.  Proceedings of the Second Workshop on Figurative Language Processing Debanjan Ghosh, Avijit Vajpayee, Smaranda Muresan, and Educational Testing Service 2Data Science Institute, Columbia University {dghosh, avajpayee}@ets.org smara@columbia.edu 2020. A Report on the 2020 Sarcasm Detection Shared Task. arXiv preprint arXiv:2005.05814. GLUE Benchmark Leaderboard. (n.d.). Retrieved November 11, 2020, from https://gluebenchmark.com/leaderboard Hankyol Lee, Youngjae Yu, and Gunhee Kim. Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context.  arXiv preprint arXiv:2006.06259. Ingham, F. (2018, November 27). Dissecting BERT Part 2: BERT Specifics. Retrieved November 11, 2020, from https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73 Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, and Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.  arXiv preprint arXiv:1810.04805. Nikhil Jaiswal 2020. Neural Sarcasm Detection using Conversation Context. Proceedings of the Second Workshop on Figurative Language Processing Tanvi Dadu and Kartikey Pant 2020. Sarcasm Detection using Context Separators in Online Discourse.  arXiv preprint arXiv:2006.00850. Xiangjue Dong, Changmao Li, and Jinho D. Choi 2020.Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media.  arXiv preprint arXiv:2005.11424. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach.  Conference paper at ICLR 2020 Suleiman Khan, Ph.D. ""BERT, RoBERTa, DistilBERT, XLNet - Which One to Use?"" Medium , Towards Data Science, 17 Oct. 2019, towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8. 10 UIUC Online MCS - CS410: Text Information Systems Fall 2020 Team GFZ (Bei Zhao, Yiming Gu, Ryan Fraser) Project Progress Report Progress Made For our project, we decided to compete in the Text Classification Competition. In summary, we have become more familiar with deep learning generally and BERT in particular, and have implemented a working model that surpasses the competition baseline. Our weekly status is outlined below for more detail: * Week 1: the team spent time individually researching various approaches to solving this problem. We explored Kaggle competition notebooks, browsed academic papers, and searched the internet for tools that could be useful. We then aggregated and summarized our findings. * Week 2: the team agreed to focus on using the BERT deep learning model for our submission. All of us are new to deep learning, so we spent this week learning more about deep learning generally and BERT in particular. * Week 3 and 4: to better understand BERT and prepare ourselves for the competition, we all individually used this time to get BERT working on our local machines. This helped us understand how data needed to be cleaned and prepared for use, and also helped us understand the behavior of the model when different parameters change. * Week 5: we are currently working on enhancing model performance. We are working to get our model running on Google Colab and we are also experimenting with a few changes to our approach. First, we are testing if we can improve performance by cleaning the dataset in different ways. Second, we are attempting to implement an enhanced version of BERT called RoBERTa to see if this helps. Remaining Tasks 1. Fine tune the model and prepare it for final submission a. Try c1+c2+c3 preprocessing of context with response and c3+c2+c1 preprocessing of context b. Try other models such as XLM-RoBERTa, ALBERT, SqueezeBERT, etc. c. Change model parameters to optimize performance d. Change train_test_split parameters and max_seq_len parameters to optimize performance 2. Document how to use the code 3. Document how the code is implemented 4. Software usage tutorial presentation Challenges/Issues Being Faced In general, things are going very smoothly. Our toughest challenge so far has been identifying a suitable solution to the problem and writing code so that it works for this specific dataset and use case. Getting up to speed quickly on deep learning, while also taking this course, has proven time consuming."
https://github.com/ccasey645/CourseProject	"Team Members: NetID's of all team members: cdblair2. Team of 1. cdblair2 (Casey Blair) is the captain. Topic: I have chosen to update Metapy's Python version compatibility to Python 3.8 on both Windows 10 and OSX 10.15. Description: Metapy currently will not install on Windows 10 or Mac OSX 10.15 with Python 3.8.*. My project will focus on updating Metapy's code to be compatible with Python 3.8 as it is the most stable recent version of Python. For security reasons, Python codebases around the world have been upgraded from Python 2.* to Python 3.* since Python 2.*'s deprecation in January of 2020. Python 2 is no longer being maintained and using Python 2.* code in production is a security risk. However, when running the Metapy toolkit on my assignments in this class, my classmates and I were only able to run Metapy with Python 2.7 (there were posts in Piazza and Slack about using Python 2.7 exclusively as Python 3.* compatibility was reported broken by everyone who attempted to use Python 3). If I want to use this immensely useful toolkit in production code, I'm going to have to update the Metapy toolkit's Python compatibility to Python 3.* to run with any production Python server/project, so I would like to contribute to this open source toolkit by updating the repo to work with Python 3.8. Justifying the workload is at least 20 hours: I've attempted to run Metapy on OSX and Windows 10 using Python 3.8 and neither would install correctly: both OS's failed to install Metapy during the setup.py file's when using the command line command ""pip install metapy"" with Python 3.8. Both OS's threw errrors related to the C bindings. I'm going to have to research which C bindings are broken, research Pybind11 and get familiar with C bindings for Python, and possibly research OS related issues to get Metapy to work with both Windows 10 and OSX 10.15. The issues with C bindings may take a while to fix as it appears that import of various CMake libraries in Windows and OSX have changed and will need substantial refactoring to work. Also I will need to get familiar with Pybind11 and C bindings for Python. And there will be a lot of testing required to make sure functionality is working as expected: example programs will need to be run (if they exist), and tests will need to be updated. CS 410 Casey Blair Final Project Progress Report 11/29/2020 My final project is to update the Metapy GitHub repo to allow for installation with Python 3. Currently, when trying to install Metapy using Pip on both Windows 10 and Mac OSX, if using a Python 3 environment, the setup wheel fails to install the Metapy package due to errors. However, if using a Python 2.7 environment on both Windows 10 and OSX, the Metapy library will install correctly. Since Python 2 was deprecated in January of 2020, updating the Metapy code base to work with Python 3.* environments is vital for using Metapy in production code bases. I have been researching Pybind11 and how Python can be used with C and C++ code to understand how the Metapy Git repo works. Metapy is a Python wrapper for the Meta Github repo (https://github.com/meta-toolkit/meta). The Meta code base is written in C++, and when the Metapy package is installed with Pip, the Meta C++ code is downloaded and installed then a Python interface is constructed so the code can be used from a Python interpreter. When installing Metapy in a Python 2.7 environment, the Pip installer downloads a tar.gz file from the Github repo's Releases page with the Metapy code already compiled for a Python 2.7 environment, which is why it can be installed correctly with a Python 2 environment. All the necessary files, including third party libraries, are already downloaded and bundled in the tar.gz file that is then used to install Metapy. However, when installing Metapy using Python 3.*, instead of using this tar.gz file from the GitHub repo's Releases page, the Pip setup wheel builds the Metapy code from scratch, which requires redownloading all third party libraries and compiling the C++ code in the Meta Git submodule. The third party library download step fails because a library the Meta repo uses is failing to download from the URL specified in the Meta Makefile build steps. The following error is repeated over and over again and the installation ultimately fails after 5 retries of downloading the missing package: -- Downloading... dst='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz' timeout='none' -- Using src='http://download.icu-project.org/files/icu4c/58.2/icu4c-58_2-src.tgz' -- [download 100% complete] * Closing connection 2 * Closing connection 0 * Closing connection 1 -- verifying file... file='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz' -- MD5 hash of /Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz does not match expected value expected: 'fac212b32b7ec7ab007a12dff1f3aea1' actual: 'ac2ff460bdda9c70e6ed803f5e4d03fb' -- Hash mismatch, removing... -- Retrying... First, I had to determine which errors were occurring on both operating systems (Windows 10 and OSX) to determine if the installation errors were related or operating system dependent. It turns out the same error is occurring on both Windows 10 and OSX which led me to believe the issue was not operating system dependent and that I needed to follow the build steps to determine why this third party library was failing to install. I had to figure out why the Metapy package was installing correctly for Python 2 but not Python 3 as it didn't make sense that if the C++ code was being compiled on my laptop why it would compile correctly for Python 2 but not Python 3 since the installer would be using the same C++ compiler. Now that I understand that this ""icu4c"" library is included in the tar.gz file downloaded for Python 2 environments I now I understand that the C++ compile step would fail for both a Python 2 and Python 3 environment because this third party library is unable to be downloaded from this broken link. After researching why this ""icu4c"" package was failing to download repeatedly, I found out that the URL used to download the ""icu4c-58_2-src.tgz"" no longer has this tgz file at this URL. I had to step though the setup.py file's build steps and figure out what exactly the build steps were. The setup.py file from the Metapy repo is actually running a C++ make file that downloads the Meta git repo (https://github.com/meta-toolkit/meta), then runs a Makefile to compile the C++ code for the host machine. The icu4c-58_2-src.tgz file download is performed in the Meta C++ Makefile system, so I need to figure out how to update this file download in this second Git repo. While fixing this icu4c-58_2-src.tgz is definitely an important part of fixing the issues with the setup.py file's installation issues, it is unclear what further steps I will need to take to fix the Python 3.* installation at this time. I will not know what other steps I will have to take to fix the C++ makefile compile steps since I'm blocked from progressing further though the compilation steps at this time until I can fix the icu4c-58_2-src.tgz file download. However, figuring out this third party download issue is vital to fixing the Python 3 installation issues as the URL currently in the Meta Git repo's Makefile build steps to download the icu4c file no longer works. But it is possible I will run in to operating system dependent issues when the C++ code is able to be compiled. But until I can get to the point where the C++ compiler is able to attempt to compile the code, I have to fix these other issues that are preventing C++ code compilation from even being started. Casey Blair Team Member Net ID's: cdblar2 CS 410 Final Project Fix Broken Metapy Installation in Python 3.8 Environments My final project is to update the Metapy GitHub repo to allow for installation with Python 3.8. Currently, when trying to install Metapy using Pip on Windows 10, Mac OSX, and Linux if using a Python 3.8 or higher environment, the install setup Python wheel fails to install the Metapy package due to errors. However, if using a Python 3.7 - 2.7 environment the Metapy library will install correctly. Since It is vital to keep this library functioning with the latest version of Python to allow this library to be used in production code bases. I have been researching Pybind11 and how Python can be used with C and C++ code to understand how the Metapy Git repo works. Metapy is a Python wrapper for the Meta Github repo (https://github.com/meta-toolkit/meta). The Meta code base is written in C++, and when the Metapy package is installed with Pip, the Meta C++ code is downloaded and installed then a Python interface is constructed so the code can be used from a Python interpreter. When installing Metapy in a Python 3.7 - 2.7 environment, the Pip installer downloads a tar.gz file from the Github repo's Releases page with the Metapy code already built for that specific Python version's environment, which is why it can be installed correctly with a Python 3.7 - 2.7 environments. The maintainers of the repository stopped making new releases after they published the Python 3.7 version in August 2018. If there is not a pre-built Tar file for the specific Python version, then Pip will use Python's installation wheel system to manually build the version on the client's computer to do the installation of Metapy. The Metapy repository is a python wrapper around the Meta library (https://github.com/meta-toolkit/meta). This Meta project's code is written in C++. The PyBind11 library allows for a Python interface to be constructed between C++ code and Python to allow for C++ code to be used in a Python interpreter. I next researched the PyBind11 Python package and learned how C++ code could be used from a Python interpreter as I have never written a Python program that used C++ code before so I needed to understand this system to understand how Metapy worked. At this point I was unsure if the C++ code was failing because the C++ code needed to be updated to be compatible with a new version of the C++ compiler, or maybe a dependency in the C++ code had changed unexpectedly which was causing the issue. I spent 3-4 hours reading the PyBind documentation so I understood how this library was linking the C++ code from the Meta submodule to the Metapy python code. I also didn't know if Metapy simply needed to be updated to be Python 3.8 compatible. When I first used this library, my Mac had 0Python 3.8 installed as well as Python 2.7 and since a release compatible with Python 3.8 has not been released it caused the setup.py script to be used instead of the just downloading the prebuilt Tar file. Because the C++ files in the Meta file needed to be compiled using the CMake files I also brushed up on C make files as it has been years since I've written one. I then downloaded the Metapy code and ran the manual build steps outlined here: (https://github.com/meta-toolkit/metapy#getting-started-the-hard-way). I ran in to many issues trying to get my C++ compiler to compile the Meta. I also had to brush up on C++ make files as the Meta repo's C++ build steps were failing, so I needed to figure out how the system was being built and it required me to refresh my knowledge of C++ make files as I have not written one in over 5 years. After this initial research, which took 7-8 hours, I started stepping though the C++ compiler errors one at a time. The problem with debugging C++ compiler errors is that when the compiler encounters the first error, it exits the compilation. So the developer doesn't know how many more errors there may be until after the fix the previous error: there can only be one error encountered at a time since the compiler exits the compilation after the first encountered error. So I knew I needed to fix all the errors the C++ compiler encountered here to allow the system to build correctly for Python 3.8. I attempted to install the Metapy library in my Python 3.8 environment using the following command (which failed): sudo python3.8 -m pip install metapy The first error I ran in to was due to a missing C header file on my operating system. This was to be expected as I just created this virtual machine with a clean install of Ubuntu and it did not have python 3.8 installed on it by default, so I had installed Python 3.8 but forgot to install some development tools associated with python. I needed to install the Debian package python3.8-dev with the following command: Sudo apt-get install python3.8-dev To install the missing Python.h header file that the C++ compiler was throwing an error because this file was misssing from my operating system. I had to research the error being thrown to figure out that the python3.8-dev package was needed to fix the missing Python.h file error being thrown by the Metapy install wheel. After fixing the Python.h missing file issue, rerunning the Metapy's installation command would fail with the following error repeating five times before exiting the installation: -- Downloading... dst='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz' timeout='none' -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'' -- [download 100% complete] * Closing connection 2 * Closing connection 0 * Closing connection 1 -- verifying file... file='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz' -- MD5 hash of /Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz does not match expected value expected: 'fac212b32b7ec7ab007a12dff1f3aea1' actual: 'ac2ff460bdda9c70e6ed803f5e4d03fb' -- Hash mismatch, removing... -- Retrying... It was very strange that this ICU Tar file download was producing a different MD5 hash after each attempt to download the Tar file. I decided to investigate this further as if the Tar file had been updated, but the expected MD5 hash had not been, then at least the same incorrect MD5 hash should be displaying each time. However, the actual MD5 hash was a different hash each time, which made me suspect a different error was occurring. I decided to attempt to visit the URL the C++ code was attempting to download the dependency from (http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz). After pasting this URL in to my web browser, instead of a TGZ file download, my browser took me to an HTML page listing all the releases of this ICU dependency. Puzzled why this URL was not a download link anymore, I navigated around this website until I found an answer. After going to the ""Source Repository"" page, at the top of this page I found my answer (http://site.icu-project.org/repository http://site.icu-project.org/repository). The code repository that used to host this ICU dependency was migrated from Subversion to GitHub. Therefore, the previous download link would not work anymore, and all pre-built TGZ files were now on GitHub. I then returned to the ICU version release page and found my version 61 listed and clicked on that link. I was redirected to a page listing the change log for this new version and found the link to the Git repo where the code now resides: https://github.com/unicode-org/icu/releases/tag/release-61-1. I navigated to the GitHub repo to find many pre-built TGZ files. I had to step though the setup.py file's build steps and figure out what exactly the build steps were ad where this ICU package was being downloaded at in the code. The setup.py file from the Metapy repo is actually running a C++ make file that downloads the Meta git repo (https://github.com/meta-toolkit/meta), then runs a Makefile to compile the C++ code for the host machine. The icu4c-61_1 -src.tgz file download is performed in the Meta C++ Makefile system, so I need to figure out how to update this file download in this second Git repo. It took me a while to search though all the C++ code in the Meta repo to find the correct location where the ICU package was being downloaded. I finally found the line in the Meta repo where the ICU package was downloaded. I tried each of the TGZ files until once successfully downloaded and the C++ compiler progressed past that point of the installation. Each attempt took 20 minutes as the C++ compiler is quite slow, so this was a manual process to debug. At this point, I was not sure how many other issues I would encounter. I didn't know if I would need to fix many more of these external dependency url's that were broken, or if this was the only one. Fortunately, this was the only broken URL and the rest of the compilation succeeded. I successfully fixed the Python 3.8 build system as the C++ compiler was able to compile the Meta repo successfully, then install the Metapy library using Pip. I did have to make forks of the Metapy and Meta repos in GitHub as my GitHub account does not have write access to the main Metapy or Meta repos. I tried to make a new branch on the Metapy and Meta repos to update the code, however, I was met with 403 unauthorized errors each time. So I needed to fork the repos and make my own version, then make merge requests from my forked repo back to the parent repo. I currently have three open GitHub pull requests to fix this issue in the original parent repos: https://github.com/meta-toolkit/meta/pull/222, https://github.com/meta-toolkit/meta/pull/220, and https://github.com/meta-toolkit/metapy/pull/10. All three pull requests are currently waiting review by the code repo's members so I have no idea when or if they will approve these changes as none of them have made an update to the repo since August 2018. However, you can currently use my forked version of Metapy to correctly install Metapy for Python 3.8 with the following command: sudo python3.8 -m pip install git+https://github.com/ccasey645/metapy.git This command, on Ubuntu 18.04, will build and install the Metapy library correctly. This command will instead download my forked version of Metapy from my GitHub repo and the installation succeeds. I did not have time to attempt the build on OSX and Windows 10 too as spent well over 20 hours figuring out the code changes needed to fix the C++ compiler errors. However, when I was attempting to run this on OSX and Windows 10 my C++ compiler installations on these OS's were in a very broken state: Windows 10 needed some new version of Visual Studio C++ library installed that would break other projects I was working on, and my OSX's clang compiler was throwing errors saying that it was not allowed to compile C files using a C++ compiler so I decided to focus just on the code changes needed to fix the errors and leave the OSX and Windows 10 TGZ building to the Metapy code repo owners. When the Metapy code repo owners accept my GitHub pull requests, there is a make-release.sh script in the Metapy repo that will build the appropriate Tar files for all three platforms, so each client computer will not have to have a 100% configured C++ compiler just to install the Metapy library. CourseProject Team Name Game of Threads NetID's cdblair2 (Casey Blair) Topic: Update Metapy repo's code to fix errors when installing with Python 3.8 environment Final Project Documentation/Report File Name: Final_Project_Final_Report.pdf link: https://github.com/ccasey645/CourseProject/blob/main/Final_Project_Final_Report.pdf Presentation Video File: link: https://drive.google.com/file/d/1Ty8QGFlu-29FeX0yPSJ502Wzu_YwygVt/view?usp=sharing On Google Drive because Git's file size limit prevented me from uploading it here. Installation Demo Video File: Mentioned this in Presentation Video File This 12 minute video demonstrated how ""sudo python3.8 -m pip install metapy"" fails, but ""sudo python3.8 -m pip install git+https://github.com/ccasey645/metapy.git"" succeeds"" link: https://drive.google.com/file/d/1cb1YbEJkYavFmk_LiHOLWqlwR72bLMAS/view?usp=sharing On Google Drive Source Code: CourseProject/metapy Note: this is a Git Submodule that will redirect you to another GitHub repository. This is where all my code is since I was updating the Metapy repo: I needed to make a fork of the original repo."
https://github.com/cds95/CourseProject	CS 410 Project Progress Report Christopher Dimitri Sastropranoto Project description Create a google chrome extension that can extract the text from a CNN news article and give a text summary about it. Progress My main goal for the early part of the project was to gain some experience using pythonOs NLTK library to process and summarize a given piece of text. IOve spent most of the time going over online tutorials and documentation about NLTK. In addition to this, IOve also spent time researching the different types of ways that text is usually summarized. These methods include Abstractive and Extractive methods. Abstractive summarization is done by analyzing the text to determine itOs meaning, which is then used to generate a summary. Extractive methods rely on scoring the words and sentences in a passage to determine important parts. These scoring functions are similar to those presented in CS 410. For the purposes of this project I will be using the latter method. After going through the documentation, I went ahead and created a Jupyter Notebook to start getting some hands on experience using NLTK. IOve done some basic preprocessing steps to tokenize and build a document-term matrix out of a sample article I pulled from CNN. Challenges The biggest challenge so far has been familiarizing myself with the NLTL library. One thing that was hard was to setup the library in the beginning as the installation steps were not as simple as just using pip. Remaining Work The last remaining step to completing the Jupyter notebook is to implement the scoring function and actual text summarization steps. Fortunately NLTK has some built in libraries that can help with this meaning that it should not take up too much time. Once this step has been completed, the following things need to be done. 1.Package Jupyter Notebook code as an API. 2.Build google chrome extension. The extension will consist of a simple UI with a button allowing the user to summarize the article. All it does is just scrape the text from the screen, pass it into the API from step 1 and Thnally outputting the result. CS 410 Project Documentation Christopher Dimitri Sastropranoto Project Goal The goal of the project was to build a google chrome extension that can summarize a CNN article. Architecture The initial plan was to host a python API that would accept GET requests from the extension. The extension would pass in the raw text to the API and get the summarized text back. I was unfortunately unable to successfully host the python server due to time constraints and a lack of experience in using either Django or Flask. In the end I worked around this issue by having all the code to summarize text live in the chrome extension. Implementation The project code is mainly split into two parts. I Thrst did some exploratory work on how to do extractive text summarization using the NLTK python library in a Jupyter Notebook environment. The notebook can be found in the exploration folder. The next step was to actually build the chrome extension. The code for the extension lives in the chrome-extension folder. Folder Structure exploration test-article.txt N> Test article used for exploratory purposes text_summarization_exploration.ipynb N> Jupyter notebook chrome-extension manifest.json N> ConThguration Thle for chrome extension content.js N> Code for scraping the page content and summarizing it user-interface.js N> Code to hand UI interactions user-interface.css N> Stylesheet for extension. user-interface.html N> HTML Thle for extension UI. icon.png N> Icon for extension. Summarization Algorithm The program relies on extractive summarization to summarize text. In extractive summarization, each sentence is given a score and the highest scoring sentences are used in the summary. This means that the summary is only limited to the content in the text. Sentences are scored based on the frequency counts of each of their non-stop word terms. 1.Tokenize words and sentences 2.Remove stop words from tokenized word list. 3.Calculate the frequency of each word and put into a dictionary. The dictionary keys are the words and the values are the counts. 4.Normalize the counts in the dictionary from step 2. 5.Calculate the score of each sentence. The score is calculated by adding up the scores of each word in a sentence. 6.Sort sentences by descending order of score. 7.Take the top N sentences by score but preserve their order. 8.Output step 7 as the result. Improvements * Incorporate inverse document frequency into the scoring function. *Use abstractive summarization instead of extractive summarization. Abstractive summarization uses deep learning models to decipher the meaning of the passage and generate new sentences for the summary. Self Evaluation I was able to accomplish the goals of the project as the extension IOve implemented successfully scrapes the text from a CNN news article and manages to give a summary for it. Running the program Refer to the instructions provided in the README.MD section. CS410 Project Proposal Christopher Dimitri Sastropranoto 678097021 Project Description The proposed project is to implement a google chrome extension that can quickly summarize a Medium articles. Who will beneTht from such a tool? Almost anyone can beneTht from such a tool. For example, I am an avid user of Medium but feel that some articles are unnecessarily lengthy and would love to save some time just getting the gist of an article by having a quick summary of what it is about. If IOm interested in the summary then I could go ahead and read the full article. Existing Tools Doing a quick google search resulted in several chrome extensions and websites that do something similar. The biggest drawback from most of them was that they required the user to either enter in a link to an article, or copy and paste the articleOs contents. The goal is for my tool is to allow the user to skip those steps and simply click a button to summarize the articleOs contents whilst on the browsing. Programming Language and other Other resources There are currently several online services that I can use to help implement this project. My plan is to use the python NLTK library to help with summarizing text and packaging that inside a Chrome Extension UI. In addition to this, I plan to do my own online research by utilizing YouTube, Medium and other websites to augment what IOve learnt in CS410. Techniques and Algorithms From my research so far, summarizing text requires some preprocessing steps such as lemmatization and POS tagging. Once this is done, the next step would be to model topic distributions in the document. In addition to this, there are several summarization techniques that need to be explored. These are Extraction-based summarization, Extractive summary and Abstract based summarization. How will you demonstrate the usefulness of your tool? If successful, I plan to demonstrate the usefulness of my tool by creating a video and also releasing it in the Google Extension store. Project timeline Project start date: 10/26/20 Work JustiThcation I feel that the scope of the project sufficiently fulThlls the 20 hour requirement as it involves multiple components. This includes doing further research into the summarization techniques, preprocessing the data, training it using NLTK and Thnally implementing a Chrome Extension UI for it.MilestoneTarget DateComplete research into summarization techniques10/28Preprocess Data11/4Train text summarizer. This step will be done in isolation using JupyterNotebook.11/8Build chrome extension and hook up to text summarizer. This step requires implementing a way for the extension to scrape the articleOs contents from the website.11/13Testing and BugThxes if needed11/16 CourseProject Running the project Jupyter Notebook Exploration 1) Install Jupyter Notebook by following instructions here https://jupyter.org/install. From the root of the project directory. 2) cd exploration 3) jupyter-notebook Chrome Extension Refer to the demo to see how to run the extension. Link to demo: https://www.youtube.com/watch?v=PoMi_EibtCs
https://github.com/cf16-uiuc/CourseProject	Progress Report For my project I am working on the classification competition. I have made some significant progress on it over the last few weeks. The tasks I have completed are preprocessing the text - cleaning it up, converting emojis into text, and preparing the data to be used for training the model. I have also spent a lot of time reading about BERT and have trained an initial model using BERT. Tasks going forward are testing my code with the test data provided to see how my initial model performs to the baseline and optimizing the model to improve performance. Depending on how that performance goes I may explore other options for training the model as well. I then need to work on creating the documentation and cleaning up and organizing the code I have written. I am currently not facing any challenges as I work on my project. Project Proposal CS410 Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will be working on this project by myself. As the only team member I will be the captain. Name: Christine Frandsen NetID: cf16 Which competition do you plan to join? I am planning to join the text classification project If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Yes, I am excited to learn more about state-of-the-art neural network classifiers. Some neural classifiers I am looking at are the perceptron network, a simple model to set a baseline, an extreme learning machine (ELM) network, and a support vector machine (SVM), although this is not generally considered a neural network. I have had a few experiences with perceptron networks and SVMs and am eager to expand my knowledge to other neural network classifiers. In terms of deep learning frame works I have worked a little bit with TensorFlow and Keras, but am a little rusty. I am hoping to incorporate those as well as potentially bringing in PyTorch as well. Which programming language do you plan to use? I plan to use Python for this project. Text Classification Competition: Twitter Sarcasm Detection This project is for CS410 during the fall 2020 semester at UIUC. More details about this competition can be found at: https://github.com/CS410Fall2020/ClassificationCompetition Implementation Overview This classifier relies on the Simple Transformer package, based on the Transformer package from HuggingFace. The documentation for Simple Transformer can be found here: https://simpletransformers.ai/docs/classification-models/#classificationmodel. All packages needed can be installed with the package manager pip. The code is broken into two files - train.py and test.py. In train.py the focus of the file is training the model. Prior to training the model we do a little bit of preprocessing of the data. First, we remove all stop words, then we replace all emoticons with text that may be able to provide information to the model that is trained. For the training we rely on the Simple Transformer package. From that package we use the binary classifier. Their setup allows us to bring in any pretrained model and adjust to our data. We looked at multiple pretrained models from HuggingFace found here: https://huggingface.co/models. In the end we discovered that simply using the BERT model yielded the best results. To apply labels to a new set of tweets we can run the test.py file. This does a similar process of converting emoticons to text and removing stop words to preprocess the data. We then read in our trained model from train.py and can use the predict() function to predict the new labels. Running the Code To run the code you need to ensure that all the required packages are installed. The code can be run from the command line by running either python train.py or python test.py, depending on which file needs to be run. The code can also be run from python IDEs. The different variables, such as file name for training data, number of epochs, or learning rate can be adjusted within the file at the beginning of the file. The end result of running train.py will be created in a folder titled outputs. The test.py file reads from the outputs folder and will output and an answer.txt file. For a detailed overview of running the code, an instructional demo can be found here: https://youtu.be/hzyMMAHryAE Functions The code contains three main functions - convert_emojis (appears in both files), bert_training (train.py), and predict_sarcasm (test.py). convert_emojis(text): This function takes a single input, text. It cycles through the list of known emojis, looking for them in the text and replacing them to allow for consistency. bert_training(model_type, model_base, train_data, early_stop, early_stop_delta, overwrite, epoch, batch_size, learning_rate, output): This function is where the model is trained. Several inputs are required, all of which are defined at the beginning of the file. This allows for models to be trained for different model types, varying epochs, and different batch sizes easily. Also allows a user to specify where the output should be written out to, making it easy to save several different models without worrying about overwriting existing models. predict_sarcasm(data_path, results, model_loc, model): This function takes a model and input text and generated labels for whether or not a tweet is sarcastic or not. The inputs for this function allow the user to specify the location of the data, what the results file should be called, where the model is located, and what type of model it is. This allows the user to easily chagne various parameters to compare different models performance. Parameter Tuning and Model Exploration The main parameters that we focused on for this project were learning rate, batch size, number of epochs, and base models. We explored using Roberta, XLNET, and Electra, before finally deciding on BERT. We also looked at using an ensemble method combining results from BERT, Electra, and Roberta. However, those results were below the baseline, so we opted to just use the BERT model. We also looked at batch size. We found that at batch size of 100 performed well. The Classic Transformer package used 8 as default. This did well, however, not well enough to beat the baseline. When we increased the batch size by much more, we found that the model tended to predict everything to be sarcastic. Notes The final trained model was not uploaded to git. Please reach out to me if you would like to see the model or have questions about it. A similar model can be generated by first running train.py before running test.py. Useful Links and Sources https://huggingface.co/models https://simpletransformers.ai/docs/binary-classification/ https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3 https://github.com/ThilinaRajapakse/simpletransformers
https://github.com/chanwoo321/CourseProject	"1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.Which paper have you chosen? 3.Which programming language do you plan to use? 4.Can you obtain the datasets used in the paper for evaluation? 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? The group captain of this project is Jonathan Kim (jck3) and the other two members of the group are Michael Xiang (mx9) and Tyler Ruckstaetter (thr2). The paper we have chosen is "" A cross-collection mixture model for comparative text mining."" The language that we will be using for this project is python. Since the paper is from 2004, it seems like the exact datasets used in the paper will be unavailable for us. However, a similar dataset will be obtainable since the nature of the data is very publicly available. As of right now, we can still use reviews of laptops as used in the paper as well as more modern laptops. War news is available on many news sites, so while the exact documents from the paper may not be directly used, they will be very similar in nature. Progress Report So far, we have researched the sources as mentioned in the paper. In order to keep this project as similar to the paper as possible, articles were pulled from the websites mentioned in the paper. This includes war news from CNN and BBC sites online and getting reviews from epinions.com. Since epinions no longer exists as a domain, an archive of the internet was used in order to extract data from the approximate time period the paper was published. This led to a couple of complications: it was unclear in the paper what articles/reviews in particular were used despite mentioning the website they were taken from. While internet archives are available to extract data from that period of time, the archives are not comprehensive. Thus, we were not able to get all of the reviews mentioned in the paper (though we believe a sufficient number of them have been obtained to still run the model in a meaningful manner). Additionally, since the specific articles/reviews were not released in the paper, it is unclear whether our extracted data matches the data used for the model in the paper. Regardless, we expect the content to be very similar, so this should not affect the results. The only task that is left to be done is to implement the model itself. This involves two parts: creating the naive mixture model and creating the mixture model described in the paper. This will likely be in a format similar to that of MP3, so it should be a doable amount of time. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Demo Video Note that the video was not recorded with the final edits to the repository. The actual results may differ/be more accurate. Table of Contents Introduction Brief Overview Obtaining and Organizing the Data Running the Software Code Information Baseline Model Cross-Collection model Introduction Hello! Our group consists of three people: Jonathan Kim, Michael Xiang, and Tyler Ruckstaetter. This repository was created for the final project for the class CS 410: Text Information Systems at the University of Illinois at Urbana-Champaign. The purpose of this project was to reproduce the Comparatrive Text Mining model described in the paper ""A Cross-Collection Mixture Model for Comparative Text Mining"", which can be found here. Brief Overview The model in the paper was created in order to solve the novel text mining problem of ""Comaparative Text Mining"". This problem consists of trying to find common themes across some collections of texts and to summarize the similarities and differences between these collections. Thus, a generative probabilistic mixture model was proposed. It performs cross-collection clustering and within-collection clustering. This is done to find themes across collections and utilize the fact that each collection may have information on a similar topic to the other collections as opposed to a completely different one. The data used in the paper and the model implemented in this repository were laptop reviews and war news. In particular, the laptop reviews analyzed the Apple iBook Mac Notebook, the Dell Inspiron 8200, and the IBM ThinkPad T20 2647. The war news covered the Iraq war and the Afghanistan war. To verify the validity of this model, a simple baseline mixture model was also implemented that takes the data of all of the laptops or all of the wars and tries to cluster documents without utiliing the differences in different collections. This cross-collection mixture model works notably better than the baseline. Obtaining and Organizing the Data In order to keep the study as close as possible to the paper, the reviews available in this repository were collected based on the description of the paper. The war news was collected from articles from BBC and CNN for one year starting from November 2001. On the other hand, the laptop reviews were pulled from epinions.com. However, epinions.com is no longer available at the time this project was completed. In order to maintain as accurate of a reproduction of this model as possible, a internet archiving website was used to see what was available on epinions.com in 2001. The data is organized inside of the ""data"" folder of this repository. Each text file in the data folder represent data collected for each individual laptop and war. Inside the data folder, there is another folder called ""combined"". This folder contains two files: laptops.txt and wars.txt. Laptops.txt contain all of the laptop reviews in one file and wars.txt contain all of the war articles in one file. This was done so that the baseline model could access all of the needed reviews or articles as necessary. Running the software This software uses Python3 and uses numpy and pandas. To run the baseline mixture model, ensure that numpy, pandas, and python are properly installed (pip install numpy/pip install pandas) and run the following code in the terminal: python main.py This will run the baseline model on data/combined/laptops.txt by default. If another data set should be analyzed, provide the file location as a parameter. For example, if analysis wants to be run on war models, run the following: python main.py 200 ./data/combined/wars.txt The format being: python main.py [iterations] [dataset] To run the cross-collection mixture model, run the following code in the terminal: python cross_collection_model.py The above will run the Cross Collection model on the default dataset of the laptop reviews with the collections being mac.txt, inspiron.txt, and thinkpad.txt. If you want to run it with another set of data, you will want to run the following command: python cross_collection_model.py 10 ./data/combined/laptops.txt ./data/inspiron.txt ./data/mac.txt ./data/thinkpad.txt The format being: python cross_collection_model.py [iterations] [full dataset] [collections files separated by spaces] Note that running either model may take a long amount of time because of how much data the model is processing. We noticed that you could see pretty realistic word-probability assignments in as little as 5 iterations of testing. Code Information Baseline model The code from the baseline model is primarily based on the PLSA algorithm as used in MP3 of CS 410. This model was estimated using the EM (Estimation-Maximization) algorithm. Here is a quick overview of the functions provided: normalize(input_matrix) Normalizes the rows of a 2d input_matrix so they sum to 1. class NaiveModel(object) Class that actually runs the baseline mixture model. Includes the following methods: build_corpus(self) Fills in self.documents with a list of list of words by reading from the document path build_vocabulary(self) Constructs a list of unique works in the whole corpus and updates self.vocabulary build_term_doc_matrix(self) Constructs a term document matrix where each row represents a document, and each column represents a vocabulary term. initialize_randomly(self, number_of_topics) Randomly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize_uniformly(self, number_of_topics) Uniformly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize(self, number_of_topics, random=False) Sets up the matrices of the model using initalize_randomly or initialize_uniformly. expectation_step(self) Runs the expectation_step as part of the EM algorithm. maximization_step(self, number_of_topics) Runs the maximization_step as part of the EM algorithm. calculate_likelihood(self, number_of_topics) Calculates the log-likelihood of the model using the model's updated probability matrices. Used to determine when the EM algorithm is complete/converged. naivemodel(self, number_of_topics, max_iter, epsilon) Runs the model in its entirety on self.document_path and the provided parameters. show_top_10(matrix, model) Displays the top 10 probabilities of a topic-word-probability matrix given a model's vocabulary main(documents_path) This is the default function used when running from the terminal. Runs the model with default parameters. Cross Collection mixture model The code from the Cross Collection model is also primarily based on the PLSA algorithm as used in MP3 of CS 410 with modifications according to the paper's given formulae. This model was estimated using the EM (Estimation-Maximization) algorithm. Here is a quick overview of the functions provided (functions shared with baseline are omitted): class CCModel(object) Class that actually runs the Cross Collection mixture model. Includes the following methods: build_corpus(self) Fills in self.documents with a list of list of words by reading from the document path build_vocabulary(self) Constructs a list of unique works in the whole corpus and updates self.vocabulary build_term_doc_matrix(self) Constructs a term document matrix where each row represents a document, and each column represents a vocabulary term. initialize_randomly(self, number_of_topics) Randomly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize_uniformly(self, number_of_topics) Uniformly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize(self, number_of_topics, random=False) Sets up the matrices of the model using initalize_randomly or initialize_uniformly. expectation_step(self) Runs the expectation_step as part of the EM algorithm. maximization_step(self, number_of_topics) Runs the maximization_step as part of the EM algorithm. calculate_likelihood(self, number_of_topics) Calculates the log-likelihood of the model using the model's updated probability matrices. Used to determine when the EM algorithm is complete/converged. naivemodel(self, number_of_topics, max_iter, epsilon) Runs the model in its entirety on self.document_path and the provided parameters. show_top_10(matrix, model) Displays the top 10 probabilities of a topic-word-probability matrix given a model's vocabulary main(documents_path) This is the default function used when running from the terminal. Runs the model with default parameters. ACross-CollectionMixtureModelforComparativeTextMiningChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbanaChampaignAtulyaVelivelliDepartmentofElectricalandComputerEngineeringUniversityofIllinoisatUrbanaChampaignBeiYuGraduateSchoolofLibraryandInformationScienceUniversityofIllinoisatUrbanaChampaignABSTRACTInthispaper,wede neandstudyanoveltextminingproblem,whichwerefertoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcom-monthemesacrossallcollectionsaswellassummarizethesimilarityanddi erencesofthesecollectionsalongeachcom-montheme.Thisgeneralproblemsubsumesmanyinterest-ingapplications,includingbusinessintelligenceandopinionsummarization.Weproposeagenerativeprobabilisticmix-turemodelforcomparativetextmining.Themodelsimul-taneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.ThemodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algo-rithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.CategoriesandSubjectDescriptors:H.3.3[Informa-tionSearchandRetrieval]:TextMiningGeneralTerms:AlgorithmsKeywords:Comparativetextmining,mixturemodels,clus-tering1.INTRODUCTIONTextminingisconcernedwithextractingknowledgeandpatternsfromtext[5,6].Whiletherehasbeenmuchre-searchintextmining,mostexistingresearchisfocusedononesinglecollectionoftext.Thegoalsareoftentoextractbasicsemanticunitssuchasnamedentities,toextractrela-tionsbetweeninformationunits,ortoextracttopicthemes.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.KDD'04,August22-25,2004,Seattle,Washington,USA.Copyright2004ACM1-58113-888-1/04/0008...$5.00.Inthispaper,westudyanovelproblemoftextminingre-ferredtoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcommonthemesacrossallcollectionsaswellassummarizethesimilarityanddi er-encesofthesecollectionsalongeachcommontheme.Specif-ically,thetaskinvolves:(1)discoveringthedi erentcom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Theneedforcomparativetextminingexistsinmanydi erentapplica-tions,includingbusinessintelligence,summarizingreviewsofsimilarproducts,andcomparingdi erentopinionsaboutacommontopicingeneral.Inthispaper,westudytheCTMproblemandproposeagenerativeprobabilisticmixturemodelforCTM.Themodelsimultaneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.Themixturemodelisbasedoncomponentmultinomialdistributionmodels,eachcharacterizingadi erenttheme.Thecommonthemesandcollection-speci cthemesareexplicitlymodeled.Thepro-posedmodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algorithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.Therestofthepaperisorganizedasfollows.InSection2,webrie yintroducetheproblemofCTM.Wethenpresentabaselinesimplemixturemodelandanewcross-collectionmixturemodelinSection3andSection4.WediscusstheexperimentresultsinSection5.2.COMPARATIVETEXTMINING2.1AmotivatingexampleWiththepopularityofe-commerce,onlinecustomereval-uationsarebecomingwidelyprovidedbyonlinestoresandthird-partywebsites.Pioneerslikeamazon.comandepin-ions.comhaveaccumulatedlargeamountsofcustomerinputincludingreviews,comments,recommendationsandadvice,etc.Forexample,thenumberofreviewsinepinions.comismorethanonemillion[4].Givenaproduct,therecouldbeuptohundredsofreviews,whichisimpossibleforthereaderstogothrough.Itisthusdesirabletosummarizeacollectionofreviewsforacertaintypeofproductsinordertoprovidethereadersthemostsalientfeedbacksfromthepeers.Forreviewsummarization,themostimportanttaskistoidentifydi erentsemanticaspectsofaproductthatthereviewersmentionedandtogrouptheopinionsaccord-ingtotheseaspectstoshowsimilaritiesanddi erencesintheopinions.Forexample,supposewehavereviewsofthreedi erentbrandsoflaptops(Dell,IBM,andApple),andwewanttosummarizethereviews.Ausefulsummarywouldbeatab-ularrepresentationoftheopinionsasshowninTable1,inwhicheachrowrepresentsoneaspect(subtopic)anddi er-entcolumnscorrespondtodi erentopinions.Table1:AtabularsummarySubtopicsDellIBMAppleBatterylifelongenoughshortshortMemorygoodbadgoodSpeedslowfastfastItis,ofcourse,verydicult,ifnotimpossibletopro-ducesuchatablecompletelyautomatically.However,wecanachievealessambitiousgoal{identifyingthesemanticaspectsandidentifyingthecommonandspeci ccharacter-isticsofeachproductinanunsupervisedway.Thisisaconcreteexampleofcomparativetextmining.2.2ThegeneralproblemTheexampleaboveisonlyoneofthemanypossibleappli-cationsofcomparativetextmining.Ingeneral,thetaskofcomparativetextmininginvolves:(1)discoveringthecom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Itisveryhardtopreciselyde newhatathemeis,butitcorrespondsroughlytoatopicorsubtopic.Thegranularityofthemesisapplication-speci c.CTMisafundamentaltaskinex-ploratorytextanalysis.Inadditiontoopinioncomparisonandsummarization,ithasmanyotherapplications,suchasbusinessintelligence(comparingdi erentcompanies),cus-tomerrelationshipmanagement(comparingdi erentgroupsofcustomers),andsemanticintegrationoftext(comparingcomponenttextcollections).CTMischallenginginseveralways:(1)Itisacompletelyunsupervisedlearningtask;notrainingdataisavailable.(ItisforthesamereasonthatCTMcanbeveryusefulformanydi erentpurposes{itmakesminimumassumptionsaboutthecollectionsandinprinciplewecancompareanyarbitrarypartitionoftext.)(2)Weneedtoidentifythemesacrossdi erentcollections,whichismorechallengingthanidentifyingtopicthemesinonesinglecollection.(3)Thetaskinvolvesadiscriminationcomponent{foreachdiscov-eredtheme,wealsowanttoidentifytheuniqueinformationspeci ctoeachcollection.Suchadiscriminationtaskisdif- cultgiventhatwedonothavetrainingdata.Inaway,CTMgoesbeyondtheregularone-collectiontextminingbyrequiringan\alignment""ofmultiplecollectionsbasedoncommonthemes.Sincenotrainingdataisavailable,ingeneral,wemustrelyonunsupervisedlearningmethods,suchasclustering,toperformCTM.Inthispaper,westudyhowtouseprob-abilisticmixturemodelstoperformCTM.Belowwe rstdescribeasimplemixturemodelforclustering,whichrepre-sentsastraightforwardapplicationofanexistingtextmin-ingmethod,andthenpresentamoresophisticatedmixturemodelspeci callydesignedforCTM.3.CLUSTERINGWITHASIMPLEMIXTUREMODELqqqqqFigure1:TheSimpleMixtureModelAnaivesolutiontoCTMistotreatthemultiplecollec-tionsasonesinglecollectionandperformclustering.Ourhopeisthatsomeclusterswouldrepresentthecommonthemesacrossthecollections,whilesomeotherswouldrep-resentthemesspeci ctoonecollection(seeFigure1).Wenowpresentasimplemultinomialmixturemodelforclus-teringanarbitrarycollectionofdocuments,inwhichweassumethereareklatentcommonthemesinallcollections,andeachischaracterizedbyamultinomialworddistribu-tion(alsocalledaunigramlanguagemodel).Adocumentisregardedasasampleofamixturemodelwiththesethememodelsascomponents.We tsuchamixturemodeltotheunionofallthetextcollectionswehave,andtheobtainedcomponentmultinomialmodelscanbeusedtoanalyzethecommonthemesanddi erencesamongthecollections.Formally,letC=fC1;C2;:::;Cmgbemcomparablecol-lectionsofdocuments.Let1;:::;kbekthemeunigramlanguagemodelsandBbethebackgroundmodelforallthecollections.Adocumentdisregardedasasampleofthefollowingmixturemodel(basedonwordgeneration).pd(w)=Bp(wjB)+(1"
https://github.com/chask8ng/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chengbo322/CourseProject	Individual Project - Free Topic Sentiment Analysis in management discussion and analysis (MD&A) from company's SEC filing Chengbo Jiang chengbo6@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Net ID: Chengbo6, this will be an individual project so Chengbo Jiang will be served as the captain and other roles. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work?  Free Topic: Sentiment Analysis in management discussion and analysis (MD&A) from company's SEC filing.  The task is to extract the management and analysis (MD&A) sections of text from public companies' 10K (Annual) and 10Q (quarterly) filings from U.S. Securities and Exchange Commission (SEC) database.  After extracting the texts, I will try to classify the sentiment whether they are positive or negative using the methods and algorithms learned from CS410.  Positive and negative can be determined via counting positive or negative words from dictionary.  Other than accounting information on those filings, text information could be valuable but overwhelming. Classifying these texts information in positivity or negativity will streamline the analytical procedures of public companies as well analysts' understanding. For example, if we can filter the negatives so that we can concentrate more on analyzing the positive ones.  The planned approach: 1. Write a web crawler to download the reports from SEC's website. 2. Extract the MD&A information sections from the downloaded reports. 3. Sentiment analysis using methods such as Naive Bayes, Multinomial Naive Bayes etc.  Tools: Python (metapy, nltk, numpy, beautifulsoup etc.)  Datasets: The EDGAR (Electronic Data Gathering, Analysis, and Retrieval) database from SEC.  Expected Outcome: Expect to accurately classify the textual data in their respective categories.  Evaluation: Since I will use different methods, I will compare the results from different methods. 3. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 1. Write a web crawler to download the reports from SEC's website. 40 Hours 2. Extract the MD&A information sections from the downloaded reports. 10 Hours 3. Sentiment analysis using methods such as Naive Bayes, Multinomial Naive Bayes etc. 40 Hours CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chenyuzhao98/CourseProject	"CS 410: Final Project Progress Report Team Abracadabra Chuhan (Vivian Yu), Chenyu Zhao At this point, our group have completely follow the proposed timeline included in our project proposal. We have finished the research on currently available movie recommender systems by doing a competitive analysis. We looked at movielens (https://movielens.org/) and taste (https://www.taste.io/) which we believe share similar concept with the recommendation system we intend to develop. We have completed building up the crawler for gathering information from two target websites: IMDB and Rotten Tomatoes. Another thing we did is we applied tkinter to build a sketchy interface that facilitate the use of our system. To makes the design of our system more human-centered, we have improved our algorithm so users can choose the maximum of three different emotions at the same time. In the following two weeks, we aim to finish two main tasks: first, completing the main script that contains our algorithm and calls the interface and crawlers; second, if time permits, further polish the system by prototyping as a mobile app. The challenge we are facing right now is due to the web structure of Rotten Tomatoes websites: more detailed information about the movie, such as grading and runtime are stored inside the movie profile links. Even though we have successfully enables our program to look into the profile links and capture intended information, it takes a while for Python to actually process it. Our group is still working on figuring out the best way to present the movie information without taking excessive time. MovieMood CS 410 Final Project: A Mood-Based Movie Recommendation System Team Introduction A collaborative work is done by Vivian Chuhan Yu (chuhany2) and Chenyu Zhao (chenyu5). Being a Psych major, Vivian applied her expertise in designing the connection between human emotions and movie genres. She is also responsible for designing the graphical user interface using tkinter, and presenting the information in Treeview. Chenyu is responsible for building the web scraper, designing the logistic regression algorithm, and implement the Cosine Similarity analysis. He is also responsible for writing the documentation and narrating the presentation. Project Overview This is a Python-based movie recommendation system that implemented text-retrieval techniques and Graphical User Interface. One special thing about this system is that its recommendations were tailored around users' emotion of the moment. There are so many existing movie recommender systems available on the market, but only a small number of them were designed based on users' psychological needs. The main objective of this project is to fill this gap by making traditional recommender system more user-driven. Emotion associated with Genre of Movie There are 10 categories of emotion the system presented to users to choose from. These are 5 postive emotions (""Happy"", ""Satisfied"", ""Peaceful"", ""Excited"", ""Content"") and 5 negative emotions (""Sad"", ""Angry"", ""Fearful"", ""Depressed"", ""Sorrowful""). These emotions taken as inputs from the GUI interface we built through tkinter (please refer to interface.py): The correspondence of every emotion with genre of movies are set up as below: - Happy - Horror - Sad - Drama - Satisfied - Animation - Angry - Romance - Peaceful - Fantasy - Fearful - Adventure - Excited - Crime - Depressed - Comedy - Content - Mystery - Sorrowful - Action Based on the inputted emotion, the system is going to be selected from the corresponding genre based on their ratings given by two websites: IMDB and Rotten Tomatoes. The reason why we are collecting movie information from both websites is that we believe the system is able to capture a more full-scaled opinions from movie lovers. Application of Crawling Because we intend to scrape two websites with different web structure, we developed one IMDB crawler and another RT crawler to extract movie information. Check out scraper.py for more details. Here are two example movie pages of IMDB and Rotten Tomatoes: As you can see, comparing to IMDB, Rotten Tomatoes includes the majority of movie information in each movie profile link. Our crawler had to look up each link to capture hidden information, such as movie length, maturity grading, cast, etc. Therefore, it is unavoidable that the program takes more time to scrape RT pages. Ranking and Scoring We would pull user rating scores from both IMDb and Rotten Tomatoes. Due to the different rating scales used by IMDb and Rotten Tomatoes, we would first convert both scores to a 10-point scale for the ease of comparison. We would also take the number of ratings into consideration, as larger number of ratings tends to make the overall rating more credible. Therefore, we would run logistic regression function on the number of ratings, and add it as an additional weightage to the final movie score. Present Movie Information After users indicate their moods, the program is going to look up the corresponding link to the movie page and present movie information as Treeview, which is a module included by the tkinter library displaying a hierarchical collection of items. Here is an example output of the list of recommended movies: Note: Not every movie has all information listed. If the crawler cannot find relevant information, it will automatically fill the space with ""Not Found"". You May Also Like... After users chose their favorite movie from the list, we would run a Cosine Similarity analysis to recommend 3 similar movies based on the summary. Here is an example of movies similar to Toy Story 4: Self-evaluation The work is equally distributed between the two teammates, and we were able to complete our mood-based movie recommender system as intended. We chose not to build a seperate mobile application, but instead spend the time working on additional features like cosine similarity and logistic regression. In the end, we have obtained the expected outcome. Environment Set-up Please check out requirements.txt for information. You can install all packages at once using $ pip install -r requirements.txt. Please use Python 3. Otherwise you will need to import tkinter.ttk separately because it is not a submodule of tkinter in Python2 How to use? After making sure you have all packages installed, activate the program through main.py. The program will start running immediately. The scraping process may take up to 30 seconds. Please do not close the tkinter window when the program is running. Video Presentation YouTube link"
https://github.com/chiragcshetty/CourseProject	CourseProject Student: Chirag C. Shetty (cshetty2) Paper: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 [link] Introduction The paper explores a further improvement like PLSA in mining topics. In PLSA, k topics are mined from the entire collection. However, collection may have subset and we may be interested in knowing the topics within a collection while also comparing across different collections. The paper adds one more level of generative variable (lambda_c) and tries to achieve this. Data The original paper from 2004 had used a set on news articles and reviews fro epionions.com. The site is no longer active and the dataset wasn't archived anywhere. So I decided to write a scraper, starting with the codes used in the MP's. I chose CNN, which has a search feature on its webpage. So I scrap the webpage resulting from searching a topic of interest and extract the news articles. This mostly involved handcrafting the extraction process. Procedure for scraping The main python file is called scrap.py Edit the 'name' variable to indicate the topic. Files extracted will be stored with this name no_pages: Number of pages to search. Each page has 10 articles Run scrap.py (tested for python3.5), by setting dir_url to a topic search page on cnn webpage Example: For example this webpage shows for the search 'election': https://www.cnn.com/search?q=election run python (3.5 used) scrap.py. The extracted docs will be stored in the folder 'cnn' You can run it for as many topics as you wish Baseline model For baseline, the paper uses the standard PLSA model. Starting with PLSA code from MP3, background model was added. Thus complete PLSA was implemented at plsa_proj.py. Cross-Collection Mixture Model The model is implemented at ccmix.py. Following at the EM update equations from the paper Procedure: Run scrap.py, by setting dir_url to a topic search page on cnn webpage. Set appropritae variables as described in scrap.py Set N - number of docs of each kind in the collection name_set=list of names of each collection eg: ['elon','bezos'] Set number_of_topics Run the code The output displays top_n words in each distribution Important notes 1) In calculating c(w,d) that count of word w in doc d across all words and docs, smoothing must be applied. No c(w,d) should be exactly 0. Esle it'll cause divison by zero problem. In the code, term_doc_matrix stores c(w,d) 2) In the EM update steps given in the paper, observe the update for P(w/theta j,i) i.e the collection specific word distributions. Since both numerator and denominator are summed over the entire collection, P(w/theta j,i) will not capture features specific to the sub-collections. They will all behave similarly. Hence in implementation, the summations are only taken over the docs in collection concerned CS 440 Project: Cross-Collection Mixture ModelStudent: Chirag C. Shetty (cshetty2Paper: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining KDD 2004. ACM, New York, NY, USA, 743748. DOI10.1145/1014052.1014150 [link]Github link: https://github.com/chiragcshetty/CourseProjectIntroductionThe paper explores a further improvement like PLSA in mining topics. In PLSA, k topics are mined from the entire collection. However, collection may have subset and we may be interested in knowing the topics within a collection while also comparing across different collections. The paper adds one more level of generativevariable (lambda_c) and tries to achieve this.DataThe original paper from 2004 had used a set on news articles and reviews fro epionions.com. The site is no longer active and the dataset wasn't archived anywhere. So I decided to write a scraper, starting with the codes used in the MP's. I chose CNN, which has a search feature on its webpage. So I scrap the webpage resulting from searching a topic of interest and extract the news articles. This mostly involved handcrafting the extraction process.Procedure for scrapingThe main python file is called scrap.py Edit the 'name' variable to indicate the topic. Files extracted will be stored with this name no_pages: Number of pages to search. Each page has 10 articles Run scrap.py (tested for python3.5, by setting dir_url to a topic search page on cnn webpage Example: For example this webpage shows for the search 'election': https://www.cnn.com/search?q=election run python 3.5 used) scrap.py. The extracted docs will be stored in the folder 'cnn' You can run it for as many topics as you wishBaseline modelFor baseline, the paper uses the standard PLSA model. Starting with PLSA code from MP3, background model was added. Thus complete PLSA was implemented at plsa_proj.py. Cross-Collection Mixture ModelThe model is implemented at ccmix.py. Following at the EM update equations from the paperProcedure: Run scrap.py, by setting dir_url to a topic search page on cnn webpage. Set appropritae variables as described in scrap.py Set N - number of docs of each kind in the collection name_set=list of names of each collection eg: ['elon','bezos'] Set number_of_topics Run the code The output displays top_n words in each distributionImportant notes1 In calculating c(w,d) that count of word w in doc d across all words and docs, smoothing must be applied. No c(w,d) should be exactly 0. Esle it'll cause divison by zero problem. In the code, term_doc_matrix stores c(w,d)2 In the EM update steps given in the paper, observe the update for P(w/theta j,i) i.e the collection specific word distributions. Since both numerator and denominator are summed over the entire collection, P(w/theta j,i) will not capture features specific to the sub-collections. They will all behave similarly. Hence in implementation, the summations are only taken over the docs in collection concernedExperiments and ResultsTo experiment we need related document sub-collections, each of which have a common theme. One good example of such a collection is about famous people in related fields. I chose 'Elon Musk' and 'Bill Gates'. Both are billionaire businessmen, hence there will be similarities in news articles about them. However of-late they are in news for very different reasons. So each sub-collection has its own features. Articles are scrapped from cnn, in order of recency and stored in folder cnn. There are 29 files for each category.PLSA Baselinelamba_b is kept at 0.9. Increasing it too much seemed to include informative but words into background model. The word 'tesla' for instance for the chosen dataset. Decreasing lamba_b too much lets stopwords leak into topic distributions. Around 0.9 seemed the right compromise. Number of topics is taken to be 2PLSA gives the following result:Clearly the topic 0 refers to 'Elon Musk' with words like 'tesla', 'rocket', 'texas' Muskmoving to Texas has been in news a lot recently). Topic 1 is not as clearly associated with Gates. However given Gates' charity work, especially in healthcare, the topic 1 makes senseCCM Modellamba_b was retained at 0.9 and lambda_c was taken as 0.7. With same dataset as above this is the result of CCMWith lamba_c = 0.6lamba_c= 0.4The difference between Collection 1 and collection 2 specific distribution is stark and informative. They clearly are clustering around Musk's articles and Gates' article. However, the topics themselves do not show much distinction. There is someflavour of topic 0 being about technology while topic 1 is about society and governance. But one can not decisively say so. It might also be possible that the articles donot have enough variety to force the EMto cluster well. Better data with more latent topics may reveal further benefits of the CCM model. Also, knobs of lambda_c and lambda_b can be optimized further.____________________________________________________________________________________________
https://github.com/chriistinahu/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chuanyueshen/CourseProject	"CS 410 Text Information System: Final Project Proposal TEAM PYTHON Team members: Y= Chuanyue Shen (NetID: cs11, team leader) Y= Jianjia Zhang (NetID: jianjia2) Y= Runpeng Nie (NetID: runpeng3) Project name: In-class Text Classification Competition Answer required questions: Y= Are you prepared to learn state-of-the-art neural network classifiers? o Yes. Y= Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. o Neural classifiers: DNN, RNN, LSTM, and CNN. o Deep learning frameworks: TensorFlow, PyTorch Y= Models we may explore: o We may start with a self-developed DNN architecture to get to know about the dataset. Then we may move on to implement some state of art CNN architectures and tune the learning parameters to achieve a higher accuracy. We may also use some pretrained model to expediate the model training. Relevant prior experience: The team members are familiar with Python language and have some limited experience with mini machine learning projects during Machine Learning related courses. We chose this competition to gain some hands on in text classification and to explore machine learning approaches. Programming language: Python Timeline: Y= Model buildup: week 11 - 12 Y= Training and testing: week 12-14 Y= Model improvement: week 14-15 Y= Final Report: week 15-16 CS 410: Project Progress Report TEAM PYTHON Team members: * Chuanyue Shen (NetID: cs11, team leader) * Jianjia Zhang (NetID: jianjia2) * Runpeng Nie (NetID: runpeng3) Tasks have been completed: 1. Data preprocessing: Training data and test data store in JSON line file. Each data item has three fields. The first field of the training data is a label, indicating whether the response of this data is sarcasm or not. The first field of the test data is its ID. Both training data and test data have a response field and a context field. The response field stores the tweet to be classified, and the context field stores the conversation context of the response relatively. Both response and context are string data. Because the data is tweets, so there are lots of emoji objects and @USER marks. The first step is removing the emojis and @USER. First of all, the regular expression package is used since all emojis are encoded in Unicode. By using re.compile() function the emoji Unicode pattern is defined, and emojis are removed through re.sun() function. @USER is relatively easier to remove. Just like what we learned from lectures, there are lots of meaningless stop words like ""the"", ""a"" etc, they are almost useless for text classification. The NLTK package provides an English stopwords list, by adding ""@USER"" to the list and removing all words that appear in the list from response and context. Besides, for text classification, the punctuation character is useless too since the punctuation does not hold sentiment like normal words. Words are combined except punctuation through using string.punctuation. After these operations, the data is basically cleaned. Only having a clean dataset is not enough. In English, with the change of grammar and context, there are many words with similar roots that have a similar meaning. For instance computer, computing, and computational. Their appearance increases the complexity of matrix operations. In order to improve performance and raise classification accuracy, words need to be grouped/replaced by their stem word. NLTK's PortStemmer tool is very useful. By replacing some words with their stem word, the computation complexity dropped significantly. 2. Model implementation Y= Naive Bayes o For this model, we choose the Gaussian distribution to fit the distribution of the count of each word. Then use Naive Bayes classifier to fit the data. o Result: # Precision = 0.5371 # Recall = 0.7967 # F1 = 0.6416 * SVM o For this model, we preprocess the data using TF-IDF methods. Due to the reason that we have too many unique words and which will result in around 20,000 features, but the training data has only 5000 rows which will possibly result in underfitting. We remove the words with term count less than 3. After the data cleaning, the unique words are around 6,000, which is reasonable compared to original unique words. o Parameters: # min_df = 3 o Result: # Precision = 0.5829 # Recall = 0.8633 # F1 = 0.6959 * LSTM (Passed the baseline) o For this model, we use three densely-connected layers and use Sigmoid as the activation function in the output layer. We choose binary cross entropy as the loss function, and RMSprop as the optimizer. We take the 5000 strings as the input and for each string we choose at most max_len = 100 words from the right side to left side. o Parameters: max_words = 5000, max_len = 100, batch_size = 128, epochs = 50, dropout_value = 0.5 o Result # Precision = 0.6068 # Recall = 0.8967 # F1 = 0.7238 * CNN model (in progress): o We start with a basic CNN model based on PyTorch framework. The model uses three convolutional layers with LeakyReLU and BatchNorm2d, and Sigmoid as the activation function in the output layer. We use SGD as the optimizer, and NLLLoss as the loss function. With proper fine tuning, the model is expected to pass the baseline and beat the LSTM model. Tasks are pending: * Finish the CNN model and fine-tune the learning parameters * Implement some other state of art models/methods, such as transformer-based machine learning technique BERT Challenges: * During tuning LSTM models, we found it is not easy to improve the precision value. * Current completed models cannot achieve much higher F1 scores. We will try to achieve the F1 scores by using CNN model and/or BERT model and finetuning the parameters. CS 410 Final Project - Classification Competition This is TEAM PYHTON's repo for CS 410 final project classification competition. This competition is about Twitter Sarcasm Detection. We implemented Naive Bayes, SVM, and LSTM to classify a Tweet as Sarcasm or Not sarcasm. LSTM model gave the best prediction for the test dataset. Team members: Chuanyue Shen (cs11), Jianjia Zhang (jianjia2), Runpeng Nie (runpeng3) Prerequisite Please use Python3 and install the following packages: numpy jsonlines nltk string re autocorrect Keras sklearn Pandas It is recommended to run our program on machines with GPU. Run the code Data cleaning and preprocessing Type python preProcessData.py Train the model and make predictions To run the LSTM model, type python lstm.py To run the SVM model, type python svm.py To run the Naive Bayes model, type python navieBayes.py Test dataset prediction After running the model, the test dataset prediction will be saved in the local directory, named answer.txt Reference https://www.kaggle.com/kredy10/simple-lstm-for-text-classification Presentation https://youtu.be/IC9ncGVvbcQ More details about the project Source code Please refer to the Source code part to the ""Run the code"" part mentioned above. The test set prediction of our best results can be found in answer.txt. The F1 score of one of our best results using LSTM beat the baseline and can be found in the Livedatalab leaderboard under the name of cs11 and/or jianjia2. Implementation details Data preprocessing: Training data and test data store in JSON line file. Each data item has three fields. The first field of the training data is a label, indicating whether the response of this data is sarcasm or not. The first field of the test data is its ID. Both training data and test data have a response field and a context field. The response field stores the tweet to be classified, and the context field stores the conversation context of the response relatively. Both response and context are string data. Because the data is the tweets, so there are lots of emoji objects and @USER marks. The first step is removing the emojis and @USER. First of all, the regular expression package is used since all emojis are encoded in Unicode. By using re.compile() function the emoji Unicode pattern is defined, and emojis are removed through re.sun() function. @USER is relatively easier to remove. Just like what we learned from lectures, there are lots of meaningless stop words like ""the"", ""a"" etc, they are almost useless for text classification. The NLTK package provides an English stopwords list, by adding ""@USER"" to the list and removing all words that appear in the list from response and context. Besides, for text classification, the punctuation character is useless too since the punctuation does not hold sentiment like normal words. Words are combined except punctuation through using string.punctuation. After these operations, the data is cleaned. Only having a clean dataset is not enough. In English, with the change of grammar and context, there are many words with similar roots that have a similar meaning. For instance computer, computing, and computational. Their appearance increases the complexity of matrix operations. To improve performance and raise classification accuracy, words need to be grouped/replaced by their stem word. NLTK's PortStemmer tool is very useful. By replacing some words with their stem word, the computation complexity dropped significantly. With the steps mentioned above, we can generate a n * 3 numpy array. The 3 columns store label, response, and context, respectively. The array is saved in 'dataStep1.csv' file. Using the n * 3 numpy array, we can build a term document matrix that counts the frequency of each vocabulary in each tweet. The term document matrix is saved in 'term_doc_matrix.csv' file. 'dataStep1.csv' is mainly used in LSTM model implementation with some further processing. 'term_doc_matrix.csv' is mainly used in Naive Bayes and SVM models. Model implementation and test result * Naive Bayes For this model, we choose the Gaussian distribution to fit the distribution of the count of each word. Then use the Naive Bayes classifier to fit the data. Result: # Precision = 0.5371 # Recall = 0.7967 # F1 = 0.6416 * SVM For this model, we preprocess the data using TF-IDF methods. Due to the reason that we have too many unique words and which will result in around 20,000 features, but the training data has only 5000 rows which will possibly result in underfitting. We remove the words with term count less than 3. After the data cleaning, the unique words are around 6,000, which is reasonable compared to original unique words. In the training process, we shuffle the training dataset and split it as training data (0.8) and validation data (0.2). Result: # Precision = 0.5829 # Recall = 0.8633 # F1 = 0.6959 * LSTM (best model) For this model, we create a neural network with LSTM, and word embeddings were learned while fitting the neutral network. We try to manipulate the number of layers and layer depths to find the best model architecture. Our final best RNN with LSTM model is composed of an embedding layer, a LSTM layer, 3 densely-connected layers, and activation and dropout layers in between. 'ReLu' is used for the activation function. Dropout = 0.5 is used to reduce overfitting. We adopt Sigmoid as the activation function in the output layer. In the training process, we try to compare different loss functions (mse, binary cross entropy), optimizers (Adam, SGD, RMSprop), and batch sizes (16,32,64,128). Our final best model using binary cross entropy as the loss function, RMSprop as the optimizer, and batch size = 128. Also, we shuffle the training dataset and split it as training data (0.8) and validation data (0.2). For the data input, we further process the 'dataStep1.csv'. At first, we take the 5000 strings as the input and for each string, we choose at most max_len = 100 words from the right side to the left side. And then, based on the vocabulary, we store the index of each word for each string in the table (matrix_sequences) with its size as 5000 by 100. Overall, the data input for the LSTM model would be a matrix with a size of 5000 by 100. Result # Precision = 0.6068 # Recall = 0.8967 # F1 = 0.7238 Contribution All team members made equal contribution to the project and commited 20 hours+ per person to this project."
https://github.com/chungfaith1/CourseProject	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. For the final project submission, please go to the ""documentation"" folder to read the Final Project Report and Presentation. The link to the demo is here: https://www.youtube.com/watch?v=K2cQH4pPyBk&feature=youtu.be"
https://github.com/clairebrekken/CourseProject	"Progress Made Thus Far: Wehavestartedbuildingafoundationforanapplicationthatwillaccuratelyclassifysarcastic tweets.Westartedbybrainstormingdifferentclassificationmodelingtechniques,featuresto engineer,andresearchingexistingwhitepapersonmoreadvancedapplications.Throughour initialresearch,wedecidedtoimplementaseriesofmodelsrangingfromnaivetoadvanced includingDecisionTreeClassifier,LogisticRegression,LinearSVC,SVCwithradial-basis kernel,K-NearestNeighborsClassifier,GaussianNaiveBayes,RandomForestClassifier, AdaBoostClassifierandGradientBoostingClassifier.Wefirstspot-checkedthesemodels (usingdefaultparameters)via10-foldcross-validationontheentiretrainingset.Theresults showedthatlinearmodelssuchaslogisticregressionandlinearSVCworkedaswellasdidthe morecomplexnonlinearmodels.Wethenproceededtoperformhyper-parametertuning(using 10-foldcross-validationandF1scoreasthemetric)tooptimizethemodels.Equallyas importantasthemodelingtechniques,weneededtoengineerfeaturestocapturethenuances thatdeterminewhetheratweetissarcastic.Sofar,wehaveimplementedfeaturestorepresent thenumberofuserstaggedinthetweet,numberofhashtags,lengthoftweet,andnumberof charactersinthetweet.Wealsohavetriedtocapturethesentimentofthetweetbycreating featurestorepresentellipses,laughter,affirmations,negations,interjections,intensifiers, punctuationandemojis.Aftercreatingthesefeatures,wealsousedTSNEtovisualizeour featurestogetanideaofhowtheyclusterandrepresentthedatathusfar.Overall,wehave seenthebestperformancewiththelinear/regressionmodelscomparedtomorecomplex modelssuchasAdaBoostortheGradientBoostedClassifier.Ourbestmetricsthusfarareas follows: Precision = 0.6006, Recall = 0.8355 and F1 = 0.6988. Remaining Tasks: Ourlargestareaforimprovementofourcurrentworkistoengineeradditionalfeaturesto capturemoreofthenuancesofsarcastictweets.Sinceourcomplexmodelshavenotperformed wellthusfar,wethinkthisindicatesthatourfeaturesarenotcomplexenoughtocapture characteristicsofsarcastictweets.Wethinkourbiggestwinswillcomefromcreatingfeatures basedonthecontextofeachtweet.Sincesarcasmisoftenaresponseaspartofa conversation,wearehopingthiswillimprovemodelperformance.Wealsoplanon implementingfeaturestorepresenttextpatternsinsarcastictweetslikepartsofspeech, n-grams,andtopicsrepresentedinthetweetusingLDAorasimilartechnique.Wealsothinkit wouldbeinterestingtoimplementadeeplearningmodelaftertheadditionalfeatureengineering andevaluatehowthisperformsagainsttheothermodels.Additionally,weplantoexplorean Ensemblemodelthatcombinesthemodelsthathaveperformedwelltoseeifwecanoptimize performancethatway.Intheend,weplantohavearobustsuiteofmodelsthataretrainedon data with features that capture the nuances of sarcastic tweets. Challenges/Issues: Ourmainobstaclewearecurrentlyfacingisthatwearenotreachingbaselineperformance. Withtheremainingtasksthatwehavedetailedabove,webelievewewillbeabletoreach baselineaccuracywithourfinalimplementation.Ourotherchallengehasbeentime.Between finishingupcontentforthecourseandpreparingforthesecondexam,wehavenotbeenableto prioritizeworkingonthisproject.Halfofourteamhasnowcompletedtheexamsowearenot concernedwithtimebeingachallengeaswecompletethisprojectoverthenextcoupleof weeks. Project proposal 1. What are the names and NetIDs of all your team members? Team members: * Claire Brekken (brekken2) - Captain * YiZi Xiao (yizix2) 2. Which competition do you plan to join? We plan to join the text classification competition. 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Yes we are prepared to learn the state-of-the-art neural network classifiers. The classifiers we have heard of and plan to explore are: * Logistic regression classifier * Deep neural network classifier (DNN) * Recurrent neural network classifiers * LSTM models * Convolutional neural network classifiers (CNN) Relevant experiences: * Logistic regression model to detect seizures in EEG data (medical device application) * Logistic regression to predict likelihood of orders happening in the future (supply chain/logistics application) * DNN for classify respiratory sounds (medical device application) * DNN for estimated time of arrival (supply chain/logistics application) * CNN for handwritten and image recognition (school work) 4. Which programming language do you plan to use? We plan to use Python. Text Classification Competition: Twitter Sarcasm Detection Demos: - feature_engineering_demo.zip - yizi_xiao_demo.zip - bert_demo.zip Code: Requirements: Python version >= 3.7 pip3 install -r requirements.txt Files: There are 3 Jupyter notebooks which contain the code developed for this project: feature_engineering.ipynb ml.ipynb bert.ipynb feature_engineering.ipynb This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. This notebook outputs data to be used in the conventional ML models. ml.ipynb: This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. It is also dependent on feature_engineering.py to have been run previously and that the output of that notebook has been saved. bert.ipynb This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. Project Specification Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/cnj3/CourseProject	"Progress Report for Project Chameleon (Jacob Huss, Chaitanya Jujjavarapu, and Edward Park) 1. We have successfully scraped the needed IEM data from the IEM website into a CSV. We then wrote IEMcsvTranformer.py, which took this data and calculated the normalized price for each date. These normalized prices were stored in data/IEMPrices.txt. We then wrote a function in DataReader.py to read these prices into a dictionary. For the 2nd part of our scraping process, we had to create a program that fetches the data from all 47000 xml files between the dates of May 2000 and October 2000, and then parses the text of each article so that only our matching query terms were pulled out and outputted and cleaned up into a text file through a 'bag of words' approach. 2. Now that we have our data completely scraped and prepared, we need to implement the algorithm detailed in the paper. In order to do this, we will first need to break down our algorithm into smaller pieces that can be tackled by each team member. Doing this will require us to first gain a better understanding of the algorithm itself. 3. The biggest challenge that we are facing is understanding how the algorithm for our function works. 1.Team members: a. Jacob Huss, NetID: jnhuss2 (team captain) b. Chaitanya Jujjavarapu, NetID: cnj3 c. Edward Park, NetID: edwardp3 Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback: https://dl-acm-org.proxy2.library.illinois.edu/doi/epdf/10.1145/2505515.2505612 Python Yes. We have access to all the data used in the first experiment described by the paper: a. We will scrape the New York Times data from this site: https://spiderbites.nytimes.com/2000/ b. We will scrape the IEM data from this site: https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 N/A N/A CS 410 Final Project jnhuss2, cnj3, edwardp3 Overview We are working on reproducing the paper, ""Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback."" Our code scrapes two datasets, IEM Price History data and the New York Times site map data. It uses these datasets to generate common topics in New York Times articles that are correlated with change in the IEM prices. The algorithm uses an iterative approach. It first uses LDA to generate topics. It then analyzes these topics to determine which topics and words are most correlated to change in the IEM data. These results are used to generate priors, which are used as input to the next iteration of LDA. How the Software is Implemented The primary functionality of the ITMTF algorithm is in the file main.py. In the following section, we break this file down into the various functions that are used to implement it. This file data_reader.py is used by main.py to read in the IEM and NYT data from the data files IEMPrices.txt and articles.txt. Both of these txt documents are held in the data directory. article_compiler.py consists of the code that was used to scrape the NYT data and create articles.txt. iem_csv_tranformer.py consists of the code that was used to create IEMPrices.txt from the data held in IEMPrices.csv. Functions In main.py: read_data - reads in data from articles.txt and IEMPrices.txt run_itmtf - runs the ITMTF algorithm with nyt_data and runs methods to determine the significant topics get_topic_total_probability_by_date - finds the probability of topics by date and creates a multidimensional array find_significant_topics - finds the significant topics using the granger algorithm granger_significance_test - runs grainger significance test off of a multidimensional array that is inputted as a parameter get_pos_neg_words - states whether the word has a positive or negative correlation with the IEM prices determine_new_priors - creates a weight for each word to be used next time LDA is run get_avg_confidence - gets the average confidence amongst all of the words that have been chosen to be significant get_avg_purity - gets the average purity amongst all of the topics In data_reader.py get_iem_prices - reads in IEMPrices.txt and returns a dictionary of date object keys and the iem normalized price values read_data_file Usage of Software / How to Run APIs Used To program our software, we used Python. We used Python because of its simplicity and packages. For scraping the New York times article data, we used bs4's BeautifulSoup. We used the gensim library in multiple locations. In main.py, we used gensim to create a dictionary that acts as a vocabulary list for the entire corpus. The keys in the dictionary act as ids and the values are the strings of words, which is nice because each word gets an id associated with it. In data_reader.py, gensim is used to clean up the characters and words scraped from the New York times data. We used gensim methods, such as strip_punctuation and remove_stopwords. Next, we used statsmodels.tsa.stattools in main.py because it contains a method called grangercausalitytests, which we used for the granger test algorithm. We also used scipy.stats because it contains a method for the pearson correlation algorithm, pearsonr. How to Run To run the code, clone the repository and open it in your terminal. After this, install all of the needed packages and run python main.py. sh $ pip install numpy $ pip install datetime $ pip install bs4 $ pip install gensim $ pip install statsmodels $ pip install scipy $ python iem_csv_transformer.py $ python main.py Running this program took our computers about 25 minutes to complete. It runs the ITMTF algorithm a total of 4 times, each time varying the number of topics that is generated by LDA. Each time the ITMTF algorithm is run, its core functions are iterated 5 times (LDA runs 5 times). With each iteration, the program prints out the topics it generated that are significant as well as the words that were most correlated to positive and negative movement in the IEM price. Once ITMTF is run all 4 times, it prints out the average purity and causality for each iteration in each run of the algorithm. Team Member Contributions Jacob sh Scraped the IEM data. Wrote the code to read in IEM and New York Times data files. Wrote the backend code to find the significant words, differentiate if they are positive and negative, and make a list of words that ""cause"" or are related to changes in the IEM betting prices. Chaitanya sh Worked with Edward to scrape the New York Times data. Cleaned the code, worked on implementing the method to find significant words, documented the code, and wrote the documentation Edward sh Worked with Chaitanya to scrape the New York Times data. (Ghosted us for the rest of the project) Presentation You can view the final presentation using this YouTube link: https://youtu.be/5NiqwlT-tu4 It is also in the GitHub repository."
https://github.com/codylw2/CourseProject	"Cody Webster Progress Report Progress made thus far Generic Wrote script(s) to process documents and convert to a json format for easy loading in the future. Json files contain tokenized representations of the text and queries Cranfield Wrote script to convert json text data into Cranfield datasets as required for the metapy module. Adapted the scripts for homework to find the optimal values of weights in for the JM and BM25 algorithms. Adapted the scripts for homework to ranking documents. BERT Downloaded Google's MS-Marco pre-trained BERT model. Wrote script to examine documents and determine missing vocab that is common to the corpus. Adapted the scripts in the git repo (https://github.com/cognitiveailab/ranking) to finetune and train the model with the tensorflow_ranking python module. Adapted the scripts in the aforementioned git repo to run the scripts in a Docker container running in Window's Subsystem for Linux on my desktop's GPU. Rewrote the script for running to Docker to run outside of Docker for use with Google COLAB. Wrote script to convert ranking output into a predictions file. Remaining Tasks Generic Consolidate scripts and files that are spread over several directories and add into the git repo. Cranfield Run again with titles included in the ""text"" (not likely to beat baseline) BERT Finish running the BERT model and compile results. Each document/query combination that is longer than 512 broken tokens has been into segments. I will test whether a mean, geometric mean, or max score within the segment is a better metric. Test if I can use a larger BERT model with Google Colab (currently using BERT Small) Any challenges Adapting the examples in the git repo so that I can run the BERT model on my system or in git Hardware limitations on my PC. Partially solved by using Google COLAB Scoring the documents with BERT takes an excessive amount of time due to the large number of documents in the corpus and the limit of 512 tokens per analysis run. This excessive wait on results has seriously slowed my progress. Actually beating the baseline performance. Cody Webster Progress Report 1) Progress made thus far Generic Wrote script(s) to process documents and convert to a json format for easy loading in the future. Json files contain tokenized representations of the text and queries Cranfield Wrote script to convert json text data into Cranfield datasets as required for the metapy module. Adapted the scripts for homework to find the optimal values of weights in for the JM and BM25 algorithms. Adapted the scripts for homework to ranking documents. BERT Downloaded Google's MS-Marco pre-trained BERT model. Wrote script to examine documents and determine missing vocab that is common to the corpus. Adapted the scripts in the git repo (https://github.com/cognitiveailab/ranking) to finetune and train the model with the tensorflow_ranking python module. Adapted the scripts in the aforementioned git repo to run the scripts in a Docker container running in Window's Subsystem for Linux on my desktop's GPU. Rewrote the script for running to Docker to run outside of Docker for use with Google COLAB. Wrote script to convert ranking output into a predictions file. 2) Remaining Tasks Generic Consolidate scripts and files that are spread over several directories and add into the git repo. Cranfield Run again with titles included in the ""text"" (not likely to beat baseline) BERT Finish running the BERT model and compile results. Each document/query combination that is longer than 512 broken tokens has been into segments. I will test whether a mean, geometric mean, or max score within the segment is a better metric. Test if I can use a larger BERT model with Google Colab (currently using BERT Small) 3) Any challenges * Adapting the examples in the git repo so that I can run the BERT model on my system or in git * Hardware limitations on my PC. Partially solved by using Google COLAB * Scoring the documents with BERT takes an excessive amount of time due to the large number of documents in the corpus and the limit of 512 tokens per analysis run. This excessive wait on results has seriously slowed my progress. * Actually beating the baseline performance. IR Competition Project Documentation Cody Webster Contents Overview 3 Project Demonstration 3 Project Results 3 DirichletPrior 3 JelinekMercer 4 OkapiBM25 4 BM25+ 4 BERT 5 SciBERT *difficulty running this model limited options 6 Tensorflow Ranking custom model 7 Assumptions 7 Installation Instructions 7 What to install if running locally 7 What python packages to install 8 How to Run 8 Prepare Data (Required if local) 8 (Method 1) Run BM25+ or another ranker (reproduces best results) 9 Option 1 (Google Colab) 9 Option 2 (run locally) 9 (Method 2) Run Tensorflow Ranking 9 Option 1 (Google Colab) 9 Option 2 (run locally) 9 (Method 3) Run a BERT model 9 Option 1 (Google Colab) 9 Option 2 (run locally) 10 Script API Documentation 10 General 10 Metapy (BM25, ...) 12 Tensorflow Ranking 13 BERT 16 Resources 20 Citations 20 Installation Guidance 20 Stopwords 20 BM25 21 Tensorflow Ranking 21 Tensorflow Ranking (how to serve a model) 21 Tensorflow Ranking (how to predict) 21 BERT Checkpoint Conversion 21 BERT 21 SciBERT 21 Docker 21 Overview The purpose of this project is to participate in the IR Competition. For this project I developed and extensively tested three main ways to rank documents. The first is ranking documents using a ranking function either packaged with metapy or through a custom definition implemented through metapy. Initially, the best results that I achieved with this method were through a custom implementation of BM25+. Through a longshot attempt on the last day, I actually beat all of my other rankings using metapy's BM25 implementation. The second is ranking documents using a custom model that is trained using the python library tensorflow_ranking. The third option is to use a pretrained model and it's associated vocabulary and then finetune it using tensorflow_ranking. Overall the best results that I have achieved have come from running the OkapiBM25 algorithm implemented with metapy and using parameters that were produced from a brute force optimization. Project Demonstration I have uploaded ""cs410_project_demo.mp4"" into the git repository. I have also uploaded the file to Illinois Media: https://mediaspace.illinois.edu/media/t/1_07py0q5f Project Results Unfortunately, despite my best attempt I was not able to beat the initial baseline performance score as defined by the class professor or TAs. Nobody else was able to beat the initial baseline either so it is likely that baseline was an unreasonable standard for us to achieve with our limited knowledge and experience. In this section I will briefly detail some of the variations and experiments that I attempted. It is not possible for me capture every variation that I tried but I will cover as best as I can. I do not have a consistent record of the score for every attempt so I will not be including those scores. On the last day of the competition, the class administrators lowered the score of the baseline. I decided to give it another shot because I really had nothing to lose. I was able to beat the new baseline using the standard BM25 algorithm implemented via metapy. The only difference between what I did on the last day and what I had done on previous days was that I allowed one of the parameters to vary more than I initially had. I will detail that more within the BM25 section. DirichletPrior This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. Initial attempts with this algorithm were made using the body text only. Later attempts also used the title and abstract/intro. Further experimentation with the KLDivergencePRF implementation in metapy and variations on its parameters was also performed but performance lagged behind other approaches. JelinekMercer This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. This algorithm was only ever attempted with the body text and results were deemed not good enough to justify further experimentation. OkapiBM25 This algorithm was implemented using metapy. The value of the three parameters was optimized on the training dataset using a brute force approach of testing every reasonable combination of parameters and selecting the set that achieved the greatest. Due to the number of variations that I tried with this algorithm, I had to optimize the parameters a number of times and I developed a multi-processing script that was capable of doing this at a much quicker pace. Initially I only attempted to run the algorithm on the body text for each paper but the performance for that was poor. With the body text I attempted to optimize a different NDCG values, 10, 20, and 50 respectively. None of them produced significant improvements in the overall performance. Next, I attempted to use the title, abstract, and introduction along with the body text and that produce marginal improvements in the score. Another boost came once I removed the body text from the dataset and only trained on the title, abstract, and introduction. I also attempted to use metapy's implementation of Rocchio feedback. I optimized the parameters for Rocchio in a similar manner to the normal OkapiBM25 algorithm. Ultimately the Rocchio feedback produced worse results then the standalone ranker so I excluded it from further test with this algorithm. The performance was initially still inadequate so I pursued further methods. One the last day, after I noticed that they had lowered the baseline, I attempted some new variations of this algorithm. I did not run the other algorithms because they all take longer to run and I did not have time to implement anything new. The new variation that I tried on the last day was to let the k3 parameter vary. I had previously not done this due to the faulty assumption that it would only hurt my results because that was the experience that I had during MP2.2. Almost immediately while running the pooled optimization, I was able to see that the performance was superior my previous results. I selected a number of the different parameter combinations to try from the generated set. On the third try I was able to beat the baseline using k1=2.0, b=0.75, and k3=4450. I am immensely frustrated that I have wasted so much effort trying to find other ways to rank documents and the answer was so simple but I am happy to have beaten the baseline. BM25+ This algorithm was implemented with metapy's api using the definition of BM25+ from (Trotman et al, 2014). Since it was implemented in Python and C++ like the native metapy implementations, it ran slower. I optimized it in the same way that I optimized the OkapiBM25 algorithm. This implementation ultimately outperformed the native OkapiBM25 algorithm, but it did not beat the baseline. Better results were obtained when using this in conjunction with the BERT implementation. I tried numerous variations on this algorithm in an attempt to increase my score on the test dataset. Similar to the other algorithms, I initially tried to only use the body text for my analysis. This did not produce the results that I desired. I have briefly listed some of the other variations that I tried: attempted with body text only attempted with title, abstract, intro, and body text attempted with title, abstract, and intro attempted with title only attempted with Rocchio pseudo feedback, varying the parameters to Rocchio attempted to remove all urls in the corpus attempted to replace all variants of the words coronavirus, covid-19, 2019-ncov, and sars-ncov-2 with coronavirus attempted to run with query text, question text, or narrative text attempted to run with all variations of query combined attempted different variations of metapy analyzer chains attempted to remove docs with duplicate s2_id but different uids, docs were functionally duplicates attempted to use a date cutoff for document attempted to pre-tokenize the queries and documents using BERT's tokenizer attempted to use multiple datasets with various weightings and combine into one result used different variations of stopwords that were gathered online (listed in resources) Ultimately, I was not able to beat the baseline using this custom algorithm, so I continued my exploration of other possibilities. BERT I had completed my Technical Review over the BERT model and had learned through that about its superior performance when classifying documents. I chose this as the algorithm to attempt to rank with. It was a significant struggle to get BERT to work for document ranking. There are very few tutorials online for how to achieve this and I spent a significant amount of time trying to get it to work. I ultimately was able to find a tutorial provided by Peter Jansen from the University of Arizona (link in resources). This tutorial gave an example of using the tensorflow_ranking module with BERT to achieve document ranking. Unfortunately, the tutorial relied on Docker to serve a fine-tuned model. I had to do a significant amount of researching about how to get Docker to work with a tensorflow model on Windows. In order to get it to work, I had to upgrade my Windows 10 OS to the Development version so that it would support the latest Windows Subsytem for Linux (WSL). I needed the latest WSL because that was what nVidia required for their latest CUDA drivers for GPUs. I needed the latest CUDA drivers because that was the only way to get Docker to run on my GPU using WSL. I also had to install Bazel for Windows because the training script was meant to be compiled to run. This lengthy setup process is obviously unsuitable for the rapid development that I needed and because it is not realistic for reviewers to reproduce this setup on their own machines. Docker is also not supported in Google Colab so I needed to arrive at a repeatable and easily setup solution for the graders. I was able to remove the need for Docker and the need to serve the model at all by exploring the documentation for tensorflow. This exploration give me insight into how to directly load the model and use it to predict document scores. I implemented this methodology into the scripts and was thus able to create a version that can be easily ran within Google Colab for easy review. Once I had an effective way of running the model, I was able to experiment with various ways to utilized it. I attempted to run three variations of the pre-trained BERT model, all provided by Google Research, BERT-Mini, BERT-Small, and BERT-Base. The BERT-Base model was too large to effectively train without utilizing TPUs. I choose to do my testing with the BERT-Mini model because it allowed me the most flexibility with my training parameters. Some key parameters used are list size and batch size. The size of the model along with those two parameters determine what hardware is required to run the model. If you try to run on inadequate hardware than you will easily run out of memory and be unable to train the model. I attempted a number of variations in ways to score or setup the data. Ultimately, I was not able to beat either baseline when using BERT but I was able to improve upon my initial results with BM25+. I have detailed some of the variations that I tried below. varied the list size varied the batch size updated vocabulary to include most common coronavirus variants attempted to run with query, question, or narrative attempted to vary the max token sequence size 256, 512, 1024 relevance = score of first doc segment relevance = max score of all doc segments relevance = mean score of all doc segments ignore body text and only use title+abstract+intro varied the number of training steps used both NCDG approximated loss and softmax loss re-rank top 1000, 2000, 5000, and 10000 results of BM25+ SciBERT *difficulty running this model limited options This model was initially very promising but the issues with getting it to run ultimately made it insufficient for what I needed. This model was create by Iz Beltagy et al. for scientific research. This model is essentially just a BERT-Base model that was pretrained on a scientific corpus instead of the generic one. The vocabulary consists of more scientific terms as a result and should theoretically be able to perform better at ranking the scientific documents in the CORD-19 corpus. I updated a few of the unused vocabulary items to the command variants of coronavirus and included the drug remdesivir as well. Unfortunately, due to the size of the model I was very limited in the list size and batch size options that I could try. I firmly believe that with a larger list size I would have been able to adequately finetune this model and it would have performed better than the BERT model. My inability to get the model to run on the TPUs available through Google Colab prevented me from realizing this goal. Tensorflow Ranking custom model This model was created by basing it off of the example in the tensorflow ranking repository. I left the structure of the context features and the example features the same. The context features contain the query tokens and the example features contain the document tokens and when training, the relevance judgement of the file. I attempted to change the script to use the argparse module instead of the flags from the absl module but when that was attempted the script stopped producing the results of the training data to stdout. I determine that there are likely scripts deeper within the tensorflow ranking module that are using these flags to determine various facets of the training process, so I restore the flags. The model consists of 3 hidden layers at a size of 64, 32, and 16 respectively. During testing the dropout rate of the model was adjust to 0.65 from the default of 0.80. This adjustment produced better results but further drops in this dropout rate risked overfitting the model to the training data. The batch size was set to 1 and the list size maximized so that it would train on more docs for a single query at once. Based on empirical results that I observed during experimentation, this produced better results than increasing batch size and lowering the list size. The maximum limit for the list_size was 100 depending on the hardware available in Colab and if it exceeded that value then it would run out of memory when training. One other significant variable was the max sequence length to use for each example. I tested with both 512 and 1024. The results did not appear to differ significantly between the two but this variable is important because it defines how much of the document can be captured per example. For this model I only used the title, abstract, and introduction because previous experimentation on other models and rankers showed that the body text was not helpful. Ultimately the results produced with this method were worse the results obtained from the BERT models. Assumptions If you are running this locally then you are running this code on Windows 10 machine that has ample RAM and a CUDA capable gpu. Every script can run on Linux but I have not generated scripts within this repo to accommodate that. All file paths are relative to the base directory of the repository Installation Instructions What to install if running locally Acquire the datasets: Files should be downloaded and unzipped here: .\competition\datasets The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing Python Instructions: https://docs.conda.io/projects/conda/en/latest/user-guide/install/ Latest Nvidia Drivers (required to run BERT or Tensorflow Ranking on GPU) Instructions: https://docs.nvidia.com/cuda/wsl-user-guide/index.html Instructions require installation of other software and packages you must follow all of them BERT Model (required to run BERT) Due to the size of the models, they are not directly included in the repository Google BERT Models (recommended): Google offers a variety of different sizes for the models to run. I ran using a BERT Mini but could possibly use a BERT Small as well. Anything larger likely requires a TPU through Google Colab to run. https://github.com/google-research/bert SciBERT Model (not recommended, requires TPU): https://github.com/allenai/scibert/ What python packages to install To run the full capability of all scripts defined in this project it is required that you setup two environments. All but one script can run in the main environment but the BERT checkpoint convertor relies on the dev version of a module that conflicts with requirements of the other modules: Main Python Environment: This is the main environment and should be used when running most of the scripts. You can install the latest stable versions of these modules to run the project. Python Version: 3.7 Pip Packages: tensorflow_ranking metapy pytoml BERT Conversion Environment: This is required because there are conflicts in the dependencies of the modules required to convert a model's checkpoints and the modules used to run it. Python Version: 3.7 Pip Packages: tf-models-nightly How to Run **WARNING: running some of these scripts will require a large amount of RAM if you use the full dataset** * Your python environment should be activated before running any scripts Prepare Data (Required if local) Generate the json file that contains the information for each dataset. This is required in order to run any of the different methodologies. File Path: .\competition\create_bert_data.bat (Method 1) Run BM25+ or another ranker (reproduces best results) Option 1 (Google Colab) Run each cell individually from top to bottom to fully. If you view the file within Github there is a link at the top of the file to open in Google Colab. This is the only Google Colab file that does not require a Google Colab Pro account. The free account should be capable of running this notebook. File Path: .\colab_cranfield_metapy.ipynb Option 2 (run locally) Generate the Cranfield Datasets that will be required to run the ranker File Path: .\competition\cranfield_metapy\cm_create_data.bat Run the ranker of your choice (default arguments already in the file) File Path: .\competition\cranfield_metapy\cm_rank_docs.bat (Method 2) Run Tensorflow Ranking Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. T This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_tfr.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\tfr_custom\tfr_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\tfr_custom\tfr_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\tfr_custom\tfr_predict.bat (Method 3) Run a BERT model Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_bert.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\bert\bert_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\bert\bert_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\bert\bert_predict.bat Script API Documentation **Unless otherwise specified paths are relative to the root of the repository General File: ./competition/check_covid_variants.py Purpose: Check for coronavirus variants in the corpus Source: developed by project team API: variant_file: the file that the coronavirus variants will be output to. File can already exist. Existing variants will be loaded. known_variants: known variants of the coronavirus that exist in the corpus and will be used to determine other variants doc_keys: the keys to use when search the document dictionary for variants run_type: the dataset that will be searched for variants input_dir: the directory that contains the json representation of the datasets to be processed Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. It loads the json for one or both datasets and then processes the text to find variants of some predefined words in the corpus and queries. The text is processed using python multiprocessing module in order to speed up execution of the script. File: ./competition/checkpoint_converter.py Purpose: Converter BERT checkpoint files from Tensorflow v1 to v2+ Source: Tensorflow Model Garden Repository API: bert_config_file: Bert configuration file to define core bert layers. checkpoint_to_convert: Initial checkpoint from a pretrained BERT model core (that is, only the BertModel, with no task heads.) converted_checkpoint_path: Name for the created object-based V2 checkpoint. checkpoint_model_name: The name of the model when saving the checkpoint, i.e., the checkpoint will be saved using: tf.train.Checkpoint(FLAGS.checkpoint_model_name=model). converted_model: Whether to convert the checkpoint to a `BertEncoder` model or a `BertPretrainerV2` model (with mlm but without classification heads). Detailed Description: This script was not written by me but was lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script is meant to convert a BERT module that you downloaded into a version that is capable of being ran using tensorflow version 2.0+. File: ./competition/create_bert_data.py Purpose: Combine and compile the information from the dataset into and easily loadable json for use by other scripts Source: developed by project team API: vocab_file: The file containing the vocabulary to be used when tokenizing a text variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants tokenize: If use, this will store a tokenized representation of the script in the output json. Must be used with `vocab_file` input_dir: The directory that contains all of the source files for the dataset output_dir: The directory where the json representations of the documents will be stored Detailed Description: This is a python script that was tested in python 3.7. The inputs to the script will determine the exact behavior at run time but lets discuss the more complete case of the training dataset. When the training dataset is specified it will start by loading the queries from there original xml format into a python dictionary. If the script is ran with the tokenize option then each of the three variants of the query (query, question, and narrative) will be tokenized according to the vocabulary defined in the specified vocab file. This json is then dumped into a target directory. For the training dataset it will also load the query relevance judgements. A list of all of the documents and their associated uid, title, abstract, publication date, and list of files containing the text representation is pulled from the metadata.csv file. The list of documents to further process is pruned done to the same as the list of relevance judgements. For each document to process the script iterates through the list of files that contains the text representation until it finds a suitable candidate or exhausts all options. The text from the representation file is loaded in the document dictionary object. If the tokenize option is used, the documents will then be tokenized through the use of a multi-processed pool. The resulting output is then written to the disk. Metapy (BM25, ...) File: ./competition/cranfield_metapy/create_cranfield.py Purpose: Use the json representation of the dataset to create a cranfield dataset for use with metapy Source: developed by project team API: variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants query_keys: The keys that will be used when creating the cranfield query file. If multiple keys are specified, text will be combined. doc_keys: The keys that will be used when creating the cranfield document files. Multiple keys can be specified and multiple keys can be combined into a single document. If creating separate datasets, use a ';' to separate keys. If combining keys for a dataset use a ':' to separate keys: cranfield_dir: The directory to use as a base for generating the cranfield data. input_dir: The directory where the json representations of the documents are stored Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. This uses the loaded json representation of the dataset to produce a use specified cranfield dataset for use with metapy. Each cranfield dataset can be uniquely defined by the user to consist of a one-to-one relationship with keys in the document dictionary or it can consist of multiple keys combined. This allows for flexibility in how the expansive each iteration of ranking testing is. File: ./competition/cranfield_metapy/search_eval.py Purpose: Rank the documents in the corpus and create the predictions file Source: original version from MP 2.2 and heavily modified to fit use case API: config_template: The template file that will be used for creating the configs for each run of the ranker run_type: The dataset that will be searched for variants dat_keys: The keys to use that indicate the name of the cranfield dataset(s). This corresponds to the first key used for every section of 'doc_keys' parameter and the 'create_cranfield.py' script. doc_weights: The weights to use when combining the rankings of multiple datasets. ranker: The ranker to use for ranking the documents. Valid rankers can be found in the script. params: The value(s) for the ranker parameters. Multiple values should be separated by `;`. cranfield_dir: The directory that is the base for the cranfield dataset(s) predict_dir: The directory to contain the predictions file. remove_idx: Delete an existing inverted index and create a new one. If no index exists it will not fail Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to specify what ranker to run from a predefined list of rankers. The user can also run rankings over multiple datasets and specify the weight that each ranking will contribute to the final prediction rankings. File: ./competition/cranfield_metapy/search_eval_pool.py Purpose: Used to find the optimal parameters for ranking algorithms. Not for production use. Source: original version from MP 2.2 and heavily modified to fit use case API: N/A Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to attempt to optimize the parameters for a predefined list of rankers. The optimization takes place using a multi-processed pool so that it can run numerous iterations over a predefined range of values. Each iteration of the optimization loop is evaulated based on normalized cumulative gain at 20 documents. Tensorflow Ranking File: ./competition/tfr_custom/tfr_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/tfr_custom/tfr_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_tfrecord.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: data_format: Data format defined in data.py. train_path: Input file path used for training. eval_path: Input file path used for eval. vocab_path: Vocabulary path for query and document tokens. model_dir: Output directory for models. batch_size: The batch size for train. num_train_steps: Number of steps for train. learning_rate: Learning rate for optimizer. dropout_rate: The dropout rate before output layer. hidden_layer_dims: Sizes for hidden layers. list_size: List size used for training. Use None for dynamic list size. group_size: Group size used in score function. loss: The RankingLossKey for the loss function. weights_feature_name: The name of the feature where unbiased learning-to-rank weights are stored. listwise_inference: If true, exports accept `data_format` while serving. use_document_interactions: If true, uses cross-document interactions to generate scores. embedding_dim: max size of any query or document Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of tfr_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by tfr_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/tfr_custom/tfr_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. BERT File: ./competition/bert/bert_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/bert/bert_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_example.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: local_training: If true, run training locally. train_input_pattern: Input file path pattern used for training. eval_input_pattern: Input file path pattern used for eval. learning_rate: Learning rate for the optimizer. train_batch_size: Number of input records used per batch for training. eval_batch_size: Number of input records used per batch for eval. checkpoint_secs: Saves a model checkpoint every checkpoint_secs seconds. num_checkpoints: Saves at most num_checkpoints checkpoints in workspace. num_train_steps: Number of training iterations. Default means continuous training. num_eval_steps: Number of evaluation iterations. loss: The RankingLossKey deciding the loss function used in training. list_size: List size used for training. convert_labels_to_binary: If true, relevance labels are set to either 0 or 1. model_dir: Output directory for models. dropout_rate: The dropout rate. bert_config_file: The config json file corresponding to the pre-trained BERT model. This specifies the model architecture. Please download the model from the link: https://github.com/google-research/bert bert_init_ckpt: Initial checkpoint from a pre-trained BERT model. Please download from the link: https://github.com/google-research/bert bert_max_seq_length: The maximum input sequence length (#words) after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. bert_num_warmup_steps: This is used for adjust learning rate. If global_step < num_warmup_steps, the learning rate will be `global_step/num_warmup_steps * init_lr`. This is implemented in the bert/optimization.py file. Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of bert_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by bert_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/bert/bert_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. Resources Citations Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf. 2019. TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2970-2978 Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. 2019. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962v2 G. V. Cormack, C. L. A. Clarke, and Stefan Buttcher. 2009. Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods Andrew Trotman, Antti Puurula, Blake Burgess. 2014. Improvements to BM25 and Language Models Examined Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21-25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331303 Iz Beltagy, Kyle Lo, Arman Cohan. 2019. SCIBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620 Installation Guidance tensorflow gpu requirements: https://www.tensorflow.org/install/gpu CUDA: https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal cuDNN: https://developer.nvidia.com/rdp/cudnn-archive install .deb: https://www.quora.com/Is-it-possible-to-install-a-deb-package-in-Windows#:~:text=Potentially%20yes%2C%20as%20long%20as,deb Stopwords https://www.ranks.nl/stopwords https://countwordsfree.com/stopwords BM25 https://github.com/vespa-engine/cord-19/blob/master/cord-19-queries.md https://docs.vespa.ai/documentation/reference/bm25.html Tensorflow Ranking http://cognitiveai.org/2020/09/08/using-tensorflow-ranking-bert-tfr-bert-an-end-to-end-example/ https://github.com/cognitiveailab/ranking https://github.com/tensorflow/ranking https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb Tensorflow Ranking (how to serve a model) https://www.tensorflow.org/tfx/serving/serving_basic https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#serve_your_model_with_tensorflow_serving https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel https://github.com/cognitiveailab/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_client_predict_from_json.py Tensorflow Ranking (how to predict) https://github.com/tensorflow/ranking/issues/48 https://stackoverflow.com/questions/59528975/tf-estimator-predict-slow-with-tensorflow-ranking-module BERT Checkpoint Conversion https://github.com/tensorflow/models/tree/master/official/nlp/bert BERT https://github.com/google-research/bert https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html https://arxiv.org/pdf/1812.00073.pdf SciBERT https://www.aclweb.org/anthology/D19-1371/ https://huggingface.co/gsarti/scibert-nli https://github.com/allenai/scibert/ Docker https://superuser.com/questions/1382472/how-do-i-find-and-enable-the-virtualization-setting-on-windows-10 https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled https://docs.docker.com/docker-for-windows/install/ IR Competition Project Documentation Cody Webster Contents Overview ....................................................................................................................................................... 3 Project Demonstration ................................................................................................................................. 3 Project Results .............................................................................................................................................. 3 DirichletPrior ............................................................................................................................................ 3 JelinekMercer ........................................................................................................................................... 4 OkapiBM25 ............................................................................................................................................... 4 BM25+ ....................................................................................................................................................... 4 BERT .......................................................................................................................................................... 5 SciBERT *difficulty running this model limited options .......................................................................... 6 Tensorflow Ranking custom model ......................................................................................................... 7 Assumptions ................................................................................................................................................. 7 Installation Instructions ............................................................................................................................... 7 What to install if running locally .............................................................................................................. 7 What python packages to install ............................................................................................................. 8 How to Run ................................................................................................................................................... 8 Prepare Data (Required if local) .............................................................................................................. 8 (Method 1) Run BM25+ or another ranker (reproduces best results) ................................................... 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) ............................................................................................................................ 9 (Method 2) Run Tensorflow Ranking ....................................................................................................... 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) ............................................................................................................................ 9 (Method 3) Run a BERT model ................................................................................................................. 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) .......................................................................................................................... 10 Script API Documentation .......................................................................................................................... 10 General ................................................................................................................................................... 10 Metapy (BM25, ...) .................................................................................................................................. 12 Tensorflow Ranking ................................................................................................................................ 13 BERT ........................................................................................................................................................ 16 Resources .................................................................................................................................................... 20 Citations .................................................................................................................................................. 20 Installation Guidance ............................................................................................................................. 20 Stopwords ............................................................................................................................................... 20 BM25 ....................................................................................................................................................... 21 Tensorflow Ranking ................................................................................................................................ 21 Tensorflow Ranking (how to serve a model) ........................................................................................ 21 Tensorflow Ranking (how to predict) .................................................................................................... 21 BERT Checkpoint Conversion ................................................................................................................. 21 BERT ........................................................................................................................................................ 21 SciBERT.................................................................................................................................................... 21 Docker ..................................................................................................................................................... 21 Overview The purpose of this project is to participate in the IR Competition. For this project I developed and extensively tested three main ways to rank documents. The first is ranking documents using a ranking function either packaged with metapy or through a custom definition implemented through metapy. Initially, the best results that I achieved with this method were through a custom implementation of BM25+. Through a longshot attempt on the last day, I actually beat all of my other rankings using metapy's BM25 implementation. The second is ranking documents using a custom model that is trained using the python library tensorflow_ranking. The third option is to use a pretrained model and it's associated vocabulary and then finetune it using tensorflow_ranking. Overall the best results that I have achieved have come from running the OkapiBM25 algorithm implemented with metapy and using parameters that were produced from a brute force optimization. Project Demonstration I have uploaded ""cs410_project_demo.mp4"" into the git repository. I have also uploaded the file to Illinois Media: https://mediaspace.illinois.edu/media/t/1_07py0q5f Project Results Unfortunately, despite my best attempt I was not able to beat the initial baseline performance score as defined by the class professor or TAs. Nobody else was able to beat the initial baseline either so it is likely that baseline was an unreasonable standard for us to achieve with our limited knowledge and experience. In this section I will briefly detail some of the variations and experiments that I attempted. It is not possible for me capture every variation that I tried but I will cover as best as I can. I do not have a consistent record of the score for every attempt so I will not be including those scores. On the last day of the competition, the class administrators lowered the score of the baseline. I decided to give it another shot because I really had nothing to lose. I was able to beat the new baseline using the standard BM25 algorithm implemented via metapy. The only difference between what I did on the last day and what I had done on previous days was that I allowed one of the parameters to vary more than I initially had. I will detail that more within the BM25 section. DirichletPrior This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. Initial attempts with this algorithm were made using the body text only. Later attempts also used the title and abstract/intro. Further experimentation with the KLDivergencePRF implementation in metapy and variations on its parameters was also performed but performance lagged behind other approaches. JelinekMercer This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. This algorithm was only ever attempted with the body text and results were deemed not good enough to justify further experimentation. OkapiBM25 This algorithm was implemented using metapy. The value of the three parameters was optimized on the training dataset using a brute force approach of testing every reasonable combination of parameters and selecting the set that achieved the greatest. Due to the number of variations that I tried with this algorithm, I had to optimize the parameters a number of times and I developed a multi-processing script that was capable of doing this at a much quicker pace. Initially I only attempted to run the algorithm on the body text for each paper but the performance for that was poor. With the body text I attempted to optimize a different NDCG values, 10, 20, and 50 respectively. None of them produced significant improvements in the overall performance. Next, I attempted to use the title, abstract, and introduction along with the body text and that produce marginal improvements in the score. Another boost came once I removed the body text from the dataset and only trained on the title, abstract, and introduction. I also attempted to use metapy's implementation of Rocchio feedback. I optimized the parameters for Rocchio in a similar manner to the normal OkapiBM25 algorithm. Ultimately the Rocchio feedback produced worse results then the standalone ranker so I excluded it from further test with this algorithm. The performance was initially still inadequate so I pursued further methods. One the last day, after I noticed that they had lowered the baseline, I attempted some new variations of this algorithm. I did not run the other algorithms because they all take longer to run and I did not have time to implement anything new. The new variation that I tried on the last day was to let the k3 parameter vary. I had previously not done this due to the faulty assumption that it would only hurt my results because that was the experience that I had during MP2.2. Almost immediately while running the pooled optimization, I was able to see that the performance was superior my previous results. I selected a number of the different parameter combinations to try from the generated set. On the third try I was able to beat the baseline using k1=2.0, b=0.75, and k3=4450. I am immensely frustrated that I have wasted so much effort trying to find other ways to rank documents and the answer was so simple but I am happy to have beaten the baseline. BM25+ This algorithm was implemented with metapy's api using the definition of BM25+ from (Trotman et al, 2014). Since it was implemented in Python and C++ like the native metapy implementations, it ran slower. I optimized it in the same way that I optimized the OkapiBM25 algorithm. This implementation ultimately outperformed the native OkapiBM25 algorithm, but it did not beat the baseline. Better results were obtained when using this in conjunction with the BERT implementation. I tried numerous variations on this algorithm in an attempt to increase my score on the test dataset. Similar to the other algorithms, I initially tried to only use the body text for my analysis. This did not produce the results that I desired. I have briefly listed some of the other variations that I tried: * attempted with body text only * attempted with title, abstract, intro, and body text * attempted with title, abstract, and intro * attempted with title only * attempted with Rocchio pseudo feedback, varying the parameters to Rocchio * attempted to remove all urls in the corpus * attempted to replace all variants of the words coronavirus, covid-19, 2019-ncov, and sars-ncov-2 with coronavirus * attempted to run with query text, question text, or narrative text * attempted to run with all variations of query combined * attempted different variations of metapy analyzer chains * attempted to remove docs with duplicate s2_id but different uids, docs were functionally duplicates * attempted to use a date cutoff for document * attempted to pre-tokenize the queries and documents using BERT's tokenizer * attempted to use multiple datasets with various weightings and combine into one result * used different variations of stopwords that were gathered online (listed in resources) Ultimately, I was not able to beat the baseline using this custom algorithm, so I continued my exploration of other possibilities. BERT I had completed my Technical Review over the BERT model and had learned through that about its superior performance when classifying documents. I chose this as the algorithm to attempt to rank with. It was a significant struggle to get BERT to work for document ranking. There are very few tutorials online for how to achieve this and I spent a significant amount of time trying to get it to work. I ultimately was able to find a tutorial provided by Peter Jansen from the University of Arizona (link in resources). This tutorial gave an example of using the tensorflow_ranking module with BERT to achieve document ranking. Unfortunately, the tutorial relied on Docker to serve a fine-tuned model. I had to do a significant amount of researching about how to get Docker to work with a tensorflow model on Windows. In order to get it to work, I had to upgrade my Windows 10 OS to the Development version so that it would support the latest Windows Subsytem for Linux (WSL). I needed the latest WSL because that was what nVidia required for their latest CUDA drivers for GPUs. I needed the latest CUDA drivers because that was the only way to get Docker to run on my GPU using WSL. I also had to install Bazel for Windows because the training script was meant to be compiled to run. This lengthy setup process is obviously unsuitable for the rapid development that I needed and because it is not realistic for reviewers to reproduce this setup on their own machines. Docker is also not supported in Google Colab so I needed to arrive at a repeatable and easily setup solution for the graders. I was able to remove the need for Docker and the need to serve the model at all by exploring the documentation for tensorflow. This exploration give me insight into how to directly load the model and use it to predict document scores. I implemented this methodology into the scripts and was thus able to create a version that can be easily ran within Google Colab for easy review. Once I had an effective way of running the model, I was able to experiment with various ways to utilized it. I attempted to run three variations of the pre-trained BERT model, all provided by Google Research, BERT-Mini, BERT-Small, and BERT-Base. The BERT-Base model was too large to effectively train without utilizing TPUs. I choose to do my testing with the BERT-Mini model because it allowed me the most flexibility with my training parameters. Some key parameters used are list size and batch size. The size of the model along with those two parameters determine what hardware is required to run the model. If you try to run on inadequate hardware than you will easily run out of memory and be unable to train the model. I attempted a number of variations in ways to score or setup the data. Ultimately, I was not able to beat either baseline when using BERT but I was able to improve upon my initial results with BM25+. I have detailed some of the variations that I tried below. * varied the list size * varied the batch size * updated vocabulary to include most common coronavirus variants * attempted to run with query, question, or narrative * attempted to vary the max token sequence size 256, 512, 1024 * relevance = score of first doc segment * relevance = max score of all doc segments * relevance = mean score of all doc segments * ignore body text and only use title+abstract+intro * varied the number of training steps * used both NCDG approximated loss and softmax loss * re-rank top 1000, 2000, 5000, and 10000 results of BM25+ SciBERT *difficulty running this model limited options This model was initially very promising but the issues with getting it to run ultimately made it insufficient for what I needed. This model was create by Iz Beltagy et al. for scientific research. This model is essentially just a BERT-Base model that was pretrained on a scientific corpus instead of the generic one. The vocabulary consists of more scientific terms as a result and should theoretically be able to perform better at ranking the scientific documents in the CORD-19 corpus. I updated a few of the unused vocabulary items to the command variants of coronavirus and included the drug remdesivir as well. Unfortunately, due to the size of the model I was very limited in the list size and batch size options that I could try. I firmly believe that with a larger list size I would have been able to adequately finetune this model and it would have performed better than the BERT model. My inability to get the model to run on the TPUs available through Google Colab prevented me from realizing this goal. Tensorflow Ranking custom model This model was created by basing it off of the example in the tensorflow ranking repository. I left the structure of the context features and the example features the same. The context features contain the query tokens and the example features contain the document tokens and when training, the relevance judgement of the file. I attempted to change the script to use the argparse module instead of the flags from the absl module but when that was attempted the script stopped producing the results of the training data to stdout. I determine that there are likely scripts deeper within the tensorflow ranking module that are using these flags to determine various facets of the training process, so I restore the flags. The model consists of 3 hidden layers at a size of 64, 32, and 16 respectively. During testing the dropout rate of the model was adjust to 0.65 from the default of 0.80. This adjustment produced better results but further drops in this dropout rate risked overfitting the model to the training data. The batch size was set to 1 and the list size maximized so that it would train on more docs for a single query at once. Based on empirical results that I observed during experimentation, this produced better results than increasing batch size and lowering the list size. The maximum limit for the list_size was 100 depending on the hardware available in Colab and if it exceeded that value then it would run out of memory when training. One other significant variable was the max sequence length to use for each example. I tested with both 512 and 1024. The results did not appear to differ significantly between the two but this variable is important because it defines how much of the document can be captured per example. For this model I only used the title, abstract, and introduction because previous experimentation on other models and rankers showed that the body text was not helpful. Ultimately the results produced with this method were worse the results obtained from the BERT models. Assumptions * If you are running this locally then you are running this code on Windows 10 machine that has ample RAM and a CUDA capable gpu. Every script can run on Linux but I have not generated scripts within this repo to accommodate that. * All file paths are relative to the base directory of the repository Installation Instructions What to install if running locally Acquire the datasets: Files should be downloaded and unzipped here: .\competition\datasets The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing Python Instructions: https://docs.conda.io/projects/conda/en/latest/user-guide/install/ Latest Nvidia Drivers (required to run BERT or Tensorflow Ranking on GPU) Instructions: https://docs.nvidia.com/cuda/wsl-user-guide/index.html Instructions require installation of other software and packages you must follow all of them BERT Model (required to run BERT) Due to the size of the models, they are not directly included in the repository Google BERT Models (recommended): Google offers a variety of different sizes for the models to run. I ran using a BERT Mini but could possibly use a BERT Small as well. Anything larger likely requires a TPU through Google Colab to run. https://github.com/google-research/bert SciBERT Model (not recommended, requires TPU): https://github.com/allenai/scibert/ What python packages to install To run the full capability of all scripts defined in this project it is required that you setup two environments. All but one script can run in the main environment but the BERT checkpoint convertor relies on the dev version of a module that conflicts with requirements of the other modules: Main Python Environment: This is the main environment and should be used when running most of the scripts. You can install the latest stable versions of these modules to run the project. Python Version: 3.7 Pip Packages: tensorflow_ranking metapy pytoml BERT Conversion Environment: This is required because there are conflicts in the dependencies of the modules required to convert a model's checkpoints and the modules used to run it. Python Version: 3.7 Pip Packages: tf-models-nightly How to Run **WARNING: running some of these scripts will require a large amount of RAM if you use the full dataset** * Your python environment should be activated before running any scripts Prepare Data (Required if local) Generate the json file that contains the information for each dataset. This is required in order to run any of the different methodologies. File Path: .\competition\create_bert_data.bat (Method 1) Run BM25+ or another ranker (reproduces best results) Option 1 (Google Colab) Run each cell individually from top to bottom to fully. If you view the file within Github there is a link at the top of the file to open in Google Colab. This is the only Google Colab file that does not require a Google Colab Pro account. The free account should be capable of running this notebook. File Path: .\colab_cranfield_metapy.ipynb Option 2 (run locally) Generate the Cranfield Datasets that will be required to run the ranker File Path: .\competition\cranfield_metapy\cm_create_data.bat Run the ranker of your choice (default arguments already in the file) File Path: .\competition\cranfield_metapy\cm_rank_docs.bat (Method 2) Run Tensorflow Ranking Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. T This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_tfr.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\tfr_custom\tfr_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\tfr_custom\tfr_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\tfr_custom\tfr_predict.bat (Method 3) Run a BERT model Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_bert.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\bert\bert_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\bert\bert_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\bert\bert_predict.bat Script API Documentation **Unless otherwise specified paths are relative to the root of the repository General File: ./competition/check_covid_variants.py Purpose: Check for coronavirus variants in the corpus Source: developed by project team API: variant_file: the file that the coronavirus variants will be output to. File can already exist. Existing variants will be loaded. known_variants: known variants of the coronavirus that exist in the corpus and will be used to determine other variants doc_keys: the keys to use when search the document dictionary for variants run_type: the dataset that will be searched for variants input_dir: the directory that contains the json representation of the datasets to be processed Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. It loads the json for one or both datasets and then processes the text to find variants of some predefined words in the corpus and queries. The text is processed using python multiprocessing module in order to speed up execution of the script. File: ./competition/checkpoint_converter.py Purpose: Converter BERT checkpoint files from Tensorflow v1 to v2+ Source: Tensorflow Model Garden Repository API: bert_config_file: Bert configuration file to define core bert layers. checkpoint_to_convert: Initial checkpoint from a pretrained BERT model core (that is, only the BertModel, with no task heads.) converted_checkpoint_path: Name for the created object-based V2 checkpoint. checkpoint_model_name: The name of the model when saving the checkpoint, i.e., the checkpoint will be saved using: tf.train.Checkpoint(FLAGS.checkpoint_model_name=model). converted_model: Whether to convert the checkpoint to a `BertEncoder` model or a `BertPretrainerV2` model (with mlm but without classification heads). Detailed Description: This script was not written by me but was lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script is meant to convert a BERT module that you downloaded into a version that is capable of being ran using tensorflow version 2.0+. File: ./competition/create_bert_data.py Purpose: Combine and compile the information from the dataset into and easily loadable json for use by other scripts Source: developed by project team API: vocab_file: The file containing the vocabulary to be used when tokenizing a text variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants tokenize: If use, this will store a tokenized representation of the script in the output json. Must be used with `vocab_file` input_dir: The directory that contains all of the source files for the dataset output_dir: The directory where the json representations of the documents will be stored Detailed Description: This is a python script that was tested in python 3.7. The inputs to the script will determine the exact behavior at run time but lets discuss the more complete case of the training dataset. When the training dataset is specified it will start by loading the queries from there original xml format into a python dictionary. If the script is ran with the tokenize option then each of the three variants of the query (query, question, and narrative) will be tokenized according to the vocabulary defined in the specified vocab file. This json is then dumped into a target directory. For the training dataset it will also load the query relevance judgements. A list of all of the documents and their associated uid, title, abstract, publication date, and list of files containing the text representation is pulled from the metadata.csv file. The list of documents to further process is pruned done to the same as the list of relevance judgements. For each document to process the script iterates through the list of files that contains the text representation until it finds a suitable candidate or exhausts all options. The text from the representation file is loaded in the document dictionary object. If the tokenize option is used, the documents will then be tokenized through the use of a multi-processed pool. The resulting output is then written to the disk. Metapy (BM25, ...) File: ./competition/cranfield_metapy/create_cranfield.py Purpose: Use the json representation of the dataset to create a cranfield dataset for use with metapy Source: developed by project team API: variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants query_keys: The keys that will be used when creating the cranfield query file. If multiple keys are specified, text will be combined. doc_keys: The keys that will be used when creating the cranfield document files. Multiple keys can be specified and multiple keys can be combined into a single document. If creating separate datasets, use a ';' to separate keys. If combining keys for a dataset use a ':' to separate keys: cranfield_dir: The directory to use as a base for generating the cranfield data. input_dir: The directory where the json representations of the documents are stored Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. This uses the loaded json representation of the dataset to produce a use specified cranfield dataset for use with metapy. Each cranfield dataset can be uniquely defined by the user to consist of a one-to-one relationship with keys in the document dictionary or it can consist of multiple keys combined. This allows for flexibility in how the expansive each iteration of ranking testing is. File: ./competition/cranfield_metapy/search_eval.py Purpose: Rank the documents in the corpus and create the predictions file Source: original version from MP 2.2 and heavily modified to fit use case API: config_template: The template file that will be used for creating the configs for each run of the ranker run_type: The dataset that will be searched for variants dat_keys: The keys to use that indicate the name of the cranfield dataset(s). This corresponds to the first key used for every section of 'doc_keys' parameter and the 'create_cranfield.py' script. doc_weights: The weights to use when combining the rankings of multiple datasets. ranker: The ranker to use for ranking the documents. Valid rankers can be found in the script. params: The value(s) for the ranker parameters. Multiple values should be separated by `;`. cranfield_dir: The directory that is the base for the cranfield dataset(s) predict_dir: The directory to contain the predictions file. remove_idx: Delete an existing inverted index and create a new one. If no index exists it will not fail Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to specify what ranker to run from a predefined list of rankers. The user can also run rankings over multiple datasets and specify the weight that each ranking will contribute to the final prediction rankings. File: ./competition/cranfield_metapy/search_eval_pool.py Purpose: Used to find the optimal parameters for ranking algorithms. Not for production use. Source: original version from MP 2.2 and heavily modified to fit use case API: N/A Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to attempt to optimize the parameters for a predefined list of rankers. The optimization takes place using a multi-processed pool so that it can run numerous iterations over a predefined range of values. Each iteration of the optimization loop is evaulated based on normalized cumulative gain at 20 documents. Tensorflow Ranking File: ./competition/tfr_custom/tfr_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/tfr_custom/tfr_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_tfrecord.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: data_format: Data format defined in data.py. train_path: Input file path used for training. eval_path: Input file path used for eval. vocab_path: Vocabulary path for query and document tokens. model_dir: Output directory for models. batch_size: The batch size for train. num_train_steps: Number of steps for train. learning_rate: Learning rate for optimizer. dropout_rate: The dropout rate before output layer. hidden_layer_dims: Sizes for hidden layers. list_size: List size used for training. Use None for dynamic list size. group_size: Group size used in score function. loss: The RankingLossKey for the loss function. weights_feature_name: The name of the feature where unbiased learning-to-rank weights are stored. listwise_inference: If true, exports accept `data_format` while serving. use_document_interactions: If true, uses cross-document interactions to generate scores. embedding_dim: max size of any query or document Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of tfr_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by tfr_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/tfr_custom/tfr_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. BERT File: ./competition/bert/bert_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/bert/bert_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_example.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: local_training: If true, run training locally. train_input_pattern: Input file path pattern used for training. eval_input_pattern: Input file path pattern used for eval. learning_rate: Learning rate for the optimizer. train_batch_size: Number of input records used per batch for training. eval_batch_size: Number of input records used per batch for eval. checkpoint_secs: Saves a model checkpoint every checkpoint_secs seconds. num_checkpoints: Saves at most num_checkpoints checkpoints in workspace. num_train_steps: Number of training iterations. Default means continuous training. num_eval_steps: Number of evaluation iterations. loss: The RankingLossKey deciding the loss function used in training. list_size: List size used for training. convert_labels_to_binary: If true, relevance labels are set to either 0 or 1. model_dir: Output directory for models. dropout_rate: The dropout rate. bert_config_file: The config json file corresponding to the pre-trained BERT model. This specifies the model architecture. Please download the model from the link: https://github.com/google-research/bert bert_init_ckpt: Initial checkpoint from a pre-trained BERT model. Please download from the link: https://github.com/google-research/bert bert_max_seq_length: The maximum input sequence length (#words) after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. bert_num_warmup_steps: This is used for adjust learning rate. If global_step < num_warmup_steps, the learning rate will be `global_step/num_warmup_steps * init_lr`. This is implemented in the bert/optimization.py file. Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of bert_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by bert_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/bert/bert_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. Resources Citations Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf. 2019. TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2970-2978 Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. 2019. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962v2 G. V. Cormack, C. L. A. Clarke, and Stefan Buttcher. 2009. Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods Andrew Trotman, Antti Puurula, Blake Burgess. 2014. Improvements to BM25 and Language Models Examined Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21-25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331303 Iz Beltagy, Kyle Lo, Arman Cohan. 2019. SCIBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620 Installation Guidance tensorflow gpu requirements: https://www.tensorflow.org/install/gpu CUDA: https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal cuDNN: https://developer.nvidia.com/rdp/cudnn-archive install .deb: https://www.quora.com/Is-it-possible-to-install-a-deb-package-in-Windows#:~:text=Potentially%20yes%2C%20as%20long%20as,deb Stopwords https://www.ranks.nl/stopwords https://countwordsfree.com/stopwords BM25 https://github.com/vespa-engine/cord-19/blob/master/cord-19-queries.md https://docs.vespa.ai/documentation/reference/bm25.html Tensorflow Ranking http://cognitiveai.org/2020/09/08/using-tensorflow-ranking-bert-tfr-bert-an-end-to-end-example/ https://github.com/cognitiveailab/ranking https://github.com/tensorflow/ranking https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb Tensorflow Ranking (how to serve a model) https://www.tensorflow.org/tfx/serving/serving_basic https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#serve_your_model_with_tensorflow_serving https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel https://github.com/cognitiveailab/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_client_predict_from_json.py Tensorflow Ranking (how to predict) https://github.com/tensorflow/ranking/issues/48 https://stackoverflow.com/questions/59528975/tf-estimator-predict-slow-with-tensorflow-ranking-module BERT Checkpoint Conversion https://github.com/tensorflow/models/tree/master/official/nlp/bert BERT https://github.com/google-research/bert https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html https://arxiv.org/pdf/1812.00073.pdf SciBERT https://www.aclweb.org/anthology/D19-1371/ https://huggingface.co/gsarti/scibert-nli https://github.com/allenai/scibert/ Docker https://superuser.com/questions/1382472/how-do-i-find-and-enable-the-virtualization-setting-on-windows-10 https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled https://docs.docker.com/docker-for-windows/install/ Cody Webster codylw2@illinois.edu Project Proposal Team Members: codylw2 Project: I intend to participate in the Information Retrieval (IR) Competition. I am prepared to learn state of the art retrieval methods such as Reciprocal Rank Fusion or Inverse Square Rank Fusion and how to utilize machine learning through a method of machine-learned ranking. If I choose to utilize a rank fusion method, I will combine different statistical and probabilistic models that are optimized on the given data set. I have little relevant prior experience in IR outside of this course. I intend to program my project using python. Cody Webster codylw2@illinois.edu Project Proposal Team Members: codylw2 Project: I intend to participate in the Information Retrieval (IR) Competition. I am prepared to learn state of the art retrieval methods such as Reciprocal Rank Fusion or Inverse Square Rank Fusion and how to utilize machine learning through a method of machine-learned ranking. If I choose to utilize a rank fusion method, I will combine different statistical and probabilistic models that are optimized on the given data set. I have little relevant prior experience in IR outside of this course. I intend to program my project using python. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/czhu99/CourseProject	"UntitledDecember13,20201Project1.1CS410,TextInformationRetrieval1.1.1UniversityofIllinoisatUrbana-Champaign,Fall2020RaminMelikov,ChrisZhu,FrancisAllobaDue:12/13/2020,11:59PMCST2ReproductionofMiningCausalTopicsinTextDataKim,H.D.,Castellanos,M.,Hsu,M.,Zhai,C.X.,Rietz,T.,&Diermeier,D.(2013).Miningcausaltopicsintextdata:Iterativetopicmodelingwithtimeseriesfeedback.InCIKM2013-Proceedingsofthe22ndACMInternationalConferenceonInformationandKnowledgeMan-agement(pp.885-890).(InternationalConferenceonInformationandKnowledgeManagement,Proceedings).https://doi.org/10.1145/2505515.25056123VideoIntroductiontotheProjecthttps://www.youtube.com/watch?v=2RAoMGm07t8Pleasewatchthevideooverviewoftheprojectfirst.4RepositoryLocationhttps://github.com/czhu99/CourseProject4.0.1ThisisaKNIMEProjectKNIMEisanAdvancedAnalyticsPlatform.Itisavailableforfreeathttp://www.KNIME.comAftertheKNIMEisdownloadedandinstalled,youhavetoimporttheworkflowtoseehowitworks.Theworkflowinintherepository.Workflowisnamedpaper_replication.knwf.15SomecodefromourprojectThecodebelowisusedinKNIMEtogetthearticlesthataretaggedwithBushorGoreanditthendoessomeprocessingthatproducesatablewith2columns:adatecolumnandastringcolumn.Inthestringcolumneachrowrepresentseacharticle.Eachstringislowercased,lematized,filtered,etc.Seecodeforexactsteps.frompandasimportDataFrameimportosfrombs4importBeautifulSoupimportmetapyimportpandasaspdbase_dir='D:/git/text_information_systems/project_files/project/nyt_corpus/data/2000'defextract_data(filename):returnBeautifulSoup(open(filename,encoding='utf8'))deflist_files(dir):return[os.path.join(r,n)forr,_,finos.walk(dir)forninf]blobs=[]forfile_pathinlist_files(base_dir):blobs.append(extract_data(file_path))filtered=[blobforblobinblobsif[person.get_text()forpersoninblob.find_all('person')ifperson.get_text()in['Bush,GeorgeW(Gov)','Gore,Al(VicePres)']2]]tokenized={}date=[]articles=[]forarticleinfiltered:doc=metapy.index.Document()year=article.find('meta',attrs={'name':""publication_year""}).get(""content"")month=article.find('meta',attrs={'name':""publication_month""}).get(""content"")day=article.find('meta',attrs={'name':""publication_day_of_month""}).get(""content"")doc.content(article.body.get_text())tok=metapy.analyzers.ICUTokenizer(suppress_tags=True)tok=metapy.analyzers.LowercaseFilter(tok)tok=metapy.analyzers.ListFilter(tok,""D:/git/text_information_systems/project_files/project/nyt_corpus/data/lemur-stopwords.txt"",metapy.analyzers.ListFilter.Type.Reject)tok=metapy.analyzers.Porter2Filter(tok)tok=metapy.analyzers.LengthFilter(tok,min=2,max=30)tok.set_content(doc.content())articles.append("""".join([tokenfortokenintokifnotany(c.isdigit()orc=='.'forcintoken)]))date.append(str(year)+'-'+str(month)+'-'+str(day))tokenized['date']=datetokenized['articles']=articlesoutput_table=pd.DataFrame.from_dict(tokenized)Thefollowingcodegetsthep-valuefortheGrangerCausalitytestfromstatsmodels.tsa.stattoolsimportgrangercausalitytestsimportpandasaspdgr=grangercausalitytests(input_table[['price','topic_sum']],1,verbose=False)p=gr[1][0]['ssr_ftest'][1]dict={'p':p}output_table=pd.DataFrame(dict,index=[0])6TeamContributionsForourcollaborationprocessduringthisproject,wedidpairprogrammingwithallthreememberspresentonavideocall.RamindidthemajorityofthecodingonhismachinewhileChrisandFrancisviewedthescreenandgaveinputandideas.3 CS 410 Francis Alobba (falobba2) Ramin Melikov (melikov2) Chris Zhu (cjzhu2) Project Progress Report 1) Which tasks have been completed? So far, we have successfully begun mining through the NYT Corpus data and have already gotten the specific data that we need to replicate the paper into our Jupyter Notebook. 2) Which tasks are pending? The next steps we need to complete include implementing the algorithm used in the paper and of course running the experiment from the paper using said algorithm. 3) Are you facing any challenges? No challenges thus far. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrativeWhat are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrativeduties than team members.duties than team members.Members: { 'net_id' : ['melikov2', 'falobba2', 'cjzhu2'], 'name' : ['Ramin Melikov', 'Francis Alobba', 'Chris Zhu']}Captain: {'Chris Zhu'}Which paper have you chosen?Which paper have you chosen?Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series FeedbackWhich programming language do you plan to use?Which programming language do you plan to use?PythonCan you obtain the datasets used in the paper for evaluation?Can you obtain the datasets used in the paper for evaluation?YesIf you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, orIf you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, oranother dataset that is similar in nature)?another dataset that is similar in nature)?N/AIf you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the methodIf you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the methodintroduced in the paper?introduced in the paper?N/A Project CS 410, Text Information Retrieval University of Illinois at Urbana-Champaign, Fall 2020 Ramin Melikov, Chris Zhu, Francis Alloba Due: 12/13/2020, 11:59 PM CST Reproduction of Mining Causal Topics in Text Data Kim, H. D., Castellanos, M., Hsu, M., Zhai, C. X., Rietz, T., & Diermeier, D. (2013). Mining causal topics in text data: Iterative topic modeling with time series feedback. In CIKM 2013 - Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (pp. 885-890). (International Conference on Information and Knowledge Management, Proceedings). https://doi.org/10.1145/2505515.2505612 Video Introduction to the Project https://www.youtube.com/watch?v=2RAoMGm07t8 Please watch the video overview of the project first. Repository Location https://github.com/czhu99/CourseProject This is a KNIME Project KNIME is an Advanced Analytics Platform. It is available for free at http://www.KNIME.com After the KNIME is downloaded and installed, you have to import the workflow to see how it works. The workflow in in the repository. Workflow is named paper_replication.knwf. Some code from our project The code below is used in KNIME to get the articles that are tagged with Bush or Gore and it then does some processing that produces a table with 2 columns: a date column and a string column. In the string column each row represents each article. Each string is lowercased, lematized, filtered, etc. See code for exact steps. ````python from pandas import DataFrame import os from bs4 import BeautifulSoup import metapy import pandas as pd base_dir = 'D:/git/text_information_systems/project_files/project/nyt_corpus/data/2000' def extract_data(filename): return BeautifulSoup(open(filename, encoding = 'utf8')) def list_files(dir): return [os.path.join(r, n) for r, _, f in os.walk(dir) for n in f] blobs = [] for file_path in list_files(base_dir): blobs.append(extract_data(file_path)) filtered = [ blob for blob in blobs if [ person.get_text() for person in blob.find_all('person') if person.get_text() in ['Bush, George W (Gov)', 'Gore, Al (Vice Pres)'] ] ] tokenized = {} date = [] articles = [] for article in filtered: doc = metapy.index.Document() year = article.find('meta', attrs = {'name':""publication_year""}).get(""content"") month = article.find('meta', attrs = {'name':""publication_month""}).get(""content"") day = article.find('meta', attrs = {'name':""publication_day_of_month""}).get(""content"") doc.content(article.body.get_text()) tok = metapy.analyzers.ICUTokenizer(suppress_tags=True) tok = metapy.analyzers.LowercaseFilter(tok) tok = metapy.analyzers.ListFilter(tok, ""D:/git/text_information_systems/project_files/project/nyt_corpus/data/lemur-stopwords.txt"", metapy.analyzers.ListFilter.Type.Reject) tok = metapy.analyzers.Porter2Filter(tok) tok = metapy.analyzers.LengthFilter(tok, min=2, max=30) tok.set_content(doc.content()) articles.append("" "".join([token for token in tok if not any(c.isdigit() or c == '.' for c in token)])) date.append(str(year) + '-' + str(month) + '-' + str(day)) tokenized['date'] = date tokenized['articles'] = articles output_table = pd.DataFrame.from_dict(tokenized) ```` The following code gets the p-value for the Granger Causality test ````python from statsmodels.tsa.stattools import grangercausalitytests import pandas as pd gr = grangercausalitytests(input_table[['price', 'topic_sum']], 1, verbose = False) p = gr[1][0]['ssr_ftest'][1] dict = {'p': p} output_table = pd.DataFrame(dict, index = [0]) ```` Team Contributions For our collaboration process during this project, we did pair programming with all three members present on a video call. Ramin did the majority of the coding on his machine while Chris and Francis viewed the screen and gave input and ideas."
https://github.com/danco14/CourseProject	Progress Report 1) Progress made thus far So far, I have developed two different text classification models. Initially, I created an iPython notebook that is used to run all of the code. Two different models are then trained on the datasets The first model is LSTM, which is an RNN, and the second model is BERT. Both of the architectures are implemented and are running in the iPython notebook. Currently, I am developing both of these models for the competition, and I will choose which one to submit based on which has the better results. Both models have been both pre-trained and trained on the provided training dataset. The model files are imported into the notebook and then run. I am currently using the pytorch data loader to run the datasets. Since I do not have a GPU, the models are being run on google colab. 2) Remaining tasks I still need to pass the baseline accuracies. Each model still has to have its hyperparameters tuned to perform better on the test dataset. The architecture of the models may also have to be modified/developed if the resulting accuracy does not pass the baseline after some grid searching. 3) Any challenges/issues being faced A challenge that I am currently facing is having the models not overfit the training data. Most overfitting must also be changed by reducing the model architecture, and having to change the provided BERT model is tedious. Project Proposal Name: Darren Anco (Captain) netid: danco2 Competition *Text ClassiThcation I am prepared to learn state-of-the-art neural network classiThers for this project. Neural Network ClassiThers and Frameworks Some neural network classiThers that I have heard of are: *AlexNet *VGGNet *GoogLeNet *ResNet *ResNeXt *DenseNet I have used AlexNet before in MPs for other classes. For the rest, I learned about their structures and features from CS 498DL, but I have not used them in any projects. For frameworks, I have used: *PyTorch *Tensorssow I have a decent amount of experience with both of these frameworks. I have used them before in CS 440, CS 498DL, and CS 498AML. I have also worked with these frameworks on various projects outside of class. Programming Language For this project, I plan to use python. Text Classification Competition Project Team: Darren Anco (danco2)
https://github.com/darrenmuliawan/CourseProject	"Project Progress Report Darren Muliawan I have been able to get the Python script to run forever on the Cloud. I am also able to run the ExpertSearch system on my local machine. I am currently still working on implementing the service to monitor the faculty websites and automatically add new faculty members to the dataset. So far, I have not faced any challenges. Project Proposal Darren Muliawan - darrenm2 (captain) For the final project, I choose to work on improving the ExpertSearch System, focusing on automatically crawling faculty webpages. The ExpertSearch's data right now is coming from the MP2 submissions of the previous course offering. I am planning to improve the system by creating a service that keeps on monitoring the faculty homepages from the MP2.1 signup sheets to look for a new faculty members that was added after the MP2 submissions of the previous course offering. To show that my implementation works better, I need to show that there are actually new faculty members that is not included in the ExpertSearch's dataset. This crawler will be a separate system that will update the dataset that ExpertSearch uses. I am going to use Python to complete this project. The main tasks that need to be completed for this project includes, 1. Automatic crawling for the faculty homepages 2. Ability to detect changes of faculty members 3. Updating the dataset that ExpertSearch uses These tasks should be able to be completed in ~20 hours. Project Overview This project is made to improve the existing ExpertSearch system. My goal was to update the dataset used by ExpertSearch by creating a script that checks the URLs in the data/MP2_Part1 Signup - Sheet1.csv if they have new faculty members that wasn't added to the current ExpertSearch's dataset. Implementation In order to find new faculty members, first I get the original email list that the current ExpertSearch used from data/emails. I used faculty member's email to identify unique faculty member, since each person will most likely only have 1 email, and store it in Python dictionary for fast lookup. The next step is to crawl each faculty homepage URL from data/MP2_Part1 Signup - Sheet1.csv, and scrape each page to get the list of faculty member's email addresses. If the email address does not exist in the dictionary key, then I assume that this faculty member was new (added after ExpertSearch was created). For each new faculty member, the crawler will get their bio URL, updates the dataset, and update other files which then will be used to create the index. Algorithm Get the list of faculty homepage URLs from data/MP2_Part1 Signup - Sheet1.csv Scrape each URL and grab the a element that has href mailto: prefix, which is an indicator for an email address. Look for their personal page URL by recursively check its HTML structure. The crawler will use the email_element found in step 1, grab its parent, then iterate its children to find another a element with href that starts with either / or http which, possibly, contains the link for their personal page. Codeblock for recursively checks HTML structure, ``` for a in soup.select(""a[href^=\""mailto:\""]""): email = a[""href""].split(""mailto:"")[1] if email not in email_table: # NEW FACULTY MEMBER FOUND, FIND ITS PAGE LINK element = a.parent while True: a2 = element.find(""a"") # CHECK IF THE LINK IS (MAYBE) A FACULTY MEMBER'S PAGE if a2.has_attr('href'): href = a2['href'].encode('ascii') if a2 != a and (href.startswith(""/"") or href.startswith(""http"")): new_faculty_bios_url.append(href) emails.append(email) new_unis.append(uni) new_depts.append(dept) found += 1 break element = element.parent if element is None: break 4. Scrape the new member's bio page and store it indata/compiled_bios/n.txt, where n is some number. 5. Updatedata/email,data/depts,data/location, anddata/uniswith the new member's information 6. Use the modifiedextraction/extract_names.pyto updatedata/names.txt7. Use the modifiedwrite_file_names.pyto updatedata/compiled_bios/dataset-full-corpus.txtanddata/compiled_bios/metadata.dat8. Rebuild the index withmetapy.index.make_inverted_index(searchconfig)``` Limitations There are some limitations for this improvement due to the limitation of time since I am working on this project solo, such as, When the crawler looks for the faculty member's bio URL, it assumes that the URL is located somewhere in other a element's href. If the link is not in the href, then it won't be able to find the bio URL. It may also find an incorrect URL if the link is not in other element's href. ExpertSearch's script to get the faculty member's name uses Stanford Named Entity Recognizer (NER) Tagger, which doesn't 100% correctly detect the faculty member's name ExpertSearch's script to get the location relies on Google Maps API which may not be free since we are dealing with thousands of faculty members. So for the new faculty members, I set the location to be UNKNOWN, United States There is a mismatch in the number of records for data/urls Need to restart the app everytime the crawler finished to view the new faculty members How to run the code Note: Make sure you have both Python 2.7 and Python3 since some of ExpertSearch script does not work on Python 2.7 and some of them does not work on Python 3. For reference, I am using Python 2.7.16 and Python 3.7.7 Run crawler scripts python crawler.py [max_found] [run_forever] max_found is the maximum number of new faculty members that you want to find before it updates the dataset. run_forever sets to true if you want the crawler to loop forever. For example, python crawler.py 10 true For continuous checking and without limit, either set max_found to be -1 and run_forever to be true or run python crawler.py. Run ExpertSearch app gunicorn server:app -b 127.0.0.1:8095 How to check this project Open the original ExpertSearch http://timan102.cs.illinois.edu/expertsearch// and open the updated ExpertSearch at localhost:8095. Find the name of recently added faculty members by looking at new_bios/trial-n.txt and open the bio URL. The new members should only appear in the updated ExpertSearch. Video Link https://mediaspace.illinois.edu/media/t/1_ybtlfoxk"
https://github.com/davidmg4/CourseProject	"David Gutierrez CS410: Text Information Systems Final Project Proposal Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Individual team Name: David Gutierrez NetID: davidmg4 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Sentiment analysis based on geography in the United States using Twitter. This tweet-based text analysis will break down each geographic region (50 states) into a sentiment analysis based on sub-topics as government (e.g. corruption), weather (e.g. natural disasters), quality of life (e.g. cost of living), pollution (e.g. air quality), lifestyle (e.g. traffic) to help people make informed decisions about where to live without the potential biases and pitfalls present in survey data. I expect that this will line up with publicly available information, but may present some interesting challenges inherent to social media such as the use of irony and sarcasm. I plan to use Twitter's built-in API and library along with the NLTK library to train the classifier and implement the analysis. Once this data is collected, I plan to use numpy's libraries to perform a statistical analysis to measure the significance of each sentiment and create a threshold value above which the positive or negative value will be considered significant on a per-state basis. Then they will be compared against other states to generate an interactive grid with which user will be able to sort by state and sub-topic as well as apply filters to narrow down to only specific states that they care about Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Download Twitter tweet training database/information retrieval via web-scraping: 5 hours Develop Codebase to parse both geography and sub-topic information: 10 hours Analysis of data : 5 hours Presentation in web-based intuitive and interactive data visualization: 5 hours David Gutierrez CS410: Text Information Systems Final Project Proposal Fall 2020 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a.Individual team i.Name: David Gutierrez ii.NetID: davidmg4 2.What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? a.Topic: Sentiment analysis based on geography in the United States using Twitter. This tweet-based text analysis will break down each geographic region (50 states) into a sentiment analysis based on sub-topics as government (e.g. corruption), weather (e.g. natural disasters), quality of life (e.g. cost of living), pollution (e.g. air quality), lifestyle (e.g. traffic) to help people make informed decisions about where to live without the potential biases and pitfalls present in survey data. I expect that this will line up with publicly available information, but may present some interesting challenges inherent to social media such as the use of irony and sarcasm. I plan to use Twitter's built-in API and library along with the NLTK library to train the classifier and implement the analysis. Once this data is collected, I plan to use numpy's libraries to perform a statistical analysis to measure the significance of each sentiment and create a threshold value above which the positive or negative value will be considered significant on a per-state basis. Then they will be compared against other states to generate an interactive grid with which user will be able to sort by state and sub-topic as well as apply filters to narrow down to only specific states that they care about 3.Which programming language do you plan to use? a.Python 4.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a.Download Twitter tweet training database/information retrieval via web-scraping: 5 hours b.Develop Codebase to parse both geography and sub-topic information: 10 hours c.Analysis of data : 5 hours d.Presentation in web-based intuitive and interactive data visualization: 5 hours David Gutierrez CS410: Text Information Systems Fall 2020 November 29, 2020 Course Project Progress Report Individual team (NetID: davidmg4) 1) Progress made thus far: I have successfully applied and been approved for a Twitter developer account. This has enabled me to pull all of the most recent tweets in a search for the previous ten days (instead of the prior year as I had hoped). I have been able to implement code to pull the text data into a CSV file. The resultant data file is then read into a second script that loads it into Pandas data frames and parses the text data using stop-words from NLTK and pulling a sentiment analysis for each tweet using TextBlob. Zero values are dropped and then the remaining sentiment scores are averaged together by state and topic to produce the final values, visualized as percentages. This data is then dropped into an HTML file that features a map of US states as well as a sortable table for each state in addition to a composite score to gauge overall sentiment across the five topics. 2) Remaining tasks: a. Re-run the Twitter scraper to get the full text of tweets (currently truncated at 140 characters) b. Fully implement dataViewer.html to pull data automatically from the CSV file c. Rework the CSS on the same to enable a visual histogram for each topic when hovering over the map for a particular state and color enhancements for numerical values d. Code cleanup and integration into a single workflow 3) Any challenges/issues being faced: a. Time series data limited to prior 7-10 days b. API limited to 15-minute intervals (takes all day to collect five topic-words for all 50 states) c. Spam/bot usage is rampant on Twitter d. Overlap in states/topics (including a gross over-weighting of political data due to residual election data) e. Location data also severely limited and often inaccurate f. Limited data for less populated states g. Some topic words are insufficiently measurable (quality of life) or inherently biased (e.g. pollution) Sentiment Analysis of American States by Topic Natural Language Processing of Tweets from the Last 7 Days Using Python David Gutierrez University of Illinois at Urbana-Champaign CS410: Text Information Systems Overview Individual team (NetID: davidmg4) Goal: Conduct a working sentiment analysis based on ""Quality of Life"" topics for all 50 states using Twitter data to get a nearly-real time view of each state's qualities from real users. Topics chosen: Government Weather Economy Nature Lifestyle GitHub repo: https://github.com/davidmg4/CourseProject Workflow Apply for Twitter developer account Install and implement Tweepy API for scraping Tweet data in Python Write each to a CSV file with state and topic tags Clean tweets to prepare for Natural Language Processing (remove non-text data, stop words, etc.) Conduct analysis using Pandas Dataframe data structures and TextBlob's sentiment analysis algorithm Include mean for each category and average them an 'overall score' for view in summary CSV file as well as an interactive HTML file Resources Tweepy (Twitter API) Pandas (Dataframes) NLTK (Stopwords) Textblob (Sentiment Analysis) JSON/JQuery/Ajax (Web Features) Bootstrap (Web Formatting) Step 1: Setup Step 2: Tweet Scraping Step 3: NLP and Analysis Step 3: NLP and Analysis (cont'd) Results Results Good template for folks who are indecisive about where they might want to move or travel Allows for ranking and filtering of resultant data Novel use of NLP for practical data and a ""finger on the pulse"" of social media users More context information would be useful, but still robust enough for most users Limitations Most limitations of this project are specific to Twitter, subject to further study: Free/educational developer account limited to tweet data from the last 7 to 10 days Tweepy API rate limits cause lengthy delays in scraping time Tweets by and large not geotagged for a specific location so including the state name was next-best-option Sentiments only make sense on a relative scale - subject to fluctuation by type of users in the system as well as data availability limitations (e.g. not many tweets in/about North Dakota) Ideal case is to have a real time stream of Twitter data analysis uploaded to the web with histogram data Conclusion Demonstrates a functional workflow for taking topic and context data to generate non-text data Provides useable data interaction to compare and rank states based on topic sentiment as well as overall score Easily adapted for other topics and/or geographies Potential for even more data visualization and interactivity in the future Lessons Learned Many readily available libraries and packages for NLP in Python Robust tools for reading and writing to CSV files Ability to craft and aggregate HTML code/documents from simple Python scripts Analysis only limited by quality of data (and processing time) Had a lot of fun! CS410 Course Project Documentation File Sentiment Analysis of American States by Topic by David Gutierrez Individual team (NetID: davidmg4) Natural Language Processing of Tweets from the Last 7 Days Using Python CS410: Text Information Systems University of Illinois at Urbana-Champaign Video Link to Walkthrough: https://youtu.be/3uJ9P5MayGI Note: private API keys for Tweepy Twitter API withheld. If you would like to run this code on your own machine, please email me at @illinois.edu"
https://github.com/davidtt2/CourseProject	"David Tran (davidtt2) CS410 Fall2020 Project Final Report and Documentation 1 CS410 Project Final Report: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject Overview of the Function of the Code This project uses a text retrieval method through Python's pandas & Selenium in order to gain information about the top companies in the technology industry. After retrieving that information, the Python file will generate a json file within the Angular project that will be read and displayed in the user interface. From the user interface, the user can search for technology companies by name. The expected results from this code is that whenever the Python file is ran, it will obtain the top technology companies for that year. For example, if this code was run in 2021, it will produce similar results with those top companies without any failure or bugs. Software Implementation (How to Run) 1) Clone project (git clone https://github.com/davidtt2/CourseProject.git) 2) Run ""CS410 Project Data.py"" (keep file structure unchanged) - Requires file to be in same directory as cs410-project - Requires chromedriver.exe in same directory as py file (also requires Chrome) - Different versions can be downloaded at https://chromedriver.chromium.org/downloads - Python file will run Selenium webdriver scripts - After, it will create a companies.json info file at root and in Angular project 3) cd to /cs410-project/src 4) Run the Angular script (ng serve -o) 5) UI will open in browser Documented Usages for this Project This project relies on Python and Angular. Modules that may need to be imported/installed to run: - npm install @angular/cli - ng add @angular/material - pip install pandas - pip install selenium - others Future Goals The current future goals are to add a dropdown for each company that shows their career websites and have useful information to prospective students looking for a place to join. Besides having this dropdown with extra information, I plan to implement a recommendation system based on the companies that the user has searched for. Team Member Contributions Because this was a single member team, I completed all the work on my own. Task Project Hours Research and UI Mockup 5 Hours Topic Mining to Retrieve Data in Python 15 Hours Parsing Useful Information from Data Retrieved 10 Hours Developing the UI in Angular 10 Hours Connecting the Angular UI with the Data 5 Hours Testing the UI and Python 5 Hours Total: 50 Hours David Tran (davidtt2) CS410 Fall2020 Project Final Report and Documentation 2 Current User Interface Video https://youtu.be/mfuLOdaO55Q Please contact me for any comments or questions. CS410 Final Project Progress Report: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject 1) Which tasks have been completed? Task 1: Created a wireframe mockup of the interface and its interactions - Designed mockup for site - Decided which data is relevant Task 2: Basic layout of the user interface in Angular and TypeScript - Basic components set up - Mock data inputted until Python data scrape is finished Task 3: Basic method of data collection in Python - Collected data from relevant websites - Removed html tags 2) Which tasks are pending? Task 1: Clean up the data retrieved from Python data scrape Task 2: Add more information in a drop-down under each company Task 3: Add content-based filtering based on user click to generate recommendation. Task 4: Fix up the user interface 3) Are you facing any challenges? Challenge 1: Figuring out how to transfer data from Python scrape to TypeScript Challenge 2: Formatting the objects in the user interface Screenshot of Current UI: (work in progress) CS410 Final Project Proposal: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. One team member Name NetID Captain David Tan Sang Tran davidtt2 Yes 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Free Topic/Task: I plan to create a website that displays information about top software companies, including technologies used, headquarters information, links to applications, internship opportunities, and other relevant information. (This project would be similar to the ExpertSearch, but I will build this project from scratch). Interesting: This will be interesting because it would be good for Computer Science students to see a compiled list of all relevant companies that they can apply to after graduation. Technologies, Tools, Systems: I plan to use Python with a text mining algorithm to parse links and collect data about all of the top software companies available. Selenium and requests will be used to collect the information, and then Pandas will be used to transfer the data into csv sheets. After the data is collected, TypeScript and Angular will be used to display the data and UI. Datasets will be the internet and various links which would be where the information is collected. Expected Outcome: The expected outcome is a website that houses all the top technology company information for students to view and use for applications. Self-Evaluation: I will evaluate my work based on how well I meet my own expectations. I expect a fully functioning website that will have a nice UI and all the needed information about the various companies. 3. Which programming language do you plan to use? To collect the data, I will use Python, Selenium, Requests, and Pandas. For the website, I will use Angular, TypeScript, ng-charts, and Angular Material library. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Hours Research and UI Mockup 3 Hours Topic Mining to Retrieve Data in Python 10 Hours Parsing Useful Information from Data Retrieved 5 Hours Developing the UI in Angular 10 Hours Connecting the Angular UI with the Data 2 Hours Testing the UI and Python 5 Hours Total: 35 Hours https://www.youtube.com/watch?v=mfuLOdaO55Q Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. CourseProject CS410Fall2020 David Tan Sang Tran (davidtt2) { Individual Team } David Tran Project Proposal David Tran Progress Report David Tran Final Report and Documentation David Tran Project Video How to Run 1) Clone project (git clone https://github.com/davidtt2/CourseProject.git) 2) Run ""CS410 Project Data.py"" (keep file structure unchanged) - Requires file to be in same directory as cs410-project - Requires chromedriver.exe in same directory as py file (also requires Chrome) - Different versions can be downloaded at https://chromedriver.chromium.org/downloads - Python file will run Selenium webdriver scripts - After, it will create a companies.json info file at root and in Angular project 3) cd to /cs410-project/src 4) Run the Angular script (ng serve -o) 5) UI will open in browser Modules that may need to be imported/installed to run: - npm install @angular/cli - ng add @angular/material - pip install pandas - pip install selenium - others Description This project uses a text retrieval method through Python's pandas & Selenium in order to gain information about the top companies in the technology industry. After retrieving that information, the Python file will generate a json file within the Angular project that will be read and displayed in the user interface. From the user interface, the user can search for technology companies by name. Please contact me for any assistance or comments. (davidtt2@illinois.edu)"
https://github.com/difangu/CourseProject	"12/10/2020 CS 410 - Text Information System Difan Gu, Yanyue Wang, Wen Long Team APlus Final Documentation Overview: Oftentimes,usersfoundit'shardtofathomtheinformationthattheybarelylearntbefore.For example,howareweabletoanswerthequestion,""whatisdatascience?"",especiallytoan outsider?Howareweabletoclarifythedefinitionof""datascience""or""computerscience""by borrowingmorebasicorcommonwordsortermstofurtherhelpouruserstounderstand? Ourimplementationofasemanticannotationalgorithmbasedonthepaper,""Generating semanticannotationsforfrequentpatternswithcontextanalysis"",canachievethegoal.The finalgoalistoautomaticallydeciphercertainwords,terms,andevensentencesbyproviding its highly-associated while distinct frequent patterns in semantic text form. Inourcase,tobemorespecific,weusedthealgorithmtosummarizewhatspecialtyeach collegepublishedinmajorcomputerscienceconferences.TheDigitalBibliography&Library Project(DBLP)computersciencebibliographyactsasgoodstudymaterialforourproject.It containsthemetadataofmorethan1.8millionpublicationsinthousandsofjournalsand conferencesproceedingserieswrittenbyover1millionauthors.Itfirststartedtobea bibliographyondatabasesystemsandlogicprogrammingbuthassinceexpandedtoall fields of computer science. Fromthiswell-structureddataset,weselectedthreetopU.S.-baseduniversitiesincluding MassachusettsInstituteofTechnology(MIT),GeorgiaInstituteofTechnology(GT),andthe UniversityofMaryland(UMD)asourusecase.Byimplementingthealgorithm,wecan extractaseriesofwordsortermstodifferentiatetheiracademicfocusbasedonthousands ofpapertitlespublishedthroughouttheyears:somecollegeswillbemoreinclinedtodata analysis,theotherswillmoreconcentrateonwirelesssystems.Intherealworld,the utilizationofautomaticannotationcanalsobeuniversal:userscanusethealgorithmto understandnot-well-definedtextinformationsuchas""NLP"",""MachineLearning""and""Deep Learning"" that is not defined in the dictionary such as Merriam Webster. Step 1: Load, clean raw data, and tokenization AscanbeseeninFigure1,thesoftwarefirstconvertstheXMLfiledownloadedfromDBLP intoastringformatandoutputatable.Everypublicationwassavedasasinglestring.Then alltheelementsinsidetheXMLschemaweresavedasaniteminsideadictionary,withthe keybeingthenameoftheelementandthevaluebeingthecontent.Anotherlisthasbeen createdforeverysinglepublicationasanelement,describingtheaffiliateduniversityaswell asthepublicationtitle.Wethenrankedtheseuniversitiesbytheirnumberofpublications andprintedthetop50.Everytitleofthepublicationwasthentokenizedwordbyword.Each token was assigned an integer for further mining. Figure 1. Workflow of Step 1. Step 2: Selecting sample titles from target schools Step2servesasapre-processingstepforpatternmining.Werankedtop10universities aroundtheworldbasedontheirpublicationandsavedtheirtitlesinalongsinglestring,but onlythosepublicationsfromUSuniversitieswereusedforpatternminingsimplybecause English titles are easier to understand. The universities chosen are MIT, GT, and UMD. Step 3: Pattern Mining Inthisstep,weloadedpublicationtitlesbytheaboveuniversitiesandloadedthefrequent patternsminedbyCloSpan,asoftwarepackageofminingclosedsequentialpatternsina sequence database. Step 4. Feature pattern selection Redundancyremovalwasperformedsothatduplicateitemsareremovedfromthemined frequentpatterns.Theresultingpatternsfromeachuniversitywererankedbasedonmutual information. The integrated patterns were finally converted to words via a built-in dictionary. Technical Details: a)Load, clean raw data, and tokenization -input: -dblp.xml is the source file downloaded from the dblp computer science bibliography Figure 2. DBLP XML File -implementation of the software -script file: dblp_1.ipynb -language: PySpark -Environment: Azure Synapse(Spark) -output -dblp_school.txtisthetextdocumentsthatcontainallschoolnamesthatarein the same order as dblp_title.txt Figure 3. School Names in DBLP -dblp_title.txtisthetextdocumentthatcontainsalltitlesofpaperspublishedby varietiesofcollegesacrosstheworld.Itsharesthesameorderas dblp_school.txt.Mostimportantly,allofthewordshavebeenconvertedinto integers where the mapping can be found in dblp_word.txt Figure 4. Integerized Publication Titles. -dblp_word.txtisthedictionarythathasthemappingbetweenintegersto words. Figure 5. The Word-to-integer Dictionary b)Selecting Sample Titles from Target Schools: -Input: -dblp_school.txtisthetextdocumentsthatcontainallschoolnamesthatarein the same order as dblp_title.txt -dblp_title.txtisthetextdocumentthatcontainsalltitlesofpaperspublishedby varietiesofcollegesacrosstheworld.Itsharesthesameorderas dblp_school.txt.Mostimportantly,allofthewordshavebeenconvertedinto integers where the mapping can be found in dblp_word.txt -dblp_word.txt is the dictionary that has the mapping between integers to words. -Process: -school_pattern_creator.ipynbisthepre-processingstepthatachievesthe goalofselectingthesampleschoolthatuserswanttomakethecomparison. In our case, we select all of the titles published by 3 major U.S colleges. - -Output: Theoutputisaseriesoftitlesgroupedbyeachcollege.Forexample,inthepicturebelow whereyoucanfind0ismappedtoUniversityofSatilde,thereforeoutput0.txtincludesall titlesbelongingtoUniversityofSatilde.However,inordertoimproveinterpretability,we chooseoutput3.txt,output5.txt,andoutput8.txtasoursamples.However,weencourageour users to explore more colleges. -school0.txt -school1.txt -school2.txt -school3.txt -school4.txt -school5.txt -school6.txt -school7.txt -school8.txt -school9.txt c)Pattern Mining: -Input: -school2.txt contains all titles from MIT -school8.txt contains all titles from GT -school9.txt contains all titles from UoM -Process: -CloSpanisaclosedpatternminingalgorithmthatisabletofindaseriesof highlyassociatedpatternsinthesequencedatabase.ItwasproposedbyYan etal.(2003).WeuseCloSpantoextract""mostcommon""wordorterm patterns in each title grouped by each college. -spmf.jarisrequiredtorunCloSpaninthesamedirectory.The followingjavacodeistogeneratetheclosedfrequentpatternsfor eachcollege.Thereisonly1mainparameter,calledsupport.It definespatterncommonality.Userscanadjusttheparametersto explore. -java -jar spmf.jar run CloSpan school3.txt output3.txt 1% -java -jar spmf.jar run CloSpan school5.txt output5.txt 0.5% -java -jar spmf.jar run CloSpan school8.txt output8.txt 3% -Output: -output2.txt is the frequent pattern from University of Maryland -output8.txt is the frequent pattern from MIT -output9.txt is the frequent pattern from Georgia Institute of Technology d)Feature Pattern Selection: -input: -school2.txt -school8.txt -school9.txt -output2.txt -output8.txt -output9.txt i -dblp_word.txt -Process: -RemoveRedundancyistomitigateredundancyissuesinthefrequentpattern. Forexample,inthepictureshownbelow,therearealotofredundantwords inasingletransactionduetothewaywepre-processtheterms.Andwe foundsimplytakinguniqueitemsinthepatternwhilekeepingtheword/term orders will be effective. Forexample,[1,-1,1,1,1,-1,2,-2]willbereducedto[1,-1,2,-2]whereall positiveintegersrepresentauniquewordwhile-1separatestwoitemsetsand -2 imply the end of the sentence. -FeaturePatternSelectionbyMutualInformationistoextractthemostdistinct patternamongalloftheschoolswithhighfrequency.Twoschoolsmighthave sharedthesamefocus,forexample,bothMITandGTmightfocuson ""system""whileMITmightpaymoreattentionto""design""whileGTmightgive moreweightto""architecture"".Therefore,""system""isnotthebestcandidateto definethedifferencebetweenMITandGT,while""design""and""architecture"" are.Wewanttogivemoreweightto""design""and""architecture""howevernot losing the importance of ""system"" as well. -WordConversionisthelaststep.Tillthepreviousstep,weworkedona seriesofnumbersforeachtransactionthatrepresentsauniqueword.Now weconvertbacktowordsbasedonthedictionarysothatweareableto understand the meaning. -Output: -Eachcollegewithitsmostdistinctandrepresentativewordsortermsto demonstratetheiracademicfocusincomputerscienceconferences.Allofthe words/termshavebeenrankeddescendingly.Wewillgetthetop5 terms/words for demonstration. Conclusion: Wemaysenseadifferentfocusforeachschool.Forexample,MITfocusesalotonsystem controlandpower/energy-relatedtopics;GTfocusesmoreonprogrammingandframework; UoMputsalotofattentiononwirelessnetworksaswellasobjectrecognition,whichmight be in the Computer Vision area. *MIT:""systemcontrol"",""power,energy"",""control,analysis"",""power,applications"", ""systems, large"" *GT: ""programming"", ""framework"", ""management"", ""problems"", ""approach"" *UoM:""resource,wireless"",""embedded,systems"",""networks,resource"",""recognition, object"", ""social"" Demo Steps: We've shared voiced video  here 1.Download file annotation from our  Github 2.Open Terminal and cd to the directory annotation 3.python school_pattern_creator.py 4.java -jar spmf.jar run CloSpan ./school_output/school9.txt ./pattern_output/output9.txt 1% 5.java -jar spmf.jar run CloSpan ./school_output/school2.txt ./pattern_output/output2.txt 0.5% 6.java -jar spmf.jar run CloSpan ./school_output/school8.txt ./pattern_output/output8.txt 3% 7.python pattern_decipher.py 8.you can find the final output in ./annotation/outcome Team Responsibilities: *DifanGuisresponsibleforthecorealgorithmdevelopmentincludingmutual information,datapreprocessing,patternminingandfrequentpatternselection,also contributed to the final report, documentation and presentation. *WenLongisresponsiblefordatapreprocessingfromXML,associationmining.He also contributed to the final report and documentation. *YanyueWanghasbrainstormedandresearchedpublicationsconcerningfrequent pattern mining. She also documented the pattern mining process. Reference: KDD'06:Proceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledge discoveryanddataminingAugust2006Pages337-346 https://doi.org/10.1145/1150402.1150441 CloSpan:MiningClosedSequentialPatternsinLargeDatasets,byX.Yan,J.Han,andR. Afshar. Proc. of 2003 SIAM Int. Conf. Data Mining (SDM'03), 2003 11/28/2020 CS 410 - Text Information System Difan Gu, Yanyue Wang, Wen Long Team APlus Progress Report 1) Which tasks have been completed? -Researched pattern mining and generated ideas on how to demonstrate the significance of analyzing frequent patterns with context units. -Understood the main steps and algorithms (hierarchical and one-step microclustering, etc.) mentioned in the paper. -Loaded and cleaned source data from https://dblp.uni-trier.de/xml -Implemented vector space modeling on the DBLP dataset mentioned in the paper, constructed a set of frequent models. -Selected context units as patterns (minimal units that carry semantic information in a dataset). -In summary, we completed about 30% of the paper. 2) Which tasks are pending? -Redundancy removing -Strength weighting for context units -Extracting strongest context indicator -Extracting representative transactions -Extracting semantically similar patterns 3) Are you facing any challenges? -Some parts of algorithms are too complicated to implement within a short period of time. We may need simplification in order to deliver a reasonable outcome. -dataset is large and we may need some time to perform more detailed data cleaning and manipulation -collaboration of the project development Generating Semantic Annotations for Frequent Patterns with Context Analysis Click Here for Voiced Presentation & Demo for Grader Oftentimes, users found it's hard to fathom the information that they barely learnt before. For example, how are we able to answer the question, ""what is data science?"", especially to an outsider? How are we able to clarify the definition of ""data science"" or ""computer science"" by borrowing more basic or common words or terms to further help our users to understand? Our implementation of a semantic annotation algorithm based on the paper, ""Generating semantic annotations for frequent patterns with context analysis"", can achieve the goal. The final goal is to automatically decipher certain words, terms, and even sentences by providing its highly-associated while distinct frequent patterns in semantic text form. In our case, to be more specific, we used the algorithm to summarize what specialty each college published in major computer science conferences. The Digital Bibliography & Library Project (DBLP) computer science bibliography acts as good study material for our project. It contains the metadata of more than 1.8 million publications in thousands of journals and conferences proceeding series written by over 1 million authors. It first started to be a bibliography on database systems and logic programming but has since expanded to all fields of computer science. From this well-structured dataset, we selected three top U.S.-based universities including Massachusetts Institute of Technology (MIT), Georgia Institute of Technology (GT), and the University of Maryland as our user case. By implementing the algorithm, we can extract a series of words or terms to differentiate their academic focus based on thousands of paper titles published throughout the years: some colleges will be more inclined to data analysis, the others will more concentrate on wireless systems. In the real world, the utilization of automatic annotation can also be universal: users can use the algorithm to understand not-well-defined text information such as ""NLP"", ""Machine Learning"" and ""Deep Learning"" that is not defined in the dictionary such as Merriam Webster. Reference: KDD '06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data miningAugust 2006 Pages 337-346https://doi.org/10.1145/1150402.1150441"
https://github.com/dilipis/CourseProject	Project Proposal Document 1. This will be an individual project. Name: Dilip Ravindran NetID: dilipr2 2. Text Classification Competition 3. I am prepared to learn state-of-the-art neural network classifiers. I have heard of deep learning frameworks like TensorFlow and PyTorch. I am also aware of classifiers like LSTM, GRU, BERT etc. However, I do not have any working experience in any of these tools/ frameworks. I hope to use this as an opportunity to get familiar with these frameworks and get some hands-on experience. 4. Python Progress made * Figuring out how BERT can be used for text classification * Setting up Google colab and running some sample text classifications Remaining tasks * Implement the solution * Documentation * Creating presentation Challenges * Figuring out an optimal model * Need to devote a lot of time in the coming days as progress has been slow. Sarcasm detection using BERT This project uses NLP techniques to classify if tweets are sarcastic or not. BERT is used to train the model and arrive at the predictions. How to run the code The executable code resides in the file Sentiment_Analysis_with_BERT.ipynb. This code needs to be directly executed from Google Colab. Click on the button below to open the file in Colab. Once in Colab, the code needs to run on a GPU. From Colab, navigate to Edit> Notebook Settings. Select GPU from the Hardware accelerator dropdown The notebook can be executed by executing all the code blocks in order by clicking on the black 'Play' button at the top of each block. In the end, all the predictions are stored in answer.txt in the output folder in the workspace. A video tutorial is available HERE How the code works This project uses BERT (Bidirectional Encoder Representations from Transformers) which is a state-of-the-art machine learning model used for NLP tasks. BERT is a pre-trained NLP model which can be further trained to solve several text classification problems. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The HuggingFace Transformers library is used to get the BERT model that works with Tensorflow. This is how the code works at the high level Copy the testing and training data from github to the Colab workspace Read the testing and training data from jsonl file and convert them into a csv file Clean the input data by removing URL and USER tags from the tweets Split the training dataset into training and validation. This will be used to train the model. Extract only the required columns for further processing. Create the BERT model and tokenizer Convert the training and validation data into the BERT format using the helper functions defined above Use model.compile to set the optimizer, loss function that BERT will use to train the model Call model.fit to actually train the model based on the training and validation data Make predictions on the test data based on the trained model. Write the resuts to answer.txt in the output folder in the workspace. Dependencies python tensorflow transformers pandas sklearn os urllib jsonlines csv References Sentiment Analysis in 10 Minutes with BERT and TensorFlow by Orhan G. Yalcin FigLang2020-Sarcasm-detection Github
https://github.com/dinghuaminghui/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/dixonliang/CS410CourseProject	"Sentiment Analysis of Soccer Games Natural Language Processing of Twitter in Python using Tweepy, TextBlob, and BM25Okapi Dixon Liang University of Illinois Urbana-Champaign MCS - CS 410 General Information Example Game: Chelsea vs. Leeds United (12/5/2020), Chelsea won 3-1. Ziyech and Koch went off injured Player Ratings Consensus: Entire Chelsea team played well Several players could have been MOTM Entire Leeds team was OK Two Parts Sentiment Analysis using TextBlob Ranking using BM25Okapi Everything can be found on my GitHub (https://github.com/dixonliang) File used: ""12_5_20_ChelseaLeeds_Demo.ipynb"" Empty demo: ""Demo.ipynb"" Open to collaboration  Setting Game Parameters Sentiment Algorithm Parameters Number of tweets to be retrieved for each player (limited by API) Threshold for subjectivity Date range Sentiment Algorithm: Part 1 Sentiment Algorithm: Part 2 Sentiment Results Chelsea Example Tweets Kante (CM): [['RT @StatmanDave: N'Golo Kante vs. Leeds United [Chelsea rank]:\n\n12 ball recoveries [1st]\n3 tackles won [=1st]\n2 interceptions [=1st]\n\nDoing...', -0.8] ['RT @marshyleeds: Kante was the best player on the field and made the difference for Chelsea. Has that change of pace in midfield we sometim...', 1.0]] The negative sentiment tweet does not appear to be accurate... We can use BM25Okapi to further investigate... Mendy (GK): [@goal Our starting 11 have all scored\nRemains Mendy\nHold on a second, he is keeping goals from entering, win win for Chelsea', 0.4], ['The presence of Mendy and Thiago have given Chelsea a huge impact at defensive line https://t.co/CD93QjlYLc', 0.4]"" Did not have any negative sentiment tweets Leeds Example Tweets Philips (CM): [Kevin Phillips deserves to play for a top 5 team, the guy is so good. He literally runs this leeds team.', 0.6] Some negative tweets, but not particularly focused on him Likely best player on the field for Leeds based on this - seemed to be agreed by other match ratings Cooper (CB): [['@leeds_lord Cooper was terrible. Hideously exposed.', -1.0], ['@kennybrown1964 @maz7555 Llorente was class. It's cooper who is and always has been the weak link. Hopefully that's... https://t.co/68jwJ2rX0W', -0.375] [""@Laurencewegner I thought he was good Cooper did well, yeah but for me have to credit Chelsea's movement in the box... https://t.co/JjWhYFbfGW"", 0.7] Some very negative tweets, some positive tweets Implies wasn't great, but wasn't terrible as some suggest BM25Okapi for Context Chelsea BM25Okapi Results Kante matching most of the positive terms Giroud, Werner, Chilwell all had decent games as well Werner seems to have had a great game taking the two into consideration Ziyech injury, Chilwell foul (VAR) Leeds BM25Okapi Results For most part matches the sentiment rating Bamford was the goal scorer for Leeds Phillips confirmed best player for Leeds based on the two Koch injury Dallas matches the most ""bad terms"", was one of the lowest ranked players in sentiment Conclusion and Improvements Provides accurate idea of what happened in each game BM25Okapi is flexible, will provided needed context in the cases where sentiment analysis might not make sense Can easily be adapted to other sports Can be extended for multiple games or even an entire season Algorithm likely needs to be improved Efficiency poor and API limitations Best to be used with statistical analysis Open to work with others!  11/30/20 Which tasks have been completed? Pretty much all the source code has all been finished. The remaining coding work will be if I want to make any additional improvements. I have also implemented all the code into a Jupyter Notebook which I will use for a demo. Most of the tutorial and description in that has been finished. I have broken the code down into two parts: basic sentiment analysis and implementation of BM25Okapi to give some more context of the contents of score. The first part of the code uses Tweepy to source tweets and then Textblob's Sentiment Analysis to classify. Every starting player from each team will receive an average sentiment score based on the classification of these tweets. There is also code that outputs the results visually. The second part of the code uses BM25Okapi ranking to find specific tweets that might have contributed to the sentiment analysis. These findings are also visualized. Which tasks are pending? The two main tasks that are pending are the demo run in video and documentation. If I have some more time, perhaps more can be done to improve the overall implementation / clean up of the code for efficiency in the future. I am currently working through the documentation on my GitHub page which is part of the README. The documentation will provide further detail as to what exactly each part of the code does as well as my thought process. I have also chosen to use a Jupyter Notebook to provide a step by step on how to run through my code. I have written most of the instructions and tutorial in that but will likely refine it some more. Given the limitations of the Twitter API access (more below), I will likely wait to run the demo along with the video closer to the due date. I will likely provide one notebook with the results of my run through so others can follow along with the results and then a notebook with a blank implementation for free use. Are you facing any challenges? There are a few challenges that either fall into the category of text retrieval regarding the API or further improvement of the algorithm. The Twitter API access is an issue as the free version only retrieves a limited number of tweets over only the past 7 days. This makes it a bit difficult to set a demo of the code, but I will likely just have to set a game that I want to use for the demo run in the coming week to not cut it too close to the deadline. The other challenge is that Tweepy is slow in retrieving many tweets, especially given the Twitter API limitations of how many times I can call it. For speed purposes, I will likely have to set the number of tweets per player at 100 for my demo. As for the algorithm, there are some areas that could be improved beyond the basic implementation. Teams and players can go by a handful of names so it would be useful to be able to count all occasions. However, for the most part, there is usually a more popular name that each team or player is referred to. As for most NLP, more work can be done in finding deeper meaning in words especially when the tweets are more contextually complex. If multiple players are mentioned in a tweet or previous references are made, it can be difficult to classify if a tweet is positive or negative for a particular player. 11/30/20 Which tasks have been completed? Pretty much all the source code has all been finished. The remaining coding work will be if I want to make any additional improvements. I have also implemented all the code into a Jupyter Notebook which I will use for a demo. Most of the tutorial and description in that has been finished. I have broken the code down into two parts: basic sentiment analysis and implementation of BM25Okapi to give some more context of the contents of score. The first part of the code uses Tweepy to source tweets and then Textblob's Sentiment Analysis to classify. Every starting player from each team will receive an average sentiment score based on the classification of these tweets. There is also code that outputs the results visually. The second part of the code uses BM25Okapi ranking to find specific tweets that might have contributed to the sentiment analysis. These findings are also visualized. Which tasks are pending? The two main tasks that are pending are the demo run in video and documentation. If I have some more time, perhaps more can be done to improve the overall implementation / clean up of the code for efficiency in the future. I am currently working through the documentation on my GitHub page which is part of the README. The documentation will provide further detail as to what exactly each part of the code does as well as my thought process. I have also chosen to use a Jupyter Notebook to provide a step by step on how to run through my code. I have written most of the instructions and tutorial in that but will likely refine it some more. Given the limitations of the Twitter API access (more below), I will likely wait to run the demo along with the video closer to the due date. I will likely provide one notebook with the results of my run through so others can follow along with the results and then a notebook with a blank implementation for free use. Are you facing any challenges? There are a few challenges that either fall into the category of text retrieval regarding the API or further improvement of the algorithm. The Twitter API access is an issue as the free version only retrieves a limited number of tweets over only the past 7 days. This makes it a bit difficult to set a demo of the code, but I will likely just have to set a game that I want to use for the demo run in the coming week to not cut it too close to the deadline. The other challenge is that Tweepy is slow in retrieving many tweets, especially given the Twitter API limitations of how many times I can call it. For speed purposes, I will likely have to set the number of tweets per player at 100 for my demo. As for the algorithm, there are some areas that could be improved beyond the basic implementation. Teams and players can go by a handful of names so it would be useful to be able to count all occasions. However, for the most part, there is usually a more popular name that each team or player is referred to. As for most NLP, more work can be done in finding deeper meaning in words especially when the tweets are more contextually complex. If multiple players are mentioned in a tweet or previous references are made, it can be difficult to classify if a tweet is positive or negative for a particular player. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Dixon Liang dixonl2@illinois.edu What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free topic is to create a sentiment analysis of soccer games from the English Premier League using Twitter. Specifically, I am interested in discovering which players from either team had a good or poor game based on sentiment of tweets. This is important or interesting because based on this analysis, we can use it to come up with a detailed ""form"" analysis to see which players have been playing well over an extended period. Although specific individuals from Twitter might not be the best pundits of games, I will be using a ""wisdom of the crowd"" type approach on the quality of the data gathered. The main task I will be doing is taking tweets from a sample game in the past and categorizing the words in tweets related to certain players for a positive or negative sentiment. After this categorization, I will be able to aggregate and determine which players had a good or poor game. My planned approach is to use a particular game in the past few weeks as a demo. I would text mine all the tweets related to the game using a filter of the time period around / during the game and then those using a hashtag related to the game. I would then further analyze the tweets that have mentions of specific players and the words in context. Based on the context of the tweets, I would categorize the tweets related to players as either ""positive', ""neutral"", or ""negative"". Totaling the sentiments for each player during the game should give me a classification for each individual player determining their performance. I will likely to be able to further quantify based on some measure on how many ""positive"" or ""negative"" tweets each player has been categorized. The main tool I will be using is ""Tweepy"" which is a Python package to read tweets from the Twitter API. The main dataset will be the tweets from the time period around the games that I have chosen, and that I have categorized as relevant. If I have the time, I would also like to incorporate one of the functions from the course into my project. I will have a better idea through the planning process, but as of now, I would likely treat each tweet as a ""document"". The most likely adaptation will be to create a likelihood model using the game tweets as the primary data set. An interesting application would be to try to categorize tweets relating to players to specific parts of the game which would be the ""topics"". My expected outcome is to produce a report detailing the findings from one or several games. I should have enough data per game to show all the players' performances who were involved in the game. In this report, I will show which players were categorized as having good games or poor games based on the categorizing of tweets. In a further breakdown, by using a likelihood model, I will be also be able to show the topics where a player performed well or poor. As an example, a positive tweet might be relating to a specific player's passes during the game. I will evaluate my work based on reviewing some ""match ratings"" by pundit type publications to see if my reviews based on sentiment are in line. Although I anticipate some differences, if this works successfully, there should not be big differences in the way each player performance is viewed against the experts. I will also try to watch the games that I use to review myself if my ratings make basic sense. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I will be working alone on this project. I anticipate the breakdown of time spent on the project as follows: Initial Research and Outline (2-4 hours) Familiarity of Tweepy and Other Tools (2-4 hours) Text Retrieval and Data Cleaning (2-4 hours) Initial Implementation of Algorithm (10-20 hours) Testing and Improvements (10-20 hours) Final Reports, Documentation, and Demo (5-10 hours) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Dixon Liang o dixonl2@illinois.edu 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free topic is to create a sentiment analysis of soccer games from the English Premier League using Twitter. Specifically, I am interested in discovering which players from either team had a good or poor game based on sentiment of tweets. This is important or interesting because based on this analysis, we can use it to come up with a detailed ""form"" analysis to see which players have been playing well over an extended period. Although specific individuals from Twitter might not be the best pundits of games, I will be using a ""wisdom of the crowd"" type approach on the quality of the data gathered. The main task I will be doing is taking tweets from a sample game in the past and categorizing the words in tweets related to certain players for a positive or negative sentiment. After this categorization, I will be able to aggregate and determine which players had a good or poor game. My planned approach is to use a particular game in the past few weeks as a demo. I would text mine all the tweets related to the game using a filter of the time period around / during the game and then those using a hashtag related to the game. I would then further analyze the tweets that have mentions of specific players and the words in context. Based on the context of the tweets, I would categorize the tweets related to players as either ""positive', ""neutral"", or ""negative"". Totaling the sentiments for each player during the game should give me a classification for each individual player determining their performance. I will likely to be able to further quantify based on some measure on how many ""positive"" or ""negative"" tweets each player has been categorized. The main tool I will be using is ""Tweepy"" which is a Python package to read tweets from the Twitter API. The main dataset will be the tweets from the time period around the games that I have chosen, and that I have categorized as relevant. If I have the time, I would also like to incorporate one of the functions from the course into my project. I will have a better idea through the planning process, but as of now, I would likely treat each tweet as a ""document"". The most likely adaptation will be to create a likelihood model using the game tweets as the primary data set. An interesting application would be to try to categorize tweets relating to players to specific parts of the game which would be the ""topics"". My expected outcome is to produce a report detailing the findings from one or several games. I should have enough data per game to show all the players' performances who were involved in the game. In this report, I will show which players were categorized as having good games or poor games based on the categorizing of tweets. In a further breakdown, by using a likelihood model, I will be also be able to show the topics where a player performed well or poor. As an example, a positive tweet might be relating to a specific player's passes during the game. I will evaluate my work based on reviewing some ""match ratings"" by pundit type publications to see if my reviews based on sentiment are in line. Although I anticipate some differences, if this works successfully, there should not be big differences in the way each player performance is viewed against the experts. I will also try to watch the games that I use to review myself if my ratings make basic sense. 3. Which programming language do you plan to use? * Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I will be working alone on this project. I anticipate the breakdown of time spent on the project as follows: i. Initial Research and Outline (2-4 hours) ii. Familiarity of Tweepy and Other Tools (2-4 hours) iii. Text Retrieval and Data Cleaning (2-4 hours) iv. Initial Implementation of Algorithm (10-20 hours) v. Testing and Improvements (10-20 hours) vi. Final Reports, Documentation, and Demo (5-10 hours) Sentiment Analysis for Soccer Games (CS 410 Course Project) and Documentation NOTE: You will need to provide your own Twitter API keys if you want to run the blank demo file. I have provided a notebook with already run code in file ""12_5_20_ChelseaLeeds_Demo.ipynb"" which goes along with the demo materials (video and presentation) for grading purposes to avoid having to go through the process. Introduction This is the repo for my course project for CS 410 Text Information Systems for my Masters in Computer Science at University of Illinois Urbana-Champaign. The main idea of this repo is to provide the code, documentaion, and demo for a basic sentiment analysis for soccer games using Python, Tweepy, TextBlob, and BM25Okapi. The easiest way to use this code is to use the Jupyter Notebook demo in this repo. A video tutorial on Youtube is also provided. The source code is available as well. Please feel free to reach out to me if you would like to collaborate :) . Files YouTube Demo Link: https://www.youtube.com/watch?v=UuY7dO8bq0M&ab_channel=DixonLiang ChelseaLeeds_Presentation.pptx - The powerpoint presentation used in the video tutorial 12_5_20_ChelseaLeeds_Demo.ipynb - Demo file that goes along with the presentation and video link Project Proposal.pdf - The initial project proposal. Progress Report.pdf - Progress Report as of 11/30/20. maincode.py - The main source code demo.ipynb - Empty demo code in Jupyter Notebook for free use Tweepy Review.docx - Review done on the Tweepy package for the course team1_sentiment.png - Example sentiment bar chart for Team 1 (new file will be saved down if main code is run) team1_BM25positive.png - Example positive BM25 average ranking for Team 1 (new file will be saved down if main code is run) team1_BM25negative.png - Example negative BM25 average ranking for Team 1 (new file will be saved down if main code is run) team2_sentiment.png - Example sentiment bar chart for Team 2 (new file will be saved down if main code is run) team2_BM25positive.png - Example positive BM25 average ranking for Team 2 (new file will be saved down if main code is run) team2_BM25negative.png - Example negative BM25 average ranking for Team 2 (new file will be saved down if main code is run) Background We will be using Tweepy to source tweets from the Twitter API and TextBlob to provide a framework for natural language processing to provide sentiment analysis. In addition, we will use PyPi's implementation of BM25Okapi to provide context of the sentiment analysis. Ideally, the result of this code will show the relative sentiment of a player's performance during a recent game. By using wisdom of the crowds, we hope to gain an idea of how the player performed. Using BM25Okapi, we will also be able to use relevant terms to see what might have caused sentiment to go way or another (ex. player scored a goal or provided an assist, etc.) Using PyPlot, we will also be able to visualize the results. Technically, this code can be used for any soccer game, but given the popularity and language barrier, EPL games are likely to provide the most meaningful results. Adjustments could be made for La Liga or Serie A using Spanish or Italian NLP. Please feel free to reach out as I welcome any collaboration as the code can be improved and applied to different sports or different applications all together :) . A run through of the source code is provided below. Code Documentation Introduction Packages Needed: To begin, we need several packages installed and imported. These are: Tweepy, TextBlob, Numpy, Rank_BM25, and Matplotlib.pyplot. Documentation and links are found here: http://docs.tweepy.org/en/latest/api.html https://textblob.readthedocs.io/en/dev/api_reference.html https://numpy.org/doc/ https://pypi.org/project/rank-bm25/ https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.html Most importantly, we will need access to the Twitter API, which can be gained by having a Twitter profile. You will be provided four keys of strings of letters and numbers which you will need to enter in the box below: consumer key, consumer secret, access token, access token secret. These will be used in the below code area. ```shell consumer_key = """" consumer_secret = """" access_token = """" access_token_secret = """" auth = tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) api = tweepy.API(auth,wait_on_rate_limit=True) ``` Game Parameters We will need to set the parameters for the game we are interested in; this includes the two teams names and the starting 11 for each team. ```shell team1 = """" team2 = """" team1 team1_Player1 = """" team1_Player2 = """" team1_Player3 = """" team1_Player4 = """" team1_Player5 = """" team1_Player6 = """" team1_Player7 = """" team1_Player8 = """" team1_Player9 = """" team1_Player10 = """" team1_Player11 = """" team2 team2_Player1 = """" team2_Player2 = """" team2_Player3 = """" team2_Player4 = """" team2_Player5 = """" team2_Player6 = """" team2_Player7 = """" team2_Player8 = """" team2_Player9 = """" team2_Player10 = """" team2_Player11 = """" ``` After setting the game parameters, there are a few algorithm paramters we will need to set. To begin, the number of tweets that we want to retrieve is set as a parameter for the algorithm. This may also affect how quickly the algorithm runs because of limitations in the package and the free version of the Twitter API. The threshold for objectivity/subjectivity is also set. 0 is defined as purely objective and 1 is defined as subjective. Ideally for the most results, we want a low threshold, 0.10 has been suggested, but any threshold can be set. The date periods for when we want to retrieve tweets from is also se; for best results, it is suggested to only use the day of the game and the day after the game. The free version of the Twitter API limits searches to within the 7 days. Sentiment Analysis: ```shell define the number of tweets we want to sort for and subjective threshold number_of_tweets = 100 # how many tweets we want to search for threshold = 0.10 # threshold for subjectivity [0,1] setting date range, ideally run day after the game date_since = ""2020-11-21"" date_until = ""2020-11-22"" ``` For the BM25Okapi algorithm, there are just two sets of parameters we must set. The first is the set positive terms we want to use for context. Some suggestions are in the default query already. Similarily, for the second set of parameters, it is a set of negative terms. BM25Okapi: shell positive_terms = ""assist good excellent great"" # search queries, positive terms negative_terms = ""poor bad miss own awful"" # negative terms The BM25Okapi portion of the code will combine all of the tweets for every player together which will they treat each tweet as a document as part of a corpus. Then using the positive array, it will then go through each document ranking it based on how many of the positive terms each tweet matches. The higher ranked the tweet is, the more relevant it is to that query. Given we are using positive terms, the idea is that the tweet is more reflective of positive results in relation to those terms during the game for the respective player. The same will be done with the negative query. Once the rankings are done, each players average ranking for each query is provided, similarily to the sentiment array above. These two arrays will then be used for charting. BM25 incorporates search ranking concepts such as IDF (inverse document frequency), which is a filter for commonly used terms as well as TF (term frequency), which gives higher ranking for more matching of terms. A brief summary of how exactly the formula ranks can be found here: https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html Running the Code After setting the above parameters, the entire ""maincode.py"" can be run which will then output the relevant visualizations for this task. The code will retrieve the set number of tweets for each player and then use TextBlob's sentiment analysis tool to rate the sentiment of each tweet. If the tweet crosses the set threshold, the senitment for that tweet will be used for an average of all of the sentiment for that respective player. This array of sentiments of players will then be used for our graphs below. The functions that are used for the generation of these visualizations are listed below. Visual Output Functions plot_bar_team1_sentiment(): Using pyplot, this function will chart Team 1's senitment by player in the form of a horizontal bar chart. The function will take the sentiment array as mentioned above and plot the respective average for each player. If the sentiment is more towards the right, the player's sentiment for that game will be more positive. If the senitment is more towards the left, the player's sentiment for that game will be more negative. plot_bar_team2_sentiment(): Same as the above but with the players for Team 2. plot_bar_team1_BM25positive(): Using pyplot again, this function will chart Team 1's BM25Okapi rankings in the form of a horizontal bar chart. plot_bar_team2_BM25positive(): Same as above but for Team 2. plot_bar_team1_BM25negative(): Same as above but for the negative query and Team 1. plot_bar_team2_BM25negative(): Same as above but for the negative query and Team 2. Text Output Functions display_tweets(team, player_number): This function will take in two arguments, the team name and the player number (which can be referenced above on the parameters). The function will then display the ten highest and ten lowest sentiment tweets for that player. shell ['RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'RT @goal: Thiago Silva \nHavertz \nPulisic \n\nRudiger \nChilwell \nWerner \n\nChelsea reveal their team to play Newcastle \n\n#NEWCHE https:/...', '@ChelseaFC Chelsea had a clean with Rudiger and Zouma playing together.  We are winning this league', 'RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'NEWCASTLE 0-2 CHELSEA: GODFREY  ""The only player wey dun improve Chelsea na Mendy, ZOUMA AND RUDIGER STILL NO GET... https://t.co/TYZOd3mZ9X', 'RT @kingmali_: @ChelseaFC MOTM kante\nLovely clean sheet Mendy\nWell done Tammy\nRudiger is not fit to be a Chelsea player PERIOD!\nEmerson is...', 'RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'RT @AbsoluteChelsea: Frank Lampard says Antonio Rudiger was brilliant on his first Premier League start of the season for #Chelsea against...', ""Are you more confident about Chelsea's defensive options and depth than at the start of the season?\n\nhttps://t.co/enuSsURsmJ""] rank_top(corpus,terms): This function is in relation to the BM25Okapi rankings. It takes in two arguments, a corpus (in this case, will be a series of tweets) and then a search query (in this case, positive or negative term array). This function will display the top ten ranked tweets in the corpus given the query. An example would be if we wanted to see the top ranked tweets for a specific player. shell ['@Chelsea_Era @EBL2017 Werner was playing bumdesliga, I don't doubt he's got a good scoring record in that league. H... https://t.co/veBdvRxnxQ', ""https://t.co/cTxtOa9fGf\nMendy &amp; Chilwell both had their 'worst' game in a Chelsea shirt today, and were still excel... https://t.co/gmjF62mTX3"", '@AlexGoldberg_ Kovacic done ok today but gives the ball away too much in dangerous areas, against a better team Che... https://t.co/d0y3hpDjgX', '@afcjxmes Kovacic was Chelsea's worst midfielder today, gave the ball away in dangerous areas too many times, Kante... https://t.co/rPLqctpkjc', ""Timo Werner is 'undroppable'.\n\nN'Golo Kante is back doing what he does best.\n\nFrank Lampard is about to settle on a... https://t.co/oXxsMrxKh7"", ""Timo Werner is 'undroppable'.\n\nN'Golo Kante is back doing what he does best.\n\nFrank Lampard is about to settle on a... https://t.co/oXxsMrxKh7"", 'RT @Football__Tweet: Edouard Mendy has kept 7 clean sheets in his first 9 Chelsea games.\n\nTalk about an upgrade on the most expensive goalk...', '@tessderry1 Ths international break suckssss...timo chilwell mount grealish theirlegs lookd tired.....\n\nNext match... https://t.co/L57jzK0DyO', 'Frank Lampard expressed his delight as Chelsea kept another clean sheet in their 0-2 win against Newcastle at St Ja... https://t.co/d3HMLpYWoX', 'Saturdays added assist:\nMount (Chelsea v Newcastle) pass leading to own goal. https://t.co/xtIdUJXHLQ'] Helper Functions sentiment_element(element): This is a simple function that will be used for Python's sort implementation for an array. In this case, we are interested in sorting be the second element (for each entry in the serntiment array is the sentiment score) which is what this function does. rank_scores(corpus,terms): This function is in relation to the BM25Okapi rankings. It takes in two arguments, a corpus (in this case, will be a series of tweets) and then a search query (in this case, positive or negative term array). It is the function that will actually use PyPi's implementation of BM25Okapi to give each tweet a rank in relation the entire corpus. Before passing into the implementation, both the corpus and term query will be tokenized. Intro Tweepy is a Python library to access the Twitter API. The Tweepy library provides functions to easily retrieve text from Twitter in addition to other useful methods to interact with the application. Given the extensive use of social media and social media's increasing relevance in today's world for all kinds of data, Tweepy can be used in instrumental ways for text retrieval for all kinds of analysis. Getting Started and API Authentication is done through your own Twitter profile using tokens. Authentication is easy as it uses OAuth 1a with the tokens simply given to you in your account and then can be applied directly with the following two lines of code. auth = tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) This is straight forward as a user just needs a Twitter account and ""apply"" for a developer account which just consists of some basic questions as to what you want to do with the API. Tweepy provides several interesting features including being able to interact through a user profile such as posting tweets, retweeting, interacting with other users, etc. The most basic feature seems to be interacting with your own timeline. The first example that the documentation uses is to print the first 20 tweets from your home timeline. However, given the context of this class, we are most interested in the ""search"" features. Search Features An important note to keep in mind is that the search methods of Tweepy are limited based on what type of API account you have - ranging from a free account to enterprise / premium account. For a full historical search, an enterprise / premium account is needed which allows a user to search through the entirety of the history of Twitter from 2006. Unfortunately, in a free search, not all tweets will be made available - however, for most general purposes in analyzing trends or sentiment, this should be good enough. The search function takes in a total of ten arguments which the user can declare. API.search(q[, geocode][, lang][, locale][, result_type][, count][, until][, since_id][, max_id][, include_entities]) The query can be up 500 characters maximum. For example, we can use an array of search terms if interested in several queries. The rest of the arguments act as filters. We can filter for things such as location or language or a time interval. Unless using a premium account, tweet history is limited to just the past seven days. Upon completing a search, Tweepy returns what is called a SearchResults object. The best way to interact with this object is through calling the search function in Tweepy's pagination method. Pagination Tweepy also provides an easy to iterate through different objects in Twitter using something called a Cursor object. for status in tweepy.Cursor(api.user_timeline).items(): By using a cursor object, we can treat each tweet as an ""item"" which allows us to specify exactly how many tweets we want to use for analysis per search. This makes iterating a lot easier. This feature can also be used for other features of Tweepy that requires iteration. For example if we wanted the last 100 tweets about 'cars"", we would query ""cars"" in the search method and enter ""100"" in the items() field. Additional Features Streaming may be of interest for users that want to interact or analyze in real time. Everything real time related is done through StreamListenter. Through this instance, we can then filter out exactly which tweets we might be interested; we can filter for certain users or certain keywords. myStreamListener = MyStreamListener() myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener) For those who want to interact with Twitter using ""bot"" like control, Tweepy also provides many methods that are useful. We can interact with our own profile without ever using the Twitter site from posting our own updates to adding followers. There are also methods that allow us to interact with our direct messages. Conclusion Tweepy provides a very easy to use package for those interested in mining tweets from Twitter. With regards to text retrieval, Tweepy makes it quite easy especially with the cursor method which helps with pagination. If combined with another type of text package such as TextBlob, there are a lot of possibilities in what we can do such as conducting sentiment analysis of events or subjects."
https://github.com/duadua9/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/dvillevald/CourseProject	Using MD&A of company's SEC filing to predict future stock price CS410 Project Progress Report by Dmitry Villevald ( dmitryv2@illinois.edu ) Project Progress Report: Q: Which tasks have been completed? A: The following tasks have been completed: *I completed the code which pulls the index files from the SEC EDGAR database for each quarter and extracts Q-10 and K-10 filings for the selected companies. *From every filing I built a dictionary including all words references in the document with their frequencies (i.e. bag of words representation.) *Finally, these term frequencies were compared with Loughran and McDonald Sentiment Word Lists and four sentiment scores were calculated for each filing - Positive, Negative, Uncertain and Litigious. Q: Which tasks are pending? A: Pending tasks: *Upload the stock returns data for the selected companies (forward from the date of filing) and estimate performance of several sentiment-based investment strategies. Q: Are you facing any challenges? A: Not really. It turned out that extracting only the MD&A section from the filing is difficult because in many filings this section is a placeholder containing a list of links to the other sections of the document where the related topics are discussed. Because of this I chose to build the bag of words for the entire filing. This, in my opinion, would be a better option (compared to selecting only the filings where MD&A is a well-structured separate section) because it will ensure that the sentiment words are captured regardless where in the document they appear. Using MD&A of company's SEC filing to predict future stock price CS410 Final Project Proposal (Individual project) Project coordinator/Author: Dmitry Villevald ( dmitryv2@illinois.edu ) For my final course project I would like to choose a free topic (Option 5) and explore the impact of the information extracted from the Management's Discussion and Analysis (MD&A) section of the company's SEC filing on the future stock price. Q: What are the names and NetIDs of all your team members? Who is the captain? A: This is  individual  project by Dmitry Villevald (dmitryv2) Q: What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? A: I would like to explore how the changes in a sentiment of the company's Management's Discussion and Analysis (MD&A) impact the future stock prices. MD&A section is an unaudited section of quarterly and annual SEC filings where the company's management discusses the current status of a company and, more importantly, the future risks and opportunities. I want to explore if investors overreact to changes in management sentiment in the long run. This knowledge would be important for investors trying to decide what to do with a company stock when management sentiment changes. My plan is, first, to pull 10Q and 10K SEC filings of the companies - members of SP&500 or Dow Jones index - over a few recent years from SEC EDGAR database, parse the data and extract MD&A sections. Then, I plan to determine a sentiment score of each MD&A document using the vocabulary of positive and negative words (for example, Loughran and McDonald sentiment word lists). Third, I plan to select the events with significant changes in sentiment scores and see how these changes are correlated with the changes in company's stock price in a quarter following the filing and its announcement (using Yahoo Finance data stock price data or similar sources) adjusted for market or sector/industry returns. If the stock markets are efficient then one would not expect a significant investors' over- or under-reaction to changes in sentiment which I want to confirm. The performance metric could be the confusion matrix showing the relationships between the significant (i.e. higher, in absolute value, than a certain threshold value) changes in MD&A sentiment score - positive or negative - and the significant - positive or negative - next-quarter company's stock returns. Q: Which programming language do you plan to use? A: Python Q: Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. A: I plan to spend about 3 hours to write and test the script which will download, parse and clean MD&A data. Then it would take another 7-10 hours to actually perform these tasks (i.e. downloading, parsing and cleaning the data.) Another 2-5 hours would be spent on extracting the sentiment from MD&A sections and identifying the cases when sentiment score experiences significant changes. About 2 hours will be spent on exploring the relationships between the stock prices and sentiment scores and drawing the conclusions. Finally, I expect to spend about 5-6 hours to document my process/findings and to create a demo/presentation. CS 410 Course Final Project Report Using the sentiment analysis of company's SEC filings to predict future stock returns Author: Dmitry Villevald (dmitryv2@illinois.edu) Goal For my final project I chose a free topic (Option 5) and explored the impact of the sentiment extracted from company's 10-Q and 10-k SEC filings on the company's stock price. My expectation was that while there could be a minor impact from the sentiment changes in the filings on the stock price for some companies, this impact is likely too small and statistically insignificant for building stock investment strategy based on the sentiment only. Project Proposal File Project Proposal dmitryv2.pdf contains the project proposal. Project Progress Report File Project Progress Report dmitryv2.pdf is a project progress report. Self-Evaluation I completed most of what I planned. I was able to download from the SEC EDGAR database the index files with the titles and types of the public companies' filings and then extract the quarterly (10-Q) and annual (10-K) report for the selected companies. Originally I planned to extract only Management Analysis and Discussion (MD&A) section of 10-Q and 10-K filings for the sentiment analysis. However, after I learned that in many reports the MD&A is a list of links to the other sections of the document, I decided that a better approach would be to use the entire SEC filing for the sentiment analysis. I managed to parse those filings and build a bad-of-words (unigram) representation of each filing with the term frequencies. Then, by comparing these representations with LoughranMcDonald sentiment lists I was able to calculate four sentiment scores (Positive, Negative, Uncertain and Litigious) for each company's filing. Finally, after I loaded historical stock prices from Yahoo Finance, I calculated the forward returns (which include the dividents and are adjusted for stock splits) and joined them with the sentiment data. Finally, I was able to estimate expected returns for a few sentiment-based investment strategies. Main Results Although I only explored a few years (2016-2020) of filings for about 30 large US companies, I was able to observe some interesting results. For example, I found that Negative, Uncertain and Litigious sentiments are strongly correlated which suggests that if we use one of there three sentiments in out investment strategy, the incremental impact from including the other two will probably be marginal if any. Regarding the correlation between the future returns and changes in the sentiment scores of SEC filings, I observed some positive correlation between 1-month-forward returns from the date of filing and the quarterly changes in Negative, Uncertain and Litigious sentiment scores. For example, as the chart below shows, for MCD and APPL stocks the increase in negative/uncertain/litigious sentiment scores was followed by larger positive stock returns over the following month which is probably contrary to what most would expect. In conclusion, while the changes in the sentiment of company's filings seem to have some impact on the future stock returns, this impact is not large and, given a small data sample, is likely statistically insignificant. Additional research with more companies and longer time period is needed to build a viable stock investment strategy based on a sentiment of companies' SEC filings. Note that the charts above were built from the output of Python script demo.py - file Sentiment scores of SEC filings with forward stock returns.csv - referenced below in subsection Outputs. Demo (Video) The video with demonstration of how to run script demo.py and produce the output can be found here Documented Source Code (folder demo) The Python script demo.py 1) Downloads 10-Q and 10-K SEC filings for selected companies. Ticker, CIK and Company (company name) should be provided (for each companies one is interested in) in the input file /investment_universe/tickers_and_ciks.csv (Note that the input file for the demo contains this data for two companies - McDonalds Corp. and Apple Inc.) Ticker is used to load the historical company's stock prices to backtest investment strategy while CIK (the Central Index Key) is required to download company's filings from SEC's EDGAR database. 2) Build a bag-of-words representation and calculates term frequency for each SEC filing. 3) Calculates sentiment scores (Positive, Negative, Uncertain and Litigious) for each report. 4) Downloads historical company's stock prices and calculates weekly, quarterly and yearly forward returns, starting from the filing date (+ execution_lag_days to mitigate a look-ahead bias via simulation of a more realistic and conservative scenario where the stocks are purchased/sold on the next business day after the filing date). 5) Combines together sentiment scores and foward returns, calculates returns of a few simple investment strategies and saves the results in folder /results. In addition to the instructions below, the Python script demo.py has detailed comments on each step. Also, this video shows how to run the script. Instructions 1) Before running the script demo.py, please assign to variable base_path the location of (path to) the project folder. Example: base_path = '/Users/dmitryvillevald/Documents/UIUC/CS 410 Text Information Systems/Final Project/demo' 2) The following packages have to be installed to run the script: requests, BeautifulSoup, json, urllib, yfinance, pandas, and csv Inputs The script demo.py takes the following inputs: 1) Input file /investment_universe/tickers_and_ciks.csv with a list of companies (stocks) you want to test the investment strategy on. For each company one has to provide: - Ticker - the exchange ticker of a company's stock. Ticker is used to load the historical company's stock prices to backtest the investment strategy - CIK - the Central Index Key of a company in SEC EDGAR database. CIK is used to download the company's filings from SEC's EDGAR database for sentiment analysis - Company - the Company's name which is used for reference only Note that the input file for the demo contains this data for two companies - McDonalds Corp. and Apple Inc. 2) Four files with LoughranMcDonald sentiment lists (positive.csv, negative.csv, uncertain.csv and litigious.csv) which are located in folder /sentiment_word_lists Outputs The script demo.py outputs two files and places them in folder /results: 1) File Sentiment scores of SEC filings with forward stock returns.csv contains (1) a history of the sentiment scores extracted from SEC filings of the selected companies, (2) the changes (quarterly and yearly) of those sentiment scores and (3) one-week, one-month and one-quarter forward total stock returns (including dividends and adjusted for stock splits and spinoffs) starting from the date of SEC filing. 2) File Investment strategies results.csv contains the results of testing simple investment strategy where one takes a long position in a stock (i.e buys it) when the sentiment percent change value exceeds the long threshold and takes a short position in a stock (i.e. sells it short) when the sentiment percent change falls below the short threshold. Credits 1) https://gist.github.com/madewitt/29bceb51c494ef9ea1d34f9474aa4b3c 2) https://github.com/weiwangchun/cs410 3) https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm 4) https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists
https://github.com/enaena633/CourseProject	"MiningCausalTopicsinTextData:IterativeTopicModelingwithTimeSeriesFeedbackHyunDukKimDept.ofComputerScienceUniversityofIllinoisatUrbana-Champaignhkim277@illinois.eduMaluCastellanosInformationAnalyticsLabHPLaboratoriesmalu.castellanos@hp.comMeichunHsuInformationAnalyticsLabHPLaboratoriesmeichun.hsu@hp.comChengXiangZhaiDept.ofComputerScienceUniversityofIllinoisatUrbana-Champaignczhai@illinois.eduThomasRietzDept.ofFinanceTheUniversityofIowathomas-rietz@uiowa.eduDanielDiermeierKelloggSchoolofManagementNorthwesternUniversityd-diermeier@kellogg.northwestern.eduABSTRACTManyapplicationsrequireanalyzingtextualtopicsinconjunctionwithexternaltimeseriesvariablessuchasstockprices.Wede-velopanovelgeneraltextminingframeworkfordiscoveringsuchcausaltopicsfromtext.Ourframeworknaturallycombinesanygivenprobabilistictopicmodelwithtime-seriescausalanalysistodiscovertopicsthatarebothcoherentsemanticallyandcorrelatedwithtimeseriesdata.Weiterativelyrefinetopics,increasingthecorrelationofdiscoveredtopicswiththetimeseries.Timeseriesdataprovidesfeedbackateachiterationbyimposingpriordistribu-tionsonparameters.Experimentalresultsshowthattheproposedframeworkiseffective.CategoriesandSubjectDescriptorsI.2.7[ArtificialIntelligence]:NaturalLanguageProcessing--textanalysis;H.3.1[InformationStorageandRetrieval]:ContentAnalysisandIndexing--Linguisticprocessing;H.3[InformationStorageandRetrieval]:InformationSearchandRetrievalKeywordsCausalTopicMining,IterativeTopicMining,TimeSeries1.INTRODUCTIONAnalyzingandunderstandingtextdatacanprovideusefulinfor-mation.Probabilistictopicmodels[4,8]haveprovenveryusefulforminingtextdatainarangeofareasincludingopinionanalysis[11,17],textinformationretrieval[19],imageretrieval[9],naturallanguageprocessing[6],andsocialnetworkanalysis[12].Mostexistingtopicmodelingtechniquesfocusontextalone.However,texttopicsoftenoccurinconjunctionwithothervari-ablesthroughtime.Suchdatacallsforintegratedanalysisoftextandnon-texttimeseriesdata.Thecausalrelationshipsbetweenthetwomaybeofparticularinterest.Forexample,newsaboutPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.CIKM'13,Oct.27-Nov.1,2013,SanFrancisco,CA,USA.Copyrightisheldbytheowner/author(s).PublicationrightslicensedtoACM.ACM978-1-4503-2263-8/13/10...$15.00.http://dx.doi.org/10.1145/2505515.2505612.companiescanaffectstockprices.Howdoparticulartopicsleadtoincreasingordecreasingprices?Cantopicshelpforecastfuturepricechanges?Similarexamplesoccurinmanydomains.Howdoacompany'sproductsalesriseandfallinresponsetotextinadver-tisingorreviews?Understandingthecausalrelationshipscanim-provefuturesalesstrategies.Inelectioncampaigns,newsanalysisnewsmayexplainwhyacandidate'ssupportchangessignificantlyinthepolls.Understandingthecausalrelationshipscanimprovefuturecampaignstrategies.Whiletherearemanyvariantsoftopicmodels[2,3,18],noexistingmodelincorporatesjointlytextandassociated""external""timeseriesvariablestofindcausaltopics.Aprecise,conceptualdefinitionof""causaltopics""isbeyondourscopehere.Wemakeacomputationaldefinition:acausaltopicisasemanticallycoherenttopicfromtextdatathathasstrong,possiblylagged,associationswithanon-textualtimeseriesvariable.Thisallowsfortwo-wayrelationships:topicsmayaffectthetimeseriesand/orviceversa.Thus,weidentifypotentialcausalrelationship.Interpretationwillvarywiththeexactapplicationandmodelingorknowledgespecifictotheapplication.Ourmethodcanhelpananalystquicklyidentifyasmallsetofpossiblycausaltopicsforfurtheranalysis.1Abasicapproachtoidentifyingcausaltopicsisto:(1)findtop-icswithtopicmodelingtechniquesthen(2)identifycausaltopicsusingcorrelationsandcausalitytests.However,becausethetopicmodelingstepignoresthetimeseriesdata,itdoesnotfocusexanteontopicsthatareparticularlycloselyrelatedtoit.Eveniftwotimeseriesvariablesarecompletelydifferent(e.g.,stockpricesforanairlinevs.nationalelectionpolls),thecandidatetopicsetwouldnotchange.Sometopicsfromthissinglesetmayhappentobecor-relatedwithanygiventimeseries,butthemodelingstagedoesnotseekparticularlyrelaventinformationinanyway.Weproposeanovelgeneraltextminingframework:IterativeTopicModelingwithTimeSeriesFeedback(ITMTF),fordiscover-ingcausaltopicsfromtext.ITMTFnaturallycombinesprobabilis-tictopicmodelingwithtimeseriescausalanalysistouncovertopicsthatarebothcoherentsemanticallyandcorrelatedwithtimeseriesdata.ITMTFcanaccommodateanytopicmodelingtechniqueandanycausalitymeasure.Thisgeneralityisasignificantadvantageandwhichenablesuserstoeasilyadaptdifferenttopicmodelsandcausalitymeasuresasneededforspecificapplications.Thus,theframeworkcansupportmanydifferentapplications.ITMTFiter-ativelyrefinesatopicmodel,graduallyincreasingthecorrelationofdiscoveredtopicswiththetimeseriesdatathroughafeedback1Ourdefinitionallowsusing""correlation""and""cause""inter-changeablyasconvenient.mechanism.Ineachiteration,thetimeseriesdatainformsapriordistributionofparametersthatfeedsbackintothetopicmodel.Thus,thediscoveredtopicsaredynamicallyadaptedtofitthepat-ternsofdifferenttimeseriesdata.WeevaluateITMTFonanewsdatasetwithmultiplestockpricetimeseries,includingstockpricesfromtheIowaElectronicMar-ketsandthoseoftwolargeUScompanies(AmericanAirlinesandApple).TheresultsshowthatITMTFcaneffectivelydiscovercausaltopicsfromtextdataandtheiterativeprocessimprovesthequalityofthediscoveredcausaltopics.Themaincontributionsofthispaperinclude:1)Weintroduceanovelproblemsetupfordiscoveringcausaltopicsfromtextdatawithsupervisionbytimeseriesdata,whichhasmanyapplicationsinmultipledomains.2)Ourgeneralframeworkforsolvingthisnewproblemnaturallycombinesanyprobabilistictopicmodelwithanycorrelationorcausalanalysismethodfortimeseriesdata.Thisnoveliterativefeedbackalgorithmusestimeseriesdatatoinfluencetopicdiscoveryfromtextbyiterativelyimposingatime-series-basedpriorontopicmodelparameters.3)Weevaluatethepro-posedframeworkandalgorithmsonrealapplicationdatasetsandshowthattheproposedframeworkcaneffectivelyexploittimese-riesdatatosuperviseandimprovetopicdiscoveryfromtextdata.2.RELATEDWORKTherearetwobasictopicmodels:ProbabilisticLatentSeman-ticAnalysis(PLSA)[8]andLatentDirichletAnalysis(LDA)[4].Bothfocusonwordco-occurrences.Recentadvancedtechniquesanalyzethedynamicsoftopicsonatimeline[2,18].However,theydonotconductintegratedanalysesoftopicsandexternalvari-ables;thetopicanalysisisseparatefromtheexternaltimeseries.Thereisalsosomeefforttoincorporateexternalknowledgeinmodeling:Supervisedtopicmodels[3]andLabeledLDA[16].TheformerusessupervisedLDAwhichincorporatesareferencevalue(e.g.moviereviewarticlewithmovierating)inthemodelingpro-cess.Themodeledtopicshowsbetterpredictionpowerontheref-erencevaluethansimpleLDA.ThelatterextendssupervisedLDAassociatecategoricallabelsandeventextlabelsfortopics.Anotherwayofincorporatingexternalknowledgeistouseconjugatepriorprobabilitiesinthetopicmodelingprocess[13].TopicSentimentMixture(TSM)modelespositiveandnegativesentimenttopicsus-ingseedsentimentwordssuchas""good""or""bad.""Whilethesemethodsshowthattopicminingcanbeguidedbyexternalvari-ables,noneachiveourobjectiveofcapturingthecorrelationstruc-turebetweentexttopicsandexternaltimeseriesvariables.More-over,whilethesemodelsarespecializedforsupervisionwithspe-cificexternaldata,ourgeneralapproachcanflexiblycombineanyreasonabletopicmodelwithanycausalanalysismethod.Researchonstockpredictionusingfinancialnewscontentalsorelatestoourwork[14].Suchresearchtypicallyidentifiesthemostpredictivewordsandlabelsnewsaccordingitseffectonstockpricesonaspecificdayusingasupervisedregressionoraclassi-ficationproblemsetup.Incontrast,wesearchforgeneralcausaltopicswithunsupervisedmethods.Grangertesting[7]ispopularfortestingcausalityineconomicsusinglead/lagrelationshipsacrossmultipletimeseries.Recentev-idenceshowsthatGrangertestscanbeusedinanopinionminingcontext:predictingstockpricemovementswithasentimentcurve[5].However,Grangertestinghasnotbeenuseddirectlyintextminingandtopicanalysis.[10]providesaverybriefdescriptionofademosystembasedonourframework.Thisfocusesondescribingthesystemcomponentsandsampleresults.Here,wedescribethegeneralproblemandframeworkindetailandevaluatethealgorithmsrigorously.3.MININGCAUSALTOPICSINTEXTWITHSUPERVISIONOFTIMESERIESDATAWeformulateanewresearchproblem:causaltopicmining.Con-sidertimeseriesdatax1,...,xn,withtimestampst1,...,tn,andacollectionoftimestampeddocumentsfromthesameperiod,D={(d1,td1),...,(dm,tdm)}.ThegoalistodiscoverasetofcausaltopicsT1,...,TkwithassociatedtimelagsL1,...,Lk.AcausaltopicTiwithtimelagLiisatopicthatissemanticallycoherentandhasastrongcorrelationwiththetimeseriesdatawithtimelagLi.NotethatLicanbepositiveornegative,correspondingtotop-icsthatmightcause,ormightbecausedby,timeseriesdata.Anaturalfirststepistouseexistingworkontopicmodelingtoidentifygenerallycoherenttopicsandthenfindtopicsthatcorrelatehighlywiththeexternaltimeseriesvariables.However,thishasadrawback:topicformationiscompletelyindependentofthetimeseriesdata.Regardlessofhowmuchtwotimeseriesdiffer,ourcandidatesetoftopicstochoosefromremainsexactlythesame.Thisisclearlynon-optimalandleadsustoouriterativesolution.4.ITERATIVETOPICMODELINGWITHTIMESERIESFEEDBACKWehavetwocriteriatooptimize:topiccoherenceandtopiccor-relation.Wewanttoretainthegeneralityofthetopicmodelingframeworkwhileextendingitallowthetimeseriesvariabletoinflu-encetopicformationsowecanoptimizebothcriteriaoveramoreflexibletopicspace.4.1CausalanalysiswithtimeseriesdataPotential""causal""relationshipsbetweentimesseriesareidenti-fiedthroughcontemporaneousand/orlaggedcorrelationmeasures(e.g.,Grangertests).Thecorrelationlagstructuresuggestsdirec-tionalcausality.IfcurrentobservationsintimeseriesAcorrelatedwithlaterobservationsinB,Aissaidto""cause""B.AsimpleandverycommonmeasureusesPearsoncorrelations,contemporaneouslyorwithleadsandlags.Correlationsrangefrom-1to+1withthesignindicatingthedirectionofcorrelationandcanbeusedas""impact""measureshere.Acorrelationsignificancedependsonitsvalueandthenumberofobservations.Grangertestsaremorestructuredmeasuresofcausality,measur-ingstatisticalsignificanceatdifferenttimelagsusingautoregres-siontoidentifycausalrelationships.Letytandxtbetwotimeseries.Toseeifxt""Grangercauses""ytwithmaximumptimelag,runthefollowingregression:yt=a0+a1yt CS410 Course Project Team JEM: Reproducing Paper on Casual Topic Mining Zhangzhou Yu (leader) Matthew McCarty Jack Ma Presentation/Demo A presentation and demonstration of installing and running the application is available at https://mediaspace.illinois.edu/media/t/1_yra0qvjp . Overview This repository contains code to replicate an experiment done in a paper regarding causal topic mining with time series feedback: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The intent of this paper is to develop a method to consider additional contextual data (specifically, in the form of a time series) to supplement topic mining. The paper discusses two scenarios (presidential elections and stock prices); we chose to replicate the former. The specific experiment that was replicated involves determining topics from New York Times (NYT) articles from May-October 2000, with the additional context of betting odds for Bush and Gore winning the 2000 Presidential election. There are two files which are used as input for the Python code. One is the time series data for the betting odds, which is located in time_series.csv (Iowa2000PresidentOdds.csv is the raw data). The second input file, consolidated_nyt.tsv, is a list of NYT articles between May and October 2000. The NYT articles were filtered by 'Bush' and 'Gore' keywords to ensure that non-relevant documents were not considered for topic generation. The article date is also included with the article content, so that the time context of the article's publication can be considered with the presidential odds time series. The output of the program will be a list of topics, and the top three words within each topics. Unlike the plain vanilla PSLA algorithm, these topics highlight words that are highly correlated with the change of betting odds for Bush or Gore winning the election. The number of topics is determined by a parameter tn, and the paper discusses the performance of the algorithm with varying values of tn. For the purposes of our experiment reproduction, we chose tn=10. Software Implementation The experiment was reproduced in Python (version 3.8.6) with the help of several libraries, which are listed below: numpy - for general linear algebra operations gensim - for generating a mapping between a token id and the normalized words which they represent statsmodels - for the time-series causality test The algorithm itself is a modified version of the PLSA algorithm, which was initially implemented for a homework assignment (MP3) in CS410 at UIUC. The plsawithprior.py file contains a plsa class which contains many variables of use, some of which are highlighted below: term_doc_matrix - word count of terms in a given document document_topic_prob - the probability of p(z | d) where z represents a specific topic and d represents a specific document mu - the strength of the prior probability (when mu=0, the result would match PLSA with no prior) prior - the prior probability of p(w | z) where w represents a word and z represents a specific topic topic_word_prob = the posterior probability of p(w | z) Additional descriptions of software programs are illustrated below: * Granger_Casuality_Test.py- first clean the presidential betting odds time series raw data Iowa2000PresidentOdds.csv and output the cleaned data time_series.csv; implemented the granger casuality test function ready for use in main.py * calc_prior.py - calculate the prior of significant words within significant topics from the granger casuality test significance level output * sanitize_nyt.py - text data extraction, filtering and cleaning on the NYT articles from May-October 2000; output consolidated_nyt.tsv where each line contains documents with 'Bush' and 'Gore' keywords from one day (the number of lines in this clean text data matches with the number of rows in the clean time series data; readily available for use in the granger casuality test) * main.py - include all the functions discussed above; consolidated main program Future modifications could be made to the algorithm to change how the prior is generated (based on other time-series/non-document data source). Software Usage Run git clone https://github.com/enaena633/CourseProject.git to clone the code repository. Install Python 3. Install the following python libraries (via pip, etc.): numpy gensim statsmodels Run python main.py in the repository directory. Results The following list is the top 3 words in the ten topics that were mined from the New York Times documents: ad win ralli night lehrer trillion econom recent try support governor alaska state governor alaska governor clarenc right night win tuesdai wetston abm recent offic men try win ralli church These results are different from the paper's results, which are included below: tax cut 1 screen pataki giuliani enthusiasm door symbolic oil energy prices pres al vice love tucker presented partial abortion privatization court supreme abortion gun control nra news w top This can be explained by the following: The implementation of several elements of the algorithm (Granger causality test, PLSA, etc.) were implemented in Python, whereas the paper used R. We used the gensim package to perform stemming of words (which would cause words like econom to appear instead of economy or economic). gensim was also used to remove stop words. The paper does not specify whether a background language model was used in its implementation of PLSA or if any stop word removal was done. The EM algorithm is guaranteed to converge to a local (but not necessarily global) maximum, which causes output to be different even with the same implementation when different random starting values are used. Certain parameters in the paper are not specified (e.g., the threshold value gamma for the significance cutoff for words at the topic level, we used 90%). Team Member Contributions All team members were engaged and involved in reproducing the experiment from the paper sourced above. In addition to weekly meetings where everyone contributed, individual team members were responsible for the following: Zhangzhou Yu (leader) - Time series data retrieval/cleaning, Granger causality test, administrative/organizational tasks Matthew McCarty - Text data retrieval/cleaning, library research, documentation, presentation/demo recording Jack Ma - PLSA augmentation to include use of contextual time series data, prior implementation, consolidate/structure software programs 1) Progress made thus far * We have obtained the data, cleaned and filtered only articles containing ""Gore"" or ""Bush"" to start with * We have mostly understood the whole paper and identified the approach to implement the PLSA with Dirichlet prior * We have split up the remaining work 2) Remaining tasks * Further clean the data into only paragraphs containing ""Gore"" and ""Bush"" and remove all non-alphanumerical characters * Work on the granger causality test combining the time series data with the word counts of document data * Calculated the prior according the formula specified in the paper and update the maximization step of PLSA with the given mu and prior previously calculated 3) Any challenges/issues being faced * For now we are not running into any execution issues yet Project Proposal for Team JEM 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team JEM is made up of the following three members. The team coordinator/leader is Zhangzhou Yu (zy37@illinois.edu). o Zhangzhou Yu (zy37@illinois.edu), o Matthew McCarty (mdm12@illinois.edu), o Jack Ma (jma46@illinois.edu) 2. Which paper have you chosen? Causal topic modeling o Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python (as we found a package to do granger causality test). 4. Can you obtain the datasets used in the paper for evaluation? We are in the process of gaining access to the text datasets on Linguistic Data Consortium for the New York Times corpus from May through October 2000. The access is pending upon approval by the UIUC admin access, where the professor is actively engaged on this issue. Currently, we are able to access prices from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market as well as the stock prices of Apple and American Airlines during that time period. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? If we are unable to access the text data on the Linguistic Data Consortium for the New York Times Corpus for the 2000 presidential election, we will investigate using the online New York Times archives (or some other accessible newspaper archive) for the 2016 presidential election articles along with stock prices of American Airlines and Apple of the same time period. 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? To demonstrate that we have successfully reproduced the method introduced in the paper, we will output significant topic lists that affected the 2016 presidential election and stock prices of American Airlines and Apple in 2016 respectively. We will check the results of the top topic lists against what truly happened in that time frame."
https://github.com/everbrightw/CourseProject	CS410 Progress reportNetids: td2 (Tianli Ding)yimengh2 (Yimeng Han) yusenw2 (Yusen Wang) Progress made: We were setting up the environment to make EducationalWeb work on our end and reading the source code and project structures. We have also proposed a file to parse a whole pdf file into single files. We are planning to write a scraping script to automatically download slides from several course websites and auto integrate those slides with the current system. Remaining task:We are planning to use beautifulsoup and simple URL requests to auto download slides from several UIUC coursesO websites to scale up the current systemChallenges faced:This is a huge system with lots of folders and files. Therefore we still need more time to understand and make more improvements. CS411 report By Yusen Wang, Yimeng Han, Tianli Ding Video Introduction:  https://youtu.be/rsyHiEcATLI 1) An overview of the function of the code (i.e., what it does and what it can be used for). First, our implementation has a general crawler that can automatically scrape lecture slides in different courses from the UIUC CS courses platform ( https://courses.grainger.illinois.edu ). We were scraping all links from course websites and filtering them by finding potential links that could lead us to a lecture slide. Then, we split the slides crawled from those websites and split them into 1 page pdfs stored in folders corresponding to course names. After that, we implement a Jaccard similarity function to select related slides of each page, record the result in a csv file. Then based on the relationships of the slides, we render the final result on the website. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. In crawling/scraper.py & crawling/utils.py : For the crawler, we choose the  https://courses.grainger.illinois.edu  as our start page to scrape our courses' url that are listed on this website. For this final project, we are only scraping courses that have 'CS' as a prefix. After we have all the course urls, we will iterate all of them and step into every course website to find where could the lecture slides/notes be located using keywords(['slides', 'slide', 'lecture','lectures','note','notes','resources','resource']). Then we start to scrape all the tags <a href=true and determine whether the links are a potential lecture slide by simply checking whether the url ends with .pdf and .pptx (since we have already filtered those links with the keyword, this constraint could very likely be the lecture slides that we want to scrape). And we parse those links to a formal format and download them into the corresponding folder. All the tasks are automated and we can expect to see a system with a large scale of lecture slides using this crawler. In pdf.js/static/getRelatedFiles.py : We first extract text in pdf pages, tokenize the content and select key words after eliminating punctuations, stopwords. Then compare Jaccard similarities of the keywords from one pdf with all other pdfs, when the similarity is greater than 0.3, we mark these two pdfs as related. Furthermore, we limit the size of the related slides as 12, so that when rendering them on the website, the list won't be too long. Then, we record them in ranking.csv after changing the format according to the original format in the platform EducationalWeb; and record all slide names in slide_names.txt. In pdf.js/parsePDF.py : We put the results of the scraped folders under raw_slides. We filtered out those corrupted slides.Then we first followed the naming convention to change the course and slides name, then split each of the slides each long pdf into a folder containing single slides for rendering. In model.py:  We modified the model.py file so that other than cs410, other courses can also be rendered in the webpage. Moreover, we changed the path  related_slides_path, slides_path, so that the related files are derived from our own algorithm and include all slides from different courses. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. The following instructions have been tested with Python2.7 on Linux and MacOS 1.You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html 2.Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ 3.Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static =========== NEWLY ADDED FEATURES ========= 4.Run python scraper.py from CourseProject/crawling/ to scrape lecture slides from the website 5.Then run python parsePDF under EducationalWeb/pdf.js to normalize the slides name and save one PDF into a folder with single slides. 6.Run python getRelatedFiles.py in EducationalWeb/pdf.js/static to get every single slide's related slides with ranking scores 7.From EducationalWeb/pdf.js/build/generic/web, run the following command: gulp server 8.In another terminal window, run python app.py from EducationalWeb/ 9.The site should be available at  http://localhost:8096/ 4) Brief description of contribution of each team member in case of a multi-person team. We reviewed the original code on the EducationalWeb, analyzed the structures and functions in their repository, then ran code based on the instructions together. Then we had meetings discussing the structures and functions that we aim to implement. Yusen is mainly responsible for implementing the crawling part,   Tia nli normalizes the slides name and saves one PDF into a folder with single slides, Yimeng takes charge of implementing the 'related slides' algorithm. After one finished his/her part, other team members did code review and added comments for improvements. We wrote documentations and recorded demo videos together. EducationalWeb video introduction https://youtu.be/rsyHiEcATLI how to run The following instructions have been tested with Python2.7 on Linux and MacOS You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ Run python scraper.py from CourseProject/crawling/ to scrape lecture slides from the website Then run python parsePDF under EducationalWeb/pdf.js/ to normalize the slides name and save one PDF into a folder with single slides. Run python getRelatedFiles.py in EducationalWeb/pdf.js/static to get every single slide's related slides with ranking scores From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/
https://github.com/falobba2/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/garciadiazjaime/CourseProject	Progress Report CS-410 / Fall 2020 Which tasks have been completed? The following tasks marked with an x are in progress; they haven't been completed yet but a good progress has been made. [x] Instagram crawler [x] Food classifier Which tasks are pending? The following task are pending, not much progress has been made here. [ ] Food API [ ] Web application (Demo) [ ] Documentation Are you facing any challenges? Instagram crawler It took some time to build the Instagram crawler but a stable version is not in place. Food classifier Giving a supervised approached is been used, it has been time consuming traning the classifier, but good progress has been made so far. Project Proposal CS-410 / Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. jaimeg4 (Individual) What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic (Free): Chicago Food Instagram Crawler. Description: Provide an easy way to find up-to-date food options for the Chicago area using Instagram as the source. Task: Instagram has a public API that provides access to recent the posts, and with the help of the hashtags we can determine if it's a post related to food; if that's the case a crawler will extract the coordinates used for the location and a classifier will try to guess the food category, finally the information will be saved into a database and exposed throught a REST API. Important: At the moment there are a couple of alternatives like Ubereats or Yelp, however sometimes their data is outdated or their pictures are of poor quality; so the importance of the project is to provide up-to-date information and quality pictures using the Instagram community. Approach & Tools: To mention some of the components needed: a) Instagram crawler (Nodejs script). b) Food classifier (Tensorflow.js). c) Food API (Express nodejs). d) Web application (Svelte Javascript Framework). Outcome: An interactive web application that will show Chicago food options and a way to filter them by categories (classifiers). Evaluation: The project will be evaluated by the progress of the web application, which will be a Proof-of-Concept. Demo What to eat in Chicago? CourseProject Chicago Food Instagram Crawler. Proposal Progress Report Documentation Presentation Demo Contact details Twitter @jaumint
https://github.com/gnsandeep/CourseProject	"CourseProject CS410Fall2020 Course Project : Text Classification Team members: 1. Sandeep Nanjegowda ( sgn3@illinois.edu) 2. Sunitha Vijayanarayan 3. Valentina Mondal Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7-b1b43508023f Live Lab Data Linked Git Hub Account : gnsandeep Public Domain Public Domain CS-410 Text Information Systems PROJECT DOCUMENTATION Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 INDEX An overview of the function of the code ---------------------------------------------------------------------------3 Implementation of models for Tweet Classification -------------------------------------------------------------4 2.1 BERT ----------------------------------------------------------------------------------------------------------------------4 2.2 SVM -----------------------------------------------------------------------------------------------------------------------5 2.3 CNN -----------------------------------------------------------------------------------------------------------------------6 2.4 LSTM ----------------------------------------------------------------------------------------------------------------------8 2.5 GRU -----------------------------------------------------------------------------------------------------------------------9 2.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------10 2.7 Bi-directional Models (GRU & LSTM) ----------------------------------------------------------------------------11 Tweet Classification Model Usage ------------------------------------------------------------------------------------13 3.1 BERT ---------------------------------------------------------------------------------------------------------------------13 3.2 SVM ---------------------------------------------------------------------------------------------------------------------14 3.3 CNN ----------------------------------------------------------------------------------------------------------------------15 3.4 LSTM --------------------------------------------------------------------------------------------------------------------15 3.5 GRU ----------------------------------------------------------------------------------------------------------------------16 3.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------17 3.7 Bi-directional LSTM and GRU -------------------------------------------------------------------------------------18 Results -----------------------------------------------------------------------------------------------------------------------19 Collaboration ---------------------------------------------------------------------------------------------------------------20 Presentation ----------------------------------------------------------------------------------------------------------------21 REFERENCES ------------------------------------------------------------------------------------------------------------------21 Overview Our goal is to classify the given test set data of twitter responses as ""SARCASM"" or ""NOT SARCASM"" by using various classification methods. Have classified the test data using Linear Regression, Naive Bayes, GRU, CNN, LSTM, SVM, Bidirectional and BERT models. While predicting the label of the ""response"" based on the ""context"". Context which is an ordered list of dialogue for which response is a reply to the last dialogue in the context. Test data has unique ids along with tweet ""responses"" to be classified. We build, train several different models using given trained data and predict the test data using the trained model and generate an ""answer.txt"" file that has unique ids along with the predicted label. Since there are 1800 ids in test dataset, our generated ""answer.txt"" file has exactly 1800 rows. At a high level, we perform the below steps: Preprocess both the training and test data by removing the html tags, converting the tweets to lower case, removing punctuations and numbers, removing stop words, converting emojis and emoticons, removing single character and multiple spaces, removing left over special characters. Split the test dataset into train and test data. Create the embedding matrix using glove or word2vec embeddings. Create the model using embedding layer, adding layers such as LSTM, CNN etc. Fit the model on train data Predict the model on test data and generating predicted labels Once the answer.txt file is generated and uploaded to git hub, predicted label values are compared with the actual result set to obtain the precision, recall and F-score value. Precision indicates fraction of relevant instances among the retrieved instances. Precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly. Recall indicates fraction of the total amount of relevant instances that were retrieved. Recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. F-score is the harmonic mean of the precision and recall. The highest possible value of an F-score is 1. Our baseline to be achieved is 0.723 We were successfully able to beat the baseline using Logistic Regression and BERT model. We will be walking through further in below points regarding implementation and usage for all the models we tried. The models that we tried can be used to classify any tweets that has response and context attributes. The models can be leveraged for other applications with appropriate changes such as classification of tags in news headline, sentiment analysis for movie reviews etc. Implementation of Models for Tweet Classification This section describes implementation of different Tweet Classification Models 2.1 BERT We have used BERT (Bidirectional Encoder Representations from Transformers) model BertForSequenceClassification for Tweet classification. Model is built using Jupyter Notebook. Jupyter Notebook has following Sections in same order. Chris McCormick Blogs and code about BERT were very helpful for using BERT for classification. We have used certain sections of this code from (McCormick) in our Model. 2.1.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. URLs, @, non-ASCII characters, extra space, &, <, > are removed, space is inserted between punctuation marks. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.1.2 Model We have used transformers package from Hugging Face which will give us a PyTorch interface for working with BERT. We have installed transformers package. 2.1.3 Tokenizing Input and creating DataLoaders To feed our text to BERT, it must be split into tokens, we need to add special tokens to start and end, Pad and Truncate all sentences to a single constant length (we have used max length as 256), differentiate real tokens from padding tokens with attention mask. and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. We have used ""uncased"" version - ""bert-base-uncased"". Training dataset is split for training and validation. We will also create an iterator for our training, validation and test dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. Note: We have used SequentialSampler for validation and test datasets. 2.1.4 Pre-Trained BERT Model After this we have loaded Pre-Trained BERT model - BertForSequenceClassification with a single linear classification layer on top. 2.1.5 Model Parameters and Learning Rate Scheduler We have used AdamW Optimizer with learning rate of 2e-5 and eps of 1e-8 and get_linear_schedule_with_warmup is created out of the Optimizer. 2.1.6 Utility Functions Utility function format_time is created for formatting time and utility function flat_accuracy is created for calculating accuracy. 2.1.7 Training Loop Each pass in our loop we have a training phase and a validation phase Training: Unpack our data inputs and labels Load data onto the GPU for acceleration Clear out the gradients calculated in the previous pass. In PyTorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. Forward pass (feed input data through the network) Backward pass (backpropagation) Tell the network to update parameters with optimizer.step() Track variables for monitoring progress Evalution: Unpack our data inputs and labels Load data onto the GPU for acceleration Forward pass (feed input data through the network) Compute loss on our validation data and track variables for monitoring progress 2.1.8 Evaluating Test Data Prediction is done on test dataset. Test dataset is read in batches and prediction is appended to list, which is then added to test panda's data frame and answer_BERT.txt is created. 2.2 SVM We have used Support Vector Machines model for Tweet classification. Code is written in python language. We have used certain sections of code from (Bronchal) in our model: 2.2.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. HTML tags, punctuation, numbers, single character, multiple spaces, stop words, left over special characters are removed. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.2.2 Tokenizing Input Twitter-aware tokenizer designed to be flexible and easy to adapt to new domains and tasks. Tuple regex_strings define a list of regular expression strings. regex_strings strings are put, in order, into a compiled regular expression object called word_re. The tokenization is done by word_re.findall(s), where s is the user-supplied string, inside the tokenize() method of the class Tokenizer. When instantiating Tokenizer objects, there is a single option: preserve_case. By default, it is set to True. If it is set to False, then the tokenizer will downcase everything except for emoticons. 2.2.3 Vectorize CountVectorizer converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. The following parameters are used: Analyzer: feature is made of word n-grams. Tokenizer: override the string tokenization step while preserving the preprocessing and n-grams generation steps Lowercase: convert all characters to lowercase before tokenizing. Ngram_range: The lower and upper boundary of the range of n-values for different word n-grams to be extracted i.e. (1,1) Stop_words: a built-in stop word list for English is used 2.2.4 Cross validation and grid search We use cross validation and grid search to find good hyperparameters for our SVM model. We build a pipeline to get features from the validation folds when building each training model. GridSearchCV implements a ""fit"" and ""predict"" method. Snapshot of code below for reference. It exhaustively searches over specified parameter values for an estimator. param_grid enables searching over any sequence of parameter settings. cv determines the cross-validation splitting strategy. n_jobs as -1 means to use all processors in parallel. Verbose controls the verbosity of messages. Scoring is to evaluate the predictions on the test set. 2.2.5 Evaluating Test Data Model with the best hyperparameters works on test data and basis the prediction, labels generated with value greater than 0.5 as ""SARCASM"" else ""NOT_SARCASM"" is appended to test panda's data frame and answer_SVM.txt is created. 2.3 CNN We have used Convolutional neural network model for Tweet classification. Code is written in python language. We have used certain sections of this code from (Celeni) in our model: 2.3.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for CNN model. 2.3.2 Word embedding Word embedding is vector representation of a particular word. Weight matrix is created from word2vec gensim model. And then embedding vectors are obtained from word2vec and using it as weights of non-trainable keras embedding layer. Corpus for twitter data for word2vec named as ""3000tweets_notbinary"" was referred from (Celeni) We also build the embeddings using Global Vectors for Word Representation. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Corpus can be downloaded from the link (Paletto) that we used as embedding file in our code to build the weight matrix. 2.3.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of CNN layer that creates a convolution kernel that is convoluted with the layer input over a single spatial dimension to produce a tensor of outputs. The action ReLU is applied to outputs as well. We set the number of filters to the dimensionality of the output space. Kernel_size is to specify the length of the 1D convolution window. Further we maxpool that summarize the most activated presence of a feature. Pooling is required to down sample the detection of features in feature maps. We also use global pooling that that down sample the entire feature map to a single value. This is same as setting the pool_size to the size of the input feature map. We then use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. Code snippet below: 2.3.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.3.5 Evaluating Test Data Once the model is created, model.predict() is run to predict for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_CNN.txt is created. 2.4 LSTM We have used Long Short-Term Memory network model for Tweet classification. Code is written in python language. We have used certain sections of this code in our model from (nana). 2.4.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for LSTM model. 2.4.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for LSTM model. 2.4.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of LSTM layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.4.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.4.5 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_LSTM.txt is created. 2.5 GRU We have tried GRU (Gated Recurrent Unit) model for Tweet classification. Code is written in python language. GRU is a special type of Recurrent Neural network. This type of sequence model can retain information from long ago, without washing it through time or remove information which is irrelevant to the prediction. Some of the code & hyper-parameter values specifically those related to ReduceLROnPlateau were decided based on the blog by (Kohli) 2.5.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for GRU model. 2.5.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for GRU model. 2.5.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.5.4 ReduceLROnPlateau This option was used so that the learning rate to be reduced when training is not progressing. 2.5.5 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.5.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.5.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_GRU.txt is created. 2.6 Naive Bayes and Linear Regression Naive Bayes and Logistic Regression Models are built using Jupyter Notebook for Tweet Classification. Jupyter Notebook has following Sections in same order. Bert Carremans Blog and code about Naive Bayes and Linear Regression were very helpful. We have used certain sections of code in our Model from the reference (Carremans) 2.6.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. We have Python class TextCounts , it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class extracts additional features like word counts, hash tags, mentions, capital words, question marks, urls and emojis. We have Python class CleanText it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class removes mentions, urls, oneword, punctuations, digits, Stopwords , performs stemming and converts to lower case. Test and Training data is passed through this classes fit methods. Extra features extracted for training data is combined with Cleaned data for both Training and Test data. Steps for Test data is done after training. We have Python class ColumnExtractor it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class is used for selecting columns in test and training dataset. 2.6.2 Hyperparameter tuning and cross-validation We first declare parameters for grid search. We have set parameters_vect , parameters_mnb and parameters_logreg parameters. We have parameters for TF-IDF like max_df , min_df , ngram_range , parameters for Linear Regression like clf_c and clf_penality , parameters for Naiive bayes like alpha. we have function grid_vect1 which does grid search. This function uses skilearn pipeline and it is based on below code from the reference (Scikit Learn) . 2.6.3 Model Training and Prediction Naiive Bayes and Logistic Regression models are created and passed to grid_vect1 which vectorizes the data using TF-IDF in ski learn pipeline and does grid search for best Hyper parameters. Once we find the best hyperparameters, we fit both Naive bayes and Logistic regression models with best parameters and perform prediction on test data. Predicted data is added to pandas data frame as new column and answer.txt is created for both models. 2.7 Bi-directional Models (GRU & LSTM) We have tried bi-directional versions of both GRU (Gated Recurrent Unit) model & LSTM model for Tweet classification. Code is written in python language. Bidirectional models are an extension of traditional LSTM & GRU models that can improve model performance on sequence classification problems. In problems where all timesteps of the input sequence are available, Bidirectional models train two instead of one LSTMs on the input sequence. Some of the code was referenced from (LillySimeonova) 2.7.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for these models as well. In addition, we also combined the additional text counts features calculated in the LogisticRegression model and combined them with the text features in a bid to improve model performance on the test set. 2.7.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for Bidirectional LSTM model & Bidirectional GRU. 2.7.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU/LSTM layers with units specifying the dimensionality of the output space. We are then combining text features with count features using multiple input models. All the other layers are added exactly like GRU/LSTM models already discussed in section 2.5 & 2.4. Combining of text features with non-text was coded by referring (Freischlag) We compile the model using the binary cross-entropy loss function since it predicts a binary value and Adam optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.7.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.7.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.7.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_Bidirectional.txt is created. Tweet Classification Models Usage This Section describes steps for running different Tweet Classification Models which are described in Section 2. 3.1 BERT Google Colab is preferred for running BERT model. We have used Colab for training and evaluating test dataset. 3.1.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose BERTSeq.ipynb 3.1.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files. 3.1.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.1.4 Result Predicted values are stored in answer_BERTSeq.txt, we can see the file in Files section Colab. 3.2 SVM SVM model can be run locally. We have used python language. 3.2.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/CS410Fall2020/ClassificationCompetition/tree/main/data to your local path under data folder. 3.2.2 SVM code Load the SVM.py code in your local path 3.2.3 Prerequisites Ensure to download and install the below libraries and modules to run the code to not throw any errors 3.2.4 Run Run SVM.py code in your local. 3.2.5 Result Predicted values are stored in your local path as 'answer_SVM.txt' 3.3 CNN CNN model can be run locally. We have python language. 3.3.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.3.2 CNN code Load the CNN.py code in your local path 3.3.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.3.4 Run Run CNN.py code in your local. 3.3.5 Result Predicted values are stored in your local path as 'answer_CNN.txt' 3.4 LSTM LSTM model can be run locally. We have used python language. 3.4.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.4.2 LSTM code Jupyter notebook LSTM.ipynb . 3.4.3 Prerequisites Please download Glove embedding file glove.6B.100d.txt ( it is available under glove.6B.zip) and place it in data folder. We can download Glove embedding file from https://nlp.stanford.edu/projects/glove/ Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip 3.4.4 Run Open the Jupyter notebook LSTM.ipynb and click Kernel Restart and Run All. 3.4.5 Result Predicted values are stored in your local path as 'answer_LSTM.txt' 3.5 GRU GRU model can be run locally. We have used python language. 3.5.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.5.2 GRU code Load the GRU.py code in your local path 3.5.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.5.4 Run Run GRU.py code in your local. 3.5.5 Result Predicted values are stored in your local path as 'answer_GRU.txt' 3.6 Naive Bayes and Logistic Regression Google Colab is preferred for running Naive Bayes and Logistic Regression model. We have used Colab for training and evaluating test dataset. 3.6.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose LGNB_CV.ipynb 3.6.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files from https://github.com/gnsandeep/CourseProject/tree/main/data. 3.6.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.6.4 Result Predicted values are stored in answerNB.txt and answerCVLG.txt, we can see the file in Files section Colab. 3.7 Bi-directional Models (GRU & LSTM) Bi-directional models can be run locally. We have used python language. 3.7.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.7.2 Bi-directional model code Load the BidirectionalModels.py code in your local path 3.7.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.7.4 Run Run BidirectionalModels.py code in your local. 3.7.5 Result Predicted values are stored in your local path as 'answer_blstm.txt' & answer_bgru.txt Results Below are some of the results that were captured in leaderboard live data lab for individual model: Model Precision Recall F1 Base Line 0.723 0.723 0.723 BERT 0.7181913774973712 0.7588888888888888 0.7379794705564559 Logistic Regression 0.6593406593406593 0.8 0.7228915662650602 Naive Bayes 0.6177215189873417 0.8133333333333334 0.702158273381295 SVM 0.6225716928769658 0.7477777777777778 0.6794548207975769 CNN 0.6376811594202898 0.6844444444444444 0.660235798499464 LSTM 0.620242214532872 0.7966666666666666 0.6974708171206226 GRU 0.5968109339407744 0.8733333333333333 0.7090663058186738 Bidirectional- GRU 0.6535819430814525 0.74 0.69411151641479 Bidirectional -LSTM 0.639927073837739 0.78 0.703054581872809 Screen shot for BERT Collaboration Brief description of contribution of each team member in case of a multi-person team. We had frequent meetings, we discussed about the models we had learnt, built, tested and results. We also did code reviews, incorporated suggestions. Sandeep: Build and test of LSTM on word embeddings using glove. Build and test of Naive Bayes and Logistic Regression. Build and test of BERT. Documentation for BERT, Naive Bayes and Logistic Regression models. Bidirectional LSTM along with combining Text & Non-Text features. Voice over project presentation and demo. Investigation & Test of Bi-directional LSTM model with multiple input features. Sunitha: Build and test of GRU on word embeddings using word2vec and glove. Investigation & Test of Bi-directional GRU model with multiple input features. Implementation for additional preprocessing steps to convert emojis and emoticons to text. Implementation of ModelCheckpoint ,ReduceLROnPlateau. Test effects of additional emoji pre-processing, EarlyStopping, ModelCheckpointing & ReduceLROnPlateau on GRU, LSTM & CNN models. Documentation for GRU & Bi-directional Models. Valentina: Creating word vectors by word2vec method, create weight matrix from word2vec gensim model, getting embedding vectors from word2vec and using it as weights of non-trainable keras embedding layer. Build and test of SVM model using cross validation and grid search to find good hyperparameters by building a pipeline. Implementation of early stopping - Build network and train it until validation loss reduces. Build and test of CNN on word embeddings using word2vec and glove. Convert test and train features to InputFeatures that BERT understands, create model using pooled output, layer for tuning, dropout, labels conversion to one hot encoding; and get predictions. Investigation & Test of LSTM model with word2vec, glove and different parameters. Documentation for project report, documentation overview, SVM, CNN and LSTM models. Presentation Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7-b1b43508023f References [Online] / auth. Scikit Learn. - http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html. https://mccormickml.com/2019/07/22/BERT-fine-tuning/ [Online] / auth. McCormick. Combining numerical and text features in deep neural networks [Online] / auth. Freischlag Christian. - https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4. https://github.com/ibrahimcelenli/cnn-word2vec-tweets-classification / auth. Celeni Ibrahim. https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation [Online] / auth. Paletto J.D.. https://www.kaggle.com/lbronchal/sentiment-analysis-with-svm [Online] / auth. Bronchal Luis. Sarcasm Detection using LSTM, GRU, (85% Accuracy) [Online] / auth. Kohli Nikhil. - https://www.kaggle.com/nikhilkohli/sarcasm-detection-using-lstm-gru-85-accuracy. Sentiment Analysis in Python with keras and LSTM [Online] / auth. nana roblex. - https://www.kaggle.com/roblexnana/sentiment-analysis-with-keras-and-lstm. Sentiment Analysis with Bidirectional LSTM [Online] / auth. LillySimeonova. - https://www.kaggle.com/liliasimeonova/sentiment-analysis-with-bidirectional-lstm. Sentiment Analysis with Text Mining [Online] / auth. Carremans Bert. - https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27. Public Domain CS-410 Text Information Systems PROJECT DOCUMENTATION Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 Public Domain INDEX 1. An overview of the function of the code ---------------------------------------------------------------------------3 2. Implementation of models for Tweet Classification -------------------------------------------------------------4 2.1 BERT ----------------------------------------------------------------------------------------------------------------------4 2.2 SVM -----------------------------------------------------------------------------------------------------------------------5 2.3 CNN -----------------------------------------------------------------------------------------------------------------------6 2.4 LSTM ----------------------------------------------------------------------------------------------------------------------8 2.5 GRU -----------------------------------------------------------------------------------------------------------------------9 2.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------10 2.7 Bi-directional Models (GRU & LSTM) ----------------------------------------------------------------------------11 3. Tweet Classification Model Usage ------------------------------------------------------------------------------------13 3.1 BERT ---------------------------------------------------------------------------------------------------------------------13 3.2 SVM ---------------------------------------------------------------------------------------------------------------------14 3.3 CNN ----------------------------------------------------------------------------------------------------------------------15 3.4 LSTM --------------------------------------------------------------------------------------------------------------------15 3.5 GRU ----------------------------------------------------------------------------------------------------------------------16 3.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------17 3.7 Bi-directional LSTM and GRU -------------------------------------------------------------------------------------18 4. Results -----------------------------------------------------------------------------------------------------------------------19 5. Collaboration ---------------------------------------------------------------------------------------------------------------20 6. Presentation ----------------------------------------------------------------------------------------------------------------21 REFERENCES ------------------------------------------------------------------------------------------------------------------21 Public Domain 1. Overview Our goal is to classify the given test set data of twitter responses as ""SARCASM"" or ""NOT SARCASM"" by using various classification methods. Have classified the test data using Linear Regression, Naive Bayes, GRU, CNN, LSTM, SVM, Bidirectional and BERT models. While predicting the label of the ""response"" based on the ""context"". Context which is an ordered list of dialogue for which response is a reply to the last dialogue in the context. Test data has unique ids along with tweet ""responses"" to be classified. We build, train several different models using given trained data and predict the test data using the trained model and generate an ""answer.txt"" file that has unique ids along with the predicted label. Since there are 1800 ids in test dataset, our generated ""answer.txt"" file has exactly 1800 rows. At a high level, we perform the below steps: * Preprocess both the training and test data by removing the html tags, converting the tweets to lower case, removing punctuations and numbers, removing stop words, converting emojis and emoticons, removing single character and multiple spaces, removing left over special characters. * Split the test dataset into train and test data. * Create the embedding matrix using glove or word2vec embeddings. * Create the model using embedding layer, adding layers such as LSTM, CNN etc. * Fit the model on train data * Predict the model on test data and generating predicted labels Once the answer.txt file is generated and uploaded to git hub, predicted label values are compared with the actual result set to obtain the precision, recall and F-score value. Precision indicates fraction of relevant instances among the retrieved instances. Precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly. Recall indicates fraction of the total amount of relevant instances that were retrieved. Recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. F-score is the harmonic mean of the precision and recall. The highest possible value of an F-score is 1. Our baseline to be achieved is 0.723 We were successfully able to beat the baseline using Logistic Regression and BERT model. We will be walking through further in below points regarding implementation and usage for all the models we tried. The models that we tried can be used to classify any tweets that has response and context attributes. The models can be leveraged for other applications with appropriate changes such as classification of tags in news headline, sentiment analysis for movie reviews etc. Public Domain 2. Implementation of Models for Tweet Classification This section describes implementation of different Tweet Classification Models 2.1 BERT We have used BERT (Bidirectional Encoder Representations from Transformers) model BertForSequenceClassification for Tweet classification. Model is built using Jupyter Notebook. Jupyter Notebook has following Sections in same order. Chris McCormick Blogs and code about BERT were very helpful for using BERT for classification. We have used certain sections of this code from (McCormick) in our Model. 2.1.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. URLs, @, non-ASCII characters, extra space, &, <, > are removed, space is inserted between punctuation marks. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.1.2 Model We have used transformers package from Hugging Face which will give us a PyTorch interface for working with BERT. We have installed transformers package. 2.1.3 Tokenizing Input and creating DataLoaders To feed our text to BERT, it must be split into tokens, we need to add special tokens to start and end, Pad and Truncate all sentences to a single constant length (we have used max length as 256), differentiate real tokens from padding tokens with attention mask. and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. We have used ""uncased"" version - ""bert-base-uncased"". Training dataset is split for training and validation. We will also create an iterator for our training, validation and test dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. Note: We have used SequentialSampler for validation and test datasets. 2.1.4 Pre-Trained BERT Model After this we have loaded Pre-Trained BERT model - BertForSequenceClassification with a single linear classification layer on top. 2.1.5 Model Parameters and Learning Rate Scheduler Public Domain We have used AdamW Optimizer with learning rate of 2e-5 and eps of 1e-8 and get_linear_schedule_with_warmup is created out of the Optimizer. 2.1.6 Utility Functions Utility function format_time is created for formatting time and utility function flat_accuracy is created for calculating accuracy. 2.1.7 Training Loop Each pass in our loop we have a training phase and a validation phase Training: * Unpack our data inputs and labels * Load data onto the GPU for acceleration * Clear out the gradients calculated in the previous pass. o In PyTorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. * Forward pass (feed input data through the network) * Backward pass (backpropagation) * Tell the network to update parameters with optimizer.step() * Track variables for monitoring progress Evalution: * Unpack our data inputs and labels * Load data onto the GPU for acceleration * Forward pass (feed input data through the network) * Compute loss on our validation data and track variables for monitoring progress 2.1.8 Evaluating Test Data Prediction is done on test dataset. Test dataset is read in batches and prediction is appended to list, which is then added to test panda's data frame and answer_BERT.txt is created. 2.2 SVM We have used Support Vector Machines model for Tweet classification. Code is written in python language. We have used certain sections of code from (Bronchal) in our model: 2.2.1 Preprocessing of Data Public Domain Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. HTML tags, punctuation, numbers, single character, multiple spaces, stop words, left over special characters are removed. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.2.2 Tokenizing Input Twitter-aware tokenizer designed to be flexible and easy to adapt to new domains and tasks. Tuple regex_strings define a list of regular expression strings. regex_strings strings are put, in order, into a compiled regular expression object called word_re. The tokenization is done by word_re.findall(s), where s is the user-supplied string, inside the tokenize() method of the class Tokenizer. When instantiating Tokenizer objects, there is a single option: preserve_case. By default, it is set to True. If it is set to False, then the tokenizer will downcase everything except for emoticons. 2.2.3 Vectorize CountVectorizer converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. The following parameters are used: Analyzer: feature is made of word n-grams. Tokenizer: override the string tokenization step while preserving the preprocessing and n-grams generation steps Lowercase: convert all characters to lowercase before tokenizing. Ngram_range: The lower and upper boundary of the range of n-values for different word n-grams to be extracted i.e. (1,1) Stop_words: a built-in stop word list for English is used 2.2.4 Cross validation and grid search We use cross validation and grid search to find good hyperparameters for our SVM model. We build a pipeline to get features from the validation folds when building each training model. GridSearchCV implements a ""fit"" and ""predict"" method. Snapshot of code below for reference. It exhaustively searches over specified parameter values for an estimator. param_grid enables searching over any sequence of parameter settings. cv determines the cross-validation splitting strategy. n_jobs as -1 means to use all processors in parallel. Verbose controls the verbosity of messages. Scoring is to evaluate the predictions on the test set. Public Domain 2.2.5 Evaluating Test Data Model with the best hyperparameters works on test data and basis the prediction, labels generated with value greater than 0.5 as ""SARCASM"" else ""NOT_SARCASM"" is appended to test panda's data frame and answer_SVM.txt is created. 2.3 CNN We have used Convolutional neural network model for Tweet classification. Code is written in python language. We have used certain sections of this code from (Celeni) in our model: 2.3.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for CNN model. 2.3.2 Word embedding Word embedding is vector representation of a particular word. Weight matrix is created from word2vec gensim model. And then embedding vectors are obtained from word2vec and using it as weights of non-trainable keras embedding layer. Corpus for twitter data for word2vec named as ""3000tweets_notbinary"" was referred from (Celeni) We also build the embeddings using Global Vectors for Word Representation. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Corpus can be downloaded from the link (Paletto) that we used as embedding file in our code to build the weight matrix. 2.3.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of CNN layer that creates a convolution kernel that is convoluted with the layer input over a single spatial dimension to produce a tensor of outputs. The action ReLU is applied to outputs as well. We set the number of filters to the dimensionality of the output space. Kernel_size is to specify the length of the 1D convolution window. Further we maxpool that summarize the most activated presence of a feature. Pooling is required to down sample the detection of features in feature maps. We also use global pooling that that down sample the entire feature map to a single value. This is same as setting the pool_size to the size of the input feature map. We then use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. Public Domain Code snippet below: 2.3.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.3.5 Evaluating Test Data Once the model is created, model.predict() is run to predict for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_CNN.txt is created. 2.4 LSTM We have used Long Short-Term Memory network model for Tweet classification. Code is written in python language. We have used certain sections of this code in our model from (nana). 2.4.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for LSTM model. 2.4.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for LSTM model. 2.4.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of LSTM layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Public Domain Code snippet below: 2.4.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.4.5 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_LSTM.txt is created. 2.5 GRU We have tried GRU (Gated Recurrent Unit) model for Tweet classification. Code is written in python language. GRU is a special type of Recurrent Neural network. This type of sequence model can retain information from long ago, without washing it through time or remove information which is irrelevant to the prediction. Some of the code & hyper-parameter values specifically those related to ReduceLROnPlateau were decided based on the blog by (Kohli) 2.5.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for GRU model. 2.5.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for GRU model. 2.5.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform Public Domain a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.5.4 ReduceLROnPlateau This option was used so that the learning rate to be reduced when training is not progressing. 2.5.5 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.5.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.5.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_GRU.txt is created. 2.6 Naive Bayes and Linear Regression Naive Bayes and Logistic Regression Models are built using Jupyter Notebook for Tweet Classification. Jupyter Notebook has following Sections in same order. Bert Carremans Blog and code about Naive Bayes and Linear Regression were very helpful. We have used certain sections of code in our Model from the reference (Carremans) 2.6.1 Preprocessing of Data Public Domain Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. We have Python class TextCounts , it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class extracts additional features like word counts, hash tags, mentions, capital words, question marks, urls and emojis. We have Python class CleanText it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class removes mentions, urls, oneword, punctuations, digits, Stopwords , performs stemming and converts to lower case. Test and Training data is passed through this classes fit methods. Extra features extracted for training data is combined with Cleaned data for both Training and Test data. Steps for Test data is done after training. We have Python class ColumnExtractor it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class is used for selecting columns in test and training dataset. 2.6.2 Hyperparameter tuning and cross-validation We first declare parameters for grid search. We have set parameters_vect , parameters_mnb and parameters_logreg parameters. We have parameters for TF-IDF like max_df , min_df , ngram_range , parameters for Linear Regression like clf_c and clf_penality , parameters for Naiive bayes like alpha. we have function grid_vect1 which does grid search. This function uses skilearn pipeline and it is based on below code from the reference (Scikit Learn) . 2.6.3 Model Training and Prediction Naiive Bayes and Logistic Regression models are created and passed to grid_vect1 which vectorizes the data using TF-IDF in ski learn pipeline and does grid search for best Hyper parameters. Once we find the best hyperparameters, we fit both Naive bayes and Logistic regression models with best parameters and perform prediction on test data. Predicted data is added to pandas data frame as new column and answer.txt is created for both models. 2.7 Bi-directional Models (GRU & LSTM) We have tried bi-directional versions of both GRU (Gated Recurrent Unit) model & LSTM model for Tweet classification. Code is written in python language. Bidirectional models are an extension of traditional LSTM & GRU models that can improve model performance on sequence classification problems. In problems where all timesteps of the input sequence are available, Bidirectional models train two instead of one LSTMs on the input sequence. Some of the code was referenced from (LillySimeonova) Public Domain 2.7.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for these models as well. In addition, we also combined the additional text counts features calculated in the LogisticRegression model and combined them with the text features in a bid to improve model performance on the test set. 2.7.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for Bidirectional LSTM model & Bidirectional GRU. 2.7.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU/LSTM layers with units specifying the dimensionality of the output space. We are then combining text features with count features using multiple input models. All the other layers are added exactly like GRU/LSTM models already discussed in section 2.5 & 2.4. Combining of text features with non-text was coded by referring (Freischlag) We compile the model using the binary cross-entropy loss function since it predicts a binary value and Adam optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.7.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.7.5 Model Checkpointing Public Domain Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.7.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_Bidirectional.txt is created. 3. Tweet Classification Models Usage This Section describes steps for running different Tweet Classification Models which are described in Section 2. 3.1 BERT Google Colab is preferred for running BERT model. We have used Colab for training and evaluating test dataset. 3.1.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose BERTSeq.ipynb 3.1.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files. 3.1.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.1.4 Result Predicted values are stored in answer_BERTSeq.txt, we can see the file in Files section Colab. Public Domain 3.2 SVM SVM model can be run locally. We have used python language. 3.2.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/CS410Fall2020/ClassificationCompetition/tree/main/data to your local path under data folder. 3.2.2 SVM code Load the SVM.py code in your local path 3.2.3 Prerequisites Ensure to download and install the below libraries and modules to run the code to not throw any errors 3.2.4 Run Run SVM.py code in your local. 3.2.5 Result Predicted values are stored in your local path as 'answer_SVM.txt' 3.3 CNN CNN model can be run locally. We have python language. 3.3.1 Load Training and Test data Public Domain First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.3.2 CNN code Load the CNN.py code in your local path 3.3.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.3.4 Run Run CNN.py code in your local. 3.3.5 Result Predicted values are stored in your local path as 'answer_CNN.txt' 3.4 LSTM LSTM model can be run locally. We have used python language. 3.4.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.4.2 LSTM code Jupyter notebook LSTM.ipynb . 3.4.3 Prerequisites Public Domain Please download Glove embedding file glove.6B.100d.txt ( it is available under glove.6B.zip) and place it in data folder. We can download Glove embedding file from https://nlp.stanford.edu/projects/glove/ Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip 3.4.4 Run Open the Jupyter notebook LSTM.ipynb and click Kernel Restart and Run All. 3.4.5 Result Predicted values are stored in your local path as 'answer_LSTM.txt' 3.5 GRU GRU model can be run locally. We have used python language. 3.5.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.5.2 GRU code Load the GRU.py code in your local path 3.5.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors Public Domain 3.5.4 Run Run GRU.py code in your local. 3.5.5 Result Predicted values are stored in your local path as 'answer_GRU.txt' 3.6 Naive Bayes and Logistic Regression Google Colab is preferred for running Naive Bayes and Logistic Regression model. We have used Colab for training and evaluating test dataset. 3.6.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose LGNB_CV.ipynb 3.6.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files from https://github.com/gnsandeep/CourseProject/tree/main/data. Public Domain 3.6.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.6.4 Result Predicted values are stored in answerNB.txt and answerCVLG.txt, we can see the file in Files section Colab. 3.7 Bi-directional Models (GRU & LSTM) Bi-directional models can be run locally. We have used python language. 3.7.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.7.2 Bi-directional model code Load the BidirectionalModels.py code in your local path 3.7.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors Public Domain 3.7.4 Run Run BidirectionalModels.py code in your local. 3.7.5 Result Predicted values are stored in your local path as 'answer_blstm.txt' & answer_bgru.txt 4. Results Below are some of the results that were captured in leaderboard live data lab for individual model: Public Domain Model Precision Recall F1 Base Line 0.723 0.723 0.723 BERT 0.7181913774973712 0.7588888888888888 0.7379794705564559 Logistic Regression 0.6593406593406593 0.8 0.7228915662650602 Naive Bayes 0.6177215189873417 0.8133333333333334 0.702158273381295 SVM 0.6225716928769658 0.7477777777777778 0.6794548207975769 CNN 0.6376811594202898 0.6844444444444444 0.660235798499464 LSTM 0.620242214532872 0.7966666666666666 0.6974708171206226 GRU 0.5968109339407744 0.8733333333333333 0.7090663058186738 Bidirectional- GRU 0.6535819430814525 0.74 0.69411151641479 Bidirectional -LSTM 0.639927073837739 0.78 0.703054581872809 Screen shot for BERT 5. Collaboration Brief description of contribution of each team member in case of a multi-person team. We had frequent meetings, we discussed about the models we had learnt, built, tested and results. We also did code reviews, incorporated suggestions. Sandeep: 1. Build and test of LSTM on word embeddings using glove. 2. Build and test of Naive Bayes and Logistic Regression. 3. Build and test of BERT. 4. Documentation for BERT, Naive Bayes and Logistic Regression models. 5. Bidirectional LSTM along with combining Text & Non-Text features. 6. Voice over project presentation and demo. 7. Investigation & Test of Bi-directional LSTM model with multiple input features. Sunitha: 1. Build and test of GRU on word embeddings using word2vec and glove. 2. Investigation & Test of Bi-directional GRU model with multiple input features. 3. Implementation for additional preprocessing steps to convert emojis and emoticons to text. 4. Implementation of ModelCheckpoint ,ReduceLROnPlateau. Public Domain 5. Test effects of additional emoji pre-processing, EarlyStopping, ModelCheckpointing & ReduceLROnPlateau on GRU, LSTM & CNN models. 6. Documentation for GRU & Bi-directional Models. Valentina: 1. Creating word vectors by word2vec method, create weight matrix from word2vec gensim model, getting embedding vectors from word2vec and using it as weights of non-trainable keras embedding layer. 2. Build and test of SVM model using cross validation and grid search to find good hyperparameters by building a pipeline. 3. Implementation of early stopping - Build network and train it until validation loss reduces. 4. Build and test of CNN on word embeddings using word2vec and glove. 5. Convert test and train features to InputFeatures that BERT understands, create model using pooled output, layer for tuning, dropout, labels conversion to one hot encoding; and get predictions. 6. Investigation & Test of LSTM model with word2vec, glove and different parameters. 7. Documentation for project report, documentation overview, SVM, CNN and LSTM models. 6. Presentation Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7-b1b43508023f References [Online] / auth. Scikit Learn. - http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html. https://mccormickml.com/2019/07/22/BERT-fine-tuning/ [Online] / auth. McCormick. Combining numerical and text features in deep neural networks [Online] / auth. Freischlag Christian. - https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4. https://github.com/ibrahimcelenli/cnn-word2vec-tweets-classification / auth. Celeni Ibrahim. https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation [Online] / auth. Paletto J.D.. https://www.kaggle.com/lbronchal/sentiment-analysis-with-svm [Online] / auth. Bronchal Luis. Public Domain Sarcasm Detection using LSTM, GRU, (85% Accuracy) [Online] / auth. Kohli Nikhil. - https://www.kaggle.com/nikhilkohli/sarcasm-detection-using-lstm-gru-85-accuracy. Sentiment Analysis in Python with keras and LSTM [Online] / auth. nana roblex. - https://www.kaggle.com/roblexnana/sentiment-analysis-with-keras-and-lstm. Sentiment Analysis with Bidirectional LSTM [Online] / auth. LillySimeonova. - https://www.kaggle.com/liliasimeonova/sentiment-analysis-with-bidirectional-lstm. Sentiment Analysis with Text Mining [Online] / auth. Carremans Bert. - https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27. PROJECT PROGRESS REPORT Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 1. Progress made so far Models Tried: * Logistic Regression * Naive Bayes * LSTM (Long-short term memory) * GRU (Gated recurring units) * CNN (Convolutional Neural Network) * SVM (Support Vector Machine) Pre-processing & Feature extraction: * Have tried word embeddings like glove and word2vector on the models: LSTM, GRU and CNN. * New features like word counts, emojis, hash tags were extracted and using for Logistic Regression and Naive bayes. * Both test and training set data was initially processed to remove html tags, punctuations, numbers, single characters, multiple spaces. * Models were trained and fitted using the given twitter trained set data by splitting it into training and test set. Further the model was applied on the test data set to find out the precision, recall and F-Score. More Details about models tried: * Logistic Regression: Logistic Regression model performed well on the test set data and gave precision of 0.658, recall of 0.808 and F-score of 0.725. Were able to beat the baseline using the LR model. * Naive Bayes: Naive Bayes did well on test data and gave F-Score of 0.702 * Neural Network Models - LSTM, GRU & CNN: We tried 3 neural network models LSTM, GRU, CNN with different combinations of hyper parameters. The maximum F-score obtained in all these neural network methods with different combinations of hyper parameters was 0.69. We tried word embedding with glove and word2vec but got almost the same results and not able to beat the baseline. Also tried without Glove and word2vec by training on all the words, but that model performed well only in local tests and did poorly in live data lab. * SVM: For SVM, after using cross validation and grid search to find the good hyperparameter for SVM model, still the model could not beat the baseline with the test set data. Got an F-score of around 0.68. 2. Remaining tasks * We have extracted additional features such as word counts, number of has tags, number of emojis in each of the tweet and used them in Logistic Regression. We are planning to use these features in Neural Network Models. * Project documentation and presentation. * We are looking at adding additional pre-processing steps like converting emojis and emoticons to text. 3. Any challenges/issues being faced * Unable to beat the baseline (i.e., F-score of 0.723) in live lab leaderboard using LSTM, GRU and CNN. * For neural network models like LSTM and GRU, overfitting is a problem as we get very good results on the graded test set in some cases but are not able to replicate the results on the leaderboard. * Some models took very long time to train based on the hyper-parameters chosen."
https://github.com/gotplt/CourseProject	"ManBearPig:FinalReportspirosthanasoulasst19@illinois.eduDecember15,2020DescriptionThemanbearpigprojectisanattempttounderstandpartsofthemandocsystemandtolaythefoundationsforfulltextsearchinit.Mandocisasetoftoolsthatdisplayandindexmanormdoc lestousersofUNIXsystems.Themainpurposeofthese lesaretoprovidedocumentationforcommands,APIs,systemcomponentsetcinaconsistentwaytotheusers.TheMdoclanguageanditshistoryUNIXmanualpagesoriginallywerewritteninalanguagecalledro whichitselfwasadescendantofanevenearliersystemcalledRUNOFForiginallywrittenbyJerrySaltzerfortheCompatibleTimeSharingSystemaround1964.Thatlanguageprovidedmacrosthatcontrolledthetypesettingoftext,like.CENTERforcenteringor.brforlinebreaking.Around1970itwasrewrittenasro bydougmcIllroyandbobmorris,andthenitwasportedtotheUNIXsystembykenthompson.Thereitwas rstusedfordocumentingtheaspectsofthesystemandsincethenithasremainedthepreferredwayforperformingthesetasks.TheGNUsystemhasalsobroughtforwardtheinfosystemwhichissimilar,butithasn'tgainedwidespreadadoption.Mdocoriginallyappearedasatro macropackagein4.4BSD.Itwasthensigni cantlyupdatedbyWernerLembergandRuslanErmilovingro -1.17.ThestandaloneimplementationthatispartofthemandocutilityweuseforthisprojectwrittenbyKristapsDzonsonsanditappearedinOpenBSD4.6.Mdocallowsthesemanticannotationofwordsandphrases,andalsosupportsdocumenthyperlinking.Inanmdocdocument,linesbeginningwiththecontrolcharacter`.'arecalled\macrolines"".The rstwordisthemacroname.Itconsistsoftwoorthreeletters.Mostmacronamesbeginwithacapitalletter.Thewordsfollowingthemacronameareargumentstothemacro,optionallyincludingthenamesofother,callablemacros.Linesnotbeginningwiththecontrolcharacterarecalled\textlines"".Theyprovidefree-formtexttobeprintedtheformattingofthetextdependsontherespectiveprocessingcontextwhichiscontrolledbytheparentmacro.Anexamplesomemdoctestfora ctionalutlitycalled""progname""couldbeasfollows.DdMdocdate.DtPROGNAMEsection.Os.ShNAME.Nmprogname.Ndonelineaboutwhatitdoes..ShLIBRARY.Forsections2,3,and9only..NotusedinOpenBSD..ShSYNOPSIS.Nmprogname.OpFloptions.Ar.ShDESCRIPTIONThe.Nmutilityprocessesfiles...SearchingmdocsTraditionallythemanualpagescanbesearchedwithacommandnamesapropos(originatingfromthefrenchexpressionapropos,whichmeans""about"").Alsotomakethingsevenmoreconfusing,anotherwaytosearchmanualpageshasbeeninvokingthemancommandwitha-k(keyword) ag.Inthemandocsystemalthoughthissyntaxexistsforcompatibility,itjustinvokestheaproposcommandonthebackend.ThisisapointwhereacrossUNIXsystems,thingscanbegintodivergegreatly.Inthemandocsystemthatweareexaminingtheaproposandwhatisutilitiesquerymanualpagedatabasesgeneratedbythemakewhatis1command,Bydefault,apropossearchesformakewhatisdatabasesusingcase-insensitiveextendedregularexpressionmatchingovermanualnamesanddescriptions(the.Nmand.Ndmacrokeys).InthemandocsystemthesedatabasesarebasicallyhashtablesbasedontheohashopenhashinghelperfunctionswrittenoriginallybyMarcEspieforOpenBSD.OnotherimplementationsthoughliketheGNUonesinsomelinuxdistributions(again,thismeansnotmandoc,butcompletelydi erentmanualpagesystems,whicharepresentedherejustforreference)thedatabasesareimplementeddi erently.Belowaretheoptionsforthesystemshippedwithdebianlinuxofthemandbdatabaseformatsandhowitcompareswithmandocintermsofasyncaccess,databasenamingandbackend.NameTypeAsyncFilenamemandocdbHashed(ohash)Yessection/arch/title.seciondebianman/BerkeleydbBinarytreeYesindex.btdebianman/GNUgdbmHashedYesindex.dbdebianman/UNIXndbmHashedNoindex.(dir|pag)Aswehaveseenthecurrentsearchfunctionalityforallsystemsallowsthequeryofveryspeci ckeywordsinveryspeci cpartsofthedocument.Mandocgreatlyimprovedthestateoftheartwhenitappearedbecauseitalsoallowedcertainsemanticsearchcapabilities.Forexampleyoucouldperformand/oroperationsondi erentmacrostore neyoursearchresult.Butstilltheresultwouldbejustthemanualpagenameandsection,andthekeywordwouldhavetobeinaneasilyindexablepartofthemdoc,becauseasitcanbeseenfromtheexampleabove,thefreetextisintermixedwithtypesettinginformation.SmallstepsforwardExtractingtextOnegoodadditiontothesystemwouldbetoenablefulltextsearch.Todosowe rsthavetoextractthetextinasapureformaswecan,andthensomehowindexit.Forthistaskasmallutlitywaswritten(whichborrowsheavilyfromthedemandoccommand)toextractrelevanttextfromanmdocpage.Thecurrentresultisfarfromperfectasitneedstomakedecisionsabout,spaces,linebrakes,capitalizationetc,butitstillachievesthegoalforthemostpart.Itisabletoextracttextwithoutformattingmacros.Therelevantcodeforthislivesundercode/extracttext.c,anditworksbyrecursivelyparsingthemdocstructures(theycanbeembedded)inordertooutputonlythewordsthatarenotlanguagetokens.matchingusingtrigramsSincethemakewhatisdatabaseisalreadyinahashtableformat,itwouldmakesensetochoosearepresen-tationthatmapswelltothatbackendifitistoeverbemergedinthemaincodebase.Wewanttheusertobeabletoenterasmallsetofwordsandtofetchtheresultsofthemanualpagethistextsequenceexistsin.Alsowewouldlikefortheusertohavetheabilityto ndamatchinglineoftextmidsentence.Considerforexampleapartofamanualpagestatingthat""Amanualpageconsistsofseveralsections."".Ifweonlykeptaninvertedindexofwordsandouruserwantedtolookforthestring""ageconsist"",oursystemcouldleadhimquiteastraysincenoneofthewordspageandconsistsmatchthequeryproperly.Thereforeasdiscussedbeforeintheproposaldocuments,wewillfollowtheapproachthatRussCoxusedwhileimple-mentingthebackendforgooglecodesearch,whichconsistsofsplittingthetextintri-gramsandstoringtheiroccurences.Underthetrigramtransformationtheword""word""createsthesetofthefollowingtrigramsw,wo,wor,ord,rd,d.Anexampleprogramthatperforsthistransformationonitsargumentscanbefoundincode/wordstotrigrams.c.RunningtheexamplecodeUnderthedirectorycode/thereisaMake lethatbuildsthetwobinaries.Underthedirectorycode/input/existsomesample lesforinputtothetestprograms.Forthecodetobecompiledthemandocsourcecode2shouldbecompiledandexistingatthesamelevelofdirectoryasthecourseprojectcode.Belowaresomesamplerunsofthetwoprovidedbinaries.>cdcode;>make;cccI../../mandoc/extracttext.cccextracttext.oL../../mandoc/L/lib/x8664linuxgnu/lmandoclzoextracttextccowordstotrigramswordstotrigrams.c>./extracttextinput/apropos.1;operatingonfileinput/apropos.1OctoberAPROPOSNAMEaproposwhatissearchmanualpagedatabasesSYNOPSISaproposafkfilepathpathoutkeyarchsectionexpressionDESCRIPTIONTheaproposandwhatisutilitiesquerymanualpagedatabasesgeneratedbymakewhatisevaluatingexpressionforeachfileineachdatabaseBydefaulttheydisplaythenamessectionnumbersanddescriptionlinesofallmatchingmanualsBydefaultapropossearchesformakewhatisdatabasesinthedefaultpathsstipulatedbymanandusescaseinsensitiveextendedregularexpressionmatchingovermanualnamesanddescriptionstheandmacrokeysMultipletermsimplypairwisewhatisissynonymforaproposTheoptionsareasfollows:InsteadofshowingonlythetitlelinesshowthecompletemanualpagesjustlikemanwouldIfthestandardoutputisterminaldeviceandisnotnldotsnn>./wordstotrigramsthequickbrownfoxjumpedoverthelazydogarg:thetrigrams[t][th][the][he][e]arg:quicktrigrams[q][qu][qui][uic][ick][ck][k]arg:browntrigrams[b][br][bro][row][own][wn][n]arg:foxtrigrams[f][fo][fox][ox][x]arg:jumpedtrigrams[j][ju][jum][ump][mpe][ped][ed][d]arg:overtrigrams[o][ov][ove][ver][er][r]arg:thetrigrams[t][th][the][he][e]arg:lazytrigrams[l][la][laz][azy][zy][y]arg:dogtrigrams[d][do][dog][og][g]NextstepsSincewenowhaveseenthatwecansuccesfullyextractwordsfromthemdocformatandgeneratetrigramsforwordswewouldneedtocreateahashtableusingtheohashfunctionsthatwillresembletheexistingmakewhatisdatabases.Currentlythevaluesofthesehashesarejustthenameandthesectionofthemanualpage,butsincewecangivetheabilitytomatchanywhereinapage,itwouldalsomakesensetoatleastprovidea lepointertobeginningoftheactualtext.Thiswouldbeaveryhardproblemsincewecannotcorrelatewheresomethingisgoingtoberenderedasaposition,towherethetextisonthemdocdocument.Thereforeitwouldmakemoresensetojustgivethenameofthepagebackandthenuseoursystem'spager(less/moreetc)tonavigatetothatexactstring.3 ProgressReportforManBearPigspirosthanasoulasst19@illinois.eduNovember28,2020StatusAsoutlinedintheprojectproposalthegoalofthisprojectistodocumentthecurrentstatusandlaythefoundationforprovidingfulltextsearchcapabilitiestothemandocsystem.Mandocisamodernimplementationofamanualpagesystemhttps://mandoc.bsd.lvandpartoftheprojectistoidentifyitscurrentsearchcapabilities(howthecommandapropos/man-kwithakeywordworks)andprovideadesignandpartsofanimplementationforafulltextsearchoptiononit.ProgressmadesofarIhaveanalyzedthecurrentwaythataproposacceptssearchtermsandpassesthemtothecurrentdatabaseimplementation.Alsoihavedesignedaninitialalgorithmbasedonn-gramsandamatchingdatabaseformatthatcouldreplace(orgosidebyside)withthecurrentdatabaseinordertoprovidesomebasicfulltextsearchcapabilities.Ihavealsowrittenascraperwhichcanconsumethemandoclanguageandextractthewordsthataretobefedtothedatabasebuilder.RemainingtasksActuallyimplementadatabaseformatthatconformstotheapropossearchapi.Connectittotheaproposcodeandissuequeries.Writeteststomakesurethatthewordsthathavebeeninputedhavematchingn-gramsinthecorrectformatinthebinarydatabase.WritethereportthatdescribestheworkdoneChallengesfacedBeingalargecodebaseitrequiresmanyhoursoffamiliriazingwithapartofitbeforeyoucanstartmakingsenseofhowtointerfacewithit.AlsobeingapureCproject,alwaysrequirestreadingcarefullywiththingslikememorymanagement,IOetcsotheprogressdoneisslowerthanitwouldbeinacompletelydynamic/scriptinglanguageworkingwithasmallerframework.membersst19/soloproject1 ManBearPig.Anattemptatprovidingnativefulltextsearchtothemandocsystem.spirosthanasoulasst19@illinois.eduNovember2,2020DescriptionThegoalofthisprojectwouldbetocreateareportandpossiblycodeimprovementstowardsprovidingabackendthatsupportsFullTextSearchcapabilitiesforthemandocproject(https://mandoc.bsd.lv/).BackgroundUNIXsystemprovidetheirdocumentationtotheuserthroughasetoftoolscollectivelyreferredtoastheManualPagesystem.Thewellknownman(1)commandexiststodayonallUNIXsystemsbutevenonotherplatformslikeMacOSXandandroid.Searchingecientlykeywordsandsemanticshasbeenofparamountimportancefortheusertoquicklygettotherelevantmanualpageandthecommandapropos(1)traditionallyservedthatpurpose,meaningdoingdatabaselookups.Thedatabasesarebuiltwiththemakewhatis(1)tool.ProjectproposalWewillinvestigatetheCsourcecodeofthemandocproject,targetedonthemodulesofsearchinganddatabasecreation.Thegoalofthisprojectwouldbetolayapathforfulltextsearchcapabilitiesfromtheaproposcommand.Currentlyonlycertainwordsofamanualpageareindexedandtheirsemanticinformationstoredwiththem,inapersistedtoa ledatabasethatontheouterlevelisimplementedasahashmap.Toallowforthefulltextsearchcapabilitieswewillimplementadatabasebasedontrigramskeyinganinvertedindexofthefulltextbeingcontainedinamanualpage,afterithasbeenparsedfromthemdocparsersandonlythecontentremains.Indetailthegoaloftheprojectwouldbetocreatetheequivalentdatabaseofthemakewhatis(1)dbthatiscurrentlycreated,butwhichstoresthetrigrams.Duetolackoftimenooptimizationsforverylargedatabasesaregoingtobeimplementedandthetestinginputwillbeconstrainedenoughtomakesurethedatastructureswillbeableto tinmemory.Thegenerateddatabasewillbeevaluatedbydumpingthecontentsandmakingsureallthetrigramsthatshouldbeproducedandonlythosearecontainedwithinit.Atestharnesstoensurethatwillbeprovided.Iftimepermitsthesearchcapabilitieswillbeattemptedtoconnecttothedatabasethroughtheaproposcommandandqueryforatextstring.notetothereviewer:althoughiwouldloveto nishthewholethingbutitmightbeunfeasableinaround25hrsthatihavebudgeteditforit.Myintentionthoughistolaythefoundationsothatapatchwillbeeventuallymergedinthemandoccodebase,nottodemosomethingthatnoonewilleveruserProposedWork owWeproposethattheanalysisanddevelopmentwillbesplitacross56hrman-daysofworkDay1CodeandDocumentationanalysis.Reviewingthemakewhatisutilityandtheresultingdatabasesitcreatesexaminetherelevantcode owand ndwheretopluginthenewfunctionality.Day2Designofthebinary leformatthatwillstoretothetrigramsdatastructure,aswellastheparsingfunctionstoextractthem.Day3DevelopmentandDocumentationDay4Development,DocumentationandtestharnessDay5Finalreport1membersst19/soloproject2 CourseProject for st19 ManBearPig. An attempt at providing Full Text Search to mandoc The goal of this project would be to attempt an implementation of a trigram based full text search database for the mandoc project (https://mandoc.bsd.lv/) Background UNIX system provide their documentation to the user through a set of tools collectively referred to as the Manual Page system. The well known man(1) command exists today on all UNIX systems but even on other platforms like MacOSX and android. Searching efficiently keywords and semantics has been of paramount importance for the user to quickly get to the relevant manual page and the command apropos(1) traditionally served that purpose, meaning doing database lookups. The database are built with the makewhatis(1) tool. Project proposal We will investigate the C source code of the mandoc project, targeted on the modules of searching and database building. We will understand how it works and how to extract manual page text using the mdoc library functions. Then we shall create a simplified trigram database of an an inverted index with the goal to connect it to the search capabilities. Final report and code You can read the final report in the pdf , and see the provided final-presentation.mp4 To compile the mandoc source code you can fetch it from https://mandoc.bsd.lv/ and compile it at the same directory level as this project. members st19 / solo project"
https://github.com/hc2111/CourseProject	Progress Report: 1. Progress made thus far a.Implementation of classifier and training on google cloud. 2. Remaining tasks, a.Improve model b.Complete Submission Documentation c.Complete Project Demo Vid 3. Any challenges/issues being faced. a.N/A 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a.Harry Chen => NetID: hchen223 => Captain 2.Which competition do you plan to join? a.Text Classification 3.If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods a.Yes, I am prepared to learn state of the art neural network classifiers. Some classifiers and frameworks I have heard of include SVM, perceptron network, Logistic regression classifiers, Deep neutral neural network classifier. I've has experiences with svm's in the past and look forward to learning more about other classifiers. 4.Which programming language do you plan to use? a.I plan to use python CourseProject Explain your model For the competition I used the BERT(Bidirectional Encoder Representations from Transformers) model. This model is a cutting edge model for classification. BERT's main innovation is applying the bidirectional training of Transformer, a popular attention model, to language modeling. How I performed the training: Since BERT training on CPU was incredably slow, I utilized a google cloud VM to train the model on the gpu, this enabled much faster training speeds and more experimentation with parameters and tuning. Experiments with other methods: Prior to using BERT, I tried to create my own model and tunings, however they failed to come close to the baseline, so I expanded my options and opted to utilize BERT. How to run: Download the Data folder from this link: https://drive.google.com/file/d/1OqLtj9BTnob45huOsFN_fMN23hL_WrGi/view?usp=sharing and the jupyter notebook, and then run all the cells. demo video: https://drive.google.com/file/d/1LIkJRzyLRYKK2roMzF5CP208Hlj24ehw/view?usp=sharing
https://github.com/henryg3/CourseProject	Henry Guan (henryg3), Kevin Yu (yuey8) CS 410 Project Progress Report - Improving a System (EducationalWeb) 11/29/2020 1.So far, we have made some progress on our project itself; we believe we are close to finishing the bulk slide downloader. 2.Remaining tasks we have to complete are finishing the bulk downloader, scaling up the system, and improving the UI where necessary. 3.Challenges we are currently facing are that although the bulk downloader works, it only downloads the first 10 slides of each weekly course lecture. Additionally, there is a lot of code to navigate (220,000+ lines of code in one of the files), so it takes a while to figure out where we can implement what we proposed in our project proposal. In order to run and test our code: Please read the README for our codebase. Video link: https://www.youtube.com/watch?v=bV4k16nsyQY&feature=youtu.be Scaling up the system: This allows for more courses to be added to EducationalWeb, and this is done by modifying model.py within the codebase, specifically, modifying the functions that take in the list of courses that are passed into EducationalWeb, as well as being able to correctly parse and sort slides. Bulk-Download: This utility allows the user to download up to five pdf slides at a time compared to one pdf slide before. We implemented additional functions on top of the original download in pdf.js. We first connected our function and button correctly onto the event bus, and for each time the bulk-download button is clicked, the current lecture page index is parsed as a parameter, and we are able to download the next few slides using that index. Since the pdf data is loaded one at a time, the bulk-download feature uses downloadByUrl instead of regular download by data. Multiple-pages skip: This utility allows the user to skip five pages at a time(next or prev), and makes the process of bulk-downloading much easier. The frontend of this utility is implemented in slide.html, and includes simply two buttons and their on-click events. The backend is implemented in model.py and is built on top of the original next and prev functionalities. EducationalWeb System Project Proposal Questions 1.yuey8, henryg3; the captain is henryg3 2.The system we have chosen is the EducationalWeb System. The subtopic(s) we have chosen under the system is to allow downloading slides in bulk and scaling up the current system. 3.For bulk downloading, we plan to implement a web bulk downloader on the current website. And to scale up the system, we plan to obtain datas from platforms such as coursera or UIUC courses using web crawling techniques. 4.We will demonstrate that our function works as expected by showing how the base system will perform without this function, versus the functionality of the system after the functions are implemented. 5.Our code will utilize the system by adding more contents from different sources and enables bulk downloading. 6.We are planning to use Python and JavaScript. 7.The workload of our topic is at least 40 hours (2 people in our team). a.Understanding the EducationalWeb System: approximately 1~2 hours b.Researching about our techniques 3~4 hours c.Bulk downloading: approximately 12~15 hours d.Implementing web crawling techniques to scale up the system: 24~27 hours Video Link: https://www.youtube.com/watch?v=bV4k16nsyQY&feature=youtu.be How to run our code: Please git clone this repository to whichever directory you'd like. You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/
https://github.com/hernang2/CourseProject	"Documentation Overview This crawler is used to mine faculty university faculties and generate a ""json"" file with an entry per professor. This entry will have the following fields: faculty, url, location, email, name, top_terms and bio. The purpose of this was to feed the Expert Search app with structured text that had additional information but couldn't fit the data to work with the MeTA corpus types. It can be used to gather info on professors, including the top five terms related to their bios. Implementation The code borrows from MP2.1 to crawl through faculties' professor profiles. We give an array of faculty home pages and from there the script tries to infer what links to professor profiles we have in it. Then using nltk's word_tokenize we determine the words with the highest frequencies in the bios text, by counting frequency of words and normalizing by max frequency. After that we use a ""heuristic"" with the aid of BeautifulSoup and regex to try to gather the email, facultie and name of the professor. I couldn't implement location as I didn't setup the maps api that was used in the Expert Search's get_location.py script. At the end, it dumps a json with the faculties data in a file called ""bios_json.txt"" under the sample folder, alongside the main.py Usage We assume a machine running python 3.5 using the pip installer The script that needs to run is main To run this script you need to install the following packages: * bs4: pip install beautifulsoup4 * selenium: pip install -U selenium * nltk: pip install nltk For selenium we assume you're testing with the firefox dirver. If not, get the driver for your browser and use it in the constructor on main.py line 181. Some browsers might get tricky. To specify which university faculties you want to crawl, modify dir_url in main.py line 185 ExpertSearch - Extracting relevant information As stated by the project topics document, the goal of this project is to enhance the ExpertSearch app (https://github.com/CS410Fall2020/ExpertSearch/) to convert the unstructured text in faculty webpages into more structured text, so that we can more accurately extract email and faculty names from the faculty member bios crawled data. Progress -Setup a vm running a Linux distro and set it up with Python 2.7 -Get code from git repo -Run up with gunicorn and necessary packages on localhost -Started modifying the code around the ranker for the structured text update Remaining tasks -Determine best text structure -Update results structure in server.py's search method to use this new structure -Push code to this repo -Host website on the cloud Challenges -This App is made to run using Python 2.7 on Linux. Had some issues setting up the vm I'm using to work on and getting sure python, pip, gunicorn and other packages were installed and running using the correct version -I can see potential issues related to python 2.7 when deploying to the cloud, but Python 2.7 still has support until next year, enough to deliver the project. ExpertSearch - Extracting relevant information This is the final project for the UIUC Text Information Systems course. For a visual walkthrough you can use the following links: You can find the video about how to instal and run: https://mediaspace.illinois.edu/media/1_sktdzqnu You can find the video about how the code works: https://mediaspace.illinois.edu/media/1_ht2q1bdw To view presentation go to: https://1drv.ms/u/s!AsAuk2iSocrzkKROEBv6Q6XLC0_Rbw?e=8fbitD # Overview The puropose of this project is to be able to crawl through faculty pages and gather info on professors that gets stored in a structured data format. This structured that would have the fields that the Expert Search system currently has, like email, name and faculty, but additionally I'm trying to gather top terms that can be used to give a better idea of the bio's expertise. # Implementation To gather the top terms I used the nltk library to tokenize the professor's bio. Then I calculate the maximum frequency and normalize the counts of all the terms using this figure. After giltering for stopwords and single characters, I order by frequency and take the first five elements. bio = visible_text.strip() stopwords = nltk.corpus.stopwords.words('english') word_frequencies = {} for word in nltk.word_tokenize(bio): if word not in stopwords: if word not in word_frequencies.keys(): word_frequencies[word] = 1 else: word_frequencies[word] += 1 max_frequency = max(word_frequencies.values()) for word in word_frequencies.keys(): word_frequencies[word] = (word_frequencies[word] / max_frequency) word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True) top_terms = list(k[0] for k in word_frequencies if len(k[0]) > 1)[:5] To generate the json, I crawled through faculty pages from MP2.1's sheet looking for hyperlinks. If the hyperlink has content that resembles a name and a valid href it starts the mining. It asumes the name is nearby. It browses to the url, gathers all non-html text and does a little format as the bio. It looks for some types of tags and looks top see if they match an email regex, or departments after tokenizing and filtering for small sentences that include at least one word related to the description of a department. After that it collects everything in a json (for this code look at Crawler/sample/main.py line 113) # Limitations The professor gathering algorithm is a hit and miss, because it has many false positives, like Student Info, Academic Resources, etc. I didn't compare it against a database of common names (and I guess someone could be named Academic Resources). Maybe main failure was connecting it with the Expert Search App, as it depends of some certain valid formats that the corpus that gets fed to MeTApy. I couldn't make it work of Json, or strip MeTApy entirely an replce the ranking code with a different library, like nltk."
https://github.com/hetadesai26/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/houyuan2/CS410CourseProject	"If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Yuechen Liu yuechen7 Houyuan Sha houyuan2 Ruobin Wang ruobinw2 Tianren Zhou tianren2 captain 2.What system have you chosen? Which subtopic(s) under the system? Improving A System: Expert search system 3.Briefly describe the datasets, algorithms or techniques you plan to use For the dataset, we can obtain the current collection of faculty pages dataset from the course TA. We plan to use ElasticSearch to implement our search engine. https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html We will import the dataset to ElasticSearch and implement corresponding unstructured queries to obtain the relevant result. We will implement a python backend and html frontend, which should provide an accessible interface for the user. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We attempt to provide an alternative method to query the existing expert dataset using elastic search. In doing so, we attempt to achieve better accuracy and speed. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We plan to implement APIs in the python server. The frontend will communicate with backend using the API. 6.Which programming language do you plan to use? Python, reactjs, js 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Total 20*4 = 80 hours *Frontend (30 hours) *Backend - api (20 hours) *Backend - db, search engine (30 hours) At the final stage of your project, you need to deliver the following: *Your documented source code. *A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Progress thus far: 1.Deployed elastic search API and the visualization tool Kibana in Docker containers 2.Extracted email address from bios using regex 3.Populated the elastic search database with url, bio, and email data 4.Built the basic front end that is able for API and backend to test queries including search bars and outputs Remaining Tasks: 1.Build a clear front end that is enough to show results properly 2.Extend more sophisticated search using ElasticSearch 3.Filter unrelated information given certain keywords 4.Enhance performance of original ranking system of ExpertSearch Challenges: 1.Extracting faculty names from bios is challenging. Plan to resolve this via Python Spacy library 2.Query in ElasticSearch is new to us. We need to learn more about how to write queries in this engine. Video Presentation https://www.youtube.com/watch?v=TLylfLThiGk Setup install Docker (https://www.docker.com/get-started, install Docker Desktop) install python3 (at least 3.7) in project folder, run the following in terminal: pip3 install flask\ pip3 install regex\ pip3 install elasticsearch\ pip3 install spacy\ python3 -m spacy download en\ docker-compose up -d wait 3 minutes for docker containers to be set up in terminal: python3 setup.py wait for all index to be created, this could take up to 3 hours, due to NLP and indexing Server in terminal: python3 server.py navigate to http://127.0.0.1:5000/ Goal Our project aims to develop a search tool for experts in different fields. An user can enter a search phrase(e.g Data Mining) in the search bar and our application will return the likely results according to similarity between the bios of the expert and the search phrase. The user can inspect the validity of the result by accessing the homepage of the expert using returned URL. Furthermore, the user can contact the expert via the returned email. Overview Our project is based on elastic search, a document-orientated database that provides unstructured search functionality. The code base is divided to 3 parts. In ""setup.py"", the application would read the input text files to obtain faculty bios and the corresponding homepage URL. The application would then attempt to extract the faculty name and email from the bios using spacy, a NLP package. These information would be stored into the elastic search database. In ""server.py"", the application would query the database with the input typed in search bar, and return the corresponding results. Comparison Original Project: https://github.com/CS410Fall2020/ExpertSearch/ \ Compared to the original project, which used the Stanford model, our project took a different route. We used elastic search as text tokenization and ranking tool. We also used python3 spacy to extract email and name from faculty bios. We attempt to extract more names and emails from the input bios. We had more success with email, but name extact is less successful due to the limitation of NLP. Another difference is our search engine is key word based, which is more relaxed compared to the original project. Moreover, our project allows user to specify how many results to be returned. Finally, our front end is more fine-tuned. Contributions Houyuan Sha: Set up the docker container for elastic search; Extract name and email from faculty bios; Store faculty information to elastic search database Yuechen Liu: Set up Flask; Write api connecting database and frontend Tianren Zhou: Write query and comments for future extension Ruobin Wang: Designed user interfaces; Set up front-end"
https://github.com/hpandeycodeit/CourseProject	"Project Proposal: CS 410 Topic: Sentiment Analysis Team Canyon What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - Amrutha Gujjar agujjar2@illinois.edu - Balakumar Balasubramaniam bbalasub@illinois.edu - Himanshu Pandey hpandey3@illinois.edu Himanshu Pandey is the Captain What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic Name: Sentiment Analysis The task is to capture the sentiments on the data. For eg: In current Presidential Campaign, we read the news through different channels including social media. Streaming the social media data like Twitter, can give a dataset on the topic ""Presidential Campaign"" and then we will process the data to find and evaluate the sentiments of the users on both the Presidential Campaign. Dataset: We will be using the twitter data for this project. Tools: Scrapping/Streaming the twitter data can be done through R or Python in Real Time. Evaluation: Precision/Recall Measures Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The task is divided into following sub tasks: - Data streaming and cleaning. We are planning on stream the data at different times from the Twitter and once we have 30-40k rows, we can start cleaning the data. 8 hours - Data cleaning and Analysis 6 - 8 hours - Writing the code to train the data 20 hours - Test and refine 8 - 10 hours - Improving the model accuracy 6 - 8 hours - Evaluation of the test results 6 - 8 hours - Generating charts 6 - 8 hours columns = [Author, Date, Tweet] filename nrows start_time end_time isRowOrderReversed twitter.csv 17720 2020-11-04 06:02:56 2020-11-04 06:24:23 Yes twitter_2.csv 2655 2020-11-04 06:38:39 2020-11-04 06:38:40 Yes RealTime.csv 186323 2020-11-04 07:54:55 2020-11-04 08:26:11 No RealTime2.csv 658280 2020-11-04 08:32:26 2020-11-04 10:16:56 No Project Progress Report: CS 410 Topic: Sentiment Analysis Team Canyon Which tasks have been completed? Following Tasks have been completed: - Data Streaming/Scrapping - Data Cleaning and - Analysis Data Streaming/Scrapping: We used the twitter to stream real time data during the presidential elections 2020. The raw data contains following three columns ""Author,Date,Tweet"" tweets with hashtags ""'Elections2020', 'ElectionNight', 'Elections', 'Trump', 'Biden'"" To real time stream data from Twitter, we have created an app in Twitter. Using ""tweepy"" library in Python and using the ""consumer_key"", ""consumer_secret"", ""access_token"", and ""access_token_secret"" from the app created in Twitter, we have streamed the live data during the event to fetch around 658280 Tweets. Data Cleaning: We used the MeTA analysis toolkit to clean the raw data that was scraped from Twitter. This involved using 'stemming' to treat base words as the same, in order to reduce the amount of noise in the analysis. Analysis: Current data is analyzed to create time-series of n-gram counts charts are plotted and embedded into html files. These Plots contain the top-20 most popular n-grams. We have computed for 2,3,4 and 5-grams. As per the proposal we have created the initial Sentiment Analysis for Presidential Elections 2020. Which tasks are pending? - Evaluation of the test results using Precision/Recall Measures are pending. - Need to test the generation of charts on a different dataset to test the streamline working of the project. Are you facing/faced any challenges? There is no cap on the amount of data that needs to be collected to generate the correct sentiments. In other words, when should we stop scrapping the data from Twitter? This was one of the challenges we faced while scrapping the data from Twitter. In addition to the above, the limitations from Twitter to the number of calls that can be made to collect the data. Data cleaning is an issue as all the data from twitter is not text. It includes images/gifs/smiles and videos too. Cleaning such data was not straight forward. Still need to integrate the Evaluation of results using Precision and Recall measures. We are hoping to complete this in time. About the project This project provides a general framework for capturing sentiment trends from streamed twitter data. We demonstrate our software by capturing the US 2020 election related tweets, in the morning after the election, and applying n-gram frequency trends, and PLSA. The framework is written to be such that it is easy to add new modules, and perform new analytics. Using this dataset, we show that n-gram analysis captures many of the prominent characteristics of the election -- including ""biden win"", ""claim victory"", and ""trump premature claim"". We then apply a sentiment analysis, and show that the positive sentiment towards trump decreases between 840-1000 AM, while biden's positive sentiment marginally increases. Finally, we do a PLSA analysis on the data to identify the top-10 topics that are of the greatest importance. We show that the PLSA analysis captures biden's win in georgia, importance of swing states pennsylvania, and wisconsin, and vote count stop related messages. Interestingly, the top topic turned out to be ""claims of a premature result"". This package can be used as a template for processing any other twitter data stream. DATA Collection Data was collected using the Twitter API and python code. The code ""twitter_data.py"" was run during the Presidential Election Night 2020 to collect the most relevant tweets that covered hashtags such as 'Elections2020', 'ElectionNight', 'Elections', 'Trump', 'Biden'. In all, we collected approximately 650K tweets. Data Cleaning We used metapy for initial clean up of the data. We used the MeTA analysis toolkit to clean the raw data that was scraped from Twitter. This involved using 'stemming' to treat base words as the same, in order to reduce the amount of noise in the analysis. One important aspect of this work is the necessity to remove ""emojis"". These contaminate the data, and create biases in the results. So, we removed any emojis with the text. DATA Analysis Current data is analyzed to create time-series of n-gram counts at 5 minute intervals. Clickable/zoomable charts are automatically generated, and embedded into html files. These Plots contain the top-20 most popular n-grams (for n=2,3,4,5). As per the proposal we have created the sentiment trend analysis results for the US presidential elections in 2020. Finally, PLSA analysis picks up the most salient topics. Requirements Check our requirements file for the required libraries and run pip install requirements.txt. Following are the required libraries for this project. metapy emoji pandas plotly textblob Run the project Clone the project and follow the steps below: Run python driver_twitter.py only. This python file does the following tasks: It takes the input ""RealTime2.csv"" file Cleans the Data by running ""data_cleaner.py"" After, cleaning runs ""ngram_analyzer.py"" to analyze ngram frequencies Calculates the sentiments by running ""aggregate_sentiment_analyzer.py"" Stores the charts/graphs in ""./figures/html/"" directory TOOLs/Languages Python Twitter API ## Team Amrutha\ Bala\ Himanshu ## Video Link Presentation"
https://github.com/icyguy64/CourseProject	"Text Classification Competition: Sarcasm Detection Final Report Chua Yeow Long, ylchua2@illinois.edu Introduction Inthiscompetition,wearerequiredtoperformsarcasmdetection/classificationoftwittertweets. Thedatasetconsistsofaresponsewhichisthetweettobeclassified,contextwhichisthe conversationcontextoftheresponseandthelabelaswellastheidforidentificationoftweets whenmakingsubmissions.Thegoalistopredictthelabelofthetweet""response""while optionallyusingcontext(i.e.,theimmediateorthefullcontext).Fromthesizeofthedataset whichis5,000fortrainingand1,800fortesting,transferlearningmodelswillneedtobeused instead of building neural network models from scratch. Installation of libraries I'll use jupyter notebooks to illustrate the workflow and we'll need NLTK for text pre-processing, Keras/TensorFlow to build our neural networks and standard sklearn libraries. To install the libraries, just do a pip install or conda install (in the case of anaconda). Overall Plan I'll first preprocess the text data by removing stop-words using NLTK's stop-words and non-alphabetic characters such as symbols before feeding the data into 3 different models. The first is to use a simple word occurrence/count model and the model did not work out very well as the model predicted not-sarcasm for all the test dataset. The second is to use GLoVe embedding and adding some LSTM and dropout layers and training just the newly added layers. A 85% train and 15% validation split was performed to obtain the validation dataset. For the input text data, I have tried with and without context and there was not much a difference in the results. The last and third method is to use BERT and re-train the entire BERT layers using the dataset on a pretty, I would say beefy machine. The best F1-score I have obtained is 0.7378 with context information and 0.716 without context which is pretty significant. I have explored using additional feature engineering and GPT-2/XLM models but given the time constraints, I was not able to obtain a better F1-score. pip install tensorflow-gpu pip install nltk pip install sklearn Code Walkthrough For this section, you can just refer to the accompanied jupyter notebook or the video presentation. Results I have not included the results for the CountVectorizer model as it predicted all 0's/Not-Sarcasm. The best F1-score I have obtained using GLoVe embedding together with LSTM layers was 0.661 both with and without context. The best F1-score I have obtained using BERT is 0.7378 with context information and 0.716 without context. The baseline required is 0.723. Conclusion It was a fun journey to be playing with the state-of-the-art in NLP, playing with BERT and GLoVe embeddings as well as GPT-2/XLM. Although I was able to surpass the baseline only using BERT, I believe with more feature engineering and tinkering with the models, the baseline should be achievable. Description F1-score GLoVe embedding with LSTM layers without context 0.661 GLoVe embedding with LSTM layers 0.661 BERT without context 0.716 BERT with context 0.7378 notebook_finalDecember12,20201TextClassificationCompetition:SarcasmDetectionChuaYeowLong,ylchua2@illinois.eduInthisnotebook,i'llfirsttryusingasimplewordcountmodelusingsklearn'sCountVectorizertoperformsarcasmdetection.Next,I'lluseGLoVeembeddingandaddneuralnetworklayerstowardstheend,trainingjusttheaddedlayersusingtheprovideddataset.Finally,I'llfinetuneBERTlayers,justfine-tuningtheentireBERTmodelwhichconsistsofhundredsofmillionsofparametersusingthetrainingdatasetwithaprettybeefymachine.We'llfirstneedtoimportthenecessarylibraries.[2]:importpandasaspdimportmatplotlib.pyplotaspltimportseabornassnsimportnumpyasnpfromnltk.corpusimportstopwordsfromnltk.utilimportngramsfromsklearn.feature_extraction.textimportCountVectorizerfromcollectionsimportdefaultdictfromcollectionsimportCounterplt.style.use('ggplot')stop=set(stopwords.words('english'))importrefromnltk.tokenizeimportword_tokenizeimportgensimimportstringfromkeras.preprocessing.textimportTokenizerfromkeras.preprocessing.sequenceimportpad_sequencesfromtqdmimporttqdmfromkeras.modelsimportSequentialfromkeras.layersimportEmbedding,LSTM,Dense,SpatialDropout1Dfromkeras.initializersimportConstantfromsklearn.model_selectionimporttrain_test_splitfromkeras.optimizersimportAdamWe'llmakeuseofpandas'read_jsonmoduletoloadthedataintoapandasdataframewherewe'llperformourwork1[3]:df=pd.read_json('data/train.jsonl',lines=True)[4]:df.head()[4]:labelresponse\0SARCASM@USER@USER@USERIdon'tgetthis..obviousl...1SARCASM@USER@USERtryingtoprotestabout.Talking...2SARCASM@USER@USER@USERHemakesaninsaneaboutof...3SARCASM@USER@USERMeanwhileTrumpwon'tevenrelease...4SARCASM@USER@USERPrettySuretheAnti-LincolnCrowd...context0[Aminorchilddeservesprivacyandshouldbe...1[@USER@USERWhyishealoser?He'sjustaP...2[DonaldJ.Trumpisguiltyascharged.Thee...3[JamieRaskintankedDougCollins.Collinslo...4[Man...y'allgone""bothsides""theapoca...1.1Data-PreprocessingWe'lljustdoabitofpreprocessingbyremovingnon-alphabetsandremovingstopwords[6]:fromnltk.corpusimportstopwordsimportre[7]:defrefineWords(s):letters_only=re.sub(""[^a-zA-Z]"","""",str(s))words=letters_only.lower().split()stops=set(stopwords.words(""english""))meaningful_words=[wforwinwordsifnotwinstops]return("""".join(meaningful_words))1.2WithorwithoutcontextWe'lljustcombinethecontextintotheresponse.Isupposehavingextrainformation/contextisalwaysgood.[8]:df['response']=df['response'].apply(refineWords)df['context']=df['context'].apply(refineWords)df['response']=df['context']+''+df['response']We'llneedtochangethepredictorvariablewhichconsistsof'SARCASM'and'NON_SARCASM'to'1'sand'0'ssothatwecanfeedthemintothemodel2[9]:defsarcasm_mapping(input):ifinput=='SARCASM':return1else:return0df['label']=df['label'].apply(sarcasm_mapping)Weinitializeasimplecountvectorizerfromsklearnforourfirstmodel.[10]:fromsklearn.feature_extraction.textimportCountVectorizervectorizer=CountVectorizer(analyzer=""word"",\tokenizer=None,\preprocessor=None,\stop_words=None,\max_features=5000)Weduplicatedcopiesofthetrainingdatasetsowehaveindividualcopiestoworkonforeachmodel.[]:df_tmp=df.copy()df_tmp_bert=df.copy()df_tmp_xlm=df.copy()1.3Data-PreprocessingWe'llneedtoensureourtrainingdataisinthecorrectformatforfurtheranalysis[]:df[""response""]=vectorizer.fit_transform(df[""response""]).toarray()df[""context""]=vectorizer.fit_transform(df[""context""]).toarray()1.4ModellingWe'lltrainourfirstmodelwhichisasimplewordcountmodel.[11]:fromsklearn.ensembleimportRandomForestClassifierforest=RandomForestClassifier(n_estimators=100)features_forest=df[[""response"",""context""]].valuesmy_forest=forest.fit(features_forest,df['label'])Wecheckthedistributionofthetargetvariabletoseeandindeed,thedatasetisperfectlybalanced.[12]:df['label'].value_counts()[12]:1250002500Name:label,dtype:int6431.5Data-PreprocessingfortestsetWedoasimilarpreprocessingforthetestsetaswell.[13]:test_df=pd.read_json('data/test.jsonl',lines=True)[14]:test_df.head()[14]:idresponse\0twitter_1@USER@USER@USERMy3yearold,thatjustfi...1twitter_2@USER@USERHowmanyverifiablelieshasheto...2twitter_3@USER@USER@USERMaybeDocsjustascrubofa...3twitter_4@USER@USERisjustacoverupfortherealha...4twitter_5@USER@USER@USERTheironybeingthatheeven...context0[Wellnowthat'sproblematicAF<URL>,@USER...1[LastweektheFakeNewssaidthatasectiono...2[@USERLet'sAplaudBrettWhenhedeservesi...3[Womengenerallyhatethispresident.What's...4[DearmediaRemoaners,youexcitedlysharing...Wesimplycombinethecontextintotheresponse.[15]:test_df['response']=test_df['response'].apply(refineWords)test_df['context']=test_df['context'].apply(refineWords)test_df['response']=test_df['context']+''+test_df['response']Weduplicatethetestdatasetsothatwehaveindividualcopiestoworkon[16]:test_df_tmp=test_df.copy()test_df_tmp_bert=test_df.copy()test_df_tmp_xlm=test_df.copy()Weneedtomakesurethatthetestdataisinthesameformatasthetraindatasowedoessentiallythesameanalysis[]:test_df[""response""]=vectorizer.fit_transform(test_df[""response""]).toarray()test_df[""context""]=vectorizer.fit_transform(test_df[""context""]).toarray()Theinputformatneedstobeinaformofanarray[17]:features_forest_test=test_df[[""response"",""context""]].valuesWeperformpredictionsofthetestfeaturesusingthetrainedCountVectorizermodel[18]:my_prediction=my_forest.predict(features_forest_test)4Weconvertthepredictionsintoapandasdataframesowecanuseto_csvtoeasilyoutputourpredictionsintherightformat[20]:test_df['label']=pd.DataFrame(my_prediction)Wedoavalue_countonthepredictionstocheckhowwellthemodelworked.Wellthemodelpredicted100%nosarcasmwhichisclearlyproblematic.[21]:test_df['label'].value_counts()[21]:01800Name:label,dtype:int64[22]:test_df.describe()[22]:responsecontextlabelcount1800.0000001800.0000001800.0mean0.0027780.0022220.0std0.0971700.0745230.0min0.0000000.0000000.025%0.0000000.0000000.050%0.0000000.0000000.075%0.0000000.0000000.0max4.0000003.0000000.0Afterwearedonemakingpredictionsusingthetrainedmodelwe'llneedtochangethe1'sand0'softhelabelcolumnbacktosarcasmandnon-sarcasm.[23]:defsarcasm_reverse_mapping(input):ifinput==1:return'SARCASM'else:return'NOT_SARCASM'test_df['label']=test_df['label'].apply(sarcasm_reverse_mapping)Wemakeuseofto_csvofthepandaslibrarytocreateouranswer.txt[24]:#test_df[['id','label']].to_csv('answer.txt',index=False,header=None)1.6ModellingusingGLoVeembeddingandsomeneuralnetworklayersFirst,we'llneedtocreatethecorpus.We'llneedthetqdmmoduleandNLTK'swordtokenizeaswellasstopwordstopreprocessourdata.[25]:defcreate_corpus(df):corpus=[]fortweetintqdm(df['response']):5words=[word.lower()forwordinword_tokenize(tweet)if((word.,-isalpha()==1)&(wordnotinstop))]corpus.append(words)returncorpusWeconcat/combinethetrainingandtestdatasetsocreateacorpusofboththetrainandtestdatasets.[26]:df_tmp['response']=df_tmp['response'].astype('string')test_df_tmp['response']=test_df_tmp['response'].astype('string')df_new=df_tmp.append(test_df_tmp)corpus=create_corpus(df_new)100%||6800/6800[00:01<00:00,3772.90it/s]We'llfirstneedtodownloadthegloveembeddingandloaditensuringthecorrectformats,[27]:embedding_dict={}withopen('data/glove.6B.200d.txt','r',encoding=""utf8"")asf:forlineinf:values=line.split()word=values[0]vectors=np.asarray(values[1:],'float32')embedding_dict[word]=vectorsf.close()WefirststartbyinitializingaKerastokenizerandtrainitwiththecorpusweobtainedearlier.Weperformtruncatingandpaddingtogetsequencesofthesamelength.[28]:MAX_LEN=50tokenizer_obj=Tokenizer()tokenizer_obj.fit_on_texts(corpus)sequences=tokenizer_obj.texts_to_sequences(corpus)tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')Ourtokenizerhasawordnumbercountintheformofword_index.We'llneeditlater.[29]:word_index=tokenizer_obj.word_indexWe'llneedtomakesureoftqdmmoduleandGLoVeembeddingtoobtaintheembeddingmatrixtobeusedtocreateanembeddinglayerusingKeras.[30]:num_words=len(word_index)+1embedding_matrix=np.zeros((num_words,200))6forword,iintqdm(word_index.items()):ifi>num_words:continueemb_vec=embedding_dict.get(word)ifemb_vecisnotNone:embedding_matrix[i]=emb_vec100%||34445/34445[00:00<00:00,569822.52it/s]We'llbuildourneuralnetworksequentially.WefirstaddtheembeddinglayerandaddLSTMlayersofdecreasingnodeswithdropouttoreduceoverfitting.[31]:model=Sequential()embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN,trainable=False)model.add(embedding)model.add(SpatialDropout1D(0.10))model.add(LSTM(128*2,dropout=0.10,recurrent_dropout=0.10,,-return_sequences=True))model.add(Dense(64*2,activation='relu'))model.add(LSTM(128,dropout=0.10,recurrent_dropout=0.10,,-return_sequences=True))model.add(Dense(64,activation='relu'))model.add(LSTM(64,dropout=0.10,recurrent_dropout=0.10,return_sequences=True))model.add(Dense(32,activation='relu'))model.add(LSTM(32,dropout=0.10,recurrent_dropout=0.10,return_sequences=True))model.add(Dense(16,activation='relu'))model.add(LSTM(16,dropout=0.10,recurrent_dropout=0.10))model.add(Dense(1,activation='sigmoid'))We'llmakeuseoftheAdamoptimizerwithasmalllearningrate.We'llcompilethemodelspecifyingtheloss,optimizerandmetricstooptimizeourmodelfor.[]:optimzer=Adam(learning_rate=1e-5*10)model.,-compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])Wesplitourdatabackintotrainandtestdatasets.Thefirst5000isourtrainingdatatherestistest.[32]:train=tweet_pad[:5000]test=tweet_pad[5000:]7We'llperformatrain-validationsplittoobtainvalidationdata.[33]:X_train,X_test,y_train,y_test=train_test_split(train,df_tmp['label'].,-values,test_size=0.15)Wecannowstarttrainingourmodelspecifyingthebatchsize,epochsandvalidationdatasets.[34]:history=model.,-fit(X_train,y_train,batch_size=4*16,epochs=1,validation_data=(X_test,y_test),verbose=2)Trainon4250samples,validateon750samplesEpoch1/1-11s-loss:0.6914-accuracy:0.5214-val_loss:0.6810-val_accuracy:0.6293Weperformpredictionsusingourtrainedmodel.Asthepredictionsarenotstrictly1'sand0'swe'lljustsimplyroundthemtothenearestinteger.[35]:y_pre=model.predict(test)test_df['label']=np.round(y_pre).astype(int)We'llneedtodoamappingof0'sand1'sto'NOT_SARCASM'and'SARCASM'[36]:defsarcasm_reverse_mapping(input):ifinput==1:return'SARCASM'else:return'NOT_SARCASM'test_df['label']=test_df['label'].apply(sarcasm_reverse_mapping)Weconvertourpredictionsintoacsvfilewiththerightformatwiththefollowing.[37]:#test_df[['id','label']].to_csv('answer.txt',index=False,header=None)2ModellingusingBERT,retrainingtheentireBERTarchitecturetothedata.We'llusetheofficialtokenizationscriptfromtensorflow.Youcandownloadthetokenization.pybyusingwgetordownloadingitmanually.[38]:!wget--quiethttps://raw.githubusercontent.com/tensorflow/models/master/,-official/nlp/bert/tokenization.pySYSTEM_WGETRC=c:/progra~1/wget/etc/wgetrcsyswgetrc=C:\ProgramFiles(x86)\GnuWin32/etc/wgetrcWe'llimportthenecessarylibrariesneededforthissection.8[39]:importtensorflowastffromtensorflow.keras.layersimportDense,Inputfromtensorflow.keras.optimizersimportAdamfromtensorflow.keras.modelsimportModelfromtensorflow.keras.callbacksimportModelCheckpointimporttensorflow_hubashubimporttokenizationWe'llneedtoencodetheinputtextstoBERT'sinputformat.Foreachrowofthetrainingdata,wefirsttokenizethetextusingNLTKtokenizerbeforeproceedingtoconverttheobtainedtokensintoIDs.[40]:defbert_encode(texts,tokenizer,max_len=512):all_tokens=[]all_masks=[]all_segments=[]fortextintexts:text=tokenizer.tokenize(text)text=text[:max_len-2]input_sequence=[""[CLS]""]+text+[""[SEP]""]pad_len=max_len-len(input_sequence)tokens=tokenizer.convert_tokens_to_ids(input_sequence)tokens+=[0]*pad_lenpad_masks=[1]*len(input_sequence)+[0]*pad_lensegment_ids=[0]*max_lenall_tokens.append(tokens)all_masks.append(pad_masks)all_segments.append(segment_ids)returnnp.array(all_tokens),np.array(all_masks),np.array(all_segments)We'llcreatetheBERTlayerspecifyingtheinputsandcreationoftheBERTlayerusingtheinputstoobtaintheoutputsequence.Sincewearere-trainingtheentireBERTarchitecturetothedataset,we'lljustaddasigmoiddenselayertoperformclassification.WespecifytheBERTmodelandtheadamoptimizer,optimizationlossandaccuracymetrics.[41]:defbuild_model(bert_layer,max_len=512):input_word_ids=Input(shape=(max_len,),dtype=tf.int32,,-name=""input_word_ids"")input_mask=Input(shape=(max_len,),dtype=tf.int32,name=""input_mask"")segment_ids=Input(shape=(max_len,),dtype=tf.int32,name=""segment_ids"")_,sequence_output=bert_layer([input_word_ids,input_mask,segment_ids])9clf_output=sequence_output[:,0,:]#WithoutDropoutout=Dense(1,activation='sigmoid')(clf_output)model=Model(inputs=[input_word_ids,input_mask,segment_ids],outputs=out)model.compile(Adam(lr=learning_rate),loss='binary_crossentropy',,-metrics=['accuracy'])returnmodelWeloadBERTfromTensorFlowHub.TensorFlowHubprovidespre-trainedmodelsforustouseandweloadthemodelusingTensorFlowhubmodule.[42]:module_url=""https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1""bert_layer=hub.KerasLayer(module_url,trainable=True)We'llneedtoloadthetokenizerfromtheBERTlayer.[43]:vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()tokenizer=tokenization.FullTokenizer(vocab_file,do_lower_case)Next,weprocessthetextintoBERTrequiredformatssuchastokens,masksandsegmentflags.[44]:train_input=bert_encode(df_tmp_bert.response.values,tokenizer,max_len=160)test_input=bert_encode(test_df_tmp_bert.response.values,tokenizer,,-max_len=160)train_labels=df_tmp_bert.label.valuesThisaretheparameterswe'llneedforourBERTmodel.Duetolimitedprocessingcapacity,thereaintmuchroomformetoplaywith.[45]:Max_length=42Dropout_num=0learning_rate=6e-6valid=0.15epochs_num=5batch_size_num=4ids_error_corrected=TrueWebuildourBERTmodelandnotethatwehave335milliontraininableparameters.[46]:model_BERT=build_model(bert_layer,max_len=160)model_BERT.summary()Model:""model""________________________________________________________________________________10__________________Layer(type)OutputShapeParam#Connectedto==================================================================================================input_word_ids(InputLayer)[(None,160)]0__________________________________________________________________________________________________input_mask(InputLayer)[(None,160)]0__________________________________________________________________________________________________segment_ids(InputLayer)[(None,160)]0__________________________________________________________________________________________________keras_layer(KerasLayer)[(None,1024),(None335141889input_word_ids[0][0]input_mask[0][0]segment_ids[0][0]__________________________________________________________________________________________________tf_op_layer_strided_slice(Tens[(None,1024)]0keras_layer[0][1]__________________________________________________________________________________________________dense(Dense)(None,1)1025tf_op_layer_strided_slice[0][0]==================================================================================================Totalparams:335,142,914Trainableparams:335,142,913Non-trainableparams:1__________________________________________________________________________________________________We'lltraintheBERTmodel,serializingthebestmodeltoafileforlaterpredictions.Finally,wegettotrainourBERTmodel.[47]:checkpoint=ModelCheckpoint('model_BERT.h5',monitor='val_loss',,-save_best_only=True)train_history=model_BERT.fit(train_input,train_labels,validation_split=valid,epochs=epochs_num,callbacks=[checkpoint],batch_size=batch_size_num)Trainon4250samples,validateon750samples11Epoch1/54250/4250[==============================]-303s71ms/sample-loss:0.5174-accuracy:0.7588-val_loss:0.5598-val_accuracy:0.6080Epoch2/54250/4250[==============================]-278s65ms/sample-loss:0.4388-accuracy:0.8024-val_loss:0.4221-val_accuracy:0.7653Epoch3/54250/4250[==============================]-265s62ms/sample-loss:0.3094-accuracy:0.8727-val_loss:1.3605-val_accuracy:0.4267Epoch4/54250/4250[==============================]-265s62ms/sample-loss:0.1140-accuracy:0.9614-val_loss:2.7396-val_accuracy:0.3747Epoch5/54250/4250[==============================]-265s62ms/sample-loss:0.0422-accuracy:0.9861-val_loss:2.8826-val_accuracy:0.4853Weperformpredictionsbyloadingweightsfromthefileweserializedthemodelearlier.[48]:model_BERT.load_weights('model_BERT.h5')test_pred_BERT=model_BERT.predict(test_input)test_pred_BERT_int=test_pred_BERT.round().astype('int')We'llneedtoperformamappingofthe1'sand0'sbackto'SARCASM'and'NOT_SARCASM'forsubmission.[49]:test_df['label']=test_pred_BERT_intdefsarcasm_reverse_mapping(input):ifinput==1:return'SARCASM'else:return'NOT_SARCASM'test_df['label']=test_df['label'].apply(sarcasm_reverse_mapping)Wegeneratethe'answer.txt'intheformatrequiredforsubmission.[]:#test_df[['id','label']].to_csv('answer.txt',index=False,header=None)12 Course Project Progress Report Chua Yeow Long, ylchua2@illinois.edu Text ClassiThcation Competition Introduction For the text classiThcation competition on sarcasm detection, IOll utilise transfer learning using embeddings such as GLoVe and BERT which are both major milestones in modern NLP rather than training a machine learning model from scratch. GLoVe embeddings is one of the Thrst applications of transfer learning in NLP and with the introduction of BERT by Google in 2018, it addresses the shortcomings of LSTMs and other modern NLP techniques and also more recently with huge models such as XLM and GPT-2 achieved state-of-the-art performance. Methodology & Results IOll Thrst preprocess the text data by removing stop-words and non-alphabetic characters such as symbols before feeding the data into 3 different models. The Thrst is to use an GLoVe embedding and adding some LSTM and dropout layers and start training the model. A 85% train and 15% validation split is performed to obtain the validation data. For the input text data, I have tried with and without context and only for the BERT case the F1-score is pretty signiThcant. I have tried different LSTM layers and nodes as well as dropout layers and the best f1-score I have obtained is around 0.661. The second method is to use BERT and re-train the entire BERT layers using the dataset on a pretty beefy machine. The best F1-score I have obtained is 0.7378 with context information and 0.716 without context. The baseline F1-score is 0.723 and the top F1-score is by awe with 0.7653. Future-works IOll try and see if I can get a better score by exploring two different approaches: feature engineering and/or GPT-2/XLM transfer learning based methods when there is time to spare. I have included the references for BERT as well as GPT-2 in the references section below. References Language Models are Unsupervised Multitask Learners Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Iiya Sutskever Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia PolosukhinDescriptionF1-scoreGLoVe Embedding with LSTM layers without context0.661GLoVe Embedding with LSTM layers0.661BERT without context0.716BERT0.7378 Project Proposal In your project proposal, please answer the following questions:What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. CHUA Yeow Long, ylchua2@illinois.edu Which competition do you plan to join? Text classification competition If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Yes, IOm prepared to learn state-of-the-art neural network classifiers. Name some neural classifiers and deep learning frameworks that you may have heard of. Deep learning frameworks such as Pytorch and TensorFlow. Artificial neural networks(ANN), LSTMs, CNN, RNN, transformers, transfer learning Describe any relevant prior experience with such methods Built machine learning models using sklearn libraries XGBoost, SVM to perform classification not limited to NLP Built deep learning neural networks using Keras/TensorFlow and PyTorch Text pre-processing using SpaCy and NLTK Transfer Learning using SimpleRepresentations or HuggingFace Which programming language do you plan to use? Python Text Classification Competition: Sarcasm Detection ylchua2@illinois.edu Project Proposal - proposal_ylchua2.pdf Project Progress Report - progress_report.pdf Project Code - notebook.ipynb (working copy), notebook_final.ipynb (notebook with outputs kept), notebook_final.pdf (pdf version), answer_1p0_0p7378 (predictions that achieved 100% accuracy and F1-Score of 0.7378), glove.6B.200d.txt(https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) Documentation - final_report.pdf (Simple writeup of the final project, installation, overall plan) Presentation - https://youtu.be/dE7jiio5QAM"
https://github.com/j5un/CourseProject	"Documentation: BERT fine-tuning for Twitter sarcasm detection Name : Junzhe Sun  NetID : junzhes2  Team : Junzhe Sun (individual) Competition : text classification competition Overview This project aims to detect sarcasm from Twitter posts. This is a text classification task related to sentiment analysis. The training data set includes text-label pairs, therefore constituting a supervised machine learning problem. I experimented with two different methods, including FastText and BERT. I found BERT with fine-tuning to have the better performance, which achieved a higher f1 score than the baseline. The code is implemented in Python under the Google Colab environment. Two Jupyter notebooks are available, one using fastText and the other using BERT in PyTorch. Both notebooks are self-contained, in that data loading, data cleaning/processing, training and prediction are available in each notebook. The input to both notebooks are a training data set with text-label pairs, and a test data set without labels for prediction. The notebook outputs predicted labels after training the respective neural network using the input data. They can be used for general text classification tasks beyond the binary classification task in this project. Implementation details Model Two models were tested for the text classification competition. I first tried fastText, a shallow-learning library that can be used for both word embedding and text classification. FastText is a close sibling to Word2Vec, and was introduced by Bojanowski et al. (2017) from Facebook AI Research. FastText is a method that aims to extend word vectors to capture subword information using character n-grams, instead of representing each word in the vocabulary as distinct vectors. This concept can be extended to the bag of word (BoW) representation of texts using latent vectors of words and word n-grams, which leads to a simple yet efficient baseline method for text classification (Joulin et al., 2016). The second model is a transformer network known as BERT, which stands for Bidirectional Encoder Representations from Transformers and is introduced by Devlin et al. (2018) from Google AI Language. The transformers is a type of deep neural network that's based on attention mechanism (Vaswani et al., 2017), and has recently gained a lot of popularity in the NLP community. The BERT model is designed to pre-train deep bidirectional representations of language models using unlabeled text. Bidirectional training means that representations are trained by jointly conditioning on both left and right context in all layers of the deep network. Consequently, a pre-trained BERT model, available from a variety of sources, can be very conveniently obtained and fine-tuned with just one additional output layer for a wide range of tasks, such as text classification. The Transformers library from Hugging Face provides a wide selection of pre-trained, general-purpose architectures for NLP tasks, including BERT. In particular, they provide the BertForSequenceClassificaiton model which attaches a pooling layer and a linear layer after the BERT network. It can be used for the Twitter sarcasm detection task. Training Data loading and data cleaning are performed using Pandas. Consecutive spaces were replaced by a single whitespace, and leading and trailing spaces were removed. Some special characters, such as ""@USER"", were removed, while Emoji's were kept as they capture emotions. Emoji needs to be converted to known tokens for the network, and the emoji library provides a convenient function to do so. For example,  emoji.demojize( '' ) returns ':thinking_face:' .   Cases were also kept because in my tests they tend to improve the f1 score (for both models). Punctuations are treated differently depending on the model. They were removed in the case of fastText, while for BERT, they were kept since BERT has embedding for most of the punctuations. The data cleaning steps are the same for both the training and test data sets. The training data (after data cleaning) is first shuffled randomly in order to mix SARCASM and NOT_SARCASM entries. Then a total of 5000 examples in the training data are split into a training set of 4500 examples and a validation set of 500 examples (10% split). The validation set is used for periodically evaluating the current model for tuning the model's hyperparameters and preventing overfitting. A minor challenge is that, since the training data set is not very big, splitting it into training and validation sets can be tricky. My solution is to first use the validation to find the optimal hyper parameters, and then retrain the model using the entire training set. For fastText, the most important hyperparameters include learning rate, number of epochs and wordNgrams. For BERT, some of the hyperparameters include learning rate, number of epochs, drop out rate and maximum length of text that can be handled by the tokenizer. Result and Deliverables For fastText using the tri-gram model, it is unable to beat the baseline, having an f1 score of 0.6642. BERT with fine tuning is able to reach an f1 score of 0.7362 within a few rounds to hyperparameter tuning. All model parameters allowed to be updated during the training process. I found four epochs is enough for the fine turning, while running more epochs tends to overfit the training data set and leads to a lower f1 score on the test data set. The code/deliverables include two Jupyter notebooks. More detailed comments are available in the notebooks about the specific functions of each cell. *01_fasttext4Twitter.ipynb : This notebook contains implementation using the fastText library. The data loading and data cleaning steps use Pandas. The preprocessed training data set is written to disk in a format that's compatible with fastText. Training and prediction follows the steps of the text classification example in fastText's documentation ([5]), and only involves a few lines of code. To obtain additional metrics, the scikit-learn library is used to compute precision, recall and f1 score. Finally, both the trained model and the predicted labels are written to disk. *02_bert4Twitter_pytorch.ipynb : This notebook contains implementation using PyTorch and the transformers library from Hugging Face. Pre-trained BertForSequenceClassification and BertTokenizer is downloaded from Hugging Face, and additional fine tuning of Bert network weights is performed using the training data. Similar to fastText, data is preprocessed using Pandas. To convert from Pandas data frame to PyTorch compatible data format, the datasets library from Hugging Face is used. To obtain additional metrics, the scikit-learn library is used to compute precision, recall and f1 score. Hugging Face also provides a simple but feature-complete Trainer class for training/fine-tuning the network ([6]). Trained model is then set to evaluation mode, and test data is transferred to GPU for prediction. Finally, both the trained model and the predicted labels are written to disk. Usage of software This project is implemented using Google Colab. Therefore, the most convenient way to reproduce and verify the results is through Google Colab and using a GPU runtime environment. Google Colab provides a free Nvidia Tesla T4 GPU for 12 hours for continuous usage for free. Open the Jupyter notebooks in Google Colab, map the correct Google Drive that contains the data and trained model, then modify the ""filepath"" variable to reflect the correct path. The notebook can be executed block by block, or select run all under Runtime. For 01_fasttext4Twitter.ipynb, since the training is so efficient, there is no need to load the trained model. On the other hand, for 02_bert4Twitter_pytorch.ipynb, the deep neural network can take a while to train. If one would like to avoid re-training the model, skip the training block and execute the Demo part instead (but don't skip the data loading blocks). Team Contribution This is a one-person team. Tutorial A software tutorial presentation is uploaded to a Box drive accessible with the link: https://uofi.box.com/s/wedyjqpel0rw2uxprax10mh2rnwxhu7t Note that although the tutorial is 15 mins long, the review can feel free to watch only the first 10 mins, which is about the BERT approach. The last 5 mins talks about fastText, which is optional to watch. Finally, to avoid re-training the BERT model, a trained model is available with the link: https://uofi.box.com/s/t64lnt83ck2m3khtrk4s3r46fjhd5gsk References [1] Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5, 135-146. [2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. [4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30, 5998-6008. [5]  https://fasttext.cc/docs/en/supervised-tutorial.html [6]  https://huggingface.co/transformers/training.html [7]  https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad [8]  https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/ Final Project Progress Report - Text Classification Competition Name : Junzhe Sun NetID : junzhes2 Captain : Junzhe Sun (individual) Competition : text classification competition Progress : For the text classification competition, I looked at a few different options and decided to use a pre-trained BERT model with fine tuning. 1) Which tasks have been completed? A : I looked at different shallow/deep learning methods, including FastText, LSTM and Transformer neural networks. I first trained a FastText model, a shallow neural network, using the training data set, but wasn't able to beat the baseline. Then I used a pre-trained BERT model from huggingface using PyTorch and fine tuned it using the training data set. The prediction using the BERT model was able to beat the baseline. 2) Which tasks are pending? A : (1) Further fine-tuning the BERT model to improve F1 score. (2) Source code documentation. (3) Software code submission with documentation/report. (4) Record software usage tutorial presentation. 3) Are you facing any challenges? A : No big challenge remains. A minor challenge is that, since the training data set is not very big, splitting it into training and validation sets can be tricky. In addition, I will probably need a more automated and systematic way for hyperparameter tuning (train-test split, number of epochs, rate for drop-out layer, etc.). Final Project Proposal - Text Classification Competition Name : Junzhe Sun NetID : junzhes2 Captain : Junzhe Sun (individual) Competition : text classification competition Method : I have learned about deep neural networks on the coursera platform before (Deep Learning Specialization). I understand the theory behind CNNs and RNNs (e.g. LSTM and GRU). I am interested in learning more about transformer networks such as BERT as well as CNNs for natural language processing (NLP). I have used Tensorflow before in course projects, and I know about Pytorch from reading publications. Programming Language : Python. In particular, I will be using Pandas and Tensorflow 2. CS410 Text Information Systems Course Project Author: Name: Junzhe Sun Team: Individual NetID: junzhes2 Content: ./Documentation.pdf: software documentation ./notebooks: Jupyter notebooks for Twitter Sarcasm Detection ./data: training and test data ./Proposal.pdf: project proposal ./ProgressReport: project progress report Tutorial: Link to software tutorial Note that although the tutorial is 15 mins long, the review can feel free to watch only the first 10 mins, which is about the BERT approach. The last 5 mins talks about fastText, which is optional to watch."
https://github.com/jacobvp2/CourseProject	"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221653786A cross-collection mixture model for comparative text miningConference Paper * January 2004DOI: 10.1145/1014052.1014150 * Source: DBLPCITATIONS247READS2083 authors, including:Some of the authors of this publication are also working on these related projects:Health News Quality View projectAtulya Velivelli15 PUBLICATIONS 370 CITATIONS SEE PROFILEBei YuSyracuse University40 PUBLICATIONS 1,103 CITATIONS SEE PROFILEAll content following this page was uploaded by Bei Yu on 31 July 2016.The user has requested enhancement of the downloaded file.ACross-CollectionMixtureModelforComparativeTextMiningChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbanaChampaignAtulyaVelivelliDepartmentofElectricalandComputerEngineeringUniversityofIllinoisatUrbanaChampaignBeiYuGraduateSchoolofLibraryandInformationScienceUniversityofIllinoisatUrbanaChampaignABSTRACTInthispaper,wede neandstudyanoveltextminingproblem,whichwerefertoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcom-monthemesacrossallcollectionsaswellassummarizethesimilarityanddi erencesofthesecollectionsalongeachcom-montheme.Thisgeneralproblemsubsumesmanyinterest-ingapplications,includingbusinessintelligenceandopinionsummarization.Weproposeagenerativeprobabilisticmix-turemodelforcomparativetextmining.Themodelsimul-taneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.ThemodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algo-rithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.CategoriesandSubjectDescriptors:H.3.3[Informa-tionSearchandRetrieval]:TextMiningGeneralTerms:AlgorithmsKeywords:Comparativetextmining,mixturemodels,clus-tering1.INTRODUCTIONTextminingisconcernedwithextractingknowledgeandpatternsfromtext[5,6].Whiletherehasbeenmuchre-searchintextmining,mostexistingresearchisfocusedononesinglecollectionoftext.Thegoalsareoftentoextractbasicsemanticunitssuchasnamedentities,toextractrela-tionsbetweeninformationunits,ortoextracttopicthemes.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.KDD'04,August22-25,2004,Seattle,Washington,USA.Copyright2004ACM1-58113-888-1/04/0008...$5.00.Inthispaper,westudyanovelproblemoftextminingre-ferredtoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcommonthemesacrossallcollectionsaswellassummarizethesimilarityanddi er-encesofthesecollectionsalongeachcommontheme.Specif-ically,thetaskinvolves:(1)discoveringthedi erentcom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Theneedforcomparativetextminingexistsinmanydi erentapplica-tions,includingbusinessintelligence,summarizingreviewsofsimilarproducts,andcomparingdi erentopinionsaboutacommontopicingeneral.Inthispaper,westudytheCTMproblemandproposeagenerativeprobabilisticmixturemodelforCTM.Themodelsimultaneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.Themixturemodelisbasedoncomponentmultinomialdistributionmodels,eachcharacterizingadi erenttheme.Thecommonthemesandcollection-speci cthemesareexplicitlymodeled.Thepro-posedmodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algorithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.Therestofthepaperisorganizedasfollows.InSection2,webrie yintroducetheproblemofCTM.Wethenpresentabaselinesimplemixturemodelandanewcross-collectionmixturemodelinSection3andSection4.WediscusstheexperimentresultsinSection5.2.COMPARATIVETEXTMINING2.1AmotivatingexampleWiththepopularityofe-commerce,onlinecustomereval-uationsarebecomingwidelyprovidedbyonlinestoresandthird-partywebsites.Pioneerslikeamazon.comandepin-ions.comhaveaccumulatedlargeamountsofcustomerinputincludingreviews,comments,recommendationsandadvice,etc.Forexample,thenumberofreviewsinepinions.comismorethanonemillion[4].Givenaproduct,therecouldbeuptohundredsofreviews,whichisimpossibleforthereaderstogothrough.Itisthusdesirabletosummarizeacollectionofreviewsforacertaintypeofproductsinordertoprovidethereadersthemostsalientfeedbacksfromthepeers.Forreviewsummarization,themostimportanttaskistoidentifydi erentsemanticaspectsofaproductthatthereviewersmentionedandtogrouptheopinionsaccord-ingtotheseaspectstoshowsimilaritiesanddi erencesintheopinions.Forexample,supposewehavereviewsofthreedi erentbrandsoflaptops(Dell,IBM,andApple),andwewanttosummarizethereviews.Ausefulsummarywouldbeatab-ularrepresentationoftheopinionsasshowninTable1,inwhicheachrowrepresentsoneaspect(subtopic)anddi er-entcolumnscorrespondtodi erentopinions.Table1:AtabularsummarySubtopicsDellIBMAppleBatterylifelongenoughshortshortMemorygoodbadgoodSpeedslowfastfastItis,ofcourse,verydicult,ifnotimpossibletopro-ducesuchatablecompletelyautomatically.However,wecanachievealessambitiousgoal{identifyingthesemanticaspectsandidentifyingthecommonandspeci ccharacter-isticsofeachproductinanunsupervisedway.Thisisaconcreteexampleofcomparativetextmining.2.2ThegeneralproblemTheexampleaboveisonlyoneofthemanypossibleappli-cationsofcomparativetextmining.Ingeneral,thetaskofcomparativetextmininginvolves:(1)discoveringthecom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Itisveryhardtopreciselyde newhatathemeis,butitcorrespondsroughlytoatopicorsubtopic.Thegranularityofthemesisapplication-speci c.CTMisafundamentaltaskinex-ploratorytextanalysis.Inadditiontoopinioncomparisonandsummarization,ithasmanyotherapplications,suchasbusinessintelligence(comparingdi erentcompanies),cus-tomerrelationshipmanagement(comparingdi erentgroupsofcustomers),andsemanticintegrationoftext(comparingcomponenttextcollections).CTMischallenginginseveralways:(1)Itisacompletelyunsupervisedlearningtask;notrainingdataisavailable.(ItisforthesamereasonthatCTMcanbeveryusefulformanydi erentpurposes{itmakesminimumassumptionsaboutthecollectionsandinprinciplewecancompareanyarbitrarypartitionoftext.)(2)Weneedtoidentifythemesacrossdi erentcollections,whichismorechallengingthanidentifyingtopicthemesinonesinglecollection.(3)Thetaskinvolvesadiscriminationcomponent{foreachdiscov-eredtheme,wealsowanttoidentifytheuniqueinformationspeci ctoeachcollection.Suchadiscriminationtaskisdif- cultgiventhatwedonothavetrainingdata.Inaway,CTMgoesbeyondtheregularone-collectiontextminingbyrequiringan\alignment""ofmultiplecollectionsbasedoncommonthemes.Sincenotrainingdataisavailable,ingeneral,wemustrelyonunsupervisedlearningmethods,suchasclustering,toperformCTM.Inthispaper,westudyhowtouseprob-abilisticmixturemodelstoperformCTM.Belowwe rstdescribeasimplemixturemodelforclustering,whichrepre-sentsastraightforwardapplicationofanexistingtextmin-ingmethod,andthenpresentamoresophisticatedmixturemodelspeci callydesignedforCTM.3.CLUSTERINGWITHASIMPLEMIXTUREMODELqqqqqFigure1:TheSimpleMixtureModelAnaivesolutiontoCTMistotreatthemultiplecollec-tionsasonesinglecollectionandperformclustering.Ourhopeisthatsomeclusterswouldrepresentthecommonthemesacrossthecollections,whilesomeotherswouldrep-resentthemesspeci ctoonecollection(seeFigure1).Wenowpresentasimplemultinomialmixturemodelforclus-teringanarbitrarycollectionofdocuments,inwhichweassumethereareklatentcommonthemesinallcollections,andeachischaracterizedbyamultinomialworddistribu-tion(alsocalledaunigramlanguagemodel).Adocumentisregardedasasampleofamixturemodelwiththesethememodelsascomponents.We tsuchamixturemodeltotheunionofallthetextcollectionswehave,andtheobtainedcomponentmultinomialmodelscanbeusedtoanalyzethecommonthemesanddi erencesamongthecollections.Formally,letC=fC1;C2;:::;Cmgbemcomparablecol-lectionsofdocuments.Let1;:::;kbekthemeunigramlanguagemodelsandBbethebackgroundmodelforallthecollections.Adocumentdisregardedasasampleofthefollowingmixturemodel(basedonwordgeneration).pd(w)=Bp(wjB)+(1 Team GOAT! (Getting Our Act Together) Reproducing Paper: A cross-collection mixture model for comparative text mining 1: Progress As a group we have decided to choose a rather large dataset from the New York Times regarding the presidential election. Our paper that we are modeling also used the Expectation-Maximization (EM) algorithm to evaluate a model on two different text data sets one being a news article data set. We are currently doing more research on the EM algorithm as this was a rather recent lesson in our class. Obviously, we have a GitHub repository to collaborate on for our coding portions. Also using Google Docs for the paper creation. 2: Remaining Tasks We have to implement our algorithm and then use an experiment to verify our implementation. 3: Challenges/Issues Faced Unfortunately, one of our group members lost a family member over the break, so we had to reallocate some of the workload recently. Other than that, we are having issues figuring out helpful libraries and strategies so that our EM algorithm can work on large datasets. CS410 Course Project Team GOAT Background ## Reproducing a Paper. For our final project, we attempted to reproduce results from the (contexual text mining) research paper listed below: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 The actual paper has been included in our repository for your own reference. Setup/Dependencies ## This repo assumes that the user has Python3 as well as Pip. If not, they can be found here. Our project has two package dependencies, newsapi and numpy. Using the command ""pip install -r requirements.txt"" the user should be able to install the correct versions of both packages. At this point, cd into directory /CourseProject/, run mixture.py, and follow the prompts in order to run our implementation of the Collective Text Mining comparison model. Data ## Our data is rather fluid in this case. We have an example of our demonstration video in a youtube link here. In an attempt to keep the data as similar as possible to the original paper, please use terms ""Iraq"" and ""Afghanistan"" when prompted. TEAM GOAT: PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. *Captain Bolded 2. Which paper have you chosen? ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? Yes"
https://github.com/jaeroong/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jasonzhang929/CourseProject	"Progress report Siyuan Zhang (siyuan3) 1.Task completed: For this project, I have already downloaded the dataset, wrote a couple of manually engineered features and trained basic models( linear regression, logistic regression, SVM) using these features as inputs on the training dataset. 2. Pending tasks: The performance not yet reaching the baseline but close. IOm planning on adding more features and using more advanced models such as deep neural networks for classiThcation. Im conThdent that I will be able to outperform the baseline by a fair amount of margins. 3. Challenges: Not really at this point, everything is ok and progress being made matches my plan. Project Proposal 1 Names: Siyuan Zhang siyuan3 2 Competition: Text classiThcation 3 Detail IOm planning on trying a mixture of models for this text. This includes classical models such as regression models and SVMs to train a set of engineered features. IOm also planning on using more advanced models such as deep neural networks to achieve better performance. 4. Language Python3 Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ... Text ClassiThcation Competition - Twitter Sarcasm Detection CS410 Fall 2020 Siyuan Zhang(siyuan3@illinois.edu) Introduction The goal of our project is to design implement and test models that can classify the existence of sarcasm in twitter messages. Since this is a competition that focuses on the performance of the model we built, there is a baseline requirement of a f1 score greater than 0.723. Proposed Methods Our methods mainly focusing on using machine learning techniques since the problem we try to tackle here is mainly a text classiThcation problem. A large part of applied machine learning is about feature engineering so a large part of our project is about discovering and experimenting different feature extraction methodologies. After the features are generated from the dataset, we then deployed different machine learning algorithms to train and classify the data set provided. We compare the performance of all the classiThers we experimented. Preprocessing We implemented couple standard text preprocessing functions to clean our data. Methods we used includes special character processing, upper/lower case process, punctuation signs, stemming and lemmatization, as well as stop words. We apply these data cleaning process before the data is fed into the classiThers for training. However, later we determined that such data cleaning actually degraded the test time performance by couple percent so we took this part out in the Thnal version of our testing code. The reason for this is that we suspect in twitter message there are lots of special words and characters that actually play an important role in expressing the message and data cleaning takes a lot of these important information out which makes it harder to classify the sarcasm in the messages. Feature Engineering We considered different features to use in this project. Since the context data is provided in the dataset in addition to the responses, we leverage both to extract useful features. We considered word count vectors, TF-IDF vectors, word embeddings and NLP features for this task. Among those features, we found the TF-IDF vectors to be great features that can achieve good classiThcation performance on the test set so it was selected as out feature. We extract TF-IDF weights from both the response string as well as the concatenated context string across the data. We adopted these weights for both the unigram model and the bigram model. Machine learning classiThers The classiThers we considered here includes support vector machines, random forests, Boosting from the classical machine learning techniques as well as multilayer perception model from the deep learning models. For all models we believe since they were used extensively in many applications and were able to achieve good performance, they should deliver promising results on the task as well. For each of the models, we performed random searches and grid searches on all the model hyper-parameters to select the best parameters for the actual training and testing process. Implementation Details Thanks to the extensive amount of research and development done in this area in the past decades, we were able to make good use of many standard packages and libraries to achieve a lot of functionalities that we require in this project. We mainly used pandas data frame to load the dataset for easier processing. The TF-IDF vectors are extracted using TThdfVectorizer from the scikit learn library. We set it to use both the unigram and bigram from the data and extracts features from both the response and the context column. After both vectors are calculated, we concatenate them for each data point and use as the features for the machine learning classiThers. We also leveraged different classiThers mainly from the eras, sklearn and xgboost library. There are very well written versions of these algorithms in these libraries and we use them to train and test these classsiThers. After picking the best model, we then producing the test time output using the Test dataset as input and saved the result into the Answer text Thle for benchmarking. Experiments Detail Since the test set is reserved for grading, we split the train set into a 4 1 split and leave 20% of our training data as validation set during out model selection and tuning process. We Thrst perform random search on hyper parameters of each of out models to narrow down the search range of the parameters. Then we performed grid search over the parameters to determine the best performing hyper parameters for all the models. Afterwards, all models are trained on the train set and their accuracy were compared to determine the best model. Although some model tested very well during the training phase, their test time performance is not as good due to potential overThtting so we had to do models selection manually by uploading to GitHub and compare the test set performance on the livedatalab server. Below is the detailed performance of all out models on different metrics. From the experiment data above, we can see that the random forest classiTher not only have a very high training time performance, but also a very good test time f1 score and Training AccuracyTest PrecisionTest RecallTest F1SVM0.82720.6580.6880.6725Random forest0.99980.6150.8940.7288Boosting0.92160.6210.7180.6656Multilayer Percep0.91820.5200.9230.6612is above the 0.723 baseline f1 performance. Other 3 models especially the boosting and the multilayer perception model perform well during training time put didnOt achieve similar performance on the test and the reason can be potential overThtting. The Thnal parameter we used on the random forest is 1000 estimator with a decision threshold of 0.465. Conclusion In this project we experimented many techniques on text classiThcation including feature engineering and machine learning classiThers. The Thnal performance of our best model using TF-IDF feature vectors combined with random forest classiTher together is able to achieve a good F1 score of 0.7288 which is beyond the baseline. We are happy with the result and this project has been an amazing learning experience for me. Software Used Installation guide The following software packages needs to be installed in order to run our code: python pandas nltk tensorssow keras numpy scipy scikit_learn xgboost These dependencies can be downloaded and installed automatically by Thrst cloning our repo to local and then run the following line in the terminal: Install -r requirements.txt Our code is located in the sarcasm_detection.py, simply run the code and generated prediction result of the test set will be saved in Answer.txt All of out code is fully documented, please refer to our code for more implementation details. Reference https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classiThcation-in-python/ https://www.mfz.es/machine-learning/an-end-to-end-machine-learning-project-part-i-text-classiThcation-in-python/ Siyuan Zhang(siyuan3@illinois.edu) CS410 Project Text Classification Competition Team Siyuan Text Classification Twitter Sarcasm Detection Our approach Text classification Preprocessing Feature Engineering Machine Learning Techniques Experiment Results How to Install requirements In command line: git clone https://github.com/jasonzhang929/CourseProject.git Install -r requirements.txt How to run our code All of our code are located at sarcasm_detection.py # define svm model svm = svm.SVC(random_state=8, kernel='linear', C=0.1, probability=True) # define random forest model rf_model = RandomForestClassifier(n_estimators=1000, random_state=17, warm_start=True, verbose=1) # specify which model to use model = rf_model # run svm model # run_svm() # run mlp model # run_MLP() # run xgboost boosting model # run_boost() # run random forest or svm model here run_model(model) Code Demo Thank you!"
https://github.com/jennz0/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jessehenn/CourseProject	CourseProject Overview Team: PiazzaEd Topic: Integrating Piazza with Educational Web Project Proposal Project Proposal Project Progress Report Project Progress Report Final Artifacts Written Documentation Source Code Repository Demo Video* Implementation Details Overview Video* Survey Overview Video Survey Results in CSV *NOTE:The OneDrive movie viewer may show the video as blurry. Download it and view it locally for best results.
https://github.com/jhinukb/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jiezheng5/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jinningli/CourseProject	"CS410 Final Project Report Text Classification Competition: Twitter Sarcasm Detection Our Team: Team name: Salty Fish Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Introduction The final project we choose is the text classification competition. The main task of the competition is to do the sentiment analysis of a given tweet to determine whether it is sarcasm or not. We first built a model using pre-trained LSTM to do the classification. We only used the response data to train the model and we did not beat the baseline. We did not make use of the context data, and we also did some research on the state-of-art network models of the sentiment analysis. We then built a BERT model using a pre-trained BERT. We used BCEloss to finetune the parameters during training and added a fully connected layer to output the classification result. With this method we beat the baseline, and the best result we achieved is (0.6832, 0.8433, 0.7549), precision, recall and F1 score respectively. LSTM Baseline Introduction of LSTM LSTM is the abbreviation of Long short-term memory. LSTM is a special kind of RNN, which solves the deficiency of gradient vanishing and exploding during long-term training that RNNs have. LSTM consists of three gate structures:forget gate, input gate and output gate. Fig. Overall structure of the LSTM The forget gate layer is implemented by a sigmoid layer. It looks at and ,and outputs aht-1xt value, denoted as , between 0 and 1 for each number in the cell state , where isftCt-1ht-1 the output at timestamp t-1 and is the input at timestamp t.xt The input gate layer is used to update the information in cell state. This process has two steps. The first step is to use a sigmoid layer to decide which value to get updated, denoted as , and the second step is to use a tanh layer to create a vector of new candidate values,it . Then we can update the cell state using the previous output from the forget gate andC't the input gate. The new cell state is calculated as:Ct fiCt= t*Ct-1+ t*C't The output gate layer is to decide the output value. First, we use a sigmoid layer to decide what parts of the cell state will be output. Then, we put the cell state through tanh and multiply by the output of the sigmoid layer, denoted as , and then we can get the finalot output .ht Result of using LSTM We first tried to use the pre-trained LSTM from pytorch and we added two fully connected layers to make the classification given the output of the LSTM. In this case we get two classifications, 'SARCASM' and 'NOT_SARCASM'. The best result we get using this method is: From the result we can see that the pre-trained LSTM model without fine-tuning can not beat the baseline, this is probably because LSTM does not perform well when dealing with long term dependency problems. So we decide to try another model, BERT, to get better performances. The LSTM model is implemented by Python 3.7.0, PyTorch 1.2.0 and trained on Intel x86 CPU devices with 8 cores. The code can be found in https://github.com/jinningli/CourseProject . Please refer to the Code Manual section for the usage of our code. Precision Recall F1 Score 0.6109979633401222 0.6666666666666666 0.6376195536663125 BERT Model and Logits Ensemble BERT Model BERT is a language representation model, which is pre-trained on unlabeled text data at first and fine-tuned with an additional output layer for different NLP tasks. BERT can learn a bidirectional representation for each word on both left and right context, compared to the static word embedding learned in word2vec. BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) as shown in Fig. Fig. BERT architecture based on Transformer Encoder. Two unsupervised tasks are used to pre-train BERT. The first task is Masked language model (MLM) where some percentage of the input tokens are masked at random, and the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary for prediction. The second task is Next Sentence Prediction (NSP) where the hidden vector at [CLS] token is used to classify whether the given two sentences are continuous sentences or not. Both tasks can be trained jointly as in Fig. Fig. Two unsupervised tasks for BERT pre-training. During fine-tuning, the BERT model is first initialized with the pre-trained parameters, and a fully connected layer is added as the output layer. All of the parameters are tuned using labeled data from the downstream tasks as shown in Fig. Fig. Procedure of BERT fine-tuning. Since BERT obtained the state of the art result in eleven NLP tasks once published, variants of work are proposed based on the BERT model and push the performance of NLP tasks a big step forward such as XLNet and ALBERT. Our method is based on the BERT model. We load the weights of BERT from PyTorch-Transformers which is pre-trained on BooksCorpus (800M words) and English Wikipedia (2,500M words). A fully connected layer is added as the output layer to classify Twitter response into ""SARCASM"" or not. All the parameters are tuned in the training process with BCELoss. The Twitter dataset is splitted into training dataset with 4000 samples and validation dataset with 1000 samples. Validation dataset is used to tune the hyperparameters during training and find 2 epochs is the best since the validation loss starts to grow up after it probably due to the limited dataset. Finally, the model is trained on the whole dataset including both training dataset and validation dataset. The model is implemented by Python 3.7.0, PyTorch 1.2.0. We train the model on NVIDIA TITAN V GPU, 16 cores Intel x86 CPU machine. The code can be found in https://github.com/jinningli/CourseProject . Please refer to the Code Manual section for the usage of our code. Fig. Train Loss and Validation Loss From the figure above, we can see that the validation loss is relatively low around epoch 2-4. Then after epoch 4 the validation loss is high due to overfitting. This is mainly because the size of the training dataset is small, so in this case the model tries to overfit the data in the training dataset rather than generalize from patterns observed from the dataset. This results in the low training loss and high validation loss after epoch 4. Therefore, we only take the result of 2-4 epochs for further ensemble. In our final submission, we used all the 5000 samples (without validation) in the training dataset to train the model to get better performances. Logits Ensemble In order to further improve the performance of our model on the testset, we tried to submit several results to the LiveDataLab OnlineJudge system and collect the Precision, Recall, and F1 Score response. The scores are shown in the figure below. We find that our epoch 3 model and epoch 4 model have the similar highest F1 Score. In addition, their precisions and recalls are varied. The ensemble method should be effective by taking the average of logits values (the output of neural network). Fig. Precision, Recall, and F1 Score on Testset (Response after submission) We ensemble the logits by taking the average vglogitogit / na=nili The prediction is the argmax of the average logits red argmax(avglogit)p= The ensemble method further improves our F1 Score from 0.7519 to 0.7549 Table 1: Ensemble Improvement Conclusion In conclusion, we built two models, an LSTM model and a BERT model to accomplish the twitter sarcasm detection. The LSTM model with a pre-trained LSTM and two fully connected layers did not beat the baseline. The BERT model with a pre-trained BERT and a fully connected layer using BCEloss beats the baseline. Code Manual Evaluate Model and Ensemble Download pre-trained model for our method Download  checkpoint.zip  from https://drive.google.com/file/d/1nRucz1yDqyoYR8jLeP6fKZt8FpqJBBUA/view?usp=sharing unzip checkpoint.zip mkdir checkpoint; mkdir checkpoint/run0 mv checkpoint checkpoint/run0/checkpoint Evaluate and Generating Predictions python3 evaluate.py --run run0 --use_bert --device 3 Model Precision Recall F1 Score BERT Epoch 3 0.6667 0.8622 0.7519 BERT Epoch 4 0.6876 0.8267 0.7508 BERT Epoch 3&4 Ensemble 0.6832 0.8433 0.7549 Ensemble # modify ensemble_paths in ensemble.py python3 ensemble.py All Parameters --run  Evaluating on runX. e.g. --run=run1 --use_bert  Use bert model. If not, using LSTM model. --device  GPU device to use Train Model Start Training with Validation and Evaluation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 --use_valid --eval Start Training without Validation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 Start Training using LSTM model python3 main.py --device 2 --lr 2e-5 --epochs 20 All Parameters --epochs  Number of epochs for training --batch_size  Batch size --lr  Learning rate for optimizer --run  Continue training on runX. Eg. --run=run1 --eval  Evaluate on training set --use_valid  Use valid dataset. If not, using the whole dataset for training --use_bert  Use bert model. If not, using the LSTM model --split_ratio  When using validation, percentage of trainset --device  GPU device to use Appendix Screenshot of our scores and rank: Progress Report Team: Salty Fish Members: Jinning Li (jinning4) Jialong Yin (jialong2) Jianzhe Sun (jianzhe2) Task: Text Classification Competition (Sarcasm Detection) 1) Which tasks have been completed? 1. We have surveyed related background knowledge and previous researches about the sarcasm detection task and sentiment analysis. 2. We have implemented the code to process the tweet data. 3. We have a quick implementation on our basic LSTM model with Pytorch. The network architecture of this model includes one LSTM layers and two dense layers. We only use the response data to train the model. We currently achieve (Precision: 0.61 Recall:0.67 F1: 0.64) on the testset. 2) Which tasks are pending? 1. We are trying to develop a more complex framework which can help make use of the context data. 2. We try to make use of some popular network architectures which were proved to be effective on the sentiment analysis task. 3. Final hyper-parameter adjusting and model ensemble. 3) Are you facing any challenges? 1. The most challenge part is how to make use of the context data provided in the dataset. Every piece of tweet in the dataset is a reply to another tweet. However, sometimes they agree with each other while sometimes they are not. 2. Overfitting. Our basic model performs much better on the trainset than the testset. We have tried using regularization and reducing the parameters but the improvement is limited. CS410 Project Proposal Team members: Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Competition to join: text classification competition Additional Questions to Answer: Q: Are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods A: The state-of-the-art neural network classifiers include Multi-layer Perceptron (MLP), Support Vector Machine (SVM), Naive Bayes, Decision Tree, Logistic Regression, etc. I have heard of some deep learning frameworks for text classification including RNN, GRU, CNN, LSTM, Bi-LSTM, Word2Vec, Conv-RNN, BERT, etc. My relevant prior experience includes training a Word2Vec model to transform the words into vectors and apply random forest and logistic regression after the embedding to classify text. Programming language: Python CS410 Course Project - Text Classification Competition: Twitter Sarcasm Detection Team name: Salty Fish Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Video The video to introduce our method and the usage of code can be found in Youtube https://youtu.be/eSR7B8TYnJY It's not uploaded to github because it exceeds 100 MB. Code Manual Source code can be found in ./code Evaluate Model and Ensemble Download pre-trained model Download checkpoint.zip from https://drive.google.com/file/d/1nRucz1yDqyoYR8jLeP6fKZt8FpqJBBUA/view?usp=sharing unzip checkpoint.zip mkdir checkpoint mkdir checkpoint/run0 mv checkpoint checkpoint/run0/checkpoint Evaluate and Generating Predictions python3 evaluate.py --run run0 --use_bert --device 3 Ensemble ``` modify ensemble_paths in ensemble.py python3 ensemble.py ``` All Parameters --run Evaluating on runX. e.g. --run=run1 --use_bert Use bert model. If not, using LSTM model. --device GPU device to use Train Model Start Training with Validation and Evaluation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 --use_valid --eval Start Training without Validation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 Start Training using LSTM model python3 main.py --device 2 --lr 2e-5 --epochs 20 All Parameters --epochs Number of epochs for training --batch_size Batch size --lr Learning rate for optimizer --run Continue training on runX. Eg. --run=run1 --eval Evaluate on training set --use_valid Use valid dataset. If not, using the whole dataset for training --use_bert Use bert model. If not, using the LSTM model --split_ratio When using validation, percentage of trainset --device GPU device to use Appendix: Competetion Data Format Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/jkghub/CourseProject	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Jeffrey Kuhn (kuhn9) I plan to complete the project on my own. Which paper have you chosen? ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No, not the identical dataset. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, it's possible to get a very similar dataset. I can retrieve articles and reviews from BBC / CNN. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/joel515/CourseProject	"Joel Kopp (joelk2) CS 410 - Fall 2020 Final Project Progress Report 1. COMPLETED: Abstracts have been scraped from the ACM Digital Library for two prominent data mining researchers: Jiawei Han (UIUC) and Philip S. Yu (UIC). To match the temporal time frame, abstracts were only obtained up until the end of 2005. It is likely that Han's abstracts were used in the actual paper, but I am not so sure about Yu. Label Author School Link Author A Jiawei Han UIUC ACM DL Author B Philip S. Yu UIC ACM DL An initial PLSA model was repurposed using the model created in MP3. A ""frequent pattern mining"" topic was found withing Han's abstracts, including a good match of coverage, but not for Yu's. I will attempt to contact the author to determine see if I can find the name of the second author. Otherwise, I will proceed with the two authors named above. (The scraper and initial data can be found here.) 2. PENDING: I need to implement the Fixed Coverage CPLSA model described in the paper. At this point I feel I understand the mechanics behind it, it is just a matter of coding it. Following that, I will compile the documentation and compose the video tutorial. 3. CHALLENGES: The main challenge is duplicating the data set used by the authors. While this is not entirely necessary, I feel that it would provide a good sanity check for my work if the data sets match. Joel Kopp (joelk2) CS 410 - Fall 2020 Final Project Proposal 1) Team members Joel Kopp (joelk2) 2) Chosen paper Subtopic - Contextual text mining Paper - ""A mixture model for contextual text mining"" Experiment - Temporal-Author-Topic analysis 3) Programming language Python 4) Datasets The dataset will be comprised of abstracts from 2 authors with substantial works that span a large-enough time frame. The abstracts will be scraped from profiles in the ACM Digital Library, similar to the experiment in the paper itself. A sample abstract can be found here: https://dl.acm.org/doi/10.1145/3410992.3410995. AMixtureModelforContextualTextMiningQiaozhuMeiDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbana,IL61801qmei2@uiuc.eduChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbana,IL61801czhai@cs.uiuc.eduABSTRACTContextualtextminingisconcernedwithextractingtopicalthemesfromatextcollectionwithcontextinformation(e.g.,timeandlocation)andcomparing/analyzingthevariationsofthemesoverdifferentcontexts.Sincethetopicscoveredinadocumentareusuallyrelatedtothecontextofthedoc-ument,analyzingtopicalthemeswithincontextcanpoten-tiallyrevealmanyinterestingthemepatterns.Inthispaper,weproposeanewgeneralprobabilisticmodelforcontextualtextminingthatcancoverseveralexistingmodelsasspecialcases.Specifically,weextendtheprobabilisticlatentseman-ticanalysis(PLSA)modelbyintroducingcontextvariablestomodelthecontextofadocument.Theproposedmixturemodel,calledcontextualprobabilisticlatentsemanticanal-ysis(CPLSA)model,canbeappliedtomanyinterestingminingtasks,suchastemporaltextmining,spatiotempo-raltextmining,author-topicanalysis,andcross-collectioncomparativeanalysis.Empiricalexperimentsshowthattheproposedmixturemodelcandiscoverthemesandtheircon-textualvariationseffectively.CategoriesandSubjectDescriptors:H.3.3[Informa-tionSearchandRetrieval]:TextMiningGeneralTerms:AlgorithmsKeywords:Contextualtextmining,context,mixturemodel,EMalgorithm,themepattern,clustering1.INTRODUCTIONAtextdocumentisoftenassociatedwithvariouskindsofcontextinformation,suchasthetimeandlocationatwhichthedocumentwasproduced,theauthor(s)whowrotethedocument,anditspublisher.Thecontentsoftextdoc-umentswiththesameorsimilarcontextareoftencorre-latedinsomeway.Forexample,newsarticleswrittenintheperiodofsomemajoreventalltendtobeinfluencedbytheeventinsomeway,andpaperswrittenbythesameresearchertendtosharesimilartopics.Inordertorevealin-terestingcontentpatternsinsuchcontextualizedtextdata,itisnecessarytoconsidercontextinformationwhenana-PermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforproThtorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationontheThrstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspeciThcpermissionand/orafee.KDD'06,August20D23,2006,Philadelphia,Pennsylvania,USA.Copyright2006ACM1-59593-339-5/06/0008...$5.00.lyzingthetopicscoveredinsuchdata.Indeed,therehavebeenseveralrecentstudiesinthisdirection.Forexample,thetimestampsoftextdocumentshavebeenconsideredinsomerecentworkontemporaltextmining[9,16,14,4].Also,author-topicanalysisisstudiedin[17],andcross-collectioncomparativetextminingisstudiedin[18].Allthesestudiesconsidersomekindsofcontextinformation,i.e.,time,authorship,andsubcollection.Time,authorship,andsubcollectionarebynomeanstheonlypossiblecontextinformationofadocument.Infact,anymetadataentryofadocumentcanindicateacontextandalldocumentswiththesamevalueofthismetadataen-trycanbeconsideredasinthesamecontext.Forexample,thesourceofanewsarticle,theauthor'sagegroup,occu-pation,andlocationofaweblogarticle,andthecitationfrequencyofaresearchpaper,areallreasonablecontextinformation.Moreover,adocumentmaybelongtomulti-plecontexts,andanycombinationofitsmetadataentriesmakesa""complex""context.Byanalyzingthevariationsoftopicsoverthesecontexts,alotofinterestingtextminingtaskscanbeaddressed,suchasspatiotemporaltextmining,author-topicevolutionaryanalysisovertime,andopinioncomparisonoverdifferentagegroupsandoccupations.However,existingtechniquesareusuallytunedforsomespecifictasks,andarenotapplicabletoconsiderotherkindsofcontexts.Forexample,onecannotdirectlyusethetem-poraltextminingtechniquestomodeltheoccupationofauthors.Thisindicatesaseriouslimitationofexistingcon-textualanalysisofthemes:everytimewhenanewcombina-tionofcontextinformationistobeconsidered,peoplehavetoseekforsolutionsinanadhocway.Therefore,itishighlydesirabletointroduceageneraltextminingproblem,contextualtextmining,whichisabstractedfromafamilyoftextminingtaskswithvarioustypesofcontextualanalysis.Itisdesirabletoderiveamodelthatishighlygeneraltoconductthecommontasksofthesespecificcontextualtextminingproblems,andeasytobeappliedtoeachofthemwithappropriateregularization.Inthiswork,wedefinethegeneralproblemofContex-tualTextMining(CtxTM)anditscommontasks,whichisabstractedfromafamilyofspecifictextminingproblems.Weextendtheprobabilisticlatentsemanticanalysis(PLSA)modeltoincorporatecontextinformation,anddevelopacontextualprobabilisticlatentsemanticanalysis(CPLSA)modeltofacilitatecontextualtextmininginageneralway.Byfittingthemodeltothetextdatatomine,wecan(1)discovertheglobalsalientthemesfromthecollectionofdoc-uments;(2)analyzethecontentvariationofthethemesinanygivenviewofcontext;and(3)analyzethecoverageofthemesassociatedwithanygivencontext.Thesetasksaregeneralandcanbeeasilyappliedtodiffer-entspecificcontextualtextminingproblems.Inthispaper,weshowthatmanyexistingcontextualthemeanalysisprob-lemscanbedefinedasspecialcasesofCtxTM,andcanbesolvedwithregularizedversionsofthemixturemodelweproposed,correspondingtothecontextinformationandtheminingtasksitinvolves.Althoughitmaynotbetheonlypossiblemodelforcontextualtextmining,themodelisquiteflexibletoadaptdifferentassumptions.2.CONTEXTUALTEXTMININGGivenacollectionofdocumentswithcontextinformation,weassumethatthereisasetoftopics,orthemesinthecol-lectionwhichvaryoverdifferentcontexts.Ourgoalisgen-erallytoconductcontext-sensitiveanalysisofthesethemes.Asstressedinpreviouswork[14],athemeinacontextual-izedtextcollectionDisaprobabilisticdistributionofwordsthatcharacterizesasemanticallycoherenttopicorsubtopic.Withoutlossofgenerality,wewillassumethatthereareal-togetherkmajorthemesinourcollection,Th={th1,...,thk}.Tomodelthecontextofadocument,weintroduceacon-ceptcalledcontextfeature,whichisdefinedasanymeta-dataofadocument(e.g.,thetimestampintemporaltextminingorauthorshipinauthor-topicanalysis).Thecontextthatadocumentbelongstocanbeindicatedbythecontextfea-turesofthisdocument,whichisformallydefinedasfollows:Definition1(Context)LetF={f1,f2,...,f|F|}beasetofcontextfeatures.AContextcinadocumentcollectionisdecidedbyanycombinationofcontextfea-turesinF,formallyc2|F|.Thewholesetofpossi-blecontextsisdenotedasC={c1,...,cn}.SupposeD={(D1,C1),...,(D|D|,C|D|)}isacollectionofdocuments,eachdocumentDiisasequenceofwordsfromavocabularysetV={w1,...,w|V|},andCiFisasetofcontextfeatureswhichareassociatedwiththedocumentDi.AdocumentDibelongstoacontextciff.Cic.Thistellsusthatadocumentcanbelongtomultiplecontexts.Inanotherword,thecontextsarepossibletooverlap.Incontextualtextmining,ourgoalistoanalyzethetop-ics/subtopicsinsuchatextcollectioninacontext-sensitiveway.Specifically,wewouldliketomodelthekmajorthemesandhowtheyvaryaccordingtodifferentcontexts,andwouldalsoliketomodelthecoverageofdifferentthemesinadoc-umentordocumentsthatsharecertainkindsofcontext.Toaccommodatecontext-sensitivethemeanalysis,wecon-sidervariationsofthesekthemesoverdifferentcontexts.Forexample,ifthecontextweareinterestedinistime,wewillassumethatthereisapotentiallydistinct""version""ofthekthemesineachdifferenttimeperiod;differentsuch""ver-sions""modelthevariationsofthemesacrosstimestamps.WeformallydefinesuchavariationasaViewofthemes.Definition2(View)Aviewofthemesinacontextu-alizedtextcollectionDisasequenceofthemesthi1,...,thik,wherethilisthevariationofthemethlaccordingtoviewvi.Wewillassumethattherearenviewsinourcollection,v1,...,vn,eachcorrespondstoacontextci.Therefore,adoc-umentisassumedtopotentiallyhavemultipleviews;pre-ciselywhichviewsaretakendependsonthedocumentanditscontext.Eachviewviisassumedtobetakeninanydoc-umentsinthecontextci,whichcanalsobeoverlapping.Definition3(ContextSupport)Thesupportofacon-textci,s(ci)isthesetofdocumentsincontextci,i.e.,s(ci)={Dj|Cjci}.Sinceeachcontextisassociatedwithaview,wealsocalls(ci)asthesupportoftheviewvi.Toanalyzethestrengthofthemes,wefurthermodelthevariablecoverageofdifferentthemesinadocument.Forex-ample,somedocumentswouldfavorsomeparticularthemesandthuswouldhavealargercoverageofthem.Definition4(Coverage)Acoverageofthemesinadocument(kj)isadistributionoverthethemesp(l|ki).Clearly,kl=1p(l|kj)=1.Wewillassumethattherearemdistinctthemecoveragesinourcollection,k1,...,km.Forexample,ifweassumethateachdocumenthasapotentiallydistinctthemecoverage,thenm=|D|.Ingeneral,however,adocumentcancoverthemesaccordingtomultiplecoverages.Forexample,ifweareinterestedinmodelingthemecoverageassociatedwithtimestamps,wemayassumethattheactualthemecoverageinadocumentwouldbeamixtureofthedocument-specificthemecoverageandanotherthemecoverageassociatedwiththetimecontextofthedocument.Weusec(kj)todenotethecontextswherethecoveragekjisapplicable,andwealsodefinethesupportofacoverageinthesamewayaswedefinethatofacontext.Definition5(CoverageSupport)Thesupportofacoveragekj,s(kj)isthesetofdocumentsinwhichthecov-eragekjistaken,i.e.,s(kj)={Di|cc(kj)s.t.Cic}.Thelatentstructureofthemes,viewsandcoveragesinacontextualizeddocumentcollectionisillustratedinFigure1. View0: th01, th02, .... , th0k View1: th11, th12, .... , th1k Viewn: thn1, thn2, .... , thnk .... Theme: 1, 2, .... , k Coverage0: Coverage1: Coveragem: .... .... .... .... c0 c1 cn c2 Collection Document Contexts Figure1:Thetheme-view-coveragestructureinatextcollectionWiththesedefinitions,thetaskofContextualTextMining(CtxTM)canbedefinedastorecoverthenviews,vi=(thi1,...,thik),i=1,...,n,andthemthemecoveragesk1,...,kmfromthecollectionD,andtoanalyzetheminacontext-sensitiveway.Therearemanydifferentwaystoan-alyzetheviewsandthemecoverages.Belowwediscussafewinterestingcases.1.Themeextraction:Wemayextracttheglobalsalientthemes.Althougheachthemethlvariesindifferentcontexts,itisalsobeneficialtohaveanexplicitmodelforthlinaglobalview.Basically,thiswillgiveusthecommoninformationthatissharedbyallthevariationsofthlinalldif-ferentcontexts.Inpractice,wealwaysincludeaglobalviewv0,whichcorrespondstoaglobalcontextc0=F.Clearly,alldocumentsDiDbelongtoc0sinceCic0.2.Viewcomparison:Wemaycomparethenviews.Thecomparisonofathemethlfromdifferentviewsusuallyrepresentsthecontentvariationofthlcorrespondingtodif-ferentcontexts.Bycomparingthilforeachviewviwhichcorrespondstocontextci,wecananalyzetheinfluenceofthecontextcionthecontentsofthl.3.Coveragecomparison:Wemaycomparethemcov-erages.Thevariationsofp(l|kj)cantellushowlikelythliscoveredbythedocumentsinthecoveragesupports(kj).Byassociatingp(l|kj)withincontextsc(kj)thatkjisapplica-ble,wecananalyzehowcloselyathemeisassociatedtoacontext,orhowcontext-sensitiveathemeis.4.Others:Withcontextualtextmining,wecanalsoanalyzeotherproblemssuchastheinfluenceofanindivid-ualcontextfeatureonthethemecoverage,e.g.,thetheme-locationdistributioninspatiotemporalthemeanalysis.Amongthesecases,2and3arethemostimportant,whichdistinguishcontextualtextminingfromthetraditionalthemeextractionwork,andtheapplicationofthemfacilitateothertypesofanalysis.Withthedefinitionofthegeneralproblemofcontextualtextmining(CtxTM),wecanshowthatsomespecificcon-textualtextminingproblemsarespecialcasesofCtxTM.Forexample,intemporaltextmining,eachcontextfeatureisatimestamp.Therefore,acontextiseitheratimestamporasetofconsecutivetimestamps,ortimeperiod.Aviewofthemesistakeninallthedocumentsinthecorrespondingtime.Thegoaloftemporaltextminingismainlytocomparethecoveragevariationoverdifferentcontexts(e.g.,themelifecyclesin[14]),andsometimesalsothecontentvariationofthemesoverdifferentviews,(e.g.,evolutionarythemepat-ternin[14]).Inauthor-topicanalysis,eachcontextfeatureisanauthor,andeachcontextiseitheranauthororasetofauthors.Eachviewisthentakeninthedocumentwiththesameauthororauthors.Weareinterestedincomparingthecontentvariationsoverdifferentviews(authors)[17].3.ACONTEXTUALMIXTUREMODELInthissection,weproposeanextensionoftheProbabilis-ticLatentSemanticAnalysis(PLSA)model[7,8],calledContextualProbabilisticLatentSemanticAnalysis(CPLSA)model,forcontextualtextmining.Ourmainideaistoal-lowadocumenttobegeneratedusingmultipleviewsandmultiplecoverages.Theviewsandcoveragesactuallyusedinadocumentusuallydependonitscontext,whichcouldbethetimeorlocationwherethedocumentiswritten,thesourcefromwhichthedocumentcomes,oranyothermeta-data.WefirstproposethegeneralCPLSAmodel,andthenintroducetwosimplifiedversionsofthismodelthatarees-peciallysuitablefortworepresentativetasksofcontextualtextmining.3.1TheCPLSAModelInCPLSA,weassumethatdocumentD(withcontextC)isgeneratedbygeneratingeachwordinitasfollows:(1)Chooseaviewviaccordingtotheviewdistributionp(vi|D,C).(2)Chooseacoveragekjaccordingtothecov-eragedistributionp(kj|D,C).(3)Generateawordusingthil.Formally,thelog-likelihoodofthewholecollectionislogp(D)=(D,C)DwVc(w,D)log(ni=1p(vi|D,C)xmj=1p(kj|D,C)kl=1p(l|kj)p(w|thil))Theparametersaretheviewselectionprobabilityp(vi|D,C),thecoveragedistributionselectionprobabilityp(kj|D,C),thecoveragedistributionp(l|kj),andthethemedistribu-tionp(w|thil).Asamixturemodel,wehaveatotalofnxkmultino-mialdistributioncomponentmodels.Eachsetofkmultino-mialdistributions,thi1,...,thik,representsapotentiallydis-tinctviewofthetopicsthatweareinterestedin.How-ever,whilewecanpotentiallyusealltheviewstogenerateadocument,oftenthegenerationofaparticulardocumentDinaparticularcontextConlyinvolvesasubsetoftheseviews.Thisisbecauseinanyinterestingcontextminingscenario,differentviewsgenerallyhavedifferentsupportingdocuments,thoughitisalsocommonfortheviewstoover-lapinsomesupportingdocuments.Morespecifically,theviewselectiondistributionp(vi|D,C)determineswhichviewswillactuallybeusedwhengenerat-ingwordsindocumentD.Thisdistributionwouldassignzeroprobabilitiestothoseviewsthatarenotselected.Forexample,iftheviewsthatwearetomodelcorrespondtothetemporalcontextofadocumentandwehaveoneglobalviewspanningintheentiretimeperiod,thenadocumentattimepointtiwouldbegeneratedusingtwodifferentviews-theviewcorrespondingtotimepointtiandtheglobalview,whichisappliedtoallthedocuments.Orthogonaltothechoiceofviews,wealsoassumethatwehavechoicesofthemecoveragedistributions.Thediffer-entcoveragedistributionsaretoreflecttheunevencoverageoftopicsindifferentcontextandtocapturethecommoncoveragepatterns.Forexample,ifwesuspectthatthecov-eragemayvarydependingonthelocationoftheauthors,wecanassociateaparticularcoveragedistributiontoeachlocation,whichwillbesharedbyallthedocumentsinthelocation.Afterwelearnsuchcoveragedistributions,wecanthencomparethemacrossdifferentlocations.Onceagain,exactlywhichcoveragedistributionstousewoulddependonthecontextofthedocumenttobegenerated.Themixturemodelcanbefittoacontextualizedcollec-tionDusingamaximumlikelihoodestimator.TheEMal-gorithm[5]canbeusedinastraightforwardwaytoestimatetheparameters;theupdatingformulasareasfollows:p(zw,i,j,l=1)=p(t)(vi|D,C)p(t)(kj|D,C)p(t)(l|kj)p(t)(w|thil)ni=1p(t)(vi|D,C)mj=1p(t)(kj|D,C)kl=1p(t)(l|kj)p(t)(w|thil)p(t+1)(vi|D,C)=wVc(w,D)mj=1kl=1p(zw,i,j,l=1)ni=1wVc(w,D)mj=1kl=1p(zw,i,j,l=1)p(t+1)(kj|D,C)=wVc(w,D)ni=1kl=1p(zw,i,j,l=1)mj=1wVc(w,D)ni=1kl=1p(zw,i,j,l=1)p(t+1)(l|kj)=(D,C)DwVc(w,D)ni=1p(zw,i,j,l=1)kl=1(D,C)DwVc(w,D)ni=1p(zw,i,j,l=1)p(t+1)(w|thil)=(D,C)Dc(w,D)mj=1p(zw,i,j,l=1)wV(D,C)Dc(w,D)mj=1p(zw,i,j,l=1)However,sincethemodelhasmanyparametersandhasahigh-degreeoffreedom,fittingitwithamaximumlike-lihoodestimator,ingeneral,wouldfaceaseriousproblemofmultiplelocalmaxima.Fortunately,incontextualtextmining,wealmostalwaysassociatethemwithappropriatepartitionsofcontext.Asaresult,themodelisoftenhighlyconstrained.Forexample,ifallweareinterestedinistocomparenon-overlappingviewsacrossdifferenttime,thenp(kj|D,C)becomesadeltafunction,i.e.,p(kj|D,C)=1ifandonlyifkjisthecoveragedistributionforthetimecontextofD,andp(kj|D,C)=0forallotherkj's.Unfortunately,evenwithsuchconstraints,themodelmaystillhavemanyfreeparameterstoestimate.Onepossibil-ityistoaddsomeparametricconstraintsuchasassumingallcoveragedistributionsarefromthesameDirichletdistri-butionasdoneinLDA[2],whichwouldclearlyreducethenumberoffreeparameters;indeed,wecaneasilygeneralizeourmodelinthesamewayasLDAgeneralizesPLSA[8].However,oneconcernwithsuchastrategyisthatthepara-metricconstraintisartificialandmayrestrictthecapacityofthemodeltoextractdiscriminativethemes,whichisourgoalincontextualtextmining.Anotherapproachistofur-therregularizetheestimationofthemodelbyheuristicallysearchingforagoodinitialpointinEM;specificheuristicswoulddependontheparticularcontextualtextminingtask.Thisapproachisadoptedinourexperimentsandwillbefur-therdiscussedinSection3.2.Inordertomodelthenoise(e.g.,commonEnglishwords)inthetext,wecoulddesignatethefirstthemeasmodelingsuchnoise.Thatis,allth1j'swillbesettomodelthenoise.Wemayfurthertieallofthemsothatwehavejustonecommonbackgroundunigramlanguagemodelth1.Thiscanalsoberegardedasapplyinganinfinitelystrongprioronthefirstthemeinallviews.3.2SpecialVersionsofCPLSAConsideringthatthemostimportanttasksofcontextualtextminingareviewcomparisonandthemecoveragecom-parisonacrosscontexts,asdiscussedinSection2,wein-troducetwospecialcasesofCPLSA,whichareparticularlyusefultodothesetwotasks.WefirstintroducethespecialversionofCPLSAtofacili-tateviewcomparison.Insomecases,weareonlyinterestedtomodelthecontentvariationofthemesacrosscontexts,e.g.,whenweareanalyzingthethemeevolutionsovertime[14],orcomparingthecommonthemesandcorrespondingspecificthemesacrosssubcollections[18].Inthesecases,wecanfairlyassumethatthethemecoverageovercontextsisfixed,thusdoesnotdependonthecontextsthatadocu-mentisin.Underthisassumption,themj=1p(kj|D,C)inthemodelwillbesimplifiedasmj=1p(kj|D).Ifwefurtherassumethatthereisonlyonecoveragekapplicabletoeachdocument,thelog-likelihoodfunctioncanbewrittenas(D,C)DwVc(w,D)log(ni=1p(vi|D,C)kl=1p(l|kD)p(w|thil))wherekDisthecoverageassociatedwiththedocumentD.Wecallthissimplifiedversionofmodelasfixed-coveragecontextualmixturemodel(FC-CPLSA).Ifwehavethreeviews,whereoneistheglobalviewandtheothertwocorrespondtosubcollections,itwillallowustocomparethecommonthemesandspecificthemesinthetwoviews,asdiscussedin[18].Ifeachviewcorrespondstoatimestamp,thismodelwillallowustoanalyzethecontentevolutionsofthemesovertime,asdiscussedin[14].Insomeothercases,weareonlyinterestedtomodelthevariationofthemecoverageovercontexts,e.g.,whenweareanalyzingthelifecycles(i.e.,strengthvariationsovertime)ofthemes.Inthesecases,wearenotinterestedinthecontentvariationoflocalthemes,andthusmaketheassumptionthatdifferentviewsofthemesarestable.Withthisassumption,wecansimplifythemodellikelihoodas(D,C)DwVc(w,D)log(mj=1p(kj|D,C)kl=1p(l|kj)p(w|thl))wherep(w|thl)istheglobalworddistributionofthemel,whichdoesnotvaryacrosscontexts.Wecallthissimplifiedmodelasfixed-viewcontextualmixturemodel(FV-CPLSA).Iftheonlycontextfeatureistime,wehavetwotypesofcoveragedistributionskDandkT,wherekDisthecoveragedistributioncorrespondingtoeachdocumentandkTisthethemecoverageforeachtimeperiod.Thiswillallowustomodelthethemelifecycles,asintroducedin[14].Ifwehavetwocontextfeatures,timeandlocation,andeachcontextisacombinationoftimestampandlocation,wealsohavetwogroupsofthemecoveragedistributions,kDandkTL.Thiswillallowustoanalyzethespatiotemporalthemedistributionsinaspatiotemporaltextminingframework.Withthesetwospecialsimplifiedversions,theCPLSAmodelcanbeappliedtosolveabroadfamilyoftextminingproblemswithcontextualanalysis.4.EXPERIMENTSWeapplythegeneralCPLSAmodelpresentedinSection3tothreedifferentdatasetsandtextminingtasks.Empiricalresultsshowthatthismodelcanmodelthethemesandtheirvariationsacrossdifferentcontextseffectively.4.1Temporal-Author-TopicanalysisInthisexperiment,weevaluatetheperformanceoftheCPLSAmodelsonauthor-topiccomparativeanalysis.Iftwoauthorshavesimilarresearchinterest,weassumethatthereisasetofcommonthemeswhichcanbefoundintheirpublications.Sincedifferentauthorhasdifferentpreferencesandfocuses,thecontentofthesethemeswillalsovarycor-respondingtoeachauthor.Previousworkonauthor-topicanalysisonlyconsidertheauthorshipofdocumentsasthecontext[17].Intuitively,however,thetopicsthatanau-thorfavorsalsoevoluteovertime.Weaddanothertypeofcontextinformation,i.e.,publicationtime,totesttheeffec-tivenessofourmodelonhandlingmultipletypesofcontexts.Wecollecttheabstractsof282paperspublishedbytwofamousDataMiningresearchersfromACMDigitallibrary.Wesplitthewholetimelineintothreespans:beforetheyear1993,from1993to1999,andaftertheyear1999.Thiswillgiveus12possibleviewsasinTable1.Sincewearenotinterestedinanalyzingthecoveragevariationsacrosscontexts(i.e.timeandauthors),weassumethecoverageofthemesonlydependsondocumentsbutnotonthecontexts.#ContextViewsFeatures(AandBaretwoauthors)0GlobalView1A;B;<92;93~99;00~05A,<92;A,93~99;A,00~052B,<92;B,93~99;B,00~05Table1:PossibleViewsinAuthor-TopicAnalysisTherefore,weusetheFC-CPLSAmodelpresentedinSec-tion3.2tomodelthethemesandtheirviewscorrespondingtodifferentcontexts.Ourgoalisthustoestimatealltheparametersintheregularizedmodel,andcomparep(w|thjl)overdifferentviewvj.ToavoidtheEMalgorithmbeingtrappedinsuboptimallocalmaximums,weneedtomakeassociationsbetweeneachthjltoitscorrespondingglobalviewthl.WeachievethisbyselectingagoodstartingpointfortheEMalgorithm.Specif-ically,webeginwithapriorofalargep(v0|D,C)toview0,whichistheglobalview.Thisensuresustogetthestrongsignalofglobalthemesinsteadoflocalbiasedthemes.Inthefollowingiterations,wegraduallydecaythispriorandVews:GlobalAuthorAAuthorBAuthorA:2000~1993~19992000~pattern0.110689project0.0444375research0.0550772close0.0805878rule0.0616733index0.0430914frequent0.040613itemset0.0432976next0.0308254pattern0.072078distribute0.0567852graph0.0343051frequent-pattern0.0393intertransaction0.03072transition0.0308254sequential0.0462879researcher0.0324659web0.0306886Authorsequential0.0359059support0.0264818panel0.0275384minsupport0.03526algorithm0.0217309gspan0.0273849Topicmethod0.0214187associate0.0258175technical0.0275384length0.0315721over0.0162951substructure0.02005Analysispattern-growth0.02035frequent0.0181942technology0.0258949threshold0.0296533fdm0.0227141gindex0.016431condense0.0184008closet0.0176081article0.0154127frequent0.0196054study0.0116576bide0.016431increment0.0138457apriori0.0170468revolution0.0154127top-k0.0176324scable0011357.magnitude0.0151909constraint0.0130636prefixspan0.0130272tremendous0.0154127without0.0175662pass0.011357size0.0114699push0.0103159pseudo0.0109016innovate0.0154127fp-tree0.0102471disclose0.011357xml0.010954Table2:Comparisonofthecontentoftheme""FrequentPatternMining""overdifferentviewsterminatetheEMalgorithmearlywhentheaverageviewdistributionforview0(i.e.,DDp(v0|D,C)/|D|)dropsunderathreshold,say0.1.ThisgivesusagoodstartingpointfortheEMalgorithm.Then,wedothisprocedureagainformultipletrialsandselectthebeststartpoint(i.e.,theonewiththehighestlikelihood).Finally,weruntheEMalgorithmbeginningwiththisselectedstartpointuntilitconverges.Theresultsforthisexperimentareselectivelypresentedinthefollowingtable.InTable2,weseethatthecontentofthisselectedthemevariesoverdifferentviews.Fromtheglobalview,inwhichalldocumentsareincluded,wecantellthatthisthemeistalkingaboutfrequentpatternmining.FromtheviewofAu-thorA,weseespecificfrequentpatternminingtechniquessuchasdatabaseprojection,apriori,prefixspan,andcloset.FromtheviewofAuthorB,weseethatheisnotasdeepintotechniquesofminingfrequentpatterns,butrathermoreas-sociatedwithintroductionalandinnovatedworkoffrequentpatternmining.Fromtheviewoftheyearsbefore1993,thecorrespondingthemebarelyhasanyconnectiontofre-quentpatternmining.Thisisreasonablehowever,sincethefirstandmostinfluentialpaperoffrequentpatternminingwaspublishedin1993.Fromtheviewofyear1993to1999,weseethatthisthemeevolutestotalkaboutassociationrules,whichisperhapsthemostimportantapplicationoffrequentpatternminingatthattime.Specifictechniques,suchasfdm(FastDistributedMiningofassociaterules)ap-pearshighintheworddistribution.Fromtheviewoftheyearsafter1999,itisinterestingtoseetheappearanceofmorenewapplicationsoffrequentpatternmining,suchasgraphsandweb.Thetermscorrespondingtospecifictech-niquesofmininggraphpatternsandsequentialpatterns,e.g.,gspanandbide,arewithhighprobabilitiesinthethemeworddistribution.Intheviewcorrespondingtoacombinedcontext(AuthorAandafter1999),thetoptermsinclude""close"",""top-k"",and""fp-tree"",whichwellrevealthepref-erencesofauthorAinfrequentpatternmining.Theviewspecificthemeforthecombinedcontext""AuthorBafter1999""isnotwellassociatedwiththeglobalthemeagain,whichisconsistenttothefactthatAuthorBisnotactivateinfrequentpatternmininganymoreafter2000.ThisexperimentshowsthattheCPLSAmodelcanex-tractandcomparethethemevariationsoverdifferentviewseffectively.4.2SpatiotemporalthemeanalysisInthisexperiments,weshowtheeffectivenessofCPLSAmodelsonspatiotemporalanalysisofthemes.Thecontextfeaturesweconsiderinthisexperimentistimestampsandlocationinformationofdocuments.Thetasksofthisspe-cificcontextualtextminingproblemare:(1)extractglobalthemesfromthecollection,whicharesharedbydifferenttimeandlocations;(2)foreachtimestamp,computethedistributionofthemeandlocations,fromwhichwecandrawthethemedistributionsnapshotsoverlocations;and(3)comparetheviewsofthemesacrosscontexts.Itisinterestingtoseethatthesecondtaskisnotacom-montaskofCtxTM.LetacontextCbedenotedas(t,l)wheretandlrefertotimeandlocation,thetaskistoesti-matep(k|D,(t,l))foreachk,andP(th,l|t)foranyt:p(th,l|t)=k:(t,l)c(k)p(th|k)p(k|t,l)p(l|t)Wecollect9377MSNSpacedocumentswithatime-boundedquerysubmittedtoGoogleblogsearch,withthekeywords""HurricaneKatrina"".Inthisdataset,7118documentspro-videexplicitlocationinformation,andthelocationsofoth-ersaretaggedas""unknown"".Wesegmentthetimestampsintosixweeks,extractandcomparethecommonthemesoverdifferentlocationsinUnitedStates.Eachcombinationofthe50Statesandsixweekconsistsaunique""context"",whichgivesus50*6=300contexts.Sincetherearemanycontexts,itisdifficulttoestimatealltheviewsprecisely.Sinceweareonlyinterestedinthestrengthvariationsofglobalthemesoverallthecontexts,itisreason-abletosimplifythemodelbyassumingthatthecontentoftheglobalthemesdoesnotvaryovercontexts.Therefore,weusetheFV-CPLSAmodelpresentedinSection3.2tomodeltheglobalthemesandtheircoveragevariationsovertimeandlocations.Wefurtherassumethatp(kC|D,C)isaconstantthatcontrolstheimpactofthecontextonselectingthecoverageofthemes.Byestimatingthefreeparameters,ourgoalistocomputethetheme-locationcoverage:p(th,l|t)=p(th,l,t)thlp(th,l,t)=p(th|kt,l)p(t,l)thlP(th|kt,l)P(t,l)wherep(kt,l|t,l)=1,p(t,l)canbecomputedfromthewordcountintimeperiodtatlocationldividedbythetotalwordcountinthecollection.Withp(th,l|t)computed,wecanvisualizethetheme-locationcoveragebyfulfillp(th,l|t)inasnapshotmap.InFigure2,weshowoneofthe10globalthemesweextractedfromtheblogdatasetanditstheme-locationcoverageatdifferenttime.Fromthetoptermsinthistheme,wecaninferthatthisthemeistalkingaboutaidanddonationsthatweremadetothehurricaneaffectedareas.Figure2welldemonstratestheevolutionoftheme-locationcoverageoverdifferenttimeperiods.Adetaileddescriptionofthemevariationovertimeandlocationcanbefoundin[13].ThenexttaskissimilartotheexperimentinSection4.1,whichistocomparetheviewsofthemesacrosscontexts.Specifically,wepartitionthestatesintofourgroups:Af-fectedStates;PeripheralStates;CoastStates;andInlandStates.Wepartitionthetimelineintospanswiththelengthoftwoweeks.ThenweusetheFC-CPLSAmodeltocom-paretheviewsofthemescorrespondingtodifferentcontexts.TheresultsareselectivelyshowninTable3.Itiseasytoseethatfromtheviewof""PeripheryStates"",thecontentofthetheme""donation""isquitesimilartothecommonthemeextractedinFigure2.Peopletendtotalkaboutdonationsandsupplieswithfood.However,fromtheviewof""AffectedAreas"",whichcorrespondstothehur-(a)Week1:08/23-08/29(b)WeekThree:09/06-09/12(c)WeekFive:09/20-09/26Figure2:Selectedsnapshotsfortheme""AidandDonation""ofHurricaneKatrina.AffectedStatesPeripheralStatesWeek1-2Week5-6medical0.0192donate0.0238donate0.0351their0.0142comfort0.0141relief0.0204help0.0296help0.0120health0.0137red0.0132relief0.0181family0.0091ship0.0133cross0.0105red0.0151rebuild0.0088volunteer0.0129link0.0086please0.0143school0.0080hospital0.0090food0.0078cross0.0142children0.0068team0.0081medical0.0074need0.0134need0.0061assist0.0081supply0.0069volunteer0.0120health0.0059care0.0072charity0.0067victim0.0084evacuee0.0057service0.0053volunteer0.0060blood0.0057parish0.0051Table3:Comparisonofthecontentofthetheme""AidandDonation""overdifferentviewsricaneaffectedstatessuchasLouisiana,peoplecaremoreaboutmedicalaidandhospitalcares.Inthefirsttwoweeks,theviewofthisthemeisstillquitesimilartothecommontheme.Howeverinthelasttwoweeks,wecannoticethatthe""helps""becomemoreaboutrebuildingandhelpingthereturningevacuees.Thisgroupofexperimentsshowthatourgeneralmodeliseffectivetoanalyzespatiotemporalthemepatterns.4.3EventImpactAnalysisInmanyscenarios,acollectionofdocumentsareusuallyassociatedwithaseriesofevents.Forexample,weblogsusuallyreflectpeople'sopinionsabouttheeventshappen-ing.Theresearchtopicscoveredbyscientificliteraturesarealsolikelytobeaffectedbytheinfluentialrelatedevents,suchastheinventionofWWW,andtheproposingofanewresearchdirection.Theimpactofsucheventcanusuallybeanalyzedbycomparingthethemesinthedocumentspub-lishedbeforeversusaftertheevent.Inthisexperiment,weapplyCPLSAontheproblemofeventimpactanalysis.Sinceeacheventgivesapossiblesegmentationofthetimeline,thisanalysisalsoprovidesanevaluationofCPLSAonmodelingoverlappingviewsthatarenotorthogonaltoeachother.Althoughtheexperimentsinprevioussectionsalsocoverssomeoverlappingviews(e.g.,aviewcorrespondingtoalocationandaviewcorrespondingtoatimestamp),theseoverlapsarecausedbydifferenttypesof,ororthog-onalcontextfeatures(e.g.,timeandlocation).Inrealityhowever,theoverlappingviewswiththesametypeofcon-textfeatureisdesirable.Forexample,abusinessanalyzermayneedtoanalyzeandcomparethecustomers'opinionsinthefirstweek,inthefirstmonth,inthefirstseason,orinthefirstyearafteranewproductisreleased.Onestrengthofourmodelisthatweallowtheanalysisviewsthatoverlapwitheachother.Inthisexperiment,weevaluateourmodeloneventimpactanalysisandoverlappingviewanalysis.Wecollecttheabstractsof1472paperspublishedin28years'SIGIRconferencesfromACMDigitalLibrary.WeselecttwoinfluentialeventstotheInformationRetrievalcommunityinthe90s.OneisthebeginningofTextRE-trievalConferences(TREC)in1992,whichprovidelarge-scalestandardtextdatasetsandjudgementsformanyre-trievalproblems.TheotheristheintroductionoflanguagemodelintoInformationRetrievalin1998,whichbeganagenreofresearchandledtoalotofpublications.OurgoalistousetheCPLSAmodeltorevealtheimpactofthesetwoeventsinIRresearch,i.e.,howthecontentofresearchtopicschangeafterthetwoevents.Toachievethis,weassigntheabstractsinSIGIRproceed-ingsintofourcontexts,eachcorrespondstoatimespan.Thefirstcontextincludesallthedocumentswerepublishedbe-fore1993,inwhichisthefirstSIGIRconferenceafterthestartofTREC.Thesecondcontextcontainsdocumentspub-lishedonorafterthat.Thethirdcontextincludesabstractsbeforetheyear1998,inwhichthefirstpaperoflanguagemodelininformationretrievalwaspublished.Thefourthcontextcontainsallabstractspublishedonorafter1998.Itisclearthatthereareoverlapsbetweenthesecontexts.Wealsoincludeaglobalview,whichcorrespondstoalltheabstractsinSIGIRproceedings.WeusethesamestrategyaspresentedinSection4.1toavoidtheEMalgorithmtobetrappedinunexpectedlocalmaximums.Weextract10salientglobalthemesfromthiscollectionandpresentthemostinterestingone.FromtheglobalviewinTable4,weseethatthisthemeistalkingaboutretrievalmodels,especiallytermweight-ingandrelevancefeedback.Thecontentofthiscommonthemevariesfromdifferentviews.FromthePre-Trecview,whichcorrespondstothetimebefore1993,weseethatvec-torspacemodeldominates,andbooleanqueriesaremen-tionedfrequently.InthePost-Trecview,however,weno-ticethatXMLretrievalmodelhasbeenpaidmoreattentionto.Also,weseespecifictypesofdata(email)andothertermsrelatedtothenatureofTREC(e.g.,collect,judge-ment,rank).Itismoreinterestingwhencomparingtheview""Pre-LanguageModel""and""Post-LanguageModel"".Weseethatbefore1998,theretrievalmodelsaredominatedbyprobabilisticmodels.After1998,however,itisveryclearthatlanguagemodeldominatesthetheme.Thetoprankedtermshavechangedtoindicatelanguagemodels,parame-terestimations,likelihoodandprobabilitydistributions,andlanguagemodelsmoothing.Thisisconsistentwithourpriorknowledge.Theoverlappingviews,forexamplePre-LMandPre-Trec,dosharesomecontentbutclearlywithdifferentfocuses.Pre-Trec,whichismorefaraway,emphasizesvectorspacemodelwhilePre-LMemphasizesprobabilisticmodels.Thisexperimentshowsthatourmethodiseffectivetoana-lyzeeventimpactandmodeltheoverlappingviews.5.RELATEDWORKThemostrelevantworkistheProbabilisticLatentSe-manticAnalysismodel(PLSA)proposedbyHofmann[7,8],whichmodelsadocumentasamixtureofaspects,whereeachaspectisrepresentedbyamultinomialdistributionoverthewholevocabulary.OurCPLSAmodelisanaturalex-tensionofPLSAtoincorporatecontext.ToavoidoverfittinginPLSA,Bleiandco-authorsproposedagenerativeaspectmodelcalledLatentDirichletAllocation(LDA),whichcouldViews:GlobalPre-TrecPost-TrecPre-LanguageModelPost-LanguageModelterm0.159983vector0.0514067xml0.0677684probabilist0.0777954model0.16867relevance0.0751814concept0.0297583element0.0212121model0.0431573language0.0752643weight0.0659849extend0.0297405email0.0197383logic0.0403557estimate0.0520434feedback0.0372254model0.0291697collect0.0191258ir0.0337741parameter0.0281169SIGIRindependence0.031063space0.0236088locate0.0187425boolean0.028073distribution0.0268227model0.0309212boolean0.0151455judgment0.0140086fuzzy0.0201544probable0.0205655frequent0.0233021function0.0123171rank0.010205algebra0.0199632smooth0.0197662probabilist0.018762u0.00898533overlap0.00975133probable0.0124902score0.0166799document0.0173198feedback0.00860945contextual0.00936265estimate0.0119202retrieval0.0137085assume0.0172082specify0.0083182solution0.00913weight0.0111257markov0.0118979dependency0.0157547correlate0.00779721subtopic0.00791172rank0.0107045likelihood0.00585364Table4:ComparisonofthemecontentoverdifferentviewsinSIGIRcollectionalsoextractasetofthemesfromadocumentcollection[2].LDA,however,doesnotmodelcontexteither.Althoughwehavenotexploredit,onecanalsomakeLDAcontextualizedinthesamewayaswehavedonetoPLSAinthispaper.Re-cently,someextensionsofthisworkhaveconsideredsomespecifictypesofcontext.Forexample,temporalcontextisconsideredin[6,16,4,14].Multi-collectioncontextisan-alyzedin[18].Author-topicanalysisisproposedin[17].Lietal.proposedaprobabilisticmodeltodetectretrospec-tivenewseventsbyexplainingthegenerationof""fourWs1""fromeachnewsarticle[11].Ourworkisageneralizationofthesestudiesofspecificcontextandprovidesageneralprob-abilisticmodelwhichcanbeappliedtoallkindsofcontext.TemporalcontextisalsoaddressedinKleinberg'sworkondiscoveringburstyandhierarchicalstructuresinstreams[9]andsomeworkontopic/event/trenddetectionandtracking(e.g.,[1,3,12,10,15]).However,mostofthisworkassumesonedocumentonlybelongstoonetopicandcannotbeeasilygeneralizedtoanalyzeothercontexts.6.CONCLUSIONSInthispaper,wepresentastudyofthegeneralproblemofcontextualtextmining.Weformallydefinedthebasictasksofcontextualthemeanalysis,andproposedanovelprob-abilisticmixturemodeltoextractthemesandmodeltheircontentandcoveragevariationsoverdifferent,possiblyover-lappingcontexts.Theproblemdefinitionandtheproposedmodelarequitegeneralandcoverafamilyofspecificcontex-tualthemeanalysisproblemsandmethodsasspecialcases.Empiricalexperimentsonthreedifferentdatasetsshowthattheproposedmodeliseffectiveforextractingthethemesandcomparingtheviewsandcoveragesofthemesacrossquitedifferentcontexts.Ourworkisaninitialsteptowardageneralmodelforcon-textualtextmining.Animportantfutureresearchdirectionistofurtherstudyhowtobetterestimatetheproposedmix-turemodelasdiscussedinSection3.1.Anotherimportantfutureresearchdirectionistocreateevaluationcriteriaandjudgementssothatwecanquantitativelyevaluatedifferentcontextualtextminingapproaches.7.ACKNOWLEDGMENTSThisworkwasinpartsupportedbytheNationalScienceFoundationunderawardnumbers0425852,0347933,and0428472.8.REFERENCES[1]J.Allan,J.Carbonell,G.Doddington,J.Yamron,andY.Yang.Topicdetectionandtrackingpilotstudy:Finalreport.InProceedingsofDARPABroadcastNewsTranscriptionandUnderstandingWorkshop,1998.1who,when,whereandwhat(keywords)[2]D.M.Blei,A.Y.Ng,andM.I.Jordan.Latentdirichletallocation.J.Mach.Learn.Res.,3:993-1022,2003.[3]S.BoykinandA.Merlino.Machinelearningofeventsegmentationfornewsondemand.Commun.ACM,43(2):35-41,2000.[4]C.C.Chen,M.C.Chen,andM.-S.Chen.Liped:Hmm-basedlifeprofilesforadaptiveeventdetection.InProceedingofKDD'05,pages556-561,2005.[5]A.P.Dempster,N.M.Laird,andD.B.Rubin.MaximumlikelihoodfromincompletedataviatheEMalgorithm.JournalofRoyalStatist.Soc.B,39:1-38,1977.[6]T.L.GriffithsandM.Steyvers.Fidingscientifictopics.ProceedingsoftheNationalAcademyofSciences,101(suppl.1):5228-5235,2004.[7]T.Hofmann.Probabilisticlatentsemanticanalysis.InProceedingsofUAI'99.[8]T.Hofmann.Probabilisticlatentsemanticindexing.InProceedingsofACMSIGIR'99.[9]J.Kleinberg.Burstyandhierarchicalstructureinstreams.InProceedingsofKDD'02,pages91-101.[10]A.Kontostathis,L.Galitsky,W.M.Pottenger,S.Roy,andD.J.Phelps.Asurveyofemergingtrenddetectionintextualdatamining.SurveyofTextMining,pages185-224,2003.[11]Z.Li,B.Wang,M.Li,andW.-Y.Ma.Aprobabilisticmodelforretrospectivenewseventdetection.InProceedingsofSIGIR'05,pages106-113,2005.[12]J.MaandS.Perkins.Onlinenoveltydetectionontemporalsequences.InProceedingsofKDD'03,pages613-618,2003.[13]Q.Mei,C.Liu,H.Su,andC.Zhai.Aprobabilisticapproachtospatiotemporalthemepatternminingonweblogs.InProceedingsofWWW'06,pages533-542,2006.[14]Q.MeiandC.Zhai.Discoveringevolutionarythemepatternsfromtext:anexplorationoftemporaltextmining.InProceedingofKDD'05,pages198-207,2005.[15]R.Nallapati,A.Feng,F.Peng,andJ.Allan.Eventthreadingwithinnewstopics.InProceedingsofCIKM'04,pages446-453,2004.[16]J.Perkio,W.Buntine,andS.Perttu.Exploringindependenttrendsinatopic-basedsearchengine.InProceedingsofWI'04,pages664-668,2004.[17]M.Steyvers,P.Smyth,M.Rosen-Zvi,andT.Griffiths.Probabilisticauthor-topicmodelsforinformationdiscovery.InProceedingsofKDD'04,pages306-315,2004.[18]C.Zhai,A.Velivelli,andB.Yu.Across-collectionmixturemodelforcomparativetextmining.InProceedingsofKDD'04,pages743-748,2004. CS410 Course Project - FC-CPLSA Background This project attempts to duplicate the Temporal-Author-Topic analysis in section 4.1 of the paper A Mixture Model for Contextual Text Mining (KDD 2006) by Qiaozhu Mei and ChengXiang Zhai. This model replicates the fixed-coverage contextual mixture model (FC-CPLSA) covered in section 3.2 of the research paper. The FC-CPLSA model is a specialized version of the CPLSA mixture model where the coverage over different contexts remains fixed. The experiment attempts to perform an author-topic comparitive analysis on reasearch paper abstracts between two different authors over three different time periods. The context features here are the author of the paper and the time which it was written. In this case, each context featre, and combinations thereof, are evaluated as different ""views"". The combination of all context features together form a global topic view. The mixture model is constructed such that each subview is then related to the global view, but specific to one of the two authors, one of the three timeframes, or one of the six combinations of author and timeframe. The following table from the reasearch paper illustrates the 12 applicable views more clearly: | # Context Features | Views | | --------------------- | ------------------------------------------ | | 0 | Global View | | 1 | Author A; Author B; < 1992; 1993 to 1999; 2000 to 2005 | | 2 | A and < 1992; A and 1993 to 1999; A and 2000 to 2005; B and < 1992; B and 1993 to 1999; B and 2000 to 2005 | The code works by essentially adding a ""view"" matrix to the vanilla PLSA algorithm. This matrix holds the probability of a document/context belonging to a specific view. Since a document and context has zero probability of belonging to a view that does not pertain to that context, the corresponding location of that document/view on the matrix is initially zeroed out. This is how the contextual features were incorporated as additional features to the mixture model. Data An attempt was made to replicate the data used in the Temporal-Author-Topic experiment of section 4.1, which originally consisted of the abstracts from ""two famous Data Mining researchers"" from the ACM Digital Library prior to the papers publication in 2006. Since the names of the two authors was not provided, an guess was made using the listing found here. Abstracts were then scraped for Jiawei Han (UIUC) and Philip S. Yu (UIC) published prior to 2006. The processed CSV file containing the associated metadata can be found here. Setup This setup assumes that the user has git and Python >= 3.7 installed. Clone this repository on a local workstation: git clone https://github.com/joel515/CourseProject.git One the package is cloned, cd into the CourseProject directory and run the setup.py installation script: python setup.py install This should pull the necessary dependencies to run the mixture model. If this fails, or if you prefer to install libraries manually, the list of dependencies is as follows: numpy pandas nltk The latest versions of each should suffice. Test Usage To use, you will either need to cd into the CourseProject/cplsa folder, or use the relative or absolute path to the CourseProject/cplsa/cplsa.py script. The output from either command will be a file titled ""CPLSA-<timestamp>.out"" containing run metadata and a full list of topic/view coverages. Quick and Dirty To run the script using the data provided and achieve the optimal coverage results, at least the best results that I had achieved, run the following command: python cplsa.py ../data/all_abstracts.csv ""author==1:author==2"" ""year<=1992:year>=1993 and year<=1999:year>=2000"" -t 20 -p 100 -wi 50 -th 0.3 --noSTEM Note that this will take some time to run on a normal workstation (roughly an hour). It will find an optimal solution for the 12 views with 20 different topics. Word stemming is omitted from the vocabulary preprocessing. By default, it will run 50 different ""warmup"" runs to find the optimal starting point (up to 50 iterations each) with an artificially large prior of 100 set on the global view to ""ensure a strong signal from global themes"", as per the original research paper. Each warmup run will iterate until the mean global view probability reaches 0.3 (slightly modified from the paper, which suggests 0.1), or until it hits 50 iterations. The optimal starting point (the one with the largest MLE) is ""pickled"" and then restarted for the full, 1000 iteration analysis. Convergence is reached when the difference between the previous log-likelihood and the current one is less than 0.001 (around 380 iterations or so for this dataset). Quicker and Dirtier You can also run a less optimal set of iterations to simply check that the package is running correctly: python cplsa.py ../data/all_abstracts.csv ""author==1:author==2"" ""year<=1992:year>=1993 and year<=1999:year>=2000"" -t 5 -w 2 -th 0.15 -e 0.1 --noSTEM This should run in much less time, but will give less than optimal, but decent, results. This time we are only asking to evaluate 5 topics with only 2 warm up runs, iterating until the mean global view probability reaches 0.15. The convergence criterion (i.e., the difference between the previous and current log-likelihood) is now only 0.1, so it should converge much quicker. General Usage What follows is a description of the inputs and arguments of the FC-CPLSA package for a general analysis. Inputs There are 3 required inputs to run this context mixture model: a CSV file containing the documents and associated metadata, and two strings containing Boolean operations to categorize two columns of the metadata, separated by colons. CSV File The CSV file can contain any amount of information, as long as there is one column labeled text containing the documents to be evaluated, and two additional labeled columns containing metadata to use as context. For instance, the data associated with this project looks like the following: | id | author | year | text | | --- | -- | -------- | -------------------- | | 1 | 1 | 2005 | Graphs have become ... | | ... | ... | ... | ... | | 363 | 2 | 1985 | The performance of ... | The text column contains the abstracts from either author 1 or 2. It is important to note the spelling and case of the column titles for the contextual metadata (author and year in this case), as they will be used in the following inputs to generate the views. Note that the id column is ancillary in this case and hence ignored. View Specification The next two inputs are used to generate the various contextual views to use in the mixture model. The format for the inputs should be strings enclosed in double quotes. Each input will refer to only one of the metadata columns and contain multiple Python-formatted Boolean operations to perform on that column's metadata, with each operation separated by a colon. Each Boolean operation in the string is used to extract a one-feature view. The code will then combine the different combinations of Boolean operations from the two inputs to extract two-feature views. In our example, the second input is ""author==1:author==2"", which consists of two valid Python Boolean operations to perform on the author column of metadata. The input will create two views, one consisting of the text from the author labeled 1, and the other consisting of the text from the author labeled 2. Likewise, the third input performs similar Boolean operations, this time on the year column (""year<=1992:year>=1993 and year<=1999:year>=2000""). This input contains three valid Python Boolean operations which will result in three one-feature views. Specifically, one view containing text from years prior to 1993 (year<=1992), one view containing text from the period between 1993 and 1999 (year>=1993 and year<=1999), and one view containing text after 1999 (year>=2000). The code will automatically create two-feature views through merging each combination of Boolean operations with a logical ""AND"". So in our example, the code will create the following six additional operations: author==1 and year<=1992, author==1 and year>=1993 and year<=1999, author==1 and year>=2000, author==2 and year<=1992, author==2 and year>=1993 and year<=1999, and author==2 and year>=2000 In total, if there are n operations specified in the first input string, and m operations specified in the second string, we will end up with n + m + nm + 1 views. In our example, we will have one global view, five one-feature views, and six two-feature views, for a total of 12 views. Numerical Arguments -w, --warmup [Integer, default=20] The number of warm-up E-M runs to perform to discover the best starting point. The mixture model will initialize the probability matrices randomly, leading to potentially local maxima. To find the optimal result, the code starts at different random points and uses the initial run that gives the maximum log-likelihood. -p, --prior [Float, default=1.0] Prior to assign to the global view probabilities. According to Mei et. al., to ensure a strong signal from the global themes, we need to assign an artificially large prior to the global view probabilities. -th, --threshold [Float, default=0.1] Mean global view probability threshold for warm-up convergence. The warm-up iterations will run until the mean probability for all of the global views falls below this value. -wi, --warmup_iter [Integer, default=25] Maximum number of warm-up E-M iterations per run. It is possible for the mean global view probability to converge higher than the supplied threshold. In this case, a maximum number of iterations is specified to kill that warm-up run. This starting point will be discarded. -t, --topics [Integer, default=10] Number of global topics/themes to extract. -i, --iterations [Integer, default=1000] Maximum number of E-M iterations if mixture model convergence cannot be obtained. -e, --epsilon [Float, default=0.001] Minimum log-likelihood estimate error for convergence. E-M convergence occurs when the difference between the prior log-likelihood estimate and the current log-likelihood estimate falls below this value. Flag Arguments -s, --save If specified, will save out the Corpus object containing the vocabulary and final matrix values as a pickled file upon E-M completion. This pickled file can be extracted later for further examination. Preprocessor Flags By default, the code will preprocess the vocabulary by performing a lower-case transformation, stopword removal, and Porter word stemming. The code will also automatically remove any non-ASCII characters, numbers, and any punctuation except -. The following flags can be used to override this automation. -noASCII, --noASCII Switch off non-ASCII character removal. -noLC, --noLC Switch off lower-case transformation. -noPUNC, --noPUNC Switch off punctuation removal. -noNUM, --noNUM Switch off number removal. -noSTEM, --noSTEM Switch off Porter word stemming (uses the PorterStemmer functionality from the nltk.stem library). -noSTOP, --noSTOP Switch off stopword removal (uses the English stopwords list from the nltk.corpus library). Results At a glance, the code seems to do a good job at finding general themes throughout the abstracts. With enough topics specified (in this case 20), it will capture a global ""frequent pattern mining"" theme similar to what was presented in the paper by Mei, et. al. | Topic: 15 | View: global | | --------- | ------------ | | mining | 0.06549781470307586 | | patterns | 0.04130088878399839 | | pattern | 0.031002929953296773 | | sequential | 0.017802974506818257 | | frequent | 0.015000041355054937 | | structures | 0.01246208861695447 | | algorithms | 0.010986754324057484 | | approach | 0.0106817830308471 | | efficient | 0.010681189139252758 | | information | 0.008901491869253191 | It also captures themes that are representative of their context - temporal and author context is captured by the view coverage. In this case, coverages for the ""frequent pattern mining"" global topic for author 1 published after 2000 matched up decently with the paper. Indeed, author 1 does seem to spend more time covering frequent pattern mining during this timeframe. | Topic: 15 | View: author==1 and year>=2000 | | --------- | ------------ | | mining | 0.0597014953071478 | | pattern | 0.03731343384903124 | | frequent | 0.029850750414016154 | | patterns | 0.029850745701384865 | | frequent-pattern | 0.022388049893103487 | | databases | 0.018656738476720114 | | examine | 0.018656723360443182 | | effectiveness | 0.018656708244252907 | | sequential | 0.01492538594417156 | | study | 0.014925382719363917 | There are instances that the linkage between views and global themes is not always fully captured, however. Many of the views also seem to be very localized - only giving coverages of 1 or 2 abstracts - albeit within the proper context. For instance, it seems that the author 2 view captures some themes of frequent pattern mining (""segmentation"" does appear in frequent pattern mining abstracts for author 2), but a closer look shows that the overall theme for this view seems to be leaning toward segmentation approaches to proxy caching. The following coverage seems to be very specific to one particular abstract: | Topic: 15 | View: author==2 | | --------- | ------------ | | media | 0.07857142857142857 | | caching | 0.06428571428571428 | | segmentation | 0.02857142857142857 | | large | 0.02857142857142857 | | cache | 0.02142857142857143 | | segments | 0.02142857142857143 | | size | 0.02142857142857143 | | whole | 0.02142857142857143 | | proxy | 0.02142857142857143 | | objects | 0.02142857142857143 | The reason for this may be my application of the global view prior. The prior is applied by replacing the values for each document for the global view in the view probability matrix with the specified prior. The view probability matrix is subsequently normalized. It is possible that this is not the best way to do this, or, my priors were too high, giving too strong of a signal to the global view. Multiple priors were tried, and it did seem that the higher priors performed slightly better, however. Another issue may be my implementation of the maximization step, or specifically how I formulated the coverage distribution. In general, any summations over coverages were removed, since we are using a fixed coverage approximation. Additionally, the coverage distrubtion under this assumption becomes the probability of a topic given a document's coverage. Therefore, for the formulation for p(l\|kD), I removed the summation over all documents/contexts, leaving a matrix of size number_of_topics by number_of_documents. This essentially gives us the document coverage probability from the vanilla PLSA formulation. I am not 100% sure that this would be the correct formulation."
https://github.com/john-james-sf/CourseProject	IR Competition Project Proposal John James jtjames2 This project aims to leverage the information retrieval competition to explore state-of-the-art learn to rank system configuration document retrieval models. Learn to rank document retrieval system configuration is an area of active research that seeks to improve document retrieval performance by predicting system configuration parameters for a query that maximizes the likelihood of relevant documents. Approach The proposed approach is adapted from Deveaud et. al [1] and is comprised of the following four phases:  Pre-processing: Given a collection, the first step is to determine all feasible system configurations C and conduct document indexing. Next, the documents are ranked and the effectiveness of each query/system configuration pair is stored to file. The query and system configuration features serve as the feature set and the effectiveness scores represent the labels of the training set.  Training: This step makes use of the training examples constructed from the query features, system configuration features, and relevance labels measured by the respective evaluation metric. The learning to rank algorithm trains a model to maximize the effectiveness metric (e.g. nDCG), Once the training is completed, a learned model is generated.  Document Ranking: Taking as input, an unseen test set of queries, the trained model produces a ranked list of system configurations for each query.  Evaluation: Finally, the top ranked system configuration for each query is applied and the overall system performance is computed. Features The learn to rank features include:  Retrieval Model Features such as: o Absolute Discount Smoothing o Dirichlet Prior Smoothing o Jelinek-Mercer Smoothing o Okapi BM25 o Pivoted Length Normalization  Query Features including: o Query Statistics e.g., Mean and standard deviation variants of IDF o Linquistic Features: Synonyms Hyponyms, Meronyms, etc... I look forward to exploring these techniques in the IR competition! CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/juan4bit/CourseProject	"Pattern annotation Table of Contents vim-markdown-toc GFM Introduction Prerequisites Installation Implementation Selecting DBLP inproceedings (conferences) Pattern extraction and compression Semantic annotation Tutorial Selecting DBLP inproceedings (conferences) Pattern extraction and compression Semantic annotation Presentation vim-markdown-toc Introduction In this project I will try to reproduce the results of the following paper: * Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 This paper proposes using paradigmatic and syntagmatic relationships from text data in order to annotate non-text data with semantic context in the same way a dictionary would define a word with synonyms (paradigmatic patterns) and examples of the word being used in a context (syntagmatic context). In this particular implementation, the non-text data represents items from a transactional database, instantiated as authors from major computer conferences, and the text data represents the titles from such conferences. Prerequisites You will need a Unix machine to run this tool, I used Ubuntu 20.04 at the time of developing and testing. You will also need to install Python 3 and the latest Java Runtime Environment. Installation Clone the project from the code repository: sh git clone https://github.com/juan4bit/CourseProject.git cd CourseProject Once in the project folder, install the following Python dependencies: sh python -m pip install -U pip wheel Optional: You may additionally want to setup a virtual environment for this project by running: sh python -m venv ~/.envs/CourseProject source ~/.envs/CourseProject/bin/activate Finally, install the following Python dependencies: sh pip install -r requirements.txt And download the SPMF tool and place the jar file in the ./lib folder of the project directory. You may also need to give this file executable permissions: sh chmod +x spmf.jar Implementation I decided to implement this paper in three stages that I describe in detail below. Selecting DBLP inproceedings (conferences) The original paper tests the algorithm by selecting a subset of conferences from the DBLP dataset, a 3GB+ XML file with bibliographic entries on major computer science journals, theses and proceedings. For the purpose of this paper, we are only interested on the proceedings (conferences). Furthermore, because the amount of conferences is too large, I also decided to select only those falling in a given date range (determined by year). There are existing tools to manipulate and filter XML files, namely XSL templates, but because of the large size of the original DBLP dataset, I was not able to get any of them working so instead I decided to implement my own script which incrementally reads the DBLP file and selects only the items and fields I'm interested in, that is, the conference titles and its authors. Additionally, I also preprocess the title text through stemming and removal of stop words as described in the original paper. This stage can be run via the python main.py scrape command which I will explain how to use in the next section. Pattern extraction and compression Before extracting paradigmatic and syntagmatic relationships, we first need to mine patterns from the non-text data (authors) and text data (titles). The paper suggests using FP-Close algorithm for the former and CloSpan for the latter which instead of implementing them myself I decided to use a third party tool, SPMF, that I call internally from my script code, please make sure the dependency is installed as described in the previous section. After the Closed Frequent Patterns are extracted, the paper suggests two compression algorithms to reduce the overall number of entries for future stages in the pipeline, aiming to reduce redundancy among the patterns in each dataset. I decided to implement one of the algorithms, One-pass Microclustering, where you calculate the Jaccardian distance between each pair of patterns in a given dataset and one by one, it starts to either append a pattern to an existing cluster if the Jaccardian distance to the farthest item in the group falls below a threshold or creates a brand new cluster for the pattern. From each cluster then I select the pattern that is, on average, closer to the other patterns in the group, that is, the medoid pattern. This stage can be run via the python main.py mine command which I will explain how to use in the next section. Semantic annotation Given a preprocessed DBLP dataset and a list of Frequent Patterns for conference authors and titles, the last stage of the pipeline is a script that expects a query pattern as input (either an author, a list of authors or a text phrase) and returns its: Syntagmatic context. Given the definition of Mutual Information , where x and y are binary variables that represent each whether a pattern shows up in the database or not and P represents either the single or joint probability of these events happening, the algorithm scores each pattern from the list against the query and selects the top N as its context. Mutual Information can be understood as the reduction of uncertainty from a pattern once another one (e.g. the query) is given, the more reduction, the more likely these two patterns are related. Paradigmatic patterns. Given the set of patterns determined in the previous step, the algorithm calculates vectors of Mutual Information scores from each pattern in the original list against the patterns from the query context. Then it computes the cosine similarity between these vectors and the one from the query context and selects the top N patterns as paradigmatic relations. The rationale is that if the context patterns from the query reduce uncertainty in a similar way for another pattern, then it's likely that these two patterns are similar given a context. Paradigmatic transactions. The last step is similar to the previous one except in how the context of a transaction is calculated. For each pattern of the query context, the algorithm checks whether it appears in a transaction or not, if it does then it scores 1, otherwise -1. The original paper assigns 0 for a non-match but in practice I found out this value didn't yield good results. This stage can be run via the python main.py annotate command which I will explain how to use in the next section. Tutorial As mentioned in the previous section, there are three stages in the pipeline that can be run as individual commands. The tool provides help messages when running python main.py --help or python main.py cmd --help, where cmd can be either scrape, mine or annotate. In this section, I'll explain in detail these help messages and provide examples but before you start, make sure to download the DBLP dataset and unzip the XML file together with its DTD definition file. Selecting DBLP inproceedings (conferences) Running python main.py scrape --help yields the following output: ```sh usage: main.py scrape [-h] --dblp_file DBLP_FILE --article_file ARTICLE_FILE [--from_year FROM_YEAR] optional arguments: -h, --help show this help message and exit --dblp_file DBLP_FILE REQUIRED: the path to the DBLP input file --article_file ARTICLE_FILE REQUIRED: the path where the selected articles will be printed in XML format --from_year FROM_YEAR selects articles from no earlier than the provided year ``` So you can select conferences from the DBLP dataset by running a command similar to this one: sh python main.py scrape --dblp_file dblp.xml --article_file articles.xml --from_year 2010 This will read dblp.xml, select all inproceedings (conferences) no earlier than 2010 and print the preprocessed items in articles.xml in the following format: ```xml xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""? Fast multipoint evaluation and interpolation of polynomials in the LCH-basi s over F 2020 fast multipoint evaluate interpolation polynomial lch-basis f axel mathieu-mahias michael quisquater ... ``` The title field stores the original title of the conference while label stores its preprocessed version (after stemming and then removing stop words). Pattern extraction and compression Running python main.py mine --help yields the following output: ```sh usage: main.py mine [-h] --dblp_file DBLP_FILE --title_file TITLE_FILE --author_file AUTHOR_FILE --title_support TITLE_SUPPORT --author_support AUTHOR_SUPPORT --title_distance TITLE_DISTANCE --author_distance AUTHOR_DISTANCE optional arguments: -h, --help show this help message and exit --dblp_file DBLP_FILE REQUIRED: the path to the DBLP input file --title_file TITLE_FILE REQUIRED: the path where the title patterns will be printed --author_file AUTHOR_FILE REQUIRED: the path where the author patterns will be printed --title_support TITLE_SUPPORT REQUIRED: the minimum support [0, 1] for title patterns. --author_support AUTHOR_SUPPORT REQUIRED: the minimum support [0, 1] for author patterns. --title_distance TITLE_DISTANCE REQUIRED: the Jaccard threshold [0, 1] to use when compressing title patterns --author_distance AUTHOR_DISTANCE REQUIRED: the Jaccard threshold [0, 1] to use when compressing author patterns ``` So you can mine and compress patterns by running a command similar to this one: sh python main.py mine --dblp_file articles.xml --title_file titles.txt --author_file authors.txt --title_support 0.003 --author_support 0.001 --title_distance 0.9 --author_distance 0.9 This will read a preprocessed DBLP dataset (it assumes a label field exists for each inproceedings element which is not the case in the original DBLP file) and print to titles.txt and authors.txt the list of titles and authors respectively that were mined as frequent patterns. Title subsequences (that's what CloSpan generates) will be space separated while author itemsets will be semicolon separated. Title and author support represent the coverage percentage that a pattern needs to exhibit to be considered frequent, in this case, a title subsequence needs to show up in 0.3% of the transactions and an author itemset 0.1%. Title and author distances are the Jaccardian threshold described in the previous section, used to compress the pattern list by removing redundancy, the larger the threshold, the more agressive the compression is. Semantic annotation Running python main.py annotate --help yields the following output: ```sh usage: main.py annotate [-h] --db_file DB_FILE --title_file TITLE_FILE --author_file AUTHOR_FILE -q QUERY --type {author,title} -n1 N_CONTEXT -n2 N_SYNONYMS -n3 N_EXAMPLES optional arguments: -h, --help show this help message and exit --db_file DB_FILE REQUIRED: the XML input file with all the transactions --title_file TITLE_FILE REQUIRED: the input file that stores the patterns for titles --author_file AUTHOR_FILE REQUIRED: the input file that stores the patterns for authors -q QUERY, --query QUERY REQUIRED: the query pattern to enrich with semantic annotations --type {author,title} REQUIRED: the type of the query pattern -n1 N_CONTEXT, --n_context N_CONTEXT REQUIRED: the number of context indicators to select -n2 N_SYNONYMS, --n_synonyms N_SYNONYMS REQUIRED: the number of semantically similar patterns to select -n3 N_EXAMPLES, --n_examples N_EXAMPLES REQUIRED: the number of representative transactions to select ``` So you can find the semantic annotations of a given query pattern by running a command similar to this one: sh python main.py annotate --dblp_file articles.xml --title_file titles.txt --author_file authors.txt -q ""network"" --type title -n1 7 -n2 5 -n3 3 DBLP file, title file and author file paths point to the preprocessed DBLP dataset and the list of title and author patterns respectively. The query type can be either title or author, for the former just write any phrase surrounded by double quotes and for the latter write a semicolon-separated list of authors (casing doesn't matter but letter matching has to be identical so beware typos) also surrounded by quotes. N1, N2 and N3 represent the number of syntagmatic patterns and the number of paradigmatic patterns and transactions to retrieve respectively. The output will look something like the XML below: xml <definition> <pattern> <title>network</title> </pattern> <context> <pattern> <title>deep neural</title> </pattern> <pattern> <title>convolution neural</title> </pattern> <pattern> <title>network image</title> </pattern> <pattern> <title>graph neural</title> </pattern> <pattern> <title>generative adversarial</title> </pattern> <pattern> <title>graph convolution</title> </pattern> <pattern> <title>network 3d</title> </pattern> </context> <synonyms> <pattern> <title>method</title> </pattern> <pattern> <title>optimize</title> </pattern> <pattern> <title>deep learning</title> </pattern> <pattern> <title>explore</title> </pattern> <pattern> <title>evaluate</title> </pattern> </synonyms> <examples> <transaction> <title>Dual-domain Deep Convolutional Neural Networks for Image Demoireing.</title> <author>vien gia an</author> <author>hyunkook park</author> <author>chul lee</author> </transaction> <transaction> <title>A topological encoding convolutional neural network for segmentation of 3D mu ltiphoton images of brain vasculature using persistent homology.</title> <author>mohammad haft-javaherian</author> <author>martin villiger</author> <author>chris b. schaffer</author> <author>nozomi nishimura</author> <author>polina golland</author> <author>brett e. bouma</author> </transaction> <transaction> <title>FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary.</ title> <author>yingzhen yang</author> <author>jiahui yu</author> <author>nebojsa jojic</author> <author>jun huan</author> <author>thomas s. huang</author> </transaction> </examples> </definition> Presentation You can find a recorded version of the tutorial here."
https://github.com/kaipak/CourseProject	Team Kayak Karlin Dye and Kai Pak CS 410 Text Information Systems, Fall 2020 December 13, 2020 BERT For 100% Accuracy (Sarcasm) 13th December, 2020 Introduction In this paper, we discuss the implementation of a pre-trained BERT NN model to a binary classification problem based on NLP characteristics to detect sarcasm in a dataset consisting of a series of Twitter messages (tweets). We find that the BERT model performs exceedingly well with minimal data processing and hyperparameter tuning. Through repeated experiments, the team consistently scored .71-.75 on F1 with a final F1 score on the leaderboard of 0.745 and beating the baseline. The following sections provide some information on the BERT language model, the infrastructure, language, and libraries for this project, the code, a discussion about the training process and results, and instructions on how to reproduce our final output. Introduction to BERT Language models are key components for applications ranging from speech recognition to information retrieval. In the realm of information retrieval, often a unigram model or bag of words representation is employed. However, there are much more advanced language models that employ deep neural networks that can take word context into account and output embeddings of words in a continuous space. BERT, or Bidirectional Encoder Representations from Transformers, is one of these deep learning language models that is able to include context from both the left and right sides of words. BERT was developed by researchers at Google AI Language. More specifically the BERT language model is a pre-trained neural network that through transfer learning can be refined for a particular corpus of new documents. Since the output of word embeddings from the BERT language model includes contextual understanding the embeddings perform well in the realm of text classification. The BERT neural network architecture consists of 24 layers of transformer blocks, 16 attention heads, and 340 million parameters. The BERT model employs a preprocessing step for input text that can take into account a pair of sequential sentences and includes information about the word positions, sentence positions, and the words themselves. This preprocessing step for text input is an important component of the model training process. Once input is properly preprocessed the BERT model uses some novel techniques as part of the training process. One challenge involved is how to include both left and right context in a final output embedding. The BERT researchers solved this problem by utilizing a technique called Masked LM (MLM) which masks 15% of input tokens at random and then uses a classification layer to predict the masked token. An additional strategy that the BERT model uses to help encode context is a technique called Next Sentence Prediction (NSP). The process involves an additional classification layer that is given pairs of sentences and is trained to predict whether one of the sentences in the pair is subsequent from the first. This technique is reliant on the preprocessing of text input that provides information about sentence positions. During training a random sentence from the corpus is paired with an input sentence 50% of the time and the other 50% of the time is paired with the actual subsequent sentence pair from the input. BERT has been pre-trained on an extremely large corpus of text and is often fine-tuned for a specific application. BERT can be used in a wide range of applications including sentiment analysis, next sentence classification, question answering tasks, named entity recognition (NER), and many more. The use of BERT for many of these tasks have achieved state-of-the-art results. There are many options for using BERT for text classification tasks. One common approach is the use of the PyTorch Transformers library from HuggingFace. This library contains pre-trained BERT models of different sizes and for different languages and use-cases. The full documentation for how to use the library is available at: https://huggingface.co/transformers/model_doc/bert.html Model Code Two classes were written to handle data processing and the model. Utilizing huggingface package with Python and Pytorch as deep learning framework, we wrote the following classes: DataPrep The  DataPrep  class is used to process the train.jsonl and test.jsonl files and write out three CSV files that are used for training the classification model and one CSV file that will be used for getting predictions for submission. The  response_only  argument can be used when instantiating the class to either write out the tweet response by itself or both the response and context tweets concatenated together. The  train_test_split  method is used to split the train.jsonl data into a training, validation, and test set to be used for model training. When training the neural network the validation set is used to monitor progress. After training is complete the test set is used to do a final evaluation of the models performance. The  write_data  method writes out three CSV files for the training, validation, and testing along with a file that will be used to generate the predictions for submission. SarcasmDetector The  SarcasmDetector  class is used to instantiate, train, evaluate, and perform predictions with the BERT based neural network model. For the sake of brevity not all methods and attributes will be described in this paper. A sample notebook in the repository will provide additional detail on how to properly use this class. The primary usage of the class for model training, evaluation, and prediction uses the following sequence of methods: 1.Instantiate the class with arguments indicating the directories where the CSV data resides, the model weights should be saved, and the training tensorboard logs should be saved. 2.Use the  tokenize_data  method to tokenize the training, validation, and test text data appropriately for use in the BERT pre-trained model. The names of each file will need to be passed along with an appropriate batch size for training (if you are running out of memory when attempting to train reduce this value). 3.The  tune  method is then utilized to perform a gridsearch for the optimal learning rate hyperparameter. A list of learning rates and a list of the number of epochs to train for must be supplied as arguments when using this method. 4.The  evaluate  method can then be used to provide precision, recall, and f1 metrics for the trained model using the test set. 5.Lastly, the  predict  method uses the trained model to create predictions on the tweets supplied for submission. The output of this method is a dataframe that can then be written to disk for submission. Infrastructure We trained our models on the Collab Google environment which is a virtualized jupyter notebook utilizing Tesla GPUs for training. Results TensorBoard was used to log training and validation loss during the hyperparameter tuning process. The screenshot below shows the results from a number of training runs utilizing different learning rates and number of epochs. It turned out that using the tweet responses concatenated with context and a learning rate of 8e-7 was the best performing model. Below are the precision, recall, and f1 scores for this model on the held out test data. CourseProject - BERT For 100% Accuracy (Sarcasm) This readme serves as technical documentation and overview of the project code. Please review the PDF included in this repository for more detailed information on the BERT model, results, and technical details. Technical Requirements The code has been tested on Python 3.8.x and requires the following core packages. torch >= 1.5 transformers >= 3.5.1 tensorflow >= 2.3.1 pandas >= 1.1.4 numpy >= 1.18.5 scikit-learn matplotlib seaborn It is highly recommended to use virtual environments for running this code, or on a dedicated cloud environment such as Google Colab. Running in Google Colab This is probably the easiest way to get things going. The TeamKayak_SarcasmDetectorDemo.ipynb notebook located in this repo gives a good overview of the environment set up steps you'll need to run this repo. Install required packages You will want to be able to sync to Google Drive so you can check out this repo and have it available to Colab. Running the cell below will install the Google package and mount your GDrive. Then, run the sample git clone command replacing my_directory to a location of your convenience. from google.colab import drive drive.mount('/content/drive', force_remount=True) !git clone https://github.com/kaipak/CourseProject.git my_directory !pip install transformer Set up Environment You will need to set up some directories in your GDrive to hold the input datasets and so the code can also output processed data, model checkpoints, metrics, etc. You can use the following cell as a template. ``` Set Paths for Google Drive source_folder = '/content/drive/My Drive/Data' destination_folder = '/content/drive/My Drive/Model/Response' code_folder = '/content/drive/My Drive/CourseProject/src' train_log_dir = '/content/drive/My Drive/logs/tensorboard/train/' ``` Then, import needed packages to your notebook: ``` import sys, os from pathlib import Path import random import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sys.path.append(code_folder) from model import SarcasmDetector from data_prep import DataPrep ``` Prepare Data The input data for the model can be processed using our data_prep class: ``` Prepare Data data_prepper = DataPrep(train_path=source_folder + '/train.jsonl', sub_path=source_folder + '/test.jsonl', response_only=False) data_prepper.train_test_split() data_prepper.write_data(datapath=source_folder) ``` Most of the parameters have sane default values, but you can see the docstrings to alter if you like. Train a Model The model should be ready to train now. ``` Instantiate Model SarcModel = SarcasmDetector(input_dir=source_folder, output_dir=destination_folder train_log_dir = train_log_dir ``` Tokenize text Then, tokenize the input data: ``` SarcModel.tokenize_data(train_fname = 'train.csv', validate_fname = 'validate.csv', test_fname = 'test.csv', batch_size = 8 ``` Start up Tensorboard This will give you feedback on how training is going. Run the magic command. %tensorboard --logdir Train! There's a tune() method that does grid search to find best set of hyperparameters but just running on lists of singletons is essesntially as running train(). ``` learning rate list lr_list = [5e-7] number epochs list num_epochs_list = [15] tune hyperparameters SarcModel.tune(lr_list=lr_list, num_epochs_list=num_epochs_list ``` Evaluate You can then run the trained model against the test set to get some metrics on how well it does on unseen observations. SarcModel.evaluate(model_name = 'lr_5e-07_epochs_15_') Kai Pak and Karlin Dye November 29, 2020 CS410 Team Kayak Project Progress Report Progress made thus far We have implemented a BERT based pretrained model using huggingface transformers on Pytorch framework. We have already beat baseline and our score is currently #18 on the leaderboard (kaipak) Model has been deployed on Google Collab as well as on local workstation with GPU. Remaining tasks We plan on continuing fine tuning the model including experimenting with combinations of context and response. Develop systematic way of searching for optimal hyperparameter tuning including finding optimal learning rate Clean code Write documentation Create short presentation Any challenges/issues faced Investigate variances in performance when no parameters are modified Could use better GPU CS 410 Team Kayak Course Project Proposal Project Topic: Text Classification Competition - Twitter Sarcasm Detection What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Kayak Names NetIDs Kai Pak - Captain kaipak2 Karlin Dye karlind2 Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Neural Network Architectures: RNN, CNN, LSTM, Bi-LSTM, C-LSTM, BERT Deep learning frameworks experience: Tensorflow, Pytorch, Keras, Fastai Platforms: Google Colab, AWS Sagemaker and AI/ML tools Past Experience: Have used neural networks for image classification, segmentation, and entity detection and classification. Which programming language do you plan to use? Python
https://github.com/kaiyuandou/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kn13-kiran/CourseProject	RESTAURANT MENU CRAWLER & CLASSIFIER Kiranmayee Nimashakavi (cs410 COURSE PROJECT) PROJECT OVERVIEW Context & Objective A restaurant review company would like to provide recommendation on menu items across the world. To accomplish this goal, the first stage of the pipeline is to identify the URLs that relevant (i.e. find the URLs that contain menu information) This tool crawls the URLs, classifies the pages that contain menu. Components Scrapy An open source and collaborative framework (python based) for extracting the data you need from websites. BeautifulSoup Beautiful Soup is a Python library for pulling data out of HTML and XML file Metapy Python bindings for MeTA toolkit that tokenizes documents, creates indexes, inverted indexes, classifies, creates topic models etc., Scrapy architecture Project design 1. List of URLs Main.py Duplink Remover PageDataOutput Scrapy Engine Pipelines Menu Spider Spiders Meta Py Menu Classifier 2b. Downloaded Content 4. Classify using Page Data 6. Uses pre-indexed training data set 5. Uses MetaPy to classify Training Dataset + Indexes Beautiful Soup 3. Extract Text data from Page 7. Save Menu Links, processed links Down loader 2a. Download pages Final results 8. Save Menu Links, processed links Design/code level overview Contains all scrapy plug-ins Contains custom spider code Stores log files Contains the pre-classified seed data Contains All URLs that were crawled URLs classified as MenuPages demo WebCrawler for Restaurant Menus: From a given list of URLs, this crawler looks for pages that are relevant to restaurants, pages that contain restaurant menu and downloads the menus and drops the remaining pages. The high-level process can be summarized as: From a Base URL, crawl the home page. Using title, head, page tags identify whether that site will contain Restaurant menu or not? Only for Restaurant menu related sites, find all links Classify the pages that contain the menu and keep the menu pages. What is completed: Architecture Design Finalized tools and technologies. Design of base classes Key Components: Classifier: Classifier uses title, head and page tags to classify relevance of the page to find out whether a page is a restaurant page or not. If it is a restaurants page, then further crawling is performed. WebCrawler: WebCrawler is used to crawl main pages, links under that page etc. It stops crawling page if relevance score goes to zero or lesser (i.e., that site doesn't contain menu). Web crawler takes inputs seed data and classifier instance which has to inherit from web classifier. Using seed URLs, it starts extracting title, head, page tags identify whether that category is restaurant, or the page contains menu etc., Which tasks are pending? Training - manually create training data set by using existing known restaurant sites, sites containing menu information, non-restaurant pages and converting them to metapy format. Code, Integration testing - Developing the scraping code, classifier code and integrate them and test it end2end. Are you facing any challenges? Integrating classifier with Scrapy. Identifying pages that contains menus from other pages in a site. Restaurant Menu Crawler & Classifier A restaurant review company would like to provide recommendation on menu items across the world. To accomplish this goal, the first stage of the pipeline is to identify the URLs that relevant (i.e. find the URLs that contain menu information). This project focuses on the first stage. The summary of the video can be found herehere Components There are two major components of this project: Menu classifier - which uses anchor text, page title head>title, and body>p data to classify relevance of the page. This uses NaiveBayes classification and provides score either 0 or 1. Menu Spider - Web crawler that downloads, parses and extract the data from the links. Training data set for classifier - I've manually created, curated and labelled menu data from menupages.com dataset/sample/menu.txt - contains menu data from different types of restaurants in different cities. dataset/sample/other.txt - contains non menu relevant data. Functional Flow main.py: The driver code that triggers the crawling process. This component takes target URLs (URL that needs to be checked for Menu information) as an input. First, MenuClassifer is initialized using the training dataset, creates indexes and inverted indexes using MeTA. On target urls, driver code starts crawling using Scrapy engine. Scrapy Engine has a call back mechanism that calls two main components Menu Spider and Pipelines for every page that is downloaded. MenuSpider: Responsible for extracting, parsing page content and holds them in the plain text fields. This plain content is passed to the MenuClassifier and score is calculated. Based on the score, further processing of the links with in that page is determined. If the score is <=0 further parsing of the links is abandoned. Crawling ends when all the links are exhaused or when the score reaches 0. Pipelines: DuplicateLinkRemover: Tracks already processed URls and removes circular links so that they don't get processed again. PageDataOutput : Stores the pageURL, scores to finalresults directory. Project Dependencies This projects uses some third party components, you have to install these components first to run this project * conda - to create python 3.5 virtual environment * python 3.5+ * scrapy 2.3.0 * metapy * Beautiful Soup version 4+ Install scrapy Crawler depends scrapy. Before running this project, install scrapy using following commands. pip install scrapy Install metapy We are using metapy---Python bindings for MeTA. Install metapy using the following commands. pip install metapy Install Beautiful Soup pip install beautifulsoup4 Running the project There is main.py file which needs to be invoked by passing target_urls (comma separated urls). python main.py url1,url2,...,urln For example - python main.py https://papillonrestaurant.com,http://www.cnn.com This command will generate output to finalresults folder. restaurant_menu_crawler_all_links.txt - holds all the urls that were crawled. restaurant_menu_crawler_menu_links.txt - holds the urls that contain menu informaiton. menucrawler/log -> contains log file. Termination This project will keep crawling until resources is exhausted or no more relevant url to crawl.If you want to stop crawling immediately then press [ctrl^c] which unsafely stop crawler immediately. Future work Improving Sample Data - I could add additional data to include different types of cuisines. Improving Classification - Current classification is binary , it can be further improved to include type of cuisine. Tracking Zipcodes - The next part of this pipeline requires us to store the zipcode information associated with the restaurant. Currently, zipcode information is not saved, but, it can also be added to the output. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This is a solo project. My name is Kiranmayee Nimashakavi. My NetId is kn13@illinois.edu. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? WebCrawler and classifier for MenuPages.com or some site that offers restaurant menus. The objective is to crawl the entire site and collect all pages and then classify the pages that has menu information. This information can be used to create a dish specific search on the restaurant menu. My approach is to build a non-recursive crawler that extracts all pages from a given list of sites and use some classifier as (naive bayes) to classify the pages. I've not decided on the tools to use on this project yet. The specific set of systems that are used are the list of websites that will be given as an input to the program. Expected outcome is to store the Menu pages to a specific directory. I will manually test the output data by supplying some random collection of URLs. If time permits, I will implement the dish search on the dataset that was classified as a menu page. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The following activities has to be performed to accomplish this project. Design and implementation of the non-recursive crawler. - 5 hours Design and implementation of the classifier - 3 hours Unit and manual testing of the crawler and classifier - 5 hours Performance tuning of crawler and classifier - 3 hours Documentation of the project - 4 hours Presentation of the project -2 hours
https://github.com/kplhung/CourseProject	"Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai December 13th, 2020 Documentation 1. Code functionality overview This code parses a large data set1 and extracts a set of documents consisting of paragraphs related to the 2000 United States presidential election, contested between Republican George W. Bush and Democrat Al Gore. It then goes on to apply latent Dirichlet allocation (LDA), a generative statistical model, to this document set, thus identifying candidate topics in the set. Normalized prices based on the Democratic candidate, Al Gore, are then extracted from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market2, and Granger testing is performed to determine possible causality of the candidate topics, indicating which topical key words are most likely to impact the candidate's IEM price positively or negatively--that is, their likelihood of being elected. This can be taken as a prior for future iterations of causal topic mining. In terms of future applications, the code can be modified to identify target paragraphs of any nature from The New York Times corpus. Various external time series data can also be used in conjunction--physical data comes to mind as a useful application. Or, public opinion polling on issues like climate change or social problems could be coupled with data from The New York Times corpus in order to find any sort of causal relationship between media coverage and public opinion surrounding those issues. 1 Namely, The New York Times corpus, from May to October 2000 2 A prediction market allowing traders to buy and sell contracts based on that year's presidential election. Hung 2 2. Software implementation Note: mining_causal_topics.ipynb combines code and results from identify_candidate_topics.py, iowa_electronic_markets.py, and granger_testing.py into one coherent file; this notebook is the easiest place to see code and results past the election paragraph parsing stage, though documentation has been organized based on the subfiles for ease. find_election_paragraphs.py: filters May through October 2000 New York Times articles for election-related paragraphs - find_election_paragraphs(directory): given a directory containing XML article data, walks through all subdirectories and parses all files. Writes all election-related paragraphs (those containing ""Gore"" or ""Bush"") to a file called ""election_paragraphs.csv"" o Helper functions: # parse_xml(xml, election_paragraphs): checks XML article for person tag, then parses relevant articles' full text for relevant paragraphs # contains_election_words(text): returns true if and only if ""Bush"" or ""Gore"" is a substring of the input text # format_date(file_path): returns formatted date from directory path identify_candidate_topics.py: applies LDA to the document set to identify candidate topics - Data wrangling o Wrangle and clean paragraph data by removing punctuation and lowercasing text o Remove stop words from the paragraph data and vectorize the text using scikit-learn, then learn the vocabulary dictionary, deriving a document-term matrix over the paragraph set - Topic modeling with LDA o Using LDA, learn the documents as bags of words and identify candidate topics in the paragraph set o Transform the document-term matrix according to the fitted LDA model, and index on date (dates range from May 1st, 2000 to October 31st, 2000) Hung 3 iowa_electronic_markets.py: wrangles, cleans, and normalizes IEM 2000 Presidential Winner-Takes-All Market data - Data wrangling o format_date(date): given IEM-style date (mm/dd/yy), returns NYT-style date (yyyy-mm-dd) o Read from ""iem_2000.txt""--candidate price data from May to October 2000-- and drop irrelevant columns (namely: units, volume, low price, high price, and average price), keeping only date, contract, and last price data; index on date o Derive lists of Democratic and Republican candidate prices based on the ""contract"" field (""Dem"" indicates Democrat; ""Rep"" indicates Republican) o Compute normalized prices based on the Democratic candidate # For each date index, normalized price = Dem price / (Dem price + Rep price) - Election coverage vs. election forecast o Concatenate the normalized price data to the topic coverage data from identify_candidate_topics.py; this brings together topic words and price data, indexed by date granger_testing.py: perform Granger tests to determine significant causal words and their impact - Granger testing o Perform Granger tests3 to test causality with max lags up to 3 - Significant causal words o Compute average p-values across time lags (from 1 up to 3) for each candidate topic term, based on parameter F tests 3 Granger tests compute lagged correlation measures to find potential causal relationships between time series--here, they look for potential causal relationships between New York Times coverage and candidate electability Hung 4 o Sort candidate topic terms by causality probability, where a smaller p-value corresponds to a larger probability that there exists some type of causal relationship between the two time-indexed data - Prior influence o Determine positive impact terms and negative impact terms, where positive impact terms are the ones with p-values in the lower half, and negative impact terms are the ones with p-values in the upper half 3. Usage - Install Jupyter using the documentation provided here - Open terminal and run the following command: o git clone https://github.com/kplhung/CourseProject.git - Launch Jupyter Notebook and navigate to the CourseProject directory - Open and run find_election_paragraphs.ipynb o [Kernel] - [Restart & Run All] - Open and run mining_causal_topics.ipynb o [Kernel] - [Restart and Run All] 4. Team member contributions This was an individual project. Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai November 29th, 2020 Progress Report 1. Completed tasks - Filter The New York Times corpus for articles from May 2000 to October 2000 that mention ""Bush"" or ""Gore."" - Apply latent Dirichlet allocation (LDA) to the cleaned New York Times corpus to identify possible topics. - Use Granger testing to test causality and to derive the set of candidate causal topics with time lags. 2. Pending tasks - Apply Granger testing to find the most significant causal words among top words for each candidate topic, and track these words' impact values. - Define a prior on the parameters of the topic model using these significant terms and their corresponding impact values, and apply LDA to the New York Times corpus using this prior. - Iterate, and replicate one set of experimental results by using prices from the Iowa Electronic Markets 2000 Presidential Winner-Takes-All Market. 3. Challenges - Fully understanding how to set the strength of the prior in each iteration by using m. Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai October 25th, 2020 Project Proposal Project Topic: Reproducing a Paper on Causal Topic Modeling 1. Team Name: Pei Lun Hung (project coordinator/leader) NetID: prhung2 Email address: prhung2@illinois.edu 2. Paper I plan to reproduce the results from Hyun Duk Kim et al.'s paper, ""Mining causal topics in text data: Iterative topic modeling with time series feedback.""1 3. Implementation Language I plan to implement the project in Python. 4. Data Set The paper uses The New York Times Annotated Corpus. As such, I have registered for an account with the Linguistic Data Consortium and am awaiting acceptance from admin. 5 Backup Data Set There is a digital archive2 of The New York Times, with articles from the 1850s to the present day. As backup, I can scrape articles from January 1st, 1987 to June 19th, 2007 from this digital archive in order to obtain a similar data set to the aforementioned corpus, sans annotations. 1 Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 2 https://spiderbites.nytimes.com/ CourseProject Topic: Reproducing a Paper on Causal Topic Modeling Reference: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Video: https://www.screencast.com/t/yiDQ5OrjQwj Documentation Documentation is reproduced below. However, for better formatting and reading, please see the PDF version. 1. Code functionality overview This code parses a large data set and extracts a set of documents consisting of paragraphs related to the 2000 United States presidential election, contested between Republican George W. Bush and Democrat Al Gore. It then goes on to apply latent Dirichlet allocation (LDA), a generative statistical model, to this document set, thus identifying candidate topics in the set. Normalized prices based on the Democratic candidate, Al Gore, are then extracted from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market , and Granger testing is performed to determine possible causality of the candidate topics, indicating which topical key words are most likely to impact the candidate's IEM price positively or negatively--that is, their likelihood of being elected. This can be taken as a prior for future iterations of causal topic mining. In terms of future applications, the code can be modified to identify target paragraphs of any nature from The New York Times corpus. Various external time series data can also be used in conjunction--physical data comes to mind as a useful application. Or, public opinion polling on issues like climate change or social problems could be coupled with data from The New York Times corpus in order to find any sort of causal relationship between media coverage and public opinion surrounding those issues. 2. Software implementation Note: mining_causal_topics.ipynb combines code and results from identify_candidate_topics.py, iowa_electronic_markets.py, and granger_testing.py into one coherent file; this notebook is the easiest place to see code and results past the election paragraph parsing stage, though documentation has been organized based on the subfiles for ease. find_election_paragraphs.py: filters May through October 2000 New York Times articles for election-related paragraphs * find_election_paragraphs(directory): given a directory containing XML article data, walks through all subdirectories and parses all files. Writes all election-related paragraphs (those containing ""Gore"" or ""Bush"") to a file called ""election_paragraphs.csv"" Helper functions: parse_xml(xml, election_paragraphs): checks XML article for person tag, then parses relevant articles' full text for relevant paragraphs contains_election_words(text): returns true if and only if ""Bush"" or ""Gore"" is a substring of the input text * format_date(file_path): returns formatted date from directory path identify_candidate_topics.py: applies LDA to the document set to identify candidate topics * Data wrangling Wrangle and clean paragraph data by removing punctuation and lowercasing text Remove stop words from the paragraph data and vectorize the text using scikit-learn, then learn the vocabulary dictionary, deriving a document-term matrix over the paragraph set * Topic modeling with LDA Using LDA, learn the documents as bags of words and identify candidate topics in the paragraph set Transform the document-term matrix according to the fitted LDA model, and index on date (dates range from May 1st, 2000 to October 31st, 2000) iowa_electronic_markets.py: wrangles, cleans, and normalizes IEM 2000 Presidential Winner-Takes-All Market data * Data wrangling format_date(date): given IEM-style date (mm/dd/yy), returns NYT-style date (yyyy-mm-dd) Read from ""iem_2000.txt""--candidate price data from May to October 2000-- and drop irrelevant columns (namely: units, volume, low price, high price, and average price), keeping only date, contract, and last price data; index on date Derive lists of Democratic and Republican candidate prices based on the ""contract"" field (""Dem"" indicates Democrat; ""Rep"" indicates Republican) Compute normalized prices based on the Democratic candidate * For each date index, normalized price = Dem price / (Dem price + Rep price) * Election coverage vs. election forecast Concatenate the normalized price data to the topic coverage data from identify_candidate_topics.py; this brings together topic words and price data, indexed by date granger_testing.py: perform Granger tests to determine significant causal words and their impact * Granger testing Perform Granger tests to test causality with max lags up to 3 * Significant causal words Compute average p-values across time lags (from 1 up to 3) for each candidate topic term, based on parameter F tests Sort candidate topic terms by causality probability, where a smaller p-value corresponds to a larger probability that there exists some type of causal relationship between the two time-indexed data * Prior influence Determine positive impact terms and negative impact terms, where positive impact terms are the ones with p-values in the lower half, and negative impact terms are the ones with p-values in the upper half 3. Usage Install Jupyter using the documentation provided here Open terminal and run the following command: git clone https://github.com/kplhung/CourseProject.git Launch Jupyter Notebook and navigate to the CourseProject directory Open and run find_election_paragraphs.ipynb: [Kernel] -> [Restart & Run All] Once find_election_paragraphs.ipynb is finished running, open and run mining_causal_topics.ipynb: [Kernel] -> [Restart and Run All] 4. Team member contributions This was an individual project."
https://github.com/lipingxie/CourseProject	"CS410 Project Code Demo Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Demo Link: CS410 Course Project Demo - YouTube We recorded our demo and uploaded the video onto YouTube, please click on the above link to watch the video. If you experience any issues running our code or you have any questions regarding to our project details, please send us an email to organize a Zoom Meeting. Thanks. Contact Emails: Liping Xie: lipingx2@illinois.edu Zijing Chen: zijingc3@illinois.edu Zuliang Weng: zwe@illinois.edu 1 CS410 Project Detail 1. Team Information Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) 2. Project Overview 2.1 Project Topic Text Classification Competition: Twitter Sarcasm Detection. The Classification task is to classify the list of Tweets into two categories: ""SARCASM"" or ""NOT_SARCASM"". There are 2 datasets: Training set which with 5000 Tweets and Test set which with 1800 Tweets. We researched and tried many cutting-edge models that are suitable for classifying ""SARCASM"" or ""NOT_SARCASM"" in this project. There are two files provided: train.jsonl and test.jsonl. The model is trained with the data provided in train.jsonl , and test.jsonl contains the tweets that we need to classify with the trained model. The classification result is reported with the provided id. All the results are stored in the answer.txt file. 2.2 Overview of the Function of the Code For this classification task, we used PyTorch as the Deep learning framework, Python(version 3.6.9) as the Programming Language and Pre-trained BERT as the State-of-the-art neural network classifier. The main function of the code is to classify the list of Tweets into two categories: ""SARCASM"" or ""NOT_SARCASM"". To be more specific, we applied the pre-trained Twitter-specific BERT model, roBERTa-base model on the training data to fine-tune the model and then predicted on the testing data to generate final results. This code can only be used to analyze or classify Tweets data due to the Twitter-specific BERT model we used. 2 Our code is in file ""Final BERT Twitter Sarcasm Classification.ipynb"", here is the detail mappings of the code section and the content: Code Section No. Content 1 How to use Google Colab for training the model and perform the test under python 3.6.9 environment and install the Hugging Face Library 2 How to load data from the provided jsonl files and parse the data 3 How to use the pre-trained BERT model. How to formatting the data, tokenize Dataset and split the training data into training set and validation set for the use of BERT 4 How to train and fine-tune the model with different batch size, learning rate and epochs, and how to evaluate the result 5 How to use the trained model to predict the tweets provided in the test.jsonl. How to save the prediction result. 3. Software Implementation Details 3.1 Classifier Selection and Introduction We have investigated different types of Classifiers which include Word2Vec, FastText and BERT. We found BERT (Bidirectional Encoder Representations from Transformers) is the State-Of-The-Art Neural Network Classifier. According to Wikipeda, BERT is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT is a model that broke several records for how well models can handle language-based tasks(Jay Alammar, 2018). This model would look like this: (Above information and image from:http://jalammar.github.io/illustrated-bert/) To train such a model, we mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine-Tuning (Jay Alammar, 2018). BERT makes use of the encoder mechanism of Transformer, 3 an attention mechanism that learns contextual relations between words (or sub-words) in a text. At the moment, the Transformers package from Hugging Face PyTorch library is regarded as the most widely accepted and powerful pytorch interface for working with BERT. (Above information and image from:http://jalammar.github.io/illustrated-bert/) BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word (Jay Alammar, 2018). (Above information and image from:http://jalammar.github.io/illustrated-bert/) 3.1.1 Model Selection 4 Training a BERT model takes huge time, we can use the pre-trained model, and fine-tune it with our training data. With the consideration of classifying Twitter text, we used the pre-trained Twitter-specific BERT model - roBERTa-base model. The reason why we choose Twitter-specific BERT model is that the characteristics of Tweets are significantly different from traditional text such as research papers or articles. Tweets are generally short, and include frequent use of informal grammar as well as irregular vocabulary such as abbreviations. Thus, it would be inappropriate to apply language models such as bert-base-uncased, bert-large-uncased that are pre-trained on traditional text with formal grammar and regular vocabulary to analyze Tweets data (Nguyen et al., n.d.). Our testing result confirmed that the roBERTa-base model outperforms bert-base-uncased and bert-large-uncased in this classification task. The architecture for BERTweet is very similar to BERTbase, which is trained with a masked language modeling objective (Devlin, 2019). BERTweet pre-training procedure is based on RoBERTa which optimizes the BERT pre-training approach for more robust performance (Liu, 2019). For pre-training data, BERTweet uses an 80GB pre-training dataset of uncompressed texts, which contain 850M Tweets (Nguyen et al., n.d.). This is very similar to the training and testing dataset since our data is also Tweets data. Under https://huggingface.co/models, we can find there are different sub models, we compared the performance between twitter-roberta-base and twitter-roberta-base-irony, we found twitter-roberta-base performs a little bit better on the prediction task (we have provided the detail test result in section 3.7.2 ), we decided to select twitter-roberta-base in our final version code. 3.2 Environment setup 3.2.1 Setup Colab Since we need to train a large neural network, Google Colab offers free GPUs and TPUs, we used Colab to train our model to shorten our training time. Steps for setting up Colab: 1. Add Google Colab as an app to Google Drive. 2. Set the runtime type to GPU. To be more specific, we went to ""Edit"" -> ""'Notebook Settings"" -> ""Hardware accelerator"" -> ""GPU"". If you want to check if the GPU is detected, execute code in section 1.1 to confirm 3. We need to identify and specify the GPU as the device in order for torch to use the GPU. As a user, you need to install the package torch If you have not installed it before. After importing the package torch, you can run the second section of code in 1.1 to specify the GPU as the device. 3.2.2 Install the Hugging Face Library We have selected the Transformers package from Hugging Face PyTorch library, so we need to install the transformers package from Hugging Face which gives us a pytorch interface for working with BERT. You can run the code in section 1.2 to install the Hugging Face Library. 5 3.3 Data Analysis and Preparation There are 2 json format datasets provided: * train.jsonl: 5000 Tweets * test.jsonl: 1800 Tweets In each tweet in test.jsonl, it contains ""id"", ""response"" and ""context"": * Id: String identifier for sample. This id will be required when making submissions. * Response: the Tweet to be classified * Context: the conversation context of the response. The context is an ordered list of dialogue In each tweet in train.jsonl, it contains ""label"", ""response"" and ""context"": * Label: SARCASM or NOT_SARCASM * Response: the Tweet to be classified * Context: the conversation context of the response. The context is an ordered list of dialogue The length of all responses are less than 150 words, but for the context, some tweets with very long context and exceed the maximum supported input size (512) of BERT, that means we cannot use the full content of context for classification. So, we have tried two strategies: 1. Classify based on Responses only 2. Classify based on Responses and part of Context For strategy 2, according to our test, we only can include two dialogues (we have tried the last two items in each context), if we selected 3 dialogues, it reports ""CUDA out of memory"" error. So, we just use the last two items in each context as part of the content for classification, then compare the f1 result (We are intended to compare the result of two strategies, and we will detail how to get the f1 value in later sections). From above, we can see that adding the content for classification doesn't improve the prediction. So, we have decided not to include the ""content"" in each tweet for our classification task. 6 Please refer to code section 2.1 for how to load the data into Colab for test, and section 2.2 for how to read and parse the data for later use. 3.4 Tokenization 3.4.1 BERT Tokenizer There are many different pre-trained BERT models available. Each model comes with its own tokenizer. We need to make sure we use the correct tokenizer as we experiment with different models. We defined which pre-trained BERT model we will use in this step and if we need to change the model, we just need to simply update the model name. Please refer to code section 3.1 for more details. 3.4.2 Required Formatting BERT requires tokens to fit certain format: 1. Add special tokens to the start and end of each sentence. o For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence. This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output. o At the end of every sentence, we need to append the special [SEP] token. 2. Pad & truncate all sentences to a single constant length. 3. Explicitly differentiate real tokens from padding tokens with the ""attention mask"". BERT has two constraints: 1. All sentences must be padded or truncated to a single, fixed length. o For any sentences that are less than the fixed length, we need to PAD with the same special token. o For any sentences that are more than the fixed length, we need to truncate them to the fixed length, otherwise the system will report the errors. 2. The maximum sentence length is 512 tokens. 3.4.3 Tokenize Dataset For saving the memory in the model training, we set the max_length of the input token based on the max length of all the input sentences. We used encode_plus methods for token padding and attention masking, it automatically adds ""special tokens"" which are special IDs the model uses. We need to convert the lists into tensors in order to use the Pytorch properly. Please refer to the code section 3.3 for the detailed implementation. 3.4.4 Training & Validation Split and Batch Size Before training our data using BERT, data must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. 7 For the provided training data, we split them into training set and validation set, the proportion of the training set will impact the prediction accuracy of the model. We have tried the training:validation ratio as 9:1, 99:1 and 8:2. We observed when the training:validation ratio is 9:1 provides the best prediction result. Please refer to section 3.7.2 for the detail test result. We used DataLoader to help us save memory, and we define the batch size for this step. Please refer to the code section 3.4 for the detailed implementation of data split, the usage of dataloader and the setting of batch size. 3.5 Fine-tune BERT Models First we load the defined pre-trained BERT model, examine the result, then we set the key values for training the model: * Batch size (set when creating our DataLoaders) * Learning rate * Epochs Then we train the model with the training loop and provide the training result statistic (accuracy and validation lost). We use the output of the fine-turn model to predict the tweets in the test.jsonl. After that we update the parameters and train the model again. When we have a satisfied result for the pre-trained model under test, we change to other models with the same parameters. Here is the a list that we have experimented: * Bert Models: o Bert-base-uncased o bert-large-uncased * Twitter-roberta models: o cardiffnlp/twitter-roberta-base o cardiffnlp/twitter-roberta-base-irony o cardiffnlp/twitter-roberta-base-offensive Based on our test result, twitter-roberta models outperforms Bert Models on this task, we have decided not to use this model in early stage, so our testing is mainly focused on the twitter-roberta related models. Comparing the performance of three twitter-roberta models, cardiffnlp/twitter-roberta-base provides the best performance with the following settings: * Batch size: 32/64 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4/6 3.5.1 Download the Model Please refer to the code section 4.1. 3.5.2 Optimizer & Learning Rate Scheduler We use AdamW optimizer and define the epochs in the scheduler. 8 Impact analysis on the parameters: * Batch size: The batch size is the number of words to be used for calculation and updating the weight once, it impacts the training time, memory usage and model's prediction accuracy. The larger the batch size is, the more memory it will consume. When the batch size is too small, the result may not converge, and when the batch size is too big, it will cause over fitting or out of memory. Based on our test, the batch_size = 32/64 provides the best performance. * Learning rate: We have tried three learning rates: 2e-5, 1e-4, 1e-6. We noticed that when the learning rate is large, it cannot converge, and when the learning rate is small, it overfits. learning rate = 2e-5 provides the best prediction result. * Epochs: Epochs value need to be set correctly, if the value is too small, the gradient descent will not converge, if the epochs value is bigger than required, we will overfit the model and decrease the accuracy of the prediction result. We use the accuracy and validation lost in each epoch's validation result. When we notice when the epochs number increases, the accuracy increases, we increase the epoch value to test again. If the accuracy decreases or there is not much change, we decrease the epochs to the point when accuracy is not increased. Based on our test, the epochs = 4/6 provides the best performance. Please refer to the code section 4.2 for the implementation. Please refer to section 3.7.3 for the detail test result. 3.5.3 Training Loop We define and create the training loop based on the contribution of Stas Bekman. The training loop has a training phase and a validation phase. It also detects over-fitting by using validation loss. After defining the training loop, we start to fine-tune the model. Please refer to the code section 4.3. 3.6 Predict on the test data 3.6.1 Data Preparation Prepare the test data just as how we prepare the training data. Please refer to the code section 5.1. 3.6.2 Predict on the test set Generate the final prediction based on the score, and download the result as answer.txt. Submit answer.txt to livelab for the result. When the f1 result is not good, we change the parameters or pre-trained model as mentioned in 3.5. Please refer to the code section 5.2. 9 3.7 Results 3.7.1 Final Result Chosen Model: cardiffnlp/twitter-roberta-base Parameters setting: * Batch size: 32 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4 Model Training Result: The Best Prediction Result with the above parameters: * Precision: 0.7577319587628866 * Recall: 0.8166666666666667 * F1: 0.7860962566844919 3.7.2 Result Discussion We observed that when using the same parameters to fine-tune the pre-trained model, the prediction result is not the same for each time. We have confirmed that even we set seed for the random sampling and using SequentialSampler in dataloader, we cannot make the prediction consistent. Based on the information provided in Paperswithcode, the issue is caused by the Attention Dropout and Dropout: * Attention Dropout: It is a type of dropout used in attention-based architectures, where elements are randomly dropped out of the softmax in the attention equation. * Dropout: It is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability (a common value is ). At test time, all units are present, but with weights scaled by (i.e. becomes ). The idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks. According to Huggingface BertConfig, the default dropout rate is set to 0.1: * hidden_dropout_prob (float, optional, defaults to 0.1) - The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. * attention_probs_dropout_prob (float, optional, defaults to 0.1) - The dropout ratio for the attention probabilities. 10 Considering the above factors and the test result, we observe the following parameters' settings ensures we can get the F1 > 0.75 which is well above the baseline: * Batch size: 32/64 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4/6 Here is the test result for these settings: epoch Training Split Precision Recall F1 Comments 4 0.9-0.1 0.71468927 0.843333333 0.773700306 batch_size=64 Sentences=Response 4 0.9-0.1 0.69802867 0.865555556 0.77281746 batch_size=80 Sentences=Response 6 0.9-0.1 0.72380952 0.844444444 0.77948718 batch_size=64 Sentences=Response 8 0.9-0.1 0.73635427 0.794444444 0.764297167 batch_size=64 Sentences=Response 8 0.8-0.2 0.73236515 0.784444444 0.75751073 batch_size=64 Sentences=Response 4 0.8-0.2 0.70119157 0.85 0.768458061 batch_size=64 Sentences=Response 6 0.9-0.1 0.74476987 0.79111 0.767241379 batch_size=100 Sentences=Response 8 0.9-0.1 0.747288 0.76555 0.75631174 batch_size=100 Sentences=Response 5 0.9-0.1 0.72727272 0.80888 0.7659 batch_size=100 Sentences=Response 6 0.9-0.1 0.7391304 0.79333 0.76527 batch_size=80 Sentences=Response 6 0.9-0.1 0.74545454 0.77444 0.759673024 batch_size=64 Sentences=R learning rate=5e-5 6 0.9-0.1 0.73473 0.77555 0.75459 batch_size=64 Sentences=R learning rate=1e-5 6 0.9-0.1 0.7433264 0.80444 0.77267 learning rate=2e-5, batch_size=32,200 Sentences=Response 6 0.9-0.1 0.73313783 0.833333333 0.780031201 learning rate=2e-5, batch_size=64,32 Sentences=Response 4 0.9-0.1 0.74543611 0.816666667 0.77942736 learning rate=2e-5, batch_size=32,32 Sentences=Response 6 0.9-0.1 0.73615307 0.812222222 0.77231907 learning rate=2e-5, batch_size=32,32 Sentences=Response 4 0.9-0.1 0.75773196 0.816666667 0.786096257 learning rate=2e-5, batch_size=32,32 Sentences=Response 3.7.3 Testing Records Below is the table that listed all the models we've tried with some parameters and statistics. The thirteenth line achieved the best result, which ranked the second in the leadership board. epoch Training Split Precision Recall F1 Comments 0.591836735 0.805555556 0.682352941 no fine-tuning 4 0.9-0.1 0.74522293 0.78 0.762214984 11 4 0.99-0.01 predict all tweets as sarcasm. results not usable 4 0.95-0.05 0.722109534 0.791111111 0.755037116 4 0.9-0.1 0.74522293 0.78 0.762214984 4 0.9-0.1 0.691049086 0.797777778 0.740587932 Sentences=Response + Context[-2:] 4 0.9-0.1 0.648626817 0.892222222 0.751169317 Sentences=Response + Context[-2:] 4 0.9-0.1 N/A N/A N/A Sentences=Response + Context[-3:],CUDA out of memory. 4 0.9-0.1 N/A N/A N/A batch_size=64 Sentences=Response + Context[-2:], CUDA out of memory 4 0.9-0.1 0.714689266 0.843333333 0.773700306 batch_size=64 Sentences=Response 4 0.9-0.1 N/A N/A N/A batch_size=100 Sentences=Response,CUDA out of memory. 4 0.9-0.1 0.698028674 0.865555556 0.77281746 batch_size=80 Sentences=Response 6 0.9-0.1 0.723809524 0.844444444 0.77948718 batch_size=64 Sentences=Response 8 0.9-0.1 0.736354274 0.794444444 0.764297167 batch_size=64 Sentences=Response 8 0.8-0.2 0.732365145 0.784444444 0.75751073 batch_size=64 Sentences=Response 4 0.8-0.2 0.701191567 0.85 0.768458061 batch_size=64 Sentences=Response 6 0.9-0.1 0.768564356 0.69 0.727166276 batch_size=64 Sentences=Response 6 0.9-0.1 0.732291667 0.78111 0.7559 batch_size=64 Sentences=Response 2 0.9-0.1 0.5 1 0.6667 batch_size=64 Sentences=Response 6 0.9-0.1 0.74476987 0.79111 0.767241379 batch_size=100 Sentences=Response 8 0.9-0.1 0.747288 0.76555 0.75631174 batch_size=100 Sentences=Response 5 0.9-0.1 0.72727272 0.80888 0.7659 batch_size=100 Sentences=Response 6 0.9-0.1 0.7391304 0.79333 0.76527 batch_size=80 Sentences=Response 6 0.9-0.1 0.74545454 0.77444 0.759673024 batch_size=64 Sentences=R learning rate=5e-5 6 0.9-0.1 0.73473 0.77555 0.75459 batch_size=64 Sentences=R learning rate=1e-5 6 0.9-0.1 0.5 1 0.6667 learning rate = 4e-4 batch_size=64 Sentences=Response 6 0.9-0.1 0.665354331 0.563333333 0.610108303 learning rate = 1e-6 batch_size=64 Sentences=Response 6 0.9-0.1 0.73193 0.7766 0.7536 learning rate=2e-5, batch_size=64 Sentences=Response 6 0.9-0.1 0.751054852 0.791111111 0.770562771 learning rate=2e-5, batch_size=64,200 Sentences=Response 12 6 0.9-0.1 0.7433264 0.80444 0.77267 learning rate=2e-5, batch_size=32,200 Sentences=Response 6 0.9-0.1 0.73313783 0.833333333 0.780031201 learning rate=2e-5, batch_size=64,32 Sentences=Response 4 0.9-0.1 0.745436106 0.816666667 0.77942736 learning rate=2e-5, batch_size=32,32 Sentences=Response 6 0.9-0.1 0.736153072 0.812222222 0.77231907 learning rate=2e-5, batch_size=32,32 Sentences=Response 4 0.9-0.1 0.757732 0.81666667 0.7860963 learning rate=2e-5, batch_size=32,32 Sentences=Response 4. Instructions for Using the Software 4.1 Setup and Installation For this classification task, we used PyTorch as the Deep learning framework, Python as the Programming Language and pre-trained Twitter-specific BERT model, roBERTa-base model as the State-of-the-art neural network classifier. Here are the steps to execute the classifier: 1. Download these files from https://github.com/lipingxie/CourseProject : a. Final BERT Twitter Sarcasm Classification.ipynb b. test.jsonl c. train.jsonl 2. Add Google Colab as an app to Google Drive. 3. Upload ""Final BERT Twitter Sarcasm Classification.ipynb"" into Google Colab and open the file in Colab. Our code includes all the required commands to setup the environment, Colab provides the default Python(version 3.6.9)environment. 4. In Colab, in the top menu, select ""Runtime"" -> ""Change runtime type"" -> In ""Hardware accelerator"", select ""GPU"" -> click ""SAVE"" 5. You can run all the code in sequence via the top menu, select ""Runtime"" -> ""Run all"". When the code in section 2.1 is being executed, you need to upload the file manually. The upload button will be enabled once this code is being executed. Upload test.jsonl and train.jsonl at the same time. Once the files are uploaded successfully, the remaining code will continue to be executed automatically. 6. Wait for the code to be finished, it may take a long time around 15-60 minutes which depends on Colab performance on that time period. 7. The answer.txt file will be downloaded automatically (If the download action is permitted in your machine.). Sometimes the answer.txt file cannot be automatically downloaded due to your local environment issue. You can click on the folder button on the left-side bar, the file will be displayed there and you can download it manually. 8. If you would like to verify the prediction result, you need to upload the file to livelab-ClassificationCompetition project with your own account with the similar process of MP2.4. As mentioned in section 3.7.2, due to the BERT default dropout, you may not 13 get the same result as our report or on the board, but we guarantee the result can pass the baseline. 9. You can check our best prediction result of our fine-tuned model in livelab -> ClassificationCompetition project -> Leaderboard -> Record uploaded by ""Liping Xie"" 4.2 Troubleshooting If you would like to execute the code from the beginning again or load the model again, it may fail or you will have memory issues during the training stage. It seems it is caused by the Colab. The following steps can help to solve the problem: 1. Refresh the page 2. Click on the ""Additional Connection Options"" button (displayed as a small triangle icon that is next to ""RAM"" and ""Disk"" and located on top right corner. 3. Select ""manage sessions"" 4. Select the active section and click ""TERMINATE"" 5. Execute the code from the beginning again Due to the dependency of the code, it may generate unexpected results if you re-execute part of the code. Please terminate the session and run from the beginning again. 5. Team Member Contribution In this project, we all contributed a lot to each of the tasks, including writing the project proposal, researching cutting-edge pre-trained models, implementing the models on training and testing data, writing the progress report, finalizing the source code, and writing the documentation. 6. Reference * Nguyen, Dat Quoc. BERTweet: A pre-trained language model for English Tweets. Retrieved from: https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf * Jay Alammar, 2018. BERT: http://jalammar.github.io/illustrated-bert/ * Yinhan Liu, 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint, arXiv:1907.11692 * Jacob Devlin, 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, page 4186. * Paperswithcode BERT: https://paperswithcode.com/method/bert * Huggingface BertConfig: https://huggingface.co/transformers/model_doc/bert.html * Model reference: https://huggingface.co/models * Transformer coding reference: https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109 * Model training reference: https://mccormickml.com/ 14 * Pytorch coding reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py * Stas Bekman: https://github.com/stas00?tab=repositories CS410 Progress Report Team Information Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Project Topic Competition - Text Classification Project Progress Tasks have been completed: 1. Made decisions based on the options in our proposal: * State-of-the-art neural network classifier: BERT * Deep learning frameworks: PyTorch * Programming Language: Python 2. Our testing result has passed the baseline: o precision =0.7333333333333333 o recall = 0.7211111111111111 o f1 = 0.727170868347339 3. Draft version code is ready, here is what we have done related to coding: We modified and fine-tuned BERT to train the text classifier. To be more specific, we tried some pre-trained BERT models. The reason why we chose a pre-trained BERT model is that the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes less time to train our fine-tuned mode. First of all, we installed the transformers package from Hugging Face which gave us a pytorch interface for working with BERT. Next, in order to apply the pre-trained BERT, we used the tokenizer provided by the model, and we tried ""bert-base-uncased"" and ""bert-base-cased"". Since BERT has very specific formatting requirements, we loaded the data from the file and formatted it to match its requirements, such as added special tokens to the start and end of each sentence; truncated all sentences to the same length, etc. Then we used ""BertForSequenceClassification"" to train the model. This is the normal BERT model with an added single linear layer on top for classification that we used as a sentence classifier. Then we applied our model to generate predictions on the test set. Tasks are pending: 1. Use Google Colab to train and fine-tuned our model. Training a large neural network in Google Colab can save some training time. 2. Try some other pre-trained model such as bert-large-uncased. 3. Fine-tuning the model, try different batch size, learning rate, and number of epochs. 4. Discuss how much context data will be used in training, since if we use all the context data, it will take a long time to train. Challenges we are facing: 1. It takes too long to train the model locally since the context is very long. 2. The testing result is still not good enough, we need another way to improve the result. Execution Result: Result: Result: Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Project Topic: Competition - Text Classification Since it is a Text Classification Competition Project, as the instruction provided by TA, the project details will be provided in ""CS410 Project Detail.pdf"" instead of the README.md file. Please refer to ""CS410 Project Detail.pdf"" under https://github.com/lipingxie/CourseProject . Thanks. For all other information, please refer to the following documents in https://github.com/lipingxie/CourseProject: 1.Demo and Contact Information: CS410 Project Code Demo Video and Contact Info.pdf Project Code: Final_BERT_Twitter_Sarcasm_Classification.ipynb Project related datat files: result file: answer.txt input data: train.jsonl, test.jsonl If you experience any issues running our code or you have any questions regarding to our project details, please send us an email to organize a Zoom Meeting. Thanks. Contact Emails: Liping Xie: lipingx2@illinois.edu Zijing Chen: zijingc3@illinois.edu Zuliang Weng: zwe@illinois.edu ==========Updated on 15th December ========= Some reviewers reported the code will fail in the upload file step, it seems it is related to the Broswer settings or Broswer limitations. To ensure the code can be executed correctly in Colab, please use Chrome normal window. If execute the code in Chrome incognito window, the file upload button will not be enabled and just fail the step. ""Execution Reslut"" file is attached to confirm the code can be executed correctly. CS410 Course Project Proposal October 25th 2020 Team Information Team Name: Team Commonwealth Team Members: Zuliang Weng / zwe Zijing Chen / zijingc3 Liping Xie / lipingx2 (captain) Project Topic Competition - Text Classification Learning and Investigation Plan We are planning to learn the following state-of-the-art neural network classifier: BERT We do not have relevant prior project experience with this classifier. We are planning to learn the following deep learning frameworks: TensorFlow PyTorch We have basic knowledge with PyTorch, and we have used it in our self-study exercises. We will try PyTorch in our project first, if there is any issue that can only be resolved in TensorFlow, we will switch to TensorFlow. Programming Language We plan to use Python for this project."
https://github.com/liuyuxiang512/CourseProject	"CS410ProgressReportDancingText:YuxiangLiu(yuxiang@illinois.edu,leader),HongfeiMa(hongfei7@illinois.edu)1.ProgressForthetaskofidentifyingin-demandskills,wehavewrittenanauto-crawlertocrawltweetsrelatedtoagivensubject""ComputerScience""fromasocialmedia,Twitter,andsavedattributesoftweetstoafile.DuetotherulesofTwitter,thereisaratelimitwiththecrawlingsowecannotgetasmanydataaswewant.However,wecanobtainthemost-recenttweetsrelatedtotheCSsubject,whichhelpstodiscoveremergingkeywordsortopics.Wehavealsoprocessedthecrawledtweets,andanalyzedthehashtags,whichrepresentthethemesofcorrespondingtweets,topreliminarilyextracttop-ranktopicsofthesetweets.Forthetaskofdisplayingrelativedocumentsofeachtopic,wehavefinishedcrawlingPDFslidesofseveralcoursesinUIUC.However,duetothedifferenceofcoursewebsites,crawlersarerunningindependentlyindifferentpythonscripts.Wehavealsoinvestigatedsomecommonrankingalgorithms,includingBM25,Jaccardindex,cosinesimilarityandsomeprovidedalgorithmsinMeTAToolkit.2.ChallengesSincetherearemanytweetswithouthashtags,wecannotextracttopicsfromsuchtweets,butithelpstoutilizehashtagsinextractingtopics.Hence,insteadofdesigningatopicdiscoveryalgorithmforgeneraltexts,weneedtoproposeanalgorithmwhichcanextracttopicsfrombothtweetswithandwithouthashtags,andcombinethesetopicstogetthefinaltop-ranktopicsamongthesetweets.Anotherchallengeishowtodemonstratethattheextractedtopicsaretrulypopularinthesetweets,orhowtoevaluatetheperformanceofouralgorithm.EvenifwehavecrawledtweetsfromTwitter,wedonotknowwhattopicsarepopularinthesetweets.Evaluationofdifferentrankingalgorithmsseemstime-consumingbecauseitishardtogetall-roundtrainingdataset.Inaddition,theformofdisplayingourrankingresultsisstillundecided.3.RemainingworkToidentifyin-demandskills,theremainingworkistoproposeatopicdiscoveryalgorithmspecificallyfortweets,anddesignanevaluationmethodtodemonstratetheperformanceofthisalgorithm.Todisplayrelativedocumentsofeachtopic,theremainingworkistodesignaworkflowthatcanautomaticallycrawldocumentsfromvariouscoursewebsites,andevaluatetheexistingrankingalgorithms'performanceinourdataset. CS410 Project Proposal Dancing Text: Yuxiang Liu ( yuxiang@illinois.edu , leader), Hongfei Ma ( hongfei7@illinois.edu ) 1. Proposed Study Weplantoimprovethe EducationalWebSystemthroughautomaticallycreatingteachingmaterialsfor in-demandskills,whichisanextendedversionoftheexistingsystem.Tosatisfytheincreasingdemandsfrom peoplewhoarelookingforhigh-qualityeducationbutcannotgetaccesstoitwhenevernecessary,weaimto identify in-demand skills  and  create lectures and tutorials for these skills . Toidentifyin-demandskills,wewillcrawldatafromsocialmediaandidentifyemergingkeywordsortopics throughtopicextraction.Tocreatelecturesandtutorialsforskills,wewillrecommendmostrelevantslides given specific topics. In this project, we plan to mainly use python and javascript languages. 2. Impacts Of The Proposed Work Throughthisproject,wewouldenableemergingtopicidentificationandslidesrecommendationfortopics, which will help users who are interested in getting high-quality education materials at any time. 3. Project Description 3.1 Identifying in-demand skills WeplantocrawltweetsfromTwitter.Ahashtagisatypeoftagthatpeopleusuallytreatasthethemeofthe correspondingtweet,sowewoulddirectlyconsiderhashtagsasthetopicoftweets.Then,wewoulduse featuresoftweetssuchasretweets,replies,orlikestoidentifywhichhashtagsareemergingorpopular keywords/topics.Inparticular,wemaylimitourtopicswithincomputerscienceorSTEMdisciplinesto reduce/avoidskill-irrelevanttopics.Afterweobtaintheseskill-relatedtopics,wewouldusetheminthenext part of creating corresponding lectures and tutorials. 3.2 Creating lectures and tutorials for skills Inthispart,wewillfirstcrawlslidesofothercoursesinUIUC.Fortheseslides,wewillidentifythetopicsand generatetrainingdata.Next,wewillproposeanefficientrankingalgorithmtoautomaticallyselectrelevant slidesforeachspecifictopic.Onceauserselectsatopicfromthoseobtainedinthefirstpart,thesystemwill presentrelevantslidestotheuseronthewebpage.Todemonstratetheperformanceofouralgorithm,wewill separateourcrawledslides,togetherwithexistingslides,intopositivesamplesandnegativesamples,and compare our algorithm with classical algorithms such as BM25 with this generated dataset. 4. Timeline Task Estimated Time Cost Expected Completion Time Social media crawling 5h Week 11 Paper survey 5h Week 11 Emerging topic identification 15h Week 14 Slides crawling 5h Week 11 Rank algorithm design 15h Week 13 Integration with existing system 20h Week 15 CourseProject Final Project for CS410 of UIUC. Project Proposal Proposal.pdf Project Progress Report Progress_Report.pdf Project Presentation Video Presentation Documentation 1. Overview This project consists of two major tasks. The first one is to identify emerging topics in Twitter within computer science field, and the second one is to recommend relevant slides of the given topics. 1.1 Identify Emerging Topics In this task, we first crawled 680k tweets from Twitter with query ""computer science"", which limited our scale of topics. Then, in order to mine topics from these tweets, we found the optimal number of topics w.r.t. coherence value and trained the LDA topic model with the optimal number of topics. Finally, we visualized topics with word cloud by analyzing hashtags. Besides, we support identifying emerging topics by crawling the latest tweets and predicting their topics with pre-trained LDA model, while these newly crawled data is used to update our LDA model. All related files are in Identify_Topics/ directory. - Crawling/: Keep crawling tweets and form training data. - data/: Store raw data (sorted_tweets.json), processed data (pre-processed.pkl), stopwords (stopwords.txt), and pictures of topics (topic_desc_fig/). - model: Store pre-trained LDA model files. - topic_discovery.py: Extract topics from crawled tweets, evaluate models with different number of topics to find the optimal, get topics and draw pictures for them with word cloud, and predict emerging topics based on pre-trained model. - topics.json: Word distributions of topics, which will feed into next part. 1.2 Recommend Slides for Topics In this task, we first crawled 100+ course slides in UIUC. Then, taking the above word distributions of different topics as input, we used BM25 to find relevant slides. All related files are in Recommend_Slides/ directory. - pdf_download.py: Scrapes slides from four fixed UIUC course websites which are CS225, CS240, CS425 and CS447. It will download all the PDF documents to a local directory ""slides"". - pdf_miner.py: Read the slides under the ""slides"" folder and use pdfminer tool to extract text from the slides. Then, write the raw text to a ""txt"" file under the folder ""raw"". For example, if it read a PDF file ""slides/Lecture1.pdf"", there will be a text file ""raw/Lecture1.txt"" which contains the text data of the original PDF file. - filter_raw.py: Read the raw text files under the ""raw"" folder and filter these texts so that they can be used in the following ranking algorithm. It removes the stop words, meaningless numbers and some other useless words. Then, it lemmatizes and stems the words so that derivative words can be treated equally. The results are saved under the ""corpus"" folder. Each file under this folder represents the abstract of a PDF file from ""slides"" folder. For example, if it read a text file ""raw/Lecture1.txt"", there will be a filtered text file ""corpus/Lecture1.txt"" which contains the cleaned text data. - bm25.py: Read the topic file ""topics.json"" and generate queries with the distributions of keywords in each topic. Each topic generates one query. Then, for each query, run the bm25 ranking algorithm to calculate the scores of this query to each documents in the ""corpus"" folder. Finally, get the top 10 documents and write the result to the target file ""result/bm25.txt"". - doc_sim.py: Similar as ""bm25.py"". The only difference is the ranking algorithm. It calculates the cosine similarity with TF-IDF weights with each pair of query and document. Then, get the top 10 documents and write the result to the target file ""result/sim.txt"". 2. Implementation 2.1 Identify Emerging Topics Tweets Crawling This part serves to generate dataset containing recent tweets from Twitter. Due to the rate limit of Twitter, we can only crawl a small amount of tweets every 15 min. Therefore, in Crawling/ directory, we implemented a crawler which can crawl tweets automatically. Twitter_crawler.py: Crawl recent tweets that don't overlap with pre-crawled tweets. utils.py: Sort crawled tweets in terms of create time, which aims to optimize crawling and saving process. Twitter_crawler.sh: Auto-crawling bash file that runs Twitter_crawler.py and utils.py repeatedly every 15 min. Topic Mining This part is to generate topics with crawled tweets. Here we applied LDA algorithm for topic mining. All related files are in TopiccDiscovery/ directory. Implementation of topic_discovery.py is as follows: Preprocess: For each tweet, we perform 1) lower; 2) remove username, hashtag, url, number, punctuation, special character, and short word; 3) tokenization; 4) remove stopwords; 5) lemmatization; and 6) stemming. Then, we save processed data data/pre-processed.pkl for training. Find optimal number of topics: We applied LDA model with different numbers of topics from 2 to 14, and found that 10 is the optimal. Training: We set number of topics as 10, trained an LDA model on 662k processed tweets, and saved model files in model/ directory. Saving Topics: We loaded pre-trained files, saved word distributions for topics, and drew word cloud figures by analyzing hashtags for all topics. Predict: With pre-trained model, we can crawl latest tweets about computer science and make predictions to find out emerging topics among all topics. Meanwhile, we use these newly crawled tweets to update the LDA model. 2.2 Recommend Slides pdf_download.py This module does the following: Given the course website page, use Soup to extract all the elements with tag ""a"". Judge the elements whether it is a link which is end with "".txt"". If yes, concatenate the prefix and the link to get the complete url. Download the PDF file with its original name and put it into the ""slides"" folder. Functions are: getTagA(root_url): Obtain all the elements with tag ""a"" and return a list of string. downPdf(root_url, prefix, list_a): Download all the PDF files in the root_url. The argument ""prefix"" is used to complete the pdf links. It varies with different course websites. getFile(url): Get the url file to the ""slides"" folder. pdf_miner.py This module does the following: Read each PDF file under the ""slides"" folder. Create a PDF Parser for each PDF file. Then parse the pdf file and extract the text data from each page. Write the raw text data to a target file under the folder ""raw"". Functions are: parse(filename): Extract text data from a PDF file and write it to a target text file (Different PDF files write into different text files). filter_raw.py This module does the following: Read each raw text file under the ""raw"" folder. Tokenlize the text data and remove short words, numbers and stop words from the text. Lemmatize and stem words. Then, write it to a text file under the ""corpus"" folder (Different raw text files write into different target files). Functions are: get_raw_data(filepath): Read a raw text file and return a list of string. Each element in this list represents a line of data in the raw text file. pre_process(data, filename): First, use ""re"" (regular expression) to remove unwanted words. Second, use ""spacy"" to lemmatize words and ""nltk.stem"" to stem words. Finally, write these stemmed words to the target file under the ""corpus"" folder. bm25.py This module does the following: Read the topic file ""topics.json"" and generate a query based on the distributions of keywords in each topic. For example, if the topic is ""{""topic1"" : {kewords1 : 0.5, keyword2 : 0.5}}"", it will generate a query like ""keyword1 keyword1 keyword2 keyword2"" which keeps to the distributions. Treat this query as a document and compute the scores of this query to each document in our corpus with the BM25 model implemented by gensim library. Get the top 10 score document names and write the result to the result file ""result/bm25.txt"". Functions are: tokenization(filename): Read the document under the ""corpus"" folder and return a list of words in this document. read_corpus(dir_path): Read all the documents under the dir_path and return a 2-dimensional list of strings. The first dimension represents each document and the second one contains the words included in each document. simulate_query_by_topics(topic_file): Generate queries with topics. In this implementation, it generates query with a word base 100. If we have keyword1 and keyword2 with distribution of 0.2 and 0.5. It will generate a query with 20 keyword1 and 50 keyword2. Node: each topic only reserves top several keywords. Their distributions may not add up to one, but it doesn't affect their relative size. doc_sim.py: This module does the following: Read the topic file ""topics.json"" and generate a query based on the distributions of keywords in each topic. For example, if the topic is ""{""topic1"" : {kewords1 : 0.5, keyword2 : 0.5}}"", it will generate a query like ""keyword1 keyword1 keyword2 keyword2"" which keeps to the distributions. Treat this query as a document and compute the cosine similarity with TF-IDF weights with each documents under ""corpus"" folder. Get the top 10 similarity document names and write the result to the result file ""result/sim.txt"". Funcrions are the similar to those in ""bm25.py"". 3. Usage Installation This software requires python 3.5+, and it also requires external libraries that can be installed by: pip install -r requirements.txt After you have installed spacy library, you also need to load en model in spacy through: python -m spacy download en_core_web_sm Now you have all the necessary packages! Before any later steps, clone this repository: git clone https://github.com/liuyuxiang512/CourseProject Usage Example cd CourseProject Identify Emerging Topics Directory Identify_Topics/ serves to identify emerging topics in Twitter. cd Identify_Topics Crawl Tweets from Twitter You could jump this step by downloading our crawled data sorted_tweets.json, which contains 680k tweets, and save the file into data/ directory. In order to crawl tweets, you first need a Twitter developer account. Then: Create a Twitter application via https://developer.twitter.com/. Create a Twitter app to access Twitter's API. Find the authentication info in the ""Keys and Access Tokens"" tab of the app's properties, including consumer_key, consumer_secret, access_token, and access_token_secret. Fill the authentication into authentication.txt in four lines. Then, you can keep crawling tweets by cd Crawling bash Twitter_crawler.sh Find Optimal Number of Topics In this step, you can try LDA model with different numbers of topics from 2 to 14, and get corresponding coherence values. A higher coherence value means a better model. If you don't want to use our processed data Identify_Topics/data/pre-processed.pkl, you may first remove it and continue, but it will take some time. To find out the optimal number of topics, run python topic_discovery.py --tune Then you will get Tuning... Number of Topics: 2 --- Coherence Value: 0.49589240555472486 Number of Topics: 3 --- Coherence Value: 0.4752864500035534 Number of Topics: 4 --- Coherence Value: 0.4844109302488787 Number of Topics: 5 --- Coherence Value: 0.5426149238108859 Number of Topics: 6 --- Coherence Value: 0.5708485237453553 Number of Topics: 7 --- Coherence Value: 0.5514423515877226 Number of Topics: 8 --- Coherence Value: 0.5778541035204716 Number of Topics: 9 --- Coherence Value: 0.566857492981066 Number of Topics: 10 --- Coherence Value: 0.5808911042666589 Number of Topics: 11 --- Coherence Value: 0.5561191556402437 Number of Topics: 12 --- Coherence Value: 0.5699566981479943 Number of Topics: 13 --- Coherence Value: 0.5522769193550581 Number of Topics: 14 --- Coherence Value: 0.5433632323040761 ... The optimal number of topics is: 10 Therefore, the optimal number of topics is 10, and we will use 10 for our LDA model in formal training. Train With 10 as number of topics, you can now train an LDA model by running python topic_discovery.py --train This step will take a long time, but you can go ahead and directly use our pre-trained model in Identify_Topics/model/ directory for subsequent steps. Display This step is to use pre-trained model to get topics and draw word cloud for these topics. python topic_discovery.py --display Or you can see what we have got after this step: word distributions of topics Identify_Topics/topics.json and word cloud figures of topics in Identify_Topics/data/topic_desc_fig/. The following are word cloud figures for 2 topics out of 10. Predict After the previous steps, you have successfully obtained processed data (Identify_Topics/data/pre-processed.pkl), word distributions of topics (Identify_Topics/topics.json), and word cloud pictures of topics in Identify_Topics/data/topic_desc_fig/. This step is a further extension of our software. You still need a Twitter developer account to crawl latest tweets. Please refer to how to get authentication info in the above ""Crawling Tweets from Twitter"" section. You can always find out emerging topics in Twitter by running python topic_discovery.py --predict It will crawl latest tweets, predict topics for then and figure out popular ones. Meanwhile, it also uses these newly crawled data to update the LDA model. We got the following results on Dec.13th 9am: Emerging Topics ID (Ordered): 8 6 0 4 1 3 To see what these topics are, you may go to data/topic_desc_fig/ directory and find corresponding word cloud! Command Line Usage of topic_discovery.py ``` python topic_discovery.py -h usage: topic_discovery.py [-h] [-i INPUT_FILE] [-n NUM_TOPICS] [-f FIELD] [-o OUTPUT_FILE] [--train] [--display] [--tune] [--predict] Identify In-demanding Skills optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE input file contains tweets crawled from Twitter - (default: data/sorted_tweets.json). -n NUM_TOPICS, --num_topics NUM_TOPICS number of topics - (default: 10). -f FIELD, --field FIELD field of subject to mine - (default: computer science) -o OUTPUT_FILE, --output_file OUTPUT_FILE output file contains term distribution of topics - (default: topics.json). --train preprocess and train --display save topics and draw pictures --tune find the optimal number of topics --predict predict emerging topics with trained model ``` Recommend Slides Directory Recommend_Slides is to recommend related slides based on topics. cd Recommend_Slides Download the slides using the pdf_download.py. But it may be slow. You can access the PDF slides with the link: Download Slides. Then, unzip it to the ""slides"" folder. bash python3 pdf_download.py Then, we need to extract raw text from PDF files and filter these raw texts. bash python3 pdf_miner.py python3 filter_raw.py Finally, using bm25.py or doc_sim.py to calculate the final results. After this step, you can see the result files under the ""result"" folder. bash python3 bm25.py python3 doc_sim.py Other Usage main.py: Users can run this script with python3. It provides 2 kinds of command. (Note: This two commands are available after filtering the raw text). bash python3 main.py The first one is ""latest"". It will automatically run the results with existing topics in ""topics.json"". The second one is ""query"". Then it will ask you to type in a query and output 10 files that are most relevant to your query. This ranking list is based on BM25 algorithm because after our mannual evaluation, BM25 ranking performs better than cosine similarity ranking. search.py: Users can run this script with python3. It provides 2 kinds of command. (Note: This two commands are available after filtering the raw text). bash python3 search.py The first one is ""bm25"". It means that the following ranking is based on BM25 algorithm. Then it will ask you to type in a query and output 10 filenames that are most relevant to your query. The second one is ""sim"". It means that the following ranking is based on document cosine similaritiy. Then it will ask you to type in a query and output 10 filenames that are most relevant to your query."
https://github.com/losaohan/CourseProject	"CS410 Project Documentation - ExpertSearch System Overview of the function of the code The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Users can provide a piece of search query that contains name, university, specialization, location, etc. in the text box showing ""Enter Search Query"", and hit ""Enter"" on keyboard or click the icon on the right side of the text box. Two additional filters ""Locations"" and ""Universities"" are optional to further filter the desired faculty's geographic information and association name. The search result of faculties will display in the order of relevance below. There are two rows in the header area. In the first row, faculty name will show on the left bald-faced, and Area of interest (the new feature added in this project) aligns to the right. There can also be a envelop shape icon if email address is captured, and clicking it will evoke the system-default email application to draft an email to the faculty. The next icon will lead to the faculty bio page. In the second row, department and university name display on the left, while the state and country name show on the right. In the main body of search result, it shows a segment from detailed faculty introduction that contains the query information. Code structure and implementation details Information extraction The first step of the code is to extract information from the source data. The raw data is saved in the ./data/compiled_bios folder. Each file is named by a 0-based index and contains one line of faculty detailed introduction. The existing code has 3 .py files in the ./extraction folder, each has the Python code to extract email, names and location from the raw data. This project added a Jupyter notebook called ""named-entity-recognition.ipynb"" that can extract the research area of interest from the raw data. Below is a brief introduction of the code logic: Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence: ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] }) Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy. Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are."" Testing We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Here are a few examples of research area extracted: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')] Extracted information are saved under ./data folder, each named by the field extracted. For example, in the names.txt file, there are 6525 rows. Each row corresponds to the faculty name extracted from the raw data at the same index. Ranking function Those extracted information from last step are merged into a metadata.dat file under ./data/compiled_bios folder. The existing code already has 8 fields in each row in the metadata, so this project created a ./data/compiled_bios/merge_new_field_to_metadata.py file to append the newly extracted research area to the end of each row. In addition, it' also required to add the name and type to the end of file.toml file under the same folder, so that the code knows how to parse the new field added. The metadata file is then feed into the ranker located as ./data/expertsearch/ranker.py, configed by a config file located at ./data/compiled_bios/config.toml. Visualization webpage Visualization is handled by the ./templates/index.html file, which designs the layout and format of the ExpertSearch webpage. It's also controlled by ./static/index.js file to interact with the ranker and user. The new field ""Area of Interest"" needs to be added in the JavaScript file, in order to display on the search result. Main function The main function is called ./server.py, that is executed when the program launches. It uses flask to drive the ranker and visualization functions, so that a local server is running at http://localhost:8095/. It's also necessary tell the code to get the research interest in metadata and pass it to the Javascript. Installation and usage It's extremely easy to install and run the code on Mac or Linux, but needs some tricky settings to execute on Windows. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required. Installation on Windows Install Git (if you haven't) In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/ Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students. Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"": In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window. In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."". Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Hit Run: the image will start to build, and the container will launch in a few seconds. In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up On Mac/Linux Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter. In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: Team member contribution Bruno Seo: Focused on the modeling part: analyzed on the source data, identified the pattern of research interest key words, implemented the algorithms of Named Entity Recognition, and trained the extraction model. Joseph Angulo: Focused on the front-end visualization part: figured out how the web server work, designed the appearance of updated webpage, and implemented the changes to show the new field on the page. Xiaohan Liu (captain): Coordinate and focus on the high-level Python: solved the installation and execution across Windows/Mac/Linux systems, figured out the structure and workflow of the project, implemented code to prepare training data as well as code to create metadata, drafted the proposal / progress report / final document / presentation. CS410 Project Documentation - ExpertSearch System 1 Overview of the function of the code The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Users can provide a piece of search query that contains name, university, specialization, location, etc. in the text box showing ""Enter Search Query"", and hit ""Enter"" on keyboard or click the icon on the right side of the text box. Two additional filters ""Locations"" and ""Universities"" are optional to further filter the desired faculty's geographic information and association name. The search result of faculties will display in the order of relevance below. There are two rows in the header area. In the first row, faculty name will show on the left bald-faced, and Area of interest (the new feature added in this project) aligns to the right. There can also be a envelop shape icon if email address is captured, and clicking it will evoke the system-default email application to draft an email to the faculty. The next icon will lead to the faculty bio page. In the second row, department and university name display on the left, while the state and country name show on the right. In the main body of search result, it shows a segment from detailed faculty introduction that contains the query information. 2 Code structure and implementation details 2.1 Information extraction The first step of the code is to extract information from the source data. The raw data is saved in the ./data/compiled_bios folder. Each file is named by a 0-based index and contains one line of faculty detailed introduction. The existing code has 3 .py files in the ./extraction folder, each has the Python code to extract email, names and location from the raw data. This project added a Jupyter notebook called ""named-entity-recognition.ipynb"" that can extract the research area of interest from the raw data. Below is a brief introduction of the code logic:  Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence: ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] })  Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy.  Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are.""  Testing We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Here are a few examples of research area extracted: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')] Extracted information are saved under ./data folder, each named by the field extracted. For example, in the names.txt file, there are 6525 rows. Each row corresponds to the faculty name extracted from the raw data at the same index. 2.2 Ranking function Those extracted information from last step are merged into a metadata.dat file under ./data/compiled_bios folder. The existing code already has 8 fields in each row in the metadata, so this project created a ./data/compiled_bios/merge_new_field_to_metadata.py file to append the newly extracted research area to the end of each row. In addition, it' also required to add the name and type to the end of file.toml file under the same folder, so that the code knows how to parse the new field added. The metadata file is then feed into the ranker located as ./data/expertsearch/ranker.py, configed by a config file located at ./data/compiled_bios/config.toml. 2.3 Visualization webpage Visualization is handled by the ./templates/index.html file, which designs the layout and format of the ExpertSearch webpage. It's also controlled by ./static/index.js file to interact with the ranker and user. The new field ""Area of Interest"" needs to be added in the JavaScript file, in order to display on the search result. 2.4 Main function The main function is called ./server.py, that is executed when the program launches. It uses flask to drive the ranker and visualization functions, so that a local server is running at http://localhost:8095/. It's also necessary tell the code to get the research interest in metadata and pass it to the Javascript. 3 Installation and usage It's extremely easy to install and run the code on Mac or Linux, but needs some tricky settings to execute on Windows. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required. 3.1 Installation on Windows  Install Git (if you haven't)  In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git  Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/  Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students.  Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"":  In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window.  In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."".  Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Hit Run: the image will start to build, and the container will launch in a few seconds.  In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up 3.2 On Mac/Linux  Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter.  In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt  Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: 4 Team member contribution Bruno Seo: Focused on the modeling part: analyzed on the source data, identified the pattern of research interest key words, implemented the algorithms of Named Entity Recognition, and trained the extraction model. Joseph Angulo: Focused on the front-end visualization part: figured out how the web server work, designed the appearance of updated webpage, and implemented the changes to show the new field on the page. Xiaohan Liu (captain): Coordinate and focus on the high-level Python: solved the installation and execution across Windows/Mac/Linux systems, figured out the structure and workflow of the project, implemented code to prepare training data as well as code to create metadata, drafted the proposal / progress report / final document / presentation. CS 410 Project Progress Report Modification of project scope: In the original project proposal, we planned to extract and show both research interests and top 2/3 keywords in publications. After careful consideration, we think the second part will take tremendous time to download and parse publications from sources (like Arxiv or Google Scholar). Therefore, we decided to hold off the publication keywords and first focus on research interests in this project. After we finish the visualization of research interests, we may either come back to the publication keywords, or switch to another field that is contained in the faculty bio data. Progress report: The following tasks in the proposal have been finished: Digest the existing data + code + visualization and test how they work and connect (10h) Analyze the dataset for the pattern of the research of interest and publication keywords, and come up with corresponding algorithms (10h) Implement functions to retrieve those fields (15~20h) Train the new text retrieval/mining system and make some improvements (10h) The following tasks are pending: Adjust the web frontend to display the new fields (5~10h) Test the new function as well as the whole system (5~10h) Prepare the complete documentation/report (5~10h) Challenges: The biggest challenge we faced was to systematically download faculty publications using faculty name/university as the query. After spending some time engineering on this, we decided to hold this off and may either come back to it later or switch to another field that is contained in the faculty bio data. We also had some difficulties at the beginning running and debugging the program on Windows, as some of our teammates use Windows machine. With the support of Docker in Pycharm Professional version, we were finally able to run it on Windows. We are currently struggling with html and Javascript to display the new field ""research interests"" on the webpage. We think this should be manageable as it is almost the last ""technical"" part of the project. Below is a brief description of our steps to extract area of interests: Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] }) Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy. Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are."" Outcome We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Example: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')] CS410 Project --ExpertSearchSystemBRUNO SEOJOSEPH ANGULOXIAOHAN LIUIntroductionThe ExpertSearchsystem is a webpage-based search engine that provides search function for faculty information.IntroductionThe search result of faculties will display in the order of relevance below.Another Sample InputInstallationIt's extremely easy to install and run the code on Mac or LinuxOn Windows it requires some tricky settings to execute the code. This is because the Python webserver package Gunicorndoesn't support directly on Windows system, so Docker is required to run the code.Installation on WindowsInstall Git (if you haven't)In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.gitDownload and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/Download and install PycharmProfessional for Windows: https://www.jetbrains.com/pycharm/download/#section=windowsYou need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students.Installation on WindowsOpen Pycharm, open the CourseProjectfolder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"":Installation on WindowsIn the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK.Installation on WindowsA new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window.Installation on WindowsIn the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."".Installation on WindowsFill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same.Installation on WindowsHit Run: the image will start to build, and the container will launch in a few seconds.Installation on WindowsIn the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine.If you type localhost:8095 in your browser, the expert search page will show upInstallation on Mac/LinuxOpen Pycharm, open the CourseProjectfolder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter.Installation on Mac/LinuxIn the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txtInstallation on Mac/LinuxType gunicornserver:app-b 127.0.0.1:8095 in the terminal and the server will launch.Type localhost:8095 in your browser, the expert search page will show up:Thanks! CS410 Project -- ExpertSearch System Bruno seo Joseph Angulo Xiaohan Liu Introduction The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Introduction The search result of faculties will display in the order of relevance below. Another Sample Input Installation It's extremely easy to install and run the code on Mac or Linux On Windows it requires some tricky settings to execute the code. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required to run the code. Installation on Windows Install Git (if you haven't) In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/ Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students. Installation on Windows Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"": Installation on Windows In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. Installation on Windows A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window. Installation on Windows In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."". Installation on Windows Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Installation on Windows Hit Run: the image will start to build, and the container will launch in a few seconds. Installation on Windows In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up Installation on Mac/Linux Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter. Installation on Mac/Linux In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt Installation on Mac/Linux Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: Thanks! CS 410 Project Proposal Team members Joseph Angulo (jangulo2) Bruno Seo (sbseo2) Xiaohan Liu (xliu120) - captain/coordinator Project topic Improve the ExpertSearch System: Extracting relevant information from faculty bios If you are adding a function, how will you demonstrate that it works as expected? In this project, we are going to add two functions to extract faculty research interests and top 2 (or 3) keywords in publications. In the search result page, the two new fields will be displayed after the name and address items (see highlighted area below). Briefly describe the datasets, algorithms or techniques you plan to use We are going to reuse the existing data of the search engine, which is part of the codebase in GitHub. We plan to use Named Entity Recognition (NER) algorithm along with other data mining techniques introduced in this course. How will your code communicate with or utilize the system? We are also going to rely the existing code framework: new functions will be added to the ./extraction folder. New results will be merged together with existing search results and displayed on the webpage. Which programming language do you plan to use? Python will be the main language for extraction while Javascript and HTML for visualization. Main tasks and estimated time cost (60~80hrs) Digest the existing data + code + visualization and test how they work and connect (10h) Analyze the dataset for the pattern of the research of interest and publication keywords, and come up with corresponding algorithms (10h) Implement functions to retrieve those fields (15~20h) Train the new text retrieval/mining system and make some improvements (10h) Adjust the web frontend to display the new fields (5~10h) Test the new function as well as the whole system (5~10h) Prepare the complete documentation/report (5~10h) ExpertSearch To run the code, run the following command from ExpertSearch (tested with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ For details of installation or running the code on Windows, please refer to the project documentation or presentation."
https://github.com/luoxix/CourseProject	"PROJECT DOCUMENTATION Reproduce A Paper: A cross-collection mixture model for comparative text mining Overview This code is used to discover latent themes across collections. We provide two models, the simple mixture model (simplemix.py) and the cross-collection mixture model (ccmix.py). The simple mixture model treats multiple collections as one single collection and discovers k latent common themes in it. The Cross-collection mixture model explicitly distinguishes themes from different collections. It not only captures k common themes that characterize common information across all collections but also k collection-specific themes for each collection. The value of k is set by users and each theme is characterized by a multinomial word distribution. Implementation Dataset The paper uses 2 datasets: war news, and laptop reviews. We are not able to obtain exactly the same datasets as those are used in the paper. For the war news, we manually searched and downloaded 30 news articles from BBC or CNN for each of the two wars, published in one-year span (May 2003 - April 2004 for Iraq war, Nov 2001 to Oct 2002 for Afghanistan war) to approximate the war news dataset that is used in the paper. (code/data/war_dataset.txt) For the laptop reviews, we chose 3 laptops from the Amazon.com best sellers in laptop computers: Acer Aspire 5 Slim Laptop, Apple MacBook Air, HP Chromebook 14-inch HD Laptop, and manually downloaded the top 40 reviews from Amazon.com. (code/data/laptop_reviews.txt) Each document occupies one line in the .txt file, prepended by an integer that indicates which collection the document is from. Preprocessing We preprocessed the data by removing stop words (including punctuation marks), words that contain less than 3 characters, and stemming the documents to map inflected words to their stems. Preprocessing is important to ensure that the resulting clusters after applying the mixture model would contain more meaningful words for evaluation. Initialization SimpMix Model (baseline) CCMix Model In SimpMix model, there are two types of themes: the background theme, and the shared themes (between collections) In the CCMix model, there are three types of themes: the background theme, the common themes throughout all collections, and themes specific to the collections. Both SimpMix and CCMix models use the EM algorithm to find the clusters iteratively. The two models also share the same background theme (topic_word_prob_background, dimension: 1 * vocabulary_size) which is calculated by taking the maximal likelihood of each word in the whole corpus (includes all the collections) before the iterative EM steps. The other parameters !!(lambda_B) and !""(lambda_C) which are the probability of selecting the background theme and the common theme respectively. We used 0.95 for !!and 0.25 for !""as the paper suggested. Expression Name in code Initialization function Dimension !! lambda_B Scalar, 0.95 !"" lambda_C Scalar, 0.25 !(#,%) term_doc_matrix build_term_doc_matrix (self) This function counts the term frequency in each document, and computes the background distribution. number_of_collections * number_of_documents * vocabulary_size '(#|)!) topic_word_prob_background 1 * vocabulary_size ""#,% document_topic_prob initialize_randomly (self, number_of_topics) This function randomly initialize number_of_collections * number_of_documents * number_of_topics '(#|)%) topic_word_prob number_of_topics * vocabulary_size '(#|)%,&) topic_word_prob_collection_specific number_of_collections * number_of_topics * vocabulary_size # epsilon Scalar, 0.00001 The EM algorithm The implementation of the baseline SimpMix model is very similar to MP3 except for introducing a background model. Therefore, in this section, we will focus on how to implement the EM algorithm for the CCMix model. E step E step is to estimate the hidden variables: probability of selecting a theme in a mixture model. Expression Name in code Calculation function Dimension '(*#,""!,'=,) Collection specific theme topic_prob_j expectation_step (self, number_of_topics, verbose) This function performs E step number_of_collections * number_of_documents * vocabulary_size * number_of_topics '(*#,""!,'=-) Background theme topic_prob_B number_of_collections * number_of_documents * vocabulary_size * 1 '(*#,""!,'=.) Common theme topic_prob_C number_of_collections * number_of_documents * vocabulary_size * number_of_topics M step M step is to use the hidden variables to re-estimate the distributions in each theme, maximizing the likelihood. Expression Name in code Calculation function Dimension ""#,%()*+) document_topic_prob maximization_step(self, number_of_topics, verbose) This function performs M step number_of_collections * number_of_documents * number_of_topics '()*+)(#|)%) topic_word_prob number_of_topics * vocabulary_size '()*+)(#|)%,&) topic_word_prob_collection_specific number_of_collections * number_of_topics * vocabulary_size /01 '(.) likelihoods calculate_likelihood(self) This function calculates likelihood List of scalars The iteration of E-M steps continues until the difference between likelihood in adjacent iterations is less than #or the maximum iteration number is reached. Usage Python version Python 3.6 Download Repo git clone https://github.com/luoxix/CourseProject.git Install Dependencies pip install metapy pip install numpy Run code To run the simple mixture model: python simplemix.py --input_path ./data/laptop_reviews.txt --output_path ./result/result_simple_laptop.txt --lambda_b 0.95 --max_iterations 500 --number_topics 5 --number_top_words 8 --verbose True To run the cross-collection mixture model: python ccmix.py --input_path ./data/laptop_reviews.txt --output_common_path ./result/common_laptop.txt --output_specific_path ./result/specific_laptop.txt --lambda_b 0.95 --lambda_c 0.25 --max_iterations 1000 --number_topics 5 --number_top_words 8 --verbose True python ccmix.py --input_path ./data/war_dataset.txt --output_common_path ./result/common_war.txt --output_specific_path ./result/specific_war.txt --lambda_b 0.95 --lambda_c 0.25 --max_iterations 1000 --number_topics 5 --number_top_words 8 --verbose True Meaning of each argument input_path: the path of the input file which contains all collections. Each line contains a document and the first number denotes which kind of collection it is from. output_path: the path of the output file which contains k themes, for each theme, several top words with highest probability are shown. output_common_path: the path of the output file which contains common themes output_specific_path: the path of the output file which contains specific themes lambda_b: the weight of the background model lambda_c: the weight of common theme max_iterations: the number of iterations for EM algorithm number_topics: the number of latent themes number_top_wods: the number of top words which are shown in the output file verbose: whether to output the immediate information Results and Evaluations Run the code with instructions described above until the likelihood value converges. Laptop Reviews Cluster 1 Cluster 2 Cluster 3 Common theme words catalina0.0198 remov 0.0148 harddriv 0.0148 ad 0.0148 new 0.0102 second 0.0102 internet 0.0099 found 0.0099 samsung 0.0169 2400 0.0169 numer 0.0169 tech 0.0169 top 0.0169 edg 0.0169 left 0.0169 hour 0.0148 fan 0.0311 2020 0.0311 temperatur 0.0271 thermal 0.0271 cpu 0.0216 zoom 0.0203 extern 0.0203 bar 0.0203 Cluster 1 Cluster 2 Cluster 3 Common air 0.0252 connect 0.024 2020 0.022 thermal 0.0165 pictur 0.0137 poor 0.0125 bar 0.0124 new 0.0124 mode 0.0247 app 0.0179 differ 0.0172 side 0.0169 2400 0.0162 samsung 0.0162 support 0.014 download 0.013 call 0.0304 cpu 0.0162 amazon 0.0156 charg 0.0155 thermal 0.0153 brand 0.015 noth 0.012 bar 0.0115 Acer drive 0.0274 ad 0.0201 harddriv 0.0197 remov 0.0194 click 0.0178 pictur 0.0175 new 0.0165 second 0.0142 remov 0.0204 mode 0.0163 harddriv 0.0146 ad 0.0146 wouldn't 0.0129 veri 0.0121 second 0.0121 case 0.0114 call 0.0354 amazon 0.0191 did 0.0166 tech 0.0157 bla 0.0155 brand 0.015 wait 0.0139 minut 0.0135 HP connect 0.0347 amaz 0.0221 pictur 0.0206 love 0.017 unit 0.016 chrome 0.0158 drive 0.0151 nice 0.0151 cuz 0.0413 love 0.028 i'm 0.0207 didn't 0.0207 daughter 0.0207 polici 0.0207 glad 0.0207 aren't 0.0207 call 0.0306 charg 0.0204 brand 0.0197 amazon 0.0163 noth 0.0157 minut 0.0145 did 0.0139 differ 0.0136 Macbook Air upgrad 0.1062 hard 0.0704 16gb 0.0551 drive 0.0522 probabl 0.0475 beauti 0.0475 instal 0.0452 suppos 0.0448 receiv 0.0258 lock 0.0234 dissapoint 0.0162 possibl 0.0162 owner 0.0162 anazon 0.0162 recoveri 0.0162 immedi 0.0162 call 0.0304 cpu 0.0162 amazon 0.0156 charg 0.0155 thermal 0.0153 brand 0.015 noth 0.012 poor 0.0115 From the result, we can see that SimpMix model is only able to find the themes in the whole corpus. The themes are shared among collections. It tells about what topics the overall corpus covers, but it is not able to identify topic differences between different collection. On the other hand, the CCMix model is able to find the common themes throughout the collections, and is also able to identify the different specific themes in each collection. For example, in Cluster 2, all three collections share the same common theme. However, there is a high frequency of ""love"" in HP collection, whereas there is a high frequency of ""disappoint"" in the Macbook Air collection. We may infer these two opposite attitudes maybe towards the same topic in the common theme. Probably this HP laptop provides app or support that buyers love, whereas Macbook Air disappointed buyers in these two aspects. Therefore, we can conclude that, in reviews evaluation, the CCMix model is able to identify different performance of similar products on the same aspects. Another observation is that not all specific themes are well distinguished from the common theme / other specific themes within the same cluster (e.g., Cluster 3). This is probably because we use a uniform !"" for all clusters. However, for some clusters, there are more overlaps in topics among collections and less differences, and !"" should be larger to account for the common topics. The data we use are from Amazon, and many of them are expression of feelings, and purchase experience with Amazon, instead of technical reviews. Therefore, the results are more on customer satisfaction. On the other hand, performing CCMix model on technical reviews will find more about the performance of each laptop. War Dataset Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Common theme words khalifa 0.0307 o'neil 0.0199 newsweek0.0174 quran 0.0154 hous 0.0141 dyke 0.0113 cbs 0.0102 kennedi 0.0092 mirror 0.0161 threat 0.0139 palac 0.0134 wmd 0.0122 religi 0.0113 morgan 0.0092 nuclear 0.0087 pictur 0.0076 flag 0.0475 gun 0.0158 design 0.0091 nasratullah0.0088 kit 0.0088 equip 0.0087 zardad 0.0083 leak 0.0072 woodward0.0247 powel 0.0220 kerri 0.0210 marin 0.0201 matti 0.0174 gen 0.0125 clinton 0.0114 bandar 0.0110 draft 0.0237 opium 0.0211 hamdi 0.0202 rape 0.0169 wolfowitz0.0149 farmer 0.0149 poppi 0.0132 erad 0.0114 Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Common theme words Flag 0.0450 chang 0.0158 design 0.0131 women0.0096 repres 0.0095 new 0.0086 threat 0.0085 equip 0.0084 gun 0.0332 draft 0.0296 leak 0.0173 kennedi 0.0158 katharin 0.0154 o'neil 0.0143 prosecut 0.0130 secret 0.0128 Wolfowitz 0.0403 soro 0.0370 group 0.0304 rumsfeld 0.0277 independ 0.0230 money 0.0220 rais 0.0185 moveon.org 0.0168 mirror 0.0277 religion 0.0218 god 0.0207 zardad 0.0195 koran 0.0164 morgan 0.0164 dearing 0.0158 cramer 0.0135 marin 0.0178 woodward 0.0165 powel 0.0147 coalit 0.0145 matti 0.0123 gen 0.0122 opium 0.0101 sunday 0.0095 Iraq theme words flag 0.0450 chang 0.0158 design 0.0133 women 0.0096 repres 0.0095 new 0.0088 threat 0.0085 equip 0.0084 gun 0.0338 draft 0.0295 leak 0.0172 kennedi 0.0158 katharin 0.0153 o'neil 0.0142 prosecut 0.0129 secret 0.0128 wolfowitz 0.0403 soro 0.0370 group 0.0304 rumsfeld 0.0277 independ 0.0230 money 0.0220 rais 0.0185 moveon.org 0.0168 mirror 0.0277 religion 0.0218 god 0.0207 zardad 0.0195 koran 0.0164 morgan 0.0164 dearing 0.0158 cramer 0.0135 zapatero 0.0295 spain 0.0269 spanish 0.0167 coalit 0.0134 sunday 0.0132 marin 0.0124 woodward 0.0115 powel 0.0102 Afghan theme words women 0.0326 rape 0.0119 elect 0.0118 soviet 0.0117 khalifa 0.0112 live 0.0099 nasratullah 0.0092 villag 0.0091 kerri 0.0389 hamdi 0.0382 hous 0.0315 clinton 0.0217 cohen 0.0186 wednesday 0.0154 foam 0.0152 polystyren 0.0133 money 0.1082 group 0.1033 rumsfeld 0.0746 million 0.0695 rais 0.0515 candid 0.0456 link 0.0384 campaign 0.0375 mirror 0.0654 zardad 0.0472 morgan 0.0396 daili 0.0285 tortur 0.0244 qlr 0.0216 pictur 0.0212 goldsmith 0.0202 newsweek 0.0554 quran 0.0489 magazin 0.0274 toilet 0.0228 desecr 0.0228 isikoff 0.0196 dirita 0.0196 investig 0.0151 Similarly, the result for the war dataset also demonstrates the ability of CCMix in differentiating the specific themes between collections. For example, in Cluster 1, we can see that in Iraq war news, people are more interested in reporting flag and mental changes, whereas in Afghanistan war news, women raped were reported in the highest frequency. In Cluster 2, Iraq war news reported more on gun and draft, while Afghanistan war news reported more on the two persons: Kerry and Hamdi. Another observation with the war dataset is that in Cluster 1 to 4, the common theme has high similarity with the Iraq theme, and has much smaller overlap with the Afghan theme. This is probably because the Iraq specific theme is a very tight cluster where the top words have very high frequencies, such that the common theme is only able to account for partial frequencies of the top words. Only in Cluster 5, both themes are very different from the common theme. Conclusion In conclusion, we can see that CCMix model is able to address the task of comparative text mining by its capability to discover the latent common themes across all collections, and to summarize the similarity and differences of the collections along each common theme. However, the performance of CCMix varies on the choice of !! and the characteristics of the dataset that it is applied on. Contribution Xi Luo (xiluo4) Yuheng Zhang (yuhengz2) Algorithm implementation: CCMix Documentation: - Implementation - Results and Evaluations - Conclusion Algorithm implementation: SimpMix Documentation: - Overview - Usage Tutorial presentation PROGRESS REPORT Reproduce A Paper:  A cross-collection mixture model for comparative text mining 1.Data Collection (Partially Completed) Dataset 1: Laptop reviews (Completed) Dataset 2: War news (Not Started) Thisshouldbedonebymanuallysearchinganddownloading30newsarticlesfrom BBCorCNNforeachofthetwowars,publishedinoneyearspan(May2003-April 2004forIraqwar,Nov2001toOct2002forAfghanistanwar).Thiswouldbeavery close proximation of the war news dataset that is used in the paper. 2.Algorithm Implementation (Completed) ImplementationoftheCCMixmodelisfullydone,andEMconvergeswiththelaptop reviews dataset. 3.Experimental Results Gathering (Partially Completed) Theexperimentalresultsshouldbecollectedandsummarizedinatableoftheme clusters as the same way in the paper to facilitate comparison. 4.Results Comparison and Evaluation (Not Started) Baselinemodel(SimpMix)shouldbeimplemented,andtheresultsfromtheCCMix modelshouldbecomparedwiththosefromthebaselinemodeltoevaluatethe performance of CCMix model and compare with the conclusions from the paper. 5.Preparation of Documentation and Presentation (Not Started) A readme file and a video presentation should be prepared for the final submission. PROJECT PROPOSAL Reproduce A Paper:  A cross-collection mixture model for comparative text mining 1.WhatarethenamesandNetIDsofallyourteammembers?Whoisthecaptain? The captain will have more administrative duties than team members. Xi Luo (Captain): xiluo4 Yuheng Zhang: yuhengz2 2.Which paper have you chosen? ChengXiangZhai,AtulyaVelivelli,andBeiYu.2004.Across-collectionmixture modelforcomparativetextmining.InProceedingsofthe10thACMSIGKDD internationalconferenceonknowledgediscoveryanddatamining(KDD2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 3.Which programming language do you plan to use? python 4.Can you obtain the datasets used in the paper for evaluation? Thepaperuses2datasets:warnews,andlaptopreviews.Wearenotabletoobtain exactly the same datasets as those are used in the paper. 5.Ifyouanswer""no""toQuestion4,canyouobtainasimilardataset(e.g.amore recent version of the same dataset, or another dataset that is similar in nature)? Forthewarnews,wewillmanuallysearchanddownload30newsarticlesfromBBC orCNNforeachofthetwowars,publishedinoneyearspan(May2003-April2004 forIraqwar,Nov2001toOct2002forAfghanistanwar).Thiswouldbeaveryclose proximation of the war news dataset that is used in the paper. Forthelaptopreviews,wechoose3laptopsfromtheAmazon.combestsellersin laptopcomputers:AcerAspire5SlimLaptop,AppleMacBookAir,HPChromebook 14-inchHDLaptop,andwillmanuallydownloadthetop40reviewsfrom Amazon.com.Thiswouldbecomparabletothelaptopreviewsdatasetthatisusedin the paper. 6.Ifyouanswer""no""toQuestions4&5,howareyougoingtodemonstratethat you have successfully reproduced the method introduced in the paper? (Not applicable) CourseProject Project Topic Reproduce A Paper: A cross-collection mixture model for comparative text mining Tutorial Link https://www.youtube.com/watch?v=QebNvPrvisk Documentation documentation Team members Xi Luo (xiluo4) Yuheng Zhang (yuhengz2)"
https://github.com/machilusZ/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mackteng/CourseProject	CS410 Project Report Cross-collection Mixture Model for Comparative Text Mining Function Overview: This software implements the method described in the paper  A Cross-Collection Mixture Model for Comparative Text Mining .  The paper proposes a method for discovering common themes across all the collections and for each theme, discover what is unique to a particular theme for each collection. In other words, for k topics among c collections, k common themes are discovered along with kc special themes for each collection-theme pair. A theme is modeled as probability distribution over words. Implementation: The skeleton of the code is similar to that of MP3. Models to be learned are randomly initialized, hidden latent variables are declared and the EM algorithm is implemented according to the formula listed below (copied from paper): One of the issues we ran into when implementing were underflow errors due to very miniscule probabilities. Our workaround was to pad these very small probabilities to a predefined number (we used 1e-600). Usage: To run the code: python3 cross.py {collectionName} where collectionName is the name of the folder under data/collections. Inside each folder under data/collections, there should be N files that make up the N collections, with each line in each of the N files representing a document. Our provided data includes one for wine (pinot noir and chardonnay) and covid-related articles by region (usa, asia, europe). Words are tokenized using the nltk tokenizer and any word < 3 characters are discarded to remove too much background noise in the theme models. Example output: Contribution: Mackt2: Implementation of cross-collection mixture model  - (EM algorithm) Documentation  (Progress Report) Hhc3: Debugging of code  - research using log-space and padding to avoid underflow errors Data scraping  - Sanitization and manipulation of various datasets found on kaggle (ad-hoc scripting for various formats, so not included in code) Presentation  (Video) Progress Report Captitan: Mack Teng (mackt2) Member: Hsin Hsien Chung (hhc3) Progress: *Finished parsing and filtering the data sets *Finished building the matrice *Implementing cross-collection mixture model EM algorithm Challenges: *Originally, we were planning to crawl data from the web, but it took longer than expected. We used existing data sets from Kaggle.com instead. *Some data are duplicated *For the mobile phone review data set, some of the comparison is not valid. Because some mobile phone models are outdated, but we do not have starting sales time or review time to distinguish them, we need to come up with a way to make a valid set for comparison. Therefore, we filtered the data with price range. Remaining Tasks: *Might need to compare with filtered data sets or unfiltered data sets *Add documentations *Create presentation Project Proposal Team Name: Mbouncy Team Members: Captain: Mack Teng (mackt2) Member: HsinHsien Chung (hhc3) Paper Chosen: A Cross-Collection Mixture Model for Comparative Text Mining (http://www.ifp.illinois.edu/~velivell/ctm4.pdf) Programming Language: Python Dataset: Not exactly the same as the original dataset, but we will crawl for/aggregate document collections in a similar fashion as described in the paper. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mariahuipe/CourseProject	"Final Project Proposal: Student Name: Maria Fernandez 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will work individually, my NetIDs is: myf2 2. Which paper have you chosen? Pattern annotation: 1. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 3. Which programming language do you plan to use? Java 4. Can you obtain the datasets used in the paper for evaluation? Yes, I found it here: https://dblp.uni-trier.de/xml/ 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? Project Progress Report: Student Name: Maria Fernandez My project is to reproduce the paper: Generating Semantic Annotations for Frequent Patterns with Context Analysis The progress I have made: 1. I have read through the paper a few times, making sure I understand the concepts that are used in the paper, like Pattern, Transactions, Context Unit, Closed Frequent Pattern, Semantically Similar Patterns, etc. I tried focusing in understanding the section that explains the One-pass Microclustering algorithm since this is the part that I will have to work from scratch. 2. I found the dataset that I will be using since the original location is not available in the location anymore, but this is the same data set, just a newer version of it. I made sure it has the data we need. 3. I looked into python libraries to parse XM. 4. I looked into python libraries for Frequent Patterns and for Close Patterns. It is allowed to use the libraries to pre-process the data set, and create the input to the microclustering algorithm. I still have to decide what I am using here. Remaining Tasks: I have to decide what libraries I am using for Frequent Pattern and Close Pattern and I need to start coding. Any Challenges: I am working alone because with work and family responsibilities it would be hard to collaborate with others, so it is challenging to take decisions by myself. So far I am not sure they libraries I found are the best ones to use, so I am planning to ask the question in Piazza and see if the other people doing the same paper has found the same libraries or if they have other suggestions. CourseProject Maria Fernandez - Final Project. PROJECT DEMO VIDEO LINK: https://www.amazon.com/photos/shared/_pfYcGAFShmihAYWeZtZHw.qkEKw7lYgK8usmZzd2AQfI Pre-requirements: You need Java 1.8, I specifically used java version ""1.8.0_271"" If it is not installed in your system, download from: https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html I include the compile versions of the two java programs I developed, but if needed to compile, please do the following: javac -cp mmdb-2019-04-29.jar ParseDblp.java javac DblpCalcDistance.java The result will be the ParseDblp.class & DblpCalcDistance.class files STEP #1 Clone Git repository: https://github.com/mariahuipe/CourseProject/ STEP #2: OPTION A: Run the Data Pre-processing of the source data set. 1. Download data set a. Go here: https://dblp.org/xml/release/ b. Download to the file called: dblp-2020-11-01.xml.gz . Download to same directory where you clone my git repository. c. If in Unix/OS: unzip the gz file: gunzip dblp-2020-11-01.xml.gz This will create the dblp-2020-11-01.xml which is the input of the Pre-processing process. d. Make sure you have the dblp.dtd which was downloaded from my Git repository 2. Run the Pre-Processing process: java -Xmx8G -cp mmdb-2019-04-29.jar:. ParseDblp > inputfp_100K.txt This step will produced the following files: - authors.txt - inputfp_100K.txt - titles.txt NOTE: The xml file is pretty big. I was not able to process this file from my Windows laptop due to lack of memory but I was able to do it from my Mac, it still takes a few minutes. This is why I am putting this step as an optional step and I am including option B which is skipping this step and take the files that have been produced by the pre-processing. OPTION B: Copy pre-processed files from the back up directory: 1. Do the following: cp OUT_FILES_BK/authors.txt . cp OUT_FILES_BK/inputfp_100K.txt . cp OUT_FILES_BK/titles.txt . STEP #3: Run the SPMF library to create Closed Frequent Patterns from the inputfp_100K.txt file: 1. Run the following: java -jar spmf.jar run FPClose inputfp_100K.txt outputfp.txt 0% Results: you should see the file outputfp.txt More information about this library: https://www.philippe-fournier-viger.com/spmf/ STEP #4: Run the One-Pass Micrcocluster algorithm based in the paper description: 1. Do the following: java DblpCalcDistance Results: pattern_annotations.txt will be created."
https://github.com/mattzeeee/TextInformationSystemsCourseProject	"CS 410: Text Information SystemsFinal Project - Final Report Project Type: Classification Competition Team: The Classifiers November 27, 2020Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu)References and web-sites used in this code extensively:https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db https://simpletransformers.ai/docs/usage/Key Libraries Used in this Project to Date:As part of our project certain libraries are being used extensively.For the core traditional ML models as well as feature vector transformations:SKLearn Library For the implementation of the RoBERTa pre-trained neural network architecture, the following libaries are used:HuggingFace Transformer Library SimpleTransformer.ai LibraryDescription of Work In This Notebook:This code notebook explains the next steps we followed post the completion of the project progress report notebook and should be seen (or read) asa follow on to that piece of work. Our total project documentation is covered by both notebooks as well additional descriptions of further testing weperformed/optimisation as part of the project.This notebook focuses on the final implementation of the chosen RoBERTa transformer model implementation designated by the project team forinvestigation and use in the classification competition and leaderboard. Optimisation and experimentation is described in another document, howeverthis code focuses on the following:1. The creation and learning of an existing RoBERTa model widely used and available in the simpletransformers.ai library2. Some feature engineering in the form of stemming, stop word removal and general cleaning of text input data3. The inclusion of Context in the model to enhance the model performanceAdditional commentary and description can be found in the document below code snippets as to what the project team found to be working or notworking.Import key python libraries for this projectThe below cell installs all necessary key libraries used in this notebook first should they not already be installed:The below cell imports all necessary libraries used in this notebook :!pip install jsonlines !pip install pandas !pip install transformers !pip install nltk !pip install torch import os import jsonlines import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import precision_recall_fscore_support from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix import simpletransformers import torch #libraries for RoBERTa Importing of training and test dataThe following code reads both the train and test JSON files and imports the data into a python dictionary format:Count of training data entries: 5000 Count of test data entries: 1800 The following code converts the training and test data dictionaries into a Pandas DataFrame format for use later in the RoBERTa modelTraining and Test Datasets converted to Pandas DataFrames... from simpletransformers.classification import ClassificationModel,ClassificationArgs from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score test_file = 'data/test.jsonl' train_file = 'data/train.jsonl' data_train = [] iter = 1 with jsonlines.open(train_file) as f: for line in f.iter(): iter +=1 data_train.append(line) data_test = [] iter = 1 with jsonlines.open(test_file) as f: for line in f.iter(): iter +=1 data_test.append(line) print(""Count of training data entries:"") print(len(data_train)) print(""Count of test data entries:"") print(len(data_test)) train_data_pd = pd.DataFrame.from_dict(data_train) test_data_pd = pd.DataFrame.from_dict(data_test) print(""Training and Test Datasets converted to Pandas DataFrames..."") Adding Context to training and test datasets as well as basic NLP processingThis code will append the context data to the response/text data linearly so as to add context information to the various model input feature vectorsand capture the additional information therein.Additionally, some basic processing such is performed on the raw data such as stemming, removal of stop words[nltk_data] Downloading package stopwords to C:\Users\User- [nltk_data] PC\AppData\Roaming\nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to C:\Users\User- [nltk_data] PC\AppData\Roaming\nltk_data... [nltk_data] Package punkt is already up-to-date! The next code snippet performs the addition of context information which you should note ONLY includes the addition of the preceding CONTEXTsentence to the response and not the sentence before that as well (i.e. the model uses only one of the two additional CONTEXT sentences):import nltk nltk.download('stopwords') nltk.download('punkt') from nltk.corpus import stopwords from nltk.tokenize import word_tokenize, SpaceTokenizer from nltk.stem import PorterStemmer all_stopwords = stopwords.words('english') tk = SpaceTokenizer() ps = PorterStemmer() for i in range(len(train_data_pd)): train_data_pd['response'][i]=train_data_pd['response'][i]+train_data_pd['context'][i][1] train_data_pd['response'][i]=train_data_pd['response'][i].replace('@USER', '').strip().lower() text_tokens = tk.tokenize(train_data_pd['response'][i]) tokens_without_sw = [word for word in text_tokens if not word in all_stopwords] test4="""" for i in tokens_without_sw: test4 = test4 + "" ""+ps.stem(i) test4.strip() train_data_pd['response'][i]=test4 for i in range(len(test_data_pd)): test_data_pd['response'][i]=test_data_pd['response'][i]+test_data_pd['context'][i][1] test_data_pd['response'][i]=test_data_pd['response'][i].replace('@USER', '').strip().lower() Converted response PD data to include Context Data Converted response to lowercase and removed stop words as well as @USER Converted response to stem words using PortStemmer Perform a random test print of the constructed response+context after NLP pre-processing to see if all is ok:define this way : 1 . desiring the good of the other ; wanting them to thrive / flourish , which means they'd get free from the attitudes you mention ; 2 . doing whatever's in your control / power to advance their good ; at least * not * wishing them ill , * not * hating them . ok , you ' re right , but how do you love someone who hates you , and wants you to not exist ? how do you love someone doesn ' t share basic morals ? Here we turn the labelled training data text format 'SARCASM' or 'NOT SARCASM' into a binary 1/0 list to for easier input into the models:Splitting of the project training dataset randomlyThe code below splits the training dataset into a random (new) training and test dataset (i.e. only using the course project training data to train andtest locally so far):NOTE THAT RANDOM_STATE is set to zero to provide a fixed seed to the random generator for consistent results text_tokens = tk.tokenize(test_data_pd['response'][i]) tokens_without_sw = [word for word in text_tokens if not word in all_stopwords] test4="""" for i in tokens_without_sw: test4 = test4 + "" ""+ps.stem(i) test4.strip() test_data_pd['response'][i]=test4 print(""Converted response PD data to include Context Data"") print(""Converted response to lowercase and removed stop words as well as @USER"") print(""Converted response to stem words using PortStemmer"") print(test_data_pd['response'][10]) #Define the vector of actual results: Actual_Results = [] for l in data_train: if l['label'] == 'SARCASM': Actual_Results.append(1) else: Actual_Results.append(0) Training dataset split into this many train samples: 4750 Training dataset split into this many test/validation samples: 250 Final Model : RoBERTaThe below code implements the SimpleTransformers.ai implementation of RoBERTa and calculates a number of metrics including:1. Accuracy on both train and test sets2. The precision, recall and F1 score on the training data3. The precision, recall and F1 score on the test data#getting training dataset features and labels features = train_data_pd['response'] labels = train_data_pd['label'] labels = Actual_Results # Splitting of training data into train and test datarawdata_train, rawdata_test, rawlabels_train, rawlabels_test = train_test_split(features, labels, test_size = .05, random_state = print(""Training dataset split into this many train samples:"") print(len(rawdata_train)) print(""Training dataset split into this many test/validation samples:"") print(len(rawdata_test)) #Import the simpletransformers model library from simpletransformers.classification import ClassificationModel,ClassificationArgs #Create a training dataset in a Panda DataFrame format ready for the model train_df = pd.DataFrame({ 'text': rawdata_train.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_train }) #Create a test dataset in a Panda DataFrame format ready for the model test_df = pd.DataFrame({ 'text': rawdata_test.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_test }) Set up RoBERTa transformer modelThe below code establishes and imports the RoBERTa transformer model. There are two possible options here, one including cuda support (if youhave this available) and one excluding cuda support. The difference is primarily the speed of model training/estimation. the code has been set up touse or not use Cuda depending on the machine where the code is being executed (this is done through the torch.cuda.is_available() function whichreturns true if cuda is an option).Does system have CUDA support? True Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Perform model training (this may take some time if non CUDA support):# Create a ClassificationModel # for training from scratch import torch cuda_available = torch.cuda.is_available() print(""Does system have CUDA support?"") print(cuda_available) model = ClassificationModel('roberta', 'roberta-base', use_cuda=cuda_available) # You can set class weights by using the optional # for loading my pre-trained model #model = ClassificationModel('roberta', 'outputs/checkpoint-594-epoch-1', use_cuda=False) model_args = { ""reprocess_input_data"": True, ""overwrite_output_dir"": True, ""num_train_epochs"": 1, ""model_args.lazy_loading"" : True } C:\Users\User-PC\anaconda3_SP2\envs\SP 36\lib\site-packages\torch\optim\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) (594, 0.5382462045936672)Evaluate the model performance on the training dataset:Through printing of the below output of result_train, we can see relevant accuracy, precisionm, recall and F1 scores for the model based onpredictions on the training dataset. A resulting f1 score of 0.84 is observed indicating a high degree of accuracy on the training dataset for the modelpriediction.{'mcc': 0.6909803679892279, 'tp': 2196, 'tn': 1798, 'fp': 573, 'fn': 183, 'f1': 0.8531468531468531, 'acc': 0.840842105263158, 'prec': 0.7930660888407367, 'rec': 0.9230769230769231, 'eval_loss': 0.3543026091762966}Next we need to evaluate the model performance on the test dataset (the test set randomly sampled from the trainign data...not the course test set# Train the model model.train_model(train_df,args=model_args) # Evaluate the model - training result_train, model_outputs_train, wrong_predictions_train = model.eval_model(train_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) #Print training result metrics result_train for the leaderboard):From printing the below result_test metrics, we can see that the model appears to perform relatively well with an F1 score 0f 0.81...slightly lower thantraining set but still relatively good. This is a good result and indicative of a model that hopefully was not overfit to the training data{'mcc': 0.602253199474844, 'tp': 106, 'tn': 93, 'fp': 36, 'fn': 15, 'f1': 0.8060836501901141, 'acc': 0.796, 'prec': 0.7464788732394366, 'rec': 0.8760330578512396, 'eval_loss': 0.4268150741700083}Performing predictions on project test data for LiveDataLabPerform predictions on the project test data for the competition for upload to LiveDataLab for gradingPerform RoBERTa predictions for project test set# Evaluate the model - testing result_test, model_outputs_test, wrong_predictions_test = model.eval_model(test_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) #Print out the model performance metrics on the test set result_test # getting training dataset features and labels features_test = test_data_pd['response'] predictions_test, raw_outputs_test = model.predict(features_test) Write the predictions from the RoBERTa model to an output text file called 'RoBERTa_answers.txt' for storage and uploading the the coursecompetition leaderboard:By taking the resulting output file generated above and submission to the project LiveDataLab scoreboard, a resulting model score of F1 = 0.737 wasachieved (beating the baseline score of 0.723).A sample screenshot of the model is included below and can be seen under the username PIQUITO2 at rank 28#Writing the RoBERTa Classifier predictions to the output file: RoBERTa_answers.txt y_pred = predictions_test f = open(""RoBERTa_answers.txt"", ""w"") id_final_test = test_data_pd['id'] for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() CS 410: Text Information Systems Final Project - Progress Report Project Type: Classification Competition Team: The Classifiers November 27, 2020 Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) References and web-sites used in this code extensively: https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db https://simpletransformers.ai/docs/usage/ Key libraries used in this project to date: As part of our project certain libraries are being used extensively. For the core traditional ML models as well as feature vector transformations: SKLearn Library For the implementation of the RoBERTa pre-trained neural network architecture, the following libaries are used: HuggingFace Transformer Library SimpleTransformer.ai Library Description of work to date: To start our project off, we first performed research into the various machine learning model types that existing literature appeared to favour when it came to binary classification tasks such as SARCASM or NOT SARCASM detection. Post this review, we decided to focus our investigation to a limited set of models which included a range of descriminitive, generative as well as neural network based options in order to cover a spectrum of the different types. For this, we implemented four models using the SKLearn library being Linear Support Vector Machine, Gussian Naive Bayes, Logistic Regression and Random Forest models. As inputs/feature vectors to these models, we utilised TFIDF non-linear transformation on the training dataset from which our vocabulary was derived. Some cleaning of the source text data was performed, however this was not the focus of our project at the progress report stage (this will likely be investigated under the next stops of the project to improve model performance). To compliment the traditional ML models we used from SKLearn, we investigated and also implemented a first attempt at a BERT/RoBERTa model using transfer learning. It should also be noted that, for the above models, we have only used the target tweet text as a model input. We have purposely left the inclusion of the context data towards the later part of our project in order to compare and contrast model performance at this interim step. The intention is to use the performance metrics observed in our results below as a base line against which further feature engineering, model changes and the incorporation of context data will be measured. In the code below we develop the baseline ML models out of the SKLearn library to test and compare to the target BERT model that will be fully investigated as part of the project. Models considered are: 1. Learn SVM 2. Naive Bayes 3. Logistic Regression 4. Random Forest 5. RoBERTa Progress dade - results so far As we show in the charts below, RoBERTa, without context, pre-processing, or hypterparameter tuning, considerably outperforms the base line models. Please note that the model performance measures shown in the graphs below are based currently on a split of the provided training dataset only and are not reflective of the project test set results (which may certainly differ). When measuring accuracy of predictions, the clear preferred model of the five implemented shows that RoBERTa appears to take clearly take the lead relative to the other discriminative/generative models. Certain literature sources research suggest that this may be due to the model's pre-learning of general language and syntactical structure on a large data source and stored in the neural network model structure. This however is still an area of research with no definitive answers as yet. Out of the remaining models, Linear SVM appears to be the preferred traditional model relative to its peer models in the SKLearn library. This again is consistent with the observations found in our research on the topic (i.e. LSVM appears to be the strongest discriminative model). In order to more accurately measure model performance, the precision, recall and F1 scores of the respective models are calculated and shown on the second graph. The results yield a fairly similar estimation of model performance to that of accuracy above with RoBERTa the clear winner out of the model set. Next steps Given that RoBERTa, the large pre-trained language model with transformer deep neural network architecture, is the most promising direction we will use the simpletransformers framework and try to improve the models performance over the following dimensions. 1. Hyperparameter optimization 2. Data preparation 3. Adding context 4. RoBERTa model selection Focus will be given to the individual improvements along the above dimensions and then the possible combinations thereof in order to achieve the target base line F1 score for the competition. No further work will be performed on the other models which will be used as baselines for the final project submission Expected challenges/issues To date, all the models above have been tested in LiveDataLab on the project test set and have yielded a variety of results with no F1 score above the base line score 0.723 as yet. This contrasts of course to the F1 scores (notably for the RoBERTa model in particular) which appear to be higher on the training dataset split. Overfitting and a lack of good generationalisation appears to be a concern currently and part of our further investigation will be to potentially address this. Since we have chosen to focus on the RoBERTa model (and its possible variants), computational power and time to sufficiently train a neural network of this size and complexity is expected to present a challenge as we progress further. Various options such as the use of Google Colab and other cloud based solutions may need to be considered. Detailed Code Appendix Import key python libraries for this project The below cell imports all necessary key libraries used in this notebook first: In [1]: import os #import json import jsonlines import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import precision_recall_fscore_support #libraries for RoBERTa from simpletransformers.classification import ClassificationModel,ClassificationArgs from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score Importing of training and test data The following code reads both the train and test JSON files and imports the data into a python dictionary format: In [6]: test_file = 'data/test.jsonl' train_file = 'data/train.jsonl' data_train = [] iter = 1 with jsonlines.open(train_file) as f: for line in f.iter(): #data = json.load(line) #print(line) # or whatever else you'd like to do #print('processing training line: ' + str(iter)) iter +=1 data_train.append(line) #data = json.loads(line) #print(data) data_test = [] iter = 1 with jsonlines.open(test_file) as f: for line in f.iter(): #data = json.load(line) #print(line) # or whatever else you'd like to do #print('processing test line: ' + str(iter)) iter +=1 data_test.append(line) #data = json.loads(line) #print(data) print(""Count of training data entries:"") print(len(data_train)) print(""Count of test data entries:"") print(len(data_test)) Count of training data entries: 5000 Count of test data entries: 1800 The following code converts the training and test data dictionaries into a Pandas DataFrame format for use later in the SKLearn ML models In [8]: train_data_pd = pd.DataFrame.from_dict(data_train) test_data_pd = pd.DataFrame.from_dict(data_test) print(""Training and Test Datasets converted to Pandas DataFrames..."") Training and Test Datasets converted to Pandas DataFrames... In this piece of code, I manually created a vocabulary list based on all words I could find in both the test and training data. This isn't really used later in the SKLearn models but thought it handy if we extended the models somehow or customised the vocabulary In [9]: ##Create a vocabulary and transform JSON data in word vectors vocabulary = [] vocabulary_size = 0 def build_vocabulary(data,vocab_size): for l in data: for w in (l['response'].strip().split("" "")): if w not in vocabulary: #print(w) vocabulary.append(w) vocab_size = vocab_size + 1 return(vocab_size) vocabulary_size = build_vocabulary(data_train,vocabulary_size) print('Vocabulary size after loading training data only:') print(vocabulary_size) print(len(vocabulary)) vocabulary_size = build_vocabulary(data_test,vocabulary_size) print('Vocabulary size after loading training and test data:') print(vocabulary_size) print(len(vocabulary)) Vocabulary size after loading training data only: 16707 16707 Vocabulary size after loading training and test data: 20446 20446 In this piece of code we built a term document matrix (as per the MP3 assignment) to represent the raw Bag of Words vector format of the training and test data. In [9]: ##Construct the word/term vector matrix def build_term_doc_matrix(data,vocab_size): """""" Construct the term-document matrix where each row represents a document, and each column represents a vocabulary term. self.term_doc_matrix[i][j] is the count of term j in document i """""" term_doc_matrix = np.zeros([len(data), vocab_size]) j_count = 0 for j in vocabulary: i_count = 0 for i in data: w_count = i['response'].count(j) term_doc_matrix[i_count,j_count]=w_count i_count += 1 j_count += 1 return(term_doc_matrix) train_vector = build_term_doc_matrix(data_train,vocabulary_size) print(""Length of training vector TDM matrix:"") print(len(train_vector)) print(""# of entries of training vector TDM matrix:"") print(train_vector.size) test_vector = build_term_doc_matrix(data_test,vocabulary_size) print(""Length of test vector TDM matrix:"") print(len(test_vector)) print(""# of entries of test vector TDM matrix:"") print(test_vector.size) Length of training vector TDM matrix: 5000 # of entries of training vector TDM matrix: 102230000 Length of test vector TDM matrix: 1800 # of entries of test vector TDM matrix: 36802800 Here we turn the labelled training data text format 'SARCASM' or 'NOT SARCASM' into a binary 1/0 list to for easier input into the models: In [12]: #Define the vector of actual results: Actual_Results = [] for l in data_train: if l['label'] == 'SARCASM': Actual_Results.append(1) else: Actual_Results.append(0) This code imports all the SKLearn model libraries required for the testing of the chosen ML models In [10]: ## Import the various SKLearn ML models for testing: from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.svm import LinearSVC from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier Creation of TFIDF Word Vectors for training dataset In this code, I take the training data and split it onto a features and labels seperate vectors. I then use the the TfidVectorizer() function from SKLearn to construct a TFIDF word vector representation of the training dataset. Note that I've set the max features (unique words) to 5000 here in the function while we can see our vocabulary is closer to 20,000 features. This might be worth playing with a bit later for incremental accuracy. I use the 'features2' vector (in an array format) for the models input. In [13]: # getting training dataset features and labels features = train_data_pd['response'] labels = train_data_pd['label'] labels = Actual_Results # getting final test dataset features as well as features_final_test = test_data_pd['response'] id_final_test = test_data_pd['id'] ## Stemming our data #ps = PorterStemmer() #features = features.apply(lambda x: x.split()) #features = features.apply(lambda x : ' '.join([ps.stem(word) for word in x])) # vectorizing the data #from sklearn.feature_extraction.text import TfidfVectorizer tv = TfidfVectorizer(max_features = 5000) #tv = TfidfVectorizer() #tv = TfidfVectorizer(max_features = 5000, vocabulary = vocabulary) features2 = list(features) features2 = tv.fit_transform(features2).toarray() print(""TFIDF Training dataset features vector succesfully created..."") #Turn the test dataset into the vectorized format as fitted to 'tv' features_final_test2 = list(features_final_test) features_final_test2 = tv.transform(features_final_test).toarray() print(""TFIDF Test dataset features vector succesfully created..."") TFIDF Training dataset features vector succesfully created... TFIDF Test dataset features vector succesfully created... Splitting of training dataset randomly The code below splits the features2 and labels datasets into a random training and test dataset (i.e. only usinfg the course project traiing data to train and test locally so far): NOTE THAT RANDOM_STATE is set to zero to provide a fixed seed to the random generator for consistent results In [16]: # Splitting of training data into train and test data features_train, features_test, labels_train, labels_test = train_test_split(features2, labels, test_size = .05, random_state = 0) rawdata_train, rawdata_test, rawlabels_train, rawlabels_test = train_test_split(features, labels, test_size = .05, random_state = 0) print(""Training dataset split into this many train samples:"") print(len(features_train)) print(len(rawdata_train)) print(""Training dataset split into this many test/validation samples:"") print(len(features_test)) print(""# of features:"") print(len(features_train[1,:])) print(""Final test dataset is this many samples:"") print(len(features_final_test2)) print(""# of features:"") print(len(features_final_test2[1,:])) Training dataset split into this many train samples: 4750 4750 Training dataset split into this many test/validation samples: 250 # of features: 5000 Final test dataset is this many samples: 1800 # of features: 5000 Model 1: Linear SVM The below code implements the SKLearn implementation of LSVM and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [34]: # Using linear support vector classifier lsvc = LinearSVC() # training the model lsvc.fit(features_train, labels_train) # getting the score of train and test data print(lsvc.score(features_train, labels_train)) # 96.04 print(lsvc.score(features_test, labels_test)) # 74.00 LSVM_train_acc= lsvc.score(features_train, labels_train) LSVM_test_acc= lsvc.score(features_test, labels_test) y_pred = lsvc.predict(features_train) y_true = labels_train LSVM_out_train = precision_recall_fscore_support(y_true, y_pred, average='binary') print(LSVM_out_train) #96.06 y_pred = lsvc.predict(features_test) y_true = labels_test LSVM_out_test = precision_recall_fscore_support(y_true, y_pred, average='binary') print(LSVM_out_test) #72.80 0.960421052631579 0.74 (0.9581764951902969, 0.9630096679277007, 0.960587002096436, None) (0.7372881355932204, 0.71900826446281, 0.7280334728033473, None) Model 2: Guassian Naive Bayes The below code implements the SKLearn implementation of Guassian Naive Bayes and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [35]: # model 2: # Using Gaussian Naive Bayes gnb = GaussianNB() # training the model gnb.fit(features_train, labels_train) # getting the score of train and test data print(gnb.score(features_train, labels_train)) # 82.96 print(gnb.score(features_test, labels_test)) # 67.6 GNB_train_acc= gnb.score(features_train, labels_train) GNB_test_acc= gnb.score(features_test, labels_test) y_pred = gnb.predict(features_train) y_true = labels_train GNB_out_train = precision_recall_fscore_support(y_true, y_pred, average='binary') print(GNB_out_train) # 85.47 y_pred = gnb.predict(features_test) y_true = labels_test GNB_out_test = precision_recall_fscore_support(y_true, y_pred, average='binary') print(GNB_out_test) #69.43 0.8296842105263158 0.676 (0.7462358845671268, 1.0, 0.8546793605173344, None) (0.6388888888888888, 0.7603305785123967, 0.6943396226415093, None) Model 2: Logistic Regression The below code implements the SKLearn implementation of Logistic Regression and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [36]: # model 3: # Using Logistic Regression lr = LogisticRegression() # training the model lr.fit(features_train, labels_train) # getting the score of train and test data print(lr.score(features_train, labels_train)) # 85.85 print(lr.score(features_test, labels_test)) # 72.00 LR_train_acc= lr.score(features_train, labels_train) LR_test_acc= lr.score(features_test, labels_test) y_pred = lr.predict(features_train) y_true = labels_train LR_out_train = precision_recall_fscore_support(y_true, y_pred, average='binary') print(LR_out_train) # 86.15 y_pred = lr.predict(features_test) y_true = labels_test LR_out_test = precision_recall_fscore_support(y_true, y_pred, average='binary') print(LR_out_test) # 71.07 C:\Users\User-PC\Anaconda3\lib\site-packages\sklearn\linear_model\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) 0.8585263157894737 0.72 (0.8448484848484848, 0.8789407313997478, 0.8615574783683561, None) (0.7107438016528925, 0.7107438016528925, 0.7107438016528925, None) Model 4: Random Forest Classifier The below code implements the SKLearn implementation of Random Forest Classifier and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [37]: # model 4: # Random Forest Classifier rfc = RandomForestClassifier(n_estimators = 10, random_state = 0) # training the model rfc.fit(features_train, labels_train) # getting the score of train and test data print(rfc.score(features_train, labels_train)) # 99.09 print(rfc.score(features_test, labels_test)) # 66.8 RFC_train_acc= rfc.score(features_train, labels_train) RFC_test_acc= rfc.score(features_test, labels_test) y_pred = rfc.predict(features_train) y_true = labels_train RFC_out_train = precision_recall_fscore_support(y_true, y_pred, average='binary') print(RFC_out_train) # 99.09 y_pred = rfc.predict(features_test) y_true = labels_test RFC_out_test = precision_recall_fscore_support(y_true, y_pred, average='binary') print(RFC_out_test) # 63.11 0.9909473684210526 0.668 (0.9944961896697714, 0.987389659520807, 0.9909301835055896, None) (0.6826923076923077, 0.5867768595041323, 0.6311111111111112, None) Model 5: RoBERTa The below code implements the SimpleTransformers.ai implementation of RoBERTa and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [18]: from simpletransformers.classification import ClassificationModel,ClassificationArgs In [19]: train_df = pd.DataFrame({ 'text': rawdata_train.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_train }) #rawdata_train, rawdata_test, rawlabels_train, rawlabels_test In [27]: test_df = pd.DataFrame({ 'text': rawdata_test.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_test }) In [22]: # Create a ClassificationModel # for training from scratch model = ClassificationModel('roberta', 'roberta-base', use_cuda=False) # You can set class weights by using the optional weight argument # for loading my pre-trained model #model = ClassificationModel('roberta', 'outputs/checkpoint-594-epoch-1', use_cuda=False) Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. In [23]: #model_args = ClassificationArgs() #model_args.reprocess_input_data = True #model_args.overwrite_output_dir = True model_args = { ""reprocess_input_data"": True, ""overwrite_output_dir"": True } # Train the model model.train_model(train_df,args=model_args) C:\Users\User-PC\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) Out[23]: (594, 0.539390764095717) In [24]: # Evaluate the model - training result_train, model_outputs_train, wrong_predictions_train = model.eval_model(train_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) In [26]: result_train Out[26]: {'mcc': 0.7212081013757768, 'tp': 2116, 'tn': 1969, 'fp': 402, 'fn': 263, 'f1': 0.8642025730038799, 'acc': 0.86, 'prec': 0.840349483717236, 'rec': 0.8894493484657419, 'eval_loss': 0.325839846517251} In [40]: roberta_train_result=result_train roberta_train_result['acc'] Out[40]: 0.8397894736842105 In [28]: # Evaluate the model - training result_test, model_outputs_test, wrong_predictions_test = model.eval_model(test_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) In [29]: roberta_test_result=result_test roberta_test_result['acc'] result_test Out[29]: {'mcc': 0.6238264579949548, 'tp': 106, 'tn': 96, 'fp': 33, 'fn': 15, 'f1': 0.8153846153846155, 'acc': 0.808, 'prec': 0.762589928057554, 'rec': 0.8760330578512396, 'eval_loss': 0.4293329161591828} Performing predictions on project test data for LiveDataLab Perform predictions on the project test data for the competition for upload to LiveDataLab for grading In [38]: #Writing the LSVM predictions to the output file: LSVM_answers.txt y_pred = lsvc.predict(features_final_test2) f = open(""LSVM_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Guassian NB predictions to the output file: GNB_answers.txt y_pred = gnb.predict(features_final_test2) f = open(""GNB_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Linear Regression predictions to the output file: LR_answers.txt y_pred = lr.predict(features_final_test2) f = open(""LR_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Random Forest Classifier predictions to the output file: RFC_answers.txt y_pred = rfc.predict(features_final_test2) f = open(""RFC_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() Comparison of baseline models In [41]: import matplotlib.pyplot as plt; plt.rcdefaults() #import numpy as np import matplotlib.pyplot as plt #Plot accuracy on training set for each model objects = ('LSVM', 'Gussian NB', 'Linear Regression', 'Random Forest','RoBERTa') y_pos = np.arange(len(objects)) performance = [LSVM_train_acc,GNB_train_acc,LR_train_acc,RFC_train_acc,roberta_train_result['acc']] plt.bar(y_pos, performance, align='center', alpha=0.5) plt.xticks(y_pos, objects) plt.ylabel('% accuracy') plt.title('% accuracy on training set') plt.show() #Plot accuracy on test set for each model objects = ('LSVM', 'Gussian NB', 'Linear Regression', 'Random Forest','RoBERTa') y_pos = np.arange(len(objects)) performance = [LSVM_test_acc,GNB_test_acc,LR_test_acc,RFC_test_acc,roberta_test_result['acc']] plt.bar(y_pos, performance, align='center', alpha=0.5) plt.xticks(y_pos, objects) plt.ylabel('% accuracy') plt.title('% accuracy on test set') plt.savefig('acc_test.png') plt.show() # Plot the precision, recall and F1 score on the test n_groups = 5 Prec_data = (LSVM_out_test[0],GNB_out_test[0],LR_out_test[0],RFC_out_test[0],roberta_test_result['prec']) Rec_data = (LSVM_out_test[1],GNB_out_test[1],LR_out_test[1],RFC_out_test[1],roberta_test_result['rec']) F1_data = (LSVM_out_test[2],GNB_out_test[2],LR_out_test[2],RFC_out_test[2],roberta_test_result['f1']) #GNB_data = (GNB_out_test(1:3)) #LR_data = (LR_out_test(1:3)) #RFC_data = (RFC_out_test(1:3)) # create plot fig, ax = plt.subplots() index = np.arange(n_groups) bar_width = 0.25 opacity = 0.8 rects1 = plt.bar(index, Prec_data, bar_width, alpha=opacity, color='b', label='Precision') rects2 = plt.bar(index + bar_width, Rec_data, bar_width, alpha=opacity, color='g', label='Recall') rects3 = plt.bar(index + 2*bar_width, F1_data, bar_width, alpha=opacity, color='r', label='F1 Score') plt.xlabel('ML Model') plt.ylabel('Scores') plt.title('Precision, recall and F1 score per model') plt.xticks(index + bar_width, ('LSVM', 'GNB', 'LR', 'RFC','RoBERTa')) plt.legend() plt.tight_layout() plt.savefig('pr_rec_f1.png') plt.show() In [ ] CS 410: Text Information Systems Final Project - Proposal Team: The Classifiers October 22, 2020 Members Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) - captain Project Topic Option 4: Text Classification Competition Are you prepared to learn state-of-the-art neural network classifiers? All team members agree to put it in the necessary effort to learn applying state-of-the-art neural network classifiers to compete in completing the competition's task. Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods (in parentheses). Scipy (Experienced) Keras (Intermediate) TensorFlow (Intermediate) PyTorch (Intermediate) Metapy (Intermediate) Scikit-learn (Experienced) LSTM (Beginner) NLTK (Beginner) BERT (Beginner) XLNET (none) GPT2 (none) H20.GBM (none) H20.AutoML (none) Which programming language do you plan to use? Python CS 410 Text Information Systems Final Course Project Team: The Classifiers Members Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) - captain Project Topic Option 4: Text Classification Competition - Sarcasm Detection on Twitter data Project Setup and Individual Contributions The submitted work represents a well balanced team effort: Each of the members spend considerable time on researching potential models and frameworks. Praveen and Steven worked on and tested baseline models like SVM and random forest, Matthias focused on pre-trained models. When BERT type models turned out to be the most promising, we jointly concentrated on improving the performance of a fine tuned RoBERTa model. Praveen looked into improving accuracy via data preparation, Steven added context, Matthias experimented with hyperparameter tuning. What we do We are fine tuning RoBERTa. RoBERTa, a neural language model that extends BERT and was pre-trained on even more massive amounts of text data, e.g. Wikipedia and news articles with a dynamically chaning masking pattern for the missing word prediction task. We have experimented with basic NLP models such as SVM, Random Forest, Naive Bayes (see our progress report in Appendix I) We do employ some basic data cleaning like stop word removal, further experimentation into data preparation has not improved model performance. We simply concatenated all context to the twitter response that was available. We experimented with hyperparameter tuning on google colab (to exploit their GPU offerings) and found a learning rate of 2.741032760877178e-05 and number of epochs equal 4 for fine-tuning to allow us generating predictions that beat the baseline (see appendix II and III). Result We beat the baseline using the fine-tuned RoBERTa model (user mattblack): Running our Code and replicating results Screencast tutorial You can follow the screen cast presentation, available at * https://youtu.be/K9CPy_nm3vs Install instructions To install all required libraries we assume that users have a recent install of the Anaconda distribution with Python <=3.7 If you run into trouble feel free to contact use (preferably Matthias: mrumpf2@illinois.edu) Instructions for windows users (for other OS adapt accordingly) * copy / clone the project repository to your working directory e.g.: C:\UIUC\TextMining\FinalProjectSubmission * Open Anaconda Navigator and create a new python 3.7 environment * Install and open the Anaconda App ""cmd prompt"", check that the newly created environment is activated * go to https://pytorch.org/get-started/locally/ and get your install command e.g. conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch or without cpu conda install pytorch torchvision torchaudio cpuonly -c pytorch * run the install command in the cmd prompt * enter in cmd prompt: pip install simpletransformers jsonlines pandas transformers nltk torch * cd to your working directory e.g.: cd C:\UIUC\TextMining\FinalProjectSubmission * then start notebooks by entering in cmd prompt: jupyter notebook Main documents Project Report Team The Classifiers - Twitter Sarcasm Detection (with a fine-tuned RoBERTa model) We use RoBERTa as our workhorse model, see https://paperswithcode.com/method/roberta To replicate our results run the following notebook * Jupyter Notebook see 1. Project Report and Classifier Code Team The Classifiers - Twitter Sarcasm Detection.ipynb * PDF see 1. Project Report and Classifier Code Team The Classifiers - Twitter Sarcasm Detection.pdf Appendix I.The progress report - initial experimentation Jupyter Notebook see Appendix I.The progress report - initial experimentation.ipynb PDF see Appendix I.The progress report - initial experimentation.pdf Appendix II. Google Colab Hyperparameter tuning use colab at https://colab.research.google.com/ if you want to test the code, note that you need to upload the data folder to your google drive, replace folder paths, and connect to google drive from with the colab notebook * Jupyter Notebook see Appendix II. Hyperparameter tuning.ipynb * PDF see Appendix II. Hyperparameter tuning.pdf Appendix III. Google Colab Roberta Training and Sarcasm Prediction use this code if you don't have a GPU to speed up training the model and retrieving the prediction results use colab at https://colab.research.google.com/ if you want to test the code, note that you need to upload the data folder to your google drive, replace folder paths, and connect to google drive from with the colab notebook * Jupyter Notebook see Appendix III. Roberta Training and Sarcasm Prediction.ipynb * PDF see Appendix III. Roberta Training and Sarcasm Prediction.pdf"
https://github.com/mihiryerande/CS-410-Fall-2020-Anime-Text-Analytics	Anime Text Analytics This is a Course Project for UIUC CS 410: Text Information Systems (Fall 2020). To view the output of this project, please visit https://animetextanalytics.azurewebsites.net. A video presentation of the work done can be viewed here: https://tinyurl.com/animetextanalytics Team Produced by Team Nani (He  !?): * Karan Bokil (karanb2) @bokilenator * Mihir Yerande (yerande2) @mihiryerande Explanation Anime shows are categorized into various genres, such as Shonen or Mecha, for example. This project attempts to use the Latent Dirichlet Allocation (LDA) algorithm to determine such genres from text data. LDA works by training a model on input text data to obtain topics and topic coverages. We use text data, scraped and cleaned, from myanimelist.com, where there are short synopses of anime shows. The topics produced by our LDA model are referred to as LDA genres. Each genre is a probabilistic distribution over words, which would ideally reflect a genre understandable to humans. In addition, each anime show can be assigned a topic coverage (i.e. genre breakdown). For example, we might determine that a show is 71% Shonen and 29% Mecha. Website The output of the project has been published to a website, which can be found here: https://animetextanalytics.azurewebsites.net LDA Implementation This section steps through the implementation of the project from start to finish. The LDA code and output is all stored in the source_code directory. Scraper The raw text data is scraped from myanimelist.net, specifically from the list beginning here. The output has already been written to scraped.jl. The scraper is implemented in Python using the scrapy framework. See animespider.py. To run the scraper, navigate to the containing directory, and run the following command: scrapy runspider animespider.py -o scraped.jl The spider's log will automatically write to spider_log.txt, in the same directory. LDA Input In order to run LDA, the raw text must be tokenized and cleaned. The output of this step has already been written to lda_input.jl. See write_lda_input.py for the implementation. To run the text cleaning, navigate to the containing directory, and run the following command: python write_lda_input.py Print-out should appear in the console as each show's raw text is cleaned. LDA Model After cleaning the raw text, we can now train our LDA model. The trained LDA model has already been saved to lda_model. See write_lda_model.ipynb for further explanation. LDA Output After training the LDA model, we can obtain the desired output about genres. The output of this step has already been written to lda_output. See write_lda_output.ipynb for further explanation. LDA Distances After obtaining the genre-breakdowns, we can determine similarity between anime shows based on their respective breakdowns. We use the Hellinger distance utilities provided in gensim, as described here. The output of this step has already been written to lda_distance. See write_lda_distance.ipynb for further explanation. Database Due to the size of our dataset, we did not feel processing Just in Time from a website performance perspective would be good. Thus, we preprocessed most of the data from the aforementioned steps and converted into a relational database for easy access by the web framework. After attempting to utilize Azure CosmosDB as well as Azure SQL, we ended up choosing to go with a SQLite database because it is light, easy to iterate testing on, and can easily be included as part of the repo, being only 5MB and self contained. Our attempts at using CosmosDB and Azure SQL were hindered by slow upload times, as we had to parse from json and upload around 50,000 records, which would have taken several hours. The Azure Stack would have provided us some ecosystem advantages such as use of their BM25 ranking solution, Azure Cognitive Search, but nevertheless, we were able to find a different avenue for full text search. The database can be initialized simply by running python init.py from the root of the repo. This file will delete the former tables, create the new tables, parse the JSON and populate the Database alongside the relationships between the various tables. Text Retrieval Model After researching different libraries for text retrieval models, we settled on utilizing the MSearch library, as it has the most integrated support with our web framework Flask. MSearch serves as a wrapper around Whoosh, a pure Python search engine library that capitalizes on Okapi BM25 ranking function. Within app.py, we have marked fields in the various tables with __searchable__ and created a custom route and view to collect and see the results of a query. The search thus exceeds the utility of normal database queries, facilitating results across different tables with ranking. The inverted index is created during the data population phase in init.py. Web Development and Hosting The website is made using Flask, a lightweight web framework in Python. It utilizes a standard MVC architecture and communicates cleanly with the Database via SQLAlchemy. The site is hosted and deployed on Microsoft Azure using Azure Web Apps. The frontend Javascript and CSS components is all developed utilizing the Materialize framework. Routing and models are all present in app.py, see comments for details. The application can be run locally via Python 3.8 simply by running pip install -r requirements.txt and python app.py. Further documentation can be found at https://animetextanalytics.azurewebsites.net/documentation.
https://github.com/mikepigott/TextInformationSystemsCourseProject	"Project topic:  ExpertSearch: Extracting relevant information from faculty bios Group name: Remarkable Scientists Members: *Mike Pigott ( mpigott2@illinois.edu ), Team Lead *Shuopeng Zhou ( sz46@illinois.edu ) *Amitha Supragna Sandur ( asandur2@illinois.edu ) 1.What is the function of the tool? The function of the tool is to make faculty information more accessible to users. Our aim is to improve and expand the existing ExpertSearch system. 2.Who will benefit from such a tool? Users of the ExpertSearch system. 3.Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? The ExpertSearch system already leverages Named Entity Extraction, but can be inaccurate. For example, not all professors are listed when searching for ""Data Mining."" In addition, sometimes the existing Named Entity extraction tool retrieves the wrong name from the data set. Users will care about getting more accurate and complete information. 4.What existing resources can you use? *SpaCy allows for building a  custom Named Entity Extraction model . *DBPedia Spotlight  scrapes Wikipedia pages and collects structured content. *Wikipedia provides an introduction to  Named Entity Extraction  that we can leverage. 5.What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We expect to heavily leverage Named Entity Extraction for faculty names. The existing system uses regular expressions to find e-mail addresses; we may inspect the work it already does, or come up with our own. Finally, we will investigate using topic mining to extract topics from the bios. 6.How will you demonstrate the usefulness of your tool? By comparing it with the existing ExpertSearch system and showing all the improvements we make. 7.A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) By mid-Nov: *Train Models for Named Entity Extraction. *Run the existing code of ExpertSearch website. *Work on implementing new features for the system. *Submit the progress report. By first week of Dec: *Complete training the models for Topic Mining. *Complete tweaking and verifying the models against the new data sets. *Wire in the resulting model to the existing ExpertSearch system. *Work on software code submission along with documentation. Remarkable Scientists: ExpertSearch Project Status Report For this project, we worked on three different tasks: *Topic Mining of the ExpertSearch professor bios. *Improving Named Entity Recognition & Extraction of professor names from the bios. *Improving recognition and extraction of professor e-mail addresses from the bios. Below is the status report for each. Topic Mining of Professor Bios We were able to combine  spaCy ,  NLTK , and  Genism  to build an LDA topic model of the professor's bios, using 10 topics. We then used  pyLDAvis  to visualize the model, and word_cloud  to build a word cloud of the 25 highest-weighted terms in each topic. Finally, we wired the word cloud of the highest-ranked topic into the ExpertSearch search results. LDA Visualization The following was created using pyLDAvis, which is visualizing a 10-topic LDA model: Word Clouds The following word clouds show the top 25 words in each of the 10 LDA topics: Topic 1 Word Cloud Topic 2 Word Cloud Topic 3 Word Cloud Topic 4 Word Cloud Topic 5 Word Cloud Topic 6 Word Cloud Topic 7 Word Cloud Topic 8 Word Cloud Topic 9 Word Cloud Topic 10 Word Cloud ExpertSearch Search Results Finally, we were able to wire in the primary topic associated with the bio into the search results: Next Steps We intend to experiment with different topic counts in the LDA model to determine the best results. We are also interested in making the ExpertSearch system browsable by allowing the user to click on a topic word cloud and see the bios ranked in descending order of relevance to that topic. Challenges Metapy does not support the ability to return a list of documents based on its metadata fields, so it is unclear if we would be able to build this additional functionality in the time that we have. Named Entity Extraction of Professor Names We have conducted several experiments on NER(Named Entity Extraction) solutions in the market. The best open source solution would be Standford NER Tagger, which is also provided along with ExpertSearch project for version 2018. The provided tagger and compiled result are incomplete as it eliminates some name information during the tagging phase, part of missing information is critical to extract the main context from the text. We have done two improvements for solving this problem, firstly, we update NER tagger to the newest version which is 2020-11 online, and test on 3-class model and 4-class model separately. Secondly, we implemented a filtered mechanism from tagging results, filtering all names with more than one word, capturing all tokens tagged as 'Person' in a row, and building a two-layer tagging system(3-class model and 4-class model) to cross-validation. We use the 4-class model as the main model to capture 'PERSON' entities, and check each produced token whether it states in the 3-class model. Now the system we created is able to capture most names from context files, filtering names are irrelevant partially. For example, for the first compiled bio text, we are able to retrieve ' Tarek F. Abdelzaher Professor ' as full name rather than  'Tarek' , second one ' Sarita V. Adve ' as correct name rather than ' Sarita V. Adve Richard T. Cheng ' in the provided file Next Steps We will continue working on a 2-layer tagging system to cross validation results between two models. Any results show or partial show on both models state they are 'Person/Name' entities with high possibility. Additionally, we will continue on filtering results, with introducing a counting/score system. Any name tokens appearing in high frequency should have a high score indicating its the main context of text. Challenges The biggest challenge we are facing is lots of famous names and confusing information shown in the bio files. For example, 'Ann Arbor' can be a name or location. The tagging system is not able to distinguish between them. Also 'Kennedy' as a famous name, sometimes can occur multiple times in the bio page, the system even with the scoring system is not able to know 'Kennedy' is not the main context. We will introduce another non-standard model can tag 'locations' entity, then do another cross-validation between our model and the third party model result, it should lead us to a potential solution finding the main person of the context, and better filter out non-relevant names/location. Extraction of Professor E-Mail We have worked on improving the existing regex based extraction of email ids from faculty bios. The original code was extracting the usual format of email ids (user@illinois.edu) and also some false positives such as cs@uiuc. Our new code is able to extract many of the alternative email id formats, for example: "" yang.r.yang at yale.edu "" "" denisew (at) uw.edu "" "" moli96 at uw.edu "". It is also able to capture email ids that were originally missing from the first output and remove some false positives. Below is a comparison of the outputs from existing code shown on the left and output from new code shown on the right. As observed from the figure above, the new code is able to catch the ""user at illinois dot edu"" types of format email ids. We have covered cases for the following email formats: *user at illinois.edu *user at illinois dot edu *user ""at"" illinois ""dot"" edu  or  user ""at"" illinois.edu *user (at) illinois (dot) edu *We removed some erroneous outputs like website urls/sentences/repeated special characters Next Steps We plan to convert the email ids in ""user at illinois dot edu"" format into the normal ""user@illinois.edu"" format as one of our next steps. Although the new regex based code is a significant improvement over the existing code, we feel that this method is laborious. And even if we spend a lot more time trying to cover all the exhaustive number of cases, the gain in improvement for the tool as a whole wouldn't be much, as the number of cases are a lot. Hence we plan to look into machine learning based approaches to extract email ids. Challenges The challenge here is to convert the various different email formats into the usual email id format. Although we may consider keeping it as is if we discover more new email formats along the way and it becomes too heterogeneous, because we have also seen a few websites having email ids in this alternative format. CourseProject Final Presentation: https://uofi.box.com/s/7oaexhrgl33zcnzb034gln9fl97w5fo1"
https://github.com/mikhaidn/CourseProject	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I am the only contributor for this project, my netID isdmikha2 What system have you chosen? What function are you adding? How will the new function benefit the users? I have chosen to expand the EducationalWeb system. I've chosen to add the ability to compare university courses (or entire degree programs between schools) by extracting key terms from a universities graduation requirements and course syllabi.This allows students to understand whether a concept they're trying to learn is in fact a core/fringe part of their class (as compared to other courses) For example, unlike UIUC, Rose-Hulman's Multivariate Calculus course doesn't cover certain vector calculus concepts (Green's theorem, etc). They're dropped because of the shorter term (quarter system vs semester), in general this allows for more flexible curricula, majors that require vector calculus knowledge will still get it elsewhere. Regardless, there are a few questions that could be answeredL: what is the minimal set of key terms that should be covered in order for a course to be called ""multivariate calculus""? What other interesting patterns could come up when comparing course syllabi? What are the key terms that a B.S. Computer Scientist from UIUC should know? How will you demonstrate that the new function works as expected? I will demonstrate that the new functionality works by producing histograms to vizualize the word vectors of each syllabus. How will your code communicate with or utilize the system? Unsure at this time, this may just be a standalone package. Which programming language do you plan to use? python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 5 hours - get used to the old system 5 hours - generate sample data 5 hours - generate keywords for each syllabus and present them 5 hours - presentation and fine tuning CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/milan-saroj/CourseProject	"Documentation for Twitter Sarcasm Detection using BERT 1 A. Introduction: As a part of final project of CS 410: Text Information Systems of Fall 2020 from University of Illinois-Urbana Champaign, we formed a team of three members. We decided to go for the Classification competition (https://github.com/CS410Fall2020/ClassificationCompetition) where we have to create a model that will classify the given tweets as ""Sarcasm"" or ""Not Sarcasm"" and predict the class of 1800 tweets of test set. After preprocessing, we implemented different methods such as Naive bayes, LSTM (Long Short-term Memory), BiLSTM (Bidirectional LSTM), and BERT (Bidirectional Encoder Representations from Transformers). Among all, the encoding and the model using BERT gave us the F1 score that was good enough to beat the baseline as required for successful completion of the project. We ran this model over Google Colab and Jupyter Notebook. However, we got the best training time using AWS Sagemaker Notebook instance. The detailed implementation is provided in the presentation document. B. Overview of the Code: The goal of the code is to detect the sarcasm of tweets. The source code for this project performs following task in a sequential order: a. Import the training and the test set b. Text preprocessing c. BERT encoding d. Training the model with BERT layer (deep learning) e. Use trained model to make prediction on the test set with 1800 tweets (both tweet response and tweet context). The training set includes 5000 tweets with ""Response"" and ""Context"" along with the label as ""Sarcasm"" or ""Not Sarcasm"". It also makes a prediction on the imported twitter test set with output as a text file ""answer.txt"" that has columns Twitter Id and class label as ""Sarcasm"" and ""Not Sarcasm"". In a nutshell, this code trains on the twitter dataset, classifies whether new tweets are sarcasm or not and it achieves an F1 score over 0.74. This code can be used for almost any text classification task with some modifications. Even though it only reads JSON file format for now, it can read any kind of acceptable file formats such as pd.read_csv and etc. with simple changes. The code calls specific column names such as Response and Context and they need to be modified as per training set. Even though this code uses two different models, each for Response and Context, one can choose to use single column data with some modifications on the model structure. Using an activation function such as Softmax instead of Sigmoid can be used to perform a multi-classification instead of a binary classification. 2 One can modify max_length values as needed based on the text length. The lower the value is, the quicker the training process will be, but it is important that this length should be large enough to cover the size of the text (each row) for the optimal result. C. Source Code Implementation: How the source code is implemented? How source code is working together to give the prediction file? The source code can be split into groups based on the functionality as described below along with the screenshots of source code: 1. Installing required libraries and importing preinstalled libraries This source code requires some libraries other than that comes with the kernel. Here sentencepiece library later helps to receive any iterable object to feed training sentences. Wget helps to download tokenization.py file that will be later used during a tokenization. Tensorflow-hub and tensorflow-addons will be required to run some functions. See below to check the implementation and the output of installing the library. The popular ""pip install"" is used to download these libraries. 3 The picture below includes the list of other libraries that are imported in the notebook. Nltk.punkt and other tokenization tools are used for the purpose of dividing a string into substrings by splitting on the specified string. 2. Data Preprocessing: We now perform a data preprocessing. This code has several functions that will work together to preprocess the data. Once the json training and test files are read using Pandas library, the code performs a preprocessing. Some of the notable preprocessing this code performs are: a. Remove punctuation b. Remove @User c. Change abbreviations to normal meaningful strings d. Remove emojis e. Remove links and non-ASCII characters, if any f. Combine separate sentences of context of same tweet into single documents g. Change the class label string to numerical forms using labelencoder. Listed below are some preprocessing functions that we created: 4 The function preprocessing calls other helper functions such as remove_emoji, remove_punctuations, etc. and applies it to each row of the data using an efficient pandas library 5 function called ""apply"". This is an example of a vectorizing function that works on all rows at the same time and is efficient and faster. The class labels of the training dataset are in string forms as they are named ""Sarcasm"" and ""Not Sarcasm"". They are converted into numerical forms using LabelEncoder function for the training purpose and saved in as a series with a variable name ""train_label"" as shown below: We then pass the train and test data frames to this function to get the preprocessed test and train sets as shown below: 3. BERT embedding, Training and Fitting: After the preprocessing, we now move on to BERT embedding, training, and model fitting. In this step, we conducted the BERT embedding, training and model fitting. First, we downloaded bert_layer, which was a pre-trained neural network with the Transformer architecture. We chose l=24 as hidden layers. Then, we encoded texts to ids to generate the encoded tokens, masks and segments using the pre-trained bert layers. One thing to be noticed, we encoded response and context separately and combined them afterwards. Finally, we fit the Bert encoded matrices into a model with epochs size of three and batch size of six. We use 90% of the dataset as a training set and 10% as a validation set. As a result, we achieved the F1 score of 0.74 which is about 3% above the baseline. This is where the initially installed tensorflow_hub comes into play. This model has been pre-trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs 6 have been ""uncased"", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. The following function converts the token into an encoding that is later used as an input to a Bert layer (neural network). The following set of code tokenizes the sentences and embeds them using BERT. Here we chose max_len of 256. Choosing the right number was part of tuning the model since the smaller number will speed up the training process in expense of the performance of the model. This number of 256 works well for us and we got the best speed and performance with it. The following code creates a neural network model with the context and the response as inputs. However, we encoded the response and the context separately, and created separate neural network models and later concatenated them together right before they went to the output layer. 7 Lists test_generate and train_generate are the output of BERT encoding and they are required to feed into a bert layer of the neural network. We use the sigmoid function as we have a binary output. We set the learning rate to 1e-6 after a few optimizations runs and use Adam as an optimizer since it is one of the most popular optimizers in the industry nowadays. Adam moves faster at first and then slows down once it starts to get closer to the local/global minima while training. 4. Making prediction based on the trained model Once we create a model, we train it with our training set. We have used 90% of training set as the training data and 10% as the validation dataset. The values for the parameters Training set percentage, batch size and epoch were empirically determined by us to get the best f1 score and the shown values gave us the best result. Note that each epoch took about one and a half hours even after running with Sagemaker which has the larger RAM than the other options. Running them on a local computer would take almost a day to run 2-3 epoch and it is one of the reasons why we chose Sagemaker as a platform to train our model. Once the training is completed, we use our test set to make a prediction. Once the prediction is made, it is converted back to non-numerical form of class labels as ""Sarcasm"", ""Non-Sarcasm"" using LabelEncoder and inverse_transform. The prediction is later to be converted into Dataframe, and concatenated with the twitter ids from the test set so we can have dataframe with twitter ids and a prediction class. The dataframe is later saved as a text file ""answer.txt"" which is later uploaded on github. 8 Since github is already set with the webhook, once we commit and push, the result of our prediction will show on livelab. At the time of writing this documentation we got the F1 score of 0.742 and we were ranked at 30 in the leaderboard. D. How to run the Source Code? The code will install all the necessary libraries, import all required libraries, therefore there is no need for any additional installation to run this code provided that this code is ran under specific kernel of ""conda_amazonei_tensorflow2_p36"" which is available in Notebook instance of AWS Sagemaker. Here are the stepwise details on how to run the source code (notebook) on AWS Sagemaker. 1. Go to https://aws.amazon.com/ and create an account if you do not have one. 2. Once you are logged in, type SageMaker on the search tab. 3. Go on Notebook Instances as shown below. 9 4. Click on Create Notebook Instance 5. Type any name for Notebook Instance name and select Instance type. The free tier AWS version only allows certain maximum size. We use ml.c4.8xlarge to train our model which is available for a free tier account. 10 6. Once you create a notebook instance, it will take 1-2 minutes to be activated (inService). Once you see an inService sign, you can click ""Open Jupyter"" as below. It will open a notebook on your default browser. 7. Once your AWS notebook opens, upload the source code (notebook file), train.jsonl, and test.josnl file on the notebook. 11 8. Once your upload is finished, click on notebook (""Source code""). 9. Change the kernel to conda_amazonei_tensorflow2_p36 by selecting the kernel tab from the navigation menubar and select ""change kernel"" tab to choose conda_amazonei_tensorflow2_p36 kernel. 10. Once the notebook is open, go to ""Cell"" and click ""Run All"". 12 All the code cell will run and the prediction on test set will be exported as ""answer.txt"" after a few hours on the root directory that can be accessed by going File and click ""Open"". This txt file will have the prediction as per the project requirement. How did we get here? What are our experiments with other methods and hyperparameter tuning? Once we did the preprocessing, we used different machine learning algorithm to make predictions. While other ML algorithm was not giving us a good result, BiLSTM was giving us some encouraging result. Below are the screenshots of one of the models we used. This one gave us the F1 score of about 0.65 which was not good enough. After some suggestion from the discussion board, we came to realize that BERT with its attention layer will definitely help us to achieve the prediction on test set that could beat the baseline. All three of us started our own research and share ideas and references with each other. We came up with one model that started to look promising with the F1 score of 0.69. That is the time when we realized that all we need was a good hyperparameter tuning. Here are the lists of parameters that we tuned in order to get the result to beat the baseline. a. Epoch b. Learning batch size c. Learning rate d. Max Length to feed into Bert Layer (max_length) e. Test/Validation split percentage Since the training took us over 4 hours for a single epoch with 15 batch size, we needed to find a way to speed up the process. That is when we decided to go with Sagemaker where we could leverage larger RAM size (60 gb RAM with 36vCPU). After reducing the training time from 4 hours to 40 mins using Sagemaker, we were able to run the model with different 13 hyperparameters more easily. Our max_length at first was 150 but later on we changed it to 256 in order to get the better result. We were able to get closer to baseline after a few trials, but we still could not beat the baseline by a small margin. We then went back and started making changes on the preprocessing. Realizing the feature engineering was a very important aspect of the training, we made some changes such as converting abbreviated words to natural words and implementing the better handling of punctuations by keeping some punctuations such as (...). With these changes and a few more trials with the different hyperparameters, we managed to beat the baseline. How was our team effort? Who did what? After our initial meetings, we decided that we all should be working separately during the preprocessing. There were two main reason for this decision: a. It was necessary to come up with the preprocessed data since we cannot move forward without it. Therefore, coming up with a near-perfect preprocessing was important. b. Working independently would bring more creativity while preprocessing. Once we came up with our own set of the preprocessing, we had a meeting and come up with the best preprocessing code which included best parts of all three different preprocessing code. We then decided to work on different models and compare results. Saroj worked on BiLSTM and k_train library, Jiayi worked on BERT encoding and layer, while Kevin worked on Naive Bayes, and an additional feature engineering that could be possible to implement. While BiLSTM showed some good signs, BERT started to get closer to the baseline. At that point, we all started working on BERT and tried different hyperparameters to train the models. We have each tried over 15 different trainings with different hyperparameters and it took about three hours per training on average. Sometimes we ran two different models simultaneously leveraging AWS Sagemaker. Once we beat the baseline, Jiayi worked on cleaning and organizing the source code, Kevin worked on the presentation and Saroj worked on the first draft of the documentation. Later, everybody came together to finalize the source code, presentation, and demo video. It was a good team effort, and we all contributed almost the same amount of time. With the training time accounted, we worked over 35 hours each for this project. References: * https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1 * https://www.kaggle.com/rftexas/text-only-kfold-bert * https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270 * https://huggingface.co/transformers/model_doc/bert.html * https://www.kaggle.com/funxexcel/keras-bert-using-tfhub-trial Progress Report Three of us had a kick off meeting a few weeks ago, and decided to work separately on data cleaning and preprocessing. This decision was made because we won't be able to work on ML/DL implementation without preprocessing. We share our preprocessing code with each other, and make progressive changes on our individual works. Since there are many directions we can go with feature engineering such as whether to use just the response or use both response and context, keep the emojis or leave it alone, etc., we have kept all different versions of preprocessing and feature engineering codes to implement ML/DL models later on. Current progress We have put together a few different preprocessing codes, and have created ML models (like Multinomial Naive Bayes), LSTM, BiLSTM and BERT models. We have completed a whole cycle of the project such as importing the train, test file, preprocess and data cleaning, implementing DL models, and have created an ""answer.txt"" file as per the project requirement. We have individually submitted the code (pushed) through livelab, but failed to beat the baseline. Remaining Task Since we completed the whole cycle of project submission without beating the baseline, our task is to improve our models. We need to improve on feature engineering by trying a few different methods. Some of them we are thinking that could help are: 1. Leaving few punctuation marks such as ""!"" which could improve the model, 2. Run models without removing emojis which might help. 3. Need to figure out a perfect way to combine response and context. Such as do we use all the context or just the last one followed by the response, 4. Try pre-trained models like K-train 5. Fine tune the models (using different epoch, validation split allocation, etc) Challenges I.Although we achieved 0.694 of f1, we still need to improve the BERT model to beat the baseline (which is 0.723 of f1) or learn about K-train as well. II.To find a perfect way to use response and context is one of the challenges. III.Find the perfect feature engineering such as whether we need to remove all punctuation, emoji, stopwords, etc. Project Proposal: Text Classification Competition: Twitter Sarcasm Detection 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name NetID Saroj Khanal khanal2 Captain Kevin Choi gchoi17 Jiayi Chen jiayic15 2. Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Some of the neural classifiers are : ANN (Artificial Neural Network), RNN(Recurrent Neural Network), LSTM (Long Short Term Memory). ANN is a feed forward neural network where data pass forward from input to output. On the other hand,  RNN has feedback loops in the recurrent layer which enables maintaining information in 'memory' over time. However, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). To look into this issue further, LSTM networks are a type of RNN that uses special units in addition to standard units.LSTM units include a 'memory cell' that can maintain information in memory for long periods of time.There are three gates for LSTM: an input gate, an output gate and a forget gate. Gates are used to control when information enters the memory, when it's output, and when it's forgotten.This architecture lets them learn longer-term dependencies. LSTM assigned relatively more important weights on units with longer term to improve general accuracy. Two of our team members have experiences with Neural Networks and have worked on personal projects to some extent. However, for this project, our team needs to do some research, learn more about the above mentioned classifiers to properly implement to increase the accuracy of the model. However, before feeding the data into Neural Network, we will need to perform several text processing that we learn on the earlier part of the class such as tokenization, change words to lowercase, remove numerical data, remove stopwords, stemming, lemmatization, vectorization (Bag of Words), TF-IDF, use Word2Vec and etc. 4. Which programming language do you plan to use? Python (Tensorflow library, Keras API, Gensim, NLTK library, TextBlob, spaCy). CourseProject Guidelines for Reviewer Source code, train and test json files are here in the github. Please follow the link below for Demonstration video: https://mediaspace.illinois.edu/media/t/1_adl8xwo0"
https://github.com/mkhanal2/CourseProject	"CS 410: Text Information Systems Course Project Progress Report Text Classification Competition: Twitter Sarcasm Detection Submitted By: Mohan Khanal (mkhanal2) mkhanal2@illinois.edu 1) Which tasks have been completed? * Reading the tweets/response from the train and test file, cleaning the data and producing the proper training / testing vectors. * Applying deep neural network (LSTM) on the training set and using the trained model to get the prediction for the test set. * Achieved between 0.60 to 0.70 accuracy and f1 score on current progress, during multiple iteration. 2) Which tasks are pending? * Fine tune the model implemented to get higher accuracy and f1 score to beat the benchmark. * Document the approaches that was tried and the results that I got for the final report. * Create final report / presentation to be submitted at the end of the project. 3) Are you facing any challenges * Not a big issue, but training sometime takes longer time in my laptop so iterating through different ideas takes little bit longer time, especially if the parameter sets are in higher range. 2020 Text Classification Competition: Twitter Sarcasm Detection CS410 - COURSE PROJECT FINAL DOCUMENT MOHAN KHANAL (MKHANAL2@ILLINOIS.EDU) 1 Contents How to use/run the code ................................................................................................................ 2 Background ..................................................................................................................................... 2 Model Design .................................................................................................................................. 3 Detail Approach / Code Walkthrough............................................................................................. 4 Data Pre-processing .................................................................................................................... 4 Data preparation for training ...................................................................................................... 6 Model Building and Training ....................................................................................................... 8 Prediction for Test set ............................................................................................................... 14 Prediction Results ......................................................................................................................... 15 Other Approaches ......................................................................................................................... 15 Conclusion ..................................................................................................................................... 16 References .................................................................................................................................... 16 2 How to use/run the code The code is developed using python 3 (Jupiter notebook). There is folder called source_code on the GitHub (https://github.com/mkhanal2/CourseProject) where we put all the documentation and source-code for the project. Follow following instruction to run the code: * Download the folder ""source_code"" from GitHub (link above) * Makes sure you have folder called ""data"" under source_code folder which has (train.jsonl, test.jsonl) files , all of these files and folder are already in GitHub. * We would need one for file in this data folder. Go to the link below to download the ""GloVe"" twitter file we need ""glove.twitter.27B.25d.txt"". After downloading the file for the link, unizip the file and copy the file ""glove.twitter.27B.25d.txt"" onto the data folder. o Direct Link - http://nlp.stanford.edu/data/glove.twitter.27B.zip o OR: Go to below link and download below mentioned file. # https://nlp.stanford.edu/projects/glove/ # Download ""glove.twitter.27B.zip"" form this site. o Note: Chrome didn't work for me while downloading, so I used ""Microsoft Edge"" to download the above file. * Now open Jupiter Notebook on you laptop (usually within Anaconda). * Using Jupiter Notebook open the source code from the downloaded folder (source code file name: Project Source Code.ipynb) * After that you can run the code. Please make sure that all the packages used in the 2nd cell of the notebook are already installed. I have provided the instruction on 1st cell of the notebook on how to install the packages. Background As part of the final project for course CS410-Text Information System, I have done project for Twitter Sarcasm Detection which is part of the Text Classification competition. As part of this project there were two sets of data file given to us. One was for training and another was for testing (for which we would have to do our prediction). These data files contained the twitter responses. These response text were in context to some conversation happening in twitter feed. These data file contained those context conversation as-well. So, we could use both response text and context text if we want to for training our model for this classification. Since this was a classification task, the training file also had the label for each data point as ""SARCASM"" or ""NOT_SARCASM"". Our job was to predict the same thing for all the records present in the testing file. Training file had 5000 labelled data-set and testing set had 1800 data-set. So, at the end we would have to predict the label for those 1800 test data-set. This project required some machine learning task to train the model using the training data and finally predict the outcome for test-data set using that trained classification model. This 3 document explains all the details around the different approaches that were carried out to train the model and will list out the final results that I got from those trained model. Model Design As part of this project, I have used recurrent neural network models called LSTM (Long Short Term Memory) model's. These models are mostly used for sequential data. Since, the text that we are using as our input is sequential and sarcasm might depend on how the sentence is structured, so using LSTM would help us utilize the sequential nature of the data. As explained in previous section, we have ""response"" text as one set of input data that we can use and another is we have ""context"" text as another set of input that we can use. So, here were the first two sets of model that we can design using these inputs. Model-1 * Using response only as part of our input and feeding that data into the model to predict if that response is sarcastic or not. Below diagram gives the flow of the model. Model-2 * Another approach is rather than only using the response as an input we can use both response and context as our input to train our model and get output from there. Below diagram show the design for the same. Results Combined/Aggregated * Finally we can decide to combine the results from Model-1 and Model-2 and then get our final aggregated results. Combining both Model-1 and Model-2 was able to beat the baseline score for me. Below is the diagram for the combined results model. I will be discussing the detail around this model in another section. Model-1 (LSTM) Output-1 Response(text) Model-2 (LSTM) Output-2 Response(text) Context(text) Model-1 Model-2 Output-1 Output-2 Final Output Response(text) Response(text) Context(text) 4 Detail Approach / Code Walkthrough As stated in the previous section I have used LSTM as a model for training my classification model. The program is written in python (Jupter Notebook). I will walk-over the detail steps that was carried out as part of my code in this section. Here are the steps that were carried out , which is going to be covered in details: 1. Data Pre-processing - Reading the file, and cleaning the data for training 2. Data preparation for training - Creating the vocabulary, creating the input vector for text. 3. Model Building and Training - Building different models and training them with the data/label prepared 4. Prediction for Test set - Combing results from multiple model to improve prediction score. Data Pre-processing Data pre-processing is the first process that is carried out before building the model. As part of data pre-processing step , following steps are carried out: * Read the training file provided (""train.jsonl"" - 5000 data records). This is a json file, and contains attributes (""label"", ""response"" and ""context""). I have used pandas json file reader function (read_json) to read the json file. * After reading the file, the emoji and hashtags are read separately using custom built functions for reading emoji and hashtags and placed on separate columns in pandas data frame, which could be utilized to use as input for a model. The final model used for this project doesn't utilize hashtags or emoji, but emoji's were utilized on some of my model's for testing, but was not carried out to the final model. * After that the text ""response"" , which is one of the main input data-set for our model is cleaned, using the custom build function. This function will remove any junk characters and rephrase some of the wordings , trim the text to remove white-spaces, remove the stops words like (the, is, etc.) and lemmatize the text so that the similar type of words are represented the same. * After that, context goes through the same clean-up process as that for response. Before that, context data is a list/array. Which means that there could be multiple level of replies for the tweet and the response above was the reply for the last reply from the context. So, we use the list object to get the last reply from it to set the context for us. After that it goes through the same clean-up process described above for response. * Similar to the train data, test data goes through the same pre-processing step. ""test.jsonl"" (1500 data records) is similar structure of training data (only thing different is, this data doesn't have label). Below is the code screenshot for the steps explained above. 5 Data-read and clean-up code (train.jsonl): Clean-up functions: 6 Data-read and clean-up code (test.jsonl): Data preparation for training After pre-processing for the data is completed, we would need to build a input vector that can be used for training. As currently after pre-processing we only have text data, and we would need to convert it into some numeric form to build the model. So, we would need to build a input vector represented in numeric form for this. There are multiple options for building a training vector. We can tokenize the words into integer's and then choose to use a embedding layer in our network to learn about the embedding vector. or we can choose to use some pre-trained embedding vector. While both approaches were tried as part of this project, I found that pre-trained embedding vector had better results than the self-trained one, so I am using the pre-trained embedding vectors. I am using the pre-trained vector from ""GloVe"". This embedding layer part is covered in more detail in next section. Here are the steps required to build a input vector from the pre-processed text. * After text pre-processing is completed, we need to build a word vocabulary which contains all the words that we are using for training and testing as-well. * The word vocabulary in then tokenized (into integers). * Then we are going to set the length of the input vector that we are going to feed to our model/network. While looking into pre-processed response and context, the max length of response was around 200 range and for context was around 250 range. So, I am using input length of 200 for response and 250 for context. * X and X_cntx vector with length 200 and 250 are developed. X is for response and X_cntx is for context. The vector X may look line this [0 0 0 ..... 25 34 26 27], where 0 at the left are '0' padded to make the length of the vector 200. The integer on the vector are the ones that were tokenized form of the word, so each word is converted in specific integer by the tokenize function. Similarly X_cntx is in similar structure, only difference it's its length is 250. Below is the code screenshot for the steps explained above. 7 Build a word vocabulary (code) Find max length's for input vector's (Code) Building Input Vector's (Code) 8 Model Building and Training After data clean-up and preparing for the input vector, we now need to represent our word integers as vectors. What this will do is that it will build a vector which will have related word together and unrelated word farther apart, that way the numeric representation of the words becomes more meaningful for training. As previously mentioned, we have two ways to do that. Once is to train that word-vector using training data (Embedding layer) another is to use the pre-trained data. For this project I tried both, at the end the pre-trained model worked better than the self-training. So, I am using the pre-trained data for word-embedding. I am using GloVe (Global Vectors for Word Representation) for that. GloVe has multiple files that we can use. For this project I have use the glove twitter data-set with 25d vector. (glove.twitter.27B.25d.txt) Glove site: https://nlp.stanford.edu/projects/glove/ Using GloVe we will be building a embedded layer in our model, what that embedding layer will do is, it will take the input of 1-d integer vector that we build (X, X_cntx) and then convert them into 2-d vector's where each integer is converted into another vector with length 25. That 2-d vector is then passed to the neural network model. After embedding layer is built we will then build two different model's one with ""Response"" as an input and another with ""Response"" and ""Context"" as input. Below is the detail diagram of both the model's. As mentioned previously I am using LSTM model for this project. Model-1: Model-2: Response (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM Output (sigmoid activation) Response (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM Output (sigmoid activation) Context (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM 9 Below is the details steps done as part of model build and training: * The input-vector which is a integer conversion from text is first converted into pre-trained word-vector's. I am using GloVe (Global Vector for Word Representation) for this. * We need to read the GloVe vector from a file. After that all the words that were part of our word vocabulary is been fetched and embedding matrix is built with that. * The embedding matrix build will be used to create a embedding layer for our neural network model. This embedding layer's job would be to convert the integer vector into the GloVe vector representation, which can then be feed into neural network (LSTM). * After building the embedding layer, we now build an actual model. * Frist model, is a simple LSTM model which takes the response as input pass it to embedding layer and then into the LSTM network and finally a output is generated from that model. * Second model is a combination model, where I have built a two different LSTM model's one for Response as input (similar to first model) and another model is also similar LSTM but with Context as input. Finally these two LSTM's are combined to get the output for this second model. * After both the model's are built. We now have to prepare the training and testing data-set. As we have prepared the vectors X, X_cntx which has 5000 data-elements previously. We will split this data into 4000 training set and 1000 test-set (This is the test set used in training for validation). * After the split of training and test-set for validation. We now train both the model's model-1 and model-2 with these data. Below is the code snapshot for the above explained steps. Reading GloVe vector from the downloaded files (Code): 10 Building embedding matrix and Embedding Layer with GloVe vector(Code): Building First Model (Code): 11 Building Second Model (Code): Training and Test(Validation) data split from training data (Code) 12 Training Model-1 (Code): Below is the graph drawn from the training of Model-1: 13 Training Model-2 (Code): Below is the graph drawn from the training of Model-2: 14 Prediction for Test set Now, with both the model built and trained with training data, we will now predict the results for the test-set (1800 data set's that were read from train.josnl). Below is the details steps of what we do for prediction: * As we did for the training set we will first vectorized the text into integers before passing to the model as that is the input to our model. We build both response and context vectors as Z, and Z_cntx. * After building a vector now we pass the vector (Z) i.e. response only into model_1 for prediction. Similar we pass (Z and Z_cntx both) into second model for prediction. With this we now have two sets of prediction which is from model-1 and model-2. * Finally we combine the model-1 and model-2 prediction into single column, and then use some threshold defined to predict the result. * Since we are combining the results form model-1 and model-2 the aggregated value of the output will range from 0 to 2. The half-way cut-off value is 1.0, but instead of using 1.0 as our cut-off we will be using 0.5 as our cut-off to determine if it's sarcastic or not, we are doing this because this will increased our re-call by a lot with very less impact in the precision (that is what I found during the training/testing). This will increase our overall scoring of the model. * Finally, we will generate a file ""answer.txt"" with all this prediction for each of the data-set given and then the file is uploaded into GitHub to get score from LiveDataLab. Below is code for the steps explained above: Building Input Vector for test data set (Code): Predicting using Model_1 and Model_2 (Code): Combine Results from Model_1 and Model_2 (Code): Exporting the results (answer.txt) (Code): 15 Prediction Results There were multiple runs / iteration that was carried out as part of this project to beat the baseline score. The model explained above was able to beat the baseline score, that is combining two different model's and aggregating the result to get the final output. Below is the screenshot from the LiveDataLab that shows the overall score that I had which is above the baseline score. This score is as of 12/11/2020: I have also uploaded the answers.txt file in the GitHub with the source-code. https://github.com/mkhanal2/CourseProject/blob/main/source_code/answer.txt Other Approaches Apart from the approach that was discuss above (which was able to beat the benchmark score), I tried other approaches as-well. Other approaches that I tried were not able to beat the benchmark score. * Other Approach -1 (LSTM - ""response"" only model) o This approach is similar to the model-1 that I implemented above, but this approach alone was not able to beat the baseline score. The best f1 score for this model was around 0.70 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. * Other Approach -2 (LSTM - ""response"" and ""context"") o This approach is similar to the model-2 that I implemented above, but this approach alone was not able to beat the baseline score. The best f1 score for this was also around 0.70 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. * Other Approach -3 (LSTM - ""response"" and ""emoji"" model) o This model was based on model-1 above, and also one additional model which was used to train the data with ""emoji's"" only , since the emoji containing response were limited, so this model was also not able to beat the baseline score. Although this model was better from previous two approaches, the best f1 score for this was around 0.71 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. 16 Conclusion After testing through multiple models of LSTM , the combined model with ""response"" only as input and ""response and context"" as input was able to beat the baseline score. So, combining different model's with different inputs and then combining the results of output made model more efficient. Similar we could further tune/iterate through this model , or introduce new features like emoji's to possibly improve the score for this model. Also, there are other model that can be tried with this data-set , like BERT. Due to the limited time for this project, my work has been only limited to LSTM model explained above. References There were lots of learning and references that I took as part of this project, as LSTM / neural-network was kind of new topic for me. I have also taken the code snippet from some of the references list below. Listed below are websites/codes that I took reference and learned from: * https://adventuresinmachinelearning.com/keras-lstm-tutorial/ * https://machinelearningmastery.com/crash-course-deep-learning-natural-language-processing/ * https://www.aclweb.org/anthology/C16-1231.pdf * https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/ * https://www.kaggle.com/mkowoods/deep-learning-lstm-for-tweet-classification * https://github.com/AniSkywalker/SarcasmDetection * https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281 * https://arxiv.org/pdf/1911.10401.pdf * https://www.aclweb.org/anthology/2020.figlang-1.11.pdf * https://github.com/MirunaPislar/Sarcasm-Detection * https://www.youtube.com/watch?v=pMjT8GIX0co * https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet-sentiment-analysis-ced697948c47?gi=5e1b3ad1bacc * https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ * https://github.com/Suji04/NormalizedNerd/blob/master/Introduction%20to%20NLP/Sarcasm%20is%20very%20easy%20to%20detect%20GloVe%2BLSTM.ipynb * https://rcciit.org/students_projects/projects/cse/2018/GR6.pdf CS 410: Text Information Systems Course Project Proposal - Text Classification Competition: Twitter Sarcasm Detection Submitted By: Mohan Khanal (mkhanal2) mkhanal2@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. o Team Name: MK (Individual) o NetIDs for all team: (only myself) # Mohan Khanal (NetID: mkhanal2) o Captain: Myself (mkhanal2) 2. Which competition do you plan to join? * Text Classification Competition: Twitter Sarcasm Detection 3. Are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods * Yes, I am prepared to learn state-of-the-art neural network classifiers * Framework/Architecture looking to learn and utilize: Recurrent Neural Networks (RNN) , Bidirectional Encoder Representations from Transformers (BERT). * I have done machine learning project previous for Intrusion Detection system, but it was not related to text classification. I am familiar with how machine learning models / neural network works. So, will be utilizing that knowledge to work on this text classification project and will also learn new things to carry out the project. 4. Which programming language do you plan to use? * Python CourseProject Team Members Mohan Khanal (mkhanal2@illinois.edu) Text Classification Competition: Twitter Sarcasm Detection Model Used: LSTM All the details about the project has been documented in the ""CS410 Project Documentation.PDF"" file out here. https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Documentation.pdf Documents Project Proposal : https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Proposal.pdf Project Progress Report : https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20-%20Progress%20Report.pdf Project Final Document: https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Documentation.pdf Source Code Python Juputer Notebook Code: https://github.com/mkhanal2/CourseProject/blob/main/source_code/Project%20Source%20Code.ipynb Video Presentation Link --> https://mediaspace.illinois.edu/media/1_3i2g9ehq Output File answer.txt under source_code folder Link: https://github.com/mkhanal2/CourseProject/blob/main/source_code/answer.txt How to use/run the code The code is developed using python 3 (Jupiter notebook). There is folder called source_code on the GitHub (https://github.com/mkhanal2/CourseProject) where we put all the documentation and source-code for the project. Follow following instruction to run the code: - Download the folder ""source_code"" from GitHub (link above) - Makes sure you have folder called ""data"" under source_code folder which has (train.jsonl, test.jsonl) files , all of these files and folder are already in GitHub. - We would need one for file in this data folder. Go to the link below to download the ""GloVe"" twitter file we need ""glove.twitter.27B.25d.txt"". After downloading the file for the link, unizip the file and copy the file ""glove.twitter.27B.25d.txt"" onto the data folder. - Direct Link - http://nlp.stanford.edu/data/glove.twitter.27B.zip - OR: Go to below link and download below mentioned file. - https://nlp.stanford.edu/projects/glove/ - Download ""glove.twitter.27B.zip"" form this site. - Note: Chrome didn't work for me while downloading, so I used ""Microsoft Edge"" to download the above file. - Now open Jupiter Notebook on you laptop (usually within Anaconda). - Using Jupiter Notebook open the source code from the downloaded folder (source code file name: Project Source Code.ipynb) After that you can run the code. Please make sure that all the packages used in the 2nd cell of the notebook are already installed. I have provided the instruction on 1st cell of the notebook on how to install the packages. Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/mlbernardoni/CourseProject	"CS410 Project Submission Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Link to Video: https://illinois.zoom.us/rec/play/p1k2d8OXy9zlMU52qAKeYekW6uxafweLBO2aqz6R_DGSbHkY06jjyYQTueHspfeX-Fep54MHEkpEDyt1.ORGucOcp6XSypU7d?continueMode=true Initial Setup A lot of preparation when into getting the environment ready even before the ITMTF algorithm was analyzed in detail. First, data had to be collected, mined, prepped, and reduced into a form that could easily be loaded before each run. Furthermore topic mining and stats libraries had to be selected. Detailed analysis of these data curation steps and the library selection can be found in the Appendix. Detailed instruction of the steps to setup the python environment and the libraries used can be found in the last sections of the Appendix (and in the readme file). Creation of a Baseline After the data was procured and cleaned, and the python environment created, we first set about creating a baseline. A tricky prospect in any topic mining algorithm is selecting the number of topics. The Gensim library has logging that allowed us to take a reasonable guess at a preliminary topic number. We created baselines with 10, 15, 20, 25, and 30 topics. Using the logging module, we captured the coherence of each model. While the paper suggested that 30 topics was an appropriate number (section 5.2.3), the results of the coherence logging gave us a hint that 20 topics might also be a good number to analyze. (If interested, the logging code is in notebook: coherence_create_helper. The logging was also used to tune the number of passes and the number of iterations to show that the model would converge with our extreme settings for decay.) We then set about re-creating the algorithm in the paper. An analysis of the ""classical"" algorithm can be found in the section Classical ITMTF Algorithm. In addition to re-creating the algorithm, we set about creating an ""improved"" algorithm. 4.00E-014.50E-015.00E-011015202530CoherenceInstructions on running iterations with both the classical algorithm and the ""improved"" algorithm are in the comments of the main notebook: ITMTF (Note: the ITMTF notebook is the entry point to running the algorithm. Detailed comments on the parameter set up can be found at the top of the notebook) ""Improving"" the algorithm After recreating the paper's algorithm, we set about seeing if we could improve upon it. While analyzing the paper, a sentence caught our eye. Section 4.2.3 ""While we observe correlations between non-textual series and both word streams and topic streams, we do not compute correlations for all word streams. Word level analysis would give us finer grain signals. However, generating all the word frequency time series and testing correlations would be very inefficient."" The documents are stationary, thus the word series would be static over time. The word streams, along with the Granger and Pearson statistics could be pre-processed. Our data mining had already collected the words per document, and the documents per time slice. It was not difficult to create word stream and pre-process all of the Pearson and Granger stats. (Please refer to the jupyter notebook itmtf_prerun_stats to see the python code used to pre-process the Granger and Pearson statistics.) (Please refer to the appendix for all of the libraries used in this project). In our ""classic"" algorithm, after we run the granger test on the topic coverage stream, we added one step. We multiplied the topic/word probability from the model with the p-value that we had pre-processed for the word streams. We normalized this new number. The hope was that the algorithm would ""nudge"" the model into selecting words with higher statistical relevance to the betting time series and not just the words the model had selected as the top words. This change did improve the algorithm, but we wished for something ""bigger"". (The final implementation of the classic algorithm in the project directory does contain this change.) The goal of the algorithm is to iteratively improve our confidence in the topics selected by the algorithm. The topic modeling algorithm will refine topic coherence, and the ITMTF algorithm would then use word level analysis as a prior to nudge the topic modeling software toward more significant word choice for the topic. A large part of the ITMTF algorithm is the splitting of significant topics with ""positive"" words placed in one topic, and ""negative"" words placed in another. We decide to try a different (and simpler) approach. One of the benefits of topic modeling is words can have different impact in different contexts. For example, the word ""rights"" can have one impact on the betting sequence data if used in the context discussing ""second amendment rights"", and a different impact in another context discussing ""civil rights"". The classical algorithm would separate out the word ""rights"" in one of these topics. We created a simplified algorithm where the topics created by the model were left in place, only at each iteration we would analysis on every word (based upon the pre-processed word stream statistics). How the new algorithm works: 1. Run the model 2. Run a Granger analysis on the topic coverage across the time series (same as the classical algorithm) a. note this analysis is not used to refine the model, but is used to gather the confidence score for the run b. keep the current state if the confidence score of this new run is higher than the previous high confidence score 3. For each topic, multiply each word probability by the pre-process p-value of that word from the word stream analysis, and normalize 4. Use the new topic/word probabilities as a prior into the next round We ran the algorithm using 20, 30, and 40 topics. Results are shown below: The runs resulted in remarkably similar scores. All 3 results seemed to hit a peak at the 11 or 12 iteration mark, and then all 3 models recovered to hit an ultimate peak at the14-16 iteration mark. We also ran the 30 topic model over 30 iterations, to identify if there were further peaks - there were not. Results shown below: 0.70.750.80.8512345678910111213141516171819202120 Topic0.70.750.80.8512345678910111213141516171819202130 Topics0.60.70.80.912345678910111213141516171819202140 Topics0.650.70.750.80.851234567891011121314151617181920212223242526272829303130 IterationThe next step was to analyze the word output to see what was happening to the desired result. We captured the top words for significant topics at the 11-13 peak, and the peak that occurs after 16. The top words are shown below: 11 iteration peak 16 iteration peak It became clear that after the first peak centered around 11th iterations, the model began to over fit the data with only words that were highly correlated to the betting data, such as ""gore"" and ""bush"". The conclusion was for this set of data, the peak that occurs around 11 iterations produces the best data. Next we turned our attention to the number of topics. We conducted 13 iteration runs with 40, 30, and 20 topics. The produced word data is shown below: 13 iteration runs with 20, 30, 40 topics. For this set of data, 30 topics produced the best data. With 30 topics relevant data was quite dense. Next we looked at the effect of decay. We ran 30 topic runs for 13 iterations with decay settings at .001 (strong effect from priors), .5 (the default for the Gensim library), .75 (the default for several other LDA libraries), and .9 (very low effect from priors) 0G+: goreB+: bushG+: courtG+: presidentG+: campaignG+: abortionG+: georgeG+: alB+: goresG+: justices3B+: bushG+: georgeB+: policeG+: schoolG+: governorG+: familyB+: glennB+: crimeG+: avenueG+: black5B+: bushG+: goreG+: educationB+: taxB+: bushsG+: socialG+: securityG+: presidentB+: federalG+: george8B+: bushG+: goreG+: campaignB+: bushsG+: presidentG+: texasG+: georgeG+: governorG+: peopleB+: vice12B+: vidalG+: goreB+: bushnellB+: authorG+: loveG+: octB+: cityB+: theaterB+: sexB+: page21G+: goreG+: clintonG+: presidentB+: goresG+: alG+: conventionG+: campaignG+: speechB+: viceB+: bush22B+: bushG+: georgeG+: republicanG+: govG+: nationalG+: abortionG+: presidentG+: prochoiceG+: forcesB+: antiabortion4G+: goreG+: healthB+: bushG+: presidentB+: viceG+: governorB+: careG+: childrenB+: lehrerB+: companies7B+: industryG+: augG+: entertainmentG+: reB+: goresB+: cityG+: opedG+: brooklynG+: goreyG+: bartlett16B+: bushB+: bushsG+: texasG+: goreG+: governorG+: georgeG+: presidentB+: visitedG+: campaignG+: hours20G+: presidentG+: goreG+: abortionB+: bushG+: courtG+: rightsB+: viceG+: clintonG+: alG+: george22B+: articleB+: pageB+: bushG+: frontG+: georgeG+: octG+: alG+: policyB+: yesterdayG+: misstated400G+: goreG+: russiaB+: agreementG+: armsB+: chernomyrdinB+: congressB+: weaponsB+: russianG+: ministerG+: law3G+: goreB+: industryB+: bushG+: entertainmentB+: warB+: goresG+: dayB+: pineG+: coffeesB+: vidal7G+: gunB+: bushB+: controlG+: goreB+: associationG+: billG+: texasG+: lawG+: rifleG+: guns8G+: goreB+: bushG+: naderG+: campaignG+: presidentG+: alB+: viceB+: goresG+: georgeG+: vote10B+: bushB+: debatesG+: campaignG+: goreG+: debateG+: commissionG+: presidentialB+: bushsG+: presidentB+: officials19B+: bushG+: texasG+: governorB+: bushsG+: governorsG+: georgeG+: goreG+: hispanicG+: campaignG+: president22G+: administrationB+: bushG+: goreG+: goreyB+: companiesG+: issuesG+: topB+: telecommunicationsG+: alB+: federal24G+: securityG+: socialG+: goreB+: bushB+: planG+: campaignB+: goresB+: bushsG+: moneyG+: president26G+: goreG+: alB+: goresG+: conventionG+: presidentG+: campaignG+: vietnamG+: democraticG+: speechG+: family28G+: debateB+: bushG+: goreG+: presidentG+: georgeB+: viceG+: alB+: lehrerG+: peopleG+: texas29B+: bushB+: bushsG+: georgeG+: friendsG+: texasG+: campaignB+: fatherG+: familyG+: peopleB+: time30B+: articleG+: bushwickB+: yesterdayB+: amG+: misstatedG+: streetG+: brooklynG+: yorkG+: nameB+: copies31G+: abortionG+: rightsG+: presidentB+: bushG+: georgeG+: supportG+: republicanB+: decisionG+: platformB+: nominee33B+: bushG+: goreG+: presidentG+: campaignG+: alB+: viceG+: georgeG+: clintonB+: goresG+: house36B+: lazioG+: clintonG+: georgeB+: donorsB+: senateG+: republicanG+: politicalG+: campaignG+: presidentialG+: daley301B+: lazioB+: bushG+: georgeG+: yorkG+: clintonB+: laziosG+: hillaryG+: goreG+: alG+: campaign2B+: bushG+: courtG+: governorG+: georgeG+: goreG+: deathG+: presidentG+: texasG+: troopsG+: penalty4B+: drugB+: medicareB+: planB+: prescriptionG+: healthB+: bushB+: coverageB+: insuranceB+: elderlyB+: companies10G+: msB+: womenG+: imG+: husbandG+: bookG+: lifeG+: tipperG+: schiffG+: wifeB+: bushnell13B+: bushB+: commercialG+: campaignG+: adB+: advertisementG+: vietnamB+: screenB+: militaryB+: wordB+: rats20G+: goreB+: bushB+: taxB+: planB+: bushsG+: securityG+: socialB+: goresG+: presidentB+: cut25B+: bushG+: goreG+: georgeG+: alB+: lettermanG+: debateB+: jokesB+: comedyG+: presidentG+: hes26G+: blackG+: goreG+: alG+: presidentB+: articleB+: bushG+: georgeB+: playB+: viceB+: copies29B+: bushB+: baseballB+: rangersG+: teamB+: ownersB+: warG+: georgeB+: ownerB+: arlingtonG+: stadium206G+: bushwickG+: avenueB+: policeG+: yorkG+: streetB+: vidalG+: brooklynB+: theaterB+: universityG+: gorey12G+: goreB+: bushG+: percentB+: goresG+: debateG+: votersG+: presidentB+: pollB+: viceG+: people14G+: goreG+: campaignB+: bushB+: fundraisingG+: houseG+: presidentB+: millionG+: whiteG+: democraticB+: vice16B+: bushG+: texasG+: goreG+: abortionG+: georgeB+: issueG+: governorG+: presidentG+: gunG+: gov Decay output What we discovered was the new algorithm produced very similar data with differing decay settings. All runs had a peak at the 9-11 iterations, then varying fluctuations thereafter. At the lowest decay (highest impact from the prior) the fluctuations leading up to the 11th iteration peak were quite even and the can in confidence was steady. As one would expect, with the highest level of decay (lowest impact from the prior) the confidence remained quite flat. The intermediate levels of decay, .5 (the Gensim default) and .75 (the default for other LDA libraries) both showed improved confidence with a peak around the 11th iteration. The new algorithm proved to be quite effective for this set of data. Confidence steadily improved and reached a peak around the 11th iteration. After that peak, the algorithm began to over fit the data and produce topics with few, but highly significant, words. Furthermore, the new algorithm was successful without an overreliance on hyper parameter tuning. The number of topics had to be selected, but the default values for decay from the Gensim library produced results. The new algorithm did require more iterations than the classic algorithm, and the word streams had to be pre-processed. However, once tuned, the Granger analysis would only need to be conducted during the peak window (9-12 iterations) looking for the maximum confidence. 0.70.750.80.8513579111315171921Decay .0010.650.70.750.80.8513579111315171921Decay .50.60.70.80.913579111315171921Decay .750.60.70.80.913579111315171921Decay .9Classical ITMTF Algorithm The team was able to re-crate the classical algorithm from the paper. The only real variation was the use of LDA vs PLSA. The paper used PLSA, but did state that any topic mining algorithm would work. In fact, that was one of the thrusts of the paper, a general framework. As discussed below, Gensim LDA was selected as it performed an iteration in a reasonable time (2 minutes vs over a  1/2  hour for the python PLSA algorithm). Below are some of the plots produced by our implementation of the algorithm. (Please refer to the notebook Classic_Baseline_Plot) When compared to the graphs produced in the paper, our implantation produced similar results. The average causality confidence steadily increased with both implementations, with higher prior impact showing the best results. Both implementations had somewhat flat purity. We are unsure why the published results had such poor baseline numbers. The poor baseline numbers in the published word made the first iteration appear like a substantial jump. Our algorithm did show a large improvement for the first iteration (especially with the larger prior influence, as in the published paper), just not as dramatic. Another variable would be the data cleaning. One of the topics published has ""pres"" ""al"" ""vice"" as its top 3 words. This topic clearly is about Vice President Al Gore. It seems the words ""Gore"" and ""Bush"" were removed as part of the data cleaning for the paper. This would have a large impact on the model, as the p-values for the words ""gore"" and ""bush"" were the largest of all words. Appendix - Data mining and cleansing The python code used to clean the data can be viewed in the itmtf_cleaning jyputer notebook. Step 1: Data mining First we mined the raw xml data and produced a .txt for each document that had a paragraph with the words ""Gore"" or ""Bush"". We only included the paragraphs with the key words, but we kept the document intact, that is if a doc had 2 paragraphs with either the word ""Bush"" or ""Gore"" the output would be one document with those 2 paragraphs. Note this is just prep work and is not included in the project for size considerations. Step 2: Data cleansing - .\LDA_data\LDAData.csv For each file in the mined directory, we split the string into words. For each word we made each word lowercase, stripped out any character that was not alpha, and removed all stop words. We used stop words from: Onix Text Retrieval Toolkit Stop Word List 1: https://www.lextek.com/manuals/onix/stopwords1.html . We added the results for each document in a .csv file .\LDA_data\LDAData.csv. Each document is a row: cell 1 contains the year; cell 2 contains the month; cell 3 contains the day; cell 4 contains the cleansed text string of the document We also created a csv file .\LDA_data\vocabulary.csv which contains unique vocabulary words in cell 1 and the count of the term in cell 2. Step3: Data reduction - .\LDA_data\LDAreduced.csv Using the vocabulary csv .\LDA_data\vocabulary.csv from step 2, we removed any word that only occurred once or twice (all words with counts over 2 were kept). We produced a csv file .\LDA_data\vocabularyreduced.csv which contains the new list of unique vocabulary words. Using the new vocabulary, we created a new csv .\LDA_data\LDAreduced.csv in the same form as the un-reduced csv. Step 3: Word coverage per time slice - .\LDA_data\wordseries.csv Using the vocabularyreduced.csv and the LDAreduced.csv we pre=processed a csv that contains the word coverage per time slice - .\LDA_data\wordseries.csv. The first row is a header row that contains the unique words in the vocabulary, this row is not used in the algorithm, but makes the file human readable. The first column in each row contains the time slice. All subsequent columns contain the word coverage during that time slice. This pre-processed file will be used in the ITMTF algorithm. Current data mining and cleansing files in the project: .\LDA_data\LDAData.csv cleaned data .\ LDA_data\vocabulary.csv cleaned data's vocabulary .\ LDA_data\LDAreduced.csv removed words occurring 1 or 2 .\ LDA_data\vocabularyreduced.csv removed data's vocabulary .\ LDA_data\LDAwordseries.csv words counts per time slice Step 4: Betting information The betting data is publicly available at the following site: https://iemweb.biz.uiowa.edu/closed/pres00_WTA.html Python was used to clean the data, and smooth the data into both 3 day and 5 day averages. The python code can be viewed at the following site: Bush Vs Gore Betting Data - Google Drive Topic Mining Algorithm Selection The paper indicates that the LDA algorithm was used. As such, we attempted to us LDA. First we discovered the LDA algorithm pypi https://pypi.org/project/plsa/. The algorithm worked well in our test data sets, and had excellent data visualization techniques. We identified where to add new topics in the library's python code with the iteration feedback. However when we ran the full cleaned data, this library took over 12 hours to complete 1 model. One of our team members wrote a LDA algorithm in C++. The C++ algorithm was significantly faster. However, running the entire corpus caused memory issues. Time does not permit adding data swapping to disk. Following the lead of other teams discussed on Piazza, we then selected Gensim's LDA algorithm for topic mining https://radimrehurek.com/gensim/models/ldamodel.html#usage-examples. This algorithm does not have memory issues, and completes in a reasonable amount of time (under 10 min on one of team member's home desktop). Instructions for adding this library into an Anaconda environment is in the appendix. Appendix - Libraries used Gensim Python LDA - https://radimrehurek.com/gensim/models/ldamodel.html SciPy's pearson r - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html statsmodels granger causality tests - https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.grangercausalitytests.html pyLDAvis - https://pyldavis.readthedocs.io/en/latest/index.html glob - https://docs.python.org/3/library/glob.html Matplotlib - https://matplotlib.org/ Library tested but not used: pypi.orgs PLSA - https://pypi.org/project/plsa/ Appendix - Environment setup For Windows: Open an anaconda prompt, navigate to the project's directory and type: conda env create -f ITMTF.yml The created environment will be called ""Gensim"", when you open the notebook, you will have to change kernels to Gensim. See troubleshooting note below. Adding Gensim LDA library to an Anaconda environment manually: Optional - create a new Anaconda environment to install the Gensim package: 1. Open Anaconda Navigator 2. Select Environments 3. Create an environment (i.e. ""gensim"") Install genism in Anaconda 1. Open the Anaconda command prompt 2. If you created a new environment in the previous step: a. Activate the newly created environment if you created one (""Activate gensim"") b. Run: conda install nb_conda_kernels (Proceed Y) c. Run: python -m ipykernel install --user --name myenv --display-name ""Gensim"" (you can use any display name you wish, this is what will show up on Jupyter Notebook) d. Run: pip install environment_kernels 3. Run: pip install --upgrade genism 4. Run: pip install -upgrade pyldavis 5. Run: pip install -upgrade glob2 6. Run: pip install -upgrade matplotlib Start Jupyter Notebook in the directory you downloaded the project (if not your default) 1. Open the Anaconda command prompt 2. Start Jupyter Notebook in the directory you have downloaded this project (i.e., ""jupyter notebook c:\projects"") TROUBLESHOOTING NOTE: When you open the project in Jupyter Notebook, look to the upper right and you can see what environment the project is running. If this is not the environment you just set up for Gensim, select Kernel from the notebook menu and select Change kernel, and change to the correct kernel. CS410 Project Progress Report Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Tasks completed:  Data mining and cleaning of the text documents - completed.  Data mining and cleaning of the betting probabilities - completed.  Production of the word/time slice coverage for the time period - completed.  Topic mining algorithm selection - selected Gensim LDA.  Topic mining on the entire corpus - LDA algorithm implemented, baseline created  Production of the topic coverage from one run of LDA - completed.  Adding new topics back into the LDA algorithm after iteration - completed.  Ability to iterate over the prior 3 steps for the entire time period - completed  Understanding the ITMTF algorithm - completed.  Completed the setup instructions for Gensim in anaconda - completed. Tasks to do:  Code the time sequence scoring function  Code the word analysis scoring function  Code the topic splitting  Visualization of the final data Challenges:  The article has a section on m. A m of 0 means the prior is not considered. The higher the m the stronger the prior. Gensim LDA has a concept of decay (between 0 and 1). Like a m set to 0, if the decay is set to 1, the prior is not considered (similar to a m of 0). The Gensim documentation states that a decay set between .5 and 1 will converge. We ran a test with a very small decay (.001), and the topic we added remained virtually intact. We are still discovering how the decay changes the ITMTF algorithm, hopefully similar to m.  We are still discussing the topics to be carried forward to the next iteration. The article proposes a ""variable"" topic approach and discusses keeping ""buffer"" topics. Not a challenge, but something we are looking at. Detailed discussion of these steps follow: Detailed discussion Data mining and cleansing Step 1: Data mining First we mined the raw xml data and produced a .txt for each document that had a paragraph with the words ""Gore"" or ""Bush"". We only included the paragraphs with the key words, but we kept the document intact, that is if a doc had 2 paragraphs with either the word ""Bush"" or ""Gore"" the output would be one document with those 2 paragraphs. Note this is just prep work and is not included in the project for size considerations. Step 2: Data cleansing - .\lda_data\LDAData.csv For each file in the mined directory, we split the string into words. For each word we made each word lowercase, stripped out any character that was not alpha, and removed all stop words. We used stop words from: Onix Text Retrieval Toolkit Stop Word List 1: https://www.lextek.com/manuals/onix/stopwords1.html . We added the results for each document in a .csv file .\lda_data\LDAData.csv. Each document is a row: cell 1 contains the year; cell 2 contains the month; cell 3 contains the day; cell 4 contains the cleansed text string of the document We also created a csv file .\lda_data\vocabulary.csv which contains unique vocabulary words in cell 1 and the count of the term in cell 2. Step3: Data reduction - .\lda_data\LDAreduced.csv Using the vocabulary csv .\lda_data\vocabulary.csv from step 2, we removed any word that only occurred once or twice (all words with counts over 2 were kept). We produced a csv file .\lda_data\vocabularyreduced.csv which contains the new list of unique vocabulary words. Using the new vocabulary, we created a new csv .\lda_data\LDAreduced.csv in the same form as the un-reduced csv. Step 3: Word coverage per time slice - .\lda_data\wordseries.csv Using the vocabularyreduced.csv and the LDAreduced.csv we pre-processed a csv that contains the word coverage per time slice - .\lda_data\wordseries.csv. The first row is a header row that contains the unique words in the vocabulary, this row is not needed by the algorithm but was used during debugging. The first column in each row contains the time slice. All subsequent columns contain the word coverage during that time slice. This pre-processed file generates the word coverage over time used in the ITMTF algorithm. Current data mining, cleansing, and pre-processed files in the project: .\lda_data\LDAData.csv cleaned data .\lda_data\vocabulary.csv cleaned data's vocabulary .\lda_data\LDAreduced.csv removed words occurring 1 or 2 .\lda_data\vocabularyreduced.csv removed data's vocabulary list .\lda_data\wordseries.csv words counts per time slice Topic Mining Algorithm Selection The paper indicates that any topic mining algorithm can be used, but the author used the PLSA algorithm. As such, we attempted to use PLSA. First we discovered the PLSA algorithm pypi https://pypi.org/project/plsa/. The algorithm worked well in our test data sets, and had excellent data visualization techniques. The code did not have out of the box ways to add topics and topic priors, but the code was available. So we identified where to add new topics and where to set topic priors in the library's python code. However when we ran the full cleaned data, this library took over 12 hours to complete 1 model, and we would have to iterate 6-7 times. One of our team members wrote a PLSA algorithm in C++. The C++ algorithm was significantly faster. However, running the entire corpus caused memory issues. Time does not permit adding sparse matrix processing or data swapping to disk. Following the lead of other teams' discussion on Piazza, we then selected Gensim's LDA algorithm for topic mining https://radimrehurek.com/gensim/models/ldamodel.html#usage-examples. This algorithm does not have memory issues as it works on document chunks, and completes in a reasonable amount of time due to its use of sparse matrix processing (under 10 min on one of team member's home desktop). As an added bonus, the library had a way to add new topics and priors. Instructions for adding this library into an Anaconda environment is in the appendix. Algorithm Iteration Completed The first step, loading the pre-processed files into arrays, is complete. The documents are loaded into memory. The pre-processed file had the date in the first cell. While the document is loaded into memory an array is created of the docs per time slice. This array will be used to create the topic coverage per iteration. The word coverage preprocessed file is loaded into an array that can be used by the ITMFT algorithm. The loaded docs are loaded into the Gensim's dictionary format. While loading the documents, a map is created that maps the index from the pre-processed vocabulary coverage to Gensim's token index for that word. This map is used to create the new topic matrix in the correct sequence, when new topics are returned from the ITMFT algorithm. A Gensim corpus is created from the Gensim dict. As the documents do not change during the ITMFT algorithm (only the topic and topic priors), this corpus can be re-used throughout the entire ITMFT algorithm. Running the Gensim LDA model is coded. The ITMFT iteration is coded: 1. The preprocessed word coverage is passed into the algorithm 2. The document/topic probabilities are pulled from the LDA model 3. Using the document to time slice matrix, a topic coverage matrix is created from the document/topic probabilities. 4. These 2 coverage matrices are passed into the ITMFT scoring function 5. The ITMFT scoring function will create a new matrix of new topics and word probabilities prior 6. The coding of adding this matrix back into the LDA model is complete. To do: 1. Code the ITMFT scoring function 2. Code the ITMFT topic creation function Appendix Adding Gensim LDA library to an Anaconda environment Optional - create a new Anaconda environment to install the Gensim package: 1. Open Anaconda Navigator 2. Select Environments 3. Create an environment (i.e. ""gensim"") Install genism in Anaconda 1. Open the Anaconda command prompt 2. If you created a new environment in the previous step: a. Activate the newly created environment if you created one (""Activate gensim"") b. Run: conda install nb_conda_kernels (Proceed Y) c. Run: python -m ipykernel install --user --name myenv --display-name ""Gensim"" (you can use any display name you wish, this is what will show up on Jupyter Notebook) d. Run: pip install environment_kernels 3. Run: pip install --upgrade gensim Start Jupyter Notebook in the directory you downloaded the project (if not your default) 1. Open the Anaconda command prompt 2. Start Jupyter Notebook in the directory you have downloaded this project (i.e., ""jupyter notebook c:\projects"") TROUBLE SHOOTING NOTE: When you open the project in Jupyter Notebook, look to the upper right and you can see what environment the project is running If this is not the environment you just set up for Gensim, select Kernel from the notebook menu and select Change kernel, and change to the correct kernel. Team Project Proposal Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Captain: Michael Bernardoni mlb12@illinois.edu GitHub fork link: https://github.com/mlbernardoni/CourseProject Reproduce the following paper:  Subtopic: Causal topic modeling o Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Programming language: The team will initially proceed programming with Python. However if performance becomes an issue C++ will be evaluated. Tableau may be used for data visualization of the resulting datasets. Dataset: The dataset can be found at the Linguistic Data Consortium ( https://www.ldc.upenn.edu ). The team captain (mlb12@illinois.edu ) has applied to the consortium and is await approval from: University of Illinois at Urbana-Champaign, Beckman Institute. This request has also been posted on Piazza. Once approval is obtained access to the dataset will be provided."
https://github.com/mrente5/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mshaw0707/CourseProject	Progress Report 11/29/20201) Progress made thus farI am using a Jupyter notebook in Google Colab forthe project with Pytorch. I have readthe data into Pandas arrays and am working on trainingthe models. Currently I have thecode in place to split the data into training/validation/testsets and the scaffold code fortraining the models (currently investigating usingLSTM and BERT type models).2) Remaining tasks-Finish the training code to obtain the optimal hyperparametersfor the bestmodels-Format the output into answers.txt for submissionon livedatalab-Add a readme/comments. The code should be straightforwardfor anyone to runvia Google Colab (I am on the free tier)3) Any challenges/issuesNot currently blocked on anything; I am fairly confidentthat one of the models will beable to beat the baseline from the reading I havedone online regarding using Pytorchfor text classification. CS410 Fall 2020 Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Mathew Shaw, NetId: mcshaw2 I am doing the project solo. 2.Which competition do you plan to join? I will be joining the Text Classification competition. 3.If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I plan to use PyTorch to create the classifier for the competition. I have previously used the framework for some experimental image classification tasks. If PyTorch does not work out for this task I will look into alternatives like Keras. 4.Which programming language do you plan to use? Python CourseProject (Text Classification) Overview The project I chose was the sarcasm detection competition. I successfully beat the baseline by training a PyTorch BERT model using the Huggingface transformers library. I first researched what types of models could be used for this manner of text classification and found that the Huggingface library (https://huggingface.co/) had multiple pretrained models to suit this goal. Some minor data cleanup and preprocessing on the training/test data sets, along with concatinating the immediate context with the response allowed me to beat the baseline. Libraries/Languages Python Pytorch Huggingface transformers Pandas SKLearn Implementation I developed the entire project within a Google Colab (https://colab.research.google.com/) notebook. I used the free tier for all development and the final model training. I recommend opening the ipynb in Colab to run the code without needing to configure any of the libraries locally. Usage Open CS410Project.ipynb in Google Colab (https://colab.research.google.com/) The data storage for a Google Colab workspace is ephemeral, so the train.jsonl and test.jsonl files must be uploaded to the workspace storage each time. (Optional) For faster model training, click Edit -> Notebook Settings, then set the Hardware Accelerator to 'GPU' inside the popup. This was available for me using the free tier of Colab during development. The code will default to CPU computation if this is not done and the model will take longer to train. Run all code blocks in order. The file answer.txt will be created in the workspace for download. I used 5 epochs for training and it generated predictions that beat the baseline. Presentation I did not have time to do a presentation.
https://github.com/myusername-2/CourseProject	Course Project Progress Report Shai Yusov (syusov2@illinois.edu) System: EducationalWeb System Subtopic: Identifying in-demand skills 1.Progress Made Thus Far My project is about automatically identifying in-demand skills. The original plan was to scrape and analyze job postings for software engineers in New York, NY, from the job board Indeed and then extract the top skills from that dataset. So far, I have written, tested, and executed a component to scrape job postings for software engineers in New York, NY, from Indeed, to clean the scraped the data, and to save all processed job postings to a file; I have thoroughly investigated various algorithms and approaches to extracting the top keywords, and thus most in-demand skills, from the dataset; I have written and tested a component to ingest the dataset and extract the top skills using a keyword extraction algorithm; and I have compared and instrumented various approaches to extracting the top skills to achieve the best results. I have taken care to isolate the interfaces of these components from the implementation details as much as possible so that the functionality will be generic and could potentially integrate with other tools and systems in a straightforward manner. 2.Remaining Tasks The primary tasks of this project are complete. 3.Challenges and Issues When scraping job postings from Indeed, the primary issue I have faced is how to successfully extract all the needed pages in a timely manner and without being throttled. Specifically, my use case involves scanning job results pages, then obtaining job post links from that page, and so on. If executed in serial, this process would be rather slow, so I parallelized this IO-intensive process using a pool of threads that fetch and extract information from pages in parallel. This led to a significant speedup. I also had to account for various failures so I implemented retry-able mechanisms to ensure graceful error recovery. Overall, I was able to download all the data. Another issue I am facing is that, so far, the keyword extractions algorithms with the highest subjective quality of results are also the ones taking the longest. Specifically, it seems that an implementation of TextRank produces fairly relevant results but takes a little longer to finish, while other algorithm implementations of RAKE, YAKE, and TF-IDF produce somewhat more diluted results but in a shorter amount of time. Course Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. -Shai Yusov (syusov2@illinois.edu). ItOs a team of one and I will be the captain. 2.What system have you chosen? Which subtopic(s) under the system? -System: EducationalWeb System -Subtopic: Automatically creating teaching material for in-demand skills - Identifying in-demand skills 3.Briefly describe the datasets, algorithms or techniques you plan to use -I plan to crawl and analyze job postings for software engineers in New York, NY, on the job board Indeed. After obtaining these postings, the dataset will be pre-processed and fed into a component that will be responsible for identifying the emerging keywords/topics. The identification process will likely involve keyword extraction and/or topic mining. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? -I will demonstrate that the function identifies in-demand skills by running it on the input dataset, obtaining the top emerging keywords/topics, and validating the results through human judgement and known industry trends. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly -I plan to build a standalone system/component for this project. Specifically, the system will ingest a dataset and output the top emerging keywords/topics from that dataset. Although outside the scope of this project, this functionality is generic and can be plugged into other tools so long as the input specification to the system is decoupled from the source or actual content and the outputs are emitted in a standard, easy-to-digest format. 6.Which programming language do you plan to use? -I plan to use Python for this project. 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. -Writing, testing, and executing a customized web crawler: at least 10 hours -Investigating and defining the right algorithms/techniques: at least 5 hours -Writing and testing the component performing the identification: at least 10 hours CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview. Project Proposal project-proposal.pdf Project Progress Report project-progress-report.pdf Project Presentation Video Presentation Documentation 1. Overview This project contains two main components that together automatically identify in-demand skils through scraping and analysis of job postings from the job board Indeed for positions in New York, NY. indeed_scraper.py: Scrapes job postings in New York, NY, for a user-specified query. Specifically, it runs a user query on Indeed, scans results pages one by one, extracts job post links from each results page, and obtains and saves the content of each job post to a file (one per line). The output file is specified by the user. This component optionally accepts a file containing a list of proxies to use when sending requests. Can be used to run generic, user-specified queries and scrape results from the job board Indeed (For example, scrape job posts for software engineers, scrape job boards for data analysts, etc.). keyword_extractor.py: Extracts the top keywords from a collection of text documents. It accepts an input file of text documents where there is one document per line, extracts a number of top keywords as optionally specified by the user, and writes these keywords to a file that is specified by the user. Can be used to generically extract top keywords from any collection of text (For example, top skills from job postings, top keywords from research papers, top themes in customer complaints, etc.). More generally, the component indeed_scraper.py can be used as a standalone script to scrape job postings at scale from Indeed for any query and keyword_extractor.py can be used both as a standalone script and as a module to extract top keywords (or phrases). 2. Implementation indeed_scraper.py This module does the following: 1. In the main thread, creates a thread pool with a number of workers. 2. Executes the user query on Indeed and iterates through each page of the results one at a time. 3. For each page of results, extracts all job post links from that page and feeds those links to the worker threads, who will process specific job post links in parallel and will fetch the full content of each one. 4. The main thread continues to iterate through pages of results and passes specific job post links to the worker threads, while each worker thread continues to process specific job post links and captures the full content of each one. 5. Once the main thread has iterated through all pages of the search results, it waits for all the worker threads to finish processing the specific job post links that it has continuously passed them thus far. 6. Once all job posts have been processed, the thread pool terminates and the main thread writes all job posts to a file (each job post on its own line). The logic described above is found mainly in the function scrape_multithreaded(url, query, number_of_pages, results_per_page). Other important functions include: job_links_from_search(url, search_params): - Obtains all job post links from the search results page specified by url and search_params. job_from_link(link): - Obtains the content of the job post at the specified link. get_js_soup(url, params): - Obtains a BeautifulSoup object for the page specified by url and params to be used for parsing HTML content. Finally, all HTTP communication is carried out using python's requests module. keyword_extractor.py This module does the following: 1. Ingests a text file where every line is a document of text. 2. Extracts the text from the input file and runs an algorithm on all the text to produce an optionally specified number of top keywords from the text. The algorithms used to produce the keywords is TextRank, as implemented by pytextrank, but other algorithms such as YAKE, as implemented by pke, are included as well unused functions in the codebase. 3. Outputs the top keywords to console or an optionally specified output file. Note that this component can be used both as a standalone script as well as a module. Specifically, the class KeywordExtractor has a generic interface and can easily be reused in other settings: KeywordExtractor(input_file): A KeywordExtractor is a class for extracting keywords from an input file of text where there is a text document on each line. - top_keywords([n]): Extracts and returns a list of the top n keywords from the input file provided at construction. 3. Usage Installation This package requires Python 3.0+. It also requires the following external resources that can be obtained using: pip install bs4 pip install spacy pip install pytextrank python -m spacy download en_core_web_sm To install this package, clone the repository from github: git clone https://github.com/myusername-2/CourseProject.git cd CourseProject Usage Example First, we scrape job posts from Indeed for 'Software Engineer' in New York, NY. The full job descriptions will be stored in jobs.txt, one job on each line. python indeed_scraper.py --query 'Software Engineer' --pages 2 --results-per-page 25 --output-file jobs.txt Then, we want to discover the most relevant skills in all these job descriptions. So we run the keyword extractor script as follows to discover the top 50 keywords: python keyword_extractor.py -n 50 -o keywords.txt jobs.txt The top 50 keywords will be stored in a file keywords.txt. API Usage keyword_extractor.py: ```python Top 25 keywords from the collection in 'input.txt' input_file = 'input.txt' n = 25 keyword_extractor = KeywordExtractor(input_file) top_keywords = keyword_extractor.top_keywords(n=n) ``` Command Line Usage indeed_scraper.py: ``` python indeed_scraper.py --help usage: indeed_scraper.py [-h] --output-file output_file --query query [--pages pages] [--results-per-page results_per_page] Scrape job posts from Indeed. optional arguments: -h, --help Show this help message and exit --pages pages Number of pages of job results to scrape. --results-per-page results_per_page Number of job results per scraped page. required named arguments: --output-file output_file Output file containing job descriptions matching the query and with one job description per line. --query query Query to search for jobs. ``` keyword_extractor.py: ``` python keyword_extractor.py --help usage: keyword_extractor.py [-h] [-n n] [-o output_file] input_file Extract keywords from an input text. positional arguments: input_file Input file containing text. Each text document should be on its own line. optional arguments: -h, --help show this help message and exit -n n Number of top keywords. -o output_file, --output-file output_file Output file containing the top 'n' keywords in the input text. ```
https://github.com/mzhai20/CourseProject	"1 Documentation for CS410 Final Project: Improving ExpertSearch System Mengyu Zhai (NetID: mzhai) Fall 2020 1. Overview 1.1 Background This project is for Fall 2020 CS410 Text Information Systems final project option 2.2 ExpertSearch System. The information quoted below is from CS410 course project instructions and what contained is the foundation of the current project: ""The ExpertSearch system (http://timan102.cs.illinois.edu/expertsearch//) was developed by some previous CS410 students as part of their course project! The system aims to find faculty specializing in the given research areas. The underlying data and ranker currently come from the MP2 submissions of the previous course offering. You can read more about it here (Sections 3.6 and 4: Project are especially relevant). The code is available here."" 1.2 Functions of Code The current project is trying to enhance the utility of the ExpertSearch System by extracting relevant information from faculty bios. More specifically, the code uses techniques for extracting other information, i.e., faculty research interests, than what is already provided in the original system. Simply put, topic mining is performed on bios and top-keywords are found and shown as the common research areas, in addition to the bio information shown in the search result. This added function does not influence the ways that users can use to conduct searches in the system. Users can search the expert by whatever search key word they want to use and applying whatever location or university filters they choose. Now if the user is especially interested to know which experts work in a certain research area and what their respective research interests are, they can directly search by that research area they have in mind. What is improved is that in the search result, rather than just showing the matching parts in the bio of an expert (or matching parts in the bios of all the experts in the search results) as before, a set of highly possible research interests (using top-keywords) of the expert will also be 2 provided. The method is not perfect as we assume top-keywords are common research areas but cannot guarantee that is always the case. 2. Implementation 2.1 What Are Used * Python * LDA topic model: generative statistical model - to detect topics * gensim API: to lemmatize and add bi-gram tokenization (build dictionary), and train LDA model * pyLDAvis: to provide interactive visualization of LDA topic results * word_cloud: to visualize corpus key words * Other major tools/ packages involved: Jupyter notebook, numpy, pandas 2.2 Steps * With jupyter notebook, utilizing gensim API and LDA to build the topics from the combined bios (10 for each bio). o Preprocess data o Lemmatize and stop word removal o Build dictionary and use word_cloud to visualize the corpus o LDA modeling o Visualize LDA topics with pyLDAvis * Then also within juyter notebook, results are saved into a researchinterest file under ExpertSearch\data. * Finally modify the original ExpertSearch web application source code to add and display top topics as research interests. 2.3 Results The corpus outlook built using word_cloud: 3 pyLDAvis output example (interactive and topic num = 10): An example of some selected topics for a bio: 4 Another example of some selected topics for a different bio: The top topics with the highest weight get saved to researchinterest.txt and will be the one to display in expert search results. Search by key words, you will be able to see the results similar to image below: 5 You can also filter by locations and/or universities: 6 Now compare with the original system's results below, which doesn't provide top topic key-words: 3. Installation and Run Codes uploaded to github include ExpertSearch_LDA.ipynb and web application codes in ExpertSearch.zip. Be aware that the web application codes does not work on Windows (mainly because gunicorn is not supported on Windows)! 3.1 Simple Instructions (If you are familiar with the project) To run the web application code, run the following command from ExpertSearch (work with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ 3.2 More Detailed Instructions (If you are not familiar with the original project) * Download ExpertSearch.zip. * Unzip it in Python2.7 on MacOS and Linux. * Go to the folder and run the following command: 7 [ExpertSearch]$ gunicorn server:app -b 127.0.0.1:8095 * You should see something similar to below once the website is running: [ExpertSearch]$ [2020-12-11 03:07:57 +0000] [17325] [INFO] Starting gunicorn 19.10.0 [2020-12-11 03:07:57 +0000] [17325] [INFO] Listening at: http://127.0.0.1:8095 (17325) [2020-12-11 03:07:57 +0000] [17325] [INFO] Using worker: sync [2020-12-11 03:07:57 +0000] [17329] [INFO] Booting worker with pid: 17329 * The site should be available at http://localhost:8095/. Otherwise, you can open the port 8095 so people can access it from http://yourdomain:8095, for example: References: 1. [Reference code]: https://github.com/CS410Fall2020/ExpertSearch/ 2. [Reference code]: https://github.com/TeddyWang0202/BeyondLD 3. [Reference file]: https://bhaavya.github.io/files/SIGCSE2020.pdf Progress Report 11/29/2020 CS410 Fall 2020 Final Project: Improving Expert Search System Mengyu Zhai Progress made thus far: * Spent a lot of time (much more than planned) acquiring and setting up the running environment that the original code requires, which is different from what I had. * Studied the code and made necessary modifications to make the code run locally. Remaining tasks: * Continue working on the new function code and testing * Make the final demo * Upload all files Challenges/issues being faced: * None now CS410 Fall 2020 Final Project Proposal: Improving Expert Search System 1. I am doing this project individually. My name is Mengyu Zhai and NetID is mzhai. 2. In this project, I choose to improve the Expert Search System (option 2.2), and more specifically, the subtopic of Extracting relevant information from faculty bios. I plan to add the function of extracting faculty research interests, so users can search faculty by their research interests. 3. Dataset to be used is the bios available under MP2.3 on Coursera Week 5: complied_dataset.zip. According to the introduction on the Coursera page, all the faculty bio submissions from MP2.1 that were submitted by Sep 20 were compiled into this dataset, and there are a total of 16492 unique faculty bios. Algorithms or techniques to be used is topic mining on the bios available under MP2.3 on Coursera. The top-keywords per topic are the common research interests in Engineering and Science. 4. I am adding a function that is not in the original system, so it is relatively easy to demonstrate that it works as expected - In the final demo, I will perform some search with some research interests and show that it can really extract the relevant results. 5. I plan to utilize the system and code given, and then add & modify codes that can achieve the added function. 6. As the original code is in Python, I plan to use Python as the programming language. 7. As an individual project, its planned workload is about 20 hours, not including the time used for this proposal. I may spend more hours because I am not a Python expert, but I wouldn't count much of that into the workload of the project. Main Tasks Estimated Time Cost Manually pick common research interest areas (topic terms) from a typical Engineering and Science department (plan to use UIUC CS department faculty's research interests pool https://cs.illinois.edu/about/people/all-faculty ) 3 hours Study original system code and system 2 hours Work on the new function code and testing 13 hours Make the final demo 1 hour Upload all files (proposal, progress report, project code and documentation, project presentation/demo) 1 hour CourseProject Final project submissions include: - Documentation.pdf - Code - ExpertSearch_LDA.ipynb - ExpertSearch.zip (in Releases on the right side of the page, under About section) - Software usage tutorial presentation link: https://youtu.be/WXJgNFB4Nxo Software usage tutorial presentation link: https://youtu.be/WXJgNFB4Nxo"
https://github.com/n3a9/CourseProject	"Text Classification Competition  - Twitter Sarcasm Detection CS410 Fall 2020 Neeraj Aggarwal (noa2@illinois.edu), Samarth Keshari (keshari2@illinois.edu), Rishi Wadhwa (rishiw2@illinois.edu) Background The goal of this project is to build a model that can perform  Sarcasm Detection  on twitter data. The trained model should have a  F1-score above 0.723  on test data. Solution Approaches Machine Learning Standard machine learning requires us to manually select a set of relevant features and then train a machine learning model based on the features. As part of this project we trained multiple classifiers including a  Random Forest Classifier  using  Bag-of-Words of bigrams  as the feature representation. The random forest classifier performed best in this category, therefore, it was trained on many different hyperparameters. We eventually settled on 1000 trees, and a 46% confidence threshold to define as sarcasm. Deep Learning In contrast to the standard machine learning approach in deep learning we skip the manual step of feature extraction and directly feed the data to the deep learning algorithm which automatically learns features. The trained deep learning model is then used to perform the prediction task. In this project we used  Convolutional Neural Network(CNN)  based architecture along with a  pre-trained BERT Tokenizer to generate the token ids. Following are the different elements of the software built as part of this approach Model Architecture The figure below shows the architecture of the deep neural network model implemented. The input text to the neural network is first tokenized using a pre-trained BERT tokenizer. The tokenized text is then fed to an embedding layer, followed by three parallel convolutional layers. Finally, the outputs from the convolutional layer become input to a fully connected layer followed by the softmax output layer. Implementation Comparison Below table compares the F1-scores of the tuned models based on both machine learning and deep learning approaches Both the tuned models are able to achieve the goal to get F1 scores above the baseline of 0.723. Software Implementation Machine Learning As stated above, we employ pre-processing on the text data and then use the TF-IDF with n_gram = (1,2) to represent text. The TF-IDF features are then used for training the model. The trained model is finally used to perform the inference. Approach Algorithm Precision Recall F1 Deep Learning Convolutional Neural Networks 0.6227867590 0.89888888 0.7357889949977261 Machine Learning Random Forest Classifier 0.6824825174825175 0.7265952491849093 0.7265952491849093 Deep Learning - CNN Model Training Flow The figure below shows the flow of the training process that is followed to build the deep learning model from data - Response + Context. The training process consists of two components - 1) Data Processor 2) Model Builder. The Data Processor takes the context and response text from training data as input and generates required features as output. The Model Builder then takes the generated features as input and performs a sequence of steps to train and save the trained model. Both the components can be controlled based on the input parameters. Model Inference Flow The figure below shows the flow of the inference engine that is used to predict the results of test data The inference process makes use of the same Data Processor from training to generate the features from input test data - context + response text. The generated features are then fed to the inference engine which loads the trained model and performs the prediction and saves the results. Software Usage Installation Requirements The software is built with Python3.7.7 and uses the following packages. emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 You can automatically install all of these packages and download the source code by first cloning the repo   https://github.com/n3a9/CourseProject.git .   Then navigate into the project directory and run   pip install -r requirements.txt Machine Learning There are 4 machine learning models that are available for usage: - Random Forest Classifier `random_forest.py` - MLP Classifier `mlp_classifier.py` - SGD Classifier `sgd_classifier.py` - Logistic Regression `logistic_regression.py` To run the machine learning models, `python [file.py]`. It will generate an `answer.txt` in `./src/machinelearning`. Deep learning - CNN APIs As part of this project, the user can call two apis *Model Training *In order to call this api, in the command terminal the user need to #Navigate into the cloned repository to the directory  ../src/deeplearning #If required, change the parameters file ' params_config.json ' at ../src/deeplearning/parameters . Refer to the  Parameters section  below for details about the different parameters used during the model training #Run command -  python modelTrain.py #The trained model weights will be saved at  ../src/deeplearning/trained-models in ' cnn_model_weight.h5 ' file Note: For the project verification purpose, model training can be performed by changing different parameters(refer to Parameters section below). Currently by default any new trained model weights will not be saved, however, caution should be taken that any new trained model weights if saved can vary the final results. *Model Inference *In order to call this api, in the command terminal the user need to #Navigate into the cloned repository to the directory  ../src/deeplearning #If required, change the parameters file ' params_config.json ' at ../src/deeplearning/parameters . Refer to the  Parameters section  below for details about the different parameters used during the model inference. #Run command -  python modelInference.py #The final predictions will be saved at  ../src  in ' answer.txt ' file Note: For project verification purpose run only the Model Inference Parameters Below is the list of parameters that are used during the model training and inference process. Refer to ' params_config.json ' file in the cloned repository at twitter-sarcasm-detection/src/deeplearning/parameters Name Description Used In n_last_context Number of last entries from in the context list Training + Inference data-path Path to folder storing the train and test data files Training + Inference train-data-filename Name of the train file in .jsonl format Training test-data-filename Name of the test file in .jsonl format Inference processed-data-path Path to folder storing the processed train and test data files Training + Inference processed-train-data-filename Name of the processed train file in .csv format Training processed-test-data-filename Name of the processed test file in .csv format Inference features-path Path to folder storing the train and test features files Training + Inference features-train-filename Name of the train features file in .json format Training features-test-filename Name of the test features file in .json format Inference trained-model-save Flag to indicate that the weights of the trained model should be saved. By default the model will not be saved. If required, set the flag to ""X"". Training trained-model-path Path to the folder storing the trained model weights Training trained-model-weight-filename Name of the file storing the trained model weights in .h5 format Training train_test_split % of records that are needed for validation during model training. The value of this parameter should be between (0,1) Training embedding_dimensions Number of dimensions in the embedding layer of the model Training cnn_filters Number of CNN filters in the CNN layers of the model Training dnn_units Number of neurons in the fully connected layer of the model Training dropout_rate Dropout rate for the fully connected layer of the model Training verbose Training References *https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM *https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras n_epochs Number of epochs for model training Training batch_size Batch size for model training Training prediction-threshold Model predictions for test data are in terms of probabilities. For a particular test sample, if the prediction probability is above this threshold value, then the test sample is flagged as SARCASM otherwise NON-SARCASM Inference answer-file Path + filename of the final results file in .txt format Inference Team Simple Neeraj Aggarwal, Samarth Keshari, Rishi Wadhwa Our Project: Text Classification Competition Goal = to build a model that can perform Sarcasm Detection on twitter data Evaluation Target = the model should have F1 score greater than 0.723 Our Approach Approach-1 = Traditional Machine Learning Input Text Representation = Bag-of-Words (bi-gram) Algorithms = Random Forest, Logistic Regression, SGD(SVM) & MLP Approach-2 = Deep Learning Input Text Representation = Pre-trained BERT tokenizer Architecture = Convolutional Neural Networks Results Installation Python(version 3.7 - 3.8) Visit https://github.com/n3a9/CourseProject Download or Clone the Code Required Packages(pip install) emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 * You can automatically install all of these packages by navigating into the project directory './src' and then running command pip install -r requirements.txt Code Repository Structure Data in 'data' directory where we can find train and test files Source Code in 'src' directory Machine Learning code in 'machinelearning' directory - './src/machinelearning' Deep Learning in 'deeplearning' repository - './src/deeplearning' Usage - Machine Learning There are 4 machine learning models that are available for usage: - Random Forest Classifier `random_forest.py` (best result classifier) - MLP Classifier `mlp_classifier.py` - SGD(SVM) Classifier `sgd_classifier.py` - Logistic Regression `logistic_regression.py` To run the machine learning models, navigate to the machine learning directory - `./src/machinelearning` - and execute command`python [file.py]`. For example executing command 'python random_forest.py' in the terminal will run Random Forest Classifier. It will generate an `answer.txt` in `./src/machinelearning` Usage - Deep Learning There are 2 programs that are available for usage: - Model Training `modelTrain.py` - Model Inference `modelInference.py` There is a list of parameters that are used to control the training and inference programs. These parameters can be adjusted to influence the behavior of the 2 programs. *Refer to documentation to learn more about these parameters Usage - Deep Learning(Training) To run the model training, navigate to the deep learning directory - `./src/deeplearning` - and execute command`python modelTrain.py` This will save the trained model weights as `cnn_model_weight.h5` in `./src/machinelearning/trained-models` * Currently, by default any new trained model weights will not be saved. Also, caution should be taken that any new trained model weights, if saved can vary the final results Usage - Deep Learning(Inference) To run the model inference, navigate to the deep learning directory - `./src/deeplearning` - and execute command`python modelInference.py` It will generate an `answer.txt` in `./src/deeplearning` Thank You!!! For any questions please contact Neeraj Aggarwal (noa2@illinois.edu) Samarth Keshari (keshari2@illinois.edu) Rishi Wadhwa (rishiw2@illinois.edu) CS410 Project Proposal 1. Team Captain: Neeraj Aggarwal, noa2 Rishi Wadhwa, rishiw2 Samarth Keshari, keshari2 2. Competition: text classification competition 3. Yes, we are prepared to learn state-of-the-art neural network classifiers. We've heard of Tensorflow, PyTorch, and LSTM. We have relevant experience with text classification, neural networks, relisiency. We aren't experts, but we have dabbled in some of these methods in our pasts. 4. Programming Language: Python Sarcasm Detection (Project Progress) Neeraj Aggarwal(noa2@illinois.edu), Samarth Keshari(keshari2@illinois.edu), Rishi Wadhwa (rishiw2@illinois.edu) Progress As part of the project work we have used both Machine Learning and Deep Learning approaches to solve the problem. Based on our analysis we found that among different machine learning algorithms,  Random Forest Classifier  performed best after some hyperparameter tuning. Below are metrics obtained after running different algorithms. After this, we decided to change the threshold in which we determined whether the tweet was sarcastic or not, by lowering the confidence level necessary. Approach Algorithm Precision Recall F1 Machine Learning LogisticRegression 0.6723952738990333 0.6955555555555556 0.6837793555434188 Machine Learning SGDClassifier 0.6762820512820513 0.7033333333333334 0.6895424836601308 Machine Learning LinearSVC 0.6652631578947369 0.7022222222222222 0.6832432432432433 Machine Learning MLPClassifier TBD TBD TBD Machine Learning RandomForest Classifier 0.6424825174825175 0.8166666666666667 0.7191780821917808 From this, we found the RandomForestClassifier with  1000 trees  and  0.48 threshold. Eventually, the Deep Learning based approach gave the best performance results. Both the tuned Random Forest(Machine Learning) and Convolutional Neural Network(Deep Learning) are able to get F1 scores above the baseline of 0.723. Remaining Tasks We already crossed the baseline, but if we have time we could explore approaches in Deep Learning and improve CNN by tuning the hyperparameters. We could also explore different tokenization techniques and draw different insights from the tweets. For example, tokening and separating emojis and hashtags may allow us to bring significant improvements to sarcasm detection. Threshold F1 0.5 0.7191780821917808 0.4 0.7126436781609196 0.48 0.7265952491849093 0.46 0. 7231386535889435 0.44 0.7187904967602592 Approach Algorithm Precision Recall F1 Deep Learning Convolutional Neural Networks 0.6227867590 0.89888888 0.7357889949977261 Challenges and Issues None. CourseProject Twitter sarcasm detection by Samarth Keshari, Rishi Wadhwa, Neeraj Aggarwal. Installation The software is built with Python3.7.7 and uses the following packages. emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 You can automatically install all of these packages by first cloning this repo. Then navigate into the project directory and run pip install -r requirements.txt. Usage Machine Learning There are 4 machine learning models that are available for usage: Random Forest Classifier random_forest.py MLP Classifier mlp_classifier.py SGD Classifier sgd_classifier.py Logistic Regression logistic_regression.py To run the machine learning models, python [file.py]. It will generate an answer.txt in ./src/machinelearning. Deep Learning There are two APIs that you can use. Model Training cd src/deeplearning. If required, change the parameters file params_config.json at in /parameters. Refer to the parameters section below for details about the different parameters used during the model training. To run, python modelTrain.py. The trained model weights will be saved at ./trained-models in cnn_model_weight.h5 file. Note: For the project verification purpose, model training can be performed by changing different parameters(refer to Parameters section below). Currently by default any new trained model weights will not be saved, however, caution should be taken that any new trained model weights if saved can vary the final results. Model Inference cd src/deeplearning. If required, change the parameters file params_config.json at in /parameters. Refer to the parameters section below for details about the different parameters used during the modelinference. To run, python modelInference.py. The final predictions will be saved at ./src in answer.txt file. Note: For project verification purpose run only the Model Inference. Parameters Below is the list of parameters that are used during the model training and inference process. Refer to params_config.json file in the cloned repository at ./parameters | Name | Description | Used In | | ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------- | | n_last_context | Number of last entries from in the context list | Training + Inference | | data-path | Path to folder storing the train and test data files | Training + Inference | | train-data-filename | Name of the train file in .jsonl format | Training | | test-data-filename | Name of the test file in .jsonl format | Inference | | processed-data-path | Path to folder storing the processed train and test data files | Training + Inference | | processed-train-data-filename | Name of the processed train file in .csv format | Training | | processed-test-data-filename | Name of the processed test file in .csv format | Inference | | features-path | Path to folder storing the train and test features files | Training + Inference | | features-train-filename | Name of the train features file in .json format | Training | | features-test-filename | Name of the test features file in .json format | Inference | | trained-model-save | Flag to indicate that the weights of the trained model should be saved. By default the model will not be saved. If required, set the flag to ""X"". | Training | | trained-model-path | Path to the folder storing the trained model weights | Training | | trained-model-weight-filename | Name of the file storing the trained model weights in .h5 format | Training | | train_test_split | % of records that are needed for validation during model training. The value of this parameter should be between (0,1) | Training | | embedding_dimensions | Number of dimensions in the embedding layer of the model | Training | | cnn_filters | Number of CNN filters in the CNN layers of the model | Training | | dnn_units | Number of neurons in the fully connected layer of the model | Training | | dropout_rate | Dropout rate for the fully connected layer of the model | Training | | verbose | | Training | | n_epochs | Number of epochs for model training | Training | | batch_size | Batch size for model training | Training | | prediction-threshold | Model predictions for test data are in terms of probabilities. For a particular test sample, if the prediction probability is above this threshold value, then the test sample is flagged as SARCASM otherwise NON-SARCASM | Inference | | answer-file | Path + filename of the final results file in .txt format | Inference | References https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras"
https://github.com/nadiam2/CourseProject	"Final Project Documentation Project Contributors:  Nadia Mohiuddin (nadiam2), Pallavi Narayanan (pallavi3), Thaniel Tong (tztong2) Overall Use: The purpose of this project is to classify tweets into categories  SARCASM  or  NOT_SARCASM . The tweets that we categorize are written in response to other tweets, which are the context data. This is a form of sentiment analysis. How to Get the Code : How to Run the Code : Software Installation Details: *To install each Python library to use in our project, we ran ""pip install <library name>"" in our terminal before using the library in our actual code. *We then used regular import statements to use the libraries in our code. Model Used : *We used the SVM model from the sklearn.svm library. All the training was done through SVM. We did some preprocessing of the data in order to improve the performance. *First we tokenized all the words in the given tweets, then we removed stopwords,  ""@USER""  and ""<URL>""  tags in the tweet. *Then we lemmatized the data so we could get the root word. *Finally to end the preprocessing, we converted emojis into strings which helped to increase the accuracy of classification, as emojis can often be a signal of sentiment. *In order to train on context data we appended the context data to the response data to create a more robust text data to train on. *We used TF and IDF weighting to convert the words to float probabilities, giving importance to frequent terms in a document, and infrequent terms across the entire collection. *We fine tuned the hyperparameters for SVC using the RandomSearchCV algorithm from sklearn. *We then used these hyperparameters to train our SVC model with the training TF-IDF scores, and then predicted the test data labels using the testing TF-IDF scores. *Lastly, we output the test labels to a text file. What We Tried: *We also tried to preprocess the responses and contexts by stemming each word and making each word lowercase. However, these methods did not give us the optimal F1 score. *We attempted to use other sklearn classifiers such as GaussianNB, MultinomialNB, and git clone  <repository url> cd data python Sarcasm.py RandomForestClassifier, but none of these classifiers performed as well as SVC. *We tried to use the isalpha() function to remove all non alphabetic characters from the responses and contexts, but this also did not give us the optimal F1 score. *We tried using the default parameters for SVC, but those parameters did not perform as well as the hyperparameters we fine tuned using RandomSearchCV. *We also tried to fine tune parameters with GaussianNB, MultinomialNB, and RandomForestClassifier. We used GridSearch to find tune hyper parameters in Naive Bayes and a large multi-nested for-loop (mimicking GridSearch) for the RandomForestClassifier. However these still did not perform as well as SVC did. Final Data: Precision: 0.6468010517090271 Recall: 0.82 F1 score: 0.7231749142577168 Contributions: *The team started off working together, sharing ideas while one person typed with their screen shared to the other team members. *Later on, the team members split up to try different classifiers and methods of classifying the dataset. This approach was taken to save time as three computers train and test in parallel rather than waiting on one computer to run. Software Documentation/Tutorials: *Sklearn SVC *https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html *Sklearn RandomSearchCV Algorithm *https://intellipaat.com/community/18009/what-is-a-good-range-of-values-for-the-svm-svc-hyperparameters-to-be-explored-via-gridsearchcv *https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html *Lemmatization *https://www.geeksforgeeks.org/python-lemmatization-with-nltk/ *Sklearn TfidfVectorizer *https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html *Pypi emoji *https://pypi.org/project/emoji/ How To Run the Project https://www.youtube.com/watch?v=z0zpcLdSXK4&feature=youtu.be&fbclid=IwAR0OEihoSxD9j94N_Tx-AWhIP_ahQdAxT2Q-7moEabv1mOwIFmTX9URFQNg Project Progress Report 1)Progress Made : We parsed our json files/inputs, tokenized the inputs, stemmed the inputs, and tried out different methods to train on the train data and label the test data. 2)Remaining Task s: We have to figure out how to use BERT to properly label the test data given contexts and responses. 3)Any challenges/issues faced : We realized that the Machine Learning model and library we were using was not suited to detect the sarcasm in the text data we had. We also found that we need to incorporate the context data which we were not able to with our original choice of sci-kit learn. Topic: Text Classification Competition In your project proposal, please answer the following questions: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Nadia Mohiuddin (nadiam2), Thaniel Tong (tztong2), Pallavi Narayanan (pallavi3) 2.Which competition do you plan to join? Text Classification Competition (Tweet Sarcasm Detection) 3.If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We are prepared to learn the most recent, state of the art network classifiers. Neural network classifiers we know are AlexNet, GoogLeNet, AlphaGo, and Deep Blue. Some deep learning frameworks we have heard of are Pytorch, Caffe, and TensorFlow. These are some resources to help train and create deep neural networks. We do not have any prior experience using such methods/classifiers. 4.Which programming language do you plan to use? Python What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Which competition do you plan to join? If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Which programming language do you plan to use? CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/nadiawoodninja/CourseProject	"Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 Final Project Netid: nadiaw2 Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 Final Project Netid: nadiaw2 Table of Contents Abstract 2 Video 2 Code Repository 2 Demo App 2 Technical Architecture 3 Setting up Elastic Search in Google Cloud. 3 Data Pipeline 10 Fine Tuning the Engine 11 Creating a search UI to search data 12 Setting a development Environment locally on your computer. 12 Creating a UI for search experience by using App Search packages 14 Creating a search experience 14 Deploying the React app to Google Cloud Platform 17 Create the app on the App Engine 17 Deploy the app 21 Abstract In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1. Video Code Repository Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n Technical Architecture Figure 1 Setting up Elastic Search in Google Cloud. Go to https://console.cloud.google.com/ and create a project on Google Cloud. Choose Elasticsearch Service. The only reason I chose this is to get some free credit to do my project work. You can create a separate account on Elasticsearch if you want to but the trial only lasts for 14 days. A little bit about, Elasticsearch Service on Google Cloud: The service offers seamless integrated billing through your Google Cloud account for simple management and powerful customization. Once the service is setup, you can click on manage on provider, to go directly to Elastic Cloud, to manage and create a search deployment on the cloud. https://cloud.elastic.co/ Once you login, you will be taken to the Elastic Cloud dashboard. Here you can create your ""deployments"" . When you create a deployment, you are given a choice of selecting from pre-configured environments for your need. In my case, I chose the Elastic Enterprise Search solution to allow me to create a search experience on my web app. Elastic technology provides the following stack options. For my project I am using the stack outlines in red. This stack gives me the Elastic Cloud, which gives me the ability to make RESTFUL API calls to my search engine. Once your deployment is created you will be taken to the deployment dashboard. In this project, we will be focusing on using Enterprise search capability. Once you launch Enterprise Search, it will give you an option to select a product. For this project, I used App Search. An overview of the architecture of App Search stack is below. I have highlighted the architectural components which are being utilized for this project. Once the App Search is launch, it gives you an option to create a search engine. For this project, I created an engine called environmentaldata. This search engine allows a user to search through documents which contain sensor data e.g. humidity, temperature, light, battery info etc. Currently the engine contains xxxx documents. The documents were loaded into the engine by uploading JSON file to the engine. The engine requires a specific formatting for the json files to adhere to. The JSON field names have to be all lowercase or be separated by underscore. This created a need to automate the conversion of existing json files to be converted to the required format. Data Pipeline Data Ingestion program in C#: In order to quickly load json files to the engine, I created a program in C# to convert existing files to a proper json file so that it can be imported into App Search. This code can be run if you have Visual Studio free community version installed. The program requires to have a ""data"" folder where the files needed to be converted need to stored. The converted files are stored in the ""data/converted"" folder. I have included some converted file in the repo as well: https://github.com/nadiawoodninja/CourseProject/tree/main/data/converted Fine Tuning the Engine Once the documents are loaded into then engine, you can index any JSON object. The json object will become a search-optimized document within your Engine. A schema is created for you when you index your data - you do not need to specify any schema or alter your data before uploading. You can alter your schema later to set the appropriate data types. You also have the option to refine search by using features like, Relevance Tuning, Synonyms & Curations. For this project I utilized the Synonym feature as we may have data from different sensors and the same datapoint maybe spelt differently or represented differently. Creating a search UI to search data Setting a development Environment locally on your computer. Download and install Node.js from https://nodejs.org/en/ Once installation is complete run this command. We are going to use this to create a react app. npm i -g create-react-app Once the package is installed create the react app by running the command below. create-react-app 410-search-ui This command installs a light weight web server, webpack to bundle our code for deployment and Babel for compiling our JavaScript code. Once the app is created go to folder 410-search-ui and run this command. This will launch our development server on localhost:3000 npm start Creating a UI for search experience by using App Search packages Install React Search UI and the App Search connector by running these commands npm install --save @elastic/react-search-ui @elastic/search-ui-app-search-connector Creating a search experience I use Atom as my editor for React apps. The app folder contains src folder which contains all the source code. App.js is the main file where the program starts execution. The src folder also has a config folder which contains engine.json. This file contains all the configuration needed to configure your search UI. In this file you can define your ""facets"", the fields which will be displayed on your results page, your sort fields etc. Figure 2: engine.json The ability to define these configurations are provided by the packages which were installed above. Figure 3: App.js Deploying the React app to Google Cloud Platform Create the app on the App Engine Go to Google's App Engine Console and create a new project: Once the project is created, create an App Engine application. Select a region Select Node.js and standard environment Clone our app's source code from GitHub Activate the shell by clicking git clone https://github.com/nadiawoodninja/CourseProject.git Install npm by running and install other elastic search packages npm i npm install @elastic/search-ui-app-search-connector npm install @elastic/react-search-ui Build our app for deployment To do this, simply go into your app's root folder (where your ""src"" folder is), cd CourseProject cd 410-search-ui And type the following command: npm i npm run build This creates a folder named ""build"" in our root directory. Delete every thing else besides the build folder. Get rid of everything else, except for the build folder. Use these commands to remove files and folders rm <file-to-remove> rm -r <remove-recursively-like-directories-inside-directories> Add an app.yaml and deploy In the same folder where we have our ""build"" folder, create a new file named app.yaml. By the end of this step, the only things left should be the ""build"" folder and ""app.yaml"". That's all the App Engine will need to run our app. touch app.yaml nano app.yaml And add the following to its content: runtime: nodejs12 handlers: # Serve all static files with url ending with a file extension - url: /(.*\..+)$ static_files: build/\1 upload: build/(.*\..+)$ # Catch all handler to index.html - url: /.* static_files: build/index.html upload: build/index.html Deploy the app Deploy the app using the following command gcloud app deploy The app is running here https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 FINAL PROJECT NETID: NADIAW2 Table of Contents ABSTRACT ............................................................................................................................................ 2 DEMO APP ........................................................................................................................................... 2 TECHNICAL ARCHITECTURE .................................................................................................................. 3 SETTING UP ELASTIC SEARCH IN GOOGLE CLOUD. ................................................................................ 3 DATA PIPELINE ................................................................................................................................... 10 FINE TUNING THE ENGINE .................................................................................................................. 11 CREATING A SEARCH UI TO SEARCH DATA .......................................................................................... 12 SETTING A DEVELOPMENT ENVIRONMENT LOCALLY ON YOUR COMPUTER. ........................................................... 12 CREATING A UI FOR SEARCH EXPERIENCE BY USING APP SEARCH PACKAGES ......................................................... 14 CREATING A SEARCH EXPERIENCE ............................................................................................................... 14 DEPLOYING THE REACT APP TO GOOGLE CLOUD PLATFORM .............................................................. 17 CREATE THE APP ON THE APP ENGINE ......................................................................................................... 17 DEPLOY THE APP .................................................................................................................................... 21 Abstract In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1. Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Technical Architecture Figure 1 Setting up Elastic Search in Google Cloud. 1. Go to https://console.cloud.google.com/ and create a project on Google Cloud. 2. Choose Elasticsearch Service. The only reason I chose this is to get some free credit to do my project work. You can create a separate account on Elasticsearch if you want to but the trial only lasts for 14 days. 3. A little bit about, Elasticsearch Service on Google Cloud: The service offers seamless integrated billing through your Google Cloud account for simple management and powerful customization. 4. Once the service is setup, you can click on manage on provider, to go directly to Elastic Cloud, to manage and create a search deployment on the cloud. https://cloud.elastic.co/ 5. Once you login, you will be taken to the Elastic Cloud dashboard. Here you can create your ""deployments"" . 6. When you create a deployment, you are given a choice of selecting from pre-configured environments for your need. In my case, I chose the Elastic Enterprise Search solution to allow me to create a search experience on my web app. Elastic technology provides the following stack options. For my project I am using the stack outlines in red. This stack gives me the Elastic Cloud, which gives me the ability to make RESTFUL API calls to my search engine. 7. Once your deployment is created you will be taken to the deployment dashboard. In this project, we will be focusing on using Enterprise search capability. 8. Once you launch Enterprise Search, it will give you an option to select a product. For this project, I used App Search. 9. An overview of the architecture of App Search stack is below. I have highlighted the architectural components which are being utilized for this project. 10. Once the App Search is launch, it gives you an option to create a search engine. 11. For this project, I created an engine called environmentaldata. This search engine allows a user to search through documents which contain sensor data e.g. humidity, temperature, light, battery info etc. Currently the engine contains xxxx documents. The documents were loaded into the engine by uploading JSON file to the engine. 12. The engine requires a specific formatting for the json files to adhere to. The JSON field names have to be all lowercase or be separated by underscore. This created a need to automate the conversion of existing json files to be converted to the required format. Data Pipeline 13. Data Ingestion program in C#: In order to quickly load json files to the engine, I created a program in C# to convert existing files to a proper json file so that it can be imported into App Search. This code can be run if you have Visual Studio free community version installed. The program requires to have a ""data"" folder where the files needed to be converted need to stored. The converted files are stored in the ""data/converted"" folder. I have included some converted file in the repo as well: https://github.com/nadiawoodninja/CourseProject/tree/main/data/converted Fine Tuning the Engine 14. Once the documents are loaded into then engine, you can index any JSON object. The json object will become a search-optimized document within your Engine. A schema is created for you when you index your data - you do not need to specify any schema or alter your data before uploading. You can alter your schema later to set the appropriate data types. 15. You also have the option to refine search by using features like, Relevance Tuning, Synonyms & Curations. For this project I utilized the Synonym feature as we may have data from different sensors and the same datapoint maybe spelt differently or represented differently. Creating a search UI to search data Setting a development Environment locally on your computer. 16. Download and install Node.js from https://nodejs.org/en/ 17. Once installation is complete run this command. We are going to use this to create a react app. npm i -g create-react-app 18. Once the package is installed create the react app by running the command below. create-react-app 410-search-ui This command installs a light weight web server, webpack to bundle our code for deployment and Babel for compiling our JavaScript code. Once the app is created go to folder 410-search-ui and run this command. This will launch our development server on localhost:3000 npm start Creating a UI for search experience by using App Search packages 19. Install React Search UI and the App Search connector by running these commands npm install --save @elastic/react-search-ui @elastic/search-ui-app-search-connector Creating a search experience 20. I use Atom as my editor for React apps. The app folder contains src folder which contains all the source code. App.js is the main file where the program starts execution. 21. The src folder also has a config folder which contains engine.json. This file contains all the configuration needed to configure your search UI. In this file you can define your ""facets"", the fields which will be displayed on your results page, your sort fields etc. Figure 2: engine.json The ability to define these configurations are provided by the packages which were installed above. Figure 3: App.js Deploying the React app to Google Cloud Platform Create the app on the App Engine 22. Go to Google's App Engine Console and create a new project: 23. Once the project is created, create an App Engine application. 24. Select a region 25. Select Node.js and standard environment 26. Clone our app's source code from GitHub 27. Activate the shell by clicking git clone https://github.com/nadiawoodninja/CourseProject.git 28. Install npm by running and install other elastic search packages npm i npm install @elastic/search-ui-app-search-connector npm install @elastic/react-search-ui 29. Build our app for deployment To do this, simply go into your app's root folder (where your ""src"" folder is), cd CourseProject cd 410-search-ui And type the following command: npm i npm run build This creates a folder named ""build"" in our root directory. 30. Delete every thing else besides the build folder. Get rid of everything else, except for the build folder. Use these commands to remove files and folders rm <file-to-remove> rm -r <remove-recursively-like-directories-inside-directories> 31. Add an app.yaml and deploy In the same folder where we have our ""build"" folder, create a new file named app.yaml. By the end of this step, the only things left should be the ""build"" folder and ""app.yaml"". That's all the App Engine will need to run our app. touch app.yaml nano app.yaml And add the following to its content: runtime: nodejs12 handlers: # Serve all static files with url ending with a file extension - url: /(.*\..+)$ static_files: build/\1 upload: build/(.*\..+)$ # Catch all handler to index.html - url: /.* static_files: build/index.html upload: build/index.html Deploy the app 32. Deploy the app using the following command gcloud app deploy 33. The app is running here https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n Project Progress Report CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: Architecture Diagram Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Hours Status Determine how to set up, configure and deploy ElasticSearch on a cloud platform : The cloud platform determination will base on the cost difference between Azure, Google and AWS. 15 hours Completed. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. Migration of raw to json data to Elastic search server : 3 hours Completed. Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. Development and deployment of Web application: 10 hours Started. 8 hours' worth of work left. Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. Test and measure outcomes: 4 hours Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React. Project Progress Report CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Architecture Diagram Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Hours Status 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 15 hours Completed. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. 2. Migration of raw to json data to Elastic search server : 3 hours Completed. Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. 3. Development and deployment of Web application: 10 hours Started. 8 hours' worth of work left. Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. 4. Test and measure outcomes: 4 hours Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React. Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application in C#.net to use ElasticSearch to search content from a database of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in C# will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: Figure 1 The estimated time to complete this project is about 30-40 hours. Tasks and time: Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours The cloud platform determination will base on the cost difference between Azure, Google and AWS. Development and deployment of Web application: 10 hours Migration of SQL database to Elastic search server : 3 hours Test and measure outcomes: 4 hours Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application in C#.net to use ElasticSearch to search content from a database of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in C# will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Figure 1 The estimated time to complete this project is about 30-40 hours. Tasks and time: 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 2. Development and deployment of Web application: 10 hours 3. Migration of SQL database to Elastic search server : 3 hours 4. Test and measure outcomes: 4 hours CourseProject Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch Presentation Video Part 1: https://uofi.box.com/s/a9p26mi8eym9i0n3ql8d83jke6ztzjbz Part 2: https://uofi.box.com/s/gvsrrqsf9rpqbr48ha20slvg4m5eovky Final Report https://github.com/nadiawoodninja/CourseProject/blob/main/CS410ProjectFinalReport.pdf Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n NetID: nadiaw2. Individual Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web front end in React to use ElasticSearch's App Search to search documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data, transfer it to ElasticSearch platform. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of effort it takes to search for data in traditional RDMS database vs. using ElasticSearch's platform. The planned architecture is shown in Figure1: Figure 1 ============================== The estimated time to complete this project is about 30-40 hours. Tasks and time: 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 2. Development and deployment of Web application: 10 hours 3. Migration of SQL database to Elastic search server : 3 hours 4. Test and measure outcomes: 4 hours PROGRESS REPORT Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Determine how to set up, configure and deploy ElasticSearch on a cloud platform : The cloud platform determination will base on the cost difference between Azure, Google and AWS. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. Hours 15 hours Status Completed Task Migration of raw to json data to Elastic search server : Hours 3 hours Status Completed Task Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. 3. Development and deployment of Web application: Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. Hours 10 hours Status Started. 8 hours' worth of work left. Task Test and measure outcomes Hours 4 hours Status Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React. Understanding the ElasticSearch Stack takes a lot longer than expected. Had to make very conscious decisions what to implement."
https://github.com/nykznykz/CourseProject	Documentation Overview of Code 1.app.py This file contains functionality for the web app which serves recommendations to the user. See demo  here . 2.model.py This file contains logic and algorithms powering the recommendations in the webapp. 3.src/basic_approach/collaborative_filtering.py This file contains an implementation of basic collaborative filtering as mention in class during week 6. 4.notebooks/neural_network.ipynb This notebook contains an approach using neural networks to predict the user's final rating. 5.src/**/scraper.py There are various scraper.py scripts used to scrape and assemble the dataset used. Implementation details This is our stack: 1.Data Wrangling: pandas, numpy 2.Web Scraping: beautifulsoup, requests, regex *Please refer to  scraper.py  for more details 2.Model: scikit-learn, scipy, tensorflow *See  requirements.txt  for more 3.Web Framework: flask *Run  app.py  on localhost:5000 ``` 4.Front End: html & css This is the data that we managed to scrape: Content Data *all_recipes.csv *1100+ Recipes from *460+ Cuisines & Categories Content Data *all_users.csv *55K Users *73K Ratings The website we scraped this data from has much more users and ratings available but this is what we managed to collect with limited amount of time and compute. We tried the following approaches for our recommender system: 1.Basic collaborative filtering a.Suggest recipes that similar users also liked. Similarity is based on what recipes the users have rated and calculated using cosine similarity. 2.Content based filtering a.Suggest recipes similar to the recipes that the user liked. Similarly is based on the categories of the recipe and other content based features and calculated using cosine similarity. 3.Matrix Factorization a.Use singular value decomposition to discover latent factors that describe users and items. The matrix can be reconstructed and used to predict unobserved user-recipe ratings. b.This approach performed the best in terms of test rmse and predicting the ratings the users give to recipes on a held out test set. It is a classic method that performs well on the dataset we scraped. 4.Neural Networks a.Use an embedding layer to learn embeddings for users and items. Multiple dense layers are built on top of the concatenation of the embeddings to learn a function that predicts the rating. In addition content based features can be concatenated to the embeddings and used for prediction. b.It was found that adding further text based features from the title of the recipe slightly improved the test rmse. Overall the test rmse beats the basic collaborative filtering approach but loses to the Matrix Factorization approach. c.The model was found to perform better when more data was scraped and added to the dataset. It is possible that it may beat the matrix factorization approach if even more data was scraped. Usage of Software Webapp See demo  here . 1.Clone the repo ( https://github.com/nykznykz/CourseProject ) 2.Install requirements.txt a.pip install -r requirements.txt 3.Run app.py a.python app.py 4.Navigate to localhost:5000 in the browser. Basic collaborative filtering approach See this  readme  for detailed documentation. Neural Network approach Run the jupyter  notebook  and follow along. Contribution of Group Members Tanmoy: Creating the webapp (app.py), modeling with SVD and other methods (model.py) and web scraping Nikolas: Modeling using deep learning approach (notebooks/neural_network.ipynb), comparison of methods, reports & creation of video Dikra: Web scraping, modeling with basic collaborative filtering approach used in class (src/basic_approach/collaborative_filtering.py) Self evaluation Overall we have completed the tasks we set out to do: 1.Scraping real world recipe data 2.Benchmarking what we learned in class with our own methods 3.Creating a webapp to serve actual recommendations to users We think the following could have been done if more time were available: 1.Scraping more data and constructing a larger dataset 2.Engineering more user and item based features for use together with the neural network Progress Report Link to proposal:  https://github.com/nykznykz/CourseProject/blob/main/Proposal.pdf Completed Tasks We have completed the first task: 1.Scraping a.Scraping reviews & ratings b.Scraping recipes Challenges 1.Dataset size a.Whilewemanagedtoperformscrapingonthewebsitesmoothly,gettingalarge datasetofusersratingsischallenging.Weexpecttoovercomethiseventuallyas wewillcontinuescrapingandgrowingthedatasetalongwiththecompletionof subsequent tasks. 2.Sparsity a.Oneconcernisthatthedatasetmightbesparseaseachrecipeisonlyreviewed byasmallsubsetofusers.Weplantoaddressthisbyframingthe recommendationsasaclassificationproblemandindoingso,wewillbeableto only train on observed ratings. Pending Tasks 1.Construction of dataset a.Basic dataset of user_id, recipe_id, rating b.Construction of features i.Exploratory Data Analysis ii.Feature engineering 2.Setting up baseline approach 3.Model training a.Creating general model architecture b.Tuning of hyperparameters c.Others (e.g. dropout, l2 normalization, embedding layers etc.) 4.Model Evaluation a.Evaluation of baseline & actual model 5.Post analysis 6.Compiling results and writing of report Free Topic: Food Recipe Recommender System Team Members 1.Lee Kar Heng Nikolas Basil (Captain) *nblee2@illinois.edu 2.Mochammad Dikra Prasetya *mdp9@illinois.edu 3.Tanmoy Mishra *tanmoym2@illinois.edu Description For this project, we chose the task of creating a Recommender System for food recipes. From a project perspective, it would be interesting to go from  scraping real world data  (from a recipe platform with millions of users and recipes) to  training a model  to  evaluating it against an approach detailed in class . In general, we think it is also interesting and appropriate for the current situation where people are spending more time staying home and likely looking to pursue/get better at home cooking. Being in this position ourselves, we realise that finding a variety of recipes can be daunting. While we may have a sudden inspiration to cook a specific dish (e.g. lasagna) and are able to find such a recipe through pull based text retrieval methods like the search engine, finding more recipes after that point might be difficult. To tackle this problem, we aim to rely on the strengths of a push based approach such as recommendations to alleviate the decision fatigue and steer users to similar recipes that they are likely to enjoy (e.g. mac & cheese / pasta). Overview of approach 1.Scrape recipes on this site that have some number of reviews: https://www.allrecipes.com/recipe/281306/lime-ginger-chicken-kabobs-with-peanut-sauce/ a.For each recipe, scrape data including the users who gave a rating and the actual rating they gave. From there, we are able to find other reviews that this user also rated. We can then add this to our list of recipes to scrape. b.This will help build an interaction dataset where each review is rated by multiple users and each user has rated multiple recipes. c.Many users provide a detailed description of their review, explaining their rationale for the rating. If time permits, we can use this as user features when scoring future recipes for a particular user or we can mine this information to learn more about a given recipe and generate new features. 2.Scrape metadata for each recipe to be used as features: a.Things like preparation technique b.Ingredients used etc c.Maybe even images if we want to do some CV based recommendations 3.Prepare dataset from the above two steps 4.Setting up a baseline (e.g. basic collaborative filtering) a.We do something similar to what we've learnt in class: i.For a user and recipe pair, average the ratings of that recipe from similar users. ii.Similar users are defined by a similarity threshold, with previous interactions used to calculate similarity. 5.Feature engineering a.User Features: We can learn something about a particular user given the user's past reviews. b.Recipe Features: We can learn something about a particular recipe given what users have said about it as well as the contents of the recipe itself. 6.Model Training a.We aim to predict the rating a user will give to a recipe 7.Model Evaluation a.Some data will be held back for evaluation. b.We will compare the predicted vs actual ratings. Tools 1.Selenium (scraping) 2.Python & Pandas (data manipulation) 3.Tensorflow (Recommender System) 4.Numpy / Scikit Learn (evaluation) Datasets We will create our own dataset by scraping  www.allrecipes.com . The raw dataset, final dataset as well as code used to scrape and process the data will be made available. This dataset is expected to be rich in features and large in quantity https://expandedramblings.com/index.php/allrecipes-facts-statistics/ Expected Outcome We will have a rich dataset to train a good recommender system as well as evaluate on. We aim to beat the baseline approach taught in class. Evaluation RMSE: Actual ratings vs Predicted ratings NDCG: Rank recipes by ratings and calculate NDCG based on actual ratings. Both evaluation approaches should be able to be applied on the baseline and actual approach to enable comparison. Programming language Python Expected Workload We anticipate that we will easily spend more than 20*3 = 60 hours on the project due to the richness of the dataset allowing us to implement many techniques on it as well as the lengthy process likely required to obtain such a dataset. A rough breakdown of the work is as follows: 1.Scraping (Total 15 hours) a.Scraping reviews & ratings (7.5 hours) b.Scraping recipes (7.5 hours) We estimate this based on the given time allocated in MP2.1 (4hrs) while providing some buffer given the more complicated nature of the website which likely has measures to deter scraping. 2.Construction of dataset (Total 20 hours) a.Basic dataset of user_id, recipe_id, rating (3 hours) b.Construction of features i.Exploratory Data Analysis (7 hours) ii.Feature engineering (10 hours) 3.Setting up baseline approach (5 hours) 4.Model training (10 hours) a.Creating general model architecture b.Tuning of hyperparameters c.Others (e.g. dropout, l2 normalization, embedding layers etc.) 5.Model Evaluation (5 hours) a.Evaluation of baseline & actual model 6.Post analysis (5 hours) 7.Compiling results and writing of report (5 hours) Any extra time from unexpected surpluses can be channelled into subtasks like sentiment analysis/topic modelling for analysis & insights or used directly as model features. CourseProject A video presentation can be found here Documentation for the project can be found here Recipe Recommender System Driven by my curiousity of how Netflix, Youtube and Spotify serve personalized recommendations, I decided to learn how to create my own recommender system. Machine Learning Problem: Given a person's preferences in past recipes, could I predict other new recipes they might enjoy? I created Seasonings, a Recipe Recommender System. The motivation behind this web app is to help users discover personalized and new recipes, and prepare for grocery runs! I received a lot early positive feedback and plan future improvements to the UX and model. I had a lot of fun making this, and plan to use this whenever I need a jolt of inspiration in the kitchen! Data Data was scraped from allrecipes.com, as there was no public API. I narrowed the scope to focus on Chef John's recipes (from FoodWishes.com). Content Data all_recipes.csv 1100+ Recipes from 460+ Cuisines & Categories Content Data all_users.csv 55K Users 73K Ratings Tech Stack Data Wrangling: pandas, numpy Web Scraping: beautifulsoup, requests, regex Please refer to scraper.py for more details Model: scikit-learn, scipy See requirements.txt for more Web Framework: flask Run app.py on localhost:5000 ``` Front End: html & css Models Please refer to model.py Collaborative Filtering - Suggest recipes that other users similar to you also liked (Cosine Similarity) If I liked Spaghetti Al Tonno, and another user similar to me liked Perfect Prime Rib and I haven't tried it, the model would recommend that recipe. Content Based Filtering - Suggest recipes that are similar to recipes that you like (Cosine Similiarity) If I liked Spaghetti Al Tonno, the model would recommend Italian Meatballs, because Italian Meatballs are similar to Spaghetti, in terms of the categories both recipes share (Italian, World Cuisine). Matrix Factorization - Suggest recipes that you like, uncover latent factors, in a lower dimensional space (Singular Value Decomposition) If I liked Turkey, and I liked Cranberry Sauce, the model would recommend Pumpkin Pie because it picked up a latent factor that you liked Thanksgiving dishes, where the other models would not be able to. Model Evaluation My final model was a hybrid recommender that tackled the cold-start problem with a content recommender, augmented with user preferences, and factorization to rank recipes based on a voting classifier rule. Recipe Recommender System2Motivation*Increased interest in home cooking during this period.*While we usually know what we want to cook initially, we may run out of ideas at some point.*Require the help of a push-based approach like recommender systems.*Use case: I have tried and liked recipes for italian cuisine. Recommend me new recipes based on this informationSolutionSolution*Seasonings is a webapp that can recommend new recipes under a wide variety of contexts*Example contexts:*People with similar tastes also like...*Because you liked <Recipe Name>...*Tastebreakers...*All the user has to do is provide 5 anchor recipes for Seasonings to generate recommendations.Demo
https://github.com/oboffil/CourseProject	"Coordinator: Omar N Boffil NetID: Oboffil2 Oboffil2@illinois.edu Group Name: Oboffil Topic: Option 2.1 MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Metapy and its toolkit are among the best-known libraries used in python for Text mining and another usability. Still, this library is not compatible will all python versions, making it challenging to create an environment where you can use its power to build good code. In this documentation, I will give you all the steps and code to create an environment using anaconda to install Metapy and other useful libraries that you can combine with Metapy to make your code more efficient. This process will use one of the most recent python versions that will benefit users who like to work in more updated environments and let you use newer python packages and combine them with Metapy. We will do this process as simple as possible and avoiding complicated code and frustration. This documentation will work for Windows and macOS operator systems. Also, we will use anaconda GUI to make it as simple as possible. The code to install Metapy and its useful toolkit will be included in an anaconda file and a text file with all the libraries' needs. To ensure that this software works and created the right result, make sure to read the code's comment, and follow the steps described below. This will guarantee the success of this environment for Metapy. 1) Installation process: First, we will need to install anaconda if you don't have it yet. (In case you have it installed already on your computer, you can move to step 2). Otherwise, you will need to: 1.1) Go to https://docs.anaconda.com/anaconda/install/ 1.2) Select your operator system for Windows or macOS 1.3) Fallow the instruction on anaconda website to complete the installation by using the GUI or the terminal on your computer After you have installed anaconda in your computer, we can move on and create an environment that works with Metapy 2) Creating an environment with Python 3.7. Now that anaconda software is installed on your computer. You can open it and fallows: 2.1) Click on ""environment"" 2.2) Click on ""Create"" to create a new environment where we are going to work 2.3) Add a name to your environment, select Python version 3.7, and click on ""create"". This will take a few seconds, and it will create a python environment where you can start running code 3)- Getting the source code and resources: Now that we created the environment with Python 3.7 we can implement the code that will let us use the different libraries, but first, we need to download the source code and packages from https://github.com/oboffil/CourseProject.git Download the download resources.zip from the repository above and unzip it to get the folder that contains the source code (Implementation.ipynb), and the text file (package-list.txt) with the Toolkit packages that will help you to set up Metapy and other useful libraries. 4)- Implementing the source code and packages. With all tolls needed, we can complete the installation of Metapy in our environment: 4.1) Go back to anaconda and click on ""Home"" 4.2) On ""Application on"" select the environment that you just created. This will take a few seconds 4.3) Install and launch Jupyter notebook. If you haven't installed it yet, it will take a few seconds; otherwise, it will launch automatically. 4.3) Look for the directory where you saved the unzipped download resource folder mentioned in step 3 and click on Implementation.ipynb. 4.4) This contains the source code to install the libraries and packages for this tutorial. Read the comment of the source code and make sure you delete the comments before running each line. NOTE* if you get an error trying to run the code directly from the file, go back to the directory, create a new Python 3 and copy and paste the code into the newly created file. Just make sure this new file is in the same directory as the package-list.txt. This will install Metapy and some of their useful toolkit in the environment. After the installation is completed, you can import the libraries in any other user interface that you prefer in anaconda as long as you use the environment where you installed the packages and libraries initially. For example, you could import these libraries on JupyterLab, Qt Console, Jupyter Notebook, etc. Coordinator: Omar N Boffil Oboffil2@illinois.edu Group Name: Oboffil Topic: MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Project Progress Which tasks have been completed? The environment test that can handle Metapy and the toolkits is done. Also, the code with the libraries need is complete. Other codes to import the libraries and install the different tools are completed as well. 2) Which tasks are pending? I need to complete the documentation and create the tutorial video to guide the user to develop these libraries' in an anaconda environment. 3) Are you facing any challenges? Yes, I haven't found a way to run the installation of anaconda and the environment in just one run, also creating the environment by code has some issues. I'm thinking of using the GUI to implement this step and install anaconda directly from their website and then use the code to install the different packages. Coordinator: Omar N Boffil NetID: Oboffil2 Oboffil2@illinois.edu Group Name: Oboffil Topic: Option 2.1 MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability The Metapy installation and usability took me more than 4 hours to figure out how to install a compatible version of python that I can use to complete the assignment in this course. My goal with this project is to create a system and a tutorial for installing and using the tool on different platforms that will help future students complete this installation and used the toolkits without feeling the same frustration I felt at the beginning of this class. I will show how to set up an environment in the Windows and macOS operation system using anaconda and the latest Python versions to complete the different tasks and integrate the other popular toolkits. Also, I will create a system in python that will contain all necessaries steps to install this environment and the different packages need it CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Coordinator: Omar N Boffil Oboffil2@illinois.edu Group Name: Oboffil Topic: MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Tutorial Video: https://youtu.be/m9CEqIl3ADg https://youtu.be/m9CEqIl3ADg https://youtu.be/m9CEqIl3ADg"
https://github.com/oransum/CourseProject	"CS410 Text Information Systems(Fall 2020) Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Edward Ma - kcma2(Captain) Oran Chan - wlchan2 2. Which competition do you plan to join? We are planning to work on the competition of Text Classification 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We are interested in SVM, XGBoost and especially the BERT model which has proven performance on downstream tasks. Hence, we decided to use PyTorch to fine-tune this state-of-the-art method to tackle various classification tasks. Toxic comment classification is one of my experiences working on BERT. 4. Which programming language do you plan to use? Python Text Classification Competition: Twitter Sarcasm Detection Oran Chan (wlchan2) and Edward Ma (kcma2) ABSTRACT Sarcasm detection is a specific case of sentiment analysis. Instead of classifying all kinds of motion, this task only focuses on sarcasm. Given a text as input, the detection model outputs whether it is sarcastic or not. The most challenging part is that judgement of sarcasm detection is not very clearly defined or it is subjective. In this classification competition, we suggest using less human and computer resources to achieve a better result. We demonstrate how synthetic data helps to boost up model performance with manual effort. 1.INTRODUCTION In this classification competition, the training data size is 5000 with equal distribution. This size is relatively small in natural language processing (NLP). Therefore, we proposed to use transfer learning and data augmentation strategy to tackle this problem. For the transfer learning part, we will train our model based on a pre-trained model which was trained on a very large corpus to solve some basic NLP tasks. It is a promised way to start with them instead of training from scratch. It does not only provide a converge word embeddings but also shortens training time. For the data augmentation part, we leverage contextual word embeddings training methods to generate synthetic data based on limited training data. 2.DATA PROCESSING Given 5000 twitter text, we split it into training dataset and evaluation dataset with 9:1 ratio. In other words, the training dataset includes 4500 records while evaluation dataset includes 500 records. 2.1 Preprocessing As mentioned before, we adopted a pre-trained model which can take care of lots of data processing. We do not need to do lots of feature engineering based on pure text but tokenizing text into subwords. Instead of using words as a feature, we decode to use subwords. For instance, ""language"" can be represented by ""lang"", ""uage"" tokens. One of the major benefits is that it can handle out-of-vocabulary (OOV) problems. Giving that we use 26 (or 52 if case sensitive) characters, we can represent all English words. Another advantage is that it can converge rare word's embeddings as we may break down a single rare word into multiple tokens. Also, subword algorithms leverage an affix behavior to further coverage subword embeddings. In English linguistics, an affix is a morpheme that is attached to a word stem to form a new word or word form. For example, a ""dis"" prefix means opposite while a ""less"" suffix means no. The following part covers Byte pair encoding (BPE) [1] and WordPiece [2] subword algorithm. Example of Word Tokenization 2.2 BPE BPE is proposed by Sennrich et al. (2016) and the general idea is counting the frequency of subwords up to a predefined maximum number of subwords. BPE is adopted by RoBERTa [5]. The algorithm is: 1.Prepare a large enough training data (i.e. corpus) 2.Define a desired subword vocabulary size 3.Split word to sequence of characters and append the suffix ""</w>"" to the end of word with word frequency. So the basic unit is character in this stage. For example, the frequency of ""low"" is 5, then we rephrase it to ""l o w </w>"": 5 4.Generating a new subword according to the high frequency occurrence. 5.Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1. 2.3 WordPiece WordPiece is proposed by Schuster and Nakajima (2012). The idea is the same as BPE except the criteria of forming new subwords. New subwords will be formed based on likelihood but not the next highest frequency pair. WordPiece is adopted by BERT [6] The algorithm is: 1.Prepare a large enough training data (i.e. corpus) 2.Define a desired subword vocabulary size 3.Split word to sequence of characters 4.Build a languages model based on step 3 data 5.Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model. 6.Repeating step 5 until reaching subword vocabulary size which is defined in step 2 or the likelihood increases falls below a certain threshold. 2.4 Data Augmentation There are lots of different ways to generate synthetic data. One of the typical ways is replacing words by synonyms [3] over NLTK [4] library. The limitation of using NLTK's synonyms is that it does not consider context. Considered the nature of sarcasm detection, we decided to levearge neural network models to find similar meaning words when considering context. The mechanism is picking a word randomly and using a neural work model to predict the possible word to replace it by providing whole content to the neural network models. We adopted BERT and RoBERTa neural network models to perform this data augmentation and the ration is 1:1. Here is the mechanism: 1.Pick a word from input 2.Replace the picked word by reserved token (e.g. [MASK]) 3.Tokenize input with masked token 4.Feeding tokenize tokens to masked language model 5.Replace the picked word by language model prediction. Flow of Augmentation As nlpaug [7] implemented over 10 different data augmentation, we decided to leverage this library for data augmentation. In the evaluation, we tried different sizes of synthetic data to see how synthetic data affect the model performance. Also, we adopted two neural network models for comparison. Example of augmented data Type Content Original @USER @USER Stephen Jones finally losing it on Twitter by claiming the Liberty is a great stadium . Never mind his thoughts on Sarries and the salary cap breach - this is a new low even for him . <URL> Augmented Data #1 @USER @USER Stephen Jones finally losing it  at  Twitter  suddenly  claiming Kings  Liberty is a great stadium . Never mind  after  thoughts  over  Sarries and the  transfer tax  breach - this is a  historic  low  from meeting  him . <URL> Augmented Data #2 @user @user stephen jones finally losing it on twitter by claiming the  series  is its great  success  . never mind his thoughts on  soccer  but  high  salary cap pro  - football is a  serious  low even for  david  . < url > 3.MODEL ARCHITECTURE In this text classification, we evaluated both BERT and RoBERTa model and we finally picked RoBERTA as it outperforms BERT model based on our evaluation dataset. 3.1 BERT BERT (Devlin et al., 2018) is a method of pre-training language representations, meaning that it was trained as a general-purpose ""language understanding"" model on a large text corpus (like Wikipedia), and adopting it for downstream NLP tasks that we care about. BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP. BERT uses three embeddings to compute the input representations. They are token embeddings, segment embeddings and position embeddings. ""CLS"" is the reserved token to represent the start of a sequence while ""SEP"" is a separate segment (or sentence). Token embeddings are subword embeddings which represent the subword itself. Segment embeddings only include two embeddings which represent the first segment of input and second segment of input. Position embeddings refers to the position of the subword in the input. Segment embeddings help to distinguish two segments in some NLP downstream tasks such as question and answering tasks. For classification tasks, we only use a single segment. Position embeddings reflect the location as the same word may have different meanings in different positions of text. For BERT's training setup, it uses the Masked Language Model (MLM) and Next Sentence Prediction mechanism. By masking some tokens randomly, using other tokens to predict those masked tokens to learn the representations. For example, the original sentence is ""I am learning NLP"". Assuming ""NLP"" is a selected token for masking. Then 80% of time, it will show as ""I am learning [MASK]. For the Next Sentence Prediction approach, it targets to learn the relationship between sentences. The objective is classifying whether the second sentence is the next sentence or not. 3.2 RoBERTa Liu et al. (2019) studied the impact of many key hyper-parameters and training data size of BERT. They found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. RoBERTa (Robustly optimized BERT approach) is introduced and performance is either matching or exceeding original BERT. RoBERTa is developed based on BERT. By applying some modifications, it outperformed BERT model performance according to Liu et al (2019) experiments. First of all, it uses a larger training data. On the other hand, RoBERTA uses dynamic masking instead of static masking. Dynamic masking means that the masked token will be different every time. 4.EXPERIMENTS In the experiments, we mainly compare two BERT and RoBERTa with different numbers of augmentation data. The size of original training record and evaluation record are 4500 and 500 respectively while size of augmentation data are various. Taking experiment #2 as an example, it used 9000 augmentation data and 4500 original data for training. Instead of using a number of epochs for comparison, we use global steps. An epoch is one full pass through the training set, so that every sample gets the chance to be seen by model. Global steps refer to the number of batches seen by the model. For example, 1 epoch includes 4500 training records when it does not include any augmentation data while there are 13500 training records in 1 epoch when introducing 9000 augmentation data. Therefore, using global steps for comparison is a better approach. 4.1 BERT vs RoBERTa This experiment aims at demonstrating the model performance between BERT and RoBERTa. RoBERTa shows it converges to minima faster than BERT. RoBERTa reaches its best F1 score at 6756 global steps while BERT reaches at 14000 global steps. Although BERT is sligher better than RoBERTa 0.004, we decided to adopt RoBERTa after balancing between computation and model performance. Experiment Result #1 The following experiment is comparing model performances between BERT and RoBERTa when there are augmentation data. We noticed that RoBERTa outperforms BERT in 3 check global step checkpoints. Global Step BERT RoBERTa Precision Recall F1 Precision Recall F1 1126 0.797 0.768 0.782 0.649 0.960 0.774 2815 0.741 0.904 0.814 0.776 0.764 0.770 6756 0.749 0.872 0.806 0.767 0.880 0.819 7319 0.796 0.748 0.771 0.822 0.756 0.787 11823 0.816 0.708 0.758 0.805 0.76 0.782 14000 0.777 0.876 0.823 0.771 0.860 0.813 Global Step BERT RoBERTa Precision Recall F1 Precision Recall F1 Experiment Result #2 4.2 Data Augmentation Besides model architecture, we want to see whether synthetic data helps to improve performance. Different sizes of augmentation data are adopted for comparisons. It includes 2250 (0.5 times of original dataset), 4500 (same size of original dataset), 9000 (2 times of original dataset) and 45000 (10 times of original dataset). In this experiment, we want to evaluate whether augmentation data can boost up model performance. Due to the setup, experiment #1 does not include six thousandths global step so we use the nearest one which is 6756 global setup as reference. From the below figure, we can see that significant improvement when the size of augmentation data increased to 45000. Experiment Result #3 4.3 Final Submission After conducting previous experiments, we decided to further train a RoBERTa with 45000 augmentation data for final submission. Between 0 and 10000 global steps, F1 score improved quickly. After that it converged slowly until 18000 global steps. After 18000 global steps, F1 score drops until 32000 global steps. Finally, the model only predicts either SARCASM or NOT_SARCASM for the whole validation dataset. Therefore, we picked the 18000 global step checkpoint as our final submission. Finally, our precision, recall and F1 are 0.687, 0.816 and 0.746 respectively. 2000 0.711 0.876 0.785 0.736 0.880 0.801 4000 0.751 0.748 0.750 0.779 0.784 0.781 6000 0.803 0.768 0.785 0.762 0.884 0.819 # Augmentation Size Global Step Precision Recall F1 1 0 6756 0.767 0.880 0.819 2 2250 6000 0.805 0.840 0.822 3 4500 6000 0.782 0.804 0.793 4 9000 6000 0.762 0.884 0.819 5 45000 6000 0.926 0.848 0.885 # Global Step Precision Recall F1 1 2000 0.710 0.840 0.769 2 4000 0.801 0.852 0.826 Experiment Result #4 4.4 Execution We prepared a specific python file for each experiment. All of them will invoke the same python class with different parameters. The following table shows the mapping between python execution and experiment. To generate an answer for final submission, we use the following script. 3 6000 0.926 0.848 0.885 4 8000 0.924 0.976 0.949 5 10000 0.980 0.976 0.978 6 14000 0.992 0.968 0.980 7 16000 0.980 0.976 0.978 8 18000 0.980 0.988 0.984 9 20000 0.980 0.676 0.980 10 24000 0.984 0.976 0.980 11 30000 0.988 0.976 0.982 12 32000 0.98 0.980 0.980 13 34000 0 0 N/A 14 36000 0 0 N/A 15 136000 0 0 N/A Python File Model Augmentation Size run_bert_without_aug_epoch.py BERT 0 run_bert_with_aug_9000.py BERT 9000 run_roberta_without_aug_epoch.py RoBERTa 0 run_roberta_with_aug_2250.py RoBERTa 2250 run_roberta_with_aug_4500.py RoBERTa 4500 run_roberta_with_aug_9000.py RoBERTa 9000 run_roberta_with_aug_45000.py RoBERTa 45000 4.5 Presentation You may also check out this link ( https://mediaspace.illinois.edu/media/t/1_fpgtkn68 ) for our video presentation. 5.FUTURE WORK Besides data augmentation, we also brainstormed other ideas which include gathering more twitter data, evaluating other state-of-the-art models such as ELECTRA [8] and tuning hyperparameters. 6.REFERENCES 1.R. Sennrich, B. Haddow and A. Birch. Neural Machine Translation of Rare Words with Subword Units. 2015 2.M. Schuster and K. Nakajima. Japanese and Korea Voice Search. 2012 3.X. Zhang, J. Zhao and Y. LeCun. Character-level Convolutional Networks for Text Classification. 2015 4.E. Loper and S. Bird. NLTK: The Natural Language Toolkit. 2002 5.Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019 6.J. Devlin, M. Chang, K. Lee and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018 7.E. Ma. NLP Augmentation.  https://github.com/makcedward/nlpaug . 2019 8.C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M.Matena, Y. Zhou, W. Li and J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. 2019 Python File prediction.py Progress Report for Text Classification Competition: Twitter Sarcasm Detection Oran Chan (wlchan2) and Edward Ma (kcma2) Progress Made -Completed data exploration and exploitation. Having 5000 equal distributed label training data. Split data to training set and evaluation set and sizes are 4500 and 500 respectively. -Possible to find external data to enrich the dataset but considering the efforting of searching and data processing. -We proposed to generate synthetic data instead of looking for external data as it involves lower effort. Generated different sizes of synthetic data for evaluation. From 0.5 times to 10 times. -Evaluated deep neural network model architecture for building classification model -Trained model based on the pre-trained neural network model (BERT and RoBERTa) and achieved a good result which exceeds the baseline. Remaining Tasks -Refractor coding for easier understanding -Summarize the effectiveness of synthetic data. It includes the comparison among different sizes of synthetic data and models. -Prepare the documentation. The focal point is how we can leverage synthetic data to boost up model performance with minimum human effort. -Prepare the presentation material about what we did and how it works Challenges -Spends time on understanding the relationship between response and main thread content. -Learn subword algorithms such as WordPiece (adopted by BERT) and Byte Pair Encoding (adopted by RoBERTa) -Learn transformer architecture (i.e. the base architecture of BERT and RoBERTa models) -As using transformer models, computation resource requirement is high. It takes several days to complete several epochs. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/paulzuradzki/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/pdwivedi08/CourseProject	CS 410 - Text Information System: Text Classification Competition Team Members: Harsangeet Kaur (kaur13@illinois.edu): Team Member Pradeep Dwivedi (pkd3@illinois.edu): Team Lead Competition: Text Classification Competition Programming Language: We are planning to use Python as the programming language Proposal Details: We are new to machine learning and have no prior experience implementing language processing models. However, we are very interested to learn SOTA language processing models. We plan to use LSTM, GPT3 and other transformers to achieve best classification results. Progress Report for Tweet Classification Competition About the project: Our team is working on classifying the tweets in sarcasm and non-sarcasm categories. We intend to use deep learning algorithms in natural language processing to classify the tweets and get highest level of accuracy, thus, not only beating the baseline but will also aim to secure the top rank in the competition. Progress made so far: We've analyzed the problem carefully. We looked into multiple deep learning algorithms which can be used to solve our problem. We also looked into multiple transformers based deep learning algorithms too, to solve our problem. We did lot of data cleaning in the input file, so as to feed the right input to the classification algorithm. We used Python's Pandas, Reg Ex on Anaconda's Jypyter notebook for data cleaning. Based on the algorithms, which we tried so far, we've been able to solve the tweet classification problem and beat the baseline. We are currently ranked #16 in the leaderboard. We'll however try to improve on our rank. We've used Google's T5 based classification transformer to receive best result so far on the classification problem. On the Leaderboard in Livelab, our submission can be found with the id: pdwivedi08 Remaining Tasks: We'll continue to try to improve our ranking in the competition. Additionally, we've following tasks left: 1. Detailed documentation of the project 2. Presentation on the project execution 3. Code cleaning and comments update Challenges: We don't have any major challenge at this time in the project completion. We did face some challenge earlier to decide the right algorithm to use and our research and self-study did help in that. Tweet Classification Competition About the Project We've worked on the text classification competition project for the tweets. The train and test datasets are provided as part of competition and we intend to use current state-of-the-art machine learning NLP algorithms to beat the baseline on this text classiciation competition project. Presentation Link on Youtube Please refer below youtube link for the voice-over presentation for our project: https://youtu.be/H1xQwJkV5cA Team members: Harsangeet Kaur (kaur13@illinois.edu): Team Member Pradeep Dwivedi (pkd3@illinois.edu): Team Lead Our submission can be found in the spreadsheet available in CMT, with the ID: pkd3@illinois.edu Our submission on the Leaderboard in Livelab can be found for the ID: pdwivedi08 Overview This software can be used to classify tweets in Sarcasm and Not-Sarcasm categories. This can't be however, used for any other text classification or sentiment analysis with same level of accuracy or F1-score. This software achieves the high level of precision, recall and F1 score as against the generic transformers, since it has been especially trained on the tweet classification. Implementation Documentation We've made use of Google's T5 based fine-tuned transformer for twitter sarcasm detection. This model has been trained to identify sarcasm on tweets. We've used the Google Colab notebook to train the model and it took nearly 12 hours to train the model with the given data, on single GPU. We used the Trainer API from Huggingface to write the training code as it's easier to use. Also, we used the autotokenizer from the transformers library in Huggingfacce. We cleaned the test data for training and testing in such a way that the tweets are taken in correct sequence - first the orginal tweet and then it's responses in the chronolocial order. Also, we've removed all the filler words using regex and regular python functions from the tweets before using them for training and testing. We defined a function eval_conversation, to evaluate the curated tweets one-by-one and provide the output in Sarcasm and Not-Sarcams categories. We tried support vector machines (SVM) and T5 based transformer for this project. We got following values of precision, recall and F1 score with both these algorithm: precision recall f1 SVM 0.48314606741573035 0.14333333333333334 0.22107969151670953 T5 based 0.7030114226375909 0.7522222222222222 0.726784755770263 The second approach i.e. the use of T5 based transfomer helped us to beat the baseline. Our final execution matrics can be viewed on the Leaderboard in Livelab, for the id: pdwivedi08 Usage Documentation All the code of the software is written in the jupyter notebooks, which can be opened from Anaconda IDE. 'classifyTweets.ipynb' is the main notebook which has the code to execute the test dataset. The 'test.jsonl' is stored inside the Data directory and the directory is included in the github. All other libraries needed to execute this code, are part of the 'classifyTweets.ipynb' notebook and would be imported when the notebook is executed. Therefore, no additional installation of any module is needed. The project github has the video demostration of the code execution as well and that can be used to install and run this software. Since the model is running T5 based transformer and the code has few displays, it will take around 5-7 minutes for the execution of the whole notebook on a macbook pro of 8 GB memory. The execution speed in-general will vary based on the hardware of the machine, used for running the notebook. For any further question related to the installation or the working of the software, please contact our team, at the below email IDs: Harsangeet Kaur (kaur13@illinois.edu) Pradeep Dwivedi (pkd3@illinois.edu) Detail of the Contributions of Team-members Our team didn't has any prior background in natural languange processing(NLP) or machine learning (ML). Therefore, we started with understanding ML in the context of NLP and reading about it, online and on forums. Huggingface.co greatly helped us understanding the deep learning aspect of ML on NLP. Harsangeet Kaur tried support vector machine algorithm for text classification whereas Pradeep Dwivedi tried transformers for solving the problem. Together, we worked on the data cleaning, model training, software documentation and preparing the final presentation. References https://huggingface.co/mrm8488/t5-base-finetuned-sarcasm-twitter stackoverflow.com https://huggingface.co/transformers/main_classes/trainer.html https://www.w3schools.com/python/python_regex.asp https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34
https://github.com/pebblespot/CourseProject	"ReadingAssistant:ProjectProgressReportChristopherRock(cmrock2)(Captain)ZichunXu(zichunx2)KevinRos(kjros2)Due29November20201ProgressWebeganbyplanningaroad-mapforourproject,whichconsistedofasetofgoalsandgroupmeetingcheckpointstoevaluateprogress.Additionally,wecreatedaGitHubrepositorywhichholdsourprojectcode.Followingthis,weimplementedthebasicstructureforourreadingassistant.Onstart-up,textdocuments(directorypathprovidedbyuser)areloadedbytheassistant.Duringloading,thedocumentsareprocessedandaddedtoaninvertedindex.Thesedocumentsareconsideredtobethepreviously-readdocumentsbytheuser.Oncetheloadingiscomplete,theassistantwaitsforapathtoatext le(unseendocument).Giventhispath,theassistantranksthepreviously-readdocumentsusingtheunseendocumentandreturnsthemostsimilarreaddocumentnamestotheuser.Wealsoprovidedmethodsforausertoaddandremovepreviously-readdocuments.Currently,theassistantcalculatessimilarityusingtheOkapiBM25rankingfunctionalongwithvariousoptimizationtechniques,includinganinvertedindex.Toheuristicallygaugethee ectivenessofthismethod,eachteammembercollectedapproximately8-10documents.Thesedocumentswereloadedasthepreviously-readdocuments,andadditionaldocumentswereprovidedastheunseendocuments.Fromthepreliminaryexamination,theresultsseempromising.Thecodeiswritteninamodularfashion,sothatwecaneasilyextendtheassistanttousedi erentsimilarity/di erencemeasuresandmethods.Inadditiontodocument-levelBM-25,wehaveimplementedparagraph-levelBM25whichallowsmoredetailedevaluationofunseendocumentscomparedtoseendocuments.WehavealsousedtheexternallibrarygensimtoincludeLatentSemanticIndexingatadocumentlevel,withdocument-levelsimilarity.Thisiscurrentlyaseparatescriptandwillbeintegratedforthe nalproject.Ourplannedextensionsarediscussedinthefollowingsection.2RemainingTasksOur rstremainingtaskistoaddmore ne-grainedsimilarityanddi erencemeasurementtech-niques.Regardingtherankingfunctionitself,weareconsideringaddingpre-trainedwordembed-dingsandcosinesimilaritytoe ectivelyassesssimilarityanddi erencesonawordandsentencelevel.WewillcombinethiswithourOkapiBM25calculationstocompareseenandunseendocu-mentsonaparagraphgranularity.Oursecondremainingtaskistocreateauser-friendlycommandlineinterface.Thiswillallowtheusertoeasilyaddandremovedocuments,andviewthesimilaritiesanddi erencesbetween1theseenandunseendocuments.Ideally,weplantooutputadetailedsummarythatdescribestherelationshipbetweenthedocuments.3ChallengesandIssuesOneparticularchallengethatwe'veencounteredistheevaluationofourreadingassistant'se ec-tiveness.Becausewehaven'tencounteredadatasetthatexactly tsourneeds,weplantoaddressthisissuebyincorporatingafeedbackmechanismintheterminal.Thisway,userscanprovidereal-timefeedbackthatwecandynamicallyincorporateintothereadingassistant.2 FreeTopic:ReadingAssistantChristopherRock(cmrock2)(Captain)ZichunXu(zichunx2)KevinRos(kjros2)25October20201ProjectProposal1.1OverviewIntheearlymonthsof2020,thenownamedSARS-CoV-2viruswasrapidlyspreadingacrossthecountriesofthePaci candjumpingtonewlocationsineverycorneroftheglobe.The upandemicthathadlongbeenpredictedwashappening-exceptitwasn'tthe u.FirstinChina,andsoonthroughouttheworldtherewastalking,writing,researching,andpublishingaboutthevirus.Thiswasthe rstpandemicinthesmart-phoneera,andtheonlythingthatseemedtospreadfasterthantheviruswasinformation.Governments,medicalorganizations,companies,andeveryinstitutionimaginablebeganpushingoutnotjustinformation,butalsoguidelines,rules,andpolicies.\Informationoverload""issomethingpeopleintoday'ssocietyareaccustomed.Mostpeopledevelopmethodsofcopingwiththehugeamountofinformationavailable.We lterthingsthroughtrustedsources,prioritizeinformationthatisactionable,andchangeourmentalmodelofthesituationasnecessary.Howeverinthefaceofanewdangeroussituationeverypieceofinformationbecomespotentiallycritical.Dynamicsituationssuchasapandemicrequirequickreactionstonewknowledge.Itisourexperiencethatthoseinleadershipanddecision-makingrolesarepushedalargenumberofdocumentsandexpectedtobeuptodateonthisrapidlyexpandingcorpusofinformation.Newsituationscreateorganizationalchaos,andtheindividual'sstrategiestolimitinformationoverloadbreakdown.Trustedsourcesaremorediculttoidentifywheninformationcomesfrommanysourcessimultaneously.Andbecauseallinformationispotentiallyactionable,allinformationmustseeminglybereviewed.Initiallythechallengeissimplytoreadandunderstandthedocumentssent.Howeverthedicultyquicklybecomesidentifyingwhatisnewknowledge.Newdocumentsmayhavesigni cantoverlapwithpriorknowledge.Di erencesbetweendocumentsmustbereviewed,andoftentheprogressionofchangesisimportant.Informationretrieval,textmining,andrecommendersystemshavedevelopedalgorithmicstrate-giestoidentifyandextractusefulinformationfromtext-basedknowledge.Thefocusofthesetoolshasgenerallybeentopullrelevantdocumentsvia(search),orpushpotentiallyinterestingdocu-ments(recommender).Thesetechniquescanbemodi edtoassistareaderinidentifyingnewusefulinformation.Ourgoalistocreateareadingassistanttoolthatallowsausertomaintainacollectionof\seen""or\read""documents(re ectingthecurrentknowledgeoftheuser)andprovidesnoveltyscoresbasedonnewdocumentsintroducedtothecollection.Givenanewdocument,thereadingassistanttoolwillcomparethedocumenttoall\seen""documents,andprovidetheuserwithmeasuresindicating1howthenewdocumentdi ersfromthedocumentcollection.Inthisway,potentiallyusefulnewdocumentscanbeecientlyprioritizedbytheuser.1.2ProjectDescriptionThetaskofourfreetopicistodesignandimplementareadingassistantsoftwaretoolthathelpsusersdeterminethenoveltyofnever-before-seendocumentsbasedonpreviously-seendocuments.Eachuserwillhaveacollectionofreaddocuments,knowntothereadingassistant.Whentheuserisprovidedanewdocument,thereadingassistantwillquicklyscantheuser'sreaddocumentcollectionandscorethenewdocument(orsectionsofthenewdocument).Thisscorewillre ecthownovelthenewdocumentisrelativetothepreviously-readdocuments.Ideally,thiswillprovidetheuserwithahigh-levelunderstandingoftheimportanceofthedocument,allowingtheusertobetteroptimizetheirtime.Therearemanyuserswhowouldbene tfromsuchatool.AswediscussedinSection1.1,medicalresearchersanddoctorscoulduseatooltohelpsiftthroughandsortthevastamountofinformationprovidedduringeventssuchasaglobalpandemic.Inacademia,researcherscouldleveragethistoolto lterresearchpapersforinformationrelativeornoveltotheircurrentwork.Outsideofacademia,thegeneralpubliccouldusethistooltoaugmentonlinebrowsing,assuchatoolwouldallowthemtoquicklylookuppreviously-readdocumentsandnewsarticles,andinterpretnewarticlesinthecontextofwhattheyhavealreadyread.Toourknowledge,nosuchtoolcurrentlyexits.WewillusetheMetaPytoolkit1toprovideasuiteofrankingandevaluationmethodsforourtool,alongwiththepublically-availableCORD-19Coronavirusdocumentdataset2asourtrainingandtestingdata.Aditionally,wewillusethePythonprogramminglanguage.Tocreatethetool,wewillbeginbyleveragingourunderstandingoftheBM25rankingalgorithm(whichmeasuresdocumentsimilarity)toconstructan\invertedBM25""distancefunction(whichmeasuresdocumentdi erence).Inordertodemonstratetheusefulnessofourtool,wewillmanuallyscoreasubsetofdocumentsintermsofsimilaritytoacollectionofseendocuments.Insomecasestheseendocumentswillberandomlyselected,andinothercasestheywillallbeofacertaintopic.Then,wewillpassthescoreddocumentstoourtoolandseeifitcategorizesthedocumentsinlinewithourmanualscoring.WediscussaroughtimelineinSection1.31.3WorkloadWewillspendthe rst20hoursde ningandunderstandingtheprojectscope.Here,wewillbeginbyde ningwhatitmeansfortwodocumentstobedistinct(orsimilar).Wewillalsoattempttoquantitativelyde neadistancemeasurebetweendocumentsorparagraphs.Additionally,wewillde nethescopeof\seen""and\unseen""documents.Thatis,wemightneedtoassumethatthereaderhasreadmanydocumentsforrecommendationtobee ective(otherwisemanydocumentswillbeconsiderednovel).Followingthis,wewillspendthenext20hoursimplementingourdistancemeasureusingPythonandMetaPy.WewilllikelybeginwithaninvertedversionofBM25,butitisdiculttoknowhowwellitwillworkformeasuringdocumentdi erence.Thus,weexpectthatasigni cantportionofthe20hourswillbetestingoutanddebuggingvariousimplementations, ne-tuningparameters,curatingthetrainingandtestingdocuments,andadjustinganyinitialassumptionsinlightofnew1https://github.com/meta-toolkit/metapy2https://www.semanticscholar.org/cord192evidence.Oncewedecideonaspeci cimplementation,wewillde nevariousevaluationmeasuresinordertodeterminethee ectivenessofourtool.Theremaining20hourswillbespentevaluatingthetoolandtuninganyparameters.Giventhesubjectivityofrelevancescores,wewilllikelyneedtomanuallyjudgedocuments.Forexample,thiscouldincluderandomlychoosingaset\alreadyseen""documents,andhand-labelingadditionaldocumentsas\verysimilar"",\somewhatsimilar"",or\notsimilar""to\alreadyseen""documents.Then,wewouldseeifthetool'sscorescorrespondedtooursimilarityclassi cations.Inthecasethatweoverestimatedthetimeittakestocompletetheaforementionedtasks,wewill lltheremainingtimebymakingourtoolmorerobust,moreuser-friendly,ormoreexpansive.Thiswillbeaccomplishedbyintroducingvarioussimilarityscoremeasures(suchaswordembeddings),acommand-lineinterface,andconsideringadditionaldatasets,respectively.3 Reading Assistant CS410, Fall 2020 Christopher Rock (cmrock2) Zichun Xu (zichunx2) Kevin Ros (kjros2) Video presentation: https://youtu.be/RO351eoZ1ZU ### Documentation Guidelines The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Overview Problem ""Information overload"" is something people in today's society are accustomed. Most people develop methods of coping with the huge amount of information available. We filter things through trusted sources, prioritize information that is actionable, and change our mental model of the situation as necessary. Earlier this year, the 2019 novel Coronavirus epidemic turned the world on its collective head and created a flurry of information. Large volumes of text data are expected to be consumed and acted upon in a short period of time. For humans, initially the challenge is simply to read and understand the documents. However the difficulty quickly becomes identifying what is new knowledge, and remembering the source of previously seen similar knowledge. New documents may have significant overlap with prior knowledge. Differences between documents must be reviewed, and often the progression of changes is important. Information retrieval, text mining, and recommender systems have developed algorithmic strategies to identify and extract useful information from text-based knowledge. The focus of these tools has generally been to pull relevant documents via (search), or push potentially interesting documents ( recommender). These techniques can be modified to assist a reader in identifying new useful information. What is our tool? In line with our project proposal, our overall goal was to create a reading assistant tool that allows a user to maintain a collection of read documents ( reflecting the current knowledge of the user) and then provides insight about a new unread document when compared to the all read documents. The output includes: A ranked list of documents that are most similar to the unread document based on BM25 scores and LSI similarity scores, respectively. A ranked list of paragraphs that are most similar to each paragraph of the unread document based on BM25 scores and LSI similarity scores, respectively. Our initial idea from the project proposal was to focus on the differences between documents, however the reality is that there were so many ways documents could be different that this was not particularly helpful. In this final version we instead focus on the areas of similarity betwen the unread document from the corpus of read. This allows the user to then quickly hone in on areas where the new document reinforce or subtly change what they had previously discovered from the read docuements. What can it be used for? Although this is created as a command line tool, as described above the initial inspiration for this idea was the surplus of information that was being pushed out (in our case via email) during the first months of the COVID-19 pandemic. One way to use this tool would be to integrate it with a mail server, so that you could forward an email or attachment and indicate that you had or had not read the document - then the server could spit back a new email with some analysis of the document including a list (and linke) to the other read similar documents, as well as highlight passages (paragraphs in our case) of particular interest. Implementation On start-up, text documents (directory path provided by user) are loaded by the assistant. During loading, the documents are processed (remove non-ascii characters, blank lines, etc) and added to an inverted Index. These documents are considered to be the previously-read documents by the user. Once the loading is complete, the assistant waits for a path to a text file (unseen document). Given this path, the assistant ranks the previously-read documents using the unseen document and returns the most similar read document and paragraph names to the user. We also provided methods for a user to view, add and remove previously-read documents. Currently, the assistant calculates similarity scores using two approaches: Okapi BM25 and Latent Semantic Indexing (LSI) similarity. To implement the Okapi BM25 ranking function, first an inverted index is built based on the previously-read documents. From there, it calculates the term frequency, inverse document frequency, and document length normalization. Finally, the similarity score for each document is calculated and scores are sorted in descending order. In addition to document-level BM-25, we also implemented paragraph-level BM25, which follows a similar approach but considers each paragraph of the document as an individual ""document"". This allows more detailed evaluation of unseen documents. To implement LSI similarity ranking function, we utilized the external gensim library. This is currently in a separate script (gensimlsi.py) and is integrated to the reading assistant. During document pre-processing, it removes stop words, blank lines, and words that only appeared once in the document to achieve better topic discovery. It first transforms the previously-read documents to Tf-Idf vectors. It then builds an LSI model with 200 (default) topics. Note that if the number of read documents is less than 200 then the number of read documents will be used. This LSI model will discover topics based on all the previously-read documents, and map the document vectors to LSI space, i.e. describe how strongly each document is related to each topic. Upon receiving the unseen document specified by the user, it will transform the document into LSI space and compute the cosine similarity. The scores will then be sorted in descending order. Similarly, we also implemented paragraph level analysis for LSI similarity. The code is written in a modular fashion, so that we can easily extend the assistant to use different similarity/difference measures and methods Usage View our usage video here: We recommend using python 3.6 or 3.7, with gensim (and its dependencies), as well as the smart_open package. To start the reading assistant, you must first have two directories of text files. One directory should be ""read"" documents, and the other is ""unread"" documents. Start the program by running the script like so: python reading_assistant.py read_docs_path unread_docs_path [k1] [b] read_docs_path : path containing text files that have been read by the user unread_docs_path : path containing text files that have not been read by the user [k1] : value for BM25. Default: 1.2 (optional) [b] : the b value for BM25. Default: 0.75 The script will load the read documents into an inverted index, and then go into the Read-Eval-Print Loop (REPL). Once the REPL is running, you will be presented with a list of Read documents and Un-Read documents. For example, you may see the following: ``` -= READ FILES: =- 0 : covid-bhc-contact-sop-1.txt 1 : covid-isos-brief.txt 2 : covid-update-4.txt 3 : covid-dod-mgmt-guide.txt 4 : covid-update-1.txt 5 : covid-update-3.txt 6 : covid-bhc-pt.txt 7 : covid-update-2.txt 8 : covid-yoko-sop.txt 9 : covid-fragord.txt 10 : covid-bhc-extended-use.txt =- UN-READ FILES: -= 0 : covid-bhc-contact-sop-2.txt 1 : covid-annex-1.txt Please use one of the following commands: rank [unread_file_#] --> Compares new document to previously-read documents read [unread_file_#] --> add the document from the unread list to the read list forget [read_file_#] --> remove a document from the read list view document [document name] --> prints the document view paragraph [paragraph name] --> prints the paragraph set scope [integer] --> only documents above this number of standard deviations above mean ranking score are returned exit --> Exits the program ``` To see the rank of the unread document covid-annex-1.txt you would enter rank 1 at the prompt. An ""output.html"" will also be generated in the directory where the command is issued. It contains the same info as the console output and provides a better visual representation. To move a document covid-bhc-contact-sop-2.txt from the unread into the * read* grouping, type read 0. Or, to move document covid-fragord.txt from read to unread, type forget 9. To view the text of document covid-yoko-sop.txt, type 'view document covid-yoko-sop.txt'. Note that this only works with documents listed under READ FILES. Similarly, to view the first paragraph of document covid-yoko-sop.txt, type 'view paragraph covid-yoko-sop.txt_parag0'. Note that this only works with documents listed under READ FILES. The 'set scope [integer]' command determines scope of the ranking results. As each document and paragraph in READ FILES is given a ranking score, the [integer] determines the cut-off of these scores. More specifically, the [integer] is the number of standard deviations above the mean ranking score. That is, a scope of 2 means that only documents and paragraphs that are two or more standard deviations above the mean score are returned. A scope of 0 means that all documents and paragraphs above the mean ranking score are returned. Generally, a higher scope means fewer documents and paragraphs returned, but these documents and paragraphs are much more relevant. Results To heuristically gauge the effectiveness of the reading assistant, each team member collected approximately 8-10 documents. These documents were loaded as the previously-read documents, and additional documents were provided as the unseen documents. From the preliminary examination, the results seem promising and inline with our qualitative evaluation of the documents. We found these results to be interesting and potentially be useful in a real world application. Using the results from our tool, one could easily find related previously read documents. Additionally if a paragraph was interesting one could find the similar passages. Alternatively, a document highlighter with links to related documents could be created. After creating the tool the existence of similar functionality became evident in other software such as Evernote, which shows similar notes to the one the user is currently viewing. The Evernote use case is not quite the same as our stated use case, but likely relies on some similar information retrievel techniques to generate the list of similar documents. Interestingly, the original impetus behind creating the tool was to find similar and different documents, however in the process of creating this tool we came to understand how the BM25 and LSI algorithms are powered towards similarities, not differences. The root of this is that there are only a few ways a document can be similar, but many ways documents can be different. This was an interesting realization, and further thought towards how to find useful differences could be discovered was an interesting thought experiment, although we did not make significant headway into how to solve that problem. If we were to continue to develop this project further, there are a few areas where we could easily improve the tool. One would be to integrate the data structures between the BM25 and gensim LSI algorithm so that the reading and memory usage was more efficient. Additionally, the LSI algorithm is capable of adding documents without needing to completely recreate it's underying data structures. Depending on how the tool would be used (if for instance documents would be frequently added) this would improve the efficiency of the tool. Manipulation of the parameters of the algorithms would be another area where improvements could be made. Manipulation of either with the k value of BM25, or the topic number of LSI coud lead to subtle improvements in results. Creating of a data set with user rankings for comparison to the results would also be very helpful in objectively analyzing the results of these tweaks. Creation of such a data set - with human-choosen similar paragarphs - would be time consuming to create but could result in use of comparison functions such as the F1 score which would facilitate further development. Overall we felt this tool was a strong starting point to further work in the realm of a reading assistant. Team Contributions All team members were active participants throughout the entire project lifecycle process. Our team worked well together and all members contributed meaningfully to our end result. All met via Zoom on the following days (30-60 minute meetings): Sep 10th: initial team meeting and plan for future meeting timeline Oct 3rd: draft concept of reading assistant formed Oct 9th: discussion of unit 1 concepts and relation to project Oct 21st: formalized topic and planned submission of topic to CMT Oct 24th: discussed status, potential roadblocks, and plan forward Nov 14th: reviewed TA comments and initial review of BM25 document-level rankings Nov 17th: discussed additional methods to rank articles Nov 29th: reviewed progress report, paragraph ranking, and formulated final plan for code breakdown Dec 6th: reviewed integration of gensim, paragraph ranking, CLI, and initial REPL Dec 11th: reviewed final product and discussed last touches necessary to complete project Dec 13th: recorded tutorial Specific Contributions All members contributed to write-ups, review of code, reviewing submission requirements, and ensuring deadlines were met. Kevin Ros: Created initial BM25 document-level code, with necessary ability to dynamically add and remove documents. Drafted initial documents (proposal, progress report) Added initial REPL interface Added standard deviation analysis of results to simplify interpretation of ranking data Zichun Xu Created paragraph level analysis of BM25 analysis method Modified gensim LSI analysis for paragraph level analysis Christopher Rock Added gensim LSI ranking methods Added CLI and finalized REPL Added HTML generator for better visual representation of results References 1https://github.com/meta-toolkit/metapy 2https://tac.nist.gov/2008/summarization/update.summ.08.guidelines.html 3Andrei V, Arandjelovic O. Complex temporal topic evolution modelling using the Kullback-Leibler divergence and the Bhattacharyya distance. EURASIP J Bioinform Syst Biol. 2016 Sep 29;2016(1):16. doi: 10.1186/s13637-016-0050-0. PMID: 27746813; PMCID: PMC5042987. 4Liu, Heng-Hui & Huang, Yi-Ting & Chiang, Jung-Hsien. (2010). A study on paragraph ranking and recommendation by topic information retrieval from biomedical literature. ICS 2010 - International Computer Symposium. 10.1109/COMPSYM.2010.5685393. 5https://radimrehurek.com/gensim/models/lsimodel.html"
https://github.com/peterzukerman/Twitter-Sarcasm-Detection	"Our solution and winning model for the Coursera Text Classification Competition TABLE OF CONTENTS Table of Contents About The Project Project Repo Structure Our Approach Understanding Data Getting Started Built With Prerequisites Installation Usage License Contact References Text Classification Competition: Twitter Sarcasm Detection About the Project The goal of this competition/project is to classify a given sequence of tweets (responses) as sarcastic or non-sarcastic. The tweets with its corresponding immediate context and full context is provided as continous responses to each tweets.The tweets are provided with conversation context which is an ordered list of dialogue. The objective of this competition is to predict the ""label"" of the response (tweets) using the given context (either immediate or full context) We present our best model based on BERT (Bi-directional Enconding Representations from Transfomers) using pre-trained stock weights of BERT-Base model and demonstrate the winning solution able to classify sarcasm with F1-score of 76.09%. Project Repository Structure Please follow the links to navigate to respective folders - - Data - Source Code - Results - Documentation - Proposal - Progress Report - Video Presentation - Long Form Video Link - Short Form Video Link - Detailed Project Presentation Our Approach Understanding Data: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | Getting Started All our models are in notebook format (.ipynb) and can be easily replicated using Jupyter / Google Colab or any other notebook environment. We recommend anaconda distribution to create virtual environments for Python and recommend Google Colab for TensorFlow (TF)/Keras Implementations. In order to replicate, reproduce or rerun our BERT model, we recommend downloading pre-trained stock weights as given below Built With Python Google Colab Keras TensorFlow Prerequisites Git Python 3.6 or above. Jupyter or Anaconda distribution Google Colab TensorFlow GPUs Keras BERT Pre-trained Stock Weights. You can download and use pre-trained stock weights of BERT-Base Model from here Installations Following packages/libraries are required for fully functioning of our BERT Model - ```bash install data pre-processing libraries $ pip install genism $ pip install ekphrasis ``` Install/Import the necessary libraries and frameworks ```bash Install key libraries and frameworks $ pip install tensorflow-gpu $ pip install --upgrade grpcio $ pip install tqdm $ pip install bert-fo-tf2 ``` Install/Import the necessary libraries and frameworks ```bash Import the following packages/libraries/frameworks import pandas as pd import numpy as np import tensorflow as ft from tensorflow import keras import bert from bert import BertModelLayer from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights from bert.tokenization.bert_tokenization import FullTokenizer ``` Install/Import the necessary evaluation metrics ```bash from sklearn.metrics import confusion_matrix, classification_report from sklearn import preprocessing ``` Download pre-trained weights from BERT-Base model ```bash !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip !unzip uncased_L-12_H-768_A-12.zip ``` Usage To clone and run our model, you'll need Git or git GUI clients like Git Kraken for Windows or Tower for Mac and Python From your command line or terminal application or git client: ```bash Clone this repository $ git clone https://github.com/dheerajpatta/CourseProject.git Go into the repository $ cd models Install above prerequisties and dependencies pip install * Run the Jupyter Notebook https://github.com/dheerajpatta/CourseProject/blob/main/models/sarcasm_classification_bert_large.ipynb ``` Additional References - - Google Colab - All about setting up Google Colab like a Pro from here - If you want to use BERT with Colab, you can get started with the notebook BERT FineTuning with Cloud TPUs Contact Artsiom Strok (astrok2@illinois.edu) Peter Zukerman (peterz2@illinois.edu) Dheeraj Patta (npatta2@illinois.edu) License Our solution for Text Classificaiton competition - Twitter Sarcasm Detection is licensed under the terms of the GPL Open Source license and is available for free."
https://github.com/philipcori/CourseProject	"CS 410 Final Project Progress Report: Improving a System Captain: Philip Cori (pcori2), Team Member 1: Henry Moss (htmoss2), Team Member 2: Kyle Maxwell (kylem6) Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Progress Made So Far Our team has been able to recreate the data results from the paper above, using RecQ, a Python library for recommender systems which includes the SERec algorithm. We have collected preliminary results on a particular approach for measuring the closeness between friends. We experimented with using the number of mutual friends as the driving factor in measuring closeness. Specifically, we used the formula: Closeness = m / n Where m is the number of mutual friends between the two friends and n is the total number of friends between the two friends. This seemed like a reasonable approach given that based on intuition, it seems two friends would be closer if they have more mutual friends. Not only this, but two friends are also considered closer if they have less total friends, which gives more weight to their own friendship. All results use 5-fold cross validation. Baseline results of SERec algorithm recreated on local machine: -Precision: 0.0449469214437 -Recall: 0.455598102652 -F1: 0.0818217388808 -MAP: 0.146067212641 -NDCG: 0.334412555937 Results using mutual friends closeness measure: -Precision: 0.0455850128798 -Recall: 0.461858940973 -F1: 0.0829799791008 -MAP: 0.149447844847 -NDCG: 0.340728707592 As can be seen, we've managed to increase the NDCG measure by 1.89% and the MAP measure by 2.31%. The second improvement strategy we are exploring is the modification of the matrix factorization model in the Rating Component of the SERec Boost algorithm. Incorporating the Weighted Rating Matrix Factorization methods from Collaborative Filtering for Implicit Feedback Datasets (Yifan Hu et al, KDD 2009). This matrix factorization model uses the implicit feedback data as an indication of positive and negative preference associated with vastly varying confidence levels. Primarily, we are layering this model to initialize the latent factors theta and beta, the user preferences and item attributes respectively. The social exposure component is then incorporated afterwards, only requiring a small number of iterations for the expectation-maximization algorithm to compute. Results from modified matrix factorization: Top 100, 5-fold cross validation -Precision: 0.0493 -Recall: 0.4993 -F1: 0.0897 -MAP: 0.1656 -NDCG: 0.3702 This is a 10.7% increase in NDCG and 13.4% increase in MAP score from the baseline results. Remaining Tasks Regarding measuring closeness between friends, we will continue to experiment with other social network concepts in measuring closeness. The Lastfm dataset also contains data not currently used by the SERec algorithm, so we will also investigate whether this can be used as well. This data includes data about tags on different artists and which users placed which tags. We are also given time stamps to record when such tagging events occurred. Regarding the matrix factorization, there is plenty of potential to improve the integration of the Weighted Rating Matrix Factorization model. As mentioned previously, we layered the components, but it seems that the Social Exposure Component could be incorporated in all iterations of the WRMF training. Additionally, we will likely conduct tests of significance for our attempted improvements to tell whether they are actual improvements or due to randomness in the data. Challenges/Issues Faced So Far Understanding the architecture and class structure of the RecQ system has taken the majority of our work so far. It takes time to identify code areas that are adjustable that will still keep the system functioning. An additional challenge is the time needed to collect results. Running the 5 iterations of the EM algorithm proposed takes roughly 30 minutes to complete. Furthermore, it has been challenging to improve the model based on social contagion, as suggested in the original paper. Social contagion is such a broad concept that it has been more difficult than expected to understand how to quantify and incorporate this idea into our preexisting model. We will continue looking into this as well as researching any possible improvements to be found from social structural influence, which was another method mentioned in the original published paper. CS 410 Course Project: Improving a System - Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Captain: Philip Cori (pcori2), Team Member 1: Henry Moss (htmoss2), Team Member 2: Kyle Maxwell (kylem6) Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Overview Our project is improving the recommendation algorithm discussed in this paper . To improve this system, we built on an existing recommendation system framework called RecQ . This system provides a host of different recommendation algorithms and datasets to test them on, including the one proposed in this paper. The Paper Key Excerpt from the Abstract This paper is concerned with how to make efficient use of social information to improve recommendations. Most existing social recommender systems assume people share similar preferences with their social friends. Which, however, may not hold true due to various motivations of making online friends and dynamics of online social networks. Inspired by recent causal process based recommendations that first model user exposures towards items and then use these exposures to guide rating prediction, we utilize social information to capture user exposures rather than user preferences. We assume that people get information about products from their online friends and they do not have to share similar preferences, which is less restrictive and seems closer to reality. Relevant Summary Of the two methods presented in the paper, we focused on the method they describe as social boosting. We will refer to this algorithm as SERec and more specifically, SERec Boost. The primary assumption leveraged by social boosting is that a user's exposure to an item is boosted by their friends. People are likely to receive information about a product from friends' discussion and shared feelings. As such, a consumer is more likely to have exposure to an item if their friends have interacted with the item. This can help the model disambiguate between the situations in which a user did not interact with an item because they dislike the item, or they did not notice the item. SERec has two main components, the Rating Component and the Social Exposure Component. The Rating Component is a matrix factorization model for rating prediction. The Social Exposure component calculates the exposure priori for each user item pair. The modularity of the system lends itself to extensibility and improvement. As suggested in the conclusion of the paper, the system presents itself with multiple avenues for exploring alternative methods for matrix factorization, and for integrating social exposure information. Software Implementation For information about the implementation and architecture of the entire RecQ system, please refer to the original repository. Our implementations build on the architecture of this system by adding our custom algorithm extensions to the RecQ framework. Development Environment Setup 1.Install miniconda 2.Create a new miniconda environment with Python 2.7 3.Run conda install mkl-source 4.Clone this repository 5.Download the lastfm dataset and add it to a folder named ""dataset"" within the repository 6.Activate the environment and install dependencies with conda install --file requirements.txt 7.Run python main.py 8.Follow the prompt and entire the desired algorithm to run Usage The original SERec boost algorithm discussed in the paper can be run by inputting the corresponding value (""s10"") for the SERec algorithm. Similarly, each of our individual extensions can be run by inputting the corresponding value displayed in the prompt. Users can compare results with the baseline results of the original SERec boost algorithm: Evaluation statistics are generated for a recommendation list of length 100. Precision 0.0479560590416 Recall 0.485595079529 F1 0.0872914462838 MAP 0.156959948956 NDCG 0.357199124032 Team Member Contributions Philip Cori Algorithm description I implemented an algorithm for measuring friend closeness, which is a suggested extension mentioned in the original paper. It can be run by inputting ""s11"" upon running main.py. To measure friend closeness, I use the formula: closeness = n / m , where n is the number of mutual friends between the two friends, and m is the total number of friends between the two friends. The intuition behind this is that it would seem two friends are closer friends if they share many mutual friends. More mutual connections should imply more personal interaction, exposure, and therefore closeness. Furthermore, two friends are considered closer if they have less total friends, which gives more weight to their own friendship. Results Three different formulas were tested based on the above idea. 1.Closeness = n / m * 10 It can be seen that unfortunately the results did not improve from the above implementation. All measures are within 0.04% of the baseline results. From further analysis, I found that only 9.4% of friendships consist of friends with any mutual friends. This low percentage can partially explain why adding a closeness measure may not have much impact, since it depends largely on the number of mutual friends as the determining factor. On the right certain statistics are shown to give an idea of the distribution of the closeness between friends. It seems that there is a fairly wide distribution of closeness measures, which could lead to unstable results as well. 2.Closeness = n 2 / m * 10 Precision 0.0478126532299 Recall 0.48445040571 F1 0.0870353717949 MAP 0.157030933278 NDCG 0.35660261507 Mean 1.177611443146 Median 0.666666666666 Stdev 1.583484225324 Min 0.072463768115 Max 10.0 Next, I tried squaring the number of mutual friends. The logic behind this was to give this factor even more weight, such that the closeness benefits quadratically with the more mutual friends they have. However, it seemed to diminish our results slightly further. It likely over-weighed some closeness terms, causing other friend exposures to be dominated by relationships with even only a few mutual friends. 3.Closeness = log 2 (n/m + 1) * 10 Given that squaring mutual friends further decreased results, I tried using a log transform that instead introduces diminishing returns from a higher closeness score. It can be seen that the standard deviation of the closeness measure is much less and every connection is being treated more equally. I add 1 to n/m to prevent any closeness measures from becoming negative. The results now are actually a slight improvement over the original baseline results. Although precision deteriorated slightly, NDCG, MAP, F1, and Recall improved by 1.06%, 1.28%, 1.08%, and 1.11% respectively. Conclusions From these results, it can be concluded that using mutual friends and total number of friends can accurately model the closeness between two friends. However, as discovered by experimenting different transformations of this idea, it is important not to smooth this measure slightly and not overweight certain connections. A dataset that would yield even better results Precision 0.0473455166473 Recall 0.479583256631 F1 0.086182863339 MAP 0.15522424781 NDCG 0.353131761035 Mean 1.3374456997343211 Median 0.7142857142857142 Stdev 1.965461767041007 Min 0.07246376811594203 Max 39.67032967032967 Precision 0.0484710617097 Recall 0.491026838687 F1 0.0882323703985 MAP 0.158975281865 NDCG 0.360987047292 Mean 0.920228915514409 Median 0.7369655941662061 Stdev 0.6752581440071108 Min 0.10092890885078087 Max 3.4594316186372978 for the introduced formula would contain a more dense social network where more mutual friends are present, as well as a pattern that friendships with a higher n/m measure do in fact imply a stronger correlation between the way two friends ""rank"" items (ie. artists in the Lastfm dataset). Henry Moss For this project, I tried to evaluate how we could incorporate social contagion into the existing recommendation algorithm, as this was one of the last suggestions in the original paper for further improvement. Unfortunately, after a lot of research into social contagion, it seems to be a fairly arbitrary concept and difficult to measure. The recommendation algorithm is already trying to calculate how much one user is potentially influenced by the friends they are in contact with, which is a simple definition of what social contagion is. Additionally, I helped Philip experiment with his algorithm for measuring friend closeness. After trying to find a way to implement TF-IDF weighting into our algorithm, I hypothesized that it could be beneficial to have diminishing weight on increased number of friends, so that it values closer friends at an increased rate. This boosted our NDCG values from around 0.356 to 0.36. After that, I tried other rates of diminishing return with different log powers of l og 3 and l og 10 , along with Philip's idea to prevent any of the values from being negative, but found that neither were as successful as l og 2 . 1.Closeness = log 3 (n/m + 1) * 10 2.Closeness = log 10 (n/m + 1) * 10 Precision 0.0339437367304 Recall 0.344395578472 F1 0.0617967648459 MAP 0.0755357909461 NDCG 0.216285701749 Mean 0.9437401212835262 Median 0.5874549356790257 Stdev 1.07063674944364 Min 0.06572152931440885 Max 6.309297535714574 These were both unsuccessful ideas, as using log3 decreased the MAP and NDCG scores by 51.8% and 39.5%, while using log10 decreased the MAP and NDCG scores by 65.2% and 48.9%, respectively. I also looked into social structural influence and decided that one way to try to implement this into our project was by stretching the data to include friends of friends, in addition to just counting the closeness of direct friendships. I tried changing n, which was originally the number of mutual friends between user 1 and user 2, to also include any mutual friends of user 2's friends. This expands the network out a degree, and I was hoping that with more data to work with, the recommendation algorithm could be more successful. 3.Closeness = n / m * 100, where n is the number of mutual friends and friends of friends As you can see, this slightly lowered our MAP and NDCG score, which we have been using as our main measurements of improvement. I experimented with a few other factors into the algorithm, such as using log 2 again, but these were the best results I came up with overall. Kyle Maxwell Algorithm description Precision 0.0315127388535 Recall 0.319221388131 F1 0.0573627683574 MAP 0.0546393023159 NDCG 0.182298909362 Mean 0.4553789242585401 Median 0.2802872360024353 Stdev 0.5136685928528778 Min 0.03135713852858582 Max 3.0102999566398116 Precision 0.0446282847314 Recall 0.452188532552 F1 0.0812387859384 MAP 0.144929322365 NDCG 0.332269062625 Mean 11.607520985265921 Median 6.896551724137931 Stdev 15.32296991067004 Min 0.5681818181818182 Max 100.0 The potential improvement I explored was the modification of the matrix factorization model in the Rating Component of the SERec Boost algorithm. By incorporating the Weighted Rating Matrix Factorization methods from Collaborative Filtering for Implicit Feedback Datasets (Yifan Hu et al, KDD 2009) , I was able to achieve small but successful improvements. This matrix factorization latent factor model uses implicit feedback data as an indication of positive and negative preference associated with varying confidence levels. Meaning, it not only models the user's preferences of an item, but models the probability that they have consumed the item. I utilized this implicit feedback model to first converge on the user and item latent factor vectors without any use of the social connectivity information. Then, I used the SERecBoost Social Exposure Component to update the social exposure prior based on those latent factors. Finally, the original SERecBoost algorithm is initialized with the pre-trained latent factors, and only requires a small number of iterations for the expectation-maximization algorithm to adequately converge. Results Statistics at 100 Recommendations, 5-fold cross validation Conclusions As suggested in the original Collaborative Filtering with Social Exposure, there is still room for additional work and improvement. By utilizing other novel matrix factorization techniques, we were able to achieve 5.6% MAP and 3.7% NDCG score increase. Precision 0.0492 Recall 0.4981 F1 0.0895 MAP 0.1657 NDCG 0.3705 Project Proposal: Improving a System System: https://arxiv.org/pdf/1711.11458.pdf 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Philip Cori, pcori2 Team Member 1: Henry Moss, htmoss2 Team Member 2: Kyle Maxwell, kylem6 2.What system have you chosen? Are you adding a function or improving a function? What function? We will improve on the recommender system proposed in Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation by Menghan Wang, Xialin Zheng, and Yang Yang. We will attempt to improve the system in 3 ways: *Write a function to measure closeness between friends, as suggested by the paper. *Write a function that performs matrix factorization for the the Rating Component *Improve model based on recent social network analysis techniques (examples listed are social contagion and/or social structural influence). One possibility is to weigh items by an IDF term based on the items' number of occurrences in exposures between friends. 3.If you are adding a function, why is the new function important or interesting? How will it benefit the users? If you are improving a function, what are the main limitations of the current function? How are you going to improve it? How will your improvements benefit the users? Implementing these new functions could improve the MAP, recall, and NCDG metrics. Firstly, a limitation of the current algorithm is that it assumes all friends are equally close. Therefore, if we can successfully measure ""closeness"" between friends, this can allow us to give more accurate recommendations. Secondly, the current algorithm does not use matrix factorization for the rating component. Implementing this can improve our metrics. Lastly, if we are able to use IDF weighting to filter out ""main-stream"" recommendations, our users will get more unique recommendations. This will make the recommendations that they receive from similar users more genuine and useful. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? The MAP, recall, and NDCG metrics will be used to determine if our experiments improved the recommender system. 5.How will your code communicate with or utilize the system? We will directly extend the libraries of the framework and recommender system. 6.Which programming language do you plan to use? We will use Python to implement our extensions. 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Implementing these functions will take considerable time for three reasons. Firstly, it will take time to fully understand the approach presented in the paper. Secondly, it will take time to understand the open source framework that implements the approach discussed. Lastly, it will take time to implement and experiment with the new functions. Each person will do: *2h Team Meetings *1h Read Paper *1h Getting started with framework *2h Research into possible feature improvements *10-20h Implementation of feature improvement *2h Evaluation, Reporting Course Project Project Documentation https://github.com/philipcori/CourseProject/blob/main/ProjectDocumentation.pdf Project Presentation https://www.youtube.com/watch?v=U2XECocV8fU&feature=youtu.be RecQ Introduction Founder: @Coder-Yu Main Contributors: @DouTong @Niki666 @HuXiLiFeng @BigPowerZ @flyxu RecQ is a Python library for recommender systems (Python 2.7.x) in which a number of the state-of-the-art recommendation models are implemented. To run RecQ easily (no need to setup packages used in RecQ one by one), the leading open data science platform Anaconda is strongly recommended. It integrates Python interpreter, common scientific computing libraries (such as Numpy, Pandas, and Matplotlib), and package manager. All of them make it a perfect tool for data science researcher. Besides, GPU based deep models are also available (TensorFlow is required). Latest News 22/09/2020 - DiffNet proposed in SIGIR'19 has been added. 19/09/2020 - DHCF proposed in KDD'20 has been added for comparison, althought it doesn't work very well. 29/07/2020 - ESRF proposed in my TKDE manuscript (under review) has been added. 23/07/2020 - LightGCN proposed in SIGIR'20 has been added. 17/09/2019 - NGCF proposed in SIGIR'19 has been added. 13/08/2019 - RSGAN proposed in ICDM'19 has been added. 09/08/2019 - Our paper is accepted as full research paper by ICDM'19. 02/20/2019 - IRGAN proposed in SIGIR'17 has been added (tuning...) 02/12/2019 - CFGAN proposed in CIKM'18 has been added. 02/04/2019 - NeuMF proposed in WWW'17 has been added. 10/09/2018 - An Adversarial training based Model: APR has been implemented. 10/02/2018 - Two deep models: DMF CDAE have been implemented. 07/12/2018 - Algorithms supported by TensorFlow: BasicMF, PMF, SVD, EE (Implementing...) Architecture of RecQ Features Cross-platform: as a Python software, RecQ can be easily deployed and executed in any platforms, including MS Windows, Linux and Mac OS. Fast execution: RecQ is based on the fast scientific computing libraries such as Numpy and some light common data structures, which make it run much faster than other libraries based on Python. Easy configuration: RecQ configs recommenders using a configuration file. Easy expansion: RecQ provides a set of well-designed recommendation interfaces by which new algorithms can be easily implemented. Data visualization: RecQ can help visualize the input dataset without running any algorithm. How to Run it 1.Configure the **xx.conf** file in the directory named config. (xx is the name of the algorithm you want to run) 2.Run the **main.py** in the project, and then input following the prompt. How to Configure it Essential Options Entry Example Description ratings D:/MovieLens/100K.txt Set the path to input dataset. Format: each row separated by empty, tab or comma symbol. social D:/MovieLens/trusts.txt Set the path to input social dataset. Format: each row separated by empty, tab or comma symbol. ratings.setup -columns 0 1 2 -columns: (user, item, rating) columns of rating data are used; -header: to skip the first head line when reading data social.setup -columns 0 1 2 -columns: (trustor, trustee, weight) columns of social data are used; -header: to skip the first head line when reading data recommender UserKNN/ItemKNN/SlopeOne/etc. Set the recommender to use. evaluation.setup -testSet ../dataset/testset.txt Main option: -testSet, -ap, -cv -testSet path/to/test/file (need to specify the test set manually) -ap ratio (ap means that the ratings are automatically partitioned into training set and test set, the number is the ratio of test set. e.g. -ap 0.2) -cv k (-cv means cross validation, k is the number of the fold. e.g. -cv 5) Secondary option:-b, -p, -cold -b val (binarizing the rating values. Ratings equal or greater than val will be changed into 1, and ratings lower than val will be changed into 0. e.g. -b 3.0) -p (if this option is added, the cross validation wll be executed parallelly, otherwise executed one by one) -tf (model training would be conducted on TensorFlow if TensorFlow has been installed) -cold threshold (evaluation on cold-start users, users in training set with ratings more than threshold will be removed from the test set) item.ranking off -topN -1 Main option: whether to do item ranking -topN N1,N2,N3...: the length of the recommendation list. *RecQ can generate multiple evaluation results for different N at the same time output.setup on -dir ./Results/ Main option: whether to output recommendation results -dir path: the directory path of output results. Memory-based Options similarity pcc/cos Set the similarity method to use. Options: PCC, COS; num.shrinkage 25 Set the shrinkage parameter to devalue similarity value. -1: to disable simialrity shrinkage. num.neighbors 30 Set the number of neighbors used for KNN-based algorithms such as UserKNN, ItemKNN. Model-based Options num.factors 5/10/20/number Set the number of latent factors num.max.iter 100/200/number Set the maximum number of iterations for iterative recommendation algorithms. learnRate -init 0.01 -max 1 -init initial learning rate for iterative recommendation algorithms; -max: maximum learning rate (default 1); reg.lambda -u 0.05 -i 0.05 -b 0.1 -s 0.1 -u: user regularizaiton; -i: item regularization; -b: bias regularizaiton; -s: social regularization How to extend it 1.Make your new algorithm generalize the proper base class. 2.Rewrite some of the following functions as needed. - readConfiguration() - printAlgorConfig() - initModel() - buildModel() - saveModel() - loadModel() - predict() Algorithms Implemented Note: We use SGD to obtain the local minimum. So, there have some differences between the original papers and the code in terms of fomula presentation. If you have problems in understanding the code, please open an issue to ask for help. We can guarantee that all the implementations are carefully reviewed and tested. Any suggestions and criticism are welcomed. We will make efforts to improve RecQ. Rating prediction Paper SlopeOne Lemire and Maclachlan, Slope One Predictors for Online Rating-Based Collaborative Filtering, SDM 2005. PMF Salakhutdinov and Mnih, Probabilistic Matrix Factorization, NIPS 2008. SoRec Ma et al., SoRec: Social Recommendation Using Probabilistic Matrix Factorization, SIGIR 2008. SVD++ Koren, Factorization meets the neighborhood: a multifaceted collaborative filtering model, SIGKDD 2008. RSTE Ma et al., Learning to Recommend with Social Trust Ensemble, SIGIR 2009. SVD Y. Koren, Collaborative Filtering with Temporal Dynamics, SIGKDD 2009. SocialMF Jamali and Ester, A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks, RecSys 2010. EE Khoshneshin et al., Collaborative Filtering via Euclidean Embedding, RecSys2010. SoReg Ma et al., Recommender systems with social regularization, WSDM 2011. LOCABAL Tang, Jiliang, et al. Exploiting local and global social context for recommendation, AAAI 2013. SREE Li et al., Social Recommendation Using Euclidean embedding, IJCNN 2017. CUNE-MF Zhang et al., Collaborative User Network Embedding for Social Recommender Systems, SDM 2017. SocialFD Yu et al., A Social Recommender Based on Factorization and Distance Metric Learning, IEEE Access 2017. Item Ranking Paper BPR Rendle et al., BPR: Bayesian Personalized Ranking from Implicit Feedback, UAI 2009. WRMF Yifan Hu et al.Collaborative Filtering for Implicit Feedback Datasets, KDD 2009. SBPR Zhao et al., Leveraing Social Connections to Improve Personalized Ranking for Collaborative Filtering, CIKM 2014 ExpoMF Liang et al., Modeling User Exposure in Recommendation, WWW 2016. CoFactor Liang et al., Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence, RecSys2016. TBPR Wang et al. Social Recommendation with Strong and Weak Ties, CIKM 2016. CDAE Wu et al., Collaborative Denoising Auto-Encoders for Top-N Recommender Systems, WSDM 2016. DMF Xue et al., Deep Matrix Factorization Models for Recommender Systems, IJCAI 2017. NeuMF He et al. Neural Collaborative Filtering, WWW 2017. CUNE-BPR Zhang et al., Collaborative User Network Embedding for Social Recommender Systems, SDM 2017. IRGAN Wang et al., IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models, SIGIR 2017. SERec Wang et al., Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation, AAAI 2018. APR He et al., Adversarial Personalized Ranking for Recommendation, SIGIR 2018. IF-BPR Yu et al. Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation, CIKM 2018. CFGAN Chae et al. CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks, CIKM 2018. NGCF Wang et al. Neural Graph Collaborative Filtering, SIGIR 2019. DiffNet Wu et al. A Neural Influence Diffusion Model for Social Recommendation, SIGIR 2019. RSGAN Yu et al. Generating Reliable Friends via Adversarial Learning to Improve Social Recommendation, ICDM 2019. LightGCN He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, SIGIR 2020. DHCF Ji et al. Dual Channel Hypergraph Collaborative Filtering, KDD 2020. Category Generic Recommenders UserKNN ItemKNN BasicMF SlopeOne SVD PMF SVD++ EE BPR WRMF ExpoMF Social Recommenders RSTE SoRec SoReg SocialMF SBPR SREE LOCABAL SocialFD TBPR SERec Network Embedding based Recommenders CoFactor CUNE-MF CUNE-BPR IF-BPR Deep Recommenders APR CDAE DMF NeuMF CFGAN IRGAN Baselines UserMean ItemMean MostPopular Rand Related Datasets Data Set Basic Meta User Context Users Items Ratings (Scale) Density Users Links (Type) Ciao [1] 7,375 105,114 284,086 [1, 5] 0.0365% 7,375 111,781 Trust Epinions [2] 40,163 139,738 664,824 [1, 5] 0.0118% 49,289 487,183 Trust Douban [3] 2,848 39,586 894,887 [1, 5] 0.794% 2,848 35,770 Trust LastFM [4] 1,892 17,632 92,834 implicit 0.27% 1,892 25,434 Trust Reference [1]. Tang, J., Gao, H., Liu, H.: mtrust:discerning multi-faceted trust in a connected world. In: International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, Wa, Usa, February. pp. 93-102 (2012) [2]. Massa, P., Avesani, P.: Trust-aware recommender systems. In: Proceedings of the 2007 ACM conference on Recommender systems. pp. 17-24. ACM (2007) [3]. G. Zhao, X. Qian, and X. Xie, ""User-service rating prediction by exploring social users' rating behaviors,"" IEEE Transactions on Multimedia, vol. 18, no. 3, pp. 496-506, 2016. [4] Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. 2nd Workshop on Information Heterogeneity and Fusion in Recom- mender Systems (HetRec 2011). In Proceedings of the 5th ACM conference on Recommender systems (RecSys 2011). ACM, New York, NY, USA Thanks If you our project is helpful to you, please cite one of these papers. @inproceedings{yu2018adaptive, title={Adaptive implicit friends identification over heterogeneous network for social recommendation}, author={Yu, Junliang and Gao, Min and Li, Jundong and Yin, Hongzhi and Liu, Huan}, booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management}, pages={357--366}, year={2018}, organization={ACM} } @article{yu2019generating, title={Generating Reliable Friends via Adversarial Training to Improve Social Recommendation}, author={Yu, Junliang and Gao, Min and Yin, Hongzhi and Li, Jundong and Gao, Chongming and Wang, Qinyong}, journal={arXiv preprint arXiv:1909.03529}, year={2019} }"
https://github.com/pinkychauhan89/FakeNewsClassifier	"FAKE NEWS CLASSIFIER Pinky Chauhan University of Illinois at Urbana-Champaign Overview/Objective Build a classifier system based on machine learning Able to identify fake news from real/reliable news given a news title/text as input Can be integrated with social media platforms to flag/filter out potentially fake articles CLASSIFIER NEWS (TITLE, TEXT) RELIABLE NEWS FAKE NEWS Why Fake News Classifier? Increasingly prevalent Widespread on social media and websites Hard to distinguish Similar tone/style as reliable news Hard for human eye to catch differences Machine learning can help Technology/Libraries Dataset https://www.kaggle.com/c/fake-news/data train.csv: A full training dataset with the following attributes: id: unique id for a news article title: the title of a news article author: author of the news article text: the text of the article; could be incomplete label: a label that marks the article as potentially unreliable 1: unreliable/fake 0: reliable test.csv: A testing training dataset with all the same attributes at train.csv without the label. submit.csv: A sample submission to be populated with predictions of classifier on test.csv Code Structure Execution Data analysis Preprocessing Feature Extraction Vectorization Model training and tuning Performance assessment Data Analysis Data distribution (Real vs fake labels) Attributes contribution towards category Author's distribution Polarity/Sentiment Analysis Preprocessing Feature Extraction Cleaned article (title + text) used as feature Vectorization Term frequency (TF) based vector over unigrams Term frequency (TF) based vector over unigrams and bigrams Term frequency/inverse document frequency (TF-IDF) based vector over unigrams TF-IDF based vector over unigrams and bigrams TF-IDF based vector over unigrams, bigrams and trigrams Model Training & Tuning Naive bayes (Multinomial) Logistic Regression SVM using Linear SVC SGDC classifier Decision Tree Naive Bayes (Multinomial) Probabilistic classifier inspired by the Bayes theorem under a simple assumption that attributes are conditionally independent Training with smoothing Without smoothing Unigrams/Bi-grams/Trigrams term frequency vectorizer Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Logistic Regression Uses a logistic function to model a binary dependent variable Used when dependent variable is binary in nature Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Support Vector Machines (Linear SVC) Fit to the data, returning a ""best fit"" hyperplane that divides, or categorizes data LinearSVC only supports a linear kernel, is faster and can scale a lot better Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Tuning regularization SGDC (Stochastic gradient descent Classifier) Linear classifier optimized by the Stochastic gradient descent (SGD) Faster convergence Looking for the minima of the loss using SGD Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Decision Tree Builds classification or regression models in the form of a tree structure Utilizes an if-then rule Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Performance Evaluation Accuracy Precision (macro/micro) Recall (macro/micro) F1 score Confusion matrix Precision-recall curve Final Model Linear SVC with unigrams TF-IDF vectorization Export trained model using pipeline and joblib Tester Tester.ipynb Loads trained model using joblib Add text and title as inputs in notebook Kaggle https://www.kaggle.com/pinkychauhan/fakenewsclassifierusingnltk-sklearn References https://www.kaggle.com/c/fake-news/data https://scikit-learn.org/stable/user_guide.html https://medium.com/datadriveninvestor/python-data-science-getting-started-tutorial-nltk-2d8842fedfdd https://matplotlib.org/tutorials/introductory/pyplot.html https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623 Progress Report By: Pinky Chauhan Topic: Fake news classification using machine learning 1) Which tasks have been completed? o As per the recommendation of project proposal reviewer in CMT, I changed the dataset to the one suggested by the reviewer https://www.kaggle.com/c/fake-news/data o I have been acquainting myself with different classification algorithm details and also nltk, sklearn, pandas libraries to work on this project. o Data Analysis is complete using matplotlib, nltk sentiment analyzer and manual run-through to understand the observations listed in the dataset and its contribution towards the classification. o Preprocessing of data is done using nltk to setup training and test datasets, handle missing values, performing tokenization, removing stop words, lemmatization, encode categorical variables as needed. o Feature Selection to keep only the most relevant variables that are used for training. o Vectorization using sklearn libraries to map words to a corresponding vector of real numbers to find word similarities, etc. o Model design and training using several classification algorithms using sklearn libraries (Naive-Bayes, Decision tree and Logistic Regression so far) o Data and preliminary notebook are available in Github repo. 2) Which tasks are pending? o Models hyperparameter tuning and validation to assess the accuracy and avoid overfitting. o Performance evaluation of the different model algorithms used: compute and analyze the metrics precision, recall, F1 score, etc. o Create API/script that will take news text as input and generate its classification as real or fake as the result. o If time permits, will also try to add a submission of this notebook on Kaggle and evaluate accuracy against other submissions. Task Status Understand classification algorithms in depth and familiarize with nltk (I am new to machine learning world and will need to research/obtain a deeper understanding) Complete Environment setup Complete Data analysis and preprocessing Complete Feature selection and vectorization Complete Model design, training and hyperparameters tuning Model design and training complete; Tuning in progress Testing and evaluation In progress Integration with final output script/API To be done Prepare presentation To be done 3) Are you facing any challenges? o Nothing major at this time. I am relatively new to machine learning, NLTK, sklearn libraries. But there is good information available online and that has been very helpful thus far. 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Pinky Chauhan (pinkyc2) I will be working on this project individually. All administrative work will be done by me. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Fake news classification using machine learning Details/Task: I plan to build a system that is able to tell apart fake news from real news given some title and/or news content as the input. This is essentially a classification problem where I will train several models using machine learning on the following dataset from Kaggle: https://www.kaggle.com/hiteshkumargupta/fake-news-classification. These trained models will then predict the category of news item from a test dataset of fake/real news articles. Models will be evaluated based on performance metrics to choose the final model that will be used for predictions in the classifier script/API for final submission. Why fake news classifier? Fake news is becoming increasingly prevalent nowadays especially with the wide-spread usage of social media platforms which can be easily misused to propagate factually incorrect information to the users. With an average person spending many hours in each day coming across multiple posts, tweets, news articles, etc. while on social media, it becomes important to be able to segregate actual facts from cooked up fake stories/news. Such a tool can then be integrated with social media platforms to flag such articles or filter those out. It is an interesting problem to solve since fake news articles can come very close to the tone or style of the real news to make it sound authentic and hence not very easy to identify. Planned approach: I plan to divide the project into the following steps: Y= Data Analysis to understand the observations listed in the dataset and their contribution towards the classification. Y= Preprocessing to setup training and test datasets, handle missing values, performing tokenization, remove stop words, stemming, encode categorical variables. Y= Feature Selection to keep only the most relevant variables Y= Vectorization to map words to a corresponding vector of real numbers to find word similarities, etc. Y= Model design to train, tune hyperparameters, validation, test using several classification algorithms (XGBoost, Naive-Bayes, Decision tree, Linear classification, SVM, etc.) Y= Performance evaluation: compute and analyze the metrics precision, recall, F1 score, etc. Y= Create API/script that will take news text as input and generate it's classification as real or fake as the result. Tools/systems/datasets: Tools/Systems: I plan to leverage nltk for preprocessing tasks, numpy and pandas, sklearn for machine learning, matplotlib, etc. for this project. Dataset: https://www.kaggle.com/hiteshkumargupta/fake-news-classification The dataset comprises of 2 subsets: Y= train.csv with about 40000 observations for training the models and Y= test.csv with about 4000 observations for testing. The csv files comprise of the following columns: Index: Counter for each observation Title: Summary of the news article Text: body of the article Subject: Topic category of news article: political news, government news, etc. Date: Date of the news article Class: Only available in train.csv with labels as fake/real marked by users/contributors. Expected outcome: An API/python script/Jupiter notebook that can accept news title and/or text as input and output the category for the news item as fake news or real news. Evaluation: I will be training several models using different algorithms and evaluate their performances on the test dataset using precision, recall, F1 score, etc. 3. Which programming language do you plan to use? Python/Jupiter notebook 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Time Understand classification algorithms in depth and familiarize with nltk (I am new to machine learning world and will need to research/obtain a deeper understanding) 8 hours Environment setup 1 hour Data analysis and preprocessing 3 hours Feature selection and vectorization 2 hours Model design, training and hyperparameters tuning 10 hours Testing and evaluation 4 hours Integration with final output script/API 1 hour Prepare presentation 2 hours Total 31 hours Fake News Classifier: Pinky Chauhan University of Illinois at Urbana Champaign Overview: This objective of this project is to build a classifier system based on machine learning that is able to identify fake news from real/reliable news given a news title and/or news text content as the input. Such a tool can be integrated with social media platforms to flag potentially fake articles or filter those out. This is essentially a data categorization problem where I have trained several classifier models on the following dataset from Kaggle: https://www.kaggle.com/c/fake-news/data The dataset comprises of csv format files for training and testing with each file containing id, news title, text and author fields. The train.csv also has label field to categorize data as Reliable (label value 0) and Fake (label value 1) After evaluation based on various performance metrics, one of the models (in this case, Linear SVC over unigram bag-of-words/TF-IDF representation) is integrated in the final tester notebook to test with news data. The classifier takes a news article (title and text) as input and provides a prediction for the news article as either of the 2 categories: o Fake News o Reliable News Software Implementation Details: * Data Analysis: - Comprises of checking several attributes to evaluate their contribution towards classification. For such a classifier, the text and title of the news make obvious choices as features. - I also analyzed authors' distribution using pandas and polarity/sentiment differences using NLTK vader sentiment intensity analyzer library on the dataset. * Preprocessing of data: - Handling missing values by removing any rows with no text and title, preprocess data to remove any punctuations, remove any words with length 3 or less, stop words removal, tokenization and lemmatization using NLTK libraries * Feature selection: - Concatenated news title and text into article field and preprocessed it. Article comprises the feature to train the model * Vectorization - Different vector forms listed below have been used using NLTK vectorization/transformation libraries: o Term frequency (TF) based vector over unigrams bag of words representation o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams o Term frequency (TF) based vector over unigrams and bigrams o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams and bigrams o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams, bigrams and trigrams * Training/hyperparameter tuning/validation using classification models: - Models used (sklearn libraries): o Naive bayes (With/without smoothing, TF vs TF-IDF vectors, Unigram/N-gram) o Logistic Regression (TF-IDF vectors using Unigrams/N-grams) o SVM using Linear SVC (TF-IDF vectors using Unigrams/N-grams, Regularization) o SGDC classifier (TF-IDF vectors using Unigrams/N-grams) o Decision Tree (TF-IDF vectors using Unigrams/N-grams) * Performance evaluation: - Compute and analyze metrics using sklearn metrics libraries o Precision (macro/micro), recall (macro/micro), F1 (macro/micro) o Classification Accuracy o Confusion matrix to see distribution of true/false positives/negatives - Select the best performing model based on evaluation results (SVM using Linear SVC using TF-IDF vector over unigrams) - Results: * Save/export trained model: - Using pipeline to specify all steps (vectorizer/classifier), fit training data and exporting model using joblib library * Kaggle submission: - Predicted results for data in test.csv and submitted notebook/results to Kaggle (https://www.kaggle.com/pinkychauhan/fakenewsclassifierusingnltk-sklearn) - Accuracy: 94% * Create script (Jupyter notebook) that will take news text as input and generate classification as reliable news or fake news. Installation/Execution Details: Code is written using Jupyter notebook and python 3 Code structure: data: This directory contains the dataset from Kaggle (https://www.kaggle.com/c/fake-news/data). There are 3 files: o train.csv: To use for analysis, training, validation o test.csv: Test dataset for submission of results to Kaggle competition o submit.csv: File containing results/predictions for data in test.csv notebooks: This directory contains 2 notebooks: o FakeNewsClassifierTraining.ipynb: Jupyter notebook containing code/results for data analysis, cleanup, features set up, vectorization, training using various classifier algorithms, tuning and performance evaluation/comparison, model pipeline creation/export, prediction of results for test.csv for Kaggle submission o Tester.ipynb: This notebook loads the pretrained/exported model and predicts the category for a given news article. Use this notebook to test the classifier. model: This directory contains the pretrained model exported by FakeNewsClassifierTraining.ipynb notebook and loaded by Tester.ipynb results: This directory contains the summarized performance metrics from different models used for training and a copy of the submit.csv file generated from predictions for data in data/test.csv Code Setup: Install python 3 and Jupyter notebook Install the following python/machine learning libraries: o re: For regular expression matching o itertools: To iterate over data o pandas: For Data analysis/representation as Dataframes o nltk: Natural language toolkit o sklearn: For model selection, training, evaluation, export using pipeline o matplotlib: For visualization o joblib: For model export and load Checkout the project from main branch in Github Launch Jupyter notebook and navigate to the directory where project is checked out Tester.ipynb located in notebooks folder can be used for testing the classifier by providing values for title and text FakeNewsClassificationTraining.ipynb can also be executed to see all stages entailed in bulding the classifier and training/evaluation of different models Note: In case you see an issue around missing packages stopwords, punkt, vader_lexicon or wordnet, download them one time using below commands: nltk.download('vader_lexicon') nltk.download('punkt') nltk.download('stopwords') nltk.download('wordnet') References: https://www.kaggle.com/c/fake-news/data https://scikit-learn.org/stable/user_guide.html https://medium.com/datadriveninvestor/python-data-science-getting-started-tutorial-nltk-2d8842fedfdd https://matplotlib.org/tutorials/introductory/pyplot.html https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623 Fake News Classifier: By: Pinky Chauhan (University of Illinois at Urbana Champaign) This objective of this project is to build a classifier system based on machine learning that is able to identify fake news from real/reliable news given a news title and/or news text content as the input. Such a tool can be integrated with social media platforms to flag potentially fake articles or filter those out. This is essentially a data categorization problem where I have trained several classifier models on the following dataset from Kaggle: https://www.kaggle.com/c/fake-news/data After evaluation based on various performance metrics, one of the models (in this case, Linear SVC over unigram bag-of-words/TF-IDF representation) is integrated in the final tester notebook to test with news data. The classifier takes a news article (title and text) as input and provides a prediction for the news article as either of the 2 categories: - Fake News - Reliable News Final Project report/documentation: https://github.com/pinkychauhan89/CourseProject/blob/main/ProjectReport.pdf Presentation:https://github.com/pinkychauhan89/CourseProject/blob/main/Presentation.pptx"
https://github.com/pipipiii/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/pmanden-uiuc/CourseProject	"Course Project Progress Update (11/28/20) pmanden2@illinois.edu Objectives Objective of my project is to improve the ExpertSearch system. I will attempt to achieve the following enhancements in this project, as mentioned in the project proposal. 1. Given a URL, use Naive Bayes classifier to classify it as a directory page or a non-directory page 2. Given a URL use Naive Bayes classifier to classify it as a faculty page or a non-faculty page Which tasks have been completed? Getting the baseline code to work I have already spent quite a bit of time trying to get the baseline ExpertSearch code on Git to work. After trying different python versions, python library versions, trying on Linux and Windows and various experiments and debugging, I got it to work on Python 2.7 on Windows with some code changes. Now I have the baseline to do the actual implementation. Generating negative samples for directory and faculty pages I have written web scraping code to achieve the following: 1. Wrote code to get a list of all universities in US (from here - http://doors.stanford.edu/~sr/universities.html) 2. Wrote code to scrape each of the university in the list above, and identified 10 links 3. Wrote code to clean up the list to exclude directory/faculty URLs, so that it can be used as ""negative"" samples for directory and faculty classes. Now I have the positive and negatives samples for the directory and faculty URLs. Core Naive Bayes classifier code Core code that implements Naive Bayes classifier has been written. It can accept file names of positive and negative samples, load data, create term document matrix etc. Also provides a function for classification. Which Tasks are pending? 1. Need to get all the code to run on one version of Python, that I haven't been able to do so far. 2. Clean up code, better documentation I also will attempt to do the following (I am not very familiar with javascript/UI, so I may not be able to do this): 1. Change the UI so that an additional option can be added to the UI to type in a directory page or a faculty page so that classification results can be seen visually in UI Are you facing any challenges? As indicated above, I have managed to get the baseline ExpertSearch code to work on Python 2.7. However, my code (classifier etc.) doesn't run on 2.7 (it runs on 3.8). Need to figure out a way to get all code to run on one version. Dealing with Python versions and libraries continue to be a pain. Thoughts on work beyond the scope of this project In order to achieve full automation of identifying and extracting faculty pages, we will first need to automatically identify the directory and faculty pages on a university website, given a root URL for the university. This can be done if a list of university websites is available (one such list of universities in US is available here - http://doors.stanford.edu/~sr/universities.html) With full automation of identifying faculty web pages will look as below: 1. For each university a. Get list of all URLs on the website by: i. Finding the sitemap file (sitemap.xml, ...) ii. Or by crawling the website and generating a full list of all URLs on the website 2. From the list of URLs generated above a. Create 2 training sets i. One for directory pages classification (using the directory pages listing from Coursera?) ii. One for faculty pages classification (using faculty URLs provided) 3. Use Naive Bayes Classifier to identify URLs that are directory pages (If the ultimate objective is to find the faculty pages, there is really no need to find the directory pages, as the sitemap will contain the full faculty pages. And hence this step can be eliminated) 4. Use Naive Bayes classifier to identify URLs that are faculty pages 5. Scrape each faculty page classified as a faculty page My project implements part of this work. Maybe the rest can be done by a future student of this class! References https://sebastianraschka.com/Articles/2014_naive_bayes_1.html https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b https://towardsdatascience.com/implementing-a-naive-bayes-classifier-for-text-categorization-in-five-steps-f9192cdd54c3 https://www.xml-sitemaps.com/ https://code.google.com/archive/p/sitemap-generators/wikis/SitemapGenerators.wiki http://doors.stanford.edu/~sr/universities.html https://pagedart.com/blog/how-to-find-the-sitemap-of-a-website/ Course Project Proposal Prakash Manden Pmanden2@illinois.edu Note: I thought did everything as per the documentation by 10/25 deadline. However, I was not aware that a separate proposal was to be uploaded until I saw some notes on Piazza today. Instruction are a bit all over the place, and I wasn't aware of the need, otherwise I would have submitted it before the deadline. Improving a System - ExpertSystem Search I plan to implement the following project mentioned in the 'Course Project Topic' (text copied as is from the document) The ExpertSearch system (http://timan102.cs.illinois.edu/expertsearch//) was developed by some previous CS410 students as part of their course project! The system aims to find faculty specializing in the given research areas. The underlying data and ranker currently comes from the MP2 submissions of the previous course offering. You can read more about it here (Sections 3.6 and 4: Project are especially relevant). The code is available here. Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms from scratch. Automatically crawling faculty webpages Recall that you developed scrapers for faculty web-pages in MP2.1, which, in general, can be a time-consuming task. So, the question is can we automate this process? Some challenges include: * Identifying faculty directory pages: First, we need to identify the pages from where faculty web-pages can be mined. In MP2.1, we used faculty directory pages as the starting point to find faculty webpages. So, given a university website, can we automatically identify the directory pages? This can be posed as a classification task, i.e. classify a URL into a directory page vs. non-directory page. We have a huge resource of directory page URLs available in the sign-up sheet. These can be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs(e.g. other URLs on the university websites, product websites, news sites,etc.). These would be the ""negative"" examples. * Identifying faculty webpage URLs: Next, we need to extract the faculty webpages from the directory pages. This can again be posed as a classification task. Given a URL, can we identify whether it is a faculty webpage or not? We have a huge resource of faculty webpage URLs (available under MP2.3 on Coursera). These would be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs (e.g. other URLs on the university websites, product websites, news sites, etc.) to get the ""negative"" labels. Improving a System -ExpertSystem SearchCS410 Course Project, Fall 2020pmanden2@illinois.eduObjectivesFrom Improving a System -ExpertSystemSearch12ObjectivesGiven a URL classify it as directory vs non-directory pageGiven a URL classify it as faculty or non-faculty page[This project was done by myself as a one person team]Naive Bayes classifierCredit -https://sebastianraschka.com/Articles/2014_naive_bayes_1.htmlText representation -Bag of wordsImplementation*2 Classifiers built*Directory link classifier*Faculty link classifier*Directory link classifier Training data*Positive Samples collected from MP2.1signup spreadsheet (as suggested)*900 Samples are available (in file 'directory-positives.txt' in source code)*Negative Samples*A scraper utility was written to collect links from university webpages, which excluded all links with keywords such as 'directory', 'staff' etc. to ensure negative samples*6592 samples are available (in file 'directory-negatives.txt' in source code)Implementation*Faculty link classifier Training data*Positive samples*Faculty pages from MP2.3data on Coursera was used as suggested*16492 samples are available (in file 'faculty-pages-positives.txt')*Negative samples*The same scraper was used to generate links from university websites. Links with keywords such as 'faculty', 'staff' are removed to ensure negative links*6592 samples available (in file 'faculty-pages-negatives.txt')Source code -(Core file)*Classifier.py*Implements Naive Bayes Classifier in 'class naive_bayes_classifier'*Input*Name of file that contains positive samples*Name of file that contains negative samples*Number of samples to be used for training*'initialize_classifier' method*Loads the samples from the specified files*Calculates and saves term document matrix, term frequency, no of terms, total counts for each terms for both positive and negative samples*'classify' method*Accepts a url*Calculates probability of the words in urlwith laplacesmoothing for both positive and negative classes*Returns True if the positive class probability is > negative class probability*It can be run to test the code independently*To test, Run -'python Classifier.py', tests model accuracy and shows resultsModel Accuracy*Directory Classifier (with 800 samples, 100 test data)*Precision 0.96*Recall 0.94*F1 score 0.94*Faculty Classifier (with 6000 samples, 300 test data)*Precision 0.99%*Recall 0.98*F1 score 0.99Source Code -Utilities*scraper.py : Scrapes all universities listed at*http://doors.stanford.edu/~sr/universities.html*1,088 universities available*Identify 10 links from the home page of each university*Removes all links with key words that identify directory pages, such as 'directory', etc.*Removes all links with key words that identify faculty pages, such as 'faculty' etc.*Generates a list that is used as negative samples for both classifiers*Run -python scraper.py*Will take 3-4 hours to run. (Change 'no_universities_to_scrape' line 137 to test on limited number of universities)Source Code -UI, Data files *ExpertSearchapp UI was modified to test URLs*UI changes are only for testing & and not relevant to improving the system!*templates/index.html*Modified to support a new pull downmenu*static/index.js*Java script code changes to interface with backend*Data files*directory_positives.txt (Directory positive samples)*directory_negatives.txt (Directory negative samples)*faculty_positives.txt (Faculty positive samples)*faculty_negatives.txt (Faculty negative samples)Running the Application*To run ExpertSearchApp run*python server.py*Open a browser and point to http://localhost:8095/*Demo! -Play ProjectDemo.mp4 CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/pritomsaha/CourseProject	"Documentation Reproducing the Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Darius Nguyen Pritom Saha Akash Trisha Das 1)An  overview of the function of the code : Manyapplicationsneedtextualtopicstobestudiedtogetherwithexternaltimeseries. Thispaperproposesageneraltextminingsystemforthediscoveryofthistypeof causalthemesfromthetext.Weimplementedthealgorithmpresentedinthepaperin Python.Ourimplementation combinesagivenprobabilistictopicmodelwithtime-series causalanalysistodiscovertopicsthatarebothcoherentsemanticallyandcorrelated withtime-seriesdata.Asdescribedinthepaper,weiterativelyrefinethediscovered topicstoincreasethecorrelationwiththetimeseries.Timeseriesdataprovides feedback at each iteration by imposing prior distributions on parameters. Inourexperiment,weexaminethe2000U.S.Presidentialelectioncampaign.Theinput textdataisfromNewYorkTimesarticlesfromMaythroughOctoberof2000.Asa non-textualtime-seriesinput,weusepricesfromtheIowaElectronicMarkets2000 PresidentialWinner-Takes-AllMarket.Wealsoexperimentedwithstocktime-series data (AAMRQ vs. AAPL). Our implementation can determine causal topics efficiently. WeusedPLSAasourtopicmodelingmethod.GrangerTestisusedtofindoutasetof candidatecausaltopicswithlags.PearsonCorrelationisusedtofindoutword-level causalityinourimplementation.ThoughtheauthorsofthepaperusedRprogramming languagetodotheGrangerCausalitytest,wediditinPythontomakethecode compactandmanageable.Weusedonlyoneprogramminglanguagetocompletethe full implementation. 2)Documentation of how the software is implemented: The main parts of our code are as follows: 1.Data Preparation:  We used text data from the 2000 U.S. Presidential election campaign. The input text data is from the New York Times articles from May through October of 2000. We filter them for keywords ""Bush"" and ""Gore,"" and use paragraphs mentioning one or both words. Also, we scrapped time-series data from [2][3][4]. We used the ""normalized"" price of one candidate as a forecast probability of the election outcome: (Republic AvgPrice)/(Republic AvgPrice + Democratic AvgPrice). Other data cleansing steps were also taken. We also experimented with the High and Low prices of each party. 2.Generating topics:  We used PLSA (Probabilistic Latent Semantic Analysis)  topic modeling method to find out representative topics from the text data. This uses the Expectation-Maximization (EM) algorithm. We used a PLSA implementation package from [1]. This is many times faster than our previous implementation in MP3. That's why we used this implementation in our program. 3.Causal analysis with time series data:  We used the Granger test for measuring causality. We utilized the Python library  grangercausalitytests from  statsmodels.tsa.stattools  for the Granger test. The output of this part gives significant causal topics with significance >95%. Also, each topic is associated with a corresponding time lag which can describe the causality of the corresponding topic the most. 4.Word level causality : We used the Pearson Correlation test for measuring the word level causality in our implementation. Each significant topic determined by the granger test is passed through this next level for finding word-level causality. Within each topic, the words with significant positive correlation and negative correlations are separated and grouped into two distributions. These distributions work as priors in the next iteration. 5.Generating Topic Priors:  We generated topic priors for the causal topics and incorporated them into the next iteration PLSA. 3)Usage documentation: Our program was built in a Jupyter notebook and ran on Google Colab. Please see the comments we added inline for instructions on running specific blocks of code. The code blocks can be run sequentially from beginning to end to see the results. Please find the Jupyter notebook on our GitHub repository: ( https://github.com/pritomsaha/CourseProject ) 4)Participation : netid participation huy2 *Logistics: *Create project in CMT & added project meta *Group coordination/planning *Contribute to proposal/progress report/documentation/presentation *Project work: *Contribute to paper investigation, determining implementation steps, finding solution to roadblocks *Implemented stock time series cleansing and processing *Implement word-level causality modeling *Implement topic prior generation paksash2 *Logistics: *Create the project on github and upload the project proposal and project progress reports. *Contribute to proposal/progress report/documentation/presentation *Project work: *Contribute to paper investigation, determining implementation steps, finding solution to roadblocks *Extract the appropriate text data from new york time corpus [5]. *Preprocess and cleaning text data so that it can be used to train plsa model. *Finding out a working fast plsa model [1] and making necessary changes to the implementation of the plsa model so that it is appropriate for the topic modeling in the paper. *Making room for incorporating topic prior feedback to the plsa model. *Implementing the code for calculating topic coverage that is required in finding causal topics. *Combining all the modules (topic modeling, topic-level causality, word-level causality) to make it workable for running. trishad2 *Logistics *Group coordination/planning *Contributed to the proposal, progress report, presentation, and documentation References: 1.https://github.com/henryre/numba-plsa 2.https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 3.https://thestockmarketwatch.com/stock/stock-data.aspx?symbol=AAMRQ&action=showHistory&page=1&perPage=25&startMonth=4&startDay=1&startYear=2000&endMonth=9&endDay=30&endYear=2020&endDateLite=11%2F15%2F2020 4.https://finance.yahoo.com/quote/AAPL/history/ 5.http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19 *Project work : *Contributed to the paper investigation, determining implementation steps, finding the solution to roadblocks *Stock time series cleansing and processing *Scraped time-series data(Iowa Electronic Markets (IEM)3 2000 Presidential Winner-Takes-All Market, AAMRQ, AAPL stock price data) from websites [2][3][4] *Implemented topic-level causality using Granger test *Worked on hyperparameter tuning *Wrote the documentation and created the PowerPoint presentation *Fixed bugs in code 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Ans: Team members: Name NetID Darius Nguyen (Captain) huy2 Pritom Saha Akash pakash2 Trisha Das trishad2 2.Which paper have you chosen? Ans: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 3.Which programming language do you plan to use? Ans: Python 4.Can you obtain the datasets used in the paper for evaluation? Ans: We are trying to get the dataset. 5.If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6.If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? CS 410 - Final Project Progress Report Team PDT 1.Steps completed: a.Dataset collection: Both text and time series datasets are collected. b.Textdatapreprocessing:Wehavepreprocessedtextdataforusingitintothe PLSA algorithm. c.Topicmodeling:WehaveappliedthePLSAalgorithmtofindtopicsfromtext data. d.Causaltopicfiltering:Wehaveappliedthegrangertestforfindingsignificant casual topics based on time-series data. e.Word-levelcausalitymodeling:Wehaveimplementedthepartforfiltering causally related words (positive and negative) for each causally significant topic. 2.Steps outstanding: a.Topic prior generation b.Apply topic prior feedback to topic modeling 3.Challenges: a.Wearefacingsomeproblemsinunderstandingwhichdistribution(negativeor positiveorboth)foreachcausallysignificanttopictoapplyaspriortotopic modeling. Name NetID Darius Nguyen (Captain) huy2 Pritom Saha Akash pakash2 Trisha Das trishad2 CourseProject:- Reproducing the Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Team members Darius Nguyen Pritom Saha Akash Trisha Das Our program was built in a Jupyter notebook and ran on Google Colab. There are two colab file: 1. Corpus Processing: It is used to get text corpus and preprocessing the text. 2. ITMTF: This is the main colab where the ITMTF model is run sequencially. Please see the comments we added inline for instructions on running specific blocks of code. The code blocks can be run sequentially from beginning to end to see the results. Please read the Documentation.pdf for more details. Presentation https://illinois.zoom.us/rec/play/i-3hI-q_f39vHiVYxZJbjuZqNIwKxF-3-qz1Lo8t3VftqgSmI5hJuGkqAuRgABcRNnumMCWDhfHfh5PK.UiSG0GrcsaiEr6gK?continueMode=true&_x_zm_rtaid=DKz7zUCcQWy8BRcZYpWxJQ.1607897144160.874f474aff50d1bfcba2cf282ecb81a9&_x_zm_rhtaid=454"
https://github.com/pshreyareddy/CourseProject	"1 PROJECT DOCUMENTATION REPORT CS 410 TEXT INFORMATION SYSTEMS FALL 2020 TEXT CLASSIFICATION COMPETITION Twitter Sarcasm Detection Team Member Email Shreya Reddy Peesary peesary2@illinois.edu 2 INTRODUCTION Recognizing sarcasm in text is an important task for Natural Language processing to avoid misinterpretation of sarcastic statements as literal statements. The use of sarcasm is prevalent across all social media, micro-blogging and e-commerce platforms. Sarcasm detection is imperative for accurate sentiment analysis and opinion mining. It could contribute to enhanced automated feedback systems in the context of customer-based sites. Twitter is a micro-blogging platform extensively used by people to express thoughts, reviews, discussions on current events and convey information in the form of short texts. Twitter data provides a diverse corpus for sentences which implicitly contain sarcasm. The aim of this project is to classify the tweets in the given dataset as SARCASM or NOT_SARCASM and beat the base line score of F1: 0.723. DATASET FORMAT Train.jsonl: Shape: 5000 rows x 3 columns Train dataset is balanced with 2500 SARCASM and 2500 NOT_SARCASM samples. Test.jsonl: Shape: 1800 rows x 3 columns Column Definitions: * response : The Tweet to be classified * context : the conversation context of the response . The context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. * label : SARCASM or NOT_SARCASM * id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , 3 you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). SYSTEM DESIGN * The code for this project was done using Google Collaboratory (Using GPU Run Time Type). * The source code can be directly run from Collaboratory using the link by executing the cells step by step from the jupyter notebook link below: https://colab.research.google.com/github/pshreyareddy/CourseProject/blob/main/FinalSubmissionForBERT.ipynb 1. Click on open in collab button in FinalSubmissionForBERT.ipynb 2. Sign in to collab 3. Go to Runtime -- Change Runtime Type -- Select Hardware Accelerator as GPU PACKAGES USED * Python 3.6.9 * Pandas: Python data analysis library. * Numpy: Python library for working with arrays. * Re: This is used for regular expression matching. * String: For common string operations * Sklearn: Python machine learning library * TensorFlow2.1.0: an open source software library for high performance numerical computation * Keras: a deep learning API written in Python, running on top of the machine learning platform TensorFlow. * TensorFlow Hub: a repository of trained machine learning models. DATA PREPROCESSING: * Converted the context column into comma separated string. * Removed @USER from response and context columns * Removed <URL> from response and context columns. * Removed digits (0-9) * Removed special characters ,white space characters and allowing only alphanumeric characters. 4 * Converted the response and context columns to lowercase. BERT MODEL: In order to beat the base line score used state of the art language models in BERT (Bidirectional Encoder Representations from Transformers). BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT's goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata: 1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence. 2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences. 3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence. PROCESS : * Added a Keras Bert layer using Bert uncased L-12_H-768_A-12. 5 * Tokenization approach using Bert full tokenizer followed as per standard usage in tensor flow hub (https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1). * Define an Encode function to separate text into tokens, masks and segments * Split the train data set into training and validation * Apply the encode function on both response and context. 6 * Build Model with inputs as an array of context and response input word ids, mask and segment ids and output layer is a simple neural network with 1 neuron and activation function : sigmoid which is used for classification. * Hyperparameters: Optimizer : Adam learning rate : 1e-6 , Loss : Binary Cross Entropy 7 * Model layout : * Training the model : train_input (Array of train_context and train_reponse with in-turn contains input word ids,mask and segment ids ) val_input (Array of val_context and val_reponse with in-turn contains input word ids,mask and segment ids ) used epochs = 3 and batch_size = 3 Process takes around 30-40 min in Collaboratory using GPU. * Predictions: Predictions are saved in answer.txt with id and target label. 8 RESULTS: Got an F1 score of 0.7402464065708418 using this approach beating the base line score. OTHER APPROACHES : * Same Bert process with response only input without context.(F1score: 0.6905005107252298 * Simple LSTM with Stanford Glove embeddings (F1 score : 0.6645126548196015) 9 REFERENCES: * Documentation of BERT on TFHub * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding * https://www.analyticsvidhya.com/blog/2020/10/simple-text-multi-classification-task-using-keras-bert/ * https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model 10 PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Shreya Reddy Peesary (NetID : peesary2) (1-person team) 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods I have some basic hands-on and working knowledge on artificial neural networks namely CNN,RNN,LSTM,GRUs using TensorFlow and Keras. I'm willing to learn more recent state of the art techniques like Google Research's BERT etc and experiment with these techniques. 4. Which programming language do you plan to use? Python PROJECT PROGRESS REPORT CS 410 Text Information Systems Fall 2020 Completed Tasks * Became familiar with BERT and its usage for text classification problems. * Worked on cleaning of tweets by removing urls , emojis, punctuations, special characters etc. which may not help in classification. * Implemented an approach using BERT in google collab using Tensor Flow. * Fine-tuned the model and was able to beat the baseline score. Pending Tasks * Source code refactoring, optimization and final submission. * Documentation. * Tutorial presentation. Challenges/Issues No current issues. Initially, faced issues with setup and computing power with my current laptop to use tensor flow for running BERT. So, started working on google collab for coding and running and was successful in making submissions beating baseline score. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities Project Final Documentation Software Project Demo Video Link Text Classification Competition: Twitter Sarcasm Detection (https://github.com/CS410Fall2020/ClassificationCompetition) Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/purecod3/CourseProject	"Progress ReportEd Pureza (epureza2@illinois.edu)Dansi Qian (dansiq2@illinois.edu)Joe Everton (everton2@illinois.edu)Change of ScopeWe initially planned to reproduce Latent Aspect Rating Analysis without Aspect Keyword Supervision. We read through the paper, discussed among ourselves, and documented our understandings here. We had an hour-long conversation with Prof. ChengXiang Zhai (one of the authors of the paper) and email correspondence with Prof. Hongning Want (the main author), and eventually decided that none of us had the substantial math background required to reproduce the paper. Instead, we decided to reproduce Latent Dirichlet Allocation (https://dl.acm.org/doi/pdf/10.5555/944919.944937 and http://times.cs.uiuc.edu/course/510f18/notes/lda-survey.pdf), which was referenced by the original paper and also described briefly in week 9 of the course.Which tasks have we completed?*We have read both papers for Latent Dirichlet Allocation (LDA) and documented ourunderstanding here. There is no analytical solution to the E-M algorithm in LDA. Instead, the optimization can be done using variational inference or Gibbs Sampling.*We have implemented LDA using Gibbs Sampling (an initial version without learningrate, and a subsequent version that applies a learning rate across iterations).*We have implemented pre-processing for text-classification datasets, the LDA model training on term frequencies, the inference of new documents using trained model (note that LDA is generative), and compared the classification accuracy of Support Vector Machines (SVM) using term frequencies and using topic weights. There were no significant differences in accuracy between the two for the text message spam filter dataset (short documents) or for the fake news detection dataset (long documents).Which tasks are pending?*We are working on the variational inference version of LDA (https://github.com/purecod3/CourseProject/blob/main/lda_var_inf.py). *We plan to compare the accuracy of SVM classification using the variational inference version of LDA with the baseline (SVM using term frequencies).What challenges do we face?*There do not seem to be canonical ways to implement either the variational inference or the Gibbs sampling for LDA. There are some resources on implementation details but they are not necessarily correct or perform well. As aresult, we need to compare multiple resources based on our understanding of the algorithm, and potential implement multiple versions and compare their results.*There is a significant amount of hyperparameter tuning (number of topics, learning rate (and decay) for Gibbs Sampling, stop criterion definition and threshold for the variational inference, etc.). There is very little literature about how the hyperparameters should be set or tuned for LDA in practice so it would require trail and errors on our side. Project Proposal: Reproduce  Latent aspect rating analysis without aspect keyword supervision Team Members: *Ed Pureza  epureza2@illinois.edu  (Team Leader) *Dansi Qian  dansiq2@illinois.edu *Joe Everton  everton2@illinois.edu Paper Chosen: *Latent aspect rating analysis without aspect keyword supervision *Latent aspect identification (without supervision) *Latent aspect rating *Latent aspect weighting *LARAM vs. existing solutions / LARA (paper 23) OR newer for reference / comparison Programming Language *Python Notable Resources / Libraries *Numpy Dataset for Evaluation *Original dataset used in the paper:  http://times.cs.uiuc.edu/~wang296/Data/ *TripAdvisor *Amazon Rough Timeline 11/01/2020 Kickoff 11/22/2020 MVP Ready 11/29/2020 Code complete and ready for tuning if necessary 12/06/2020 Code complete and final results ready 12/13/2020 Final report ready Topic Mining with LDA (and LARAM) CS 410 Text Information Systems Course Project December 2020 Authors: Dan Qian (dansiq2), Joe Everton (everton2), Ed Pureza (epureza2) CMT ID: 84 Introduction1 Latent Aspect Rating Analysis without Aspect Keyword Supervision2 Latent Dirichlet Allocation2 Objective3 Data3 Evaluation criteria3 Tools4 Methods4 Variational Inference5 Gibbs Sampling5 Experiment Results6 Conclusion8 Successes9 Opportunities9 References9 Appendix10 LDA VS PCA10 Introduction Our team originally selected the research paper titled ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"" by Honging Wang, Yue Lu, and ChengXiang Zhai. After spending a considerable amount of time on attempting to understand the implementation details, we decided there were too many obstacles to continue recreating the research paper's results. Taking into consideration Professor Zhai's counsel, we decided to switch our topic to Latent Dirichlet Allocation. Our work is based on ""Latent Dirichlet Allocation"" by David Blei, Andrew Ng, and Michael Jordan [3]. Latent Aspect Rating Analysis without Aspect Keyword Supervision When a review is submitted for a product, it can be assumed that the reviewer had different aspects of the product in mind and a priority on those aspects that led to a given overall rating. The objective of latent aspect rating analysis without aspect keyword supervision is to identify the following in a set of reviews: *Main aspects that the reviewers considered *Weights of the identified aspects in each review and overall *Ratings given to each aspect identified The generative model involved inferring the latent aspect assignments (i.e., probability of a word drawn from a topic) and the aspect weights for each document and estimating the following corpus-level parameters: *Distribution of vocabulary words in each aspect *Prior distribution of aspects for the whole corpus *Sentiment polarity of each vocabulary word for each aspect *Average weight of each aspect in the ratings *Variance of weights for each aspect *Variance of the ratings Our main challenges included implementing the log-likelihood of each review that required an understanding of Jensen's inequality for convex functions and finding the maximums of the latent aspect assignments and prior distribution of aspects that involved implementing gradient-based optimization solutions. After an office hour session with Professor Zhai, correspondence with Dr. Hongning Wang and evaluating the effort required to bring this type of project to completion, we decided to switch our focus to Latent Dirichlet Allocation, which we found the implementation to be more feasible given the timeline and amount of available information on the subject. Latent Dirichlet Allocation Latent Dirichlet Allocation (LDA) is a generative probabilistic model for identifying topics in a collection of documents using Bayesian modeling. LDA draws the topic assignment of a word from a distribution of topics and the word from the word distribution of topics for a document. Unlike PLSA where the documents in the corpus define the probabilities, LDA can be used to generalize new documents by sampling from the distributions found by the model (i.e., the dataset used to generate the model serves as training data for new data). This is possible thanks to hyperparameters to Dirichlet distributions that generate the multinomial distributions, rather than basing them on training documents. Objective Our first objective was to have LDA find k topics from a corpus of documents. This would give us a sense at how well our implementation is doing at learning the topics from the corpus. The model will also be able to predict topic weights for new documents. Our objectives also include applying document topic weights predicted by the model to a text classification task. One way of evaluating the utility of the model discussed in the paper is to see how well the topic models can be used to to train a document classifier, compared to classifying with a plain bag of words feature set based on the same documents. Using the topic distribution inferred by the model and labeled data, a classification model can be generated. The two classification applications covered in this paper include the identification of fake news and spam messages. It is interesting to compare and contrast this approach of classification with LDA to Principal Component Analysis. We explore this relationship in the  appendix . Data The original data set could not be found. We decided to use two other publicly available datasets from Kaggle. Evaluation criteria We will use Support Vector Machine (SVM) as our classification model for both LDA outputs and document terms. Using document terms to measure a baseline, the following results were obtained: Dataset Source # of Docs # of Terms Classes Spam https://www.kaggle.com/team-ai/spam-text-message-classification 5,157 8,741 Spam: 13% Not Spam: 87% Fake News https://www.kaggle.com/mohamadalhasan/a-fake-news-dataset-around-the-syrian-war 804 10,157 Fake: 47% Not Fake: 53% Dataset Number of Features Training Data Size Precision Recall Average F1 Fake News 2,020 200 0.562 0.694 0.621 Our success criteria is to achieve better or equal performance metrics with LDA. Doing so would prove that similar or better classification accuracy can be achieved using significantly less features. Tools Python was the programming language for this project. The following packages were used: numpy scipy math pandas re random sklearn time Methods We attempted two different methods for creating the LDA models. The first method uses variational inference as explained in the original Latent Dirichlet Allocation paper [3] and further explained by Chase Geigle [4] to calculate the word assignment to a topic and the topic distribution for each document (posterior inference). The general steps are outlined in the figure below (from [3]). The second method uses Collapsed Gibbs Sampling for the posterior inference which resembles a Markov-chain Monte Carlo method. The method is described in Geigle's paper and the implementation is detailed in Sterbak's' post [7]. Our implementation differed from the original LDA paper in the following ways: 1.We used random initialization for word topic assignments and document topic distributions 2.We borrowed the method implemented in gensim for updating alpha which claims [8] to use the Newton-Raphson method described in [3]. This method uses an additional Spam 892 2,000 0.992 0.855 0.919 learning rate parameter . In some cases this alpha updating code did not work well; to get around this, we used a simpler linear combination to calculate alpha. 3.We replaced the log likelihood estimation with a distance measurement for the document topic distribution. The decision to deviate from the papers was based on our level of understanding, the level of complexity involved in original papers, and the amount of time allocated for the project. Variational Inference This method uses an EM algorithm similar to what is used in PLSA with the exception of the variables updated and parameters estimated. The corpus level parameters include the following priors: *: the parameter to the dirichlet distribution which forms the topic multinomial distribution in the training data *b: the parameter to the dirichlet distribution which forms the word multinomial distribution for each topic These parameters are estimated in the M step. The following document level variables are inferred the E step: *F: word topic assignment * How to Use Programming Language and Packages *Python 3.x *Packages: pandas, numpy, scipy, sklearn, math, re, random, time Executing Code Fork or download Github repo. Open in IDE and run file(s) or use command prompt (e.g.,  python lda_var_inf_without_smoothing.py ). Start with  lda_var_inf_without_smoothing.py . If unexpected results are encountered, try  lda_var_inf_without_smoothing_v2.py . Optionally, you can also try the other variations with  lda_gibbs_sampling.py  and lda_var_inf.py . To use a different input dataset, your file will need text and classification columns. Modify the source file ( input_path ) and column settings ( text_column ,  label_column ) in the  load_csv  function call. (vocabulary_size, training_term_doc_matrix, training_labels, testing_term_doc_matrix, testing_labels, vocabulary) = load_csv(input_path = 'FA-KES-Dataset.csv', test_set_size=100, training_set_size=200, num_stop_words=50, min_word_freq=5, text_column='article_content', label_column='labels', label_dict = {'1': 1, '0': 0}) lda_var_inf_without_smoothing_v2.py  has both datasets (fake news and spam) coded. Comment/uncomment to switch between datasets. Setting Parameters Set the following parameters to tune the model: *num_topics : number of topics to model lda.train(num_topics=10, term_doc_matrix=training_term_doc_matrix, iterations=20, e_iterations=10, e_epsilon=0.1, initial_training_set_size=50, initial_training_iterations=20) See  video walk-thru  for additional information. Latent Dirichlet Allocation This project is based on the paper written by D. Blei, A. Ng, and M. Jordan - Latent Dirichlet Allocation. https://dl.acm.org/doi/pdf/10.5555/944919.944937. Latent Dirichlet Allocation estimates topic disributions and topic word distributions in a generative model that can be used to infer topic distributions and word topic assignments for new documents. We use this modeling capability to evaluate the application of LDA on classification of documents using a significantly reduced number of features as compared to a bag of words based classification method. Team Members Ed Pureza, epureza2 (captain) Dan Qian, dansiq2 Joe Everton, everton2 Files |Deliverable|File|Description| |----------|----|-----------| |Project Proposal|Project Proposal_Reproduce Latent aspect rating analysis without aspect keyword supervision.pdf|Original project proposal submitted on October 24, 2020| |Progress Report|ProgressReport.pdf|Progress report with accomplishments, challenges, and remaining planned activities as of November 29, 2020| |Project Documentation|ProjectDocumentation.pdf|Project documentation submitted December 8, 2020| |Project Video Walk-through|https://mediaspace.illinois.edu/media/t/1_jbzbbspv|Video presentation of project| |Project Tutorial|ProjectTutorial.pdf|Project tutorial for reproducing experiments (also outlined below)| |LDA without Smoothing|lda_var_inf_without_smoothing.py|Code for running LDA using variational inference and gensim-based alpha update method| |LDA without Smooting v2|lda_var_inf_without_smoothing_v2.py|Code for running LDA using variational inference. Use if Python environment setup issues are encountered.| |LDA with Collapsed Gibbs Sampling|lda_gibbs_sampling.py|LDA implementation using Collapsed Gibbs Sampling| |Original LDA Code with Variational Inference|lda_var_inf.py|First attempt for implement LDA with variational inference method| |Fake News Dataset|FA-KES-Dataset.csv|Input dataset with news articles classified as fake news or not fake news| |Spam Dataset|spam.csv|Input dataset with news articles classified as spam or ham (not spam)| How to Use Progamming Language and Packages Python 3.x Packages: pandas, numpy, scipy, sklearn, math, re, random, time Executing Code Fork or download Github repo. Open in IDE and run file(s) or use command prompt (e.g., python lda_var_inf_without_smoothing.py). Start with lda_var_inf_without_smoothing.py. If unexpected results are encountered, try lda_var_inf_without_smoothing_v2.py. Optionally, you can also try the other variations with lda_gibbs_sampling.py and lda_var_inf.py. To use a different input dataset, your file will need text and classification columns. Modify the source file (input_path) and column settings (text_column, label_column) in the load_csv function call. python (vocabulary_size, training_term_doc_matrix, training_labels, testing_term_doc_matrix, testing_labels, vocabulary) = load_csv(input_path = 'FA-KES-Dataset.csv', test_set_size=100, training_set_size=200, num_stop_words=50, min_word_freq=5, text_column='article_content', label_column='labels', label_dict = {'1': 1, '0': 0}) lda_var_inf_without_smoothing_v2.py has both datasets (fake news and spam) coded. Comment/uncomment to switch between datasets. Setting Parameters Set the following parameters to tune the model: - num_topics: number of topics to model python lda.train(num_topics=10, term_doc_matrix=training_term_doc_matrix, iterations=20, e_iterations=10, e_epsilon=0.1, initial_training_set_size=50, initial_training_iterations=20) See video walk-thru for additional information."
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	"CS 410: Text Information System Expert Search System Project Progress Report Team Name:  BayToBay Team Captain:   Pushpit Saxena (netid: pushpit2) 1) Which tasks have been completed? -Design and architecture of the proposed modules of the project. -Extract and Crawl Web Pages -Utilized  scrapy  framework with Python to crawl web pages and identify connected links. -Utilized  BeautifulSoup  toolkit with Python to extract text from web pages. -Removed special characters using regular expressions and extracted text data from web pages. -Expert Bios page classification -PreProcessing Data -Using NLTK Python toolkit -Removed stop words -Stemming -XGBoost   Text Classifier -Tf-Idf vectorizer Name Net-Id Govindan Kutty Menon gvmenon2 Harikrishna Bojja hbojja2 Pushpit Saxena pushpit2 -Assumes that tokens are stemmed and lowercase -Remove stopwords -Used the dataset from  here  as positive text and regular web pages as negative text. -Logistic Regression  Classifier -Tf-Idf vectorizer -Assumes that tokens are stemmed and lowercase -Remove stopwords -Used the dataset from  here  as positive text and regular web pages as negative text. 2) Which tasks are pending? -Extract and Crawl Web Pages -Increase scalability to crawl and extract data from web page with higher data content and connected links -Multi-thread extract and crawl scripts to achieve parallel processing -Utilize a delimiter to segregate page identifier and content information -Redesign process to accept input web pages from a file and make the process more configurable -Validation of script for different University web pages -Expert Bios page classification -PreProcessing Data -Writing unit tests -Bi-LSTM text classification (using tensorflow/pytorch on Google Colab) -Use the data set from  here -Compare accuracy against other classifiers -XGBoost Classifier -With word2Vec vectorizer (Glove) -Assumes that tokens are stemmed and lower case -Remove stopwords -Used the dataset from  here  as positive text and regular web pages as negative text. -(Stretch goal) Topic modelling on expert bios. -Integration -Integrate different components which are being developed individually -Integrate with existing functionality of the expert search system -End to end script execution/ validation -Run scripts (in order) and validate conformance to the need/ original design. -Submission/ Demo delivery -Finalize and create deliverables for project submission -Finalize and create deliverables for demo 3) Are you facing any challenges? -Extracting and crawling web pages using lower powered CPUs and less memory on personal machines is posing a challenge from performance and scalability perspective. -Extracting/ crawling web pages of different universities using a scrapy framework requires understanding of the page structure. Page structure could be different for different web pages and this is a challenge for crawling and scraping required contents. -While we were able to collect positive training set for classifiers, collecting ""quality"" negative data set could be tricky -Collected positive training set from CS410-MP2 -Trying to use general web crawled data for negative examples. **********1)***********************1)****************1)***""distilbert-base-nli-mean-tokens""*Encoding saved at: ""BERT_encoding_classifier.bert-embeddings-for-classification.pkl""***-*-*-***[(0, '0.019*""graphics"" + 0.018*""paper"" + 0.015*""image"" + 0.014*""siggraph"" + 0.010*""computer""'), (1, '0.033*""conference"" + 0.027*""international"" + 0.018*""systems"" + 0.014*""proceedings"" + 0.013*""networks""'), (2, '0.015*""translation"" + 0.012*""speech"" + 0.010*""blandford"" + 0.010*""rogers"" + 0.009*""nadia""'), (3, '0.022*""research"" + 0.019*""function"" + 0.019*""study"" + 0.017*""details"" + 0.014*""state""'), (4, '0.014*""programming"" + 0.013*""system"" + 0.011*""architecture"" + 0.011*""software"" + 0.010*""memory""'), (5, '0.017*""electrical"" + 0.014*""engineering"" + 0.012*""power"" + 0.009*""control"" + 0.009*""signal""'), (6, '0.015*""learning"" + 0.008*""conference"" + 0.008*""machine"" + 0.007*""model"" + 0.007*""paper""'), (7, '0.030*""research"" + 0.026*""university"" + 0.025*""computer"" + 0.024*""science"" + 0.018*""engineering""'), (8, '0.016*""theory"" + 0.012*""algorithms"" + 0.011*""algorithm"" + 0.009*""graph"" + 0.008*""complexity""'), (9, '0.008*""guohong"" + 0.008*""ghosh"" + 0.008*""patrick"" + 0.008*""veeravalli"" + 0.008*""thomas""')]**************** Project Documentation: Web Scraping 1.Scrapy Framework - a.Utilized  scrapy  framework with Python to crawl web pages and identify connected links. 2.Beautiful Soup - a.Utilized  BeautifulSoup  toolkit with Python to extract text from web pages. b.Removed special characters using regular expressions and extracted text data from web pages. Classification: 1.Once the web pages are scraped using the 'Crawl-n-Extract' module (see run help below). The web pages text is saved in a raw text file with each web page text on one line (similar to MP2 exercise). 2.The classification task is to classify a given webpage as a Faculty Page (positive class: 1) or Non Faculty Page (negative class: 0). 3.We trained multiple classifiers as part of this exercise and results (F1-score and accuracy) can be found in our presentation. 4.We have built most of the model based on the text classification techniques that we have learnt as part of the course. We have mainly used Tf-Idf vectorizer and Bert based sen2Vec to vectorize the text of the web pages. *Logistic Regression 1.Used  Scikit learn Logistic regression  module as well as  scikit learn tf-idf vectorizer  for vectorizing the web pages. We have 2.Used nltk library to remove stopwords and for stemming. 3.F1-Score (hold out test set):  0.9702970297029703 *XGBoost 1.Used  XGBoost  library for training the model 2.  Scikit learn tf-idf vectorizer  for vectorizing the web pages 3.F1-Score (hold out test set):  0.9829683698296837 *Deep learning 1.Tensorflow - Used tensorflow to build the deep-learning model. a.Four layered neural network model(excluding input layer) b.Uses Adam optimizer with learning  0.0001 c.Loss is evaluated using  Binary Cross Entropy d.F1 Score on test data set:  0.9963 2.NLTK - Utilized NLTK to pre-process the data. Pre-processing involved following list of steps a.Remove stop words b.Stemming *(Experimental) BERT encoding based Logistic Regression 1.Generated the embeddings for the web pages text using a pre-trained BERT model (' distilbert-base-nli-mean-tokens ') 2.Used the  transformer  library to generate the embeddings 3.Built an experimental  Logistic regression  model a.F1 Score for this experimental model is:  0.9874055415617129 4.As, we can see there is an improvement over the Tf-IDF based Logistic Regression model, hence this experiment was successful. But we were getting better results with our custom neural network model, we didn't pursue this further. In future, we would like to expand on this idea of transferring knowledge (from BERT encoding) to inform and improve simple classifiers. This saves a lot of time as the pre-trained models are generally trained on large corpus of wikipedia/web pages and will provide a decent knowledge base to transfer to a simple classifier model which then can be trained much easily in a short span of time. Topic Modeling on Faculty bios Used  gensim  library to generate the topic model based on the  compiled bios Running information can be found in ' Run Help ' section NER Model Used  spacy  library to extract named entities, in particular faculty names and different organizations that are mentioned on their page. Both these entities will help in improving the search index. Run help: Python setup ( python >= 3.5, only web scraping needs python 3.5, other modules work with newer versions of python also ): Please run :  pip install -r source_code/requirements.txt You can also use the venv by using following commands: *cd source_code *python -m venv <venv-name> *source <venv-name>/bin/activate *python3 -m pip install -r requirements.txt Web Scrapping: 1.Scrapy (Use Python 3.5 Version): a.Script created using Scrapy framework will extract links for the web-page b.Create a scrapy project using i.""Scrapy startproject link_extractor c.Configure items.py to create class ""LinkExtactorItem"" class and define below fields for the item i.url_from ii.urk_to d.Create a new spider using below command i.Scrapy genspider uiuc e.Crawl links of a web-page using below command i.scrapy crawl uiuc -o links.csv -t csv It will extract all the links associated with web page to csv file. 2.Split  (Use Python 3.5 Version) a.Split the input file into multiple files using script ""split.py"" 3.Extract web contents  (Use Python 3.5 Version) a.Extract web contents using script ""python extract.py uiuc1.csv uiuc1.txt"". This script uses Beautiful soup as the toolkit 4.Merge(Use Python 3.5 Version) a.Merge all output files using script ""merge.py"" Web Page Classification Logistic Regression Model (python >= 3.7): Training: *Navigate to  source_code/logistic_regression *Run  python train.py *The model file will be saved at source_code/logistic_regression/logit.model Inference: *There are two ways to run inference once: *By providing a file with all the webpage (one str per web page on one line): *From root directory (TextInformationSystem-CourseProject) run following command:  python -m source_code.logistic_regression.inference <webpage data file path> *For file format look at: TextInformationSystem-CourseProject/source_code/Crawl-n-Extract/Merge/UIUC.txt *By FacultyClassifier module (which is an entry point for all of our classification models): *Create an object of FacultyClassifier class with classifier_type = 'logit' and then run predict method on list of web pages text *A sample run is defined in the ""__main__"" block of that module. XGBoost Model (python >= 3.7): Training: *Navigate to  source_code/XGboost *Run  python train.py *The model file will be saved at  source_code/XGboost/xgb.model Inference: *There are two ways to run inference once: *By providing a file with all the webpage (one str per web page on one line): *From root directory (TextInformationSystem-CourseProject) run following command:  python -m source_code.XGboost.inference <webpage data file path> *For file format look at: TextInformationSystem-CourseProject/source_code/Crawl-n-Extract/Merge/UIUC.txt *By FacultyClassifier module (which is an entry point for all of our classification models): *Create an object of FacultyClassifier class with classifier_type = 'xgb' and then run predict method on list of web pages text *A sample run is defined in the ""__main__"" block of that module. Neural Network Model (python >= 3.7): Predict Faculty Pages (Inference): Trained neural network model is of size 187MB on disk. Git has an upper cap of 100MB. For this reason, the trained model has been uploaded to box. Please download the trained neural network model from the box link below Step 1:  Download the folder 'neural_network_model_v2' from the link below. https://uofi.box.com/v/es-neural-network-model-v2 Step 2:  Ensure that directories 'model', 'vectorizer' are placed directly under 'fully_trained_model'. Step 3:  Ensure below directories are safely extracted. 'fully_trained_model/model' 'fully_trained_model/vectorizer' Step 4:  Point 'crawled_data_path' in the script 'classify/infer_crawled_data.py' point to your webpage dump.  Note:  The crawled data should have one doc per line. Separated by '#####' ( Eg:  ~/source_code/Crawl-n-Extract/Merge/UIUC.txt) Step 5:  Run the script 'infer_crawled_data' and observe the faculty links printed in the console. Note:  You can also use source_code/faculty_page_classifier/faculty_page_classifier/FacultyClassifier to programmatically use this model to run predictions. You need to initialize the FacultyClassifier with classifier_type='nn'. But this also requires copying over the model file from the location provided in Step 1 and saving it to the appropriate folder. Training the model: We suggest that you use the model we have already trained. Please follow the steps below if you intend to train a new model using the source code. *Use the script ""model_driver.py"" under the package below to train the model ""source_code.neural_network_ml_classifier.train"" *Initialize the path to training data and target location a.data_base_dir - base directory to training data. This data must have been preprocessed using PreProcessor.py b.The script then splits the data into 70% train and 30% validation dataset c.This data used then used to train the four layered Neural network model Topic Modeling on Faculty bios Used  gensim  library to generate the topic model Topic model can be run using the following notebook (this notebook contains self explanatory documentation to run the notebook): *source_code/topic_modeller/TopicModelling.ipynb NER Model Used  spacy  library to extract named entities, in particular faculty names and different organizations that are mentioned on their page. Both these entities will help in improving the search index. NER Model can run using following notebook (this notebook contains self explanatory documentation to run the notebook): *source_code/ner_model/spacy_ner.ipynb Also, extracted faculty names as well as different organizations mentioned on the faculty pages. These two data points will help in indexing the faculty bios for searching. *Commands to run this step: *cd source_code/ner_model *python extract_names_and_orgs.py <bios_dir> <destination_dir_for_names> <destination_dir_for_orgs> *E.g. bios_dir = source_code/data/compiled_bios, destination_dir_for_names = source_code/data/compiled_bios_names, destination_dir_for_orgs = source_code/data/compiled_bios_orgs Names are extracted and saved in source_code/data/compiled_bios_names/ Organizations are extracted and saved in source_code/data/compiled_bios_orgs We have used the same filenames as faculty bios for easier retrieval when this information is needed later References: Scrapy -  https://docs.scrapy.org/en/latest/ Beautiful Soup -  https://pypi.org/project/beautifulsoup4/ Beautiful Soup -  https://www.kite.com/python/docs/bs4.BeautifulSoup Scikit Learn -  https://scikit-learn.org *Logistic Regression - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html *TFIDf vectorizer- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html Gensim -  https://pypi.org/project/gensim/ *https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ XGBoost -  https://xgboost.readthedocs.io/en/latest/python/python_api.html *https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/ Spacy -  https://spacy.io/  ,  https://spacy.io/models/en#en_core_web_md Tensorflow -  https://www.tensorflow.org/api_docs/python/tf/all_symbols *https://www.tensorflow.org/tutorials/keras/text_classification_with_hub CourseProject Project Presentation Document: Final Presentation Document Project Documentation and run help: Documentation Video Presentation: Presentation If you face any issue viewing the video at the above link you can also download the video from here Project Source Code: SourceCode Team Members' Email Ids (Please feel free to reach out for any questions or clarifications): Pushpit Saxena (pushpit2@illinois.edu) Govindan Menon (gvmenon2@illinois.edu) Harikrishna Bojja (hbojja2@illinois.edu) CS-410 Text Information Systems: Final Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name Net-Id Govindan Kutty Menon gvmenon2 Harikrishna Bojja hbojja2 Pushpit Saxena pushpit2 Team Name:     BayToBay Team Captain:    Pushpit Saxena (netid: pushpit2) 2.What system have you chosen? Which subtopic(s) under the system? System ExpertSearch System Subtopics 1.Automatically crawling faculty webpages 2.Extracting relevant information from faculty bios 3.Stretch Goals: Topic Modeling on faculty bios. 3.Briefly describe the datasets, algorithms or techniques you plan to use Techniques for Subtopic: Automatically crawling faculty: 1)Users provide University URLs as input. 2)The websites (URLs) will be leveraged to crawl and collect the dataset for classifying ""faculty directory pages"" v/s ""non-faculty"". 3)To collect the dataset for classifying ""faculty webpage"" v/s ""non-faculty webpage"", the faculty directory pages will be crawled. Classification task: 1)Classify ""faculty directory pages"" v/s ""non-faculty"" 2)Classify ""faculty webpage"" v/s ""non-faculty webpage"" 3)We will try to build different classification models. Some of the models that we are planning to try and evaluate are  SVM, XGBoost, DL (hugging face transformers)  etc. We will leverage the URLs as well as text on the web-pages to extract features (vectorize text) to train the classification models. Extracting relevant information from faculty bios: 1)Enhance Regular-expression to extract email-id from the bios 2)Enhance Named Entity Recognition (NER) to identify/ extract faculty name from bios. 3)Topic mining & keyword extraction on faculty bios information (stretch goal, if time permits). 4)We are planning to use Spacy, Gensim as well as Flair (BERT) for both NER models as well as topic modelling. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Automatic Crawler: 1)We will provide an API/Console utility which can take an university home URL and then crawl the web-pages from that URL and return the list of faculty web-pages. This will demonstrate that the crawler and classifier that we have built are working. Web-pages classification tasks: 1)We will use some of the data from MP2.2 and MP2.3 assignment as suggested in the project topics document and demonstrate the performance of our models (F1 metric). Extracting relevant information from bios pages: 1)As we are planning to use some recent and advanced NER models, we will show the difference in performance between regex based email/name extraction vs our NER model and also we will clearly state whether we are able to improve performance from a simple regex based approach or not. Topic modelling: 1)We will demonstrate the top topics we can identify from the faculty bios dataset. Also, we will try to calculate topic coherence metric. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 1)We will build our own system and will try to use the datasets provided by the existing ExpertSearch system (e.g. faculty bios, names, emails etc.) for training and evaluation. 2)We will also use the regex based NER model as a baseline to compare some of the more advanced NER models that we will try to train. 6.Which programming language do you plan to use? Programming Language Python, Javascript, HTML 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Number Main tasks Hours 1 Evaluation/ Analysis of the current system/ process and any baseline models already implemented in the system. 12 hrs. 1.1 Any required training data annotation 6 hrs. 2 Researching algorithms and overall system design 10 hrs. 3 Development of functionalities envisioned 3.1 Recursive Crawler Implementation 10 hrs 3.2 Web Page Classification Model Implementation 10 hrs 3.3 Information Extraction 8 hrs 3.4 Topic Modeling 8 hrs 4 Self-evaluation and modification 24 hrs 5 Demo Preparation and Documentation 6 hrs. 6 Collaboration 3 hrs. Total 96 hrs."
https://github.com/pwasal3/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview.
https://github.com/quickcatch/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rakesh-patnaik/CourseProject	"Rakesh Patnaik (rakeshp2@illinois.edu) 12/13/2020Reproducing a Paper: Latent Aspect Rating Analysis without keyword supervision Visual depiction of the task*Input *Review texts *Overall rating *Assumed aspects in the review (Location, Room, Service etc) *Output *Latent aspects (Topic model used to extract text from review corresponding to a topic) *Rating associated to each latent aspect *Weight associated to each latent aspect *Validation *Mean squared error from ground truth overall rating.Stages in the process*Pre-processing (preprocessing_Sec5_1.py) *Lowercase *Remove punctuation characters *Remove stop words *Lemmatize *Processing and Analyzing (Main.py) *Model topics based on ""Service"", ""Cleanliness"", ""Overall"", ""Value"", ""Location"", ""Rooms"", ""Sleep Quality"" *Identify words that correlate to model topics *Use regression to identify topic rating to maximize probability to ground truth latent ratings *Use regression to identify topic weights to maximize probability to ground truth overall rating *Calculate mean squared error to ground truth ratings *Output results to results/results.txt and MSE to stdout.How to run the code*git clone https://github.com/rakesh-patnaik/CourseProject.git *cd CourseProject *python3 -m venv env *source env/bin/activate *pip install --upgrade pip *python -m pip install -r requirements.txt *python -m nltk.downloader stopwords *python -m nltk.downloader punkt *python -m nltk.downloader wordnet *python preprocessing_Sec5_1.py *python Main.pyResults*Results will be output to results/results.txt *Mean Squared Error will be output to stdout *(env) rakesh@Rakeshs-MacBook-Pro-4.local:~/work/uiuc-mcsds/cs410-fall2020/CourseProject$ python preprocessing_Sec5_1.py (env) rakesh@Rakeshs-MacBook-Pro-4.local:~/work/uiuc-mcsds/cs410-fall2020/CourseProject$ python Main.py Total reviews: 183 MSE: 2.99805326964421 Project Progress Report - 11/28/2020 - week 14 1) Which tasks have been completed? *Test DataSet *Preprocessing Test DataSet - section 5.1 of paper 2) Which tasks are pending? *Aspect Identification *Aspect rating prediction *Aspect weight prediction 3) Are you facing any challenges? *Identifying visualization methods and parameters to visualize. I will be using python matplotlib but will need to identify the correct parameters to plot. *Have yet to test if the code will run on local laptop with the entire dataset Following is the proposal to execute CS410 final project. Team Details I would be working as a individual to execute the final project NetIDs: *rakeshp2 Topic Of the Project This project would try to reproduce the following paper on the topic of Latent aspect rating analysis without aspect keyword supervision *Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Review comments by customers and users are a valuable source of feedback for businesses. Mining information and quantifying a customer review can help reduce human effort. A generic review usually has the following components: *topics or aspects such as location, service, cleanliness, specific amenities, food etc *A relative weight placed on each of the topics. Some topics might carry more weight to a certain customer and hence determines the final rating. latent aspect rating analysis ( lara ) refers to the task of inferring both opinion ratings on topical aspects ( e.g. , location , service of a hotel ) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings If a system is fed the aspects to look for in a review it would need human intervention and hence defeating the purpose of large scale data mining on review texts. A generative model that identifies the topics and weights associated with each of the topics would make the system function without supervision and hence scale up. Hence this topic of LARA without aspect keyword supervision is valuable and interesting. Implementation technology Python3 Dataset:  http://times.cs.uiuc.edu/~wang296/Data/ Project Tasks *Design *Implementation *Testing Latent Aspect Rating Analysis without aspect keyword supervision Implementation for Paper https://www.cs.virginia.edu/~hw5x/paper/p618.pdf Project Topic This project would try to reproduce the following paper on the topic of Latent aspect rating analysis without aspect keyword supervision Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Abstract from the paper Mining detailed opinions buried in the vast amount of review text data is an important, yet quite challenging task with widespread applications in multiple domains. Latent Aspect Rating Analysis (LARA) refers to the task of inferring both opinion ratings on topical aspects (e.g., location, service of a hotel) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings. A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords. However, the aspect information is not always available, and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews. In this paper, we propose a unified generative model for LARA, which does not need pre-specified aspect keywords and simultaneously mines 1) latent topical aspects, 2) rat- ings on each identified aspect, and 3) weights placed on dif- ferent aspects by a reviewer. Experiment results on two dif- ferent review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords. Because of its generality, the proposed model can be applied to ex- plore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interest- ing application tasks, such as aspect-based opinion summa- rization, personalized entity ranking and recommendation, and reviewer behavior analysis Derived Abstract Review comments by customers and users are a valuable source of feedback for businesses. Mining information and quantifying a customer review can help reduce human effort. A generic review usually has the following components: topics or aspects such as location, service, cleanliness, specific amenities, food etc A relative weight placed on each of the topics. Some topics might carry more weight to a certain customer and hence determines the final rating. latent aspect rating analysis ( lara ) refers to the task of inferring both opinion ratings on topical aspects ( e.g. , location , service of a hotel ) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings If a system is fed the aspects to look for in a review it would need human intervention and hence defeating the purpose of large scale data mining on review texts. A generative model that identifies the topics and weights associated with each of the topics would make the system function without supervision and hence scale up. Hence this topic of LARA without aspect keyword supervision is valuable and interesting. Run the project ```shell script Running the project git clone https://github.com/rakesh-patnaik/CourseProject.git cd CourseProject python3 -m venv env source env/bin/activate pip install --upgrade pip python -m pip install -r requirements.txt python -m nltk.downloader stopwords python -m nltk.downloader punkt python -m nltk.downloader wordnet python preprocessing_Sec5_1.py python Main.py ``` Demo https://github.com/rakesh-patnaik/CourseProject/blob/main/demo_presentation.pdf Implementation technology Python3 Dataset subset of TripAdvisor data from http://times.cs.uiuc.edu/~wang296/Data/"
https://github.com/raman162/UofICS410FinalProject	"CS 410 Final Project Progress Report: Topic Mining Healthcare Data Completed Tasks [x] Curate Dataset [x] Exported 20,000 labelled (positive/not positive) notes summarizing telehealth care encounters [x] Automated de-identification of PHI using DEID Software Package [x] Exported Sample Of De-Identification to work on Topic Mining Pending Tasks [ ] Topic Mining [ ] Prep De-Identification notes for Topic Mining tool [ ] Run a series of topic mining trails changing the number of topics trying to be mined [ ] Perform analysis of topic coverage for positive notes and non-positive notes [ ] Classifier [ ] Create a classifier that can determine if a telehealth note summary had a positive outcome for the patient [ ] Train on half of the de-identified dataset [ ] Run on other half of the de-identified dataset and compare the results Current Challenges/Issues Faced Due to the export of the providers from medical systems containing a lot of invalid data such as names of lab tests, diseases and specialists, the list had to be manually scrubbed to avoid the DEID tool from falsely redacting it thinking they were doctor names. This was a manual tedious process that required review of a few thousand records. Detailed Progress Updates Curate Dataset 1. Exporting Labelled Dataset Two CSV files, each containing 10,000 records was exported from the TimeDoc system. One file named positive_encounters.csv contained only notes that were labelled as a positive outcome due to the telehealth services while another file named no_positive_encounters.csv only contained notes that weren't labelled as a positive outcome for the patient. The format of the exported CSV files are as follows: <note_id>,<patient_id>,<purpose>,<duration>,<note> The <purpose> is an array of attributes of the telehealth encounter, it is selected from a pre-defined list and can provide insights to the actions of the telehealth encounter. The <duration> is the total amount of time the telehealth encounter took, and <note> is the free-text nursing note summarizing the encounter that we will be performing topic mining on. 2. Automated De-Identification of Protected Health Information (PHI) To ensure we're adhering to HIPPA ^[HIPPA Privacy Guidelines, https://www.physionet.org/content/deid/1.1/] we have to redact Protected Health Information (PHI). This was redacted using the De-Identification (DEID) Software Package ^[De-Identification Software Package, https://www.physionet.org/content/deid/1.1/]. For the DEID to be effective, it required creating separate files of patient names and identifiers pid_patientname.txt, doctor first names doctor_first_names.txt, doctor last names doctor_last_names.txt, locations local_places.txt, and company names company_names.txt. The pid_patientname.txt was created by referencing all the patients from the two exported CSV lists and curating a file formatted with each line as <PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME>. The doctor_first_names.txt and the doctor_last_names.txt files were created by referencing exporting each care team member such as their Primary Care Physician (PCP), Radiologist, etc. and writing each name to a new line. Both files were scrubbed for duplicates and invalid data. The local_places.txt was created by taking each address related for the patient and writing the city to a town to each line. The company_names.txt file was created by listing out the pharmacies and local healthcare organizations that the patient utilizes and writing each to a new line. For the DEID to perform the redaction of PHI, it required to be fed the notes in a particular format. So the exported CSV file had to be transformed to the following format: START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>|||| <DOCUMENT_CONTENT> ||||END_OF_RECORD We accomplished this transformation for both of the CSV exported files using a ruby script located at deid/convert_csv_to_text.rb and ran the following commands: ``` convert csv files to deid text format ruby deid/convert_csv_to_text.rb positive_encounters.csv ruby deid/convert_csv_to_text.rb no_positive_encounters.csv ``` The output produced two files named positive_encounters.text and no_positive_encounters.text respectively. Afterwards the new text files were copied into the DEID directory and we ran the DEID perl script to remove the PHI using the following commands: ``` redact PHI from text files perl deid.pl positive_encounters deid-output.config perl deid.pl no_positive_encounters deid-output.config ``` The output produced two PHI redacted files named positive_encounters.res and no_positive_encounters.res. To convert the files back into the CSV format, we used the following script located at deid/convert_res_to_csv.rb and ran the following commands: ``` convert redacted res files to csv ruby deid/convert_res_to_csv.rb \ positive_encounters.res \ positive_encounters.csv ruby deid/convert_res_to_csv.rb \ no_positive_encounters.res \ no_positive_encounters.csv ``` The output produced two files named positive_encounters.res.csv and no_positive_encounters.res.csv. 3. Sample De-Identified Notes Data Since the DEID is an automated tool, we had to account for the possibility on not redacting all PHI data. To minimize actual PHI distributed, 50 samples were taken from both the positive_encounters.res and no_positive_encounters.res file and manually verified to not contain PHI. After the verification, the sampled data was shared with the rest of the team so to create scripts that will perform the topic mining and analysis. This was accomplished by utilizing the deid/sample_res.rb and running the following commands: ``` sample res files per manual review ruby deid/sample_res.rb positive_encounters.res 50 ruby deid/sample_res.rb no_positive_encounters.res 50 The output produced two files only containing 50 redacted PHI documents named positive_encounters.res-sample-50.res no_positive_encounters.res-sample-50.res ``` After manual verification that all PHI was redacted, the sampled files were transformed to the original CSV format by running the following commands: ``` convert sampled res files to CSV format ruby deid/convert_res_to_csv.rb \ positive_encounters.res-sample-50.res \ positive_encounters.csv ruby deid/convert_res_to_csv.rb \ no_positive_encounters.res-sample-50.res \ no_positive_encounters.csv ``` The output produced two files named positive_encounters.res-sample-50.res.csv and no_positive_encounters.res-sample-50.res.csv. The four sampled redacted PHI documents can be provided at request by emailing rsw2@illinois.edu CS410FinalProjectProgressReport:TopicMiningHealth-careDataCompletedTasksCurateDatasetExported20,000labelled(positive/notpositive)notessummarizingtelehealthcareen-countersAutomatedde-identificationofPHIusingDEIDSoftwarePackageExportedSampleOfDe-IdentificationtoworkonTopicMiningPendingTasks#TopicMining#PrepDe-IdentificationnotesforTopicMiningtool#Runaseriesoftopicminingtrailschangingthenumberoftopicstryingtobemined#Performanalysisoftopiccoverageforpositivenotesandnon-positivenotes#Classifier#Createaclassifierthatcandetermineifatelehealthnotesummaryhadapositiveoutcomeforthepatient#Trainonhalfofthede-identifieddataset#Runonotherhalfofthede-identifieddatasetandcomparetheresultsCurrentChallenges/IssuesFacedDuetotheexportoftheprovidersfrommedicalsystemscontainingalotofinvaliddatasuchasnamesoflabtests,diseasesandspecialists,thelisthadtobemanuallyscrubbedtoavoidtheDEIDtoolfromfalselyredactingitthinkingtheyweredoctornames.Thiswasamanualtediousprocessthatrequiredreviewofafewthousandrecords.DetailedProgressUpdatesCurateDataset1.ExportingLabelledDatasetTwoCSVfiles,eachcontaining10,000recordswasexportedfromtheTimeDocsystem.Onefilenamedpositive_encounters.csvcontainedonlynotesthatwerelabelledasapositiveoutcomeduetothetelehealthserviceswhileanotherfilenamedno_positive_encounters.csvonlycontainednotesthatweren'tlabelledasapositiveoutcomeforthepatient.TheformatoftheexportedCSVfilesareasfollows:<note_id>,<patient_id>,<purpose>,<duration>,<note>The<purpose>isanarrayofattributesofthetelehealthencounter,itisselectedfromapre-definedlistandcanprovideinsightstotheactionsofthetelehealthencounter.The<duration>isthetotalamountoftimethetelehealthencountertook,and<note>isthefree-textnursingnotesummarizingtheencounterthatwewillbeperformingtopicminingon.2.AutomatedDe-IdentificationofProtectedHealthInformation(PHI)Toensurewe'readheringtoHIPPA1wehavetoredactProtectedHealthInformation(PHI).ThiswasredactedusingtheDe-Identification(DEID)SoftwarePackage2.FortheDEIDtobeeffec-tive,itrequiredcreatingseparatefilesofpatientnamesandidentifierspid_patientname.txt,doc-1HIPPAPrivacyGuidelines,https://www.physionet.org/content/deid/1.1/2De-IdentificationSoftwarePackage,https://www.physionet.org/content/deid/1.1/1torfirstnamesdoctor_first_names.txt,doctorlastnamesdoctor_last_names.txt,locationslocal_places.txt,andcompanynamescompany_names.txt.Thepid_patientname.txtwascreatedbyreferencingallthepatientsfromthetwoexportedCSVlistsandcuratingafileformat-tedwitheachlineas<PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME>.Thedoctor_first_names.txtandthedoctor_last_names.txtfileswerecreatedbyreferencingex-portingeachcareteammembersuchastheirPrimaryCarePhysician(PCP),Radiologist,etc.andwritingeachnametoanewline.Bothfileswerescrubbedforduplicatesandinvaliddata.Thelocal_places.txtwascreatedbytakingeachaddressrelatedforthepatientandwritingthecitytoatowntoeachline.Thecompany_names.txtfilewascreatedbylistingoutthepharmaciesandlocalhealthcareorganizationsthatthepatientutilizesandwritingeachtoanewline.FortheDEIDtoperformtheredactionofPHI,itrequiredtobefedthenotesinaparticularformat.SotheexportedCSVfilehadtobetransformedtothefollowingformat:START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>||||<DOCUMENT_CONTENT>||||END_OF_RECORDWeaccomplishedthistransformationforbothoftheCSVexportedfilesusingarubyscriptlocatedatdeid_support/convert_csv_to_text.rbandranthefollowingcommands:#convertcsvfilestodeidtextformatrubydeid_support/convert_csv_to_text.rbpositive_encounters.csvrubydeid_support/convert_csv_to_text.rbno_positive_encounters.csvTheoutputproducedtwofilesnamedpositive_encounters.textandno_positive_encounters.textrespectively.AfterwardsthenewtextfileswerecopiedintotheDEIDdirectoryandwerantheDEIDperlscripttoremovethePHIusingthefollowingcommands:#redactPHIfromtextfilesperldeid.plpositive_encountersdeid-output.configperldeid.plno_positive_encountersdeid-output.configTheoutputproducedtwoPHIredactedfilesnamedpositive_encounters.resandno_positive_encounters.res.ToconvertthefilesbackintotheCSVformat,weusedthefollowingscriptlocatedatdeid_support/convert_res_to_csv.rbandranthefollowingcommands:#convertredactedresfilestocsvrubydeid_support/convert_res_to_csv.rb\positive_encounters.res\positive_encounters.csvrubydeid_support/convert_res_to_csv.rb\no_positive_encounters.res\no_positive_encounters.csvTheoutputproducedtwofilesnamedpositive_encounters.res.csvandno_positive_encounters.res.csv.3.SampleDe-IdentifiedNotesDataSincetheDEIDisanautomatedtool,wehadtoaccountforthepossibilityonnotredactingallPHIdata.TominimizeactualPHIdistributed,50samplesweretakenfromboththepositive_encounters.resandno_positive_encounters.resfileandmanuallyverifiedtonotcontainPHI.Aftertheverification,thesampleddatawassharedwiththerestoftheteamsotocreatescriptsthatwillperformthetopicminingandanalysis.Thiswasaccomplishedbyutilizingthedeid_support/sample_res.rbandrunningthefollowingcommands:#sampleresfilespermanualreview2rubydeid_support/sample_res.rbpositive_encounters.res50rubydeid_support/sample_res.rbno_positive_encounters.res50Theoutputproducedtwofilesonlycontaining50redactedPHIdocumentsnamedpositive_encounters.res-sample-50.resno_positive_encounters.res-sample-50.resAftermanualverificationthatallPHIwasredacted,thesampledfilesweretransformedtotheoriginalCSVformatbyrunningthefollowingcommands:#convertsampledresfilestoCSVformatrubydeid_support/convert_res_to_csv.rb\positive_encounters.res-sample-50.res\positive_encounters.csvrubydeid_support/convert_res_to_csv.rb\no_positive_encounters.res-sample-50.res\no_positive_encounters.csvTheoutputproducedtwofilesnamedpositive_encounters.res-sample-50.res.csvandno_positive_encounters.res-sample-50.res.csv.ThefoursampledredactedPHIdocumentscanbeprovidedatrequestbyemailingrsw2@illinois.edu3 CS 410 Final Project Proposal: Topic Mining Healthcare Data Team Members Satish Reddy Asi- sasi2@illinois.edu Srikanth Bharadwaz Samudrala - sbs7@illinois.edu Raman Walwyn-Venugopal (Project Coordinator/Team Leader) - rsw2@illinois.edu Motivation TimeDoc is a telemedicine company that focuses on ensuring patients receive proactive healthcare to improve the treatment of their chronic diseases. Since 2015, TimeDoc has accumulated roughly 1.8 million unstructured text documents created by licensed healthcare professionals that summarize telemedicine encounters with patients. Out of that total dataset there are 13,496 unique documents that have been labelled as a positive outcome for the patient by their author. A positive outcome is very important for a patient as it indicates that their health was improved which also translates into them valuing the telemedicine service. Utilizing TimeDoc's data, our primary goal is to identify more patients that had these positive outcomes and identify patients that are more likely to have a positive outcome. To accomplish this our team plans to primarily use python and open source tools as described in our solution below. Solution Our assumption is that there is a relation between a patient's profile and their likelihood of having a positive outcome. If this assumption is correct we can begin recommending patients with certain profiles for healthcare professionals to focus on. Curate Dataset Expected Duration: 20 hours Expected Completion Date: November 15th To ensure we're adhering to HIPPA ^[HIPPA Privacy Guidelines, https://www.hhs.gov/hipaa/for-professionals/privacy/index.html] guidelines. The telemedicine encounter document data and the patient profiles need go through a Patient Health Information ^[Patient Health Information, https://www.hipaajournal.com/what-is-protected-health-information/] (PHI) de-identification process. We will automate the PHI de-identification for free text fields on the patient profile and the telemedicine documents utilizing a De-Identification Software Package ^[De-Identification Software Package, https://www.physionet.org/content/deid/1.1/]. All structured fields of a patient profile known to contain PHI will be replaced as well. Topic Mining and Analysis on Dataset Expected Duration: 20 hours Expected Completion Date: November 24th We plan on mining the topic models using Latent Dirichlet Allocation ^[Latent Dirichlet Allocation (LDA), https://arxiv.org/pdf/1711.04305.pdf] for both the de-identified encounter telemedicine and patient profiles from the curated dataset. We plan on using gensim ^[gensim: python library for topic Modeling, https://pypi.org/project/gensim/] and nltk ^[nltk: Natural Language Toolkit python package, https://pypi.org/project/nltk/] in python to accomplish this task. After completing the topic modeling we will investigate if there is a significant difference between telemedicine encounters that were labelled as a success story versus ones that were not. We will also be investigating if there is a significant difference between the patient profiles that had success stories versus the patient profiles that do not have any success stories. Recommendation Expected Duration: 20 hours Expected Completion Date: December 5th After we have identified the topics of telemedicine documents and patients that had positive outcomes, we would like to identify other patients that may fit this criteria. We can accomplish this by indexing all the patient profiles by their mined topic data, the likelihood of the topic will be used as the score for the vector. We will then recommend patients that have topic profiles similar to the topics of patient profiles that had positive outcomes. The similarity score will be calculated using one of the newer variations of Okapi BM25. In addition to this, we can also design a similar recommendation system to attempt to identify telemedicine documents that may contain a positive outcome for out patient. The utility for both of these systems will be evaluated by licensed healthcare professionals at TimeDoc. Miscellaneous Other libraries that may be used but not discussed are listed but not limited to; pyspark ^[pyspark, https://www.gangboard.com/blog/what-is-pyspark], scipy ^[scipy, https://www.scipy.org/scipylib/index.html], metapy ^[metapy, https://github.com/meta-toolkit/metapy], pandas, ^[pandas, https://pandas.pydata.org/], numpy, ^[numpy, https://numpy.org/] and whoosh ^[whoosh, https://whoosh.readthedocs.io/en/latest/intro.html]. CS410FinalProjectProposal:TopicMiningHealthcareDataTeamMembers*SatishReddyAsi-sasi2@illinois.edu*SrikanthBharadwazSamudrala-sbs7@illinois.edu*RamanWalwyn-Venugopal(ProjectCoordinator/TeamLeader)-rsw2@illinois.eduMotivationTimeDocisatelemedicinecompanythatfocusesonensuringpatientsreceiveproactivehealthcaretoimprovethetreatmentoftheirchronicdiseases.Since2015,TimeDochasaccumulatedroughly1.8millionunstructuredtextdocumentscreatedbylicensedhealthcareprofessionalsthatsummarizetelemedicineencounterswithpatients.Outofthattotaldatasetthereare13,496uniquedocumentsthathavebeenlabelledasapositiveoutcomeforthepatientbytheirauthor.Apositiveoutcomeisveryimportantforapatientasitindicatesthattheirhealthwasimprovedwhichalsotranslatesintothemvaluingthetelemedicineservice.UtilizingTimeDoc'sdata,ourprimarygoalistoidentifymorepatientsthathadthesepositiveoutcomesandidentifypatientsthataremorelikelytohaveapositiveoutcome.Toaccomplishthisourteamplanstoprimarilyusepythonandopensourcetoolsasdescribedinoursolutionbelow.SolutionOurassumptionisthatthereisarelationbetweenapatient'sprofileandtheirlikelihoodofhavingapositiveoutcome.Ifthisassumptioniscorrectwecanbeginrecommendingpatientswithcertainprofilesforhealthcareprofessionalstofocuson.CurateDatasetExpectedDuration:20hoursExpectedCompletionDate:November15thToensurewe'readheringtoHIPPA1guidelines.ThetelemedicineencounterdocumentdataandthepatientprofilesneedgothroughaPatientHealthInformation2(PHI)de-identificationprocess.WewillautomatethePHIde-identificationforfreetextfieldsonthepatientprofileandthetelemedicinedocumentsutilizingaDe-IdentificationSoftwarePackage3.AllstructuredfieldsofapatientprofileknowntocontainPHIwillbereplacedaswell.TopicMiningandAnalysisonDatasetExpectedDuration:20hoursExpectedCompletionDate:November24ndWeplanonminingthetopicmodelsusingLatentDirichletAllocation4forboththede-identifiedencountertelemedicineandpatientprofilesfromthecurateddataset.Weplanonusinggensim5andnltk6inpythontoaccomplishthistask.1HIPPAPrivacyGuidelines,https://www.hhs.gov/hipaa/for-professionals/privacy/index.html2PatientHealthInformation,https://www.hipaajournal.com/what-is-protected-health-information/3De-IdentificationSoftwarePackage,https://www.physionet.org/content/deid/1.1/4LatentDirichletAllocation(LDA),https://arxiv.org/pdf/1711.04305.pdf5gensim:pythonlibraryfortopicModeling,https://pypi.org/project/gensim/6nltk:NaturalLanguageToolkitpythonpackage,https://pypi.org/project/nltk/1Aftercompletingthetopicmodelingwewillinvestigateifthereisasignificantdifferencebetweentelemedicineencountersthatwerelabelledasasuccessstoryversusonesthatwerenot.Wewillalsobeinvestigatingifthereisasignificantdifferencebetweenthepatientprofilesthathadsuccessstoriesversusthepatientprofilesthatdonothaveanysuccessstories.RecommendationExpectedDuration:20hoursExpectedCompletionDate:December5thAfterwehaveidentifiedthetopicsoftelemedicinedocumentsandpatientsthathadpositiveout-comes,wewouldliketoidentifyotherpatientsthatmayfitthiscriteria.Wecanaccomplishthisbyindexingallthepatientprofilesbytheirminedtopicdata,thelikelihoodofthetopicwillbeusedasthescoreforthevector.Wewillthenrecommendpatientsthathavetopicprofilessimilartothetopicsofpatientprofilesthathadpositiveoutcomes.ThesimilarityscorewillbecalculatedusingoneofthenewervariationsofOkapiBM25.Inadditiontothis,wecanalsodesignasimilarrecommendationsystemtoattempttoidentifytelemedicinedocumentsthatmaycontainapositiveoutcomeforoutpatient.TheutilityforbothofthesesystemswillbeevaluatedbylicensedhealthcareprofessionalsatTime-Doc.MiscellaneousOtherlibrariesthatmaybeusedbutnotdiscussedarelistedbutnotlimitedto;pyspark7,scipy8,metapy9,pandas,10,numpy,11andwhoosh12.7pyspark,https://www.gangboard.com/blog/what-is-pyspark8scipy,https://www.scipy.org/scipylib/index.html9metapy,https://github.com/meta-toolkit/metapy10pandas,https://pandas.pydata.org/11numpy,https://numpy.org/12whoosh,https://whoosh.readthedocs.io/en/latest/intro.html2 CS 410 - Final Project: Topic Mining Healthcare Data & Classification Team Members Raman Walwyn-Venugopal - rsw2@illinois.edu Srikanth Bharadwaz Samudrala - sbs7@illinois.edu Satish Reddy Asi - sasi2@illinois.edu Quick Links Proposal PDF Progress Report PDF Video Demo (YOUTUBE) Overview The goal of this project is to perform topic mining and classification on telehealth encounter nursing notes for notes that documented a positive outcome for the patient form the telehealth services. To accomplish this, we divided the project into four steps; 1. curating the dataset 2. build topic miner and mine topics from dataset 3. perform analysis on topics 4. build binary classifier that attempts to predict if a document is a positive outcome for the patient Curating Dataset Requirements: - ruby 2.X - perl 5.X Note: This code was ran on ubuntu 18.04 and ubuntu 20.04 Exporting Raw Dataset The source of the data is from TimeDocHealth that has a care team that focuses on providing telehealth services to patients with multiple chronic diseases. Two CSV files, each containing 10,000 records were exported from the TimeDoc system. One file named positive_encounters.csv contained only notes that were labelled as a positive outcome due to the telehealth services while another file named no_positive_encounters.csv only contained notes that weren't labelled as a positive outcome for the patient. The format of the exported CSV files are as follows: <note_id>,<patient_id>,<purpose>,<duration>,<note>. <purpose> is an array of attributes of the telehealth encounter, it is selected from a pre-defined list and can provide insights to the actions of the telehealth encounter. <duration> is the total amount of time the telehealth encounter took <note> is the free-text nursing note summarizing the encounter. This data is what the topic mining and classification will be performed Automating De-Identification of Protected Health Information (PHI) To ensure we're adhering to HIPPA Privacy Guidelines, Protected Health Information (PHI) was redacted using De-Identification (DEID) Software Package. For the DEID to be effective, it had to be configured with the following lists: - pid_patientname.txt - patient names and identifiers. Was created by referencing all the patients from the two exported CSV lists and curating a file formatted with each line as <PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME> - doctor_first_names.txt - doctor first names. Created by exporting each care team member for the patient such as their Primary Care Provider, Radiologist, etc. - doctor_last_names.txt - doctor last names. Created using same strategy as doctor first names. - unambig_local_places.txt - locations near the patient. Created using the cities, towns of addresses for patients and businesses near them. - company_names.txt - company names. Created by listing out local healthcare organizations surrounding the patient. For the DEID to perform the redaction of PHI, it required to be fed the notes in a particular format. So the exported CSV file had to be transformed to the following format: START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>|||| <DOCUMENT_CONTENT> ||||END_OF_RECORD We accomplished this transformation for both of the CSV exported files using a ruby script located at deid/convert_csv_to_text.rb and ran the following commands: ``` convert csv files to deid text format ruby deid/convert_csv_to_text.rb demo_data/positive_encounters.csv ruby deid/convert_csv_to_text.rb demo_data/no_positive_encounters.csv ``` The output produced two files named positive_encounters.text and no_positive_encounters.text respectively. Afterwards we ran the DEID perl script to remove the PHI using the following commands: ``` enter deid directory cd deid redact PHI from text files perl deid.pl ../demo_data/positive_encounters deid-output.config perl deid.pl ../demo_data/no_positive_encounters deid-output.config ``` The output produced two PHI redacted files named positive_encounters.res and no_positive_encounters.res. To convert the files back into the CSV format, we used the following script located at deid/convert_res_to_csv.rb and ran the following commands: ``` convert redacted res files to csv ruby deid/convert_res_to_csv.rb \ demo_data/positive_encounters.res \ demo_data/positive_encounters.csv ruby deid/convert_res_to_csv.rb \ demo_data/no_positive_encounters.res \ demo_data/no_positive_encounters.csv ``` The output produced two files named positive_encounters.res.csv and no_positive_encounters.res.csv. Note: Since the DEID is an automated too, we have to account for the possibility of not redacting all PHI data. To minimize actual PHI distributed 50 samples were taken form both the positive_encounters.res and no_positive_encounters.res file and manually verified to not contain PHI. This sampled may be provided upon request by emailing rsw2@illinois.edu Topic Mining Requirements: - Python 3.X Python Libraries Used: - nltk - pandas - numpy - matplotlib/pylab - regex Extracting Documents The source to extract documents from is the notes. The Telemedicine responses are saved as CSV files with multiple fields. ""notes"" from the response file is fed as Document input to our PLSA implementation. The input responses file is in CSV file and the data is delimited by "","" character. Generating stop words Stop words are generated using standard python nltk libraries. The stopwords are saved as file and is used as input for topic_miner program. stop words can be manually edited adding any tele-medicine specific words such as patient, call, treatment, phone etc.. since these are repeated frequently in every note. stop words program is run separately and the file is saved under ""patient_data"" folder where the input files are placed under. Mining Topics from Documents The topic_miner is run with data-file (in CSV format), stop-words file as input. The additional arguments to the program include number of topics, Max Iterations, Threshold, Number of Topic words. The arguments also include the path to output files: - Document Topic Coverage - Topic Word Coverage - Vocabulary - Topic words More details about the module are available at: topic miner Note: Due to the slow performance of our manually written PLSA topic miner, we created topic miner v2 that uses an open source python PLSA package and produces the same documents as our home-crafted PLSA topic miner. Setup change to directory of topic miner cd topic_miner_v2 Create new virtual environment python -m venv venv Activate virtual environment source venv/bin/activate Install required packages pip install -r requirements.txt Run Topic Miner ``` python topic_miner.py python topic_miner.py ../demo_data/all_encounters.res.csv 10 ``` Output would be: ``` topic coverage of topic probability per document in corpus all_encounters.res.csv.10-doc-topic-cov.txt grouping of words and probabilities of topic per line all_encounters.res.csv.10-topic-word-probs-grouped.txt all the probabilities for each topic per line all_encounters.res.csv.10-topic-word-probs.txt all the words for each topic per line all_encounters.res.csv.10-topics.txt vocabulary of corpus all_encounters.res.csv.vocab ``` Topic Analysis Requirements: - Python 3.X This topic analysis script performs analysis on the results of the topic miner when both the positive and non-positive encounters are included in the whole corpus. It attempts to: 1. Identify which topics are related to positive outcomes and which topics are related to non-positive outcomes 2. Pull the top words from the positive outcome topics and non-positive outcome topics 3. Highlight which top words from positive and non-positive overlap with each other versus which words are unique to their own topics 4. generates 3 files: pos-non-pos-topics.txt, top-pos-words.txt and top-non-pos-words.txt Usage python topic_analysis/topic_analysis.py \ demo_data/all_encounters.res.csv.10-doc-topic-cov.txt \ demo_data/all_encounters.res.csv.10-topics.txt Classifier Requirements: - Python 3.X - Python Virtual Environment Package (Included in Python standard library) Overview of Functionality The text classifier is responsible for reviewing the notes of the telehealth encounters and classifying the note as positive outcome versus non-positive outcome. The classifier module has the following features: - Load positive and non-positive CSV files generated from the PHI De-identification process - Clean data by removing PHI redaction sections, non-alphanumeric characters, extra white space, lemmatization, and stop words - Generate a classifier using the RandomForestClassifier from sklearn - Evaluate classifier by collecting Recall, Precision, F1 Score, micro averages per category, and the overall classification accuracy - Store classifier to a file - Load classifier from a file - Score optimizer that steps through a combination of number of features and estimators for the classifier model and returns the optimal inputs and score The process of generating the classifier requires the docs to be cleaned and vectorized into TF-IDF weights. The vectorized version of the corpus was then split into two sets; 20% for training and 80% for testing. The model used for training is the RandomForestClassifier from sklearn which is based on a Random Forest Algorithm that uses a 'random forest' of numerous decision trees. The core of the algorithm follows the steps below: - Pick N random records from the dataset - Build a decision tree on the randomly selected N records - Choose the number of trees used in the algorithm and repeat steps 1 and 2 The algorithm is ideal for classification because it is known to reduce biases with the use of multiple randomly formed decision trees and it performs well when unknown data points are introduced. Disadvantages of the algorithm is that the complexity causes it to take longer to train and process due to the amount of decision trees. Setup ``` change directory to classifier cd classifier initlize python virtual evnrionment python -m venv venv source venv/bin/activate install dependencies pip install -r requirements.txt ``` Usage Be sure to update the following constants POSITIVE_CSV_FILE and NO_POSITIVE_CSV_FILE to the true file paths of the redacted data produced from the De-Identification process. Also update the CLASSIFIER_FILE for where you want to store the classifier. The classifier module can be run as a script to quickly generate a classifier with the pre-optimized defaults determined from testing. python classifier.py This will load the data, clean the data, generate a classifier, print out the evaluation metrics and store it to the path defined in the CLASSIFIER_FILE constant. An example of the classifier evaluation is shown below. ``` precision recall f1-score support non-positive 0.88 0.95 0.91 2014 positive 0.94 0.85 0.90 1842 accuracy 0.91 3856 macro avg 0.91 0.90 0.90 3856 weighted avg 0.91 0.91 0.90 3856 Accuracy: 0.9050829875518672 ``` The classifier can be loaded and used on new documents. Enter the python console and run the following commands ``` import classifier.py text_classifier = classifier.load(classifier.CLASSIFIER_FILE) docs = [ 'Scheduled transportation for patient appointment on Thursday', 'discussed personal goals with patient for patient to work on quitting smoking' ] predictions = classifier.predict(text_classifier, docs) print(predictions) ``` Optimizations The classification accuracy score was optimized by varying the number of features and estimators (decision trees) used in the algorithm. This was a simple iterative algorithm that calculated the accuracy for each feature/estimator combination and then returned the optimal score and the combination used to accomplish. The classifier module has an optimize_score function that accepts the following arguments: docs (default: to cleaned version of dataset) - complete corpus of documents labels (default to dataset defined) - labels each document min_features (default: 1000)- start number of features to use max_features (default: 5000)- max number of features to use feature_step (default: 250) - amount to increase number of features by min_df (default: 10) - minimum document frequency for a feature to be selected max_df (default: 0.8) - maximum document frequency for a feature to be selected min_estimators (default: 750) - start number of estimators to use max_estimators (default: 2500) - max number of estimators to use estimator_step (default: 250) - amount to increase number of estimators by It outputs a dictionary that contains the following keys: feature_steps - varying features used estimator_steps - varying estimators used scores - 2-dimension numpy array containing all scores generated. Shape is feature stpes length x estimator steps length optimal_score - The highest accuracy result from the iterations optimal_num_features - The number of features used to generate optimal score optimal_num_estimators - The number of estimators used to generate optimal score The optimal number of features used was determined to be 1500 while the optimal number of estimators was determined to be 750. BONUS: Classifying the top positive and top non-positive topic words As a bonus test, we tested the classifier predictions on the top positive and non-positive words generated from the topic analysis step. ``` enter python console python import classifier module import classifier load stored classifier text_classifier = classifier.load(classifier.CLASSIFIER_FILE) ``` Classify top positive words f = open('../demo_data/all_encounters.res.csv.2-topics.txt.top-pos-words.txt', 'r') pos_docs = [f.read()] f.close() print('top pos words: ', pos_docs[0]) print('top pos words classifier predictions: ', classifier.predict(text_classifier, pos_docs)[0]) Classify top non-positive words f = open('../demo_data/all_encounters.res.csv.2-topics.txt.top-non-pos-words.txt', 'r') non_pos_docs = [f.read()] f.close() print('top non pos words: ', non_pos_docs[0]) print('top non pos words classifier predictions: ', classifier.predict(text_classifier, non_pos_docs)[0]) Output is ``` top pos words: pharmacy appointment medication service information poa cuff call care sugar pressure concern blood morning meal state report insulin transportation time top pos words classifier predictions: positive top non pos words: pharmacy medication education exercise today appt goal inhaler level weight plan pressure knee minute state phone transportation day time ncm top non pos words classifier predictions: non-positive ``` Conclusion Automating the redaction of PHI data is plausible and should be used by data scientists trying to perform analysis on free text health data to respect patient privacy and adhere to HIPPA rules. One thing to note is that the redaction process is slow on large datasets. Redacting PHI on the 20,000 document dataset took nearly an hour running on an Intel i7 10th gen processor. To avoid this issue in a production workflow with much larger datasets, an automated redacted pipeline should be considered where as soon as a note is created, a redaction process is triggered and stored in a separate bucket. When performing topic mining with our home-crafted PLSA topic miner, we noted that performance was poor on large datasets when compared to an open-source PLSA python package. This was likely due to unoptimized implementation of the EM-algorithm when handling large matrices. While performing topic analysis, we noticed that the fewer number of topics generated made it easier to relate topics to positive outcomes and other topics to non-positive outcomes. As we increased the topic count when performing PLSA, this was no longer the case and the distributions of topics among positive corpus and non-positive corpus were similar. From this behaviour, we can infer that there is definitely a difference of themes discussed in positive outcomes but that there is definitely overlapping themes. The classifier we created performs only well on large datasets. On the sampled and demo datasets of only 100 records, the maximum classification accuracy that was achieved was 85%. When training the classifier on the complete corpus of 20,000 documents, the classification accuracy jumped to 90%. These maximum scores were calculated by using an iterative algorithm that varied the number of features and the number of estimators used by the classifier. The classifier consistently had better precision at 94% when labelling positive documents versus non-positive but had worse recall at 85% for all positive documents. This means that a user can trust the result of a classification of a positive document but cannot guarantee all to be retrieved. This would be preferred for a recommendation engine. BONUS: The classifier was also tested on the documents containing the top words from positive and non-positive topics generated from the topic analysis step. The classifier correctly classified the doc containing words from positive topics as 'positive' and the doc containing words from non-positive topics as 'non-positive'."
https://github.com/realLongjiLi/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/reckoner-david/CourseProject	Project Progress Report Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu Progress: I have made two submissions for my challenge (username: reckoner) and have achieved a best rank of 8 (with only 3 epochs and small sequence lengths). A majority of the time till now was spent on getting the framework and evaluation pipelines ready. The remaining time will be spent on training larger models with better parameter searching. Task Status Comments/Challenge Dataset Preparation Done We have to create dataset in a format so that we can try a variety of problem formulations Framework Done Model Supported: Bert, Distilbert and RoBerta Models. Task Supported: Binary Classification Sentence Pair Classification Method 1:  ( distilbert-base-uncased , Only Response, no pre-processing) Done Got Rank 20  and F1_Score:0.73 Method 2: ( distilbert-base-uncased ,Response+Context, no pre-processing) Done Got Rank 8  and F1_Score:0.756 Hyper Parameter Searching In Progress Due to GPU resource constraints we have not run this step. Next Step: *Add pre-processing. *Try on larger models (Roberta-Large, bert-large) *Do hyper parameter searching Challenges The challenge with deep learning models is the resource. Currently we have only used lighter models and smaller parameters of sequence length and epochs to save compute resources. For the most competitive baselines we will have to do a hyperparameter grid search which is costly and the free google codelab resources might not be enough to run them. In that case we will have to spend some time in streamlining our pipeline and implementing early stopping metrics. Currently it takes ~1:50 min to train 1 epoch of the model. Project Report Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu Work Done: I have made 11 submissions for my challenge (username: reckoner) and have achieved a best rank of 8. After sometime I started investigating models with lower parameters which still allow me to beat the baseline. The current leaderboard results are with a distilbert model, Epoch-8, 'max_seq_length': 256, lr: 3e-5. That model is also loaded into  `demo.ipnyb`  and can be loaded and run. Task Status Comments/Challenge Dataset Preparation Done We have to create dataset in a format so that we can try a variety of problem formulations Framework Done Model Supported: Bert, Distilbert and RoBerta Models. Task Supported: Binary Classification Sentence Pair Classification Method 1:  ( distilbert-base-uncased , Only Response, no pre-processing) Done Got Rank 20  and F1_Score:0.73 Method 2: ( distilbert-base-uncased ,Response+Context, no pre-processing) Done Got Rank 8  and F1_Score:0.756 Hyper Parameter Searching Done Used wandb hyper-parameter tuning on lr. Sequence size manually tested. Model We used a transfer learning package called  SimpleTransformers . This package supports creating and training huggingface transformer models and provides necessary abstractions to make the process faster. We make use of  https://simpletransformers.ai/docs/sentence-pair-classification/  module of the library. This module trains a transformer model to predict over a pair of sentences. The idea is to use the response of the tweet and the context as the pair of sentences. i.e. text_a , text_b => Sarcasm/Not Sarcasm Here text_a is  response text_b is  concatenation of (context 2 and context 1) For testing parameters I used a 80:20 split. Final training was done on all the data points. Other Methods Tried: *I also tried a simple classification model(within simpletransformers library) with only the response. It gave me  F1_Score:0.73 *I also tried  Roberta Large model . This model did not give me a successful result. The reason for this was that as the model was very large, only small sequence lengths were fitting in the GPU (seq: 32). This proved to be too small to capture the sentence embeddings and this variation of the model failed. *I also tried  Bert and Bert Large  model. They gave similar performance to distilbert models so I investigated only distilbert. HyperParameter Tuning: SimpleTransformers library provides hyperparameter tuning support with the wandb. I investigated optimal lr. For sequence length I investigated by hand as I observed that small changes in sequence length did not affect the scores by a lot. Optimal lr: 3.1134e-5 Sequence: 256 Epoch: 8 After 4000 steps (or 8 epochs) the model stopped automatically as we put the early stop parameter. With this when the model stops learning the training procedure stops itself. (Results reported on 1000 samples withheld from the training data) Due to GPU costs I only performed hyperparameter tuning on sentence pair classification distilbert model. Summary As I wanted to use large transformers models, I made use of a specialised library which abstracts many functions required for transfer learning. Due to this the task of using complex models such as distilbert, bert, Roberta become really easy and straightforward. CourseProject Demo Video: https://drive.google.com/file/d/1oJy1Io6Fu3mG6kxovVSx_pd4jJgacsP6/view?usp=sharing If data does not download with google drive mounting please download data from: https://drive.google.com/drive/folders/19V4W6yhWjPz_qPKqQUCs86JSRpQIhS9s?usp=sharing Project Report: https://github.com/reckoner-david/CourseProject/blob/main/Project%20Report.pdf Source Code File: https://github.com/reckoner-david/CourseProject/blob/main/demo.ipynb (Notebook links to Colab) Source Code Documentation and Setup Guide: Look at Demo Video (it has explanation too) Documentation File: https://github.com/reckoner-david/CourseProject/blob/main/Source%20Code%20Documentation.pdf Source Code Documentation Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu from  simpletransformers.classification  import  ClassificationModel This is the library we are using for transfer learning. train_args={  'reprocess_input_data' :  True ,  'overwrite_output_dir' :  True ,  'num_train_epochs' :  8 ,  'fp16' :  False ,  'sliding_window' :  False ,  'learning_rate' :  3e-05 ,  'max_seq_length' :  256 ,  'do_lower_case' :  True ,  'train_batch_size' :  8 ,  'evaluate_during_training' :  False } model = ClassificationModel( 'distilbert' ,  'distilbert-base-uncased' , args=train_args, use_cuda= False ) Train_args: It specifies the parameters for the model. We define the learning rate, epoch and max_seq_length here. We also define the do_lower_case as  true  as we use the uncased model. For final training, we train on all the data so 'evaluate_during_training' is False. model = ClassificationModel('distilbert', '/content/drive/MyDrive/Colab Notebooks/model.zip (Unzipped Files)/checkpoint-5000-epoch-8', args=train_args, use_cuda=False) We can load a saved model via this API. Just put the path of the model in. The Classification Model API handles Model instantiation Saving per epoch Training loops Prediction Loops Converting/Preprocessing of data So we don't have to explicitly handle all these details. Text Classification Competition Team: Team Name: reckoner Members: Sahil Rishi -  sahilr2@illinois.edu Individual Team Member - Sahil was chosen unanimously as the captain of the team Which competition do you plan to join? Text Classification Competition https://github.com/CS410Fall2020/ClassificationCompetition Are you prepared to learn state-of-the-art neural network classifiers: Yes, our team is keen on learning and using transformer models for classification. They have shown to be very efficient for classification and transfer learning allows training on low numbers of samples. We plan to use the hugging face transformer library. Previous Experience: Previously we have used seq2seq transformer models, BART and Bert (Tensorflow). Which programming language do you plan to use? Python
https://github.com/retrouvailles0/CourseProject	CS410 Project Proposal 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rui Liu (ruiliu7) Captain Zhenzhou Yang (zy29) 2.What system have you chosen? Which subtopic(s) under the system? We choose Option 2.2 Expert Search System. Both. We plan to finish an application where users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs   on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. 3.Briefly describe the datasets, algorithms or techniques you plan to use. *The positive datasets are the faculty web pages given by students used for mp2.1 ( https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVxixU5vvMuUaM/edit#gid=0 ). The negative datasets would be non-directory web pages we collect online. Their ratio would be close to 1:1. *We will preprocess the html data and get their text. Using TF-IDF to filter the text and select features, vectorize the data into the input of our models. *We will train models using Python sklearn packages, such as linear regression, logistic regression, naive bayes, SVM, nearest neighbors and decision trees. Then we will choose the one yielding the highest accuracy as our final model. *If none of the models in sklearn perform well, we may build a neural network model including word embedding layer and linear layers. *If all the models failed to yield good accuracies, we may adopt several improving methods such as limiting the domains that users are intended to search. 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We add a function to enable users to enter a URL link and do the crawling automatically. The way we demonstrate that it works as expected is that users will get direct feedback from our system if the entered URL is a directory page or not. If yes, the system will continue to search all URL embedded in this directory website and classify if any of them is a faculty URL page. If yes, we will add this website to the final structured output. Then users will be able to check the results from the automatic crawler. Besides, there are things we would like to improve. While we researched the provided system, we found out there are bugs that have not been catched. For example, if I enter some arbitrary meaningfulness word, the system returns back some information that is not related at all. What's more, there are times that the faculty names are not displayed properly and listed empty. Therefore we would like to catch these bugs and fix them. 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. We plan to maintain a similar user interface as the given system. However, our system will focus more on automating the crawling process of MP2.1. Users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. In order to do the identification tasks, we plan to preprocess the text information on the input URL and do vectorization using TF-IDF, then train various models using sklearn packages and then select the best one as our model for the system. By doing this, our system will be able to classify any URL given by the users. To display the structured faculty information, we plan to utilize the format of the given system. Instead of using the interface directly, we will debug it first to take care of the blocks that should not be displayed when users enter meaningfulness words. What's more, we will fill in the information provided by our system to demonstrate our work. 6.Which programming language do you plan to use? *Machine Learning: Python scikit-learn package and pytorch *Web Crawling: Python selenium package *User Interface: Python in the given Github repo. *Supplement: Python tkinter, Javascript packages (haven't been decided) 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Get familiar with the current system: 4hrs *Read related docs. *Clone repo and run the code. Get the dataset: 4hrs *Collect 900 non-directory dataset to make the ratio of positive and negative in directory dataset 1:1. *Collect 900 non-faculty dataset to make the ratio of positive and negative in faculty URL dataset 1:1. Preprocess data: 10hrs *Implement TF-IDF with various parameters to vectorize the data as the format to fit into the models and select the most optimal parameters. Develop models: 30 - 35hrs *Train models using Python sklearn packages, including linear regression, logistic regression, naive bayes, SVM, nearest neighbors and decision trees. *Build an neural network model using pytorch (haven't been decided) *Fine tune the models and do cross validation and calculate accuracy for every model. *Compare accuracy of different models and take the one with highest accuracy as the final model in our system. Design and Implement User Interface: 5hrs *Since the user interface of the current system was designed for words search, we need to customize the interface to better serve our needs. Meetings: 10hrs *Weekly meetings to keep both of the members on the same page. Project Progress Report Recalling from our project proposal, we plan to finish an application where users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs   on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. Besides that, based on the feedback of the proposal, we decided to put on more effort on the faculty directory training model. By now, the progress we made includes one training model on faculty directory and negative dataset collection. We have downloaded the positive datasets from  https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVx ixU5vvMuUaM/edit#gid=0 and collected 300 negative data points by hand. We have finished preprocessing html data and use text on the website as the input for our model. We removed the header and the redundant spaces in the text. And use Tfidftransformer from sklearn to do the feature vectorization and build vocabulary for us. Currently we have only implemented the SVM model for the directory page classification task. The model yields a pretty optimistic accuracy data around 95% using 5-fold cross validation. Therefore we have no plan on adding a neural network or limiting the domains now. Difficulties: We found out that collecting negative datasets is time exhausting. Since we would like our negative data points to be as diverse as possible, we collected every one of the website URLs by hand. We collected 300 negative data points instead of 800 and the accuracy is pretty optimistic, therefore, we have no plan on adding more negative data for now. We are still in the process of implementing other models and choosing the most optimal one in the near future. We also have the second task which is identifying whether the page contains the profile links for the faculty and crawling the data. Currently we have not yet come up with a strategy to conquer it. Course Project Introduction This repo is for CS410 course project option 2.2 Expert Search System. \ Our system is part of the automatic crawler for MP2, using which users can enter a URL link and the system will identify if the entered URL is a valid faculty directory page. In order to finish the task, we retrieve the information on those websites and process data using text retrieval methods. Then, we train several several machine learning models using the processed text data from labeled websites(positive/negative). After training, we do test and cross validation and then select the optimal model to save for future prediction. When the user enters a URL, the saved model will be retrieved and a result will be predicted using that model. Author Zhenzhou Yang(zy29@illinois.edu) \ Rui Liu(ruiliu7@illinois.edu) Dataset Positive datasets are the faculty web pages given by students used for mp2.1 ( https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVx ixU5vvMuUaM/edit#gid=0 ). The data is saved in positive_link.csv Negative datasets are non-directory web pages we collected online. The data is saved in negative_info.csv Positive/Negative Ratio 3:1 Method Process the html data of labeled data set and get their text. Use TF-IDF to filter text, select features, vectorize the data into the input of our models. Training models (sklearn) SVM SVC LinearSVC Naive Bayes MultinomialNB Tree DecisionTreeClassifier RandomForest RandomForestClassifier Linear SGDClassifier Adaptive Boosting AdaBoostClassifier Save model (joblib) User interface (tkinker) Run The code was tested using Python 3.7. Train the model angular2html python3 select_model.py Predict a URL angular2html python3 predict.py Demo https://uofi.box.com/s/apzzkkdm40upm5rvkh3qpg8sob23o4a8
https://github.com/rfraser3/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/richameher/CourseProject	"Richa Meherwal CS 410 Project Progress Report Progress made thus far 1.Set up Environment. Installed Python 3, made requirements.txt Thle and installed compatible libraries 2.Data Pre-Processing in progress- removed stop words, removed latin words. Tokenized words and removed punctuations. Performed POS tagging and extracted adjectives and nouns Remaining Tasks 1.To execute LDA and extract topics 2.To perform sentiment analysis on each topic at a dataset level Challenges faced 1. Library incompatibilities with Python version 2. UTF-8 encoding err - added diff type of encoding to read_csv in pandas 3. Spacy library incompatibility 4. No module named en-core-web-sm 5. Extracting meaningful words 6. Mapping lemmatisation functions to every row in data frames Richa Meherwal- CS 410 Course Project Documentation Presentation Available at https://mediaspace.illinois.edu/media/t/1_ammbs24f An overview of the function of the code Code can be used to do an aspect based sentiment analysis. As seen in the code, we first tokenise all reviews. Then extract bigrams NN-ADJ pairs to form a word cloud and visualise the features that stand out the most. We also extract unigrams that are NN as the aspects to use for sentiment analysis. Note that in order to train our classifier , we use the Ratings column from the Hotel dataset and then label our aspects with a pos,neu,neg sentiment. We then visualise the aspects and the associated sentiments using a bar plot. We the repeat this process over Airbnb reviews dataset and we use the sentiment classifier trained before to classify the sentiments of the aspects extracted. Note that Airbnb does not have ratings so it wouldnOt be possible for us to retrain the classifier. Software Implementation and Usage 1.(Optional) Create a Python3 virtual environment python3 -m venv py3-env-final-proj 2.(Optional) Activate virtual environment- source py3-env-final-proj/bin/activate 3.pip install jupyter 4.Install ipykernel in this environment- python3 -m ipykernel install --user Nname=final-proj (final-proj will be used as env in jupter notebook) 5.pip install -r requirements.txt 6.Start Jupyter-notebook from shell using command : jupyter notebook 7.Download repository and open the Test.ipynb file 8.Switch to final-proj kernel defined in step 4. 9.Change the file path to where the preprocessed files are i.e. under folder data and Run all the cells in the notebook Note for testers There is a joblib file that you can use to test the sentiment classifier. The classifier has been trained on HotelReviews dataset. Check Step 5.c under Hotel Review Analysis in Final_Project V4.ipynb or 4.b in Test.ipynb. I have already shown how to use it in the video presentation. Entire Code including the preprocessing and training sentiment classifier can be found in - Final_Project V4.ipynb. You can also view this ipynb file using nbviewer - https://nbviewer.jupyter.org/github/richameher/CourseProject/blob/main/code/Final_proj%20V4.html Modified Code for Testers can be found in- Test.ipynb Final Results Understanding Plots and Graphs [Hotel Review WordCloud] Use WordCloud to visualise bi-grams. NN-ADJ pairs are extracted from reviews and TF-IDF is used to retrieve top n bigrams. There will also be a bar-plot associating the sentiment with every unigram NN keyword extracted with the probability of the sentiment. [Hotel Review Aspect Sentiment Graph] In the Wordcloud we can observe that people tend to talk about the quality of rooms. Features like safety is usually associated with the hotels than Airbnbs. Also Hotels have their own website , so people also talk about the online booking system. As for the bar plot, we can see that hotels have OtheftO, OsuiteO aspects that have been associated with negative sentiment. Also the highest positive sentiment is observed among aspects like OroomO, OviewO and OmanagerO What is completed and what could be better? I have successfully been able to analyze the aspects that drives people to chose Airbnb over Hotels and vice-versa. With Airbnb, like we can see in the graphs, the motivation is driven by finding an OaffordableO yet comfortable stay. People care about OlocationO and Oworth for moneyO. With hotels, people expect OluxuryO in terms of Obig roomsO, Ospacious bathroomsO, OviewsO and so on. However, the sentiment classifier could have been better if we had an available labeled dataset for airbnb reviews. I also realised that using LDA for bi-grams does not work well but specialised algorithms for Bi-gram topic extraction can be used in future. LDA for unigrams also did not group the categories very well , but top weighted words could have been considered. Therefore, I used TF-IDF to find the key aspects and only used nouns to do so. Richa Meherwal Project Proposal CS 410 Fall 2020 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administra<ve du<es than team members. Individual Project Name- Richa Meherwal NeAd- meherwa2 Captain-Richa Meherwal 2.What is your free topic? Please give a detailed descrip<on. What is the task? Why is it important or interes<ng? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic Using Topic Mining and SenAment Analysis to compare customer level saAsfacAon in Airbnb vs Hotels Problem Statement With the rise of Airbnb, travellers are usually choosing this new type of accommodaAon over Hotels. Similarly some travellers always chose to stay in expensive Hotels. What is it that aNracts tourists to hotels over Airbnb and vice-versa. I chose this topic because I was interested in learning what level of customer saAsfacAon present in Airbnb users vs Hotels. I parAcularly wanted to compare the customer level saAsfacAon features involved with these two types of accommodaAon using topic extracAon and senAment analysis. Task To compare the customer saAsfacAon between Airbnb and Hotels using LDA and SenAment Analysis Datasets hNps://www.kaggle.com/mrinaal007/reviews hNps://zenodo.org/record/1219899#.X5Uic0Izba4 Tools jupyter notebook Nltk toolkit genism Approach 1. Complete the pre-processing of the datasets. This includes tokenisaAon, removing stop words, normalisaAon. 2. Use LDA to first extract the common topics that the customers review about in both the datasets. 3. Do a senAment analysis on each of the sentences containing the extracted topic and assign a senAment to it. Gather the associated senAment and the topic over each dataset. 4. Now under each accommodaAon type we should be able to visualise the topics that it is posiAvely credited for by the reviewers and also negaAvely. My expected outcome is to show topic and senAment level comparisons for each type of accommodaAon (Airbnb or Hotel). The idea is to see which topics are associated posiAvely or negaAvely with each of the accommodaAon types and to gain insight into where each of these services perform beNer than the other. EvaluaAon To check the senAment analyser, I would compare the senAment associated with the sentence/review to the raAng given. To check topic extracAon worked well, I will use wor2vec on all the common topics and see if they separate well. 3.Which programming language do you plan to use? Python 4.Please jus<fy that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the es<mated <me cost for each task. Tasks and hours 1. PreProcessing data - 5-6 hrs 2. Research methods for SenAment Analysis and Topic ExtracAon and deploy it - 9-12 hrs 3. Post Processing and visualising the final data - 6-7hrs 4. Cleaning code ,demo, documentaAon - 4-5 hrs N=1 Total esAmated work hours- 24- 30 hrs CourseProject Code can be used for Aspect-based Sentiment Analysis Extract NN-ADJ bigrams Extract NN unigrams Extract top n-grams using TF-IDF Sentiment Anaysis on Reviews on Aspects (unigrams or bigrams) Visualize sentiments on keywords extracted using barplots/Wordcloud Presentation and Tutorial Available at - https://mediaspace.illinois.edu/media/t/1_ammbs24f?st=0 Complete documentation and self-evaluation available in Project Documentation.pdf Code folder contains Final Project V4.ipynb (whole source code) Test.ipynb (Code for testers to run) requirements.txt (libraries to install) sentiment_analyzer.joblib (trained sentiment classifier model) Data Folder contains preprocess_airbnb.csv (Airbnb data with necesaary n-grams extracted with source code to be used for topic extraction and sentiment analysis) preprocess_hotel.csv (Hotel data with necesaary n-grams to be used for topic extraction and sentiment analysis) You can view my ipynb notebooks directly using nbviewer https://nbviewer.jupyter.org/github/richameher/CourseProject/blob/main/code/Final_proj%20V4.html Software Implementation and Usage 1. (Optional) Create a Python3 virtual environment python3 -m venv py3-env-final-proj 2. (Optional) Activate virtual environment source py3-env-final-proj/bin/activate 3. pip install jupyter 4. Install ipykernel in this environment- python3 -m ipykernel install --user --name=final-proj (final-proj will be used as env in jupter notebook) 5. pip install -r requirements.txt 6. Start Jupyter-notebook from shell using command : jupyter notebook 7. Download this github repository and open the Test.ipynb file 8. Switch to final-proj kernel defined in step 4. 9. Change the file path to where the preprocessed files are i.e. under folder data and Run all the cells in the notebook Methodology (Code in code/Final Project V4.ipynb) Read Original Datasets (Check Proposal.pdf Data for the links) Clean Text- Tokenize, Remove punctuations, tabs, whitespaces, stopwords, common words Extract bigrams- Create a bigrams column to extract all afjacent pairs of bigrams from review/text Create a bigram_list column and keep bigrams that are NN-ADJ pairs Create a unigram_list column and keep only unigrams that are NN Train a Logistic Regression classifier on sentiment and reviews of the hotel dataset as only hotel dataset has ratings (map ratings to sentiments first) Use WordCloud to visualize frequent bigrams Use TF-IDF to extract top n keywords from unigrams or bigrams Use the trained sentiment classifier to classify the sentiment for the keywords Plot a bar graph, with sentiment as labels, keywords and sentiment probability/topic extent on Y-axis Results [Hotel Review Bigram WordCloud] [Hotel Review Sentiment-Topic Extent Bar Plot] In the Wordcloud we can observe that people tend to talk about the quality of rooms. Features like safety is usually associated with the hotels than Airbnbs. Also Hotels have their own website , so people also talk about the online booking system. As for the bar plot, we can see that hotels have ""theft"", ""suite"" aspects that have been associated with negative sentiment. Also the highest positive sentiment is observed among aspects like ""room"", ""view"" and ""manager"" (Note- we can also observe the sentiment probability on y-axis instead of topic extent, Check Test.ipynb) Contribution Completed By Richa Meherwal Free Topic- Using Topic Mining and Sentiment Analysis to compare customer level satisfaction in Airbnb vs Hotels"
https://github.com/rixu1/CourseProject	"Project Proposal Project Name:  2.2 ExpertSearch System Team member: Ri Xu  (Captain), NetID: rixu2 Jinou Yang , NetID: jinouy2 The system we are choosing is Option2: Improving A System, specifically the ExpertSearch system. The main dataset we will be using is the faculty bio from  MP2.3 . Our project will make improvements to the ExpertSerach system in the following ways: 1.Add extractions on experts' phone numbers using regex pattern matching. ( 5 hours ) 2.Add extractions on experts' research interests in the format of keywords using topic mining. ( 10 hours ) 3.Improve search results preview by adding phone numbers and keywords. ( 5 hours ) 4.Build experts recommendation system based on users' search queries and browsing history. Users' interaction data can be stored and retrieved from browser cookies, BM25 can be used to find the top relevant experts to recommend. ( 20 hours ) We will implement web scraping scripts to extract phone numbers in python and store them in a local file, which will be shown in the preview on frontend. Another python script is to perform topic mining on faculty bio and extract top research interest keywords and store them in a local file. Search related function and ranker will also be implemented in python. For web frontend improvements, HTML, CSS and Javascript will be used to achieve the objectives. Progress Report (As of Nov 29th) Current Progress: Backend: We have finished writing up scripts to extract phone numbers and research interests all experts and dumped the result into files. Later we ran the script to recreate the new data file (metadata.dat) to include phone numbers and research interests for the frontend to display. Frontend: Refactored code for existing project to modularize some of the components for reusing. Integrate the phone number and research interests in expert preview cards. Remaining Tasks 1.Build recommendation system based on users' search history. Need to look into how to use cookies to store search info and recommend related experts using BM25. 2.Look into improving name recognition to reduce empty names in search results. 3.Look into improving email recognition to reduce empty email information. Challenges The biggest challenge with this project is getting familiar with the existing code base and iterating based on that. Also when extracting research interests for experts, some unrelated information like school names, locations may interfere with the results. Software Usage Tutorial Presentation Video link: https://www.youtube.com/watch?v=qGx1IDdoyLw Documentation An overview of the function of the code The existing ExpertSearch system is a web application where users can search related experts. We've made several improvements to the existing system. To be more specific, the improvements include: - Show experts' areas of interests in search results preview. - Show experts' phone number in search results preview. - Improve name matching; increase name recognition ratio from 88% to 96%. This reduces the chance when users see empty expert names in their search results. - Build a simple recommendation system based on users' past search queries. Replace the empty home page with recommendation feed. How the software is implemented The web application is implemented in the following way: server.py This is the main Flask server file which contains all backend APIs and page handlers. - /search This API accepts a few parameters such as search query, number of results expected. Metapy library and BM25 is used to query the dataset, then fetch additional information from metadata.dat for frontend to display preview. - /recommend Similar to /search, this API accepts search query keywords and returns up to 5 recommended experts per query. BM25 is also used for finding experts to recommend. If more than 5 experts matched with a given query, 5 experts are randomly selected to encourage exploration. index.js This is the main javascript file which contains most of the frontend logic. - recommend() This function is called during window.onload. It reads users' past search queries from browser cookies and talks with the backend (/recommend endpoint) to fetch recommended experts for each query term. Then it will display the experts' preview for each search query. The latest search query will show up on the top of the feed. - doSearch() This function is called when users have clicked the search button. If the query is non-empty, it talks with the backend(/search endpoint) to fetch the results. It will also store users' search query in the browser cookies for the recommendation system to pick up. - docDiv() This function will return a html div object given all preview data. It will render the expert's name, phone number, email, areas of interest, university information etc. It is used by both recommend() and doSearch() when rendering queried experts. Material icons ( https://material.io/icons/) are used for visibility. - setCookie() and getCookie() These two helper functions are used for storing and retrieving cookies stored in the browser. In this case, users' search keywords are stored in the ""history"" field in the format of comma separated strings. extraction/extract_interest.py This script is used to generate experts' area of interests. Nltk, Gensim, SnowballStemmer are used for extraction. For each document, tokenization is performed followed by stemming and lemmatization. Then it uses nltk to run POS tagging on each token and extract all the noun words. Finally we run through all noun words with a predefined word mapping to compute the final areas of interests for all documents. Results are then exported to data/interests. extraction/extract_phone_number.py This script is used to extract experts' phone numbers. Regex pattern matching is used for phone-number extraction. Results are then exported to data/phone_numbers. extraction/extract_names.py & extract_names_spacy.py & merge_name.py These three scripts are used to improve experts' names extraction. - We improved the original script extract_names.py to use the latest version of stanford-ner. Results are exported to /data/names.txt - In extract_names_spacy.py, we used the spacy NLP framework to run through all documents and extracted named entities. Results are exported to /data/names_secondary.txt - Finally we use merge_name.py to combine names from /data/names.txt and /data/names_secondary.txt . If we cannot find a name using stanford-ner, we will check and use the result from the secondary file. With both methods combined, we're able to improve the name recognition coverage from 88% to 96%. Final results then exported to a new file new_names.txt. extraction/write_file_names.py This script combines all data files (interests/phone/email/names/...) generated by extraction scripts and writes to the dataset file metadata.dat for metapy to index and rank. How to install and run the software To run the software, simply clone the repository from Github. There are a few dependency packages required to install. Following are the commands to run: pip install metapy pip install gunicorn pip install spacy pip install nltk cd to /CourserProject gunicorn server:app -b 127.0.0.1:8095 Then you should be able to access http://localhost:8095/ from your browser. Chrome browser is recommended to use. Description of contribution of each team member Team member: Ri Xu Responsible changes related to Flask servers and frontend javascript. Complete the Project Progress report. Team member: Jinou Yang Responsible for extraction scripts development and improvements. Demo video. Documentation."
https://github.com/riyv/CourseProject	Offensive Language Detection - Project progress report For our project we have decided to divide our entire project work into four basic modules, - View - Extractor - Analyzer - Repository View For view module we have decided on using react / dash combination, we may switch to static html webpage based on our needs, view will be used for two main reason, one from ui we will input what hashtags we want to search, that will be fed into our extractor system. Also, our ui will have separate dashboard, where will have visual elements of different datasets, like tweet counts, their overall sentiment, etc. Extractor This module will be used to get the tweets for a list of hashtags, we have decided to use Java for this module, we have signed up for Twitter developer account, we are using twitter official hbc api for getting our tweets, twitter/hbc: A Java HTTP client for consuming Twitter's realtime Streaming API (github.com) We have been able to complete the coding of this module, and we were able to get tweets from api successfully for particular hashtags, please find code snippets below, PAGE 1 And we are getting outputs like this, Now we are working on cleaning the tweets, so that we can use them directly to our analyzer module without any deformed text Analyzer We have decided on using Python for this module, there will be python script running in background, where we will feed our cleaned tweets, and we will scan for offensive words in that text and mark that tweet accordingly, also we are planning for doing sentiment analysis of the tweets, we are still deciding on that topic. Repository We plan on storing all our analyzed tweets on mongoDB on cloud, so that we use data from our UI component. Sentiment and Speech Analysis By: Riya Gupta, Chitra Uppalapati, and Diptam Sarkar What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - riyag3 (captain), chitrau2, diptams2 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic is conducting sentiment analysis as well as polite and impolite language detection on a set of around 1,000 of the top tweets from the daily trending hashtags, dynamically. We are trying to categorize the most popular tweets as positive, negative, neutral, polite, or impolite depending on the topic. This is an interesting project because it allows us to sense what the overall attitude and feeling is towards certain ideas when examining the most relevant tweets per hashtag. This can allow us to find certain patterns and sentiments in the trending hashtags, which can help us identify how people feel about popular discussions and products as well as how polarized specific topics on Twitter are. This project will be most helpful for identifying sentiment towards political discussions as well as new products. We plan to use Twitter HBC, the Java HTTP client for accessing the Twitter API, to fetch our 1,000 tweets. Then, we will build a system that classifies the tweets. When analyzing our data, we will experiment with different classifiers and evaluate our system using the standard classification evaluations metrics (Precision, Recall, and F-score). The expected outcome is to display our results on a web app that we will create using React. Which programming language do you plan to use? - Python, Java, Javascript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: - Scrape 1,000 tweets from daily trending hashtags (20 hours) - Input hashtags from our web app - Clean our data - Query by top number of Retweets - Store in repo (MongoDB database) - Perform sentiment analysis/language detection (positive, negative, neutral, polite, impolite) (20 hours) - Feed tweets into modules for language analysis - Update tweets with language analysis information - Output results on a web app (20 hours) - Show results in graphs - Create a good UI - Add information on how to use the tool/its purpose CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rnowak6/CourseProject	"Team Captain : Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 Final Project Introduction The project we decided to focus on creating a tool to help students navigate through multiple slide decks and be able to easily find similar slides to topics that students are attempting to search for. Our team focused on expanding the EducationalWeb System by adding functionalities to improve navigation between slides and uploading/downloading. Our datasets included the entirety of the CS 410 slides from coursera along with other courses. We decided to use similar techniques to the current Web of Slides and connect new slides to the web framework by using a combination of TF-IDF similarity and word embedding based similarity. Features Our project allows for downloading multiple slides at a time along with the option where you can add any slides you like, and then download them at the end of your session.  For the main layout of the application, we created more than one page for this application so that it can easily be expanded on in the future. With this goal in mind, we decided to create a homepage where you can see all of the current classes listed that have slides, and then once selecting, it will bring you to the slide page that is currently in production now. Additionally, we have made improvements on the current user interface of the slides page. We have changed the way the image for the slide is generated, and convert it to a jpeg to help with seamless loading and to give the page a more holistic feel. Challenges Our team faced multiple challenges with this project. It was challenging to set up the Education web system on our own devices however, luckily, we were all able to have it running on our computers. Another challenge we faced was being able to upload our source code to github since the files were so large and github struggled to upload them all however this issue was fixed by omitting the large files. Lastly, our biggest challenge was being able to have our features fully functioning. We struggled to create functioning features due to having to also learn the given code and be able to make it work with our vision. Team Captain:  Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 Progress Report Team Bogus Progress made thus far: 1.All team members were able to get the code downloaded and working on their own computer 2.5 different UIUC course slides were downloaded to be added to the website 3.Updating the user interface for the Educational Tool a.We have decided to create more than one page for this application so that it can easily be expanded on in the future. With this goal in mind, we decided to create a homepage where you can see all of the current classes listed that have slides, and then once selecting, it will bring you to the slide page that is currently in production now. b.We have made improvements on the current user interface of the slides page. We have changed the way the image for the slide is generated, and convert it to a jpeg to help with seamless loading and to give the page a more holistic feel Here is the current new front page. Remaining tasks: 1.Make final adjustments to front end User Interface a.We will create a ""Upload your own"" slides button on our new front page to allow students to upload their own slides if our algorithms permit for analysis on new slides b.Add functionality and views for notebook feature outlined below 2.Create a notebook feature on the side of the screen where a user can save specific slides to create their own study tool 3.Creating a toolbar where we can highlight and take notes on our notebook feature Any challenges/issues being faced: *Ran into numerous errors when trying to get the code working. Took much longer than expected. *Have not been able to get the code uploaded to our own team's github repository. Numerous errors in uploading. Team Captain : Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 1.What system have you chosen? Which subtopic(s) under the system? Our group has chosen to expand the EducationalWeb System by adding functionalities to improve navigation between slides and uploading/downloading. 2.Briefly describe the datasets, algorithms or techniques you plan to use Our datasets will include the entirety of the CS 410 slides from coursera. If time permits we may expand the dataset by adding slides from other UIUC courses such as HORT 106, but this is not the main priority of our project. We will use similar techniques to the current Web of Slides. To connect new slides to the web framework we will use a combination of TF-IDF similarity and word embedding based similarity. We will also introduce some kind of data structure (probably a doubly linked list) to keep track of the student's path throughout the web so they can return to a previous slide. 3.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We would like to add a popup or sidebar that displays the most relevant similar documents as you are viewing one set of slides. The relevant documents would be calculated using one of the ranking algorithms. We would like to add functionality to go ""back"" to your original slide after clicking related slides on the right hand side. This will be a good feature because that way you can go back to your original lecture after addressing anything you were confused about. We will demonstrate that this works by recording a short demo where we navigate away from the start slide and then hit the back button to return to it. We would also like to add downloading multiple slides at a time. We will be able to add a ""notebook"" option where you can add any slides you like, and then download them at the end of your session. We will demonstrate that this functionality works by testing it locally by downloading a group of files. Students will be able to upload their own pdf slides to perform analysis for them (if runtime permits, we will know more if this is possible after beginning). This could also be checked with local testing by uploading a pdf file that isn't included in the web yet. 4.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Our code will extract all of the Coursera slides and determine which slides are similar to the one we are currently viewing. It will then display the related slides on the side in a pop up or sidebar so you can easily access them. This popup/sidebar will then have an option to be closed or to help you navigate back to the original slides you were viewing. 5.Which programming language do you plan to use? Python 6.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: *Project Topic Brainstorming Session - 1 hour *Project Proposal and Team Formation Submission - 3 hours *Project Setup - 1 hour *Add algorithm to help find most related documents - 1 hour *Add pop up or sidebar that displays the results of the algorithm run - 5 hours *Add functionality to go ""back"" to your original slide after clicking related slides on the right hand side - 20 hours *Add functionality to download multiple slides at a time - 20 hours *Team Check-In Meetings (over Zoom) - 5 hours *Progress Report - 3 hours *Add functionality to allow students to upload their own slides - 20 hours *Documentation - 15 hours *Improve front end functionality- 8 hours Total Hours: 102 2.3 EducationalWeb System The EducationalWeb system ( http://timan102.cs.illinois.edu/explanation//slide/cs-410/0 ) is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and recommend relevant slides for each slide. You can read more about this in the following papers  Web of Slides ,  WOS Demo .; 2) Find an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""cap/scholar"" button on the top-right of a slide. It will try to retrieve a relevant section from the Professor's textbook that contains an explanation of the selected phrase. You can read more about the underlying algorithm  here . The code for the system is available  here . Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms. *Improving the usability and reach of the existing system Some of you might have used the system and identified potential areas of improvement. The aim of this subtopic is to refine the current version of EducationWeb. Some specific ideas include (many are borrowed from  this Piazza post ): 1.  Scale up the current system . Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. Again, it might be useful to think about automatic crawling similar to the subtopic in 2.2 above. It would be very interesting to see the interaction between slides/textbooks at a large scale!! 2.  Improve the performance of the system . Currently, loading each slide takes time. 3.  Allow downloading slides in bulk.  Currently, we can only download one slide at a time. 4.  Add more context to the explanations  (e.g. link to the specific page in the textbook) 5.  Allow adding additional courses/lectures directly from the web interface . This would also involve dynamically identifying the recommended/relevant slides for a new slide. Currently, a static file is used which contains pre-computed recommendations for each slide. 6.  Integrate the tool with Piazza/Coursera , i.e. maybe link Piazza/Coursera to the tool or vice-versa. Alternatively, add discussion forum and video capabilities to the tool so that it serves as a one-stop-shop for all users' educational needs. 7.  Link to latest related research articles : In this way, the lecture content can be automatically updated 8. You could also work on  improving  the current recommendation, search and explanation mining  algorithms  (described in the papers at the beginning of this section 2.3) *Automatically creating teaching material for in-demand skills This subtopic is an extended version of the existing EducationWeb system. There is an increasing demand for skilled workers in the industry. Quality education is not easily accessible to everyone due to barriers such as high cost, geographical and language barriers, etc. Also, instructors cannot be available 24*7 to provide personalized support to all learners. In this subtopic, the overarching aim is to tackle some of these issues. In particular, the following tasks might be good starting points. *Identifying in-demand skills:  You can crawl and analyze relevant sections of job boards, news articles, scientific articles, social media, etc. to automatically identify the  emerging keywords /topics. For this, you may refer to some papers on contextual text mining (mentioned in Option 1 of this document). *Creating lectures and tutorials for those skills:  For this, you may consider lecture slides (e.g. from Coursera courses) as the basic units of knowledge. Then, the task could be to find the most relevant slides or clusters of slides (could be across multiple courses/lectures) for a given skill (topic). You may borrow some ideas from the EducationWeb system for this. You may also use the slides in existing lectures on some topics as the ""relevant slides"" for those topics. In this way, you can automatically generate training data for supervised learning. You could also combine knowledge from multiple sources (e.g. textbook sections, slides, videos, blogs, codebases) for creating more comprehensive tutorials. A more challenging task would be to automatically  generate  the lectures/tutorials using techniques from natural language generation and abstractive text summarization. Another interesting idea is to automatically  generate agents , e.g. using Virtual Agent Interaction Framework (VAIF). This goes beyond the material covered in class but could lead to some highly innovative and state-of-the-art projects! If you choose this option, please answer the following questions in your proposal: 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2.What system have you chosen? Which subtopic(s) under the system? 3.Briefly describe the datasets, algorithms or techniques you plan to use 4.If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5.How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 6.Which programming language do you plan to use? 7.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: *Your documented source code. *A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/rohang62/CourseProject	"Rohan Goel, Paru Swaminathan, Satej Shah 1.Progress made thus far Thus far, we have set up our working environments and installed the necessary software. We have understood the task at hand and created a plan to move forward. We have cleaned the training data by removing stop words and brackets and by converting the data into word2vec vectors. 2.Remaining tasks We will now be building a model to learn to classify whether the response with context is sarcastic or not. We are planning on trying out Naive Bayes, Linear Regression, RNNs, a simple Neural Network, and a simple Random Forest Classifier. We plan to test each of these models in the week of November 30th, and are hoping to move forward with one of them, and begin hyperparameter tuning by December 7th. 3.Any challenges/issues being faced Currently, we are facing issues with how to utilize both the response and context. There are many tutorials online that help with understanding how to use one column, however, working with multiple features proves to be more challenging. Rohan Goel, Satej Shah, Paru Swaminathan CS 410: Report An overview of the function of the code (i.e., what it does and what it can be used for). At a high level, the function of the code is to predict whether a piece of text (tweets and responses) is sarcastic or not sarcastic. Our program classifies the text (context) and associates a certain label (SARCASM or NOT_SARCASM) using an id. The input of our code is a json file named test.jsonl which contains responses with an associated id and context, and another train.jsonl file with responses, id, context, and a label. The output of our code is a comma separated file named answer.txt which contains the predictions on the test dataset. The file has exactly 1800 rows and each row has an id and the predicted label. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. The first step of our development process was to create a plan. Researching the different types of classification models, we were able to distinguish a couple of models that seemed promising to complete the task of classifying sarcasm of tweets. After testing some of them, we decided to proceed with the BERT language model and NN algorithm. Our first test was with FastText, however, the performance of this algorithm reached an upper limit, and therefore, we couldn't move forward with it. We also tried using K means with two centers, however we were unable to reach baseline with this. Therefore, we opted to go with using the BERT language model. Using the pre-trained BERT model provided the feature vectors which we then used to train the Neural Network. We first used a Logistic Regression model, however, due to the simplicity of the model, it did not perform well enough. We then decided to use a Neural Network with 3 layers. Through tweaking parameters, we were able to finally get our average accuracy above the baseline. In the code, we first read in the training and test data from the given folder. We then parsed the data. After this, we assigned binary values: 1 or 0 to the labels in the training data. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. To install and run this software, you can view the code through Github ( https://github.com/rohang62/ClassificationCompetition ). After cloning the repository onto your device locally, you can install Jupyter Notebook and open it through your browser. From here you can run the code blocks using the play button on the top of the screen. In the implementation of our code, we use many frameworks and libraries that may need to be installed locally such as PyTorch, numpy, pandas, transformers, and sklearn. After running all of the blocks of code, this will create a file ""answer.txt"" in which you can view the results of the program. Within this file, you will see a line for each situation and its resulting classification. Brief description of contribution of each team member in case of a multi-person team. Each member of the team learned and contributed to the project very well! Everyone was very enthusiastic, encouraging, and open to ideas. Working remotely, communication was very imperative to doing well. Rohan took lead on the development side, and Paru and Satej focused on the research and documentation. Although, all three members of the team helped each other out whenever needed. This team worked well together and was able to support each other throughout the course of the project. Project Proposal: Team PRS 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rohan Goel:  rohang4@illinois.edu  (Team Captain) Satej Shah:  sshah273@illinois.edu Parvathi Swaminathan:  ps13@illinois.edu 2.Which competition do you plan to join? We plan to join the Text Classification competition. 3.If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Yes, we are prepared and excited to learn! Neural classifiers that we have heard of are BERT and LSTM. Some deep learning frameworks that we have heard of are PyTorch, TensorFlow, and Keras. Rohan has worked with TensorFlow and Pytorch in the past. Satej has not worked with such models. Parvathi has minimal prior experience with such models in an applied machine learning course. 4.Which programming language do you plan to use? We will be using Python in order to complete this project. CourseProject Final Project Video: https://www.youtube.com/watch?v=6Em313aRmys"
https://github.com/rohankk2/Twitter-Recommendations-based-on-text	"Twitter Recommendations based on text Create suggestions on potential people/pages/ads a user would like to follow based on their tweet/post content. 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rohan Khanna - rohank2 (Team Captain) Cesia Bulnes - cbulnes2 Tyler Wong - tylercw3 2.What is your free topic? Please give a detailed description. Our free topic is a recommendation algorithm that based on the text contents of tweets makes appropriate recommendations to the user. The recommendations can range from other tweets similar to a user's tweet, advertisements or even possible pages to follow. Consider a user who frequently tweets about a soccer club then our algorithm should recommend other followers of that club or advertisements specific to that club's products. Another example is if I frequently post about the new gaming console I should get recommendations to follow sellers such as gamestop,amazon etc on twitter as they are likely to be talking about console sale dates. These recommendation are all based on the text of a tweet. You may ask, doesn't Twitter have that already? Twitter's recommendation system currently is one page, where they recommend multiple sets of people who may be of different categories. Currently: What we want: As you can see, the only relevant recommendation is the person who wrote it, or in many cases someone who was mentioned in the tweet. There is no recommendation of other people writing similar things. In the example below, you can see that Michelle obama has a similar tweet, therefore she would be one of the top results returned since the ML/NLP algorithm recommends this content with a high % of similarity. We can then show the top tweets which can generate follows to different types of celebrities/businesses/ads etc. Recommendation of Tweets: 3.What is the task? The task is to deliver working software code, software documentation, and a software usage presentation about our topic described above. The code will fulfil our primary use case so that as someone makes a tweet, a recommendation will show up based on that singular tweet. In addition, we want to be able to give a percentage of how much a tweet is related to another page, tweet, etc. 4.Why is it important or interesting? This project is important because we want to portray similarities between a person and other people around the world. It's a way to unite people based on one singular tweet, and for people to be aware that they are not alone in terms of a specific subject. We are focusing on social aspects this year because of controversial topics, such as politics(Trump/Biden), the Black lives matter and All lives matter movements, LGBTQ, children in ICE Detention centers and many more. We hope to examine a tweet that contains keywords, and recommend other personalities/pages to follow when that tweet is put in to grow a person's network. 5.What is your planned approach? We plan on taking an iterative approach throughout this software project so that we can quickly identify blockers and make consistent progress. With that being said, we will meet weekly to discuss our current progress and any blockers that we're experiencing. We will also be splitting up the work so that we can all work in parallel. In addition, we are consistently talking through in a group chat where we can quickly get feedback on an idea or a feature. We do have a defined due date, so we will use that date as a target to deliver a minimal viable product which will deliver the major functionality. We will add on more work to that minimal viable product if we underestimated our time or if we have more capacity than expected. 6.What tools, systems or datasets are involved? The first system that comes to mind is using Twitter Developer, since we could filter real time tweets, and cross examine them to make recommendations based on people tweeting similar content. I think most of the work needed to recommend would be coming from Twitter Developer's API's. We would also be using Pycharm to code in python to show our results through the terminal. We would use the nltk tool kits to remove stop words and appropriately tag the data. To store our test data and queries we will provision a non relational database which are available from major companies such as Microsoft,Mongo,Amazon etc. 7.What is the expected outcome? The expected outcome is for someone to make a tweet, and for them to have a recommendation immediately based on that singular tweet. Currently, Twitter has a recommendation page, but not recommendations based on tweets. People get passionate about topics and I feel like this feature would increase a person's usage of Twitter. 8.How are you going to evaluate your work? As discussed in class, evaluation of a text recognition system depends on its usefulness to the end users. We plan to create a google form where users can rate how useful the results were for a particular tweet. We would look to answer the following questions through the google form all on a scale of 0-10 : 1.How accurate are the results provided by our tool ? 2.Do the results relate to the category of your tweet ? 3.How likely are you to use this tool in a production environment ? 4.How satisfied are you with the speed of the system ? 5.Does the person/page recommended reflect your interests based on a tweet? 6.Would you use this feature if Twitter enabled it? 7.Do you think it is invasive or inappropriate? The next important criteria is the speed. Google delivers its search results in approximately 0.67 seconds. We aim at a minimum to have our results show within 2 seconds. Optimistically we will target results being shown within one second. 9.Which programming language do you plan to use? Python, JS, React 10.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Hours Set up Twitter API and get familiarized 15 (5 hours per teammate) Create Github repo and create base project components 2 hours Setting up Nonrelational DB 2 hours Create software usage tutorial presentation 3 hours Develop software documentation 12 (4 hours per teammate) Develop nlp code that can process tweets and categorize the contents 8 hours Based on the categories of the contents of the tweet scrape data from the twitter apis relevant to that content. 10 hours Program a ranking algorithm that gives : 1. Top users to follow, 2. Top posts the user may like 3. Top retail sites based on their tweet. (Ads) 24 hours Create google form for feedback evaluation 1 hour (Optional) create a web app to present all 10 hours results. Include setting up api and frontend Total 77 excluding optional Dev Design Doc for MicroForce - Twitter Tweet Recommendations based on Topics Tyler Wong - - tylercw3@illinois.edu Rohan Khanna - rohank2@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu Introduction on Topic: We are currently looking at topic modeling tweets and recommending the tweets or users that are most similar in a given tweet. We are looking at using a ranking algorithm such as Okapi BM25 learned in class for ranking tweets by ranking the most similar tweets given a tweet by a user, the category of the tweets that it is related to, the top ranked users in said category, and the top categories/topics in a given sample set. For example, had we borrowed tweets from November 6th,2020, the top topic would have been politics because of the USA elections. Versus November 26 probably having TheGrammys nominations as one of the most trending topics. It will be interesting to recommend other similar tweets/users to a category presented by a single query. We will be ranking and observing the results with a sample set of 2000 users for the time in this project, and observing the past 7 days of tweets by each said user. Currently: By the end of the week, we hope to finish the database setup. We decided to use SQLite to avoid any errors on setting up our computers. Simplifying the database will allow us to work on the algorithms needed in the last two weeks of the class. In addition, we're also working on gathering all the necessary data we need from the Twitter API and importing that data into the database. We have all created twitter developer API and have started planning what we each need to do come the last two weeks of class. We plan on using multiple different entities that the Twitter API provides, such as user data and Tweet annotations and entities. Concerns: As of now the concern is having a good ranking model. We want to make sure that the ranking model is correct in terms of how close tweets are to each other. It's imperative that our team take some time to validate returned ranks to make sure there are no discrepancies. For example a query that is exactly like a tweet in our database should have such a tweet ranked as 1 versus one that is highly similar but not exactly the same. Work to do: 1.Ranking algorithm to rank the query or tweet that the user has typed, displaying the ranked tweets in commonality. a.Going off of the query, or tweet, we want to be able to display common tweets from the pool of users and their respective tweets in the past 7 days. Say my query is ""I love seafood"". The ranking will be conducted among our pool of tweets and we would show the top tweets that may have similarity in the topic/category. 2.Get 2000 users, and their last 7 days of tweets a.For the purpose of the project we want to go ahead and observe ranking in a small-ish pool of people and tweets. I think it would be informative to observe the ranking when using a pool of 1000 users versus a pool of 1000 users for example. The more tweets are available, the more we should see a higher commonality between the tweets given back from our ranking. 3.Process data a.We should process the data of tweets that we receive to eliminate words or other elements in the tweet that are not helpful in terms of ranking the tweet for our query. b.Source or guidance for processing with python: https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e 4.We should get information from 2000 users and their past 7 days of tweets into the following tables: a.original_ tweets table: i.We want to obtain the information about a tweet, who wrote it, the time posted, the content type etc. This will be stored in the original_tweets table. b.processed_tweets table: i.We want to then process the tweet text and content in order to simplify our ranking model. c.users table: i.We want to create a users table so that we can see the correlated tweets per user. We can do this multiple ways: tie the tweet id's in an array that correspond to the user. We want to also include the location of the user, number of followers, etc of what's included in the user-object https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overview/user-object d.user_category table: i.What kind of topics does this user rank mostly in? We should aggregate counts of topics that their previous seven days of tweets include in. This would be useful to observe at the end in our conclusion. If a user is most likely to post about politics, it would be interesting to see if they rank among the higher ranked tweets about politics in our ranking algorithm. e.tweet_category table i.Last but not least, we want to have a categories table where we can map the category, we can also create a list of tweets belonging in this table 5.Build an inverted index a.We should create an inverted index for every word presented in a tweet to further allow the ranking algorithm to rank a tweet accordingly. 6.Grab the top X users(on ranking BM25) that show up in terms of different categories. a.Use all of the categories (display invalid if not in the categories available) i.We want to be able to use all of the categories available in order to do ranking with the tweets related. If a query is present where it doesn't fit in with any of the categories, we shall show ""no category fits this tweet"" ii.Users being the top performing tweets for a given category 1.We want to also display top performing users for a given category. This can be done by collecting the overall topics this user tweeted about and presenting the ones with higher frequency under their id. iii.Top categories in sample set 1.Depending on the content that is aggregated in the past 7 days it would be informative to correlate these tweets with ongoing present news. Would be good to see if the ranking is similar to most searched on twitter charts and trending statistics. 7.Display top X on the terminal/UI. a.We will have an interactive terminal asking the user what they would like to display in terms of ranking as show in number 6. Twitter Recommendations Based on Text Team Members: Rohan Khanna (Team Captain) - rohank2@illinois.edu Tyler Wong - tylercw3@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu For full documentation please read our pdf report uploaded in this repo: https://github.com/rohankk2/Twitter-Recommendations-based-on-text/blob/main/Twitter%20Recommendations%20based%20on%20Text%20Final%20Report.pdf Introduction Twitter is a social media platform allowing users to connect and share thoughts and information. A notable example of Twitter and content shared was seen in 2020, with the presidency and election. There are currently recommendations on who to follow in general for Twitter users. However, when a tweet is made, that tweet does not have suggested/similar tweets that a user can react to or retweet. The purpose of this project is to give users the ability to get recommendations based on their tweets. Suppose you write ""love hamburgers and fries"", you should expect to get back a topic, and if you run that same query with our ranker, you will get a list of 10 tweets that are closest in similarity, along with the 10 users that have tweets that are closest to the content of this query. Obtaining Twitter Data You will need Python installed and all the required packages installed. pip install -r requirements.txt Obtain a Twitter developer account through the Twitter Developer Portal if you haven't already. Add the consumer key, consumer secret, access token, access token secret, and bearer token to the ""twitter_utils.py"" main method's variables. These will be used to interact with the Twitter API. Run the code to get the users and tweets. The code will check to make sure not to regenerate these, so if you run it multiple times you will need to delete the files in the ""data"" directory of the repository. python src/twitter_utils.py Data Extraction For topic extraction we use the nltk toolkits and gensim. As we learnt in class LDA is an unsupervised machine learning algorithm that uses a mixture model to generate documents. Each topic can be assigned some prior probability and each topic consists of probabilities for generating a particular word from the vocabulary. Data Retrieval/Cleaning: We developed a few different functions to perform data retrieval and cleaning of the tweets: remove_emoji(text) remove_links(text) remove_users(text) clean_tweet(tweet) All of these functions are meant to clean up the data so that we can perform better analysis of the data with better accuracy. We process all of our original tweets from the SQLite database through these functions. The remove_emoji(text)method removes any emojis that are found in the tweet because we found that emojis didn't provide much meaningful information. The remove_links(text) method removes any HTTP or HTTPs links that are found in the tweet text since that data isn't useful when determining category. The remove_users(text)method removes any ""@"" mentions for any other user. The clean_tweet(tweet)method makes the tweet all lowercase, removes punctuation, removes any stopwords, and removes any words that 2 characters or less. How to run Topic Extraction: To run the code a user can put test tweets in the test tweets.json file and just execute the python3.8 topicdeterminant.py this will classify the tweet into the most likely topic. Using this topic modelling you can draw a relationship between people who have similar tweets. Execute the following command on the terminal python3.8 topicdeterminant.py Ranker of tweets and users After loading the tweet data and creating a map of the author id's to the author's screen name, the tweets are then using tf-idf weights per word to score shared words. Given the following tweet_query ""heat is cranking"" I want to return a recommendation of tweets that are similar to the tweet_query, along with the % of their similarity. 1) To begin with, once git cloned, go to data and unzip tweets.tar.gz, this will unzip tweets.json. tar -xzvf tweets.tar.gz 2) You can choose a query to substitute into tweet_query in the ranker.py code. You may replace the writing within this query to obtain similarities with different queries. 3) To execute simply write the following in the terminal: python ranker.py Sources https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ https://github.com/enoreese/topic-modelling/blob/master/preprocessor.py https://github.com/4OH4/doc-similarity/blob/master/examples.ipynb Twitter Recommendations based on Text Report Team Members: Rohan Khanna (Team Captain) - rohank2@illinois.edu Tyler Wong - tylercw3@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu Introduction Twitter is a social media platform allowing users to connect and share thoughts and information. A notable example of Twitter and content shared was seen in 2020, with the presidency and election. There are currently recommendations on who to follow in general for Twitter users. However, when a tweet is made, that tweet does not have suggested/similar tweets that a user can react to or retweet. The purpose of this project is to give users the ability to get recommendations based on their tweets. Suppose you write ""love hamburgers and fries"", you should expect to get back a topic, and if you run that same query with our ranker, you will get a list of 10 tweets that are closest in similarity, along with the 10 users that have tweets that are closest to the content of this query. We would also get the topic of this tweet for classification purposes. Currently on twitter: What this project accomplishes: The following tweet will have a .89% of similarity. Twitter Data In order to get data from Twitter, we had to get approved for a developer key from Twitter. This is an application that we submitted which outlined our use case and was promptly approved. Even though we now have this API key, Twitter still limits your use of the platform through API. Most notably, they limit how much request you can do in one hour and they limit how much data you can get per request. Because of these request limits from the Twitter API, we decided to use an open source Python library to handle requesting data from Twitter's v1.1 API called  python-twitter . For querying data from Twitter's v2 API, we directly used Twitter's HTTP API since it's so new, there aren't many open source tools to use. To get the users we decided to get 3000 random active Twitter users. In order to do this, we queried Twitter's sample stream which provides a subset of active tweets coming in as a stream. Once we obtained a tweet from the stream, we queried the language of the tweet and to check if the user has a public profile or not. If they were english speaking and had a public profile, we obtained their user information. Once we have our random users, we got the last 7 days of tweets from them. We did this by querying for the tweet IDs and then getting each associated tweet from the v2 API. This was a very expensive operation since twitter limits how many tweets you can get, but given enough time we were able to get all this data. We ended up with around 120,000 tweets as our full data set which is included in the ""tweets.tar.gz"" file in the ""data"" folder of our GitHub repository. Example of a single Tweet data with annotations and entities How it works 1)You will need Python installed and all the required packages installed. pip install -r requirements.txt 2)Obtain a Twitter developer account through the  Twitter Developer Portal  if you haven't already. Add the consumer key, consumer secret, access token, access token secret, and bearer token to the ""twitter_utils.py"" main method's variables. These will be used to interact with the Twitter API. 3)Run the code to get the users and tweets. The code will check to make sure not to regenerate these, so if you run it multiple times you will need to delete the files in the ""data"" directory of the repository. python src/twitter_utils.py Database SQLite was used to extract the tweets and user information from the Twitter API's response. We used the Standard v1.1 API to extract information on the users. This was mainly important to later map the user id's to the screen name, location, etc. In the future, this information can be used to build recommendations of tweets based on the location of a user. Say a user lives in Miami, FL, and they tweet about an upcoming event with a celebrity. There can be a recommendation of tweets with that same geographical location. Not only could Twitter use this for marketing, but also increasing the connectivity that other social media platforms like Facebook have taken as an approach. In addition to the information on the users, we also have the tweet data from the early access Twitter v2 API. This version of the API was used because it contains the entity and context information that provides data on the subject and relations of the tweet. Some examples of entities are: Barack Obama, IBM, Mountain Dew, and San Francisco. Some examples of context are: TV Shows, TV Episodes, Podcast, Holiday, Politicians, and Video Game. SQLite stored this information in the following tables: userTable and originalTweets Topic Extraction For topic extraction we use the nltk toolkits and gensim. As we learnt in class LDA is an unsupervised machine learning algorithm that uses a mixture model to generate documents. Each topic can be assigned some prior probability and each topic consists of probabilities for generating a particular word from the vocabulary. DATA RETRIEVAL/CLEANING: We developed a few different functions to perform data retrieval and cleaning of the tweets: *remove_emoji(text) *remove_links(text) *remove_users(text) *clean_tweet(tweet) All of these functions are meant to clean up the data so that we can perform better analysis of the data with better accuracy. We process all of our original tweets from the SQLite database through these functions. The  remove_emoji(text) method removes any emojis that are found in the tweet because we found that emojis didn't provide much meaningful information. The  remove_links(text) method removes any HTTP or HTTPs links that are found in the tweet text since that data isn't useful when determining category. The  remove_users(text) method removes any ""@"" mentions for any other user. The  clean_tweet(tweet) method makes the tweet all lowercase, removes punctuation, removes any stopwords, and removes any words that are 2 characters or less. CORE ALGORITHM: Figure:1 Picture reference:  https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 We decided to go with 5 topics as this was a proof of concept and our test data was relatively small and could be described with 5 topics. After cleaning our data, we essentially create a document term matrix where the rows are the cleaned tweets and the columns are words. The matrix entries hold the count of those words in that particular tweet. For example matrix[i][j] will denote the count of that word in the tweet. This is essentially following the core concept of creating a bag of words model for our project. We then send this document count into the gensim lda model and it runs the LDA model to generate the topics. We run the lda with 20 passes for convergence and we decided to use gensim because it gives us the ability to store this model on the disk and hence reuse it for later applications. Moreover, we use the multicore lda as it can process the data parallely on multiple threads leading to better overall performance. After this for a new tweet we use this lda model and try to assign probabilities for which topic this new tweet could belong to. As shown in the demo video, we ran our topic classifier on the test tweet : Twitch is so cool! #twitch #twitchstreamer #digital Our classifier did a good job of classifying this to topic 4 which was : Topic: 4 Words: 0.020*""marketing"" + 0.013*""#digital"" + 0.008*""media"" + 0.007*""#youtube"" + 0.007*""#twitchde"" + 0.007*""#germanmediart"" + 0.007*""#twitch"" + 0.007*""#seo"" + 0.007*""#email"" + 0.007*""marketer"" Overall, this was a good approach as a proof of concept however, we definitely have a lot of room to develop on this project. How to run it : To run the code a user can put test tweets in the test tweets.json file and just execute the python3.8 topicdeterminant.py this will classify the tweet into the most likely topic. Using this topic modelling you can draw a relationship between people who have similar tweets. Execute  python3.8 topicdeterminant.py  on the terminal IMPROVEMENTS: 1.We could have worked more on cleaning the data to account for duplicate tweets in the form of retweets. Moreover, a lot of our accuracy issues were centered around bad data. We had tweets like :  oo I know what I\u2019m going to do.  These tweets are extremely hard to classify into any particular topic. There is no broad common topic that the above tweet could be classified into and hence such a tweet just corrupts our training data. 2.We could have worked to put a special emphasis on features of tweets like hashtags. Sources: 1.https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 2.http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ 3.https://github.com/enoreese/topic-modelling/blob/master/preprocessor.py Ranker of tweets and users After loading the tweet data and creating a map of the author id's to the author's screen name, the tweets are then using tf-idf weights per word to score shared words. Given the following tweet_query ""heat is cranking"" I want to return a recommendation of tweets that are similar to the tweet_query, along with the % of their similarity. We introduced a reduce_by_lemma function to reduce words to their lemma. Stop words from english nltk are the following:  {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}. We also lemma the stop words with stop_lemma. We then initialize TF-IDF Vector with the Lemma function under vectorize. The next thing to do here is create a vector between the tweet_query that we are looking for with the tweet data we collected. These vectors will be examined by creator cosine similarities between the vectors. Then we go through the flattened vectors to obtain the scores by the vectors. For score_titles, we want to get the score per tweet. Here we view tweet_data as the title since it's hard to give a title to one tweet. After, we print the authors associated with the recommended tweets based on similarity. These authors are printed for the user to know which authors may produce similar content as them. How it works 1)To begin with, once git cloned, go to data and unzip tweets.tar.gz, this will unzip tweets.json. tar -xzvf tweets.tar.gz 2)Like shown above, you can see that as a user, you can choose a query to substitute into tweet_query in the ranker.py code.  You may replace the writing within this query to obtain similarities with different queries. 3)To execute simply write the following in the terminal: python ranker.py Depending on your python version, you may have to pip install some nltk libraries. You would get a message saying whether you have to pip install nltk for example. Follow the prompt in the terminal and that should be resolved. Upon executing this you will receive the following output: Like discussed above, the results result in the number of tweets, a sample of five tweets from the total number of tweets, the top 10 tweets with similarity of content to the original tweet_query, and the top 10 users who made the tweets with most similarity. For testers: edit the query to anything random that you may think of as a tweet. Run the ranker.py to get recommendations on similar tweets. Improvements: The performance of the ranker was where we missed the mark. We could have optimized the code to perform better. We originally wanted results in 2 seconds, but when doing a lemma, the results take longer than without the lemma. With a lemma it takes approximately 30 seconds longer to obtain the results. Sources : This work was done by following the following github tutorial which explained using TF-IDF with cosine similarities. https://github.com/4OH4/doc-similarity/blob/master/examples.ipynb Contributions of Team Members Team Member Hours Worked Contributions to Project Rohan Khanna (Team Captain) 20 hours 1.Performed partial cleaning of the tweets data, ie, removing emoticons, punctuations etc. 2.Wrote the code for the main lda algorithm used to classify a tweet into a topic 3.Worked on software documentation and software usage presentation. Tyler Wong 20 hours 1.Set up Twitter API for project and familiarized myself with both versions of the Twitter API. 2.Gathered 3000 active random Twitter users and their last 7 days of claned categorized tweets using the Twitter API. 3.Worked on software documentation and software usage presentation. Cesia Bulnes 20 hours 1.Set up the skeleton of the database 2.Worked on the ranking of tweet content, to recommend top tweets a user would like based on their query/tweet, and returned the top users 3.Worked on software documentation and software usage presentation"
https://github.com/rosed2/CourseProject	"1.Progress made thus far a.So far, we have been able to preprocess our data so we can determine the aspect weight and aspect ratings. We were also able to determine the seed words in order to create our aspect segmentation and we will be using this when creating the rating regression. 2.Remaining tasks a.We have to still implement the rating regression model and determine if we were able to reproduce similar results to the research paper. We also need to finish implementing the sentiment analysis on each aspect which will lead to the creation of the aspect rating. 3.Any challenges/issues being faced a.Right now we are struggling with determining the term weights and trying to find an algorithm which will accurately assign weights to each term. We are looking into some of the lecture videos for additional help and will reach out to our TA if we are still struggling. Option chosen: Reproducing a Paper 1.Team members a.Archisha Majee, majee2 b.Captain: Rose Dinh, rosed2 2.Paper chosen: "" Latent aspect rating analysis without aspect keyword supervision."" Under subtopic: Latent aspect rating analysis 3.Programming language: python 4.Yes CS 410 CourseProject Project Proposal: We will be recreating the paper ""Latent Aspect Rating Analysis Without Aspect Keyword Supervision."" Code Overview: This code tries to implement Latent Aspect Rating Analysis (LARA) as a reproduction of research paper ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"" by Hongning Wang, Yue Lu, and ChengXiang Zhai. It takes a set of reviews and a list of aspects/topics covered within them. It also takes a list of feature words for each topic. Then, it finds the reviewer's ratings on these aspects and the weights the reviewer placed on these aspects to form the overall rating. This code can be used to analyze TripAdvisor reviews to find ratings on the following topics: Value, Rooms, Location, Cleanliness, Check in/front desk, Service, and Business service; and what weights the reviewer placed on each topic to construct the overall rating. Implementation: The code first preprocesses the reviews by removing stop words and stemming. Then the code assigns what topic each sentence in each review is about by comparing it to a list of user-defined topic feature words. The sentence is assigned the topic whose feature words it has the most of. Then, it uses sentiment analysis to determine topic ratings for each topic in each review. Next, it uses random variable initialization to determine the weights placed on each topic by the reviewer. We calculated which set of random weights returned the highest probability of getting the reviewer's overall rating from a Normal distribution whose mean was the weighted mean of the aspect ratings we found. Software Usage Tutorial Presentation: https://illinois.zoom.us/rec/share/aHaip21p63f29jFbE96x2s2LgFm9d6Pa4qIdLK3IwpMeqnh-pJqWV9ZXdbejbLHJ.ThV8W0Z9PVQhu4fv?startTime=1607902233000 Usage Notes: The code requires nltk, scipy, and numpy. Install using pip or other preferred python installation method. To use this code, download the repository, cd into the folder, and run the python file ""code.py"". Main Results: We found the aspect ratings and aspect weights for every review we parsed in the file ""test_result.txt"". To evaluate, we found the mean squared error of the aspect ratings was 2.383. In the paper, the desired result of the MSE was 2.130. Our result differed from the desired result by a very small amount. This difference could be due to the fact that we might've used a different sentiment analysis library and we weren't sure how to handle large amounts of neutral words. We also weren't sure how to determine the aspect weights, so we instead used a brute force approach where we tested random weights while the paper used gradient optimization but we couldn't get that to work. Team Contributions: Rose Dinh: Worked on preprocessing, assigning topics to each sentence, and determining ratings of topics in each review Archisha Majee: Worked on determining topic weights and finding MSE to evaluate the results"
https://github.com/rupsis/CourseProject	"IR Competition Description This is the final course project for CS 410 - Text Retrieval & Mining course at the University of Illinois at Urbana-Champaign. For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Project Team: Nathaniel Rupsis (NetID: nrupsis2) - Team Captain Hoa Le (NetID: hle30) Video Presentation Link to our video presentation for this project can be found below: https://mediaspace.illinois.edu/media/t/1_cgdhmw3n Prerequisites Packages Metapy We use Metapy to build and evaluate our search engi # Ensure your pip is up to date pip install --upgrade pip # install metapy! pip install metapy pytoml # Ensure your pip is up to date pip install --upgrade pip # install metapy! pip install metapy pytoml ne. argparse The argparse module makes it easy to write user friendly command line interfaces. # Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work # Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work argparse should work on Python 2.3 and newer. Running the program Directory: | - /test \- line.toml ... data | - /train \- line.toml ... data A test and train directory need to exist with a line.toml file which specifies the metadata. Note: the test and train dataset are too large to be uploaded to this repo. Please see the link below to download the dataset. You need to put line.toml file in the downloaded train dataset in order for build_courpse.py to create the index. The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing // line.toml type = ""line-corpus"" metadata = [{name = ""uid"", type = ""string""}] To build the index, simply run: // remove the index else the evaluator will use a cached version rm -rf idx/ && python3 build_courpse.py train where the first argument is either ""test"" or ""train"" (need to have corresponding test / train files for MetaPy to work) To run the evaluator: python search_eval.py config_train.toml where the config argument is either config_train.toml or config_test.toml File & Code Walkthrough search_eval.py This is our main file containing most of the crucial functions used for creating ranking algorithms and searching an index as well as evaluate and writing the results. Below is the list of functions in the search_eval.py file: expand_query(query): Description: This function is a simple approach to query expansion. It expands on some important keywords such as covid-19, coronavirus, test, mark, spread, etc... It also contains some pre-processing methods such as removing punctuation, converting to lower case, filtering out common words, etc... Parameter: query What the function returns: return list of query words containing new keywords. load_queries(): Description: This function retrieves query from the xml file. Parameter: None What the function returns: return tuple array of queryID and query pairs. load_ranker(cfg_file): Description: This function returns the Ranker object to evaluate Parameter: cfg_file is the path to a configuration file used to load the index. What the function returns: Ranker object. saveResults(prediction_results): Description: This function writes the results to ""predictions.txt"" file. Parameter: prediction_results (a list of 3 elements returned from running ranking algorithms) What the function returns: return list of query words containing new keywords. runQueries(queries): Description: This is our main function for creating inverted index and ranking algorithms. Parameter: it takes the output which is the query from load_queries function as input. What the function returns: The function saveResults mentioned above is called to store the output. build_courpse.py This function contains code to extract information such as title, abstract, introduction, etc... from metadata.csv using example code. It also writes the data to .dat file and create metadata file for metapy. config.toml We have two separate config file : config_test and config_train. Each file contains general settings that deal with paths related to the its corresponding dataset. We specified some details about the corpus and its analyzer. For example: dataset = ""train"" # name of the dataset corpus = ""line.toml"" # type of corpus query-judgements = ""qrels.txt"" # path to query judgement file [[analyzers]] method = ""ngram-word"" # set of co-occurring words ngram = 1 # specified to one word aka unigram filter = ""default-unigram-chain"" stopwords.txt A text file contains list of common English words that do not add much meaning to a sentence. For example: ""the"",""a"",""an"",etc...They can be safely ignored, and this is part of pre-processing to filter out useless information. train & test folders Contain the cord-19 dataset and metadata.csv file. The train data also contains query relevance file. predictions.txt This is a text file that has the same format as relevance judgment file which is (query_ID doc_uid relevance_score). It contains our final top 1000 documents per query and is used for computing nDCG score on the leaderboard. Implementation Overview Task Definitions Collecting data Unzip and extract data from the provided train.zip Collect and store information from metadata.csv such as title, abstract, introduction, etc... Building a corpus Create train/test.dat file. Each line in .dat file represents one document. Create metadata.dat file containing the collected information from step 1 part ii. Create line.toml configuration file containing corpus input formats. Implementing simple query expansion Collect query from xml file. Query are stored in a tuple array (queryID, query) Create query expansion function with some preprocessing methods. Create an inverted index Set up the config file. This file contains fields that need to be specified and makes references/pathing to several files. Write code to create inverted index. Searching the index Create bm25 ranker. Run query to search the index and returns top 1000 documents per query as sorted vector pairs (doc_id, double). Store the query-ids, results and the corresponding uid (document_id retrieved from metadata file) and write to predictions.txt file. Optimize nDCG score Submit predictions.txt file and check the nDCG score on the leaderboard. Address potential problems and identify methods to improve nDCG score. Potential Issues Encounter If UnicodeEncodeError: 'charmap' codec can't encode characters error appears, running python3 seems to fix that. One solution which we will add to our code is to write (encoding = ""utf-8"") when reading .json or .dat file. We ran into some disk problem when implementing BERT-large method for reranking. We decided to exclude it from our final code to avoid this potential issue. Key Findings & Potential Improvements Even though we beat the baseline and made it to leaderboard, we think that the result could be further enhanced. Below is what we identify that might help with improving the result: Add more information to the index (actual documents) Weight the keywords better since relevancy will definitely be based on the keywords (covid, alcohol, spread, African American, health, etc...) Further expanding on query. Query expansion has helped enhancing our result greatly. Implementing BERT large for reranking. We ran into disk problems when trying to implement it. Citation https://github.com/meta-toolkit/metapy Bhavya, Dec (2020) MP 2.4 [Source code] https://github.com/CS410Fall2020/MP2.4 Contact nrupsis@gmail.com hle027@gmail.com Team: NA coders Members: Hoa Le (NetID: hle30), Nathaniel Rupsis (NetID: nrupsis2) Project Progress Report For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Progress Made Thus Far We break down the project into smaller tasks and milestones: extracting, preprocessing, indexing, retrieving and ranking. In the extracting step, we collected Titles, Abstracts, and Introductions of papers using the Python code sample provided. Then, we fed the extracted data to the preprocess function we created. The preprocess function contains basic preprocessing tasks such as removing special characters, text.split(), stemming, etc... For data indexing, we've taken two different approaches, and each individual is working on their own implementation. By doing this, we'll have some options and flexibility when tweaking the overall system. The first approach is to build inverted indices from scratch. It will take 4 inputs (uid, title, abstract and keywords) or a dataframe containing those 4 elements and output a dictionary of inverted indices. These keywords are based on their tf-idf scores. We used the stop word file for previous assignments to obtain a list of words that will not be indexed. As for the query, we decided to go for the query field of the topic. They are easily preprocessed as the other two fields(question and narrative) contain uppercase and more special characters. The Second approach is to build a corpus .dat file with a pre-processor, and then utilizing the metapy indexing algorithm. By doing this, it allows us to focus on what to include in the index (title, author, etc), while freeing us from having to worry about the performance of the index. With both index approaches, our team has a firm handle on the data preparation, and the next task is to implement the retrieval model. For the ranking algorithm, we are looking to use okapi BM25 and combine it with other state-of-the-art methods. Remaining Tasks Even though we haven't gotten results from our okapi BM25 ranking algorithm, we are 90% sure that it won't be enough to beat the baseline. Thus, the remaining tasks would be to finalize all the steps we mentioned, testing BM 25, fine-tune various parameter settings and then look to combine it with different methods to enhance our result. We are currently looking at some of the live systems that use BM25 such as Neural Covidex. We are also exploring LDA(Latent Dirichlet Allocation) which studies semantic relationships. Challenges that We Encountered We had trouble getting started because we felt overwhelmed with the massive dataset which contains over 57000 articles. However, once we started to break down the project into smaller tasks, it became much more manageable. The preprocessing proved to be a bit of a tough task since the dataset contains highly technical papers with scientific terms. Another big challenge is to build an inverted index from scratch. I think it's extremely worthwhile and we can learn a lot from the process. Final challenge to find the advanced ranking methods to pass the baseline. Team: NA coders Members: Hoa Le (NetID: hle30), Nathaniel Rupsis (NetID: nrupsis2) Project Proposal The NA Coders will be competing in the IR (Information Retrieval) competition. Our team is prepared to research, test, and implement state of the art search methods and techniques. For this project, we'll be using python, and focusing on implementing and tuning the methods mentioned in the proposal(query expansion, feedback, rank fusion, etc). We will dig deeper into more advanced ranking techniques such as learning to rank, and Okapi BM25. We have experienced Okapi BM25 in the previous assignments and it's truly a state-of-the-art technique. I think it'll help with the project using different parameters. Learning to rank is a ranking technique where it learns to directly rank items by training a model to predict the probability of one item over another. We will be testing 3 different learning to rank algorithms: RankNet, LambdaRank and LambdaMART. We are prepared to learn more about these techniques to help us reach the baseline and achieve high standing position in the leaderboard. For query expansion, we'll be looking into utilizing  WordNet  with the  Natural Language Toolkit   to help test variations of query expansion to aid in the text retrieval model. For ranking fusion, since we are trying different IR models, it's best to have a method that can combine these models together so the overall probability that the document is relevant can be higher. Finally, time permitting, we'll test out and incorporate additional retrieval methods listed here , such as  Compound term processing  and  Contextual Searching . Team NA Coders Project team: Nathaniel Rupsis (team captain) Hoa Le Project details: This is the final course project for CS 410 - Text Retrieval & Mining course at the University of Illinois at Urbana-Champaign. For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Our team is prepared to research, test, and implement state of the art search methods and techniques. For this project, we'll be using python, and focusing on implementing and tuning the methods mentioned in the proposal(query expansion, feedback, rank fusion, etc). Time permitting, we'll test out and incorporate additional retrieval methods listed here, such as Compound term processing and Contextual Searching. The complete project proposal can be found here. Video Presentation: Link to our video presentation for this project can be found here: https://mediaspace.illinois.edu/media/t/1_cgdhmw3n Prerequisites Packages Metapy We use metapy to build and evaluate our search engine. If you have not installed metapy so far, use the following commands to get started. ```bash Ensure your pip is up to date pip install --upgrade pip install metapy! pip install metapy pytoml ``` argparse The argparse module makes it easy to write user friendly command line interfaces. argparse should work on Python 2.3 and newer. ```bash Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work ``` Running the program Directory: | - /test \- line.toml ... data | - /train \- line.toml ... data A test and train directory need to exist with a line.toml file which specifies the metaData. Note: the test and train dataset are too large to be uploaded to this repo. Please see the link below to download the dataset. You need to put line.toml file in the downloaded train dataset in order for build_courpse.py to create the index. The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing ``` // line.toml type = ""line-corpus"" metadata = [{name = ""uid"", type = ""string""}] ``` To build the index, simply run: // remove the index else the evaluator will use a cached version rm -rf idx/ && python3 build_courpse.py train where the first argument is either ""test"" or ""train"" (need to have corresponding test / train files fro MetaPy to work) To run the evaluator: python search_eval.py config_train.toml where the config argument is either config_train.toml or config_test.toml File & Code Walkthrough search_eval.py This is our main file containing most of the crucial functions used for creating ranking algorithms and searching an index as well as evaluate and writing the results. Below is the list of functions in the search_eval.py file: expand_query(query): Description: This function is a simple approach to query expansion. It expands on some important keywords such as covid-19, coronavirus, test, mark, spread, etc... It also contains some pre-processing methods such as removing punctuation, converting to lower case, filtering out common words, etc... Parameter: query What the function returns: return list of query words containing new keywords. load_queries(): Description: This function retrieves query from the xml file. Parameter: None What the function returns: return tuple array of queryID and query pairs. load_ranker(cfg_file): Description: This function returns the Ranker object to evaluate Parameter: cfg_file is the path to a configuration file used to load the index. What the function returns: Ranker object. saveResults(prediction_results): Description: This function writes the results to ""predictions.txt"" file. Parameter: prediction_results (a list of 3 elements returned from running ranking algorithms) What the function returns: return list of query words containing new keywords. runQueries(queries) : Description: This is our main function for creating inverted index and ranking algorithms. Parameter: it takes the output which is the query from load_queries function as input. What the function returns: The function saveResults mentioned above is called to store the output. build_courpse.py This function contains code to extract information such as title, abstract, introduction, etc... from metadata.csv using example code. It also writes the data to .dat file and create metadata file for metapy. config.toml We have two separate config file : config_test and config_train. Each file contains general settings that deal with paths related to the its corresponding dataset. We specified some details about the corpus and its analyzer. For example: dataset = ""train"" # name of the dataset corpus = ""line.toml"" # type of corpus query-judgements = ""qrels.txt"" # path to query judgement file [[analyzers]] method = ""ngram-word"" # set of co-occurring words ngram = 1 # specified to one word aka unigram filter = ""default-unigram-chain"" stopwords.txt A text file contains list of common English words that do not add much meaning to a sentence. For example: ""the"",""a"",""an"",etc...They can be safely ignored, and this is part of pre-processing to filter out useless information. train & test folders Contain the cord-19 dataset and metadata.csv file. The train data also contains query relevance file. predictions.txt This is a text file that has the same format as relevance judgment file which is (query_ID doc_uid relevance_score). It contains our final top 1000 documents per query and is used for computing nDCG score on the leaderboard. Implementation Overview Task Definitions Collecting data i. Unzip and extract data from the provided train.zip ii. Collect and store information from metadata.csv such as title, abstract, introduction, etc... Building a corpus i. Create train/test.dat file. Each line in .dat file represents one document. ii. Create metadata.dat file containing the collected information from step 1 part ii. iii. Create line.toml configuration file containing corpus input formats. Implementing simple query expansion i. Collect query from xml file. Query are stored in a tuple array (queryID, query) ii. Create query expansion function with some preprocessing methods. Create an inverted index i. Set up the config file. This file contains fields that need to be specified and makes references/pathing to several files. ii. Write code to create inverted index. Searching the index i. Create bm25 ranker. ii. Run query to search the index and returns top 1000 documents per query as sorted vector pairs (doc_id, double). iii. Store the query-ids, results and the corresponding uid (document_id retrieved from metadata file) and write to predictions.txt file. Optimize nDCG score i. Submit predictions.txt file and check the nDCG score on the leaderboard. Address potential problems and identify methods to improve nDCG score. Potential Issues Encounter If UnicodeEncodeError: 'charmap' codec can't encode characters error appears, running python3 seems to fix that. One solution which we will add to our code is to write (encoding = ""utf-8"") when reading .json or .dat file. We ran into some disk problem when implementing BERT-large method for reranking. We decided to exclude it from our final code to avoid this potential issue. Key Findings & Potential Improvements Even though we beat the baseline and made it to leaderboard, we think that the result could be further enhanced. Below is what we identify that might help with improving the result: 1. Add more information to the index (actual documents) 2. Weight the keywords better since relevancy will definitely be based on the keywords (covid, alcohol, spread, African American, health, etc...) 3. Further expanding on query. Query expansion has helped enhancing our result greatly. 4. Implementing BERT large for reranking. We ran into disk problems when trying to implement it. Citation https://github.com/meta-toolkit/metapy Bhavya, Dec (2020) MP 2.4 [Source code] https://github.com/CS410Fall2020/MP2.4 Contact nrupsis@gmail.com hle027@gmail.com"
https://github.com/sairanga123/CourseProject	"Improving ExpertSearch Progress Report Plan 2 Automate scraping process 2 Perform topic mining 2 Additional improvement 2 Improve UI 2 Progress 2 Automated scraping process 3 Deliverables: 3 Outputs: 3 Challenges: 3 Automated Scraper 4 Directory URL classification 5 1. Dataset preparation 5 2. Scraper 5 3. Text classification 6 Faculty URL classification 6 1. Dataset preparation 6 2. Scraper 6 3. Text classification 7 Topic Mining 7 Deliverables: 7 Outputs: 7 Challenges: 7 Topic Miner: 7 1. Corpus preparation 7 2. Model creation 8 3. Term extraction 8 Improved Email Extraction 9 Deliverables: 9 Regex Improvement: 9 UI Improvements 9 Deliverables: 9 Challenges: 9 Info Button: 9 Top 5 Topics Display: 9 Email Automation: 9 Plan Automate scraping process To identify faculty directory pages To identify faculty home pages Perform topic mining To identify top-k topics associated with each faculty Additional improvement To improve email extraction for each faculty Improve UI To display top-5 topics associated with each retrieved faculty To allow search based on any of the topics from the displayed topic cloud To prepopulate email content when clicked on a faculty's email address Progress Item Owner Status Automated Scraping Mriganka Sarma Completed: Automated scraper Data Handler Scraper Text Classifier Remaining: Optimizing parameters for classification Integration testing Challenges: None Topic Mining Zacharia Rupp Completed: Topic model Function to return top-10 words associated with query topic Remaining: Further exploration of best topics Clean up code Integration testing Challenges: Inferring topics takes considerable processing time Improved Email Extraction Improved UI Sai Ranganathan Completed: To display top 5 topics associated with each faculty member To prepopulate email field when clicked on email address To improve email extraction part 1. Challenges: None More detailed description is provided in the below sections. Automated scraping process Deliverables: Automated Scraper (auto_scraper.py) Data Handler (data_handler.py) Scraper (scraper.py) Text Classifier (text_classifier.py) Outputs: Corpus of classified Faculty Directory URLs Corpus of classified Faculty Bio URLs Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs Challenges: None Automated Scraper Automated scraper module (auto_scraper.py) automates the process in the following way: Uses the data handler to prepare a train and test set of Faculty Directory URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of directory contents Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" Saves the classified directory URLs to a file Uses the data handler to prepare a train and test set of Faculty Bio URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" Saves the classified bio URLs to a file Uses the scraper to scrape the faculty bios from the classified bio URLs Generates one document per faculty bio and saves under ExpertSearch/data/compiled_bios The automated scraper can be invoked as follows: $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t -d option specifies to generate/regenerate the train and test dataset. The dataset will be generated even if -d is not provided if the dataset doesn't exist yet. When -d is not provided, the existing dataset will be used. -t option specifies to retrain the Doc2Vec model on the train dataset. The model will be trained even if -t is not provided if the model wasn't trained and saved yet. When -t is not provided, the saved model will be loaded. The following sections describe the text classification tasks for Faculty Directory URLs and Faculty Bio URLs. Directory URL classification Dataset preparation First we need to prepare the dataset for training and testing the model. The following approach was used to prepare the dataset. Downloaded the known faculty directory pages from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. Collected top URLs from Alexa. These will serve as the ""negative"" examples. Collected the global top-50 pages of Alexa. Collected the top-50 pages for different countries. Manually verified that the pages are in English. About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. URLs for total 14 countries + top-50 global URLs from Alexa were collected. This gave 750 ""negative"" URLs. Wrote a python module (data_handler.py) for data handling that does the following: Converts the MP 2.1 sign-up data from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. Combines the top-50 Alexa URLs for 5 countries and labels them as ""test_dir"". Uses these 250 pages for testing. Mix the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Remove duplicates if any. This gives 734 URLs as the final training URLs. Mix the 400 Faculty Directory test URLs with the 250 Alexa training URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. Scraper Wrote a python module (scraper.py) for scraping the URLs collected from the above step. The scraper does the following: Gets the contents of each URL as text. Performs clean-up of non-ascii characters from the content. Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". Writes contents of each webpage as a single line of space separated words in a file meant to be the final corpus. This is done to prepare both the training corpus (""train_dataset.cor"") and the test corpus (""test_dataset.cor""). Text classification Wrote a python module (text_classifier.py) for performing the text classification task of identifying valid Faculty Directory pages from the test corpus. The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Dataset preparation The following approach was used to prepare the dataset: Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the train URLs. Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. Use the data handling module to do the following: Tag the faculty bio URLs as ""faculty"" and save to a file. Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. This will be the final file with all the train URLs. Scraper Since the ExpertSearch project already contains the faculty bios as documents, the top 1000 faculty bios are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: Scrapes the remaining train URLs from the train URLs file and appends to the train corpus (""train_bio_dataset.cor""). Uses the classified Faculty Directory URLs from the classified Directory URLs file above and gets all embedded potential faculty bio URLs as the test URLs. Scrapes the test URLs from above and adds to a test corpus (""test_bio_dataset.cor""). Text classification The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_bio_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Topic Mining Deliverables: Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Outputs: Trained topic model ('lda_mallet_model') Bag-of-words representation of corpus to be used with miner.py ('corpus_dictionary') Challenges: Inferring topics takes considerable processing time. Topic Miner: The topic miner uses gensim and mallet to create a model from the entire corpus. The process is as follows: Corpus preparation Read in compiled bios as strings Filter the string representation of each bio to: Remove stop words Extract HTML tags and elements Strip non-alphanumeric characters Strip numbers Strip words that exist in lists of terms extracted from the bios Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) Remove words shorter than four characters Split all words into a list of tokens Create list of documents which is comprised of lists of tokens for each document as described above Append bigrams and trigrams to each token list for each document Create a gensim dictionary from the above documents Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below: Create a general model with gensim.models.ldamodel.LdaModel class with 10 models Visually inspect term clusters to ensure they were meaningful Visualize clusters with pyLDAvis to assess clusters If the above criteria were not satisfactory: Tweak corpus construction After the above criteria was deemed satisfactory: Using gensim.models.wrappers.LdaMallet with the mallet library, I: Varied number of topics to create new model Assessed coherence of each model with varying number of topics Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains Chose the best model according to above criteria and saved it and the created dictionary for query inference Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query. Improved Email Extraction Deliverables: email-extraction.py Regex Improvement: There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Deliverables: Server (server.py) UI Front (index.js) Challenges: None Info Button: Information button is created at the top of each of the retrieved faculty. When the button is clicked there a table pops up that appears below the selected retrieved faculty The table will contain additional information regarding the research topics that the faculty does When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: Display the top 5 topics from the preview for each of the faculty. Display these topics in a table format when the information button is clicked Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Email Automation: Email comes pre-populated with a set subject and body. The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. 1 1 Improving ExpertSearch Progress Report Plan ........................................................................................................................................................................................ 2 Automate scraping process ....................................................................................................................................... 2 Perform topic mining ................................................................................................................................................... 2 Additional improvement .............................................................................................................................................. 2 Improve UI ....................................................................................................................................................................... 2 Progress ............................................................................................................................................................................... 2 Automated scraping process .................................................................................................................................... 3 Deliverables: ............................................................................................................................................................. 3 Outputs: ..................................................................................................................................................................... 3 Challenges: ................................................................................................................................................................ 3 Automated Scraper ................................................................................................................................................. 4 Directory URL classification ................................................................................................................................. 5 1. Dataset preparation .................................................................................................................................. 5 2. Scraper ........................................................................................................................................................ 5 3. Text classification ..................................................................................................................................... 6 Faculty URL classification ..................................................................................................................................... 6 1. Dataset preparation .................................................................................................................................. 6 2. Scraper ........................................................................................................................................................ 6 3. Text classification ..................................................................................................................................... 7 Topic Mining ................................................................................................................................................................... 7 Deliverables: ............................................................................................................................................................. 7 Outputs: ..................................................................................................................................................................... 7 Challenges: ................................................................................................................................................................ 7 Topic Miner: .............................................................................................................................................................. 7 1. Corpus preparation .................................................................................................................................. 7 2. Model creation ........................................................................................................................................... 8 3. Term extraction ......................................................................................................................................... 8 Improved Email Extraction ......................................................................................................................................... 9 Deliverables: ............................................................................................................................................................. 9 Regex Improvement: .......................................................................................................................................... 9 UI Improvements ........................................................................................................................................................... 9 Deliverables: ............................................................................................................................................................. 9 Challenges: ................................................................................................................................................................ 9 Info Button: ........................................................................................................................................................... 9 Top 5 Topics Display: ........................................................................................................................................ 9 Email Automation: .............................................................................................................................................. 9 2 Plan Automate scraping process - To identify faculty directory pages - To identify faculty home pages Perform topic mining - To identify top-k topics associated with each faculty Additional improvement - To improve email extraction for each faculty Improve UI - To display top-5 topics associated with each retrieved faculty - To allow search based on any of the topics from the displayed topic cloud - To prepopulate email content when clicked on a faculty's email address Progress Item Owner Status Automated Scraping Mriganka Sarma Completed: - Automated scraper - Data Handler - Scraper - Text Classifier Remaining: - Optimizing parameters for classification - Integration testing Challenges: - None Topic Mining Zacharia Rupp Completed: - Topic model - Function to return top-10 words associated with query topic Remaining: - Further exploration of best topics - Clean up code 3 - Integration testing Challenges: - Inferring topics takes considerable processing time Improved Email Extraction Improved UI Sai Ranganathan Completed: - To display top 5 topics associated with each faculty member - To prepopulate email field when clicked on email address - To improve email extraction part 1. Challenges: - None More detailed description is provided in the below sections. Automated scraping process Deliverables: - Automated Scraper (auto_scraper.py) - Data Handler (data_handler.py) - Scraper (scraper.py) - Text Classifier (text_classifier.py) Outputs: - Corpus of classified Faculty Directory URLs - Corpus of classified Faculty Bio URLs - Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs Challenges: - None 4 Automated Scraper Automated scraper module (auto_scraper.py) automates the process in the following way: - Uses the data handler to prepare a train and test set of Faculty Directory URLs - Uses the scraper to scrape these URLs to prepare the train and test corpus - Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of directory contents - Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" - Saves the classified directory URLs to a file - Uses the data handler to prepare a train and test set of Faculty Bio URLs - Uses the scraper to scrape these URLs to prepare the train and test corpus - Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios - Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" - Saves the classified bio URLs to a file - Uses the scraper to scrape the faculty bios from the classified bio URLs - Generates one document per faculty bio and saves under ExpertSearch/data/compiled_bios The automated scraper can be invoked as follows: $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t -d option specifies to generate/regenerate the train and test dataset. The dataset will be generated even if -d is not provided if the dataset doesn't exist yet. When -d is not provided, the existing dataset will be used. -t option specifies to retrain the Doc2Vec model on the train dataset. The model will be trained even if -t is not provided if the model wasn't trained and saved yet. When -t is not provided, the saved model will be loaded. The following sections describe the text classification tasks for Faculty Directory URLs and Faculty Bio URLs. 5 Directory URL classification 1. Dataset preparation First we need to prepare the dataset for training and testing the model. The following approach was used to prepare the dataset. o Downloaded the known faculty directory pages from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. o Collected top URLs from Alexa. These will serve as the ""negative"" examples. SS Collected the global top-50 pages of Alexa. SS Collected the top-50 pages for different countries. Manually verified that the pages are in English. o About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. o URLs for total 14 countries + top-50 global URLs from Alexa were collected. This gave 750 ""negative"" URLs. o Wrote a python module (data_handler.py) for data handling that does the following: SS Converts the MP 2.1 sign-up data from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". SS Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. SS Combines the top-50 Alexa URLs for 5 countries and labels them as ""test_dir"". Uses these 250 pages for testing. SS Mix the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Remove duplicates if any. This gives 734 URLs as the final training URLs. SS Mix the 400 Faculty Directory test URLs with the 250 Alexa training URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. 2. Scraper Wrote a python module (scraper.py) for scraping the URLs collected from the above step. The scraper does the following: o Gets the contents of each URL as text. o Performs clean-up of non-ascii characters from the content. o Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. o Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". 6 o Writes contents of each webpage as a single line of space separated words in a file meant to be the final corpus. SS This is done to prepare both the training corpus (""train_dataset.cor"") and the test corpus (""test_dataset.cor""). 3. Text classification Wrote a python module (text_classifier.py) for performing the text classification task of identifying valid Faculty Directory pages from the test corpus. The classification module does the following: o Uses gensim to build a Doc2Vec model for feature vector representation of each document. o Uses the train_dataset.cor to build the vocabulary and train the model. o Saves the model so that it can be reloaded while running next time on the same dataset. o Uses LogisticRegression as the classifier from scikit-learn module. o Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification 1. Dataset preparation The following approach was used to prepare the dataset: o Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the train URLs. o Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. o Use the data handling module to do the following: SS Tag the faculty bio URLs as ""faculty"" and save to a file. SS Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. SS This will be the final file with all the train URLs. 2. Scraper Since the ExpertSearch project already contains the faculty bios as documents, the top 1000 faculty bios are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: o Scrapes the remaining train URLs from the train URLs file and appends to the train corpus (""train_bio_dataset.cor""). o Uses the classified Faculty Directory URLs from the classified Directory URLs file above and gets all embedded potential faculty bio URLs as the test URLs. 7 o Scrapes the test URLs from above and adds to a test corpus (""test_bio_dataset.cor""). 3. Text classification The classification module does the following: o Uses gensim to build a Doc2Vec model for feature vector representation of each document. o Uses the train_bio_dataset.cor to build the vocabulary and train the model. o Saves the model so that it can be reloaded while running next time on the same dataset. o Uses LogisticRegression as the classifier from scikit-learn module. o Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Topic Mining Deliverables: - Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Outputs: - Trained topic model ('lda_mallet_model') - Bag-of-words representation of corpus to be used with miner.py ('corpus_dictionary') Challenges: - Inferring topics takes considerable processing time. Topic Miner: The topic miner uses gensim and mallet to create a model from the entire corpus. The process is as follows: 1. Corpus preparation o Read in compiled bios as strings o Filter the string representation of each bio to: - Remove stop words - Extract HTML tags and elements - Strip non-alphanumeric characters 8 - Strip numbers - Strip words that exist in lists of terms extracted from the bios - Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) - Remove words shorter than four characters - Split all words into a list of tokens o Create list of documents which is comprised of lists of tokens for each document as described above o Append bigrams and trigrams to each token list for each document o Create a gensim dictionary from the above documents o Create a bag-of-word representation of our documents: this will be our corpus. 2. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below: o Create a general model with gensim.models.ldamodel.LdaModel class with 10 models o Visually inspect term clusters to ensure they were meaningful o Visualize clusters with pyLDAvis to assess clusters o If the above criteria were not satisfactory: a. Tweak corpus construction o After the above criteria was deemed satisfactory: a. Using gensim.models.wrappers.LdaMallet with the mallet library, I: i. Varied number of topics to create new model ii. Assessed coherence of each model with varying number of topics iii. Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains iv. Chose the best model according to above criteria and saved it and the created dictionary for query inference 3. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query. 9 Improved Email Extraction Deliverables: - email-extraction.py Regex Improvement: - There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting - Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Deliverables: - Server (server.py) - UI Front (index.js) Challenges: - None Info Button: - Information button is created at the top of each of the retrieved faculty. - When the button is clicked there a table pops up that appears below the selected retrieved faculty - The table will contain additional information regarding the research topics that the faculty does - When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: - Display the top 5 topics from the preview for each of the faculty. - Display these topics in a table format when the information button is clicked - Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Email Automation: - Email comes pre-populated with a set subject and body. - The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. PROJECT REPORT Improved ExpertSearch System (Team ZMS) Zacharia Rupp (zrupp2@illinois.edu) Mriganka Sarma (ms76@illinois.edu) Sai Ranganathan (sr50@illinois.edu) Introduction 4 1. Functional Overview 4 2. Implementation Details 4 2.1. Automated scraping process 4 2.1.1. Inputs: 5 2.1.2. Outputs: 5 2.1.3. Deliverables: 5 2.1.4. Component design / Code workflow: 5 Automated Scraper (auto_scraper.py) 5 Directory URL Classification 6 Data Handler (data_handler.py) 7 Scraper (scraper.py) 8 Text Classifier (text_classifier.py) 9 Faculty URL classification 9 Data Handler (data_handler.py) 9 Scraper (scraper.py) 11 Text Classifier (text_classifier.py) 11 2.2. Topic Mining 12 2.2.1. Inputs: 12 2.2.2. Outputs: 12 2.2.3. Deliverables: 12 2.2.4. Component design / Code workflow: 12 Topic Miner (miner.py) 12 Corpus preparation 12 Model creation 13 Term extraction 14 2.3. Improved Email Extraction 16 2.3.1. Inputs: 16 2.3.2. Outputs: 16 2.3.3. Deliverables: 16 2.3.4. Component design / Code workflow: 16 Regex Improvement: 16 2.4. UI Improvements 16 2.4.1. Inputs: 16 2.4.2. Outputs: 16 2.4.3. Deliverables: 16 2.4.4. Component design / Code workflow: 17 Info Button: 17 Top 5 Topics Display: 17 Email Automation: 17 3. Usage Details 17 3.1. Setup Guide (Mac) 17 3.1.1. Repo setup 17 3.1.2. Project environment setup 18 3.2. Setup Guide (Windows) 19 3.3. Usage Guide 19 3.3.1. Running the Automated Scraper 19 3.3.2. Running the Topic Miner 20 3.3.3. Running the Backend Server 20 3.3.4. Running Faculty Search from the UI 21 3.4. Example Use Cases: 21 3.4.1. Use Case 1 - Basic use to search faculties 21 3.4.2. Use Case 2 - Find research interests of the faculty 21 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results 21 3.4.4. Use Case 4 - Connecting to faculty 21 4. Contributions 22 Introduction The ExpertSearch system is a system to search faculties who are experts in certain research areas or topics from university websites crawled from the web. The goal of our project is to improve this ExpertSearch system in a few ways, including automating the scraping process, adding topic mining for finding faculty research topics, and improving the UI to give improved visualizations and query refinement options. Functional Overview The improved ExpertSearch system enables the following functionalities: Automatic scraping of websites to identify faculty directory webpages and non-directory webpages Automatic scraping of the classified faculty directory webpages to further identify faculty bio webpages and non-bio webpages Automatic scraping of faculty bio webpages to generate one bio document per faculty and adding to the compiled bios Topic Mining from the compiled bios to extract research topics of the faculties Display top-5 research topics associated with each retrieved faculty Improved email extraction for each faculty Refine search query using any of the topics from the displayed topic cloud Prepopulate email content when clicked on a faculty's email address The Automated Scraper improves the faculty bio generation process from a vast collection of websites. The Topic Miner adds more structure to the unstructured faculty website data retrieved from a query. The enhanced UI enables succinct visualization of the structured faculty results and provides shortcuts for additional search filters and faculty connection. Together, these new features improve the utility of the ExpertSearch system to the user. Implementation Details Automated scraping process The Automated Scraper takes a set of known University websites and top 500 Alexa websites as input, performs a series of operations to classify the directory URLs and then to classify the faculty homepages. The automated scraper then scrapes the classified faculty homepages to generate faculty bio documents and adds the bios to the collection. Inputs: University websites Top-500 Alexa websites Outputs: Corpus of classified Faculty Directory URLs (classified_dir_urls.cor) Corpus of classified Faculty Bio URLs (classified_faculty_urls.cor) Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs (e.g. 6530.txt) Deliverables: Automated Scraper (auto_scraper.py) Data Handler (data_handler.py) Scraper (scraper.py) Text Classifier (text_classifier.py) Component design / Code workflow: Automated Scraper (auto_scraper.py) The Automated Scraper module automates the whole flow of generating the faculty bios from the input mixture of ""positive"" and ""negative"" URLs in the following sequence of steps: Uses the data handler (data_handler.py) to prepare a train and test set of Faculty Directory URLs Uses the scraper (scraper.py) to scrape these URLs to prepare the train and test corpus Uses the text classifier (text_classifier.py) to build and train a Doc2Vec model on the documents in the train corpus of directory contents Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" Saves the classified directory URLs to a file (classified_dir_urls.cor) Uses the data handler to prepare a train and test set of Faculty Bio URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" Saves the classified bio URLs to a file (classified_faculty_urls.cor) Uses the scraper to scrape the faculty bios from the classified bio URLs Generates one document per faculty bio (e.g. 6530.txt) and saves under ExpertSearch/data/compiled_bios The auto_scraper.py module is the entry point for the complete automatic scraping and bio generation task. The following figure (Fig. 1) shows the complete automation flow starting with the input websites till the bio generation completion. Fig. 1: Automation Control Flow / Module interactions Directory URL Classification First, let's explain the Directory URL Classification task with the help of the modules. Data Handler (data_handler.py) The Data Handler module first takes the University websites and Alexa websites as input, mixes them and partitions them into test and train URLs. Then uses the scraper module to extract the URL contents into test and train corpus as shown in the figure below (Fig. 2). Fig. 2: Dataset Preparation for Directory URL Classification Here's a detailed explanation of the approach used by the Data Handler module to prepare the URLs for the scraper. Downloaded the known faculty directory URLs from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. Collected top URLs from Alexa. These will serve as the ""negative"" examples. Collected the global top-50 pages of Alexa. Collected the top-50 pages for 14 different countries. Manually verified that the pages are in English. This gave 750 ""negative"" URLs. When the AutoScraper is launched, it invokes the DataHandler which mixes and partitions the above URLs into train and test URLs as follows: Training URLs Converts the MP 2.1 sign-up sheet from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. Mixes the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Removes duplicates if any. This gives 734 URLs as the final training URLs. The training URLs are saved in the file train_urls.cor. Test URLs Combines the top-50 Alexa URLs for 5 countries. Uses these 250 pages for testing. Mixes the 400 Faculty Directory test URLs with the 250 Alexa test URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. The test URLs are saved in the file test_urls.cor. Scraper (scraper.py) The Scraper module scrapes the contents from the above train and test URLs and prepares the train and test corpus for the classification task. The scraper does the following: Gets the contents of each URL as text. Performs clean-up of non-ascii characters from the content. Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". Writes contents of each training URL as a single line of space separated words to the training corpus (""train_dataset.cor""). Similarly, writes contents of each test URL as a single line of space separated words to the test corpus (""test_dataset.cor""). Text Classifier (text_classifier.py) The Text Classifier module uses the train and test dataset from above step to classify the Faculty Directory URLs. The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Next, let's look at the Faculty URL Classification task. Data Handler (data_handler.py) The Data Handler module now takes the existing project's known Faculty Bio URLs and mixes with some Alexa URLs to prepare the train dataset. Uses the classified Faculty Directory URLs from the above step to extract potential Faculty Bio URLs to prepare the test dataset. Then uses the scraper module to extract the URL contents into bio test and bio train corpus as shown in the figure below (Fig. 3). Fig. 3: Dataset Preparation for Faculty Bio URL Classification The following approach was used to prepare the dataset: Training URLs Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the ""positive"" train URLs. Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. Combine them. Tag the faculty bio URLs as ""faculty"" and save to the file ""train_bio_urls.cor"". Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. This will be the final file with all the train URLs. Test URLs Use the Classified Faculty Directory URLs obtained from the Directory URL Classification task above. Use the scraper to find all potential faculty bio URLs from each of these Directory URLs. Save all these potential faculty bio URLs to the file ""test_bio_urls.cor"". This will be the final file with all the test URLs. Scraper (scraper.py) Since the ExpertSearch project already contains the faculty bios as documents, the contents of the top 1000 faculty bios (0.txt ... 999.txt) are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: Scrapes the URLs from line no. 1000 till the end from the file train_bio_urls.cor and appends to the train corpus (""train_bio_dataset.cor""). Scrapes the contents of the test URLs (i.e. potential faculty bio URLs) from the test_bio_urls.cor file above and adds those contents to the test corpus (""test_bio_dataset.cor""). Text Classifier (text_classifier.py) The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_bio_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories (bio or non-bio) of the test URLs given the test dataset. Finally, the Scraper module scrapes these classified bio URLs and saves the contents of each bio URL to a new file under ExpertSearch/data/compiled_bios. Topic Mining Inputs: Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) Outputs: Trained topic model (lda_mallet_model) Bag-of-words representation of corpus to be used with miner.py (corpus_dictionary) Text representation of corpus (lda_corpus) Deliverables: Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Component design / Code workflow: Topic Miner (miner.py) The topic miner pulls a topic distribution from a document already mined if the model was trained on the document, otherwise it uses gensim and mallet to create a model from the entire corpus. The process is described below: Corpus preparation Read in compiled bios as strings Filter the string representation of each bio to: Remove stop words Extract HTML tags and elements Strip non-alphanumeric characters Strip numbers Strip words that exist in lists of terms extracted from the bios Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) Remove words shorter than four characters Split all words into a list of tokens Create list of documents which is comprised of lists of tokens for each document as described above Append bigrams and trigrams to each token list for each document Create a gensim dictionary from the above documents Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below (Fig. 4): Create a general model with gensim.models.ldamodel.LdaModel class with 10 models Visually inspect term clusters to ensure they were meaningful If the above criteria were not satisfactory: Tweak corpus construction After the above criteria was deemed satisfactory: Using gensim.models.wrappers.LdaMallet with the mallet library, I: Varied number of topics to create new model Assessed coherence of each model with varying number of topics Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains Chose the best model according to above criteria and saved it and the created dictionary for query inference Fig. 4: Workflow for building an optimal topic model. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query as shown below (Fig. 5). Fig. 5: Extracting topics from documents not included in training set. Improved Email Extraction Inputs: Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) Outputs: Extracted emails for the faculties Deliverables: Updated email extractor (extract-email.py) Component design / Code workflow: Regex Improvement: There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Inputs: Query terms in the search box Topics mined by the topic miner Outputs: Updated UI showing top-5 research topic per faculty Updated UI showing topic cloud Clickable topic terms for query refinement Prepopulated email template on-click email icon Deliverables: Updated server endpoints (server.py) Updated UI (index.js) Component design / Code workflow: Info Button: Information button is created at the top of each of the retrieved faculty. When the button is clicked there a table pops up that appears below the selected retrieved faculty The table will contain additional information regarding the research topics that the faculty does When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: Display the top 5 topics from the preview for each of the faculty. Display these topics in a table format when the information button is clicked Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Clicking on the ""Add to Query"" button will refine the current search query to include this topic. Email Automation: Email comes pre-populated with a set subject and body. The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. Usage Details The modified project has been tested on Mac and Windows with Python 2.7. Here are the setup instructions for each of these platforms. Setup Guide (Mac) Repo setup Run the following command on a terminal to clone the github repository. $ git clone https://github.com/sairanga123/CourseProject.git The directory structure of the project is as below (listing only the files/folders relevant to this project): CourseProject |__________ ExpertSearch |__________ AutoScraper | |__________ data | |__________ auto_scraper.py | |__________ data_handler.py | |__________ scraper.py | |__________ text_classifier.py | |__________ d2v.model | |__________ d2v-bio.model | |__________ data | |__________ compiled_bios | |__________ expertsearch | |__________ mallet-2.0.8 | |__________ model_files | |__________ corpus_dictionary | |__________ lda_mallet_model | |__________ lda_corpus | |__________ miner.py | |__________ extraction |__________ mallet-2.0.8 |__________ static |__________ server.py Project environment setup The project has been tested on python 2.7. Please setup a python 2.7 environment for running the project. Creating an environment from Anaconda will make many common packages available. So a quick way to start would be to setup a python 2.7 environment from Anaconda. May need to install many or all of the following python packages depending on what packages the python environment already has. gunicorn=19.10.0 flask=1.1.2 metapy=0.2.13 requests=2.25.0 pytoml=0.1.21 gensim=3.8.3 nltk=3.4 bs4=0.0.1 lxml=4.6.2 numpy=1.16.6 sklearn=0.0 Setup Guide (Windows) Windows is currently not supported. If you want to build in Windows, use Windows Subsystem for Linux and follow the steps above. Usage Guide Running the Automated Scraper Run the AutoScraper to generate the bio documents. This step has been already performed and the generated bio documents have already been added to the ExpertSearch/data/compiled_bios folder. Here are the instructions for running the AutoScraper if it needs to be run again with additional input. To run the automated scraper, first go the ExpertSearch/AutoScraper directory. Then the Automated Scraper can be invoked as follows: -d option specifies to generate/regenerate the train and test dataset. If -d switch is used: If the dataset already exists, it will be regenerated If the dataset doesn't yet exist, it will be generated If -d switch is not used: If the dataset already exists, the existing dataset will be used in the subsequent flow If the dataset doesn't yet exist, it will be generated even if -d switch is not used -t option specifies to train/retrain the Doc2Vec model on the train dataset. If -t switch is used: If a trained and saved model already exists, the model will be retrained and saved again If a trained and saved model doesn't yet exist, it will be trained and saved If -t switch is not used: If a trained and saved model already exists, the saved model will be loaded and used for inference in the subsequent flow If a trained and saved model doesn't yet exist, it will be trained and saved even if -t switch is not used Running the Topic Miner The steps for creating the topic model are documented in ExpertSearch/data/expertsearch/LDATopicModeling.ipynb. The model construction is not something that can necessarily be automated because relying on perplexity and coherence scores alone often results in topics that don't make any meaningful sense to a human. Once the topic model is constructed and saved, miner.py allows the server to load the model and make inferences. Running the Backend Server Once, the topic model has been built, we can start the server. To start the server, go to the ExpertSearch folder. Then run the following command: $ gunicorn server:app -b 127.0.0.1:8095 Running Faculty Search from the UI Now, launch a web browser and type the following URL: localhost:8095 Example Use Cases: Use Case 1 - Basic use to search faculties Let's assume that we want to find the faculties that are working on ""text mining"". Then we'd go to the UI and enter our search string as ""text mining"". The existing system would retrieve the top ranked faculty results working on ""text mining"". Use Case 2 - Find research interests of the faculty Now, the improved system will also provide an info button which will bring up an additional table of information for each faculty. This table shows the top 5 research topics the faculty is associated with. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results We now maybe interested in learning more about who are the faculties that are working on any of these research topics. We can quickly search for all the faculties working on this new research topic by simply clicking on the ""Add to Query"" button for that research topic. This will automatically modify our search query by including that new research topic without having to type it in the search box. The retrieved faculty results will show the list of faculties working on that research topic. Use Case 4 - Connecting to faculty Another way we could use the system is to click on the email icon to send an email to the faculty's email address. While we may be at a loss of words for that first email, the system will provide a pre-populated template email which will automatically address the faculty's name and also include reference to the faculty's research area. This will make connecting to an expert faculty just one click away. Contributions Item Sub-items Contributor Automated Scraping Automated Scraper to automate the complete process Data Handler to prepare the datasets for the text classification tasks Scraper to scrape the URLs Text Classifier to classify directory and bio URLs Function to generate bio documents and add to compiled bios Mriganka Sarma Topic Mining Topic model Function to return top-10 words associated with query topic Zacharia Rupp Improved Email Extraction Added regular expressions to extract emails with atypical forms (e.g. person at place dot com) Sai Ranganathan Zacharia Rupp Improved UI Display top 5 topics associated with each faculty member Display cloud of topics Pre-populate email field when clicked on email address Improve email extraction part 1. Sai Ranganathan 1 1 PROJECT REPORT Improved ExpertSearch System (Team ZMS) Zacharia Rupp (zrupp2@illinois.edu) Mriganka Sarma (ms76@illinois.edu) Sai Ranganathan (sr50@illinois.edu) 2 Introduction ................................................................................................................................. 4 1. Functional Overview ......................................................................................................... 4 2. Implementation Details .................................................................................................... 4 2.1. Automated scraping process ..................................................................................... 4 2.1.1. Inputs: ........................................................................................................................ 5 2.1.2. Outputs: .................................................................................................................... 5 2.1.3. Deliverables: ............................................................................................................ 5 2.1.4. Component design / Code workflow: ................................................................ 5 Automated Scraper (auto_scraper.py) ......................................................................... 5 Directory URL Classification .......................................................................................... 6 Data Handler (data_handler.py) ................................................................................. 7 Scraper (scraper.py) ..................................................................................................... 8 Text Classifier (text_classifier.py) ............................................................................. 9 Faculty URL classification .............................................................................................. 9 Data Handler (data_handler.py) ................................................................................. 9 Scraper (scraper.py) ................................................................................................... 11 Text Classifier (text_classifier.py) ........................................................................... 11 2.2. Topic Mining .................................................................................................................. 12 2.2.1. Inputs: ...................................................................................................................... 12 2.2.2. Outputs: .................................................................................................................. 12 2.2.3. Deliverables: .......................................................................................................... 12 2.2.4. Component design / Code workflow: .............................................................. 12 Topic Miner (miner.py) ................................................................................................... 12 Corpus preparation ..................................................................................................... 12 Model creation .............................................................................................................. 13 Term extraction ............................................................................................................ 14 2.3. Improved Email Extraction ........................................................................................ 16 2.3.1. Inputs: ...................................................................................................................... 16 2.3.2. Outputs: .................................................................................................................. 16 2.3.3. Deliverables: .......................................................................................................... 16 2.3.4. Component design / Code workflow: .............................................................. 16 Regex Improvement: ................................................................................................... 16 2.4. UI Improvements .......................................................................................................... 16 3 2.4.1. Inputs: ...................................................................................................................... 16 2.4.2. Outputs: .................................................................................................................. 16 2.4.3. Deliverables: .......................................................................................................... 16 2.4.4. Component design / Code workflow: .............................................................. 17 Info Button: ................................................................................................................... 17 Top 5 Topics Display: ................................................................................................. 17 Email Automation: ....................................................................................................... 17 3. Usage Details .................................................................................................................... 17 3.1. Setup Guide (Mac) ....................................................................................................... 17 3.1.1. Repo setup ............................................................................................................. 17 3.1.2. Project environment setup ................................................................................. 18 3.2. Setup Guide (Windows) .............................................................................................. 19 3.3. Usage Guide .................................................................................................................. 19 3.3.1. Running the Automated Scraper ...................................................................... 19 3.3.2. Running the Topic Miner .................................................................................... 20 3.3.3. Running the Backend Server ............................................................................. 20 3.3.4. Running Faculty Search from the UI ............................................................... 21 3.4. Example Use Cases: ................................................................................................... 21 3.4.1. Use Case 1 - Basic use to search faculties ................................................... 21 3.4.2. Use Case 2 - Find research interests of the faculty .................................... 21 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results ...................................................................................... 21 3.4.4. Use Case 4 - Connecting to faculty ................................................................. 21 4. Contributions .................................................................................................................... 22 4 Introduction The ExpertSearch system is a system to search faculties who are experts in certain research areas or topics from university websites crawled from the web. The goal of our project is to improve this ExpertSearch system in a few ways, including automating the scraping process, adding topic mining for finding faculty research topics, and improving the UI to give improved visualizations and query refinement options. 1. Functional Overview The improved ExpertSearch system enables the following functionalities: Y= Automatic scraping of websites to identify faculty directory webpages and non-directory webpages Y= Automatic scraping of the classified faculty directory webpages to further identify faculty bio webpages and non-bio webpages Y= Automatic scraping of faculty bio webpages to generate one bio document per faculty and adding to the compiled bios Y= Topic Mining from the compiled bios to extract research topics of the faculties Y= Display top-5 research topics associated with each retrieved faculty Y= Improved email extraction for each faculty Y= Refine search query using any of the topics from the displayed topic cloud Y= Prepopulate email content when clicked on a faculty's email address The Automated Scraper improves the faculty bio generation process from a vast collection of websites. The Topic Miner adds more structure to the unstructured faculty website data retrieved from a query. The enhanced UI enables succinct visualization of the structured faculty results and provides shortcuts for additional search filters and faculty connection. Together, these new features improve the utility of the ExpertSearch system to the user. 2. Implementation Details 2.1. Automated scraping process The Automated Scraper takes a set of known University websites and top 500 Alexa websites as input, performs a series of operations to classify the directory URLs and then to classify the faculty homepages. The automated scraper then 5 scrapes the classified faculty homepages to generate faculty bio documents and adds the bios to the collection. 2.1.1. Inputs: o University websites o Top-500 Alexa websites 2.1.2. Outputs: o Corpus of classified Faculty Directory URLs (classified_dir_urls.cor) o Corpus of classified Faculty Bio URLs (classified_faculty_urls.cor) o Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs (e.g. 6530.txt) 2.1.3. Deliverables: o Automated Scraper (auto_scraper.py) o Data Handler (data_handler.py) o Scraper (scraper.py) o Text Classifier (text_classifier.py) 2.1.4. Component design / Code workflow: Automated Scraper (auto_scraper.py) The Automated Scraper module automates the whole flow of generating the faculty bios from the input mixture of ""positive"" and ""negative"" URLs in the following sequence of steps: o Uses the data handler (data_handler.py) to prepare a train and test set of Faculty Directory URLs o Uses the scraper (scraper.py) to scrape these URLs to prepare the train and test corpus o Uses the text classifier (text_classifier.py) to build and train a Doc2Vec model on the documents in the train corpus of directory contents o Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" o Saves the classified directory URLs to a file (classified_dir_urls.cor) o Uses the data handler to prepare a train and test set of Faculty Bio URLs o Uses the scraper to scrape these URLs to prepare the train and test corpus 6 o Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios o Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" o Saves the classified bio URLs to a file (classified_faculty_urls.cor) o Uses the scraper to scrape the faculty bios from the classified bio URLs o Generates one document per faculty bio (e.g. 6530.txt) and saves under ExpertSearch/data/compiled_bios The auto_scraper.py module is the entry point for the complete automatic scraping and bio generation task. The following figure (Fig. 1) shows the complete automation flow starting with the input websites till the bio generation completion. Fig. 1: Automation Control Flow / Module interactions Directory URL Classification First, let's explain the Directory URL Classification task with the help of the modules. 7 Data Handler (data_handler.py) The Data Handler module first takes the University websites and Alexa websites as input, mixes them and partitions them into test and train URLs. Then uses the scraper module to extract the URL contents into test and train corpus as shown in the figure below (Fig. 2). Fig. 2: Dataset Preparation for Directory URL Classification 8 Here's a detailed explanation of the approach used by the Data Handler module to prepare the URLs for the scraper. Y= Downloaded the known faculty directory URLs from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. o About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. Y= Collected top URLs from Alexa. These will serve as the ""negative"" examples. o Collected the global top-50 pages of Alexa. o Collected the top-50 pages for 14 different countries. Manually verified that the pages are in English. o This gave 750 ""negative"" URLs. Y= When the AutoScraper is launched, it invokes the DataHandler which mixes and partitions the above URLs into train and test URLs as follows: o Training URLs Y= Converts the MP 2.1 sign-up sheet from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". Y= Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. Y= Mixes the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Removes duplicates if any. This gives 734 URLs as the final training URLs. Y= The training URLs are saved in the file train_urls.cor. o Test URLs Y= Combines the top-50 Alexa URLs for 5 countries. Uses these 250 pages for testing. Y= Mixes the 400 Faculty Directory test URLs with the 250 Alexa test URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. Y= The test URLs are saved in the file test_urls.cor. Scraper (scraper.py) The Scraper module scrapes the contents from the above train and test URLs and prepares the train and test corpus for the classification task. The scraper does the following: Y= Gets the contents of each URL as text. Y= Performs clean-up of non-ascii characters from the content. 9 Y= Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. Y= Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". Y= Writes contents of each training URL as a single line of space separated words to the training corpus (""train_dataset.cor""). Y= Similarly, writes contents of each test URL as a single line of space separated words to the test corpus (""test_dataset.cor""). Text Classifier (text_classifier.py) The Text Classifier module uses the train and test dataset from above step to classify the Faculty Directory URLs. The classification module does the following: Y= Uses gensim to build a Doc2Vec model for feature vector representation of each document. Y= Uses the train_dataset.cor to build the vocabulary and train the model. Y= Saves the model so that it can be reloaded while running next time on the same dataset. Y= Uses LogisticRegression as the classifier from scikit-learn module. Y= Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Next, let's look at the Faculty URL Classification task. Data Handler (data_handler.py) The Data Handler module now takes the existing project's known Faculty Bio URLs and mixes with some Alexa URLs to prepare the train dataset. Uses the classified Faculty Directory URLs from the above step to extract potential Faculty Bio URLs to prepare the test dataset. Then uses the scraper module to extract the URL contents into bio test and bio train corpus as shown in the figure below (Fig. 3). 10 Fig. 3: Dataset Preparation for Faculty Bio URL Classification 11 The following approach was used to prepare the dataset: Y= Training URLs o Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the ""positive"" train URLs. o Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. o Combine them. o Tag the faculty bio URLs as ""faculty"" and save to the file ""train_bio_urls.cor"". o Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. o This will be the final file with all the train URLs. Y= Test URLs o Use the Classified Faculty Directory URLs obtained from the Directory URL Classification task above. o Use the scraper to find all potential faculty bio URLs from each of these Directory URLs. o Save all these potential faculty bio URLs to the file ""test_bio_urls.cor"". o This will be the final file with all the test URLs. Scraper (scraper.py) Since the ExpertSearch project already contains the faculty bios as documents, the contents of the top 1000 faculty bios (0.txt ... 999.txt) are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: Y= Scrapes the URLs from line no. 1000 till the end from the file train_bio_urls.cor and appends to the train corpus (""train_bio_dataset.cor""). Y= Scrapes the contents of the test URLs (i.e. potential faculty bio URLs) from the test_bio_urls.cor file above and adds those contents to the test corpus (""test_bio_dataset.cor""). Text Classifier (text_classifier.py) The classification module does the following: Y= Uses gensim to build a Doc2Vec model for feature vector representation of each document. Y= Uses the train_bio_dataset.cor to build the vocabulary and train the model. 12 Y= Saves the model so that it can be reloaded while running next time on the same dataset. Y= Uses LogisticRegression as the classifier from scikit-learn module. Y= Uses LogisticRegression to predict the categories (bio or non-bio) of the test URLs given the test dataset. Finally, the Scraper module scrapes these classified bio URLs and saves the contents of each bio URL to a new file under ExpertSearch/data/compiled_bios. 2.2. Topic Mining 2.2.1. Inputs: o Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) 2.2.2. Outputs: o Trained topic model (lda_mallet_model) o Bag-of-words representation of corpus to be used with miner.py (corpus_dictionary) o Text representation of corpus (lda_corpus) 2.2.3. Deliverables: o Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) 2.2.4. Component design / Code workflow: Topic Miner (miner.py) The topic miner pulls a topic distribution from a document already mined if the model was trained on the document, otherwise it uses gensim and mallet to create a model from the entire corpus. The process is described below: Corpus preparation Y= Read in compiled bios as strings Y= Filter the string representation of each bio to: o Remove stop words o Extract HTML tags and elements o Strip non-alphanumeric characters 13 o Strip numbers o Strip words that exist in lists of terms extracted from the bios o Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) o Remove words shorter than four characters o Split all words into a list of tokens Y= Create list of documents which is comprised of lists of tokens for each document as described above Y= Append bigrams and trigrams to each token list for each document Y= Create a gensim dictionary from the above documents Y= Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below (Fig. 4): Y= Create a general model with gensim.models.ldamodel.LdaModel class with 10 models Y= Visually inspect term clusters to ensure they were meaningful Y= If the above criteria were not satisfactory: o Tweak corpus construction Y= After the above criteria was deemed satisfactory: o Using gensim.models.wrappers.LdaMallet with the mallet library, I: a. Varied number of topics to create new model b. Assessed coherence of each model with varying number of topics c. Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains 14 d. Chose the best model according to above criteria and saved it and the created dictionary for query inference Fig. 4: Workflow for building an optimal topic model. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with 15 that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query as shown below (Fig. 5). Fig. 5: Extracting topics from documents not included in training set. 16 2.3. Improved Email Extraction 2.3.1. Inputs: o Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) 2.3.2. Outputs: o Extracted emails for the faculties 2.3.3. Deliverables: o Updated email extractor (extract-email.py) 2.3.4. Component design / Code workflow: Regex Improvement: o There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting o Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu 2.4. UI Improvements 2.4.1. Inputs: o Query terms in the search box o Topics mined by the topic miner 2.4.2. Outputs: o Updated UI showing top-5 research topic per faculty o Updated UI showing topic cloud o Clickable topic terms for query refinement o Prepopulated email template on-click email icon 2.4.3. Deliverables: o Updated server endpoints (server.py) o Updated UI (index.js) 17 2.4.4. Component design / Code workflow: Info Button: o Information button is created at the top of each of the retrieved faculty. o When the button is clicked there a table pops up that appears below the selected retrieved faculty o The table will contain additional information regarding the research topics that the faculty does o When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: o Display the top 5 topics from the preview for each of the faculty. o Display these topics in a table format when the information button is clicked o Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. o Clicking on the ""Add to Query"" button will refine the current search query to include this topic. Email Automation: o Email comes pre-populated with a set subject and body. o The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. 3. Usage Details The modified project has been tested on Mac and Windows with Python 2.7. Here are the setup instructions for each of these platforms. 3.1. Setup Guide (Mac) 3.1.1. Repo setup Run the following command on a terminal to clone the github repository. 18 $ git clone https://github.com/sairanga123/CourseProject.git The directory structure of the project is as below (listing only the files/folders relevant to this project): CourseProject |__________ ExpertSearch |__________ AutoScraper | |__________ data | |__________ auto_scraper.py | |__________ data_handler.py | |__________ scraper.py | |__________ text_classifier.py | |__________ d2v.model | |__________ d2v-bio.model | |__________ data | |__________ compiled_bios | |__________ expertsearch | |__________ mallet-2.0.8 | |__________ model_files | |__________ corpus_dictionary | |__________ lda_mallet_model | |__________ lda_corpus | |__________ miner.py | |__________ extraction |__________ mallet-2.0.8 |__________ static |__________ server.py 3.1.2. Project environment setup The project has been tested on python 2.7. Please setup a python 2.7 environment for running the project. Creating an environment from Anaconda will make many common packages available. So a quick way to start would be to setup a python 2.7 environment from Anaconda. 19 May need to install many or all of the following python packages depending on what packages the python environment already has. - gunicorn=19.10.0 - flask=1.1.2 - metapy=0.2.13 - requests=2.25.0 - pytoml=0.1.21 - gensim=3.8.3 - nltk=3.4 - bs4=0.0.1 - lxml=4.6.2 - numpy=1.16.6 - sklearn=0.0 3.2. Setup Guide (Windows) Windows is currently not supported. If you want to build in Windows, use Windows Subsystem for Linux and follow the steps above. 3.3. Usage Guide 3.3.1. Running the Automated Scraper Run the AutoScraper to generate the bio documents. This step has been already performed and the generated bio documents have already been added to the ExpertSearch/data/compiled_bios folder. Here are the instructions for running the AutoScraper if it needs to be run again with additional input. To run the automated scraper, first go the ExpertSearch/AutoScraper directory. Then the Automated Scraper can be invoked as follows: 20 -d option specifies to generate/regenerate the train and test dataset. If -d switch is used: o If the dataset already exists, it will be regenerated o If the dataset doesn't yet exist, it will be generated If -d switch is not used: o If the dataset already exists, the existing dataset will be used in the subsequent flow o If the dataset doesn't yet exist, it will be generated even if -d switch is not used -t option specifies to train/retrain the Doc2Vec model on the train dataset. If -t switch is used: o If a trained and saved model already exists, the model will be retrained and saved again o If a trained and saved model doesn't yet exist, it will be trained and saved If -t switch is not used: o If a trained and saved model already exists, the saved model will be loaded and used for inference in the subsequent flow o If a trained and saved model doesn't yet exist, it will be trained and saved even if -t switch is not used 3.3.2. Running the Topic Miner The steps for creating the topic model are documented in ExpertSearch/data/expertsearch/LDATopicModeling.ipynb. The model construction is not something that can necessarily be automated because relying on perplexity and coherence scores alone often results in topics that don't make any meaningful sense to a human. Once the topic model is constructed and saved, miner.py allows the server to load the model and make inferences. 3.3.3. Running the Backend Server Once, the topic model has been built, we can start the server. To start the server, go to the ExpertSearch folder. Then run the following command: $ gunicorn server:app -b 127.0.0.1:8095 21 3.3.4. Running Faculty Search from the UI Now, launch a web browser and type the following URL: localhost:8095 3.4. Example Use Cases: 3.4.1. Use Case 1 - Basic use to search faculties Let's assume that we want to find the faculties that are working on ""text mining"". Then we'd go to the UI and enter our search string as ""text mining"". The existing system would retrieve the top ranked faculty results working on ""text mining"". 3.4.2. Use Case 2 - Find research interests of the faculty Now, the improved system will also provide an info button which will bring up an additional table of information for each faculty. This table shows the top 5 research topics the faculty is associated with. 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results We now maybe interested in learning more about who are the faculties that are working on any of these research topics. We can quickly search for all the faculties working on this new research topic by simply clicking on the ""Add to Query"" button for that research topic. This will automatically modify our search query by including that new research topic without having to type it in the search box. The retrieved faculty results will show the list of faculties working on that research topic. 3.4.4. Use Case 4 - Connecting to faculty Another way we could use the system is to click on the email icon to send an email to the faculty's email address. While we may be at a loss of words for that first email, the system will provide a pre-populated template email which will automatically address the faculty's name and also include reference to the faculty's research area. This will make connecting to an expert faculty just one click away. 22 4. Contributions Item Sub-items Contributor Automated Scraping Y= Automated Scraper to automate the complete process Y= Data Handler to prepare the datasets for the text classification tasks Y= Scraper to scrape the URLs Y= Text Classifier to classify directory and bio URLs Y= Function to generate bio documents and add to compiled bios Mriganka Sarma Topic Mining Y= Topic model Y= Function to return top-10 words associated with query topic Zacharia Rupp Improved Email Extraction Y= Added regular expressions to extract emails with atypical forms (e.g. person at place dot com) Sai Ranganathan Zacharia Rupp Improved UI Y= Display top 5 topics associated with each faculty member Y= Display cloud of topics Y= Pre-populate email field when clicked on email address Y= Improve email extraction part 1. Sai Ranganathan Improved ExpertSearch Project Source Code The project's source code is available under the ExpertSearch folder. Project Documentation The project's detailed report is available as: ""Project Report - Improved ExpertSearch.pdf"" The project's tutorial presentation video is available as: ""Video Presentation - Improved ExpertSearch.mp4"" PLEASE READ ""Project Report - Improved ExpertSearch.pdf"" FOR THE REPORT PLEASE VIEW ""Video Presentation - Improved ExpertSearch.mp4"" FOR THE PRESENTATION AND DEMO Improved ExpertSearch Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Captain: Sai Ranganathan (sr50) Mriganka Sarma (ms76) Zacharia Rupp (zrupp2) What system have you chosen? Which subtopic(s) under the system? We have chosen to improve ExpertSearch. Briefly describe the datasets, algorithms or techniques you plan to use Datasets: Faculty dataset scraped from MP2.1 for positive examples. Scrape of Alexa Top 500 Domains for negative examples. Techniques: Topic Mining If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Additional functionality: Topic Mining If it works, users will be able to refine queries by selecting topics from a topic cloud Impact on ndcg@k Improved functionality: Email extraction If it works, more faculty members will have email addresses associated with them. Improved UI: More granular query refinement Top-k associated topics listed under individual faculty members How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Our code will build on the ExpertSearch code by: adding a topic mining function improving email extraction automating scraping process improving UI Which programming language do you plan to use? Python JavaScript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main tasks: Automatic crawler to identify faculty directory pages (10+ hrs) Automatic crawler to identify faculty webpage URLS (10+ hrs) Improving functionality: Email extraction (10+ hrs) Adding functionality: Topic mining (10+ hrs) UI Improvements: Query refinement options (10+ hrs) Topic cloud from mined topics associated with retrieved faculty members. Top-5 topics associated with faculty member (5+ hrs) Displayed at the top of the bio excerpt Prepopulated email content when a user clicks on a faculty member's email address (5+ hrs) E.g. ""Dear Faculty Name, It's a pleasure to have gone through some of your research articles. I'd like to connect with you for discussing some ideas in the Research Area. I hope to hear from you soon."" 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members.  Team Captain: Sai Ranganathan (sr50)  Mriganka Sarma (ms76)  Zacharia Rupp (zrupp2) 2. What system have you chosen? Which subtopic(s) under the system?  We have chosen to improve ExpertSearch. 3. Briefly describe the datasets, algorithms or techniques you plan to use  Datasets:  Faculty dataset scraped from MP2.1 for positive examples.  Scrape of Alexa Top 500 Domains for negative examples.  Techniques:  Topic Mining 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better?  Additional functionality:  Topic Mining - If it works, users will be able to refine queries by selecting topics from a topic cloud o Impact on ndcg@k  Improved functionality:  Email extraction - If it works, more faculty members will have email addresses associated with them.  Improved UI:  More granular query refinement  Top-k associated topics listed under individual faculty members 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly  Our code will build on the ExpertSearch code by:  adding a topic mining function  improving email extraction  automating scraping process  improving UI 6. Which programming language do you plan to use?  Python  JavaScript 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task.  Main tasks:  Automatic crawler to identify faculty directory pages (10+ hrs)  Automatic crawler to identify faculty webpage URLS (10+ hrs)  Improving functionality: - Email extraction (10+ hrs)  Adding functionality: - Topic mining (10+ hrs)  UI Improvements: - Query refinement options (10+ hrs) o Topic cloud from mined topics associated with retrieved faculty members. - Top-5 topics associated with faculty member (5+ hrs) o Displayed at the top of the bio excerpt - Prepopulated email content when a user clicks on a faculty member's email address (5+ hrs) o E.g. ""Dear <Faculty Name>, It's a pleasure to have gone through some of your research articles. I'd like to connect with you for discussing some ideas in the <Research Area>. I hope to hear from you soon."" Improved Expert Search Team ZMS (Zacharia Rupp, Mriganka Sarma, Sai Ranganathan) Z M S Z M S Introduction What is ExpertSearch System The System As It Is Z M S The System As It Is Z M S Challenges of Current System Manual Process for Directory URL Identification Manual Process for Faculty Webpage Identification Need for Structuring of Faculty Information Need for Improved Extraction Methods Z M S Proposed Improvements Automatic Scraping of Faculty Directory and Faculty Webpages Mining Top Research Topics of Faculty Improved Email Extraction Improved UI for Better Visualization of Structured Data Z M S Setup Instructions Clone the project repository $ git clone https://github.com/sairanga123/CourseProject.git Create Anaconda python 2.7 environment Install python packages gensim nltk gunicorn bs4 metapy Z M S Usage Instructions Main Functional Categories: Automatic Scraping Topic Mining Faculty Search in UI Z M S Usage Instructions (contd.) Automatic Scraping $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t Z M S Usage Instructions (contd.) Topic Miner Work in jupyter notebook (ExpertSearch/data/expertsearch/LDATopicModeling.ipynb) Build corpus and dictionary, and create topic model Manually inspect topics with high coherence Check against known document Save best topic model, corpus, dictionary server.py accesses model with miner.py Z M S Usage Instructions (contd.) Email Extraction $ cd ExpertSearch/extraction $ python extract_email.py Different Email Formats To Cover: Eugene dot agichten at emory dot edu Rohini [@] buffalo [DOT] edu Z M S Usage Instructions (contd.) Running the Backend Server $ gunicorn server:app -b 127.0.0.1:8095 Running Faculty Search from the UI localhost:8095 Z M S Example Usage Z M S Example Usage (contd.) Z M S Example Usage (contd.) Z M S Thank You Z M S Z M S"
https://github.com/sajidws/CourseProject	"1. Executive summary Wasique Ahmad and Sajid Shaikh formed the Data Miners team. We chose the project to enhance the Educational Web system as we saw a lot of potential in it for UIUC students. We observed the opportunity to make several changes to make the application more useful, along with adding material from more courses thus expanding the scope and reach of the application. 2. Functionality completed The Data Miners team managed to complete the following enhancements to the Educational Web system. The source code is located at: https://github.com/sajidws/CourseProject (branch: master) a) Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly o Each entry used to start with something like '02 Week 1 02 Week 1 Lesson 01 Lesson 1...'. Removed the redundant parts and made it more intuitive b) Make the slide material from another course (CS 425: Distributed Systems) available to the users so that users can expand their learning to other courses o As part of this story, added 'CS 425' in the 'Courses' dropdown list c) Improve presentation of information the following screens to make the application more user-friendly: o Add week number, lecture number and topic to the current slide so that the user knows which week and lecture covers the current slide. o Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed. o Add miscellaneous UX improvements such as capitalize 'CS' (for Computer Science) wherever it appeared. d) Add a home / landing page to the application that lists the courses available to that users as soon as they 'arrive' in the application. e) Allow users to go back to the home / landing page from any pages so that users can navigate easily between different parts of the application. Sajid Shaikh completed the user stories a, b and c. Wasique Ahmad completed the user stories d and e. Detailed instructions on how to deploy and run the application have been updated in the project 'README.md', including updates for Windows 10 platform. 3. Breakdown of the tasks Here's a breakdown of the amount of effort spent by the team in various activities of the project, including comparison with the original estimates. # Task Original estimate (Hrs) Actual effort (Hrs) Status 1 Writing proposal, user stories 2 2 Done 2 Set up and build the code 6 20 Done 3 Understanding the existing system and design for the scope of our project 6 14 Done Design of enhancements 8 8 Done Development, code reviews and unit-testing 20 30 Done System / QA verification 6 5 Done Documentation 8 10 Done Demonstration preparation 4 5 Done Communication 5 Done Total 60 99 4. Notes and Experiences As can be seen from above, we spent far more time than we had estimated. One of the primary reasons for this turned out that the project had previously been tested only on Linux and MacOS, but not on Windows and the Data Miners team was more familiar on Windows platform. We decided to take up the challenge anyway to set up and deliver the project on Windows 10. Expectedly, we faced many hurdles as we were in uncharted territory, but with help from each other, course staff and a bit of luck, managed to complete the deployment of the project on Windows 10. We have updated the project README with detailed instructions (including various package versions, paths, etc.) so that future students have an easier time. Even after deploying though, some of the functionality such as Explanations and Search didn't work as expected. We started to fix those, ran into more issues (e.g. after fixing the URL for explanations, the current ranking function hung). Rather than spend more time in what could have been a long and unknown effort, we decided to focus on implementing the functionality we had planned. There were some other missteps along the way (e.g. tried to change the directory names to improve the lecture name entries in the dropdown, but that broke other functionality), but each time we learned something new and continued forward. It's not unusual in any software project to have new discoveries during execution and our experience was no different. The above experience resulted in spending much more time than we had planned but it turned out to be a learning experience and we're glad if our efforts could simplify the life of future students. 5. User story details In this section, we will dive into the details of each user story implemented, including the technical portions as well as UI screen shots of the application. A majority of the changes made to the code were in the HTML templates (*.html) and Python files, especially model.py and app.py. Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly Initially, we tried to simplify the directory names as that allowed us more control over the names that could be presented to the user. We then realized there are many other dependencies on the names of the directories (e.g. Related Slides weren't appearing). Ultimately, we changed the manner in which 'lec_names' are displayed in the UI by changing 'slide.html'. The highlight of the change was: Line 7: <li><a href=""{{base_url}}/slide/{{course_name}}/{{i}}"">{{' '.join(lec_names[i].split('_')[3].split('-')).title()}}</a></li> Trimmed entries, easy to read and see fully! Make the slide material from another course (CS 425: Distributed Systems) available to the users We initially thought just adding a folder such as 'cs-425' in 'static/slides' directory will do the job. However, that didn't work and we had to make changes in 'model.py' to remove hard-coded checks of 'cs-410'. Ultimately, this turned out to be a time-consuming activity. The PDF files from the CS 425 Distributed Systems lectures had to be split into individual pages, each named in a certain format. We have packaged a new archive 'cs-425.zip' and included its location and instructions to deploy in the README file. cs425.zip is available here: https://drive.google.com/file/d/1IWxuYF1fGHlU1VZn5xfXfyCJxUV-sIRI/view?usp=sharing CS 425 (Distributed Systems) is now available via Educational Web! The entire CS 425 course material! Add week number, lecture number and title to the current slide so that the user knows which week, lecture and topic covers the current slide While this change was relatively straightforward as we had to extract the week and lesson number by making changes in 'slide.html', it adds quite some value to the user to see which topic the current slide is covered under and to quickly determine the week and lecture number. Week, Lecture # and Topic makes it easy to see where you are! Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed The changes made here were similar by extracting the week and lecture number in 'slide.html' Add miscellaneous UX improvements such as capitalized 'CS' (Computer Science) A relatively easy change by changing 'title()' to 'upper()' in multiple places where 'CS' is going to be displayed. Easy to see week and lecture number in 'Related slides' 'CS' appears better than 'Cs' everywhere Adding landing page This is will be the landing page. List of the courses offered by EducationalWeb application will be shown here. The file 'LandingPage.html' was added in the 'templates' folder to implement the above functionality. This page extends 'base.html'. In order to load this page when the user visits the base URL ('/'), 'app.py' was modified to render 'LandingPage.html' instead of 'base.html'. This will be shown as a 1st slide after selecting course 'CS 410' from the above page. Clicking on 'Educational Web' in the top bar from any page will bring users back to the landing page. In order to implement the above change, 'base.html' was modified to change the link for 'Educational Web' to '/' instead of '#'. Team name: Data Miners Team Composition What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Wasique Ahmad: wasique2 Sajid Shaikh: sajidas2 (captain) System and Topics What system have you chosen? Which subtopic(s) under the system? Educational Web System: improving the usability and reach of the existing system. -Trim the entries in the 'Lectures' dropdown. Each line starts with something like '02 Week 1 02 Week 1 Lesson 01 Lesson 1...'. Remove the redundant parts -Add material from a couple of other courses such as CS 425: Distributed Systems, CS 445 Computational Photography, etc. with all current functionality applicable (Related slides, Download, Explanation, ...). Adding crawling or adding more courses can be considered after improving the performance of the system. -Add a home / landing page that lists the courses included in this system. Add a link to the home page so users can easily navigate back to the home page. -Improve presentation on a few screens -Add week / lecture number to 'Related slides' -Add week / lecture number to 'Search results' -Add week / lecture number to current slide shown -Bulk download -Download entire lecture slide and video -Download entire week slides and video -Download entire course slides and video (multiple files separated by each week) -Explanations: add more context and improve the presentation Datasets and Algorithms Briefly describe the datasets, algorithms or techniques you plan to use Datasets: training slides for courses such as Distributed Systems, Computational Photography, ... Algorithms and Techniques: we will explore, understand and build on the existing algorithms and techniques that are currently employed in the system. What is the function of the tool The Educational Web system is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and 2) recommend relevant slides for each slide. Who will benefit from such improvement Students, Educators, Researchers etc. @ University of Illinois Demonstration If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Demonstrate live using the  http://timan102.cs.illinois.edu/  system Code interaction with system How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will add code and data sets to the existing system to enhance the functionality and usability described above. Programming language Which programming language do you plan to use? Python Work breakdown structure Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. # Hours Task 1 2 Proposal, Writing user stories 2 6 Set up and build the code 3 6 Understanding the existing system and design 4 8 Design of enhancements 5 20 Development, code reviews and unit-testing 6 6 Testing: Verification, Load testing 7 8 Documentation 8 4 Demonstration preparations 9 4 Presentation Total 64 Interim Progress Report as of 11/29/2020 The Data Miners team has completed writing the user stories and deployed the code in their respective local systems. Based on the current understanding, design of the enhancements has started, along with some code changes and unit-testing. Because the team is using a Windows 10 environment (and the current application has been tested on Linux and MacOS so far), we encountered unforeseen issues and the amount of time spent is expected to be more than estimated for most tasks. The table below illustrates all the planned tasks, the progress made thus far and the estimated remaining work (NOTE: WIP stands for Work In Progress). All the user stories are listed below the table on the following pages. # Task Original Estimate (Hrs) Time so far (Hrs) Remaining Estimate (Hrs) Progress Status 1 Proposal, Writing user stories 2 2 0 Done 2 Set up and build the code 6 15 0 Done 3 Understanding the existing system and design 6 5 15 WIP 4 Design of enhancements 8 1 10 WIP 5 Development, code reviews and unit-testing 20 1 30 WIP 6 Testing: Verification, Load testing 6 10 Not started 7 Documentation 8 1 10 WIP 8 Demonstration preparations 4 5 Not started 9 Presentation 4 5 Not started Total 64 25 85 The audience of the following user stories is MCS students at UIUC. User stories -Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly. -Add week number and lecture number to the current slide so that the user knows which week and lecture covers the current slide. -Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed. -Add week number and lecture number to the list of slides shown in the 'Search Results' section so that the user knows which week and lecture covers each slide listed. -Allow users to download all the slides of the current lecture (instead of only the current slide), so that they do not have to download each slide individually for the lecture. Build this incrementally as follows: -Download each page of the lecture as separate PDF files. -Allow users to specify a different lecture number (defaulting to the current lecture) so that they have more flexibility of which lecture's materials to download. -Allow users to specify whether all the pages of the lecture should be downloaded as a single PDF file. -Allow users to download all the slides of the current week (instead of only the current slide), so that they do not have to download each slide individually for the week. Build this incrementally as follows: -Download each page of the week as separate PDF files. -Allow users to specify a different week number (defaulting to the current week) so that they have more flexibility of which week's materials to download. -Allow users to specify whether all the pages of the week should be downloaded as a single PDF file. -Allow users to download the video of the current lecture -Allow users to specify a different lecture for downloading a video, so that they have more flexibility of which lecture's video file to download. -Allow users to download the video of the current week as separate video files. -Allow users to specify a different week for downloading a video, so that they have more flexibility of which week's video files to download. -Do not allow users to download the video files of the entire course as it may hog the bandwidth unnecessarily and this functionality can be achieved by downloading each week's video files separately if really required. -Improve the Explanations by adding more context (such as week and lecture numbers) and improve the presentation (such as paragraphs) to so that the content is more usable and easier to understand for the audience. -Make the slides from another course (CS 425: Distributed Systems) available to the users (functionally at par with CS 410) so that users can expand their learning to other courses -As part of this story, add 'CS 425' in the 'Courses' dropdown list -Add a home / landing page to the application that lists the courses available so that users are aware of which courses are made available with this functionality. -Add a link back to the home / landing page from all pages so that users can navigate easily between different parts of the application. Challenges during setup I encountered a few challenges during setup. - In case of Gulp, I had to keep Path in a certain order. C:\Users\<NAME>\AppData\Roaming\npm\node_modules\ C:\Users\<NAME>\AppData\Roaming\npm - In case of Numpy, Numpy 1.19.4 was not compatible . So I uninstalled 1.19.4 and installed Numpy 1.19.3. CourseProject The project documentation is in the file 'CS410-Project-Documentation-Data-Miners.pdf'. The demo recording links for media space is in the text file 'Demo-Recording-Link-DataMiners-Educational-Web-Enhancements.txt' The other two documents are from previous milestones: project proposal and progress report The source code for this project is in the 'master' branch."
https://github.com/samphadnis/TeamTextDragons	"Progress Report: Text Classification Competition: Twitter Sarcasm Detection Group Name: Text Dragons Team members: Chen Yuan (cheny9), Email: cheny9@illinois.edu Sameer Phadnis (phadnis3) -  Team Leader,  Email: phadnis3@illinois.edu Abhishek Shinde (ashinde2), Email: ashinde2@illinois.edu Overview: We will be joining the text classification competition. The object of the competition is to identify sarcasm from a set of Twitter responses. The given dataset is split into two: the train dataset (5000 observations) and the test dataset (1800 observations). For the training dataset, we are given the response (which is the Tweet to be classified), the context (which is the conversation of the context) and the label. We will be using the training dataset to build the models and making predictions based on the test set. Tasks Completed: So far we have outperformed the baseline model with only a few attempts. Models we have tried so far: 1.Fasttext (did not outperform the baseline) 2.RoBERTa + simple neural networks model (outperforms the baseline) Tasks Pending: 1.Writing a documentation for our models 2.Organize and submit code Challenges: The challenge about computation power as indicated in the project proposal was addressed by using google colab. Therefore, there are no remaining challenges. 1.Our project team members are Chen Yuan (cheny9), Sameer Phadnis (phadnis3), and Abhishek Shinde (ashinde2). Team name is ""Text Dragons"" and Sameer Phadnis is team captain. 2.We plan to join the text classification competition. 3.Yes, we are prepared to learn state-of-the-art classifiers. Some popular neural classifiers consist of LeNet, AlexNet, and GoogLeNet. For a computer vision project, Abhishek's team utilized a modified LeNet convolutional neural network to classify images of sign language into the letters of the alphabet.We may also explore the below  neural classifiers and deep learning frameworks for the project: Feed Forward Neural networks: *Deep Average Network *fastText RNN (Recurrent Neural Network) based models: *Tree-LSTM *Multi-Timescale LSTM CNN (Convolutional Neural Network) based models: *Dynamic CNN Capsule Neural Network: *CapsNET Transformers: *BERT 4.We will use Python as the programming language for the project Project Proposal: Text Classification Competition: Twitter Sarcasm Detection Group Name: Text Dragons Team members: Chen Yuan (cheny9), Email: cheny9@illinois.edu Sameer Phadnis (phadnis3) -  Team Leader,  Email: phadnis3@illinois.edu Abhishek Shinde (ashinde2), Email: ashinde2@illinois.edu Overview: We will be joining the text classification competition. The object of the competition is to identify sarcasm from a set of Twitter responses. The given dataset is split into two: the train dataset (5000 observations) and the test dataset (1800 observations). For the training dataset, we are given the response (which is the Tweet to be classified), the context (which is the conversation of the context) and the label. We will be using the training dataset to build the models and making predictions based on the test set. Objective: We will be using  Python  as the main programming language for the project. We will try different models with state-of-the-art classifiers. Our potential candidates includes: 1.LeNet 2.AlexNet 3.GoogLeNet 4.ResNet 5.LSTM 6.Fasttext Abhishek's team has had previous experience on a computer vision project, which utilized a modified LeNet convolutional neural network to classify images of sign language into the letters of the alphabet. Chen has had previous experience building Wide-ResNet models to classify street view house numbers. Challenges: There are several challenges that we need to address throughout our project: 1.The Tweets may contain emojis and spam information that we need to deal with at the data preprocessing step. 2.The power of a CPU may not be sufficient to train deep neural network models. Therefore, we will need to leverage some cloud computing resources, such as google colab. Documentation for Text Classification Project I. Overview This program implements a model for detecting sarcasm in text. The training data consists of twitter feeds having context text, where the response to the context is labelled as sarcasm or not sarcasm. The objective is to define a model based on this training data that detects sarcasm in text, and use it to classify the responses in the test data as sarcasm or not sarcasm. II. Data Profiling We conducted data profiling on both the training data and test data. The datasets provided are in json format. The training data contains the following columns: *Label : Indicates whether the response is sarcasm or not sarcasm. *Response : A string which contains the Tweet response to be classified. *Context : A list which contains the conversation history in context of response. The order of the list is the same as the order of the dialogue (i.e. response directly replies to the last element of the list) The training dataset contains 5000 Tweets in total and 2500 are labeled as sarcasm and the rest are labeled not. Therefore, the training dataset is considered as a balanced dataset and no oversampling is required. We also conducted analysis on the response text length on both the training set and the testing set. The distribution shows that the training set and the testing set are similar in terms of response length distribution, and therefore, models based on the training set might be suitable for predicting the testing set. In addition to response length, we also conducted an analysis on the number of contexts. The distribution shows that for both the training set and the testing set, the majority of the responses have two corresponding contexts. Based on our calculation, the average number for contexts for the training set is 3.87 and for the testing set is 3.16. During our data preprocessing step, we figured that keeping all contexts will draw a warning message which indicates that ""Token indices sequence length is longer than the specified maximum sequence length for this model"". Therefore, we decided to use the last three available contexts. Percentile Characters(Train) Characters(Test) 0.1 71.0 59.0 0.2 85.8 76.0 0.3 97.0 92.0 0.4 108.0 108.0 0.5 117.0 127.0 0.6 126.0 149.4 0.7 145.0 176.0 0.8 190.0 214.0 0.9 254.0 265.0 III. Data Cleaning The following approaches were tried to clean up the data to see if it improved computation efficiency and\or classification accuracy: 1.Convert all text to lowercase 2.Remove all punctuations (except apostrophe) 3.Remove stop words 4.Stemming of words A separate script  clean.py  was written to process the input data and write the cleaned up data in the jsonl format. However, we found that the cleaning of the data actually resulted in a loss of accuracy as compared to the original data. This could be because the cleanup process affected the sarcasm detection learning model. Therefore the decision to clean the data must be carefully weighed to see if it adversely impacts the learning model, and experimentation with\without cleaning is essential. IV. Model Architecture There are multiple ways to build contextual sentiment analysis models. Our final decision is to use the simple method: combine contexts with the response to form a single string as input to the model( Amardeep Kumar, 2020) . Throughout the process, we have considered the following models: 1.Fasttext 2.Roberta + 1-hidden layer neural network + ReLU 3.Roberta + 1-hidden layer neural network + Softmax 4.XLMRoberta + 1-hidden layer neural network + ReLU 5.Albert + 1-hidden layer neural network + ReLU We also tried different learning rates for these models. The result shows that model #2 combined with a learning rate of 1e-5 gives us the best performance on the testing set with a f1 score of 0.787. The model architecture is as below: V.   Implementation The program uses Google Colab + Jupyter notebook to take advantage of the Google GPU for accelerated data processing. The training and test data is stored on Google Drive. The PyTorch library is used, which enables the usage of GPU as well as the use of various implementations of state-of-the-art NLP transformer libraries i.e. Pre-trained Language Models (PLMs).Transformers allow for parallelization, which makes it possible to efficiently train very big models on large amounts of data on GPU clusters.Transformer-based PLMs use much deeper network architectures, and are pre-trained on much larger amounts of text corpora to learn contextual text representations by predicting words based on their context. One of the most popular transformers is BERT, developed by Google. RoBERTa (developed by Facebook) is an extension that is more robust than BERT, and is trained using much more training data and dynamic masking. To investigate a lighter model, we researched and tested out Albert, which is essentially a lite-BERT. The benefits for training are evident; it provides two parameter reduction techniques to improve memory usage and BERT speed. According to Hugging Face, these techniques are forming two smaller matrices from an embedding matrix and providing repeated layers split on groups. The experience with this consisted of beating the baseline, but not being one of our best models. The RoBERTa PyTorch transformer library is selected in our implementation as our top model. The data is loaded using a batch size of 16. The model is trained on the training data set using epoch count of 12. The resulting model is serialized and saved in the PyTorch model format. The test data is then evaluated against the model to classify the tweets as sarcasm\not sarcasm (stored in the generated answers.txt file). VI. Instructions to Run Code 1.In Colab Notebooks under Google Drive, create a directory called  TextClassification . 2.Within the  TextClassification  directory, create another directory called  data. 3.Include the data files ( test.jsonl  and  train.jsonl ) in this  data  directory. 4.From the GitHub repository ( https://github.com/samphadnis/TeamTextDragons ), open roberta_no_pretrain.ipyn b  from the  code  directory into Colab Notebooks. 5.Since the saved model is too large to be uploaded to Github, we created a link to google drive and shared it on Github readme page. Download the model and save it under the TextClassification  directory. 6.If you want to run the whole model, including the training portion, in the navbar, navigate to ""Runtime"" and select ""Run all"". The pipeline should run, and the results  answer.txt should be generated under Colab Notebooks. 7.If you only want to generate the results with the saved model, you may skip the  Model Training  portion of the code. VII.   Conclusion The program is able to beat the baseline. The state-of-the-art Pre-trained Language Models such as RoBERTa are powerful tools for text classification. VII.   Team Contributions Sameer Phadnis ( phadnis3@illinois.edu ): Team Leader -Worked on and tested variations of data cleaning processes and documentation Chen Yuan ( cheny9@illinois.edu ) -Led the development work by setting up project infrastructure, implementing data profiling, classification pipeline, presentation and documentation Abhishek Shinde ( ashinde2@illinois.edu ) -Worked on training and testing various models, and improving the classification pipeline and documentation VIII. References -Kozlov, Alexander. ""Fine-Tuning BERT and RoBERTa for High Accuracy Text Classification in PyTorch.""  Medium , Towards Data Science, 7 Sept. 2020, towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646. -The code repository we relied on can be found here: https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20for%20text%20classification.ipynb -""Transformers.""  Transformers - Transformers 4.0.0 Documentation , huggingface.co/transformers/index.html. -Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov: ""RoBERTa: A Robustly Optimized BERT Pretraining Approach"", 2019;  http://arxiv.org/abs/1907.11692 arXiv:1907.11692 -Amardeep Kumar, Vivek Anand: ""Transformers on Sarcasm Detection with Context"", 2020;  https://www.aclweb.org/anthology/2020.figlang-1.13.pdf Contextual Twitter Sarcasm Detection CS 410: Text Information Systems Final Project This repository contains the project code, data, report and video presentation for text classification competition. Team members Sameer Phadnis: phadnis3@illinois.edu Abhishek Shinde: ashinde2@illinois.edu Chen Yuan:cheny9@illinois.edu Code environment We are running our code on the google colab platform. You will need: - Python 3.x - transformers - torch - pandas - sklearn All packages are pre-installed except transformers. Model Due to the limit of file size on Github, we are not able to upload our final saved model to the github repository. Therefore, we created a google drive link to share our final model: https://drive.google.com/file/d/1bFc4HK1N7pneVfDeVwESM3TehEK4o44w/view?usp=sharing Setup In Colab Notebooks under Google Drive, create a directory called TextClassification. Within the TextClassification directory, create another directory called data. Include the data files (test.jsonl and train.jsonl) in this data directory. From the GitHub repository (https://github.com/samphadnis/TeamTextDragons), open roberta_no_pretrain.ipynb from the code directory into Colab Notebooks. Since the saved model is too large to be uploaded to Github, we created a link to google drive and shared it on Github readme page. Download the model and save it under the TextClassification directory. If you want to run the whole model, including the training portion, in the navbar, navigate to ""Runtime"" and select ""Run all"". The pipeline should run, and the results answer.txt should be generated under Colab Notebooks. If you only want to generate the results with the saved model, you may skip the Model Training portion of the code. The subfolders should be organized as below: . +-- ... +-- Colab Notebooks | +-- TextClassification | | +-- data | | | +--train.jsonl | | | +--test.jsonl | | +-- roberta_no_pretrain.ipynb | | +-- model_RoBERTa_relu_nopretrain.pkl | +-- +-- Presentation We created a video presentation to walk through the code and show how to reproduce the results. The video can be found: https://mediaspace.illinois.edu/media/1_79uj7ghe. File Description Reports: CS410_ Project Proposal_TextDragons.pdf - Project proposal CS410 Project Progress Report.pdf - Project progress report Code: code/Profiling.ipynb - Pre-modeling analysis code/roberta_no_pretrain.ipynb - Our best model, which uses roberta + relu code/roberta_no_pretrain_softmax.ipynb - roberta + softmax code/xlmroberta_no_pretrain.ipynb - xlmroberta + relu code/albert.ipynb - albert + relu code/Fasttext.ipynb - Fast text model code/clean.py - Data preprocessing script Data: data/train.jsonl - labeled training data data/test.jsonl - unlabeled testing data Reference Kozlov, Alexander. ""Fine-Tuning BERT and RoBERTa for High Accuracy Text Classification in PyTorch."" Medium, Towards Data Science, 7 Sept. 2020, towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646. - The code repository we relied on can be found here: https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20for%20text%20classification.ipynb ""Transformers."" Transformers - Transformers 4.0.0 Documentation, huggingface.co/transformers/index.html. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov: ""RoBERTa: A Robustly Optimized BERT Pretraining Approach"", 2019; http://arxiv.org/abs/1907.11692 arXiv:1907.11692 Amardeep Kumar, Vivek Anand: ""Transformers on Sarcasm Detection with Context"", 2020; https://www.aclweb.org/anthology/2020.figlang-1.13.pdf"
https://github.com/samvalenp/CourseProject	Project Documentation | Classification Competition This is an individual project. Samuel Valenzuela (samuelv4@illinois.edu) I followed two different approaches for the classification competition. One was using BERT as an embedding layer and the other was to train BERT and use it as a classifier. The second attempt was the one that performed better than the baseline and the one I document more in detail in the notebook with a clear structure. I also provide the notebook for the first approach but is not very well documented or easy to follow as I tried many different things and the code is a bit of a mess. To run the models I recommend to use Google Colab. I always used Google Colab for this project as it runs much faster than on my laptop. It also avoids having to install libraries or dealing with python environments. First approach: sam_disbert.ipynb Second approach: sarcasm_transformers.ipynb BERT as embedding layer (sam_disbert.ipynb) My first idea to solve the classification problem was to use a pre-trained model as an embedding layer that turns the text into a vector of numbers that can be used as features for a Machine Learning model. I use BERT to get high quality features out of the data. Because BERT has been already pre-trained with massive general datasets, it is useful for using it as an embedding layer that takes the tweets and outputs a vector of numbers that can be used as features. Here I list the different variations I tried: * BERT base and DistilBERT. I tried both models for the embedding layer. DistilBERT worked better. * Logistic Regression as ML model. It worked very well obtaining a 0.69 f1. * Linear SVC as ML model. It obtained around 0.66 f1. * Random Forest as ML model. It worked the best with a 0.712 f1. Very close to the baseline. * Convolutional Neural Networks. I tried many different networks with different number of layers and neurons. It worked also well but didn't get a higher f1 than Random Forest. * Added a subset of data from Ghosh dataset for training. This dataset has over 50,000 tweets. This didn't help at all and the f1 was lower. Maybe the type of sarcasm is different in some way in each dataset. DistilBERT ML model Input text feature vector Sarcasm/ not sarcasm Fine tune transformers (sarcasm_transformers.ipynb) My second attempt was about actually training pre-trained models and using it as a classifier instead of an embedding layer. Here I only use the transformer and not an additional ML model. I tried different transformers provided from the SimpleTransformers library including BERT, DistilBERT, XLM, XLNet and RoBERTa. Most of them performed better than the baseline. The one that showed better and more consistent results is RoBERTa with 0.74 f1 score. Project Progress Report | Classification Competition Which tasks have been completed? * Research about the problem: I have completed a research about NLP and classification problems. I am not new to Machine Learning but I am new to NLP, so I had to do a deep research on the current technologies used for classification problems and for problems similar to tweets classification and sarcasm detection. * Find possible solutions: I research articles, githubs and papers on models that solve similar problems to the classification competition. I deeply went through three or four that I could base my models from. * Implement models: I implemented some models based on the ones I found during my research. I applied them to the classification problem and found a decent result on the train data partitioned in train and test. Which tasks are pending? * Surpass the baseline in ranking: my F1 score is 0.678 and the baseline is 0.723. Are you facing any challenges? * Overwhelming start: I already went through the challenge of starting and ramping up. At the begining I had very little experience on using classification models for NLP. To overcome this challenge I did an extensive research on the current state of the art and I learnt in detail some of the most popular algorithms. * Surpass the baseline in ranking: my F1 score is 0.678 and the baseline is 0.723. CourseProject Project proposal Members: samuelv4 I would like to join the text classification competition. I am prepared to learn state-of-the-art neural network classifiers. I have worked in a NN competition before and I would love to do it again. I haven't worked a lot on text classification but I have worked with LSTM networks which might help me to ramp up and learn the state-of-the-art for the competition. I am planning on using python and known libraries like TensorFlow and Keras.
https://github.com/savigovindarajan/CourseProject	"GeneratingSemanticAnnotationsforFrequentPatternswithContextAnalysisQiaozhuMei,DongXin,HongCheng,JiaweiHan,ChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbanaChampaignUrbana,IL61801fqmei2,dongxin,hcheng3,hanj,czhaig@uiuc.eduABSTRACTAsafundamentaldataminingtask,frequentpatternmininghaswidespreadapplicationsinmanydi(r)erentdomains.Re-searchinfrequentpatternmininghassofarmostlyfocusedondevelopinge+-cientalgorithmstodiscovervariouskindsoffrequentpatterns,butlittleattentionhasbeenpaidtotheimportantnextstep{interpretingthediscoveredfrequentpatterns.Althoughsomerecentworkhasstudiedthecom-pressionandsummarizationoffrequentpatterns,thepro-posedtechniquescanonlyannotateafrequentpatternwithnon-semanticalinformation(e.g.support),whichprovidesonlylimitedhelpforausertounderstandthepatterns.Inthispaper,weproposethenovelproblemofgenerat-ingsemanticannotationsforfrequentpatterns.Thegoalistoannotateafrequentpatternwithin-depth,concise,andstructuredinformationthatcanbetterindicatethehiddenmeaningsofthepattern.Weproposeageneralapproachtogeneratesuchanannotationforafrequentpatternbycon-structingitscontextmodel,selectinginformativecontextindicators,andextractingrepresentativetransactionsandsemanticallysimilarpatterns.Thisgeneralapproachhaspo-tentiallymanyapplicationssuchasgeneratingadictionary-likedescriptionforapattern,-ndingsynonympatterns,discoveringsemanticrelations,andsummarizingsemanticclassesofasetoffrequentpatterns.Experimentsondi(r)er-entdatasetsshowthatourapproachise(r)ectiveingeneratingsemanticpatternannotations.CategoriesandSubjectDescriptors:H.2.8[DatabaseManagement]:DatabaseApplications-DataMiningGeneralTerms:AlgorithmsKeywords:frequentpattern,patternannotation,patterncontext,patternsemanticanalysis1.INTRODUCTIONWithitsbroadapplicationssuchasassociationrulemin-ing[2],correlationanalysis[4],classi-cation[6],andcluster-ing[19],discoveringfrequentpatternsfromlargedatabaseshasbeenacentralresearchtopicindataminingforyears.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.KDD'06,August20-23,2006,Philadelphia,Pennsylvania,USA.Copyright2006ACM1-59593-339-5/06/0008...$5.00.Varioustechniqueshavebeendevelopedforminingfrequentitemsets[2,8],sequentialpatterns[3],graphpatterns[22],etc.Thesetechniquescanusuallyoutputalarge,completesetoffrequentpatternse+-ciently,andprovidebasicstatis-ticinformationsuchassupportforeachpattern.However,theexcessivevolumeoftheoutputpatternsetandthelackofcontextinformationhasmadeitdi+-culttointerpretandexplorethepatterns.Inmostcases,auseronlywantstoexploreasmallsetofmostinterestingpatterns,andbeforeexploringthem,tohavearoughideaabouttheirhiddenmeaningsorwhytheyareinteresting.Thisisanalogoustoliteraturereviewing.Beforedecidingwhethertoreadthroughapaper,areaderoftenwantsto-rstlookatashortsummaryofthemainideasofthepaper.Similarly,itisalsohighlydesirabletohavesuchasummaryforafre-quentpatterntoexplainorindicatethepotentialmeaningsofthepatternandtohelpauserdecidewhetherandhowtoexplorethepattern.Therefore,anewmajorchallengeinfrequentpatternmininghasbeenraisedbyresearchers,whichishowtopresentandinterpretthepatternsdiscov-ered,inordertosupporttheexplorationandanalysisofindividualpatterns.Tomeetthischallengeandfacilitatepatterninterpretation,weneedtoannotateeachfrequentpatternwithsemanticallyenriched,in-depthdescriptionsofthepatternanditsassociatedcontext.Researchershaveemployedconceptslikeclosedfrequentpattern[15],andmaximumfrequentpattern[16]toshrinkthesizeofoutputpatternsandprovidemoreinformationbeyond\support"".Recently,novelmethodshavebeenpro-posedeithertomineacompressedsetoffrequentpatterns[20]ortosummarizealargesetofpatternswiththemostrepresentativeones[21].Bothofthememployextrainfor-mationoffrequentpatternsbeyondthesimpleinformationofsupport,whichiseithertransactioncoverage[20]orpat-ternpro-les[21].Thesemethodscansuccessfullyreducethenumberofoutputpatternsandpresentonlythemostinter-estingonestotheuser.However,theinformationthatthesemethodsusetoannotateafrequentpattenisrestrictedtothemorphologicalinformationorsimplestatistics(e.g.sup-port,transactioncoverage,pro-le);fromsuchanannota-tion,userscouldnotinferthesemantics,orhiddenmeaningsofthepattern,thusstillhavetolookthroughallthedatatransactionsinwhichapatternoccursinorderto-gureoutwhetheritisworthexploring.Inthispaper,westudytheproblemofautomaticallygen-eratingsemanticannotationsforfrequentpatterns,bywhichwemeantoextractandprovideconciseandin-depthinfor-mationforafrequentpattern,whichindicatesthesemantics,orhiddenmeanings,ofthepattern.Whatisanappropriatesemanticannotationforafrequentpattern?Generally,thehiddenmeaningofapatterncanbeinferredfromthepatternswithsimilarmeanings,thedataobjectsco-occurringwithit,andthetransactionsinwhichthepatternappears.Inprinciple,weexpectsuchanannotationtobecompact,wellstructured,andindicativeofthemeaningsofthepattern.Thiscriterionisanalogoustodictionaryentries,whichannotateeachtermwithstructuredsemanticinformation.Example1:Anexampleofadictionaryentry1DictionaryTerm:\pattern""[""paet@n],noun,...de-nitions:1)aformormodelproposedforimitation2)anaturalorchancecon-guration3)...examplesentences:1)...adressmaker'spattern...2)...thepatternofevents...synonymorthesaurus:model,archetype,design,exemplar,motif,etcInExample1,weseethatinatypicaldictionaryentry,theannotationforatermisstructuredasfollows.First,somebasicnon-semanticinformationispresented.Second,agroupofde-nitionsaregiven,whichsuggeststhesemanticsoftheterm,followedbyseveralexamplesentences,whichshowtheusageofthistermincontext.Besides,asetofsynonyms,thesaurusorsemanticallysimilartermsarepre-sented,whichhavesimilarde-nitionswiththisterm.Analogically,ifwecanextractsimilartypesofsemanticin-formationforafrequentpatternandprovidesuchstructuredannotationstoauser,itwillbeveryhelpfulforhim/hertointerpretthemeaningsofthepatternandfurtherex-ploreit.Givenafrequentpattern,itistrivialtogeneratenon-semanticinformationsuchasbasicstatisticsandmor-phologicalinformation,sothemainchallengeistogeneratethesemanticdescriptionsofapattern,whichisthegoalofourwork.First,weshouldideallyprovideprecisesemanticde-nitionsforapatternlikethoseinadictionary.Unfortu-nately,thisisnotpracticalwithoutexpertiseofthedomain.Thusweopttolookforinformationthatcanindicatethesemanticsofafrequentpattern,whichpresumablycanhelpauserinfertheprecisesemantics.Ourideaisinspiredfromnaturallanguageprocessing,wherethesemanticsofawordcanbeinferredfromitscontext,andwordssharingsimilarcontextstendtobesemanticallysimilar[13].Speci-cally,byde-ningandanalyzingthecontextofapattern,wecan-ndstrongcontextindicatorsandusethemtorepresentthemeaningsofapattern.Second,wealsowanttoextractthedatatransactionsthatbestrepresentthemeaningsofthepattern,whichisanalogicaltotheexamplesentences.Finally,semanticallysimilarpatterns(SSPs)ofthegivenpattern,i.e.,patternswithsimilarcontextsastheoriginalpattern,canbeextractedandpresented.Thisissimilartothesynonymsorthesauriofatermindictionary.There-fore,anexampleofsemanticpatternannotation(SPA)canbeshownasfollows:Example2:Anexampleofannotatingafrequentpattern1TheexampleisselectedfromMerriam-Webster'sCollegiateDictionary&ThesaurusPattern:\frequentpattern""sequentialpattern;support=0.1%;closedcontextindicators:\mining"",\constraint"",\Apriori"",\FP-growth""\rakeshagrawal"",\jiaweihan"",...exampletransactions:1)miningfrequentpatternswithoutcandidate...2)...miningclosedfrequentgraphpatternssemanticallysimilarpatterns:\frequentsequentialpattern"",\graphpattern""\maximumpattern"",\frequentclosepattern"",...Theterm\frequentpattern""inthisexampleisitselfafre-quentitemset,orafrequentsequentialpatternintext.Thisdictionary-likeannotationprovidessemanticinformationre-latedto\frequentpattern"",consistingofitsstrongestcon-textindicators,themostrepresentativedatatransactions,andthemostsemanticallysimilarpatterns.Despiteitsimportance,tothebestofourknowledge,thesemanticannotationoffrequentpatternshasnotbeenwelladdressedinexistingwork.Inthiswork,wede-nethenovelproblemofgeneratingsemanticannotationsforfre-quentpatterns.Weproposeageneralapproachtoauto-maticallygeneratestructuredannotationsasshowninEx-ample2,by:1)formallyde-ningandmodelingthecontextofapattern;2)weightingcontextindicatorsbasedontheirstrengthtoindicatepatternsemantics;and3)rankingtrans-actionsandsemanticallysimilarpatternsbasedoncontextsimilarityanalysis.Empiricalexperimentsonthreedi(r)erentdatasetsshowthatouralgorithmise(r)ectiveforgeneratingsemanticpatternannotationsandcanbeappliedtovariousrealworldtasks.Thesemanticannotationsgeneratedbyouralgorithmhavepotentiallymanyotherapplications,suchasrankingpat-terns,categorizingandclusteringpatternswithsemantics,andsummarizingdatabases.Applicationsoftheproposedpatterncontextmodelandsemanticalanalysismethodarealsonotlimitedtopatternannotation;otherexampleappli-cationsincludepatterncompression,transactionclustering,patternrelationsdiscovery,andpatternsynonymdiscovery.Therestofthepaperisorganizedasfollows.InSection2,weformallyde-netheproblemofsemanticpatternannota-tionandaseriesofitsassociatedproblems.InSection3,weintroducehowthepatterncontextismodeledandinstanti-ated.PatternsemanticanalysisandannotationgenerationispresentedinSection4.WediscussourexperimentsandresultsinSection5,therelatedworkinSection6,andourconclusionsinSection7,respectively.2.PROBLEMFORMULATIONInthissection,weformallyde-netheproblemofsemanticpatternannotation(SPA).LetD=ft1;t2;:::;tngbeadatabasecontainingasetoftransactionsti,whichcanbeitemsets,sequences,orgraphs,etc.Letp(r)beapattern(e.g.,anitemset,asubsequence,orasubgraph)inDandPD=fp1;:::;plgbethesetofallsuchpatterns.Wedenotethesetoftransactionsinwhichp(r)appearsasD(r)=ftijp(r)2ti;ti2Dg.De-nition1(FrequentPattern):Apatternp(r)isfre-quentinadatasetD,ifjD(r)jjDj, 3/4 ,where 3/4 isauser-speci-edthresholdandjD(r)jjDjiscalledthesupportofp(r),usuallyde-notedass((r)).De-nition2(ContextUnit):GivenadatasetDandthesetoffrequentpatternsPD,acontextunitisabasicob-jectinDwhichcarriessemanticinformationandco-occurswithatleastonep(r)2PDinatleastonetransactionti2D.Thesetofallsuchcontextunitssatisfyingthisde-nitionisdenotedasUD.Withthisgeneralde-nition,acontextunitcanbeanitem,apattern,oratransactioninpractice,dependingonthespeci-ctaskanddata.De-nition3(PatternContext):GivenadatasetDandafrequentpatternp(r)2PD,thecontextofp(r),denotedasc((r)),isrepresentedbyaselectedsetofcontextunitsU(r)uUDsuchthateveryu2U(r)co-occurswithp(r).Eachselectedcontextunituisalsocalledacontextindicatorofp(r),associatedwithastrengthweightw(u;(r)),whichmeasureshowwellitindicatesthesemanticsofp(r).Thefollowingisanexampleofthecontextforanitem-setpatterninasmalldatasetwithonlytwotransactions.Thepossiblecontextunitsforthisdatasetaresingleitems,itemsetsandtransactions,andthecontextindicatorsoftheitemsetpatternareselectedfromthecontextunitsappear-ingwithitinthesametransactions.Example3:AnexampleofpatterncontextTransactions:t1=fdiaper,milk,babycarriage,babylotion,...gt2=fdigitalcamera,memorydisk,printer,...gContextUnits:1)items:diaper,milk,printer,...2)patterns:fdiaper,babylotiong,...3)transactions:t1,t2,...Anexemplaryfrequentpattern( 3/4 =0:5)p=fdiaper,milkgContextindicatorsofp:diaper,babycarriage,fmilk,babylotiong,t1,...Withthede-nitionsabove,wenowde-netheconceptofsemanticannotationforafrequentpatternandtherelated3subproblems.De-nition4(SemanticAnnotation):Letp(r)beafrequentpatterninadatasetD,U(r)bethesetofcontextindicatorsofp(r),andPbeasetofpatternsinD.Asemanticannotationofp(r)consistsof:1)asetofcontextindicatorsofp(r),I(r)uU(r),s.t.8u2I(r)and8u02U(r)!I(r),w(u0;(r))*w(u;(r));2)asetoftransactionsT(r)uD(r),s.t.8t2T(r)and8t02D(r)!T(r),tismoresimilartoc((r))thant0undersomesimilaritymeasure;and3)asetofpatternsP0uPs.t.8p2P0and8p02P!P0,c(p)isclosertoc((r))thanc(p0).De-nition5(ContextModeling):GivenadatasetDandasetofpossiblecontextunitsU,theproblemofContextModelingistoselectasubsetofcontextunitsU,de-neastrengthmeasurew(C/;(r))forcontextindicators,andconstructamodelofc((r))foreachgivenpatternp(r).De-nition6(TransactionExtraction):GivenadatasetD,theproblemofTransactionExtractionistode-neasim-ilaritymeasuresim(C/;c(C/))betweenatransactionandapat-terncontext,andtoextractasetofktransactionsT(r)uD(r)forfrequentpatternp(r),s.t.8t2T(r)and8t02D(r)!T(r),sim(t0;c((r)))*sim(t;c((r))).De-nition7(SemanticallySimilarPattern(SSP)Extraction):GivenadatasetDandasetofcandidatepat-ternsPc,theproblemofSemanticallySimilarPattern(SSP)Extractionistode-neasimilaritymeasuresim(c(C/);c(C/))be-tweenthecontextsoftwopatterns,andtoextractasetofkpatternsP0uPcforanyfrequentpatternp(r),s.t.8p2P0and8p02Pc!P0,sim(c(p0);c((r)))*sim(c(p);c((r))),wherec((r))isthecontextofp(r).Withthede-nitionsabove,wemayde-nethetaskofSemanticPatternAnnotation(SPA)asto:1)selectcontextunitsanddesignastrengthweightforeachunittomodelthecontextsoffrequentpatterns;2)designsimilaritymeasuresforthecontextsoftwopat-terns,andforatransactionandapatterncontext;3)foragivenfrequentpattern,extractthemostsigni-cantcontextindicators,representativetransactionsandsemanti-callysimilarpatternstoconstructastructuredannotation.Thisproblemischallenginginvariousaspects.First,wedonothavepriorknowledgeonhowtomodelthecontextofapatternorselectcontextunitswhenthecompletesetofpossiblecontextunitsishuge.Second,itisnotimmediatelyclearhowtoanalyzepatternsemantics,thusthedesignofthestrengthweightingfunctionandsimilaritymeasureisnontrivial.Finally,sincenotrainingdataisavailable,theannotationmustbegeneratedinacompletelyunsupervisedway.Thesechallenges,however,alsoindicateagreatadvan-tageoftheSPAtechniqueswewillpropose{theydonotdependonanydomainknowledgeaboutthedatasetorthepatterns.Inthefollowingtwosections,wepresentourapproachesformodelingthecontextofafrequentpatternandannotat-ingpatternsthroughsemanticcontextanalysis.3.MODELINGPATTERNCONTEXTSInthissection,wediscusshowtomodelpatterncontextsthroughselectingappropriatecontextunitsandde-ningap-propriatestrengthweights.GivenadatasetDandasetoffrequentpatternsPD,ourgoalistoselectasetofcontextunitswhichcarrysemanticinformationandcandiscriminatethemeaningsofthefrequentpatterns.Thediscriminatingpowerofeachcontextunitwillbecapturedbyitsstrengthweights.VectorSpaceModel(VSM)[17]iscommonlyusedinnat-urallanguageprocessingandinformationretrievaltomodelthecontentofatext.Forexample,ininformationretrieval,adocumentandaqueryarebothrepresentedastermvec-tors,whereeachtermisabasicconcept(i.e.,word,phrase),andeachelementofthevectorcorrespondstoatermweightredegectingtheimportanceoftheterm.Thesimilaritybe-tweendocumentsandqueriescanthusbemeasuredbythedistancebetweenthetwovectorsinthevectorspace.Forthepurposeofsemanticmodeling,werepresentatransac-tionandthecontextofafrequentpatternbothasvectorsofcontextunits.WeselectVSMbecauseitmakesnoassump-tiononthevectordimensionsandgivesthemostdegexibilitytotheselectionofdimensionsandweights.Formally,thecontextofafrequentpatternismodeledasfollows.ContextModeling:GivenadatasetD,aselectedsetofcontextunitsfu1;:::;umg,werepresentthecontextc((r))ofafrequentpatternp(r)asavectorhw1;w2;:::;wmi,wherewi=w(ui;(r))andw(C/;(r))isaweightingfunction.Atransactiontisrepresentedasavectorhv1;v2;:::;vmi,wherevi=1i(r)ui2t,otherwisevi=0.ThetwokeyissuesinaVSMaretoselectthevectordimensionsandtoassignweightsforeachdimension[17].Speci-cally,thee(r)ectivenessofcontextmodelingishighlydependentonhowtoselectcontextunitsanddesignthestrengthweights.Actually,duetothegeneralityofVSM,theproposedvector-spacepatterncontextmodelisquitegeneralandcoversdi(r)erentstrategiesforcontextunitselec-tionandweighingfunctions.Inthefollowingsubsections,we-rstdiscussthegeneralityofthecontextmodel,andthendiscussspeci-csolutionsforthetwoissuesrespectively.3.1TheGeneralityofContextModelingSomeexistingworkhasexplorednon-morphologicalinfor-mationoffrequentpatternswithsomeconceptsrelatedtothe\patterncontext""de-nedabove.Wenowshowthatthenotionof\patterncontext""ismoregeneralandcancoverthoseconceptsasspecialcases.In[21],Yanetal.introducedthepro-leofanitemsetforsummarizingitemsetpatterns,whichisrepresentedasaBernoulliDistributionVector.Infact,this\pro-le""ofafre-quentitemset(r)canbewrittenasavectorhw(o1;(r));w(o2;(r)),...,w(od;(r))ioverallthesingleitemsfoiginD.Herew(oi;(r))=Ptj2D(r)tijjD(r)j,wheretij=1ifoi2tjand0oth-erwise.Thisshowsthatthis\pro-le""isactuallyaspecialinstanceofthecontextmodelaswede-ned,wheresingleitemsareselectedascontextunits.Xinandothersproposedadistancemeasureforminingcompressedfrequent-patternsets,whichiscomputedbasedonthetransactioncoverageoftwopatterns[20].Inter-estingly,the\transactioncoverage""isalsoaspeci-cin-stanceof\patterncontext"".Givenafrequentpatternp(r),thetransactioncoverageofp(r)canbewrittenasavectorhw(t1;(r));w(t2;(r));:::;w(tk;(r))ioverallthetransactionsftiginD,whereeachtransactionisselectedasacontextunit,andw(ti;(r))=1ifp(r)2tiand0otherwise.Coveringtheconceptsinexistingworkasspeci-cinstances,thepatterncontextmodelweproposedisgeneralandhasquiteafewbene-ts.First,itdoesnotassumepatterntypes.Thepatternpro-leproposedin[21]assumesthatbothtrans-actionsandpatternsareitemsets,thusdoesnotworkforotherpatternssuchassequentialpatternsandgraphpat-terns.Second,thepatterncontextmodelingallowsdi(r)erentgranularityofcontextunitsanddi(r)erentweightingstrate-gies.Inmanycases,singleitemsarenotinformativeintermsofcarryingsemanticinformation(e.g.,singlenucleotidesinDNAsequences),andthesemanticinformationcarriedbyafulltransactionistoocomplexandnoisy(e.g.,atextdocument).Thecontextmodelingweintroducedbridgesthisgapbyallowingvariousgranularityofsemanticunits,andallowstheusertoexplorethepatternsemanticsatthelevelthatcorrespondstotheirbeliefs.Furthermore,thismodelisadaptivetodi(r)erentstrengthweightingstrategiesforcontextunits,wheretheuser'spriorknowledgeaboutthedatasetandpatternscanbeeasilypluggedin.3.2ContextUnitSelectionWiththegeneralde-nitionpresentedinSection2,theselectionofcontextunitsisquitedegexible.Inprinciple,anyobjectinthedatabasethatcarriessemanticinformationorservestodiscriminatepatternssemanticallycanbeacontextunit,thuscontextunitscanbesingleitems,transactions,patterns,oranygroupofitems/patterns,dependingonthecharacteristicsofthetaskanddata.Withoutlosinggenerality,inourworkweassumeapat-ternistheminimalunitswhichcarriessemanticinformationinadataset,andthusselectthecontextunitsaspatterns.Allkindsofunitscanbeconsideredaspatternswithaspe-ci-cgranularity.Forexample,inasequencedatabase,everysingleitemcanbeviewedasasequentialpatternoflength1,andeverytransactioncanbeviewedasasequentialpatternwhichisidenticaltothetransactionalsequence.Thechoos-ingofpatternsascontextunitsistaskdependent,andcanusuallybeoptimizedwithpriorknowledgeaboutthetaskandthedata.Forexample,wecanusewordsascontextunitsinatextdatabase,andinagraphdatabase,wepre-fersubgraphpatternstobecontextunits,sincesingleitems(i.e.,verticesandedges)arenoninformative.Thisgeneralstrategygivesmuchfreedomtoselectcontextunits.However,selectingpatternsofvariousgranularitymaycausetheredundancyofcontextbecausethesepatternsarehighlyredundant.Asdiscussedinprevioussections,weexpectthecontextunitsnotonlytocarrysemanticinforma-tionbutalsotobeasdiscriminativeaspossibletoindicatethemeaningsofapattern.However,whenvariousgranu-larityofpatternsareselectedascontextunits,someunitswillbecomelessdiscriminative,andmoreseverely,somebe-comesredundant.Forexample,whenthepattern\miningsubgraph""isaddedasacontextunit,thediscriminatingpowerofotherunitslike\miningfrequentsubgraph""and\subgraph""wouldbeweakened.Thisisbecausethetrans-actionscontainingthepattern\miningsubgraph""alwayscontain\subgraph"",andlikelyalsocontain\miningfrequentsubgraph"",whichmeansthatthesepatternsarehighlyde-pendentandnotdiscriminativetoindicatethesemanticsofthefrequentpatternsco-occurringwiththem.Thisre-dundancyalsobringsalotofunnecessarydimensionsintothecontextvectorspacewherethedimensionalityisalreadyveryhigh.Thisredundancyindimensionswilla(r)ectboththee+-ciencyandaccuracyofdistancecomputationbetweentwovectors,whichisessentialforSPA.Inourwork,weex-aminedi(r)erenttechniquestoremovetheredundancyofcon-textunitswithoutlosingthesemanticdiscriminatingpower.3.2.1RedundancyRemoval:ExistingTechniquesOnemay-rstthinkofusingexistingtechniquessuchaspatternsummarizationanddimensionreductiontoremovetheredundancyofcontextunits.Whilethecontextunitscanbeanypatternsinprinciple,wearepracticallynotinterestedinthosewithverylowfre-quencyinthedatabases.Therefore,thecontextunitsweinitiallyincludearefrequentpatterns.Thereexistmeth-odsforsummarizingfrequentpatternswithkrepresentativepatterns[21],buttheyonlyworkforitemsetpatternsandarenotgeneralenoughforourpurpose.SometechniquessuchasLSI[5]havebeendevelopedtoreducethedimensionalityinhighdimensionalspaces,espe-ciallyfortextdata.However,thesetechniquesaimtomit-igatethesparsenessofdatavectorsbyreducingthedimen-sionality,andarenottunedforremovingthe\redundant""dimensions.Thisisbecauseallthesedimensionalityreduc-tiontechniquesconsiderthateachdimensionis\important""andtheinformationitcarrieswillalwaysbepreserved,orpropagatedintothenewspace.Thisis,however,di(r)erentfromourgoalofredundancyremoval.Forexample,ifd1andd2correspondtothepatterns\AB""and\ABC""re-spectively,andifweconsiderd2toberedundantw.r.td1,wedonotexpecttheinformationofd2tobepreservedaftertheremovalofd2.3.2.2RedundancyRemoval:ClosedFrequentPat-ternSinceneitherthepatternsummarizationnorthedimen-sionalityreductiontechniqueisdirectlyapplicabletoourproblem,weexaminealternativestrategies.Noticingthattheredundancyofcontextunitsislikelytobecausedbytheinclusionofbothafrequentpatternanditssubpatterns,weexploreclosedfrequentpatterns[15]andmaximumfrequentpatterns[16]tosolvethisproblem.Amaximalfrequentpatternisafrequentpatternwhichdoesnothaveafrequentsuper-pattern.Itiseasytoshowthatmaximumfrequentpatternisnotappropriateforthisproblemsinceitmayloseimportantdiscriminativeunits.Forexample,thefrequentpattern\datacube"",althoughnotamaximumfrequentpattern,indicatesdi(r)erentsemanticsfromthefrequentpattern\predictiondatacube"",andthusshouldnotberemoved.De-nition8(ClosedFrequentPattern):Afrequentpatternp(r)isclosedifandonlyifthereexistsnosuper-patternp-ofp(r),s.t.D(r)=D-.Weassumethatacontextunitisnotredundantonlyifitisaclosedpattern.Thisassumptionisreasonablebecause8p(r)2PD,ifp(r)isnotclosed,thereisalwaysanotherfre-quentpatternp-2PD,wherep(r)up-and8ti2D,wehavep(r)2ti,p-2ti.Thisindicatesthatwecanusep-asarepresentativeofp(r)andp-withoutlosinganyse-manticdiscriminatingpower.Therefore,inourworkweuseclosedfrequentpatternsasourinitialsetofcontextunits.Thealgorithmsforminingdi(r)erentkindsofclosedfrequentpatternscanbefoundin[15,23].3.2.3RedundancyRemoval:MicroclusteringHowever,asstatedin[21],asmalldisturbancewithinthetransactionsmayresultinhundredsofsubpatternsthatcouldhavedi(r)erentsupports,whichcannotbeprunedbyclosedfrequentpatternmining.Thosesubpatternsareusu-allywithsupportsonlyslightlydi(r)erentfromthatofthemasterpattern.Therefore,theirdiscriminatingpowerforthesemanticsofthefrequentpatternsisveryweakwhentheirmasterpatternsarealsoincludedasacontextunit.Wepresentclusteringmethodstofurtherremoveredundancyfromtheclosedfrequentpatterns.Microclusteringisusuallyemployedasapreprocessingsteptogroupdatapointsfrompresumablythesameclustertoreducethenumberofdatapoints.Inourwork,we-rstintroduceadistancemeasurebetweentwofrequentpatternsandthenintroducetwomicrocluteringalgorithmstofurthergrouptheclosefrequentpatterns.De-nition9(JaccardDistance):Letp(r)andp-astwofrequentpatterns.TheJaccardDistancebetweenp(r)andp-iscomputedas:D(p(r);p-)=1!jD(r)\D-jjD(r)[D-jJaccardDistance[10]iscommonlyappliedtoclusterdatabasedontheirco-occurrenceintransactions.Ourneedistogroupthepatternsthattendtoappearinthesametransac-tions,whichiswellcapturedbyJaccardDistance.JaccardDistancehasalsobeenappliedtopatternclusteringin[20].WithJaccardDistance,weexpecttoextractclusterssuchthatthedistancesbetweeninner-clusterunitsarebounded.Wepresenttwomicroclusteringalgorithmsasfollows:IntheHierarchicalMicroclusteringmethodpresentedasAlgorithm1,weiterativelygrouptwoclustersofpatternswiththesmallestdistance,wherethedistancebetweentwoAlgorithm1HierarchicalMicroclusteringInput:TransactiondatasetD,Asetofnclosedfrequentpatterns,P=fp1;:::;pngThresholdofdistance,degOutput:Asetofpatterns,P0=fp01;:::;p0kg1:initializenclustersCi,eachasaclosedfrequentpattern;2:computetheJaccardDistancedijamongfp1;:::;png;3:setthecurrentminimaldistanced=min(dij);4:while(d<deg)5:selectdstwhere(s;t)=argmini;jdij;6:mergeclustersCsandCtintoanewclusterCu;7:foreachCv6=Cu8:computeduv=max(d(r)-)wherep(r)2Cu;p-2Cv;9:foreachCu;10:foreachp(r)2Cu;11:compute1d(r)=avg(d(r)-)wherep-2Cu;12:addp(r)intoP0,where(r)=argmini(1di);13:returnAlgorithm2One-passMicroclusteringInput:TransactiondatasetD,Asetofnclosedfrequentpatterns,P=fp1;:::;pngThresholdofdistance,degOutput:Asetofpatterns,P0=fp01;:::;p0kg1:initialize0clusters;2:computetheJaccardDistancedijamongfp1;:::;png;3:foreach(p(r)2P)4:foreachclusterCu5:~d(r);u=max(d(r)-)wherep-2Cu;6:v=argminu(~d(r);u);7:if(~d(r);v<deg)8:assignp(r)toCv9:else10:initializeanewclusterC=fp(r)g11:foreachCu;12:foreachp(r)2Cu;13:compute1d(r)=avg(d(r)-)wherep-2Cu;14:addp(r)intoP0,where(r)=argmini(1di);15:returnclustersarede-nedastheJaccarddistancebetweenthefar-thestpatternsinthetwoclusters.Thealgorithmtermi-nateswhentheminimaldistancebetweenclustersbecomeslargerthanauser-speci-edthresholddeg.Thesecondalgo-rithm,whichwecallOne-PassMicroclustering,iterativelyassignsaclosedfrequentpatternp(r)toitsnearestclusterifthedistanceisbelowdeg,wherethedistancebetweenp(r)andaclusterCisde-nedastheJaccarddistancebetweenp(r)anditsfarthestpatterninC.Bothalgorithmsgiveusasetofmicroclustersofclosedfrequentpatterns.Theybothguaranteethatthedistancebetweenanypairofpatternsinthesameclusterisbelowdeg.Onlythemedoidofeachclusterisselectedasacontextunit.Byvaryingdeg,ausercanselectcontextunitswithvariouslevelsofdiscriminatingpowerofpatternsemantics.ItisclearthatAlgorithm2onlypassesthepatternsetonceandthusismoree+-cientthanthehierarchicalalgorithm,attheexpensethatthequalityofclustersdependsontheorderofpatterns.TheperformanceofthesetwomethodsarecomparedinSection5.3.3StrengthWeightingforContextUnitsOncethecontextunitsareselected,theremainingtaskistoassignaweighttoeachdimensionofthecontextmodel,whichrepresentshowstrongthecontextunitcorrespondingtothisdimensionindicatesthemeaningofagivenpattern.Intuitively,thestrongestcontextindicatorsforapatternp(r)shouldbethoseunitsthatfrequentlyco-occurwithp(r)butinfrequentlyco-occurwithothers.Practically,manytypesofweightingfunctionscanbeusedtomeasurethestrengthofacontextindicator.Forexample,wecanassigntheweightforacontextindicatoruforp(r)asthenumberoftransactionswithbothuandp(r).However,inprinciple,agoodweightingfunctionisexpectedtosatisfyseveralconstraints:GivenasetofcontextindicatorUandafrequentpatternp(r),astrengthweightingfunctionw(C/;p(r))isgoodif8ui2U1.w(ui;p(r))*w(p(r);p(r)):thebestsemanticindicatorofp(r)isitself;2.w(ui;p(r))=w(p(r);ui):twopatternsareequallystrongtoindicatethemeaningsofeachother;3.w(ui;p(r))=0iftheappearanceofuiandp(r)isinde-pendent:uicannotindicatethesemanticsofp(r).Anobviouschoiceisco-occurrences,whichhowever,maynotbeagoodmeasure.Oneonehand,itdoesnotsatisfyconstraints3.Ontheotherhand,wewanttopenalizethecontextunitsthataregloballycommonpatternsinthecol-lection.Whichmeans,althoughtheymayco-occurmanytimeswithp(r),itmaystillnotbeagoodcontextindica-torforp(r)becauseitalsoco-occursfrequentlywithothers.Ingeneral,thecontextunitsthatarestronglycorrelatedtop(r)shouldbeweightedhigher.Inourwork,weintroduceamoreprincipledmeasure.MutualInformation(MI)iswidelyusedtomeasurethemutualindependencyoftworandomvariablesininforma-tiontheory,whichintuitivelymeasureshowmuchinforma-tionarandomvariabletellsabouttheother.Thede-nitionofmutualinformationisgivenasDe-nition10:(MutualInformation).Giventwofre-quentpatternsp(r)andp-,letX=f0;1gandY=f0;1gbetworandomvariablesfortheappearanceofp(r)andp-respectively.MutualinformationI(X;Y)iscomputedas:I(X;Y)=Xx2XXy2YP(x;y)logP(x;y)P(x)P(y)whereP(x=1;y=1)=jD(r)\D-jjDj,P(x=0;y=1)=jD-j!jD(r)\D-jjDj,P(x=1;y=0)=jD(r)j!jD(r)\D-jjDj,andP(x=0;y=0)=jDj!jD(r)[D-jjDj.Inourexperiments,weusestan-dardLaplacesmoothingtoavoidzeroprobability.ItcanbeeasilyprovedthatMutualInformationsatis-esallthethreeconstraintsandfavorsthestronglycorrelatedunits.Inourwork,weusemutualinformationtomodeltheindicativestrengthofthecontextunitsselected.Givenasetofpatternsascandidatecontextunits,weap-plyclosenesstestingandmicroclusteringtoremoveredun-dantunitsfromthisinitialset.Wethenusemutualinfor-mationastheweightingfunctionforeachindicatorselected.Givenafrequentpattern,weapplysemanticanalysiswithitscontextmodelandgenerateannotationsforthispattern,asdiscussedinthefollowingsection.4.SEMANTICANALYSISANDPATTERNANNOTATIONLetU=fu1;u2;:::;ukgbeaselectedsetofkcontextunitsandw(C/;p(r))betheunitweightingfunctionw.r.t.anyfrequentpatternp(r),i.e.I(C/;p(r)).Thecontextmodel,orcon-textvectorc((r))forp(r)ishw(u1;p(r));w(u2;p(r));:::;w(uk;p(r))i.AsintroducedinSection1,wemaketheassumptionthatthefrequentpatternsaresemanticallysimilariftheircon-textsaresimilartoeachother.Inourwork,weanalyzethesemanticsoffrequentpatternsbycomparingtheircontextmodels.Formally,De-nition11(SemanticalSimilarity):Letp(r),p-,p+-bethreefrequentpatternsinPandc((r)),c(-),c(+-)2Vkbetheircontextmodels.Letsim(c(C/);c(C/)):VkPSVk!!R+beasimilarityfunctionoftwocontextvectors.Ifsim(c((r));c(-))>sim(c((r));c(+-)),wesaythatp-issemanticallymoresim-ilartop(r)thanp+-w.r.t.sim(c(C/);c(C/)).Cosineiswidelyusedtocomputethesimilaritybetweentwovectors,andiswellexploredininformationretrievaltomeasuretherelevancebetweenadocumentandaqueryifbotharerepresentedwithavectorspacemodel[17].Inourwork,weusecosinesimilarityoftwocontextvectorstomea-surethesemanticsimilarityoftwocorrespondingfrequentpatterns.Formally,thecosinesimilarityoftwocontextvec-torsiscomputedassim(c((r));c(-))=Pki=1ai$?biqPki=1a2i$?qPki=1b2iwherec((r))=ha1;a2;:::;akiandc(-)=hb1;b2;:::;bki.Withthecontextmodelandthesemanticalsimilaritymeasure,wenowdiscusshowtogeneratesemanticanno-tationsforfrequentpatterns.4.1ExtractingStrongestContextIndicatorsLetp(r)beafrequentpatternandc((r))beitscontextmodel,whichisde-nedinthisworkasacontextvectorhw1;w2;:::;wkioverasetofcontextunitsU=fu1;u2;:::;ukg.Asde-nedinSection2,wiisaweightforuiwhichtellshowwelluiindicatesthesemanticsofp(r).Therefore,thegoalofextractingstrongestcontextindicatorsistoextractasub-setofk0contextunitsU(r)uUsuchthat8ui2U(r)and8uj2U!U(r),wehavewi,wj.Withastrengthweightingfunctionw(C/;p(r)),e.g.,mutualinformationasintroducedinSection3,wecomputewi=w(ui;p(r)),rankui2Uwithwiindescendingorderandselectthetopk0ui's.4.2ExtractingRepresentativeTransactionsLetp(r)beafrequentpattern,c((r))beitscontextmodel,andD=ft1;:::tlgbeasetoftransactions,ourgoalistoselectkttransactionsT(r)uDwithasimilarityfunctions(C/;p(r)),s.t.8t2T(r)and8t02D!T(r),s(t;p(r)),s(t0;p(r)).Toachievethis,we-rstrepresentatransactionasavec-torinthesamevectorspaceasthecontextmodelofthefrequentpatternp(r),i.e.,overfu1;u2;:::;ukg.Then,weusethecosinesimilaritypresentedinSection3tocomputethesimilaritybetweenatransactiontandthecontextofp(r).Therestisagainarankingproblem.Formally,letc(t)=hw01;w02;:::;w0kiwherew0i=1ifui2tandw0i=0otherwise.Wecomputesim(c(t);c((r)))foreacht2T(r),rankthemindescendingorderandselectthetopktt's.4.3ExtractingSemanticallySimilarPatternsLetp(r)beafrequentpattern,c((r))beitscontextmodel,andPc=fp1;:::;pcgbeasetoffrequentpatternswhicharebelievedtobegoodcandidatesforannotatingthese-manticsofp(r),i.e.,assynonyms,thesauri,ormoregenerallyasSSPs.OurgoalistoextractasubsetofkcpatternsP0cuPcwhosecontextsaremostsimilartop(r).Formally,letfc(p1);:::;c(pc)gbethecontextvectorsforfp1;:::;pcg.Wecomputesim(c(pi);c((r)))foreachpi2Pc,rankthemindescendingorder,andselectthetopkcpi's.NotethatthecandidateSSPsetforannotationisquitedegexible.ItcanbethewholesetoffrequentpatternsinD,orauser-speci-edsetofpatternsbasedonhispriorknowledge.Itcanbeasetofhomogenouspatternswithp(r),orasetofheterogenouspatterns.Forexample,itcanbeasetofpat-ternsorterminologyfromthedomainthatauserisfamiliarwith,andisusedtoannotatepatternsfromanunfamiliardomain.ThisbringsgreatdegexibilitytoapplythegeneralSPAtechniquestodi(r)erenttasks.Byexploringdi(r)erenttypesofcandidateSSPs,wecan-ndquiteafewinterest-ingapplicationsofsemanticpatternannotation,whicharediscussedinSection5.5.EXPERIMENTSANDRESULTSInthissection,wepresentexperimentresultsonthreedi(r)erentdatasetstoshowthee(r)ectivenessofthesemanticpatternannotationtechniqueforvariousreal-worldtasks.5.1DBLPDatasetThe-rstdatasetweuseisasubsetoftheDBLPdataset2.Itcontainspapersfromtheproceedingsof12majorcon-ferencesinDatabaseandDataMining.Eachtransactionconsistsoftwoparts,theauthorsandthetitleofthecor-respondingpaper.Weconsidertwotypesofpatterns:(1)frequentco-authorship,eachofwhichisafrequentitemsetofauthorsand(2)frequenttitleterms,eachofwhichisafrequentsequentialpatternofthetitlewords.Thegoalofexperimentsonthisdatasetistoshowthee(r)ectivenessoftheSPAtogenerateadictionary-likeannotationforfrequentpatterns.Ourexperimentsaredesignedasfollows:1)Givenasetofauthors/co-authors,annotateeachofthemwiththeirstrongestcontextindicators,themostrepre-sentativetitlesfromtheirpublications,andtheco-authorsortitlepatternswhicharemostsemanticallysimilartothem.Notethatthemostrepresentativetitlesdonotnecessarilymeantheirmostindeguentialwork,butratherthetitleswhichbestdistinguishtheirworkfromothers'work.2)Givenasetoftitleterms(sequentialpatterns),anno-tateeachofthemwiththeirstrongestcontextindicators,themostrepresentativetitles,themostsimilarterms,andthemostrepresentativeauthor/co-authors.Noteagainthatthemostrepresentativeauthor/co-authorsarenotnecessar-ilythemostwell-knownones,butrathertheauthorswhoaremoststronglycorrelatedtothetopics(terms).Inbothexperiments,weusethetoolsFP-Close[7]andCloSpan[23]togenerateclosedfrequentitemsetsofco-authorsandclosedsequentialpatternsoftitletermsrespec-tively.ThetitlewordsarestemmedbyKrovertzstemmer[12],whichconvertsthemorphologicalvariationsofeachEnglishwordtoitsrootform.Wesettheminimumsup-portforfrequentitemsetas10andsequentialpatternsas4,2http://www.informatik.uni-trier.de/>>ley/db/whichoutputs9926closedsequentialpatterns.WeusetheOne-PassmicroclusteringalgorithmdiscussedinSection3toremoveredundancyfromthosesequentialpatternsandgetasmallersetof3443patterns,withdeg=0:9(theaverageJaccarddistancebetweenthesepatternsis>0:95).MedoidsClusterMembersminedata,mine,dataminemineassociaterule,associate,associaterule,minerulerulemineassociate,mineassociateruleminestreamminedata,minestream,datastream,minedatastreamTable1:E(r)ectivenessofMicroclusteringTable1showsthemedoidsandclustermembersofthreemicroclustersgeneratedbytheOne-Passmicroclusteringal-gorithmdiscussedinSection3,allofwhichbeginwiththeterm\mine"".Weseethatdi(r)erentvariationsofthesameconceptaregroupedintothesamecluster,althoughallofthemareclosedpatterns.Thissuccessfullyreducesthepat-ternredundancy.Itisinterestingtoseethatthepattern\datamine""and\minedata""areassignedtodi(r)erentclus-ters,whichcannotbeachievedbytheexistingpatternsum-marizationtechniquessuchas[21].Theresultsgeneratedbyhierarchicalmicroclusteringaresimilar.InTable2,weselectivelyshowtheresultsofsemanticpatternannotations.WeseethattheSPAsystemcanauto-maticallygeneratedictionary-likeannotationsfordi(r)erentkindsoffrequentpatterns.Forfrequentitemsetslikeco-authorshiporsingleauthors,thestrongestcontextindica-torsareusuallytheirotherco-authorsanddiscriminativeti-tletermsthatappearintheirwork.Thesemanticallysimilarpatternsextractedalsoredegecttheauthorsandtermsrelatedtotheirwork.However,theseSSPsmaynotevenco-occurwiththegivenpatterninapaper.Forexample,thepattern\jiayongwang"",\jiongyang&philipsyu&weiwang""actu-allydonotco-occurwiththepattern\xifengyan&jiaweihan"",butareextractedbecausetheircontextsaresimilar.Forasingleauthor,whosecontextisusuallymorediverse,theSSPsaremorelikelytobetitletermsinsteadofauthors.Wealsopresenttheannotationsgeneratedfortitleterms,whicharefrequentsequentialpatterns.Theirstrongestcon-textindicatorsareusuallytheauthorswhotendtowritetheminthetitlesoftheirpapers,orthetermsthattendtoco-appearwiththem.TheirSSPsusuallyprovideinter-estingconceptsordescriptivetermswhichareclosetotheirmeanings,e.g.\informationretrieval!information-lter"",\xquery!complexlanguage,functionquerylanguage"".Inbothscenarios,therepresentativetransactionsextractedgiveusthetitlesofpapersthatwellcapturethemeaningofthegivenpatterns.WeonlyshowthetitlewordsinTable2foreachtransaction.TheseexperimentsshowthattheSPAcangeneratedic-tionarylikeannotationsforfrequentpatternse(r)ectively.Inthefollowingtwoexperiments,wequantitativelyevaluatetheperformanceofSPA,byapplyingittotwointerestingtasks.5.2MatchingMotifsandGOTermsAchallengingandpromisingresearchtopicincomputa-tionalbiologyistopredictthefunctionsfornewlydiscoveredproteinmotifs,whichareconservedaminoacidsequencepatternscharacterizingthefunctionofproteins.Tosolvethisproblem,researchershavestudiedhowtomatchGenePatternTypeAnnotationsxifengyanIgraph;philipsyu;mineclose;mineclosefrequent;indexapproach;graphpattern;sequentialpatternjiaweihanTgspangraph-basesubstructurepatternmineTminecloserelationalgraphconnectconstraint(SSPset=Tclospanmineclosesequentialpatternlargedatabaseco-authorpatterns)Sjiaweihan&philipsyu;jianpei&jiaweihan;jianyongwang;jiongyang&philipsyu&weiwangIspirospapadimitriou;fast;usefractal;graph;usecorrelate;christosfaloutsoTmultiattributehashusegraycode(SSPset=Trecoverelatenttime-serytheirobservesumnetworktomographyparticlefiltertitletermpatterns)TindexmultimediadatabasetutorialSusefractal;fastdatamine;datagraph;efficienttimesequence;spatialaccessmethod;discoverycorrelateinformationIwbrucecroft;webinformation;monikarauchhenzinger;jamespcallan;full-text;retrievalTwebinformationretrievalTlanguagemodelinformationretrievalSinformationuse;webinformation;probabilistinformation;informationfilter;textinformationIxquerystream;muralimani;jensteubner;treeefficientxqueryTimplementxqueryTxqueryquerylanguagexmlSxquerystream;streamxml;complexlanguage;functionquerylanguage;estimatexml;Table2:AnnotationsGeneratedforFrequentPatternsinDBLPDatasetNote:\I""meanscontextindicators;\T""meansrepresentativetransactions;\S""meanssemanticallysimilarpatterns.Weexclude12mostfrequentandnon-informativeEnglishwordsfromthecollectionwhenextractingfrequentpatterns.Ontology(GO)termswithmotifs[18].Usually,eachproteinsequence,whichcontainsanumberofmotifs,isassignedasetofGOtermsthatannotateitsfunctions.Thegoaloftheproblemistoautomaticallymatcheachindividualmo-tifwithGOtermswhichbestrepresentitsfunctions.Inthisexperiment,weformalizetheproblemas:GivenasetoftransactionsD(proteinsequenceswithmotifstaggedandGOtermsassigned),asetPoffrequentpatternsinDtobeannotated(motifs),andasetofcandidatepatternsPcwithexplicitsemantics(GOterms),ourgoalisfor8p(r)2P,-ndP0cuPcwhichbestindicatethesemanticsofp(r).Weusedthesamedatasetandjudgments(i.e.,goldstan-dard)asusedin[18].Thedatahas12181sequences,1097motifs,and3761GOterms.Wealsousethesameperfor-mancemeasureasin[18](i.e.,avariantofMeanrecip-rocalrank(MRR)[11],notatedasMRRinthefollowingsectionsforconvenience)toevaluatethee(r)ectivenessoftheSPAtechniqueontheMotif-GOtermmatchingproblem.LetG=fg1;g2;:::;gcgbeasetofGOterms.Givenamotifpatternp(r),G0=fg01;g02;:::;g0kguGisasetof\cor-rect""GOtermsforp(r)inourjudgementdata.WerankGwiththeSPAsystemandpickthetoprankedterms,whereGistreatedaseithercontextunitsorsemanticallysimilarpatternstop(r).Thiswillgiveusarankforeachgi2G,sayr(gi).MRR(w.r.t.p(r))isthencomputedasMRR(r)=1kkXi=11r(g0i)wherer(g0i)istheithcorrectGOtermforp(r).Ifg0iisnotinthetoprankedlist,weset1=r(g0i)=0.Wetaketheaverageoverallthemotifs,MRR=1=mPP(r)2PMRR(r)tomeasuretheoverallperformance,wheremisthenumberofmotifsinourjudgement-le.Clearly,0*MRR*1.AhigherMRRvalueindicatesahigherprecision,andthetop-rankedGOtermshavethehighestindeguenceonMRR,whichisintuitivelydesirable.IfwearerankingthefullcandidateGOsetforannotation,a\lazy""systemmayeitherjustgivethemthesamerankorrankthemrandomly.ItiseasytoshowthattheexpectedMRRscoreforthesetwocasesarethesame,whichisE[MRR]=1jGjjGjXi=11r(gi)wherejGjisthenumberofGOtermsinG.E[MRR]dropsmonotonouslywhenjGjincreases,whichindicatesthelargerthecandidatesetis,themoredi+-cultistherankingtask.Weusethisvalueasthebaselinetocompareourresults.WeemployallthemotifsandGOtermsascontextunits.Sincethesepatternsarenotoverlappingwitheachother,wedonotusemicroclusteringtopreprocessthecontextunits.WecomparetherankingofGOtermseitherascontextin-dicatorsorasSSPs.WealsocomparetheuseofMutualInformationandco-occurrenceasstrengthweightforcon-textunits.ThesestrategiesarecomparedinTable3:MRRUseMIUseCo-occurrenceContextStrength0.58770.6064SemanticalSimilarity0.40170.4681Random(jGj=3761)0.0023Table3:MRRofSPAonMotif-GOmatchingWeseethatSPAisquitee(r)ectiveinmatchingmotifswithGOterms,consistentlyoutperformingthebaseline.Rank-ingGOtermsascontextunitsachievesbetterresultsthanrankingthemasSSPs,whichisreasonablebecauseaGOtermusuallydescribesonlyoneaspectofamotif'sfunctionandissharedbyanumberofmotifs,thusitscontextislikelyquitedi(r)erentfromthatofamotif.Interestingly,wenoticethatalthoughMutualInformationisabettermeasureforthestrengthweightinprinciple,inthisspeci-cproblem,usingMIasstrengthweightforcon-textunitsisnotasgoodasusingsimpleco-occurrence.ThismaybebecausetherearehardlymanyGotermsthataregloballyverycommoninthisdataset,andthereforeMIoverpenalizesthefrequentpatterns.Adetaileddiscussiononwhyco-occurrencemeasureoutperformsMIonMotif-GOmatchingproblemisgivenin[18].5.3MatchingGeneSynonymsAsdiscussedinSection4.3,thealgorithmforextractingsemanticallysimilarpatternsaimsat-ndingpatternswhosemeaningisveryclosetothepatterntobeannotated.Ideally,theywouldbesynonyms,orthesauriofthegivenpattern.Thesepatternsmaynoteverco-occurwiththegivenpatternbuttendtohavesimilarcontexts,thuscannotbeextractedasstrongcontextindicators.WedoanotherexperimenttotesttheperformanceofSPAonextractingSSPs.Inbiomedicalliterature,itiscommonthatdi(r)erenttermsoraliasesareusedindi(r)erentstudiestodenotethesamegene,whichareknownasgenesynonyms(seee.g.,Table4).Thesesynonymsgenerallydonotappeartogetherbutare\replaceable""witheachother.Detectingthemcanhelpmanyliteratureminingtasks.Inthisexperiment,wetesttheapplicationofSPAtomatchinggenesynonyms.GeneidGeneSynonymsFBgn0000028abnormalchemosensoryjump6;acj6;ipou;ipou;cg9151;tipou;twinofipou;FBgn0001000femalesterile2tekele;fs2sz10;tek;fs2tek;tekele;Table4:ExamplesofgenesynonympatternsWeconstructthesynonymlistfor100degygenes,whicharerandomlyselectedfromthedataprovidedbyBioCre-AtIvETask1B3.Lingetal.collected22092abstractsfromMEDLINE4whichcontainthekeyword\Drosophila""[14].Weextractthesentencesfromthoseabstractswhichcon-tainatleastonesynonyminthesynonymlist.Onlythesynonymswithsupport,3arekept,whichgivesusasmallsetof41synonyms.Wethenmixthosesynonymswhichbelongtodi(r)erentgenesandusethealgorithmofextract-ingSSPstorecoverthematchingofsynonyms.Speci-cally,givenasynonymfromthemixedlist,werankallsynonymswiththeSSPextractionalgorithm.Theperformanceofthesystemisevaluatedbycomparingtherankedlistwiththecorrectsynonymsforthesamegene.WealsouseMRRastheevaluationmeasure.Theresultsareshownasfollows.ContextminNoMicro-One-passHierarchicalUnitssupClusteringdeg=0:9deg=0:9Closed0.15%0.51080.51610.5199Sequential0.18%0.51400.51910.5225Patterns0.24%0.52200.52450.53010.3%0.52810.52920.5281SingleWords0.4774Random0.1049(jGj=41)Table5:MRRofSPAongenesynonymmatchingFigure1:E(r)ectofmicroclusteringalgorithmsHIER:hierarchicalmicroclustering;ONEP:one-passmicroclus-tering;minsup=0.3%Avg.deg=0.96;FromTable5,weseethattheSPAalgorithmisalsoef-fectiveformatchinggenesynonyms,whichsigni-cantlyout-performstherandombaseline.Whenusingclosedsequentialpatternsascontextunits,wealwaysachievebetterresultsthanusingsinglewords(items)ascontextunits,whereahigherminimumsupport(minsup)usuallyyieldsbetterre-sults.Whenclosedsequentialpatternsareused,furthermicroclusteringindeedimprovestheperformanceofthesys-tem.However,whentheminsupishigher,thisimprovement3http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html4http://www.ncbi.nlm.nih.gov/entrez/query.fcgiisdecaying.Thisisreasonablebecausewhentheminsupishigher,thereislessredundancyamongtheoutputclosedpatterns.Usinghierarchicalmicroclusteringisslightlybet-terthanusingtheone-passalgorithm,butnotalways.Finally,wediscusstheperformanceofmicroclusteringinremovingredundantcontextunits.Thee(r)ectivenessandef--ciencyareshowninFigure1.Bothmicroclusteringmeth-odsimprovetheprecision(MRRscore)whenmoreredun-dantpatternsaregroupedintoclusters.However,whendegissettoolarge,theprecisiondecreases.Thisindicatesthatwemayhaveoverpenalizedtheredundancyandlostusefulcontextunits.Agooddegforthistaskisaround0.8.Althoughtheclusterqualitymaynotbeoptimized,theperformanceofone-passmicroclusteringiscomparabletohi-erarchicalmicroclusteringonthistask.Whileinprinciple,thehierarchicalclusteringisnote+-cient,theearlytermi-nationbyusingasmalldegsavesalotoftime.Theone-passalgorithmismoree+-cientthanthehierarchicalclustering,andisnota(r)ectedbydeg.Theoverheadthatbothalgorithmssu(r)eristhecomputationofJaccarddistancesforallpairsofpatterns,i.e.,O(n2)wherenisthenumberofpatterns.However,thiscomputationcanbecoupledinfrequentpat-ternmining,asdiscussedin[20].6.RELATEDWORKTothebestofourknowledge,theproblemofsemanticpat-ternannotationhasnotbeenwellstudiedinexistingwork.Mostfrequentpatternminingwork[2,8,3,22]focusesondiscoveringfrequentpatternse+-cientlyfromthedatabase,anddoesnotaddresstheproblemofpatternpostprocess-ing.Tosolvetheproblemofhighredundancyinpatternsdiscovered,closedfrequentpattern[15],maximumfrequentpattern[16]andtop-kclosedpattern[9]areproposedtoshrinkthesizeofoutputpatternswhilekeepingtheimpor-tantones.However,noneofthisworkprovidesadditionalinformationotherthansimplestatisticstohelpusersinter-pretthefrequentpatterns.Thecontextinformationforapatterntendstobeignored.Recently,researchersdevelopnewtechniquestoapprox-imate,summarizeafrequentpatternset[1,21],orminecompressedfrequentpatternsets[20].Althoughtheyex-ploredsomekindofcontextinformation,noneoftheworkcanprovidein-depthsemanticannotationsforfrequentpat-ternsaswedoinourwork.Thecontextmodelproposedinourworkcoversboththepatternpro-lein[21]andtrans-actioncoveragein[20]asspecialcases.Contextandsemanticanalysisarequitecommoninnatu-rallanguageandtextprocessing(seee.g.,[17,5,13]).Mostwork,however,dealswithnon-redundantword-basedcon-texts,whicharequitedi(r)erentfrompatterncontexts.Inspeci-cdomains,peoplehaveexploredthecontextofspeci-cdatapatternstosolvespeci-cproblems[18,14].Althoughnotoptimallytuned,thegeneraltechniquespro-posedinourworkcanbewellappliedtothosetasks.7.CONCLUSIONSExistingfrequentpatternminingworkusuallygeneratesahugeamountoffrequentpatternswithoutprovidingenoughinformationtointerpretthemeaningsofthepatterns.Somerecentworkintroducedpostprocessingtechniquestosum-marizeandcompressthepatternset,whichshrinksthesizeoftheoutputsetoffrequentpatternsbutdoesnotprovidesemanticinformationforpatterns.Weproposethenovelproblemofsemanticpatternan-notation(SPA){generatingsemanticannotationsforfre-quentpatterns.Asemanticannotationconsistsofasetofstrongestcontextindicators,asetofrepresentativetransac-tions,andasetofsemanticallysimilarpatterns(SSPs)toagivenfrequentpattern.Wede-neageneralvector-spacecontextforafrequentpattern.Weproposealgorithmstoexploitcontextmodelingandsemanticanalysistogeneratesemanticannotationsautomatically.Thecontextmodelingandsemanticanalysismethodwepresentedisquitegen-eralandcandealwithanytypesoffrequentpatternswithcontextinformation.Themethodcanbecoupledwithanyfrequentpatternminingtechniquesasapostprocessingsteptofacilitateinterpretationofthediscoveredpatterns.Weevaluatedourapproachonthreedi(r)erentdatasetandtasks.Theresultsshowthatourmethodscangeneratese-manticpatternannotationse(r)ectively.Asshowninourex-periments,ourmethodcanbepotentiallyappliedtomanyinterestingrealworldtasksthroughselectingdi(r)erentcon-textunitsandfocusingoncandidatepatternsforSSPs.AlthoughtheproposedSPAframeworkisquitegeneral,inthispaper,weonlystudiedsomespeci-cinstantiationoftheframeworkbasedonmutualinformationweightingandcosinesimilaritymeasure.Amajorgoalforfutureresearchistofullydevelopthepotentialoftheproposedframeworkbystudyingalternativeinstantiations.Forexample,wemayexploreotheroptionsforcontextunitweightingandseman-ticsimilaritymeasurement,thetwokeycomponentsinourframework.8.ACKNOWLEDGMENTSWethankTaoTaoandXuLingforprovidingthedatasetsofMotif-GOmatchingandgenesynonymmatching,respec-tively.ThisworkwasinpartsupportedbytheNationalScienceFoundationunderawardnumbers0425852.9.REFERENCES[1]F.Afrati,A.Gionis,andH.Mannila.Approximatingacollectionoffrequentsets.InProceedingsofthetenthACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pages12{19,2004.[2]R.Agrawal,T.Imieliski,andA.Swami.Miningassociationrulesbetweensetsofitemsinlargedatabases.InProceedingsofthe1993ACMSIGMODinternationalconferenceonManagementofdata,pages207{216,1993.[3]R.AgrawalandR.Srikant.Miningsequentialpatterns.InProceedingsoftheEleventhInternationalConferenceonDataEngineering,pages3{14,1995.[4]S.Brin,R.Motwani,andC.Silverstein.Beyondmarketbaskets:generalizingassociationrulestocorrelations.InProceedingsofthe1997ACMSIGMODinternationalconferenceonManagementofdata,pages265{276,1997.[5]S.C.Deerwester,S.T.Dumais,T.K.Landauer,G.W.Furnas,andR.A.Harshman.Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyofInformationScience,41(6):391{407,1990.[6]M.Deshpande,M.Kuramochi,andG.Karypis.Frequentsub-structure-basedapproachesforclassifyingchemicalcompounds.InProceedingsofICDM'03,page35,2003.[7]G.GrahneandJ.Zhu.E+-cientlyusingpre-x-treesinminingfrequentitemsets.InFIMI'03WorkshoponFrequentItemsetMiningImplementations.,2003.[8]J.Han,J.Pei,Y.Yin,andR.Mao.Miningfrequentpatternswithoutcandidategeneration:Afrequent-patterntreeapproach.DataMin.Knowl.Discov.,8(1):53{87,2004.[9]J.Han,J.Wang,Y.Lu,andP.Tzvetkov.Miningtop-kfrequentclosedpatternswithoutminimumsupport.InProceedingsofICDM'02,2002.[10]P.Jaccard.Nouvellesrecherchessurladistributiondegorale.Bull.Soc.VaudoiseSci.Nat.,44:223C{270,1908.[11]P.KantorandE.Voorhees.TheTREC-5confusiontrack:Comparingretrievalmethodsforscannedtext.InformationRetrieval,2:165{176,2000.[12]R.Krovetz.Viewingmorphologyasaninferenceprocess.InProceedingsofSIGIR'93,pages191{202,1993.[13]D.LinandP.Pantel.Inductionofsemanticclassesfromnaturallanguagetext.InProceedingsofKDD'01,pages317{322,2001.[14]X.Ling,J.Jiang,X.He,Q.Mei,C.Zhai,andB.Schatz.Automaticallygeneratinggenesummariesfrombiomedicalliterature.InProceedingsofPaci-cSymposiumonBiocomputing,pages40{51,2006.[15]N.Pasquier,Y.Bastide,R.Taouil,andL.Lakhal.Discoveringfrequentcloseditemsetsforassociationrules.InProceedingofthe7thInternationalConferenceonDatabaseTheory,pages398{416,1999.[16]J.RobertoJ.Bayardo.E+-cientlymininglongpatternsfromdatabases.InProceedingsofthe1998ACMSIGMODinternationalconferenceonManagementofdata,pages85{93,1998.[17]G.Salton,A.Wong,andC.S.Yang.Avectorspacemodelforautomaticindexing.Commun.ACM,18(11):613{620,1975.[18]T.Tao,C.Zhai,X.Lu,andH.Fang.Astudyofstatisticalmethodsforfunctionpredictionofproteinmotifs.AppliedBioinformatics,3(2-3):115{124,2004.[19]K.Wang,C.Xu,andB.Liu.Clusteringtransactionsusinglargeitems.InProceedingsofCIKM'99,pages483{490,1999.[20]D.Xin,J.Han,X.Yan,andH.Cheng.Miningcompressedfrequent-patternsets.InProceedingsofVLDB'05,pages709{720,2005.[21]X.Yan,H.Cheng,J.Han,andD.Xin.Summarizingitemsetpatterns:apro-le-basedapproach.InProceedingoftheeleventhACMSIGKDDinternationalconferenceonKnowledgediscoveryindatamining,pages314{323,2005.[22]X.YanandJ.Han.gspan:Graph-basedsubstructurepatternmining.InProceedingsICDM'02,pages721{724,2002.[23]X.Yan,J.Han,andR.Afshar.Clospan:Miningclosedsequentialpatternsinlargedatasets.InProceedingsofSDM'03,pages166{177,2003. Reproducing Paper on Generating Semantic Annotations for Frequent Patterns with Context Analysis Project Overview: In this project, we have tried to reproduce the model and results from the following published paper on Pattern Annotation. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. This model will automatically generate such annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary like description for a pattern, finding synonym patterns, discovering semantic relations, ranking patterns, categorizing and clustering patterns with semantics, and summarizing semantic classes of a set of frequent patterns. Experiment from the paper reproduced: Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most representative titles from their publications, and the co-authors and title patterns which are most semantically similar to them. Implementation Approach: Here, the general approach taken to automatically generate the frequent patterns and structured annotations for them by the following steps: 1. Derive the frequent patterns Author/Co-Author pattern from the Database using FP Growth algorithm. 2. Define and model the context of the pattern: Derive the context units for the Author/Co-Authors by mining the Closed Frequent Pattern to avoid any redundant pattern Derive the context units for the Titles by mining the Sequential Closed Frequent Pattern using Prefix Span Algorithm Select context units and design a strength weight for each unit to model the contexts of frequent pattern 3. Derive the list of representative transaction by finding the cosine similarity between the Frequent Pattern & the Frequent Transactions 4. Derive the Semantically Similar Author Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Frequent Author/Co-Author 5. Derive the Semantically Similar Title Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Title Words Installation and Usage: The following packages must be installed for the successful execution: https://test.pypi.org/simple/ krovetz Prefixspan, re, csv, pandas, numpy, mlxtend, sklearn Note: To use the Krovetz Stemmer installed, Visual Studio C++ is needed to be installed. Team members had issues running the program without it. Input data: Here the input dataset (DBLP Dataset) is in a specific format. DBLP dataset (a subset of around 12k transactions/titles papers from the proceedings of 12 major conferences in Data Mining; around 1k latest transactions from each of such conference). Each row represents a title presented in a conference. It has 3 fields - id (numeric), title (String) and MergedAuthors (string). The authors and co-authors associated with the title has been merged into a single column for easy analysis. Output: Once the program patternAnnotation.py is executed, it takes the DBLP_Dataset.csv as the input and generates the output.txt file in the same path where source code exists. This output file contains all the closed frequent patterns and their most representative Context Indicators, most representative transactions (capped as 4), Semantically Similar Patterns (SSPs) as per co-author patterns and title term patterns. Each record in the output file represents one closed frequent pattern and their associated details. Manually converted output to output.xlsx, with summary information of the annotated pattern is available as well: I - Context Indicator; T - Representative Transactions; SSPA - Semantically Similar Author Pattern; SSPT - Semantically Similar Title Pattern Implementation Details: Derive the frequent itemsets of Author/Co-Author: Algorithm used: FP Growth Output: 64 Frequent Author/Co-Author itemsets (Removed any frequent author without co-Author) Define and model the context of the pattern: For each of the 64 Frequent Itemsets from the above step: Derive the context units for the Author/Co-Authors: Mined closed Author/Co-Author itemsets from already mining frequent itemsets by removing any itemset with redundant support Derive the context units for the Title: Algorithm used: Prefix Span; We got limited Title Patterns, hence we did not need micro-clustering to further reduce the titles. Final Context Indicator: Merged the context units of the above two steps Context Indicator Weighting: Here we slightly modified our approach to weighting as the mutual information value was not giving the appropriate weightage, based on our analysis & understanding. We use an approach similar to IDF, where when a context unit is present in more transaction, the weightage is reduced & any context unit that uniquely appears in the transaction will have the highest weightage. Derive the list of representative transaction: Based on the context indicators identified for each of the Frequent Itemset, we identified top 4 transactions with the highest Cosine Similarity Derive the list of Semantically Similar Author/Co-Author pattern: Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their Author/Co-Author as the SSP Derive the list of Semantically Similar Author/Co-Author pattern: Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their frequent title pattern as the SSP Team Contribution: Name ID Contribution Bipin Chandra Karnati karnati3 Analysis, Implementation of Extracting Title Patterns of Frequent Authors, Context Indicator Identification, Code Integration Savitha Govindarajan savitha3 Analysis, Implementation of Extracting Itemsets of Frequent Authors, SSP Author Pattern, Co-ordination, Code Integration Utpal Mondal umondal2 Analysis, Data Extraction & Clean Up, Implementation of Cosine Similarity, Detailed analysis on MI, Documentation , Code Integration References: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 FP Growth Algorithm implementation: https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3 Prefix Span: https://pypi.org/project/prefixspan/ Cosine Similarity: https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ DBLP Dataset: 12 Major Conferences on Data Mining. 1000 latest titles from each conference ACL - Annual Meeting of the Association for Computational Linguistics (https://dblp.uni-trier.de/db/conf/acl/) ADBIS - Symposium on Advances in Databases and Information Systems (https://dblp.uni-trier.de/db/conf/adbis/) CIKM - International Conference on Information and Knowledge Management (https://dblp.uni-trier.de/db/conf/cikm/) ECIR - European Conference on Information Retrieval (https://dblp.uni-trier.de/db/conf/ecir/) ICDE - IEEE International Conference on Data Engineering (https://dblp.uni-trier.de/db/conf/icde/) ICDM - IEEE International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/icdm/) KDD - Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/kdd/) PAKDD - Pacific-Asia Conference on Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/pakdd/) SDM - SIAM International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/sdm/) SIGIR - Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (https://dblp.uni-trier.de/db/conf/sigir/) WSDM - Web Search and Data Mining (https://dblp.uni-trier.de/db/conf/wsdm/) WWW - The Web Conference (https://dblp.uni-trier.de/db/conf/www/) Reproducing the paper on Pattern Annotation Team Members: Bipin Chandra Karnati (karnati3@illinois.edu ) Savitha Govindarajan (savitha3@illinois.edu ) Utpal Mondal (umondal2@illiniois.edu) Team Leader: Savitha Govindarajan (savitha3@illinois.edu ) Team Name: Avengers Progress Report: 1. Which tasks have been completed?  Reviewed the paper to understand what is implemented, how it is implemented and the experiments in the paper  Made decision on which of the experiments need to be implemented using the code  Collected the data required to reproduce the paper & working on the required data clean up  Organized a plan and the work split up amongst each team member 2. Which tasks are pending?  Code Implementation to reproduce the paper  Validating the code to conclude the same results as in the Paper 3. Are you facing any Challenges?  No specific open challenges at this point Reproducing the paper on Pattern Annotation Team Members: Bipin Chandra Karnati (karnati3@illinois.edu ) Savitha Govindarajan (savitha3@illinois.edu ) Utpal Mondal (umondal2@illiniois.edu) Team Leader: Savitha Govindarajan (savitha3@illinois.edu ) Team Name: Avengers Paper Chosen: Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 Authors: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai Year Published: 2006 Implementation Language: Python Dataset used in paper: DLBP data set Availability of dataset: Yes. We have the data subset (https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html) & working on getting the exhaustive data set. CourseProject: CS410 - Text Information Systems Generating Semantic Annotations for Frequent Pattern with Context Analysis Project Overview: In this project, we have tried to reproduce the model and results from the following published paper on Pattern Annotation. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. This model will automatically generate such annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary like description for a pattern, finding synonym patterns, discovering semantic relations, ranking patterns, categorizing and clustering patterns with semantics, and summarizing semantic classes of a set of frequent patterns. Experiment from the paper reproduced: Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most representative titles from their publications, and the co-authors and title patterns which are most semantically similar to them. Implementation Approach: Here, the general approach taken to automatically generate the frequent patterns and structured annotations for them by the following steps: 1. Derive the frequent patterns Author/Co-Author pattern from the Database using FP Growth algorithm. 2. Define and model the context of the pattern: * Derive the context units for the Author/Co-Authors by mining the Closed Frequent Pattern to avoid any redundant pattern * Derive the context units for the Titles by mining the Sequential Closed Frequent Pattern using Prefix Span Algorithm * Select context units and design a strength weight for each unit to model the contexts of frequent pattern 3. Derive the list of representative transaction by finding the cosine similarity between the Frequent Pattern & the Frequent Transactions 4. Derive the Semantically Similar Author Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Frequent Author/Co-Author 5. Derive the Semantically Similar Title Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Title Words Installation and Usage: The following packages must be installed for the successful execution: https://test.pypi.org/simple/krovetz Prefixspan, re, csv, pandas, numpy, mlxtend, sklearn Note: To use the Krovetz Stemmer installed, Visual Studio C++ is needed to be installed. Team members had issues running the program without it. Input data: Here the input dataset (DBLP Dataset) is in a specific format. DBLP dataset (a subset of around 12k transactions/titles papers from the proceedings of 12 major conferences in Data Mining; around 1k latest transactions from each of such conference). Each row represents a title presented in a conference. It has 3 fields - id (numeric), title (String) and MergedAuthors (string). The authors and co-authors associated with the title has been merged into a single column for easy analysis. Output: Once the program patternAnnotation.py is executed, it takes the DBLP_Dataset.csv as the input and generates the output.txt file in the same path where source code exists. This output file contains all the closed frequent patterns and their most representative Context Indicators, most representative transactions (capped as 4), Semantically Similar Patterns (SSPs) as per co-author patterns and title term patterns. Each record in the output file represents one closed frequent pattern and their associated details. Implementation Details: Derive the frequent itemsets of Author/Co-Author: * Algorithm used: FP Growth * Output: 64 Frequent Author/Co-Author itemsets (Removed any frequent author without co-Author) Define and model the context of the pattern: For each of the 64 Frequent Itemsets from the above step: * Derive the context units for the Author/Co-Authors: Mined closed Author/Co-Author itemsets from already mining frequent itemsets by removing any itemset with redundant support * Derive the context units for the Title: Algorithm used: Prefix Span; We got limited Title Patterns, hence we did not need micro-clustering to further reduce the titles. * Final Context Indicator: Merged the context units of the above two steps * Context Indicator Weighting: Here we slightly modified our approach to weighting as the mutual information value was not giving the appropriate weightage, based on our analysis & understanding. We use an approach similar to IDF, where when a context unit is present in more transaction, the weightage is reduced & any context unit that uniquely appears in the transaction will have the highest weightage. Derive the list of representative transaction: * Based on the context indicators identified for each of the Frequent Itemset, we identified top 4 transactions with the highest Cosine Similarity Derive the list of Semantically Similar Author/Co-Author pattern: * Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their Author/Co-Author as the SSP Derive the list of Semantically Similar Author/Co-Author pattern: * Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their frequent title pattern as the SSP Video Presentation: https://mediaspace.illinois.edu/media/t/1_hdhp3434 References: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 FP Growth Algorithm implementation: https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3 Prefix Span: https://pypi.org/project/prefixspan/ Cosine Similarity: https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ DBLP Dataset: 12 Major Conferences on Data Mining. 1000 latest titles from each conference ACL - Annual Meeting of the Association for Computational Linguistics (https://dblp.uni-trier.de/db/conf/acl/) ADBIS - Symposium on Advances in Databases and Information Systems (https://dblp.uni-trier.de/db/conf/adbis/) CIKM - International Conference on Information and Knowledge Management (https://dblp.uni-trier.de/db/conf/cikm/) ECIR - European Conference on Information Retrieval (https://dblp.uni-trier.de/db/conf/ecir/) ICDE - IEEE International Conference on Data Engineering (https://dblp.uni-trier.de/db/conf/icde/) ICDM - IEEE International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/icdm/) KDD - Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/kdd/) PAKDD - Pacific-Asia Conference on Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/pakdd/) SDM - SIAM International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/sdm/) SIGIR - Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (https://dblp.uni-trier.de/db/conf/sigir/) WSDM - Web Search and Data Mining (https://dblp.uni-trier.de/db/conf/wsdm/) WWW - The Web Conference (https://dblp.uni-trier.de/db/conf/www/)"
https://github.com/sbitra2/CourseProject	"Text Classification Sarcasm Detection UIUC: CS-410 Course Project Introduction Problem: Generate a model to detect the sarcasm from the list of tweets. Approach: I have taken a language representation model BERT(Bi-directional Encoder Representations from Transformers) with pretrained deep bi-directional representations to train the model by using Google Tensorflow and Keras Deep learning APIs Results: With this approach I have managed to achieve the training accuracy to ~91% and validation accuracy to ~76%, On the competition leaderboard F1 Score: 0.7472 Code WalkThrough Link: https://www.youtube.com/watch?v=dXuhx-kpfaE&feature=youtu.be Demo Link: https://youtu.be/Z3Yu8VzgWMw Resources https://github.com/google-research/bert https://www.tensorflow.org/tutorials/text/classify_text_with_bert https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad Project Proposal: Name: Sirish C Bitra NetIDs: sbitra2 Captain: Sirish C Bitra Team Type: Individual Planned Competition: Text Classification Competition: Twitter Sarcasm Detection. Proposal: Currently, I'm going through the GloVe model with my Technology Review assignment and doing some research on how it suits with the Text classification competition. I believe with the use of neural network classifiers like Bi-directional LSTM with GloVe Embeddings I can achieve the expected results with this competition. Prior Experience: I don't have any prior experience in any of these models. Programming Language: Python Project: Text Classification Competition: Twitter Sarcasm Detection Status: 1. Using Google Collab and TensorFlow2: Tried couple of models with Pre trained BERT and different loss entropy like binary Cross Entropy and Sparse Categorical Entropy. 2. Need to fine tune the model using parse categorical loss entropy which gave better results for me but it is very slow. 3. WIP - performance improvement. Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/seanlai12/CourseProject	"Sean Lai Course Project CS410 Documentation Summary: This project is to improve the functionality of to save next slides in memory so users do not need to load each new slide every click. I have implemented this feature by using in-memory list within python when server is running, to load multiple slide pages at once, and as a queue, the system iterate through the list when the user clicks ""Next"" button on the EducationalWeb page. In the original, the EducationalWeb system would resolve each slide at the moment of user click, which would only allow a page to load one at a time, causing delays and negative user experience. Implementation: I have created a new function on app.py, which is the main function python file for the system, called `buffer_new_slides`. This function will iterate a number of slides, with number determined by the global variable SLIDE_BUFFER_SIZE, to fill up a python list SLIDE_BUFFER_LIST with generated slide renders. With this, users would experience a longer load when they first click through the slides, as the system would buffer multiple slide renders at once and save it to memory, but further iteration through the slides will be very fast as it's in memory. The list will work like a queue, so whenever a slide is read and ready to move on, the rendered slide is removed from the list, and a new one will be buffered for the user. This allows an in-memory speed of slide viewing experience for the user, instead of must loading a rendered slide object with `resolve_slides`, on every click. Please see my video demonstration for the code and operation for more details. Challenges: Because I did not a have a web server to test this on, the localhost server proved difficult to test if my change really made the slide viewing faster. This is due to the fact that original slide viewing was already fast enough since all the slide files were local. However, it is very small difference with speed with new change, and I expect it to be much more noticeable when the slides are up on a web server. Out of Scope: This implementation would only work on ""Next"" slide button, not going backward in pages with ""Prev"". The Prev button would work, however would mess up the order of the list. This can be fixed by implementing similar queuing system for Prev in the same way as Next, however due to time constraints I am only going to demo for Next button. How to use the software: # Documentation/Instructions The following instructions have been tested with Python2.7 on Linux and MacOS 1. You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html 2. Create the index in ElasticSearch by running `python create_es_index.py` from `EducationalWeb/` 3. Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under `EducationalWeb/static` 4. Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under `EducationalWeb/pdf.js/static/slides/` 5. From `EducationalWeb/pdf.js/build/generic/web` , run the following command: `gulp server` 6. On line 38 on `EducationalWeb/app.py`, edit the value here to an Integer for ""number of slides to buffer at a time"". I have set the default to ""5"", so you can also leave it as is. 7. In another terminal window, run `python app.py` from `EducationalWeb/` 8. The site should be available at http://localhost:8096/ Course Project Demo By: Sean Lai CS 410 Improving EducationalWeb Improve the functionality to save next slides in memory so users do not need to load each new slide every click. Faster loading for slides in bulk Better user experience Design and Implementation Original Design and Implementation After Demo Faster? It is hard to prove this on local server, since slide loading is already fast to begin with. However, I added logging message to the code so you can see how often the system renders before and after. However since the rendered slides are saved in a in-memory list, by design it should be faster. Challenges Setting up the environment took forever! Large amount of time spent on understanding the code. Load time on local server vs web server differs a lot, hard to see difference from my feature. Only had time to work on Next slide (going forward), not Prev. However, it should be implemented in a similar fashion (in-memory list rendering) Thank you Sean Lai CS410 Project Progress Report 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? Completion summary of progresses using the Task that I listed in my original proposal, Spike the usage on the original system. 3h Set up environment for the project. 1h (This took much longer than estimate) Look for function and area that needs to be updated. 2h Coding and testing. 10h (About 50% done, 5h left) Collect data for time comparison. 2h Create documentation, report, and presentation material. 2h Currently, I used bulk of my time setting up the environment on Pycharm/terminal, going through the functionality, installing dependencies, and writing few logging lines in the code for better information gathering. I was underestimating the environment setup portion which took me much longer time, where I encountered many difficulties to get the local server to actually load, due to having to reinstall python and troubleshoot few things online. However, I finally got it to work on local server and can test code now which I am happy about. I am also getting more familiar with codebase and have pretty good idea on where to add/update code to complete my project. Pending tasks. Currently I am still not complete in the coding and testing part, but I got good plans laid out on what to do. Currently I am able to manipulate the slide pages on next and prev slides, skip a slide, etc. I am thinking to implement a list or queue dataset to pre-load multiple slides when moving slides. I am still implementing and testing so don't have much more details for now, but don't seem to face any big challenges that I know of. Challenges. I think the biggest challenge is to correctly gather timing for my final report, since the local server and web server version has different timings. However, if I can compare either one of them and find my implementation speed up the program, I should be happy enough to report that as successful. Please let me know if you have questions or need further details on anything. Thanks for reading. Sean Lai CS410 Project Proposal Improving a System: Educational Web System What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. What system have you chosen? Which subtopic(s) under the system? Briefly describe the datasets, algorithms or techniques you plan to use If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Which programming language do you plan to use? Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Sean Lai - seanlai2 (Individual project) Educational Web System. Planning to improve the functionality of allowing downloading slide in bulk. Specifically, I am thinking to implement a buffer loading for previous and next slides so user experiences are better. Currently the system only allow one slide loading at a time and will only load the next or previous slide if user makes a click. Using data structures to store preloaded slides for the interface, possibly a list. Using updated functionality to improve the system on web interface. The dataset will be the slides to load on user clicks. This can be demonstrated by comparing the average time loading a slide in the original system, to the new system. This can be done in a report, and also video demonstration if required. My code will likely update a function within the original system, replacing or adding more onto the original logic. I will fork my own repo and make my update on my own environment. Python, JavaScript, and whatever else is needed. Task and hours rough estimate Spike the usage on the original system. 3h Set up environment for the project. 1h Look for function and area that needs to be updated. 2h Coding and testing. 10h Collect data for time comparison. 2h Create documentation, report, and presentation material. 2h Educational Web - http://timan102.cs.illinois.edu/explanation//slide/cs-410/0 Original Github Repo - https://github.com/CS410Fall2020/EducationalWeb CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Author - Sean Lai Email - seanlai2@illinois.edu CS410 Improving the Educational Web System. This project is to improve the functionality of to save next slides in memory so users do not need to load each new slide every click. Please read the documentation and instruction below on how to use EducationalWeb. Please take a look at Documentation.docx, for more detail on how to use the software and how the software is implemented. Documentation/Instructions The following instructions have been tested with Python2.7 on Linux and MacOS You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server On line 38 on EducationalWeb/app.py, edit the value here to an Integer for ""number of slides to buffer at a time"". I have set the default to ""5"", so you can also leave it as is. In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/"
https://github.com/seuaciuc/CourseProject	"1 CS410 Fall 2020 Course Project Progress Report Author: Thiago Seuaciuc-Osorio E-mail: thiagos2@illinois.edu NetID: thiagos2 Team: Individual Programming Language: Python Topic: Reproducing the following paper on Latent Aspect Rating Analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Completed Tasks The following initial tasks have been completed: * Reviewed paper, identified tasks and planned approach. * Obtained both relevant datasets (hotel and MP3 reviews). * Extracted and reviewed both datasets to understand format and accompanying metadata. Future Tasks * Fully ingest data and implement same pre-processing steps listed in the paper. After this step, it is expected that the dataset statistics will resemble that reported on Table 1 in the subject paper.  Expected timeframe: complete by December 2 * Implement posterior inference and model estimation steps (sections 4.1 and 4.2 of subject paper).  Expected timeframe: complete by December 7 * Apply the topical Latent Aspect Rating Analysis Model (LARAM) to both datasets (hotel and MP3 reviews) and calculate selected relevant metrics for assessment. Only the experimental results related to the subject method (LARAM) will be reproduced; experimental results obtained with other methods for comparison and reported in the paper (such as LDA, sLDA, LRR, etc) will not be reproduced. Specifically, the following results will be reproduced: o The LARAM results shown in table 2 o The LARAM results shown in table 4 o The LARAM results shown in figure 3 Two sets of results will not be reproduced: o The results in table 3, since they depend on ""ground truth"" generated by a LDA model. o The results in table 5, from the experiment where all reviews are concatenated and which was performed only to allow comparison with a Bootstrap+LRR model, since it is a degeneration of the more general case and similar metrics are already computed as part of table 4.  Expected timeframe: complete by December 10 2 * Prepare code documentation and presentation.  Expected timeframe: complete by December 14 Challenges Following the initial review and planning for the project, the following challenges were encountered: * Table 1 in the subject paper lists 2,232 different hotels reviewed. However, in reviewing the available dataset listed in the provided reference, review files for only 1,850 hotels were initially found. This does not pose a problem for the execution of the project, but it may lead to slightly different results than obtained in the subject paper. * The model equations described in section 4 of the subject paper are not fully clear or complete, as some are just referenced in other sources. These sources have been retrieved, but further study and analysis are necessary to fully determine the equations defining the LARAM model implementation. CS410 Fall 2020 Course Project Proposal Author: Thiago Seuaciuc-Osorio E-mail: thiagos2@illinois.edu NetID: thiagos2 Team: Individual Programming Language: Python Topic: Reproducing the following paper on Latent Aspect Rating Analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 In this project, the Latent Aspect Rating Analysis generative model will first be implemented as described in the paper above and then applied on the same hotel and MP3 player review datasets, effectively replicating the results of the paper. The relevant datasets have been obtained from http://timan.cs.uiuc.edu/downloads.html CS410 Fall 2020Course ProjectThiago Seuaciuc-OsorionetID: thiagos2Topic Paper*HongningWang, Yue Lu, and ChengXiangZhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505*With considerable material from reference [3] in the paper on LDA:D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:993-1022, 2003FrameworkData ProcessingModel BuildingAnalysisRaw DataProcessed File (.pkl)Model File (.pkl)*Data transfer through files*No command line interface: inputs in first lines of scriptsData ProcessingData ProcessingRaw DataProcessed File (.pkl)*Input: raw file*Output: processed file*Reviews: list of dictionaries*ReviewText: review content (list of words)*'Author' (if available)*'Product'*'Date'*'Rating': list of floats; first is overall rating*Vocabulary: list of words in corpus*Term-Document Matrix: count of each term (column) in each document (row)*Processing:*Lower case =remove punctuation =word tokenize =remove stopwords(NLTK) =remove non-alphabetical terms*Filter reviews with less than 50 words, and terms appearing in less than 10 reviews*All files in the \datafolderData ProcessingData ProcessingRaw DataProcessed File (.pkl)*MP3 & Hotels reviews in different formats; need different processing*processHOTELreviews.py*processMP3reviews.py*processMP3reviews_split.py*Splits reviews based on rating (low/high)Provide name of raw input file hereProvide name of output file to be created hereData ProcessingData ProcessingRaw DataProcessed File (.pkl)*Main available processed data files:*MP3reviews_low_100.pkl: processed data of a random sub-sample of 100 reviews with low rating (3 or lower).*MP3reviews_high_100.pkl: processed data of a random sub-sample of 100 reviews with high rating (higher than 3).*HotelReviews_100.pkl: processed data of a random sub-sample of 100 hotel reviews*Main available raw data files:*amazon_mp3_redux.txt: a small sub-sample of the amazon review dataset in its raw format that can be used to test the data processing codes.*Associated processing script: processMP3reviews.pyor processMP3reviews_split.py*Test_redux(folder): a small sub-sample of the hotel review dataset in its raw format that can be used to test the data processing codes.*Associated processing script: processHOTELreviews.pyModel Building*Follows process in subject paper:*Initialize corpus-level parameters*Compute review-level parameters*Initialize review-level parameters*Iteratively update until convergence*Corpus-level parameters are held constant in this process*Update corpus-level parameters*Review-level parameters are held constant*Recompute log-likelihood*Iterate until convergence*Input: processed data file (in \datafolder)*Output: model file saved to \modelsfolder*Corpus-level parameters*Review-level parametersModel BuildingProcessed File (.pkl)Model File (.pkl)Model Building*estimateModel.py*Data files should be in the \datafolder*Model file will be saved to the \modelsfolderModel BuildingProcessed File (.pkl)Model File (.pkl)Provide name of processed data file hereProvide name of output file to be created hereDefine number of aspects hereModel Building*Available models:*MP3model_low_100_3.pkl*Model built with 3 aspects on dataset with 100 MP3 reviews with low rating*Associated processed data file: MP3reviews_low_100.pkl*MP3model_high_100_3.pkl*Model built with 3 aspects on dataset with 100 MP3 reviews with high rating*Associated processed data file: MP3reviews_high_100.pkl*HotelModel_100_7.pkl*Model being built with 7 aspects on dataset with 100 hotel reviews*Associated processed data file: HotelReviews_100.pklModel BuildingProcessed File (.pkl)Model File (.pkl)Analysis*For various purposes*May take either or both of:*Processed data file*Model file*Available codes:*getStats.py: computes basic stats on the review data*getTopAspectWords.py: retrieves the top words of each aspect (based on aspect word distribution)AnalysisProcessed File (.pkl)Model File (.pkl)Analysis*getStats.py*Script will print on the terminal:*The number of reviews in the file*The number of unique items (products) reviewed*The average length (and standard deviation) of the reviews*The average and standard deviation of the overall ratings*No other outputs or files created*Data file should be in the \data folderAnalysisProcessed File (.pkl)Provide name of processed data file here*getTopAspectWords.py*Script print array of words to the terminal*No other outputs or files created*Data file should be in the \data folder*Model file should be in the \modelsfolderAnalysisAnalysisProcessed File (.pkl)Provide name of processed data file hereModel File (.pkl)Provide name of output file to be created hereDefine number of words per aspect hereSome Results -Review Statistics*Compared to the reported results (see Table 1 in subject report):*Slightly fewer reviews. For instance, Table 1 reports 2,232 reviewed hotels, but the available dataset only contains files for 1,850 (one is filtered out)*Higher average review length. This is likely because of the slightly different processing. For instance, here the NLTK stopwordswere used, which will differ a little from that was used in the paper.*Ratings statistics are very similar.Dataset# Items# ReviewsAvg Length (std)Avg Rating (std)MP367616012123.06 (98.00)3.75 (1.42)Hotels184947750125.73 (99.02)3.96 (1.22)Some Results: Top Words in MP3 ReviewsLow RatingHigh RatingZuneLikeWarrantyWouldHoursSoftwareBoughtGoodSoftwareUseGreatAlsoEverythingTimeWorkNewUseOneGetProductAppleZuneItunesLikePeopleOneBuyDeviceStillSoundLikeGetImGreatPlayerQualityProblemScreenMusicEasyBatteryGetOneUseBatteryGoodLikeMuchIpodIpodUnitPlayerOneGoodPlayerplayerwouldipodmusicplayerSome Results: Top Words in MP3 Reviews*Few similarities with Table 2 in the subject paper:*Low: problem, time, warranty*High: easy, sound, quality*Despite a few similarities, this list is mainly different from the one in Table 2 in the subject paper. Reasons could be:*Fewer aspects: the paper modeled 20 aspects and displayed the top 3, while here only 3 aspects were modeled. The higher number of aspects in the paper allow for better topic definition for each aspect, whereas here, the shown 3 topics need to account for all the content in the corpus.*The much smaller dataset. This was built on 100 reviews, while the paper used 16,680 divided between the two sub-groups (low/high rating)*The different list of stopwords; the list used here may have left more common English words than the list used in the paper.Note:time for model computation is the reason to have reduced the dataset (number of reviews) and the number of aspects modeled.Challenges*Model complexity*A number of high dimensional numerical optimizations at repeated iterations leads to high computation time to build models*This gets worse as the dataset and number of aspects increase*This has hampered the development of the model on the hotel dataset, where at least 7 aspects are needed for meaning assessment*Model build code has not finished running (on set with 100 reviews)*Not possible yet to perform the aspect rating analysis*Even when the model finishes, the low number of reviews will likely make the results not very robust*To solve this, efforts are needed for code optimization*Most of the time in the project was spent on research to understand the model and approach to be able to make an initial implementation of it*Better computation resources would also helpSuggested Testing ProcedureThe steps below allow for all scripts provided to be tested without taking too much time. They are already set with the inputs corresponding to this, so they can be run without changes.*Data processing scripts*RunprocessMP3reviews.pyto process the MP3 reviews inamazon_mp3_redux.txt. This will generate theMP3reviews_redux.pkl. All these files are in the\datafolder.*RunprocessHOTELreviews.pyto process the hotels reviews in the folder\Texts_redux. This will generate theHotelReviews_redux.pkl. All these files are in the\datafolder.*Model building scripts*RunestimateModel.pyon the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model fileMP3model_redux.pkl. The suggested number of aspects is 3.*Analysis scripts*Run thegetStats.pyonMP3reviews_high_100.pklto obtain the statistics on that smaller set of reviews.*Run thegetTopAspectWords.pyusing the dataMP3reviews_high_100.pkland modelMP3model_high_100_3.pklto obtain the top 10 words in each of the 3 aspects of this model on these reviews. CS410 Fall 2020 Course Project Thiago Seuaciuc-Osorio netID: thiagos2 Topic Paper Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 With considerable material from reference [3] in the paper on LDA: D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:993-1022, 2003 Framework Raw Data Processed File (.pkl) Model File (.pkl) Data transfer through files No command line interface: inputs in first lines of scripts Data Processing Raw Data Processed File (.pkl) Input: raw file Output: processed file Reviews: list of dictionaries ReviewText: review content (list of words) 'Author' (if available) 'Product' 'Date' 'Rating': list of floats; first is overall rating Vocabulary: list of words in corpus Term-Document Matrix: count of each term (column) in each document (row) Processing: Lower case  remove punctuation  word tokenize  remove stopwords (NLTK)  remove non-alphabetical terms Filter reviews with less than 50 words, and terms appearing in less than 10 reviews All files in the \data folder Data Processing Raw Data Processed File (.pkl) MP3 & Hotels reviews in different formats; need different processing processHOTELreviews.py processMP3reviews.py processMP3reviews_split.py Splits reviews based on rating (low/high) Provide name of raw input file here Provide name of output file to be created here Data Processing Raw Data Processed File (.pkl) Main available processed data files: MP3reviews_low_100.pkl: processed data of a random sub-sample of 100 reviews with low rating (3 or lower). MP3reviews_high_100.pkl: processed data of a random sub-sample of 100 reviews with high rating (higher than 3). HotelReviews_100.pkl: processed data of a random sub-sample of 100 hotel reviews Main available raw data files: amazon_mp3_redux.txt: a small sub-sample of the amazon review dataset in its raw format that can be used to test the data processing codes. Associated processing script: processMP3reviews.py or processMP3reviews_split.py Test_redux (folder): a small sub-sample of the hotel review dataset in its raw format that can be used to test the data processing codes. Associated processing script: processHOTELreviews.py Model Building Follows process in subject paper: Initialize corpus-level parameters Compute review-level parameters Initialize review-level parameters Iteratively update until convergence Corpus-level parameters are held constant in this process Update corpus-level parameters Review-level parameters are held constant Recompute log-likelihood Iterate until convergence Input: processed data file (in \data folder) Output: model file saved to \models folder Corpus-level parameters Review-level parameters Processed File (.pkl) Model File (.pkl) Model Building estimateModel.py Data files should be in the \data folder Model file will be saved to the \models folder Processed File (.pkl) Model File (.pkl) Provide name of processed data file here Provide name of output file to be created here Define number of aspects here Model Building Available models: MP3model_low_100_3.pkl Model built with 3 aspects on dataset with 100 MP3 reviews with low rating Associated processed data file: MP3reviews_low_100.pkl MP3model_high_100_3.pkl Model built with 3 aspects on dataset with 100 MP3 reviews with high rating Associated processed data file: MP3reviews_high_100.pkl HotelModel_100_7.pkl Model being built with 7 aspects on dataset with 100 hotel reviews Associated processed data file: HotelReviews_100.pkl Processed File (.pkl) Model File (.pkl) Analysis For various purposes May take either or both of: Processed data file Model file Available codes: getStats.py: computes basic stats on the review data getTopAspectWords.py: retrieves the top words of each aspect (based on aspect word distribution) Processed File (.pkl) Model File (.pkl) Analysis getStats.py Script will print on the terminal: The number of reviews in the file The number of unique items (products) reviewed The average length (and standard deviation) of the reviews The average and standard deviation of the overall ratings No other outputs or files created Data file should be in the \data folder Processed File (.pkl) Provide name of processed data file here getTopAspectWords.py Script print array of words to the terminal No other outputs or files created Data file should be in the \data folder Model file should be in the \models folder Analysis Processed File (.pkl) Provide name of processed data file here Model File (.pkl) Provide name of output file to be created here Define number of words per aspect here Some Results - Review Statistics Compared to the reported results (see Table 1 in subject report): Slightly fewer reviews. For instance, Table 1 reports 2,232 reviewed hotels, but the available dataset only contains files for 1,850 (one is filtered out) Higher average review length. This is likely because of the slightly different processing. For instance, here the NLTK stopwords were used, which will differ a little from that was used in the paper. Ratings statistics are very similar. Some Results: Top Words in MP3 Reviews Some Results: Top Words in MP3 Reviews Few similarities with Table 2 in the subject paper: Low: problem, time, warranty High: easy, sound, quality Despite a few similarities, this list is mainly different from the one in Table 2 in the subject paper. Reasons could be: Fewer aspects: the paper modeled 20 aspects and displayed the top 3, while here only 3 aspects were modeled. The higher number of aspects in the paper allow for better topic definition for each aspect, whereas here, the shown 3 topics need to account for all the content in the corpus. The much smaller dataset. This was built on 100 reviews, while the paper used 16,680 divided between the two sub-groups (low/high rating) The different list of stopwords; the list used here may have left more common English words than the list used in the paper. Note: time for model computation is the reason to have reduced the dataset (number of reviews) and the number of aspects modeled. Challenges Model complexity A number of high dimensional numerical optimizations at repeated iterations leads to high computation time to build models This gets worse as the dataset and number of aspects increase This has hampered the development of the model on the hotel dataset, where at least 7 aspects are needed for meaning assessment Model build code has not finished running (on set with 100 reviews) Not possible yet to perform the aspect rating analysis Even when the model finishes, the low number of reviews will likely make the results not very robust To solve this, efforts are needed for code optimization Most of the time in the project was spent on research to understand the model and approach to be able to make an initial implementation of it Better computation resources would also help Suggested Testing Procedure The steps below allow for all scripts provided to be tested without taking too much time. They are already set with the inputs corresponding to this, so they can be run without changes. Data processing scripts Run processMP3reviews.py to process the MP3 reviews in amazon_mp3_redux.txt. This will generate the MP3reviews_redux.pkl. All these files are in the \data folder. Run processHOTELreviews.py to process the hotels reviews in the folder \Texts_redux. This will generate the HotelReviews_redux.pkl. All these files are in the \data folder. Model building scripts Run estimateModel.py on the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model file MP3model_redux.pkl. The suggested number of aspects is 3. Analysis scripts Run the getStats.py on MP3reviews_high_100.pkl to obtain the statistics on that smaller set of reviews. Run the getTopAspectWords.py using the data MP3reviews_high_100.pkl and model MP3model_high_100_3.pkl to obtain the top 10 words in each of the 3 aspects of this model on these reviews. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. General Info Project material is structured in 3 folders: - Data: contains all data, both raw and processed. Smaller sub-samples of the datasets are provided. - Models: contains all models (pickle files with estimated model paramters). - Codes: all scripts are here. Each folder contains a README.md with specific information about their contents and structure. All code is done in Python 3.8. Data (information from one step to another) is shared through pickle files, so typically the codes take one file as input and generate others as output. Codes are set to the folder structure of this repository, so it needs to be maintained to be able to run the scripts without modification. Interface There is no command line interface built for this yet. Inputs are provided directly in the files, by modifying the first few lines of the scripts. The most common inputs are clearly identified in the beginning of the scripts. Other ""hyper-parameters"" (such as maximum number of iterations, minimum review length, etc) will appear immediately after the primary input section of the code. These typically do not need to be changed unless fine-tuning is desired. Whenever the first import command is reached, all possible inputs are done. In general, the scripts are ready to be run on some of the sample files provided if the folder structure of this repository is maintained. Most of the information in this readme and the others is summarized in the accompanying presentation slides (CS410_Fall2020_CourseProject_Tutorial.pdf or .pptx). That is probably the best place to start. Suggested Testing Procedure Because of the time for computation and size of some of the files, not all aspects of the project can be reproduced. Below is a suggested testing procedure that uses all scripts in the project to assess their functionality. The scripts provided are set to run the steps in this procedure without change if the folder structure is maintained. Data Processing Scripts Processing the entire datasets take considerable time and resources. This has been done, but the resulting files are larger than the allowable limite in GitHub. To test the codes, you can run the data processing scripts on the provided smaller datasets. These two steps will run the scripts provided to process each of the two datasets: - Run processMP3reviews.py to process the MP3 reviews in amazon_mp3_redux.txt. This will generate the MP3reviews_redux.pkl. All these files are in the \data folder. - Run processHOTELreviews.py to process the hotels reviews in the folder \Texts_redux. This will generate the HotelReviews_redux.pkl. All these files are in the \data folder. Model Building Building the model takes time. Because of this, a pre-model was built on a random sample of 100 reviews. That can be done, but it still takes a little time. To test the functionality of the model building code, you can build a model on one of the reduced datasets. - Run estimateModel.py on the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model file MP3model_redux.pkl. The suggested number of aspects is 3. Analysis Run the getStats.py on MP3reviews_high_100.pkl to obtain the statistics on that smaller set of reviews. Run the getTopAspectWords.py using the data MP3reviews_high_100.pkl and model MP3model_high_100_3.pkl to obtain the top 10 words in each of the 3 aspects of this model on these reviews."
https://github.com/shashivrat/CourseProject	Project Proposal - Text Classification Competition 1. Team Name: SSW Classifiers Members: * Saravana Somasundaram (captain) - ss129 * Shashivrat Pandey - spandey6 * Walter Tan - wstan2 2. Competition - Text Classification 3. Neural Networks: a. Convolutional Neural Network (CNN) b. Recurrent Neural Network (RNN) c. Hierarchical Attention Network (HAN) 4. References: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f c. https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn 5. The programming language we will be using is Python. Our group is prepared to learn state-of-the art network classifiers. We are excited to learn techniques to improve our ML skillset and will apply what we learned to our current/future work projects. Some Deep Learning frameworks we've heard of include PyTorch, TensorFlow, Kearas, and Sonnet. TensorFlow and PyTorch seem to be the most popular and used by many users and institutions worldwide. Our group has never worked with these technologies, but are excited to learn these new technologies for this competition. We will be using Python for this project and we will try our best to come up with an optimized code to improve the performance of application. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/sitajothi/CourseProject	"An overview of the function of the code Our project aims to analyze stock sentiment from Twitter data to help our users understand whether or not to buy or sell stocks. Understanding sentiment from tweets is very helpful as it provides textual context to the market outlook of different stocks. We started by running a pre-existing sentiment analyzer, Vader, to have a baseline performance. Then, we did research into different methods of sentiment analysis to build our own sentiment analyzer, which we trained on a kaggle dataset, and later tested on the Twitter API using recent tweets. We wanted our user to easily understand the sentiment of a stock's set of tweets, so we added a pie graph as a visualization for user's to make the stock purchasing decisions. How software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. We can consider the code to be in two different portions: creating and training the sentiment analyzer and actually running it with the tweets pulled from the Twitter API. For our sentiment analyzer, we used knowledge from class to create a sentiment analyzer that utilizes TF-IDF and a Bag-Of-Words model. From MP1, we realized that tokenizing and lemmatizing the words and then using n-gram on words would be effective. We tried many tutorials to understand how to use SpaCy, but utilized https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/  the most as it incorporated TF-IDF and the Bag-Of-Words model. While testing the model, we also played around with the n-gram_range that was fed into the BOW-vector and found that unigrams gave the best results. Figure 1: Model Results We used a kaggle dataset to train our model, which performed above the Vader benchmark. We ended up using Logistic Regression because it gave the best result compared to the other models on Scikit-Learn. We then used the Twitter API to pull the most recent tweets from Twitter pertaining to a certain stock ticker (input as '$tickerName'), run it through the model, and then output a pie chart that shows the user the percentage of tweets that had a positive and negative connotation. Based on this chart, the user can then make a judgement as to whether or not they want to short or long the stock. In order to test the overall precision/recall/F1 of the entire program, we established the ground truths of the tweets that were pulled by ourselves and then calculated the metrics. Figure 2: Performance metrics of entire program Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Firstly, the user must have access to Jupyter notebook. There is a browser version, which the user can find with a google search for ""Jupyter notebook browser."" Our application also uses the Twitter API. To be able to receive real time information, the user must create a developer portal on Twitter and gain Access Tokens and Keys. Additionally, the user must save their credentials in a json format to a file called:  twitter_credentials.json  so that the application can pull them. (e.g. {""CONSUMER_KEY"": """", ""CONSUMER_SECRET"": """", ""ACCESS_TOKEN"": """", ""ACCESS_SECRET"": """"}) Once these two steps are complete, the user can proceed to the last cell of the Jupyter notebook to change the ticker name. The ticker name must be in a string format with a preceding '$'. To run our application, the user can click on Cell -> Run All, and they will see a pie graph with the sentiment distribution corresponding to that stock (an example can be seen below). Figure 3: Example of Sentiment Distribution of Tesla Brief description of contribution of each team member in case of a multi-person team. Both members of the team had a very nice and collaborative experience with this project. In the first few weeks, both Asha and Sita researched different methods of sentiment analysis and tried to find tutorials that would be most helpful for them while creating it. Once the research phase was over, Asha mainly worked with coding the first part of the project as well as finding the benchmark metrics with Vader while Sita worked with the Twitter and graphing portion as she had a Twitter account. The full breakdown of tasks can be found below. Task Asha Sita Researching about sentiment analysis/analyzer tutorials yes yes Vader benchmark with Kaggle dataset yes Coding sentiment analyzer yes yes Fine-tuning sentiment analyzer yes Research about pulling tweets yes yes Coding Real-Time Twitter Pull yes Coding Data Visualization yes Creating ground truths for testing/accuracy purposes with the Twitter results yes yes Progress made thus far: So far, we have completed what we believe to be the hardest part of the project - creating and training the sentiment analyzer and the testing model. We first ran the Vader SentimentAnalyzer on a Kaggle Dataset to use as a benchmark. Its F1 score came out to be 0.40, so we wanted to make sure that the analyzer that we create beats this score. When we used a limit of 200 out of the +5000 columns in the Kaggle Dataset, its F1 score was 0.475. We have tried to run it on the entire set and got an F1 score of 0.54! We probably will not be running it on a dataset that big again since it took a very long time, and crashed multiple times before a successful run. Remaining tasks: *Connect to Twitter API to test model on real-time data *Figure out how to filter tweets based on a stock ticker that the user inputs *Run the test model created on these tweets and find if the sentiment regarding the particular stock is positive or negative and if the user should buy/sell respectively *Verify the output of the model (will have to establish ground truths ourselves) Challenges/issues (being) faced: *Training/testing Kaggle Dataset was very large, causing the program to crash multiple times *Precision/Recall/F1_score are all turning out to be the same number on each run. We need to figure out if this is accurate. Project coordinator: Sita Jothishankar ( slj2@illinois.edu ), Asha Agrawal ( ashaa2@illinois.edu ) What is your free topic? Twitter sentiment analysis for stock price direction. Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? What is the function of the tool and its output? This tool will scrape Twitter for a stock ticker name and then perform sentiment analysis to see how much positive and negative news there is regarding the term. Who will benefit from such a tool? Our main audience will be banks, investors, and shareholders. After finding the ratio of positive/negative news, they can incorporate this data into their views to make more robust and preemptive decisions for different commodities, stocks, companies, etc. This can help investors either buy low - sell high, or mitigate losses early on. Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? We were not able to find an actual tool that found the sentiments, but found a study that showed that there is a correlation between between tweets and stock performance: https://towardsdatascience.com/stock-prediction-using-twitter-e432b35e14bd  and http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf . With this supporting study, we believe that our project has a potential to make an impact. What existing resources can you use? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py  and https://github.com/satishrath185/Movie-Review-Sentiment-Analysis/blob/master/Sentiment%20Analysis.ipynb  can give us a launching pad for a sentiment analyzer. We will be adding to it to create a more robust analyzer. What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We will use cross validation to create a sentiment analyzer on a dataset from kaggle: https://www.kaggle.com/yash612/stockmarket-sentiment-dataset Once we have created the analyzer we will use the twitter API to perform sentiment analysis on current tweets to provide users with the most up to date information. As mentioned above, we will be using existing resources and studies to help us figure out which algorithms would be best to use. How will you demonstrate the usefulness of your tool? We will want to show correlation between stock price and the sentiment of news stories. With this correlation, we will be able to demonstrate the usefulness as the analyzer will provide our audience a preemptive measure for their stocks. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. There are 2 students on this team, so we will work on this project for 40 hours. We first have to research what algorithm we plan to use to create the analyzer and then we have to implement the algorithm. Once this is done, we have to test and train our analyzer. After this, we will use our analyzer to perform sentiment analysis on current tweets and create a user interface for the user to interact with the analyzer for most up to date information. Each part of the project (listed below in the rough timeline) will be approximately 10 hours. A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) Part 1:  Oct 26 - Nov 2 Research algorithms and learn more about sentiment analysis Part 2:  Nov 2 - Nov 16 Create and train sentiment analyzer Part 3:  Nov 16 - Nov 25 Perform sentiment analysis on current tweets using our sentiment analyzer and twitter API Part 4:  Nov 25 - Dec 9 Create user interface for user to interact with analyzer + write out documentation of our project Overview of our Stock Sentiment Analyzer Our project aims to analyze stock sentiment from Twitter data to help our users understand whether or not to buy or sell stocks. Understanding sentiment from tweets is very helpful as it provides textual context to the market outlook of different stocks. We started by running a pre-existing sentiment analyzer, Vader, to have a baseline performance. Then, we did research into different methods of sentiment analysis to build our own sentiment analyzer, which we trained on a kaggle dataset, and later tested on the Twitter API using recent tweets. We wanted our user to easily understand the sentiment of a stock's set of tweets, so we added a pie graph as a visualization for user's to make the stock purchasing decisions. Stock Sentiment Analyzer Setup and Usage Firstly, the user must have access to Jupyter notebook. There is a browser version, which the user can find with a google search for ""Jupyter notebook browser."" Our application also uses the Twitter API. To be able to receive real time information, the user must create a developer portal on Twitter and gain Access Tokens and Keys. Additionally, the user must save their credentials in a json format to a file called: twitter_credentials.json so that the application can pull them. (e.g. {""CONSUMER_KEY"": """", ""CONSUMER_SECRET"": """", ""ACCESS_TOKEN"": """", ""ACCESS_SECRET"": """"}) Once these two steps are complete, the user can proceed to the last cell of the Jupyter notebook to change the ticker name. The ticker name must be in a string format with a preceding '$'. To run our application, the user can click on Cell -> Run All, and they will see a pie graph with the sentiment distribution corresponding to that stock."
https://github.com/sjma3/CourseProject	"1) Which tasks have been completed? As of now, little actual code has been written, but solid foundational knowledge has been established in improving this UI with past examples that are similar in scope/nature. In particular, the JavaScript plugin Infinite Scroll comes to the foreground as a solid candidate to work off of. However, it remains to be seen if Infinite Scroll can work feasibly in the actual implementation of this improvement. 2) Which tasks are pending? Feasibility has yet to be confirmed with Infinite Scroll as a plug-in for the UI implementation/improvement. However, if Infinite Scroll cannot be integrated with this program, a similarly functioning minimally-viable solution will be created or utilized/cited in the native language of the ExpertSearch system. After developing a solution it will be rigorously tested to ensure that it works for as many edge cases as the scope of this project allows before the final deadline. 3) Are you facing any challenges? As of now there are no particular challenges that are outside of the bounds of expectations for a project of this nature. If there are any in the future, they may be noted when the time comes. Improvement of the ExpertSearch System Group: Steven Ma (Captain) NetID: sjma3 This project qualifies under option 2, improving an existing system. For this project, I would like to add a function to the ExpertSearch System. This added function is auto-scroll of results when a user looks through the query. This means that when the user reaches the bottom of the page, the page will automatically populate new results onto the end of it, resulting in a much smoother experience than having to repeatedly click ""Load more"" for the results. The function will work if a user is able to enter their query and get a small batch of results, then when reaching the bottom of their first batch of results, get more results automatically. This will continue until no more results remain to be shown. In reality, we will be getting a comprehensive list of results and simply adding these results in chunks to the page for the user to see. I do not want the web page to pre-load all of them beforehand and have it hidden; that would be an unrefined and expensive implementation. Rather, I wish to add to the end of the page as the user needs. Further research into this will dictate whether such an approach is feasible or not. To implement such functionality successfully, I currently plan to use the same code as the original project, Python. Ideally, I would be able to work through the current repository and extract out the necessary files, make changes to them, and return the results. I believe this implementation will take five hours to search through/comprehensively understand the working code, five hours to research similar approaches like mine and determine what is and isn't feasible for the working library, another three or four hours to build out the functionality, and an additional six or seven hours to make tests and ensure proper functionality. Thus, this will take 20 hours to complete, in my reasonable belief. Note that this project falls under 3.3, ""Adding an unlisted function to a listed system."" ""At the final stage of your project, you need to deliver the following: Your documented source code. Explain how your code communicates with or utilize the system. A demo that shows your implementation actually works."" ExpertSearch Setup (Copy-Paste from https://github.com/CS410Fall2020/ExpertSearch/) To run the code, run the following command from ExpertSearch (tested with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ Live Demo There will not be live demos scheduled for this project, unless a reviewer is absolutely unable to get the code running on their own. Instead, there is a video available to view with UIUC school credentials at https://drive.google.com/file/d/10HNOJZRPQ2lLqYDfIC213dDoGAChQEv7/view?usp=sharing. Please note the quality through video preview is quite low; downloading the video will result in higher quality results, though core functionality can still be verified at lower quality. It should be noted that the page that is shown in the first part is the current ExpertSearch website http://timan102.cs.illinois.edu/expertsearch/. The added functionality program is run locally through my personal computer. We can see how the new functionality allows users the convenience of not having to click the ""Load More"" button, while the program is simultaneously not burdensome by loading all the results at once. If reviewers want to personally run the code, they can simply download the repository and run it per the instructions above (may need to pip install some packages) to achieve/replicate the results. If issues occur, please message/email me as the author and we will get things sorted out. Please not that you should absolutely use Linux or MacOS, as gunicorn will not run in Windows, and install Python 2.7 (not any Python3 distributions). I personally used Ubuntu in developing this project. If, for any reason, one needs to access the ExpertSearch Github, it can be found at https://github.com/CS410Fall2020/ExpertSearch/ Explanation of how code utilizes system The code's primary functionality is in-built to the system, such that if pushed to the master project of ExpertSearch, it would integrate perfectly with no issues. The simple change made was to add a function that checked for if the user's screen was close to the bottom; if so, it would scroll automatically. One caveat of this approach is that it will not work until the user first clicks the ""Load More"" button. However, after contemplating, it was decided this was actually the correct functionality; if a user clicks load more, we can assume that only then they want to see more results. Perhaps some users may only wish to see the first five results and not be bombarded with an excessive number of results until they click the ""Load More"" button themselves."
https://github.com/skuretski/CourseProject	"Susan Kuretski skure2@illinois.edu CS410 - Fall 2020 Course Project Progress Report Overview This course project uses a dataset from Kaggle [ https://www.kaggle.com/sayangoswami/reddit-memes-dataset ] to perform optical character recognition on Reddit memes and do cluster analysis. The original proposal stated doing sentiment analysis, but as the reviewer suggested I have switched to cluster analysis via K-means since upvotes and downvotes are affected by multiple factors. I hope cluster analysis on these memes will uncover unlikely themes. Tasks Completed Tasks completed to this day are: *Download and clean data *Uploaded images to Google Cloud Platform storage *Performed optical character recognition with Google Cloud Vision and Translate to determine which language *Stored results in GCP Pending Tasks *Evaluation of OCR data *Processing data to do cluster analysis - make each meme into a term vector *Perform cluster analysis *Plot out clusters *Evaluation *Determining a ""good"" number of clusters via elbow method *Silhouette analysis to see how ""good"" the clusters are Challenges My biggest challenge is my unfamiliarity with K-means and unsupervised learning. I chose K-means since I don't have a ground truth or labelled dataset. I pondered halving the dataset of ~3,000 images to label it, but I don't see it as the best use of my time for this project. While K-means may seem straightforward for those experienced with it, I would imagine it will take some time for me to fine tune the number of clusters, perform evaluation, and plot the results out. As it stands now, I have spent about four hours doing the completed tasks. Another challenge is working with OCR text data which has originated from memes, which often is sarcastic, misspelled, and infers cultural or societal knowledge. There are multiple layers of ambiguity and room for error. Susan Kuretski skure2@illinois.edu CS410 - Fall 2020 Course Project Proposal Team Members Team of one: Susan Kuretski (de facto captain) -  skure2@illinois.edu Description - Option 5: Free Topic Task The objective of this project is to do sentiment analysis on images, also known as memes, from Reddit. Using Google Vision OCR, the characters from the image will be processed to tokenized strings/words, essentially this transforms the text from the meme into a bag of words. Using a probabilistic approach to sentiment analysis, I would ideally like to use naive Bayes with Laplace estimation to avoid the assignment of zero probabilities. I have never implemented an application using naive Bayes, so if this ends up being a rabbit hole, I may have to switch gears to a probabilistic semantic analysis (PLSA). Importance/Interesting In general, sentiment analysis is useful in determining users' feelings and attitudes towards certain items, whether it is a review, comment, or product. In regards to Reddit, it would be interesting to see which memes gather positive or negative sentiment and whether it correlates to upvotes or downvotes. Planned Approach (Tools, systems, datasets, evaluation) with Time Estimates Task Time Estimate 1.Get dataset from Kaggle here: https://www.kaggle.com/sayangoswami/reddit-memes-dataset - 2.Do a data cleaning pass to ensure all URLs of images are seemingly correct in terms of structure, downvotes and upvotes are integers >= 0. 0.5 hour 3.The dataset has 3327 images to be downloaded and/or stored in the cloud. Iterate through each entry in the dataset to fetch the image and store. Discard invalid images. 3 hour 4.Run images through Google Vision OCR. https://cloud.google.com/vision/docs/ocr 3 hour 5.Evaluate accuracy 6.Store results of OCR. I want to minimize the need for repeat OCR processing since it can become expensive. 1 hour 7.Transform data from OCR to usable dataset for naive Bayes (bag of words, maybe try n-grams). 4 hour 8.Define and fit the model. Use scikit-learn for Python. 10 hour ** No previous experience with naive Bayes or scikit-learn 9.Evaluate model using scikit-learn metrics and comparing with upvote/downvotes. 4 hour ** No previous experience with evaluation of this 10.Attempt to make the model better (iterate steps 6 - 8). 10+ hours? Total: 35.5 hrs Expected Outcomes The expected outcomes are: 1.Have a final percentage of accuracy from using naive Bayes with hopes of it being greater than 50% 2.Implement improvements to accuracy if initial accuracy <= 60% 3.Possible factors affecting results: slang and intentional misspellings in memes, inaccuracy of OCR Programming Languages and Systems Python with scikit-learn AWS s3 storage or Google Cloud Storage Google Vision API CS410 Fall 2020 Course Project Written by Susie Kuretski (skure2@illinois.edu) Video Link here YouTube (https://www.youtube.com/watch?v=kLedDGQIZyA) The best way to contact me is via Slack in #cs-410-text-info-syst @Susie Kuretski You can also contact me at skuretski@gmail.com or skure2@illinois.edu (not as quick). Topic Utilizing Google Vision's optical character recognition to perform K-means cluster analysis on Reddit memes Overview From this Kaggle dataset, which includes data for ~3300 Reddit memes, I extracted the images and uploaded them to a Google Cloud Platform storage bucket. Then, I ran these images through optical character recognition and translation using Google Vision and Translate. Link to API docs here. Once this was done, the results were stored in a GCP bucket as JSON data. An example would be: The starting image The JSON result after OCR { ""src_lang"": ""en"", ""text"": ""When you don't study for a test\nand get all the answers right\nSo this is the power of Ultra Instinct?\n"", ""file_name"": ""85805u.jpg"", ""id"": ""85805u"" } The goal after this was to perform K-means cluster analysis to discover groups or common themes in the memes. Step-by-Step Details For steps 3-7, I used code from Kaggle - Use DFoly1 1. Preprocessing the Data Once the Kaggle set was downloaded, I did a quick check on the data to make sure each row had an ID and link to an image with Tableau Prep. I did not find any rows without these properties. Then, I downloaded all the images to my local machine and uploaded them to Google Cloud Platform storage. Because of the way I uploaded them, I had to fix the directories with script.sh. This was a minor setback which took a couple hours to complete. 2. Performing Optical Character Recognition Once the images were in storage, I wrote a Python script to perform OCR and translation -- OcrProcessing.py. I decided to do translation as well so that I could filter out non-English text. Out of the 3327 images, 3031 met the criteria of non-null English text. Some of the memes were just images with no text in them, or they had another language as their primary text. 3. Cleaning Text Data After doing OCR, it was important to do some regular expressions to clean up things like: - contractions - newlines and whitespace - numbers - non-alphabetical characters - commonly found slang or misspellings e.g. shes -> she is or ur -> you are 4. Stop Words and Stemming For stop words and stemming, I used Natural Language Toolkit (NLTK) for Python. I did add some additional stop words based on my findings with the memes, like ""meme"" and ""rdankmemes"" which was a Reddit tag. For stemming, I used a Porter stemmer since it is relatively fast and works well with the English language. The Porter stemmer is: ""Based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes."" 1 5. TF-IDF Vectorization Once stop words and stemming was done, I transformed the text from the memes into a TF-IDF vector using sklearn. This method has many options like n-gram range, maximum or minimum document frequency, maximum features, smoothing, and using sublinear term frequency. I tried different variations of TF-IDF vectorization to see how that would affect K-means clustering. With this specific dataset, the maximum number of features seemed to be optimal around 1000-1500. Anything beyond this would cause the clustering to have a lot of outliers, resulting in imbalanced and poorly grouped clusters. I also tried the sublinear term frequency option, but that caused some irregularities as well, similar to increasing number of features. 6. Principal Component Analysis Before doing K-means, I did do PCA to reduce dimensionality in the TF-IDF vector. If we look at the TF-IDF vector, the X axis is the meme text and the Y axis is one of the 1500 features or terms. Where [X,Y] meet is the term frequency. With 1500 features and 3031 meme text data segments, it's useful to construct a new feature subspace to reduce the risk of overfitting because the data is too generalized. 7. K-means K-means is an algorithm very similar to EM algorithm where there is an assignment-like phase and then a maximize phase. In K-means, we first initialize cluster centroids randomly. Then, repeat this until convergence: - For every data point, assign to nearest centroid via Euclidean distance - Move the centroids to the center of data points assigned to it For K-means clustering, I did do multiple runs with 2-6 clusters. I set maximum iterations to 600, but generally, it converged in < 100 iterations. With sklearn, it can do multiple randomized initializations in order to find the best possible local maxima, which may or may not be the global maxima. The default is 10, but I tried different ranges which usually didn't have much variance. 8. What is a good number of clusters? I used 2 methods in determining what might a good number of clusters look like: 1. Elbow method 2. Silhouette analysis The Elbow method is a heuristic approach in which the number of clusters is plotted against the function of variance. A ""good"" number is where the curve has a definitive bend, resembling the shape of a human arm with an elbow. Generally, 3 clusters provided the best elbow. However, with some different variations of the TF-IDF vector and K-means, the bend was not explicit and was actually a smooth curve, which is one of the drawbacks of using the Elbow method. The Silhouette analysis measures the separation distance between clusters. The range of silhouette analysis can range from -1 to 1. - A value of 1 suggests the sample is far away from neighboring clusters. - A value of zero suggests the sample is very close to the boundary between two clusters. - A negative value suggests the sample may have been assigned to the wrong cluster. My results usually had values of > 0.75 for n clusters of 2-3, while it dropped off to < 0.5 for > 5 clusters. I did not observe any negative or zero values. I used code from scikit learn to do my silhouette analysis. With the Elbow method and silhouette analysis, it seemed that 3 was a good number of clusters for this data. 9. Evaluation and Results Overall, 3 clusters seemed to be the magic number based on evaluation. Selecting features > 1500 seemed to be detrimental to clustering where the clusters were very skewed and had many outliers, even with PCA dimensionality reduction. The 3 clusters had these top words: 1. will, people, now, see, man, know, time 2. win, boi, years, body, entire, million, master 3. pm, likes retweets, trump, follow, donald, will While some themes were not extremely clear like in cluster 1 and cluster 2, the third cluster was quite clear in terms of having a social media vibe. Other top words in this third cluster were realdonaldtrump, elonmusk, and replying. For improvements, it might be useful to try different stemming methods and adding more stopwords like ""will"" or ""us."" But here is where ambiguity comes into play. Without looking at each meme individually, it's hard to tell if will was in the context of an auxillary verb like ""will travel,"" or a noun like a legal document. The same goes for ""us."" Does this mean us, like the group of us, or US like the United States? It would also be interesting to see bi-grams of this. When I did the TF-IDF vectorization, I stuck with unigrams since I just wanted to use bag of words representation before getting ahead of myself. Overall, this project has been a great learning experience in terms of working with real data, using Google Vision, seeing how K-means works especially after doing EM algorithm work, and evaluation of clusters. Despite deviating from the original plan of sentiment analysis, I did get the general outcomes I wanted with cluster analysis. It would have been nice to see more clusters or more clearly defined feature words, but I think that would have come with more refinement. How to Run Anaconda If you're interested in setting this up yourself, some test data is provided. Here is what my environment looks like: - Anaconda v4.9.2 - Download here - Python v3.7.9 - Anaconda environment file here - OS: Windows Subsystem Linux 18.04 Ubuntu (optional) Git clone the repository or download the ZIP. Navigate to the directory where it is saved. With the environment.yml file, change the prefix to where your Anaconda environments are stored. For me, it is /home/skuretski/anaconda3/envs/cs410. So for you, it might be /your/directoryToAnacondaEnv/anacondaVersion/envs/cs410 Run command conda env create -f environment.yml Run conda activate cs410 Run jupyter notebook Navigate to whatever URL the jupyter notebook command logged. It is usually something like http://localhost:8888/?token=someStringHere Navigate to KMeans.ipynb from the localhost page. I've included a directory called test_data which includes some resulting OCR JSON files locally. It is not all of them, but will give you a sense of how the code works. Make sure the first cell is selected and then hit Run. Continue this in sequence. Without Anaconda Without Anaconda is possible, however, you will have to globally install some dependencies. - Python v3.7.9 - Matplotlib v3.3.2 - Numpy v1.19.2 - Pandas v1.1.3 - Seaborn v0.11.0 - NLTK v3.5 - scikit-learn v0.23.2 - scipy v1.5.2 - Wordcloud v1.8.1 - Jupyter Notebook This project is powered by Python, those listed libraries, and Jupyter notebook."
https://github.com/sonalsharma5990/CourseProject	"PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Sonal Sharma sonals3 (Captain) Maneesh Kumar Singh mksingh4 Kamlesh Chegondi Kamlesh2 2. Which paper have you chosen? Subtopic: Causal topic modeling Paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? We have registered on the LDC site and are awaiting access. TAs are notified on same via Piazza@770 post 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? Topic-level Causality Analysis From topic modeling results -> generate topic curve over time topic's coverage on each time unit (e.g. one day) Consider a weighted coverage count. Specifically compute the coverage of a topic in each document p(topic_j|document_d) => theta_j Estimate the coverage of topic j at t_i t_c_i_j as sum of thera_j over all documents with t_i timestamp. t_c_i_j = Sum(theta_j) TS_j => list of t_c_j for all the timestamps creates topic stream time series that, combined with non-textual time series data lends itself to standard time series causality measures C and testing. Select lag value is important? How => chose lag with highest significance Word-level causality and Prior Generation Chose topics with highest causality scores and further analyze the words withing each topic to generate topic proors For each word, generate a word count stream W S_w by counting frequencies in the input document collection for each day: w_c_i = Sum c(w,d) Measure correlation and significance between word streams and external non-textual time series. Then wemeasure correlations and significance between word streams andthe external non-textdual time series. This identifys words that aresignificantly correlated and their impact values. PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Sonal Sharmasonals3(Captain)Maneesh Kumar Singhmksingh4Kamlesh ChegondiKamlesh2 2. Which paper have you chosen?Subtopic: Causal topic modelingPaper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai,Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use?Python 4. Can you obtain the datasets used in the paper for evaluation?We have registered on the LDC site and are awaiting access. TAs are notified on same via Piazza@770 post 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introducedin the paper?  Project Progress Report Team: Best Bots S.No. Task Current Status Remarks/Impediments 1 Access to DataSet by all members Completed 2 Understanding the dataset Completed How to deal with gaps in dataset. Raised @1273 on Piazza 3 Find parallel time series dataset Completed 4 Finding the libraries required for implementation Completed 5 Perform Preprocessing of data 90% Completed 6 Perform LDA on news data set 10% Completed Currently performing LDA on May data 7 Apply Granger Test to determine causality relationship. 75% completed Matrix multiplication not providing expected results. Trying to do this using for loops 9 For each candidate topic apply  Causality measure  to find most significant causal words among top words in each Topic. 75% completed Matrix multiplication not providing expected results. Trying to do this using for loops 10 Record the impact values of these significance words using Pearson correlations Not Started Currently being done in Granger test. Yet to implement Pearson coefficients. We may not need it. 11 Separate positive impact terms and negative impact terms 75% completed Implementation is done. Testing in progress with actual data. 12 If orientation of words in prior step is very weak, ignore minor group 75% completed Implementation is done. Testing in progress with actual data. 13 Assign prior probabilities proportions according to significance levels 75% completed Implementation is done. Testing in progress. 14 Apply LDA to Documents using prior obtained 10% Completed Research on parameters for gensim LDA. 15 Repeat until satisfying stopping criteria (e.g. reach topic quality at some point, no more significant topic change). Completed In our experiment iteration count is 5 16. Make final project documentation report. Not Started 17. Software usage tutorial presentation Not Started Iterative Topic Modeling Framework with Time Series Feedback Abstract As part of our final project for CS410 Text Information Systems, we are reproducing paper ""Topic Modeling Framework with Time Series Feedback"". We chose to reproduce experiment of 2000 U.S. Presidential election campaign due to its low data size. We followed the steps and algorithm as mentioned in the paper and were able to get results which are very similar to results provided in the paper. With this project, we learnt about Topic Modeling and how topic modeling combined with TimeSeries feedback can be used to explain the relation between text and non-text time series. Authors Team BestBots | Name | NetId | | ------------------- | --------------------- | | Maneesh Kumar Singh | mksingh4@illinois.edu | | Sonal Sharma | sonals3@illinois.edu | | Kamlesh Chegondi | kamlesh2@illinois.edu | Table of Contents Iterative Topic Modeling Framework with Time Series Feedback Abstract Authors Team BestBots Table of Contents Project Video Algorithm Parameters Time series data Collection of documents with ts from same period Topic modeling method M Causality measure C tn mu m Gamma  Delta d Output Steps How to run? Initial Setup Run program DataSet NYT Corpus data IEM 2000 Winner takes all data Data preprocessing NY Times Corpus IEM Winner takes all data Handling of missing data Stop words removal Implementation Hurdles and Ladders Final Results Significant Topics 2000 Presidential Election Quantitative Evaluation Results Conclusion Acknowledgments References Appendix Evaluation System Software/Tools used Project Video https://youtu.be/bP7eKOCasVU Algorithm Parameters Time series data X = x_1, ... , x_n with timestamp (t_1, t_2, ..., t_n) Collection of documents with ts from same period D = {(d1,td1),..,(dm,tdm)} Topic modeling method M Identifies topics Causality measure C Significance measures (e.g. p-value) and impact orientation tn How many topics to model mu m strength of the prior Gamma  Significance Threshold Delta d Impact Threshold Output k potentially causal topics (k<=tn): (T1,L1),... (Tk, Lk) Steps Apply M to D to generate tn topics T1,..,TN Use C to find topics with significance value sig(C,X,T) > gamma(95%) CT: set of candidates causal topics with lags {(tc1, L1),..,(tck,Lk)}. For each candidate topic CT, apply C to find most significant causal words among top words w subset of T. Record the impact values of these significance words (e.g. word-leave Pearson correlations with time series variable) Define a prior on the topic model parameters using significant terms and impact values Separate positive impact terms and negative impact terms If orientation is very weak ( delta < 10%) ignore minor group Assign prior probabilities proportions according to significance levels Apply M to D using prior obtained in step 4. Repeat 2-5 until satisfying stopping criteria (e.g. reach topic quality at some point, no more significant topic change). When the process stops, CT is the output causal topic list. How to run? This program has been tested for python 3.8.5. It may work for older version of python (>=3.6), but was not tested. Here are the instructions to run this program. Initial Setup This program can be run with or without virtual environment setup. However virtual environment is highly recommended. Below are the steps required for Ubuntu 20.04.1 LTS. ```bash install virtualenvwrapper pip3 install virtualenvwrapper --user Setup virtualenvwrapper path if not already done export PATH=""$HOME/.local/bin:$PATH"" If python3 is aliased on your system, you need to setup environment variable for virtualenvwrapper to know where to pick python export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 Source virtualenvwrapper.sh so that all helper commands are added to path source ~/.local/bin/virtualenvwrapper.sh Make virtualenv. demo can be any name. mkvirtualenv demo Clone the repository git clone https://github.com/sonalsharma5990/CourseProject.git install dependencies cd CourseProject/src pip install -r requirements.txt ``` Run program There are three options to run the program. Below command uses the saved model to find significant topics and words. As it does not need to train the model, it is the fastest option and finishes in less than 5 minutes. bash python main.py You can also train the model again. Below command trains the model using the prior strength mu and eta (topic_word_prob) for 5 iterations. Please note currently it is not possible to set the mu, eta or number of iteration options from command line. These must be changed in code. As this option trains the model five times (five iterations). It takes 12 to 15 minutes to run the program. bash python main.py retrain You can run topic causal modeling for various prior strength (mu) and number of clusters (tn), as mentioned in the section 5.2.2 Quantitative evaluation results. This generates graphs between various mu, tn and average causal significance and purity. As this program runs model training for each mu/tn combination five times. This takes 40 minutes to run on a large AWS EC2 instance. ```bash python main.py graph ``` DataSet NYT Corpus data New York Times corpus was provided by TA's based on request by each team member. The dataset due to its huge size and access restriction is not included in this repository. IEM 2000 Winner takes all data The data from May-2000 to Nov-2000 was manually selected from IEM Website IEM 2000 U.S. Presidential Election: Winner-Takes-All Market The data for each month was selected using dropdown and copied to a spreadsheet. After data for all months have been collected, the spreadsheet is saved as an CSV file. Data preprocessing 2000 presidential election campaign experiment required NY Times corpus and IEM Winner Takes all data from May-2000 to Oct-2000. In additional NY Times corpus data is in XML so it was required to parse and extract required component. We wrote pre_processing.py module to take care for both tasks. It performs following tasks NY Times Corpus Using lxml library, extract body of NY corpus dataset between 01-May-2000 to 31-Oct-2000. Check if body contains Gore or Bush, If yes extract only the paragraph containing Bush and Gore. Combine all paragraphs for single article and write it as a single line NY Times output is gzipped to save space. It is stored as data.txt.gz in data/experiment_1 folder. NY Times output file contains each document as line, so line index is treated as document ID Using line index (starts from 0), a document and date mapping file is created. This file is required in order to count documents/words for causal processing. IEM Winner takes all data Using pandas, the IEM Winner takes CSV is read Filter dates between 01-May-2000 to 31-Oct-2000. Using Gore as baseline, the LastPrice column is normalized GorePrice = GorePrice/(GorePrice + BushPrice) Only date and LastPrice column is kept. As this file is small it is kept in memory for whole run. Handling of missing data On analysis of IEM data with NY Times corpus data, we found that IEM data is missing values for two dates 07-Jun-2000, 08-Jun-2000. These dates are filled with values from next (future) available last price which was available for 09-Jun-2000. Stop words removal On running the topic modeling several times we realized the initial and significant topics are dominated by most occurred terms in the corpus. E.g. almost every topic had Bush said with highest probability. We had to either implement some kind of TF-IDF for topic modeling or ignore the common words altogether. After some review we decided to go with removal of these words from the corpus as stop words. names of candidates as they are frequently used e.g. Bush, Gore common political words e.g. president, presidential, campaign parties e.g. republican, democratic states e.g. New York, Florida. These states were home states for the candidates. common verbs and words e.g. said, asked, told, went time words e.g today, yesterday, wednesday Implementation Our goal was to reproduce experiment-1 involving 2000 U.S. Presidential election campaign. We took the various parameter values as mentioned in the paper to find significant topics in causal analysis. The parameters used are Number of topics (Tn) = 30 Prior Strength (m) = 50 Significance Threshold () = 95% (0.95) Delta Threshold (To ignore topic based on impact) (d) = 10% (0.10) Number of iterations = 5 We also tried to analyse our topic modeling results quantitatively. For this we ran our experiment with different values for prior strength m and number of topics Tn. We fixed Tn = 5 during run for testing different m. m values tested: 10, 50, 100, 500, 1000 We fixed prior strength m = 50 to test different values for Tn. Tn values tested: 10, 20, 30, 40 1) Gensim LDA is used for topic modeling. That represents M in the algorithm. eta parameter was used for priors and delay is used for prior. Delay may not be correct parameter for prior strength, but after weighing other options and discussions with students, we decided to use it. @1378 on Piazza. 2) Statsmodel library granger test function. This function allowed us to test correlation with different lag. We tried granger test with up to 5 day delay and took the best lag for both topic and word significance. 3) For impact value, we calculated the average of lagged x coefficients as mentioned in the paper. If the impact was positive, we interpreted as +1 and negative value as -1. As we were only interested in most significant topic and words we didn't get a result with 0 impact. We used 1 - p value as significance score. 4) In order to select Top words for causal analysis, the paper discusses using Mass probability cutoff (ProbM). Its value was not provided. After trying with various values for ProbM we settled on 0.40. This cutoff allowed us to select most important words for causal analysis. Increasing this from 0.40 doesn't give any better results. Hurdles and Ladders 1) Algorithm to implement Topic Modelling: We had a tough call between PLSA and LDA here. MP3 PLSA is heavily un-optimized. It even fails with memory error with experiment-1 document data. (Presidential campaign vs IOWA market), whereas LDA using gensim library uses a lot of inner dependencies and the m step is not as clear(as in lectures) to incorporate Mu.(question @1378 on Piazza) Post feedback and discussion with Professor and Students we used the decay parameter as Mu to implement the paper. 2) Missing data for some dates in Non-text series We have used future value in this case after research.To justify the same, in case of stock data in week 9/11, we would miss the impact in stock if using previous values. 3) Granger Test to determine causality relationship We used 1- p value for score which amounts to almost 0 values getting 100% score. 4) Add customized stop words in data preprocessing We removed words that were not adding any value to topics found. names of candidates as they are frequently used e.g. Bush, Gore political words e.g. president, presidential parties e.g. republican, democratic states e.g. New York, Florida common verbs and words e.g. said, asked, told, went time words e.g today, yesterday, wednesday Final Results Significant Topics 2000 Presidential Election After training the LDA model for several times, we were able to match few important topics of 2000 Presidential election campaign. Tax cut, Healthcare, abortion played a major role during the campaign and we can see our method was able to find some of these major topics. | TOP 3 WORDS IN SIGNIFICANT TOPICS | | --------------------------------- | | tax lazio house | | national know administration | | tax aides advisers | | security social american | | officials aides american | | medicare tax security | | tax federal polls | | national house tax | | abortion american national | | drug american tax | Quantitative Evaluation Results As discussed in the paper, we also tried validating our results with various values of prior strength mu and number of topics (tn) with average causality and purity. In the paper, increasing values of mu and tn causes a mostly upward trend in average causality and and purity. Our reproduction however could not see the same relation. We got the highest causality and purity with the initial values of mu and tn and it mostly remains constant after that. This is something that we would like to further research. Conclusion Using Iterative Topic Modeling with Time Series Feedback (ITMTF) for causal topic mining, we were able to reproduce the paper. We were able to successfully get topics which were prominent in 2000 Presidential Election. We also tried to quantitative evaluate the topic mining results and unlike paper, our results saw little gain in confidence and purity with increasing values of prior strength (mu) and number of topics(tn). As a future task, the quantitative results need to be further researched and the findings can be utilized to improve our algorithm. Acknowledgments We would like to thank Professor ChengXiang Zhai for a great course and guidance to complete this project. We would also like to thank our TAs for always being available for any questions and issues. We are also grateful for fellow students on Piazza and Slack for motivation and fruitful discussion in completing this project. References [Kim et al. 13] Hyun DukKim, MaluCastellanos, MeichunHsu, ChengXiangZhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management(CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Appendix Evaluation System The algorithm was evaluated on AWS EC2 c5.12xLarge Ubuntu instance. Software/Tools used | Tool | Usage | Version | Link | | -------------- | ------------------------- | ------- | ----------------------------------------------- | | Python | Programming language | 3.8.5 | https://www.python.org/ | | statsmodel | Granger Test | 0.12.1 | https://www.statsmodels.org/stable/index.html | | gensim library | LDA topic modeling | 3.8.3 | https://radimrehurek.com/gensim/ | | Pandas | CSV and data processing | 1.1.5 | https://pandas.pydata.org/ | | NumPy | Array/Matrix Manipulation | 1.19.4 | https://numpy.org/ | | Tabulate | Printing results in table | 0.8.7 | https://github.com/astanin/python-tabulate | | lxml | Parse NY Times xml corpus | 4.6.2 | https://pypi.org/project/lxml/ | | Matplotlib | Draw evaluation plots | 3.3.3 | https://pypi.org/project/matplotlib/ |"
https://github.com/soumya834-msit/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/srashee2/CourseProject	CS410 Progress Report Team Starks Saad Rasheed - srashee2 - Javier Huamani - huamani2 Sai Allu - allu2 Which tasks have been completed? Our team has currently taken the The New York Times Annotated Corpus dataset and extracted the data given to us into more useful excel and CSV files. Our team has taken these csv files and have used different libraries (gensim, pandas, sklearn, and nltk) to create an example of topic modelling with time series feedback. Which tasks are pending? To successfully recreate the paper our team still needs to find the stock time series data and run that against our code, while refining the code to find the optimal execution. Lastly, we have to implement measures of quality such as a Granger test. 3) Are you facing any challenges? The stock time series data seems to be not referenced in the paper. We also have an issue with deterministic / non-deterministic results when running our code. Project Proposal Submission 1.Saad Rasheed - srashee2 - Team Leader Javier Huamani - huamani2 Sai Allu - allu2 2.Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback a.https://www.biz.uiowa.edu/faculty/trietz/papers/ITMTF.pdf 3.Python 4.Yes a.https://catalog.ldc.upenn.edu/LDC2008T19 CS410 Final Project: Iterative Topic Modeling with Time Series Feedback Team Starks Saad Rasheed - srashee2 Javier Huamani - huamani2 Sai Allu - allu2 Demonstration https://youtu.be/BC6qcxoF8XQ Purpose Team starks set out to recreate Mining Causal Topics in Text Data Title Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Authors Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier Citation Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Introduction In our final project we create a text mining application to find causal topics from corpus data. Our application takes a probabilistic topic model and using time-series data we explore topics that are causally correlated with said time-series data. We improve on the topics at each iteration by using prior distributions. To determine causality we are using Granger causality tests which is a popular testing mechanism used with lead and lag relationships across time series. Libraries pandas - Used for data manipulation and analysis scikit-learn - Used for classification, regression and clustering algorithms nltk - Used for symbolic and statistical natural language processesing pyLDAvis - Used to help interptret topics from a LDA topic model statsmodels - Used for statistical computations Files Corpus Data * NYT2000_1.csv * NYT2000_2.csv IEM Stock Data * IEM2000.xlsx Main Application * LDA.py Code Walkthrough We begin by reading in the corpus data that we segmented into two files to be able to store alongside the code. We then clean the data by removing NaN values and other filtering. We do more filtering and remove unneccesary characters and then lemmatize the data. We then read in the IEM data and normalize it. We then take the corpus data and generate the counts and vocabulary to create a document term matrix Now we're able to fit the LDA model, we use 15 topics and have found that number to be optimal. For each date we create a topic stream and aggregate topic coverages and plot them. To evaluate causality we then run Granger tests against each topic and output the p values for the f Tests against each lag. To determine the optimal lag value we aggregate p values. We sort the p values in ascending order. Of the top 25 words for each topic we run granger causality tests and pearson coefficient tests. We only continue if we get a p value of less than .05. To actually create the priors we evaluate a topic based on its negative or positive bias. If a topic has a dominated negative or positive bias we create a prior for each word and assign it to a single topic. Conversely, if there is no negative or positive bias we split the word into two topics and assign it to a single topic. Our code then iterates using the generated prior (On the first iteration the priors are empty) and fits the LDA model again according to the max iteration. How to run The easiest way to run our code is to download Anaconda and run it through jupyter notebook $ git clone https://github.com/srashee2/CourseProject.git $ cd CourseProject $ jupyter notebook You can now click on LDA.ipynb and click run all cells. It will take some time to run through the code, approximately 1 hour. Contributions Team Starks came together over the course of a few months with weekly meetings to understand, learn and recreate Iterative Topic Modeling with Time Series Feedback. More specific contributions for the team members can be found below. All team members did the following: library research, paper breakdown and documentation. Saad Rasheed - Logistical work, Corpus Text Extraction, Presentation, and LDA modeling iteration Javier Huamani - Text Filtering and Manipulation, LDA Modeling, Granger Causality, and Pearson Coefficient Tests Sai Allu - Text Filtering and Manipulation, LDA Modeling, Granger Causality, and Presentation
https://github.com/srivardhansajja/tangy	"Text Information Systems : CS410 Project Documentation ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Team O Navyaa Sanan (navyaas2) O Srivardhan Sajja (sajja3) : Team Captain Overview In our project, we were able to augment the ExpertSearch system by adding functionality which makes the system automatically crawl through faculty webpages given the primary university link/URL (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project has two main components. First, we implemented our own classifier, which given any URL uses text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we find all directory pages associated with that primary URL. We used the classifier built in part 1 for implementing part 2. Software Implementation Datasets: To train the extension, we used 800 manually labelled URLs. To test the extension, we used another 800 manually labelled URLs to check for accuracy. We were able to achieve 83.875% Accuracy, 89.28% precision, 77% recall and 82.68% F1-score. Algorithms/Techniques: To get the list of all URLs from a university website, we used spider from scrapy package. We implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit can be set manually. To make the classifier, we used classification techniques like using stop words and filter words. We also used statistical indicators like mean length of URLs and standard deviation of the training set URLs. Lastly, we used a dictionary to look at the most common words in positive training samples to help us build a classification model. Additionally, we would like mention that our extension is independent of the system. We consider this project to be an independent feature addition, which is inspired by the ExpertSearch System but does not directly rely on any preexisting code. We drew inspiration from MP2 and used the techniques taught to us in this course, but we did not have any direct reliance on any preexisting code whatsoever (aside from external Python packages). Software Implementation Details Crawler: To get the list of all URLs from a university website, we used spider from scrapy package. We implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit for the crawler can be set manually. Model Training: To train the model, we looked at used classification techniques like using stop words and filter words. We also used statistical indicators like mean length of URLs and standard deviation of the training set URLs. Lastly, we used a dictionary to look at the most common words in positive training samples to help us build a classification model. We have a list of words we ignore in model_train/crawler.py. Any URL that contains a word also contained in ignore is automatically pulled out of consideration. The rest of the training is done in model_train/trainer.py and model_train.py where we calculate the statistical indicators and return those URLs which have the same words contained as the most popular words from the positively labelled training data. To train the extension, we used 800 manually labelled URLs. To test the extension, we used another 800 manually labelled URLs to check for accuracy. We were able to achieve 83.875% Accuracy, 89.28% precision, 77% recall and 82.68% F1-score. Model Deploying: To deploy the model, most of the work is done in model_deploy /model_dep.py. There we use the model generated by us (model_deploy/ model_testing.json) and carry out the same classification process as we did to test our classification model. Flask App: The Flask app refers mostly to the frontend work we did. It connects the deployed model to the website we made and provides a framework for piping input and output. Installation and setup Clone repository on your local machine and enter the project directory: - git clone https://github.com/srivardhansajja/CourseProject.git - cd CourseProject/ Setup a virtual environment. Executing the following two lines in a terminal will set up an environment using venv within the project directory. - python 3 -m venv venv - source venv/bin/activate We have used Python3 for running and testing the project. Install all required packages by executing: - python3 -m pip install -r requirements.txt Change line 11 in crawler/crawler_handler.py to the python version that you are using. For example, python3.8, python3, python3.6 or py. The project is now set up for you to use and test. Execute - python3 main.py from the primary project directory and access the locally hosted flask website by visiting http://127.0.0.1:3000/ from a browser window. You can enter university domain names and the appropriate faculty directory URLs will be shown to you along with the crawling statistics. The program by default uses our pre-generated model. If you wish to update the parameters or tweak the model, go to model_train/trainer.py, make your required changes, and run - python3 model_train/model_train.py This will output your testing statistics, including accuracy, precision, recall and F1 score, and generate model_testing.json. Once you are satisfied with your changes, if you wish to use your model in the crawling process instead, replace model_testing.json in line 50 of model_train/model_train.py with model.json and rerun the above statement in your terminal. Be careful as this will replace our original model, and re-cloning the project is the only way to revert it, unless you make a backup of it. Project Structure Y= Source Code: o Crawler SS /crawler/crawler.py SS /crawler/crawler_handler.py o Model Training SS /model_train/trainer.py SS /model_train/model_train.py SS /model_train/train_data.txt SS /model_train/dev_data.txt o Model Deployment SS /model_deploy/model_dep.py SS /model_deploy/model.json o Flask App SS /main.py SS /templates/ SS /static/ Y= Documentation: o ProjectProposal.pdf o ProjectProgressReport.pdf o ProjectDocumentation.pdf Y= README.md o / Team contributions Throughout the course of this project, the team worked very closely and even though most of the tasks were divided by person both of us ended up working on everything in some capacity. Srivardhan focused more on implementing the web crawler, setting up the website, and making training/testing data sets. Navyaa focused mainly on doing research on URL classification, building the machine learning model, and integrating the classification model into the website. Text Information Systems : CS410 Project Progress Report ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Progress made thus far Y= Enlisted some common faculty directory URLs to give us a general sense of what they look like. This would later help us in identifying some common features. Y= Determined some common features of faculty directory URLs which act as starting points for the classifier we are writing. We took the main features of URLs like word staff or directory in them and started with these obvious features. We also identified words and features that would never be part of a faculty directory page URL. Y= Implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit can be set manually. Y= Added some preliminary filters within the spider to not allow URLs with certain keywords such as 'mail', 'publish', 'calendar', 'research', etc., to make crawling process more efficient. Y= Developed a website using the flask web framework to showcase model's results, and to amplify user experience. Y= Looked at some research done in similar areas (classification of URLs) but were not able to find much that is directly relevant. The main issue we face is figuring out common features of a general faculty directory page (length of the URL, important terms to look for etc.) Remaining tasks Y= Create a testing / training data set for training our model. Y= Finalizing some research papers which would be the inspiration for building our classifier. Y= Base our classifier off of what results we see in the training data set. Y= Fine tune our classifier to tell with greater confidence whether a website is a faculty directory URL or not. Issues and challenges faced Y= Tons of literature, scholarly articles and papers on the internet - there are tons of papers which seem relevant prima facie but turn out to be not as useful when we go into the details. This has made the task of searching for what we need for sure longer than we had anticipated. Y= Too many potential URLs associated with a primary URL. A primary URL like ""Illinois.edu"" has several valid URLs associated with it so we have to manually stop looking at valid URLs after a certain point. Y= Scraping websites with a lot of pages takes time so fine-tuning certain aspects becomes a very time-consuming task. Text Information Systems : CS410 Project Proposal ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Team  Navyaa Sanan (navyaas2)  Srivardhan Sajja (sajja3) : Team Captain System 2.2 ExpertSearch System: Automatically crawling faculty webpages We plan on augmenting the ExpertSearch system by adding functionality which would make the system automatically crawl through faculty webpages given the primary university link/url (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project will have two main components. First, we plan to implement our own classifier, which given any URL will use text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we will find all directory pages associated with that primary URL. We will be using the classifier built in part 1 for implementing part 2. Datasets, Algorithms and Techniques Datasets: To train the extension, we shall be using the starting half of urls list fro mthe Google Sheets spreadsheet of MP2.1 as positive examples, and automatically generated non-faculty directory pages of university websites as negative examples. Similarly, to test our extension, we shall be using the 2nd half of the urls in the spreadsheet for positive examples, and randomly generated real urls as negative examples. Algorithms/Techniques: To get the list of all urls from a university website, we shall use simple web crawling techniques using the built-in python packages but might dip into external packages if required. To make the classifier, we shall be using basic classification techniques and algorithms taught in this course. If we feel the need to use algorithms and techniques that are not mentioned in lectures, we will be sure to use techniques taught to us in the MPs. Overall, we will ensure that any algorithms used by us are directly or indirectly related to this course. Primary Function We are adding a function to automatically compile a list of all faculty directory pages from a given university home page url. This can be tested by comparing the generated list of urls with a list of manually compiled faculty directory page urls from different universities. The closer the pre-compiled list is to the generated one, the higher the accuracy of the web crawler and classifier is. Communication with the system Our extension is independent of the system. We consider this project to be an independent feature addition, which is inspired by the goal of this project but does not directly rely on any preexisting code. We will draw inspiration from MP2 and use the techniques taught to us in this course, but we will not have any direct reliance on any preexisting code whatsoever. Programming Language We plan to use Python3 for all developmental activities, including building the classifier and the web crawler. Work Justification Since there are 2 team members in this group, we planned this project to be 40 hours of work. Here is a breakdown of all tasks with the estimated time we expect each task to take: 1. Developing a classifier which, given a URL tells us if the URL is that of a faculty directory web page (23 hours) a. Subtask 1: Research and enlist at least 200 types of faculty directory URLs (2.5 hours) b. Subtask 2: Determine common/ identifying features of these faculty directory URLs (3 hours) c. Subtask 3: Look at research papers/scholarly texts that deal with similar classification problems and choose a classifier accordingly. Potentially write some pseudo code/ starter code for our classifier (7 hours) d. Subtask 4: Develop a training and testing data set and manually flag all websites we use. (2.5 hours) e. Subtask 5: Write the code of our classifier based on the reference. (5 hours) f. Subtask 6 : Based on the results, modify and fine tune our classifier. (3 hours) 2. Given a primary url (eg. Illinois.edu) identify all possible faculty directory pages. (17 hours) a. Subtask 1: Make a training testing data set of at least 10 primary URLs and as many directory URLS associated with the primary URL (2 hours) b. Subtask 2: Identify all the possible directory page associated URLs with our training/testing data set. (3 hours) c. Subtask 3: Read research papers / scholarly literature on how to tell whether a given URL is valid or not (4 hours) d. Subtask 4: Devise a method to find all valid URLs associated with the given primary URL. (5 hours) e. Subtask 5: Find a way to determine which subset of valid URLs are faculty directory pages. (should be just using the classifier that we built in Task 1 but since generating all possible combinations and testing out whether they are valid or not can take time, 3 hours to run the whole thing) University Faculty Directory Page Crawler This is the course project for CS 410 Text Information Systems course at the University of Illinois at Urbana Champaign. Usage tutorial: https://youtu.be/tN551flUyks Overview We were able to augment the ExpertSearch system by adding functionality which makes the system automatically crawl through faculty webpages given the primary university link/URL (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project has two main components. First, we implemented our own classifier, which given any URL uses text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we find all directory pages associated with that primary URL. We used the classifier built in part 1 for implementing part 2. Team Srivardhan Sajja Navyaa Sanan Project Setup Instructions Clone repository on your local machine and enter the project directory: git clone https://github.com/srivardhansajja/CourseProject.git cd CourseProject/ Setup a virtual environment. Executing the following two lines in a terminal will set up an environment using venv within the project directory. python3 -m venv venv source venv/bin/activate We have used Python3 for running and testing the project. Install all required packages by executing: python3 -m pip install -r requirements.txt Change line 11 in crawler/crawler_handler.py to the python version that you are using. For example, python, python3.8, python3, python3.6 or py The project is now set up for you to use and test. Execute python3 main.py from the primary project directory and access the locally hosted flask website by visiting http://127.0.0.1:3000/ from a browser window. You can enter university domain names and the appropriate faculty directory urls will be shown to you along with the crawling statistics. The program by default uses our pre-generated model. If you wish to update the parameters or tweak the model, go to model_train/trainer.py, make your required changes, and run python3 model_train/model_train.py This will output your testing statistics, including accuracy, precision, recall and F1 score, and generate model_testing.json. Once you are satisfied with your changes, if you wish to use your model in the crawling process instead, replace model_testing.json in line 50 of model_train/model_train.py with model.json and rerun the above statement in your terminal. Be careful as this will replace our original model, and re-cloning the project is the only way to revert it, unless you make a backup of it Project Structure Source Code: Crawler /crawler/crawler.py /crawler/crawler_handler.py Model Training /model_train/trainer.py /model_train/model_train.py /model_train/train_data.txt /model_train/dev_data.txt Model Deployment /model_deploy/model_dep.py /model_deploy/model.json Flask App /main.py /templates/ /static/ Documentation: ProjectProposal.pdf ProjectProgressReport.pdf ProjectDocumentation.pdf README .md / Demo and Tutorial We made a tutorial for ease of use of the software, and is hosted at https://youtu.be/tN551flUyks Model Statistics These are the statistics of our generated model, based on training and testing data sets with 800 URLs each. Accuracy: 0.83875 F1-Score: 0.8268456375838926 Precision: 0.8927536231884058 Recall: 0.77"
https://github.com/srzn/CourseProject	"ProjectDocumentationSrujanNetid:ssg7LiveLabID:srznDecember13,2020Includedistheproject ow,de nitionandfunctionofeachmoduleandusagedetails1ProjectFlowIparticipatedintheIRcompetition.IusedMP2.4codetemplatetoimplementvariousstateoftheartrankingfunctionsforIRevaluationandranking1.1ConvertingDataFormatofthedataprovidedisincompatiblewiththatofmycode.So,the rststepisdataconversion.Queriesareprovidedinxmlandneedtobeconvertedintoatext le,eachqueryperline.Actualdocumentsareprovidedasacsv lewithduplicateentriesperdocumentandalotofirrlevantinformation.Eachdocumentisidenti edbyanalphanumericid.Relevantdatafromthecsv leneedtobeextractedandpopulatedasdocumentdatawitheachdocumentoccupyingonelineinthe nal.dat le.Mycodetakesdocumentidsasnumbers.So,adictionarywithuidtonumbermappingneededtobebuilt.Theentireprocessneededtobeimplementedforbothtrainingandtestingdata.Thisconcludesdataconversion.1.2TrainingStepIused6IRrankingfunctionstotrainonthedatasettoextractoptimalparam-etersforeachoftherankingfunctions.Theuserrunningmytrainingprogramisgivenachoice/optiontoselectanyoneoftheserankingfunctions.Oneoftherankingfunction,InL2ranker,wasoverloadedwithmycustomscoringfunc-tion.Iswepteachrankingfunctionwithanumberofparametersoverthegiventrainingdatasetandlabels.ThecombinationofparametersthatleadtothebestNDCG@20weresavedtoa lenamed\Option<num>.txt""Theseweretherankingfunctionsused:Option0:BM251Option1:InL2rankerOption2:InL2rankerOption3:Jelinek-MercerSmoothingOption4:DirichletPriorSmoothingOption5:AbsoluteDiscountingSmoothing1.3TestingStepFromthe\Option<num>.txt""Iobtainedfromthetrainingstep,weusecorrespondingrankingfunctionstoobtainrankingscoresofeachdocumentw.r.taquery.Testdataisslightlydi erentfromtrainingdataset.So,invertedindiceswerebuiltseparatelyfortrainingdatasetaswell.2ModuleDe nitionsThecode lesinbothtestingandtraininghavesimilarnamesandfunctions.Followingcode lescanbefoundinmysoftwarepackage:2.1queryextr.pyThis leisusedtoconvertquery.xmlintoqueries.txtandqueries-test.txt2.2convert-dat1.pyThis leisusedtoextractdatafrommetadata.csvanddocumentjsonstobuildSarsCov.dat lewhichisinturnusedtobuildinvertedindexandusedtoevaulatetherankingfunctions2.3uidmap.pyThis leisusedtobuilddictionaries\uidmap.txt"",\uidrevmap.txt"".Thesedictionariescontaindocidtouidmappingsandvice-versa.2.4searcheval.pyThisisthemainprogramthatrunsthetrainingphasetooutputbestparametersthatgeneratethehighestNDCG2.5searchtest.pyThisisthemainprogramthatranksandscoreseachdocumentfromtestsetagainstthetestqueriesandoutputsa\testpredictions.txt""outputwhichhasdocidsinsteadofuids22.6predictgen.pyUsesthedictionariesobtainedfrom\uidmap.py""andproducesthe naloutput\predictions.txt""thathasuids3Usage3.1TrainingInthedirectorythathassearcheval.py,runpythonsearcheval.pycon g.toml'Option'where'Option'hasthesamerangeasdescribedin\Trainingstep"".You'llobtainatext le""Option0.txt""(ifyouchoseOption0).Pleasecopythis leovertothe""test""directorythathas\searchtest.py""3.2TestingAftertheabovestep,inthedirectorythathas\searchtest.py"",runpythonsearchtest.pycon g-test.toml'Option'where'Option'cantakeanyintegervaluebetween0and5.Pleasereferthedocumentationtoknowmoreabouteachchoice.Forexample,ifyouchoseOption0thenyou'llrunpythonsearchtest.pycon g-test.toml0Andthenrun,pythonpredictgen.pyTheabovecodemustbeexecutedtoobtainthe nalpredictionsinatext le.IfyouchoseOption0youwouldobtain""prediction0.txt""asyour nalpredictionsforallthequeriescappingat1000topdocumentsperquery.4Leaderboard3Figure1:Leaderboardasof12/13/20209:38PMEST4 ProjectProgressReportSrujanNetid:ssg7November29,20201IntroductiontoProjectIchosetoparticipateintheInformationRetrievalcompetition.IproposedusingaLearning-to-RankmethodologyforIRasopposedtostandalonerankers.Ialsoproposedto rstexploreevaluatingusingcombinationofrankersratherthanonerankingmethodologytoproducerankings.Finally,IproposedusingSVMmap,asupervisedlearningbasedrankingtechniqueforIR.2Estimatedstepsintheprojecttimeline2.1Step1:ChoosingmetapyforapreliminaryanalysisStatus:CompletedOn:11/27/2020Idecidedtousemetapytogetanunderstandingforthedata(CORD-19set)andhowclassicalrankersweusedthroughthecoursebehavewiththisenormousdataset.ThismeansusingtherankingtemplatefromMP2.4toseehowthatimplementationperformswiththenewdataset(includestrainandtestfoldersfromhereon)insteadofcran elddata.2.2Step2:ConvertingdataintoMeTAformatStatus:CompletedOn:11/29/2020Usingmetadata.csvanddocumentsfolder,thedatasetneedstobecreatedasa\.dat"" lewithdocumentsseparatedbyanewline.Queriesshouldbeextractedfromthegivenxmldocumentandconvertedintoatext lethatmetapy.index.IREvalcanread.Relevancejudgement lemusthave\docid""1asuint64insteadofthealphanumericformatgivenintheCORD-19dataset.Finally,buildinganinvertedindextobeusedbymetapyrankers.Figure1:ExtractedqueriesfromxmlFigure2:ExtracteddatainMeTAformat2.2.1Step3:EvaluatingwithknownrankersStatus:PendingEstimatedCompletion:12/04/2020Aftercompletingthedataconversion,usingclassicalandcustomrankers2(BM25+,InL2rankeretc.)toevaluatetheirperformancewithCORD-19dataset.2.2.2Step4:EvaluatewithacombinationofrankersStatus:PendingEstimatedCompletion:12/04/2020Dependingontheindividualrankers'performance,chooseacombinationoftoprankersbasedonthedataathand.Theoretically,thisimplementationchangesthecombinationweightsdependingonthedataset.2.3Step5:ConvertdataforSVMmapcompatibilityStatus:PendingEstimatedCompletion:12/09/20202.4Step6:EvaluateusingSVMmapStatus:PendingEstimatedCompletion:12/09/20202.5Step7:Produce nalresultsbasedonSteps3,4,and6Status:PendingEstimatedCompletion:12/12/20203ChallengesDataformatconversionsareprettychallengingowingtolotofexceptionhandling.The nalresultsneedtobeinthecompetitionformatwhichmeansthedataneedtobeconvertedbackagain.It'sdiculttocomprehendtheembeddingsdatawithoutgoingthroughSPECTERprojectimplementation.So,thedatawillhavetogounusedifIrelyentirelyonintroductionsectionforeachpaper.Therearealsoafewmissingdata eldsinthedatasetcollectionwhichneedtobehandled.TheimplementationwouldstillworkifIignoremissingdatabutwouldbeincomplete.3 ProjectProposalforCS410October25,20201.Name,NetIdandCaptain*Thisisaonememberteam*Name:SaiSrujanGudibandiNetId:ssg7Captain:SaiSrujanGudibandi2.CompetitionselectionIchoosetoparticipateintheInformationRetrievalCompetition3.ImplementationandideasfortheprojectIplantouseasupervisedlearningapproachowingtothefactthatIRismoreamenabletoempiricaltuningoveracompletelyblack-boxapproachofconstructinganunsupervisedmathematicalmodeli)FromtheexperienceofconstructinganIRsystemforMP2.4,IhaverealizedthatOkapi-BM25per-formedbestforthecranfielddataset.Whileconstructingdifferentrankers,IhavecycledthroughvariousmodelslikePivotedLengthNormalization,INL2ranking,Zhai'sBM25+,Jelinek-Mercer,andDirichletsmoothingmodels.Afterextensiveandexhaustivetuningoneachandeverymodel,Okapi-BM25emergedvictoriousovertheothers.I,however,didn'thavethechancetoimplementlearning-to-rankmethodsbycombiningmorethanonerankingmodelstoobtainasuperiorperfor-mance.Iwouldstartwithimplementingoneofthelearning-to-rankmethodologiesdescribedintheoptionalmoduleofthecoursetopitchitagainstindividualmodelstocomparetheirperformances.ii)Inimplementinglearing-to-rank,Iwishtochooseaclassificationbasedlearningtorankfromamongthedifferentapproaches.Forthispurpose,I'mcurrentlyexploringSVMmap.ImaychangemychoiceofLTRsandchooseanothermodelifI'mnotsatisfiedwiththeperformanceofapreviouslychosenone.Inadditiontoworkingonthedatasetsprovidedforthecompetition,IplanonusingLETORasabenchmarkdatasetformymodel.iii)4.ProgramminglanguageoptionIwishtoimplementthemajorityoftheprojectinPython.However,ImayuseC++and/orRintheduecourseoftheprojectifneedbethedetailsofwhichIwillthoroughlydocumentintheuserguide.ReferencesLiu,""LearningtorankforInformationRetrieval"",FoundationsandTrends(r)inInformationRetrieval,Vol.3No.3(2009)pp.225-331,2009http://projects.yisongyue.com/svmmap/https://www.microsoft.com/en-us/research/publication/letor-benchmark-collection-research-learning-rank-information-retrieval/1 CourseProject Project Proposal.pdf is the Project Proposal Document progressreport.pdf is Project Progress Report as of 11/29/2020 9:00 PM ET ==========For running the code================= All the software content is in the file Code.zip. The link to the same is https://drive.google.com/file/d/1Xep48h-O4VUR2WZna2LtugpVpYauIyWU/view?usp=sharing Due to GitHub restrictions, I was unable to upload the zip file directly here. Please use the link above to get the file. Thanks! When you uncompress Code.zip you'll have two folders - train and test. Please do not delete any file. Important files are illustrated below ---train--- queryextr.py uidmap.py convert-dat1.py search-eval.py ---test--- queryextr.py uidmap.py convert-dat1.py search_test.py predictgen.py ====Usage=== If you're skipping the training, run the following from ""test"" folder on your command prompt: python search_test.py config-test.toml 'Option' where 'Option' can take any integer value between 0 and 5. Please refer the documentation to know more about each choice. For example, if you chose Option 0 then you'll run python search_test.py config-test.toml 0 And then run, python predictgen.py The above code must be executed to obtain the final predictions in a text file. If you chose Option 0 you would obtain ""prediction0.txt"" as your final predictions for all the queries capping at 1000 top documents per query. If you want use the training phase: From ""train"" folder, run python search_eval.py config.toml 'Option' where 'Option' has the same range as described above. You'll obtain a text file ""Option0.txt"" (if you chose Option 0). Please copy this file over to the ""test"" folder and repeat the instructions above to obtain final predictions. ---End"
https://github.com/ss129/CourseProject	"Text Classification Competition: Twitter Sarcasm Detection CS410 - COURSE PROJECT DOCUMENT Team - SSW Classifiers * Saravana Somasundaram (ss129@illinois.edu) * Shashivrat Pandey (spandey6@illinois.edu) * Walter Tan (wstan2@illinois.edu) Table of Contents 1) Introduction to Team: ......................................................................................................................... 2 2) Selection of project: ............................................................................................................................ 2 3) Project Background: ............................................................................................................................ 2 4) Steps followed during implementation: ............................................................................................. 2 5) Code walkthrough:.............................................................................................................................. 3 6) Steps to run the application: ............................................................................................................... 3 7) References: ......................................................................................................................................... 3 1) Introduction to Team: We formed a team of three people to work on this project. All of us are passionate about the Data mining concepts were interested on exploring more knowledge around data mining and apply our learning from this course. Our team includes below set of people from fall season of course CS410 from University of Illinois at Urbana-Champaign: * Saravana Somasundaram (ss129) * Shashivrat Pandey (spandey6) * Walter Tan (wstan2) 2) Selection of project: After discussions, as a team we decided to proceed with Text Classification Competition: Twitter Sarcasm detection project. Other project topics that we considered are: * Automatically crawling faculty webpages * Extracting relevant information from faculty bios 3) Project Background: In this project there were two sets of data file provided to us to use in our application: * Training data (train.jsonl) * Test data(test.jsonl) We are supposed to build an application to do a prediction for Sarcasm or Not Sarcasm. The data files provided to us are twitter responses. These response texts are in context to some conversation happening in twitter feed. We were supposed to use these two datasets for training our model for this classification. Since this was a classification task, the training file also had the label for each data point as ""SARCASM"" or ""NOT_SARCASM"". Our job was to predict the same thing for all the records present in the testing file. Training file had 5000 labelled dataset and testing set had 1800 dataset. The project required some machine learning task to train the model using the training data and finally predict the outcome for test-data set using that trained classification model. The output of the application will be a text file that capture all the 1800 rows from test-data and SARCASM Vs NOT_SARCASM label. The name of the file is supposed to answer.txt. 4) Steps followed during implementation: * Analyzed the requirements for the project completion captured under document (CS 410 Project Topics - Google Docs) provided by instructors * Setup environment to execute the projects o Download train.jsonl and test.jsonl data under a folder called data o Install all the packages required for project using pip install command # Sklearn # Pandas # Nltk # Numpy * Design a framework to read train.jsonl and test.jsonl files and parse data * Process the data to remove words such as '@USER' and use the lemmatize function to clean up the data * Initialize TfidfVectorizer with the appropriate parameters * Initialize GaussianNB model and train the model using the training data * Perform predictions of labels using the test data * Write the results into answer.txt 5) Code walkthrough: We created and uploaded a video under github account that covers step by step walkthrough of our code implementation. Below is the URL and Name of the file for code walkthrough video: * URL: GitHub - ss129/CourseProject * Name: CourseProject_Demo.mp4 6) Steps to run the application: Please follow the below steps to run the application and generate the results. * Clone the below GitHub repository into your local machine - GitHub - ss129/CourseProject * Install the below packages using pip install command o Sklearn o Pandas o Nltk o Numpy * Execute the python program twitter_sarcasm_classification.py * The results will be available in the file data/answer.txt 7) References: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://realpython.com/python-keras-text-classification/#convolutional-neuralnetworks-cnn c. https://towardsdatascience.com/classification-using-neural-networks-b8e98f3a904f d. https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db Project Proposal - Text Classification Competition 1. Team Name: SSW Classifiers Members: * Saravana Somasundaram (captain) - ss129 * Shashivrat Pandey - spandey6 * Walter Tan - wstan2 2. Competition - Text Classification 3. Neural Networks we are looking to explore for Text classification: a. Convolutional Neural Network (CNN) b. Recurrent Neural Network (RNN) c. Hierarchical Attention Network (HAN) 4. References we have looked so far: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f c. https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn d. https://towardsdatascience.com/classification-using-neural-networks-b8e98f3a904f 5. The programming language we will be using is Python. Our group is prepared to learn how text classification algorithms can be implemented using state-of-the art neural networks such as Convolutional Neural Network, Recurrent Neural Network and Hierarchical Attention Network. We are excited to learn techniques to improve our ML skillset and will apply what we learned to our current/future work projects. Some Deep Learning frameworks we've heard of include PyTorch, TensorFlow, Keras, and Sonnet. TensorFlow and PyTorch seem to be the most popular and used by many users and institutions worldwide. Our group has never worked with these technologies, but are excited to learn these new technologies for this competition. We will be using Python for this project and we are confident that we will come up with an optimized code to improve the performance of application. Project Progress Report: Below is the progress we made so far on our project: 1) Which tasks have been completed? * Read through the instructions for text classification project. * Had few meetings with all group member to understand the requirements and instructions * Created a Github repository for project, cloned it from original repository and read through Readme.md file * Analyzed train and test data 2) Which tasks are pending? * Working on framework to read the file and scan through each JSON document * Next step is to apply a text classification logic to decide each document as Sarcasm Vs Not_Sarcasm 3) Are you facing any challenges? * We did not face any challenges as of now CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/ssquires/CourseProject	"CS410FinalProjectDocumentationsquires4|SamanthaSquiresContentsProjectSummary1HowtoRuntheCode1ProjectSummaryItrainedaBERTtextclassificationmodeltoidentifytweetsas'SARCASM'or'NOT_SARCASM'usingpython/tensorflow/kerasandGoogleColab.Ifoundthistutorialveryhelpfulintheprocessofwritingmycode,anddirectlyusedsomeofthecodeprovidedinthetutorial-I'vealsonotedthisdirectlyinmycodeintherelevantfunctions.ThecodecanbefoundintheGoogleColabnotebookinthisrepo(filenameTextClassificationCompetitionFinal.ipynb).BERTwasthefirstmodelIevenattemptedforthisproject,becauseIhadheardofitssuccessintextclassificationtasks.Ididn'thavemuchpreviousexperiencewithtensorflow,sothemainchallengesthatarosewereunderstandingtheformatsandshapesrequiredformodelinputandoutput.Tocompletetheproject,IfirstutilizedtheTensorflowBERTtutorialmentionedabovetowriteafunctionforbuildingakerasmodelusingapre-trainedBERTmodel,andthenafunctiontofitthismodeltothetweetclassificationtrainingset,thenmakepredictionsonthetestdataandreturnthosepredictions.Finally,Iwroteafunctiontotransformthemodel'spredictions(log-oddsvalues)totheformatrequiredbyLiveDataLab,andsavetheresulttoatextfilefordownloading.Finally,Iwrotethemaincodepipeline,whichloadsthetrainandtestdata,preprocessesitbycon-catenatingthe'response'and'context'columnsintoasinglefeatureandtransformingthelabelsfrom'NOT_SARCASM'/'SARCASM'into0/1,andfinallycallsthefunctionsthatIwroteearliertocreate,fit,andpredictusingthemodel.Afterthecodeisfinishedrunning,itoutputsafilenamedanswer.txt,whichisintheproperformattobesubmittedviaLiveDataLab.HowtoRuntheCodeAllprojectcodeislocatedinTextClassificationCompetitionFinal.ipynb.PleaserunthecodeinGoogleColabbyfollowingthestepsbelow(orwatchthedemovideotutorialhere):1.Downloadthisrepo.2.Gotohttp://colab.research.google.com/andclickUpload,thenselectthedownloadedfileTextClassificationCompetitionFinal.ipynb.ThisshouldopenthenotebookinColab.Therestoftheseinstructionsarealsoincludedinthenotebookitself,butforcompleteness:3.IntheColab""Edit""menu,goto""NotebookSettings""andselect""GPU""fromthehardwareacceleratordropdown.4.Uploadthetrainandtestdatafilesprovidedwiththecompetition(makesurethey'renamedtrain.jsonlandtest.jsonl)bygoingto""Files""intheleft-handsidebar,clickingtheuploadicon,1andselectingtrain.jsonlandtest.jsonl.Thesetwodatafilesareincludedintherepo,soyoushouldalreadyhavethemdownloaded.5.Torunthecode,select""Runtime""fromtheColabmenuandclick""Runall"".Thefirstfewcellsshouldrunquitequickly:thefinalcell,whichtrainsthemodelandpredictslabelsforthetestdata,takesmuchlonger(inmyexperience,10-12minutes).Afteraminuteorso,youshouldstarttoseeoutputtrackingthetrainingprogressofthemodel.Afterthecodefinishesrunning,theoutputfile,answer.txt,shouldbevisibleunder""Files""intheleft-handsidebar.Ifyou'dliketosavethisfile,makesuretodownloaditbeforetheruntimedisconnects.2 CS410FinalProjectProgressReportsquires4|SamanthaSquiresWhichtaskshavebeencompleted?ItrainedaBERTtextclassificationmodeltoidentifytweetsas'SARCASM'or'NOT_SARCASM'usingpython/tensorflow/kerasandGoogleColab.Ifoundthistutorialveryhelpfulintheprocess.Asoftoday(11/29/20),mysecondsubmissionhassuccessfullypassedthebaselinef1scoreperformanceonLiveDataLab,withaprecisionof0.699,recallof0.768,andf1of0.732.Whichtasksarepending?Althoughmycodeisfunctionalatthispoint,Istillneedto:-Cleanupthecodeandwritedocumentation-Automatethefinalstepofcreatinganswer.txt(Ididthismanuallyinexcelformyfirsttwosubmissions)-Createmytutorialpresentation.IfIhaveadditionaltime,Iwouldalsoliketocontinuetoimprovetheperformanceofmymodel.Areyoufacinganychallenges?SinceIamfairlynewtotensorflowandkeras,myprimarychallengessofarhavebeengettingtheinputsandoutputsofthemodelintothepropershapesandformats.Idon'tanticipatesignificantchallengesmovingforwardwiththedocumentationandpresentation.1 CS410FinalProjectProposalsquires4|SamanthaSquiresTeamThiswillbeanindividualproject(SamanthaSquires,netid:squires4).CompetitionIplantojointheTextClassificationCompetition(TwitterSarcasmDetection).PotentialNNClassifiers/DLFrameworksIplantolookintoavarietyofneural-networkbasedclassifiers(CNN,LSTM,BERT,etc.).IplantousetheKeraslibrary,whichIhaveusedbeforeforsmallneuralnetworkclassificationprojectsandtutorials.ProgrammingLanguageTheprojectwillbecompletedinPython.1 CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities."
https://github.com/steve303/CourseProject	CourseProject Summary In this project we trained a pre-built model (BERT) using the transformers library from Hugging Face on a dataset of labeled tweets, labeled for sarcasm (sarcastic/not sarcastic). We then used this model to predict the class (sarcastic/not sarcastic) of a set of provided unlabeled tweets for comparison to a competitive baseline score. We were able to beat the baseline with our model. See ./Project Documentation/final_summary.pdf for a full report. Repository Contents ./Alternative Methods & Models/: Contains additional models that we built and trained but which were unsuccessful at beating the baseline score. ./data/: Contains the test and train data provided for the competition. ./Project Documentation/: Contains the final report, demo, and other project deliverables. answer.txt: Our final file containing the classification of the test tweets which outperformed the F1 score of the baseline. Bert.ipynb: Our notebook file in which we build, train, and test the model. TEXT_PREPROCESSING.py: A dependecy of Bert.ipynb, used to preprocess the text for tokenization.
https://github.com/subhasishb-coder/CourseProject	"Project: Text Classification Competition Team: Subhasish Bose (sbose4) and Soumya Kanti Dutta (skdutta2) This documentation is created during CS 410: Text Information System Final Project and it contains the details of the project. Table of Contents Introduction ............................................................................................................................... 1 The Training dataset Content ........................................................................................................................ 1 The Test dataset content ............................................................................................................................... 2 Dataset size statistics .................................................................................................................................... 2 Project Objective ........................................................................................................................................... 2 Approach and Workflow ............................................................................................................ 2 Data Preprocessing and Feature Engineering ................................................................................................ 2 Training Models............................................................................................................................................. 3 Validation of Training Data ............................................................................................................................ 3 Running Code on Test Data and Leaderboard Score ...................................................................................... 4 Contribution .................................................................................................................................................. 4 Setup and Usage Instructions..................................................................................................... 4 Software Dependencies ................................................................................................................................. 4 Setup and Usage Instructions ........................................................................................................................ 5 References .................................................................................................................................. 5 Introduction As final project for CS 410 Text Information System, we participated in Text Classification Competition to detect Twitter Sarcasm. We were given both Training and Test datasets. The Training dataset Content label: SARCASM or NOT_SARCASM response: The Tweet to be classified context: The conversation context of the response example: {""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""]} The Test dataset content id: String identifier for sample. This id is required for project submission and grading. response: The Tweet to be classified context: The conversation context of the response example: {""id"": ""twitter_1"", ""response"": ""@USER @USER @USER My 3 year old , that just finished reading Nietzsche and then asked me : \"" ayo papa why these people always trying to cancel someone on Twitter , trying to pretend like that makes them better themselves ? \"" . To which I replied \"" idk \"" , and he just \"" cuz hoes mad \"" . Im so proud . <URL>"", ""context"": [""Well now that \u2019 s problematic AF <URL>"", ""@USER @USER My 5 year old ... asked me why they are making fun of Native Americans .."", ""@USER @USER @USER I will take shit that didn't happen for $ 100"", ""@USER @USER @USER No .. he actually in the gifted program and reads on second grade level . ... and he knows Kansas City is in Missouri""]} Dataset size statistics Train Test 5000 1800 Project Objective Our project objective is to learn from the Training dataset and predict the labels of Test dataset (SARCASM or NOT_SARCASM). Approach and Workflow Data Preprocessing and Feature Engineering - First, we read the Training and Test Data of jsonl format to Pandas data frame. Function: read_jsonl_to_dataFrame - Then we applied the following data cleaning and feature engineering steps on the Training and Test Data. Function: simple_feature_engieering_and_data_cleansing 1) Combined Response and Context Tweets in the data frame both in training and test data. 2) Converted the dataset to lower case. 3) Got rid of '@USER', '<URL>', Web URL Links, Hashtags. 4) Next, we got rid of stop words. We used nltk.corpus.stopwords for this purpose. 5) We removed the emojis as well. 6) We removed all punctuations and special characters. 7) Lastly, we stripped each word to get rid of additional 'space'. - We also used sklearn.feature_extraction.text.TfidfVectorizer to incorporate additional feature engineering with the following parameters: Y= max_features =20000 Y= min_df=1 Y= max_df=0.5 Y= binary=1 Y= use_idf=1 Y= smooth_idf=1 Y= sublinear_tf=1 Y= ngram_range=(1,3) Training Models We have tried the following algorithms on training data. a) Linear SVC b) Naive Bayes c) Logistic Regression d) Random Forest e) Neural network - BERT These experimentation code can be found in code/other_model_experimentation folder. https://github.com/subhasishb-coder/CourseProject/tree/main/code/other_model_experiments To run the BERT code the following file needs to be downloaded separately - glove.twitter.27B.100d.txt needs to be downloaded for Neural Network Among these Logistic Regression provided us the best performance. So, we designed our final code with Logistic Regression. We used sklearn.linear_model.LogisticRegression, with the following parameters. Y= class_weight='balanced' Y= solver='newton-cg' Y= C=1 Validation of Training Data We got the following performance matrix, doing a train test split of 80/20: Classification Result for Logistic Regression precision recall f1-score support NOT_SARCASM 0.78 0.72 0.75 519 SARCASM 0.72 0.78 0.75 481 accuracy 0.75 1000 macro avg 0.75 0.75 0.75 1000 weighted avg 0.75 0.75 0.75 1000 Overall accuracy for Logistic Regression 0.75 Running Code on Test Data and Leaderboard Score Once we validated the performance of Logistic Regression on training data, we applied it on Test Dataset. Functions: write_prediction_results_in_list and final_prediction_calculation. We created answer.txt file with test dataset labels which we uploaded for grading. We were able to beat the baseline. We tried with multiple times adjusting the feature vector. Leaderboard snapshot: Contribution Data Preprocessing - Subhasish Feature Engineering - Soumya Model Training - Soumya Validation and Adjustment of feature vector - Subhasish Setup and Usage Instructions Software Dependencies Y= Python==3.8.3 Y= nltk==3.5 Y= pandas==1.0.5 Y= scikit_learn==0.23.2 Setup and Usage Instructions 1. conda create -n ""project_demo"" python=3.8.3 2. conda activate project_demo 3. git clone https://github.com/subhasishb-coder/CourseProject.git 4. cd CourseProject 5. pip install nltk==3.5 6. pip install pandas==1.0.5 7. pip install scikit_learn==0.23.2 8. cd code 9. python TestClassficationCompetion_Sarcasm_Detection.py References https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db Project: Text Classification Competition Team: Subhasish Bose (sbose4) and Soumya Kanti Dutta (skdutta2) This report contains information on project progress as of 11/25/2020. Progress: We have completed the following steps: 1) Get train and test datasets from GitHub Repo. 2) Inspect the datasets to understand the format and relation. 3) Performed data cleaning steps on the dataset. 4) Performed train-test split on training dataset to test accuracy. 5) Tested the dataset with the following algorithms: a) Linear SVC b) Naive Bayes c) Logistic Regression d) Random Forest e) Neural network - BERT Remaining Tasks: The following tasks are pending: 1) Run the above-mentioned algorithms on Test Dataset. 2) Our software will go with the majority decision found from these 5 algorithms mentioned above. For example, if any 3/4/5 of these algorithms find the tweet as 'SARCASM' - our code with tag it as 'SARCASM'. 3) Create output in desired format. 4) Create project Report. 5) Create Software Usage tutorial presentation. 6) Code, Project Report and Presentation submission. Issues/ Challenges: As the test dataset is not labeled, it is difficult for us to judge accuracy of our code on it. Final Project Proposal Document Topic: Competitions 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Subhasish Bose (Captain) - sbose4@illinois.edu Soumya Kanti Dutta - skdutta2@illinois.edu 2. Which competition do you plan to join? Text Classification competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods We are prepared to learn state-of-the-art neural network classifiers. We have heard of classifiers like CNN (Convolution Neural Network), RNN (Recurrent Neural Network) and frameworks/ libraries like Keras, Spacy, NLTK, Scikit-learn. We don't have any prior experience with these. 4. Which programming language do you plan to use? Python CourseProject Introduction As final project for CS 410 Text Information System, we participated in Text Classification Competition to detect Twitter Sarcasm. We were given both Training and Test datasets. The Training dataset Content label: SARCASM or NOT_SARCASM response: The Tweet to be classified context: The conversation context of the response example: {""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""]} The Test dataset content id: String identifier for sample. This id is required for project submission and grading. response: The Tweet to be classified context: The conversation context of the response example: {""id"": ""twitter_1"", ""response"": ""@USER @USER @USER My 3 year old , that just finished reading Nietzsche and then asked me : \"" ayo papa why these people always trying to cancel someone on Twitter , trying to pretend like that makes them better themselves ? \"" . To which I replied \"" idk \"" , and he just \"" cuz hoes mad \"" . Im so proud . "", ""context"": [""Well now that \u2019 s problematic AF "", ""@USER @USER My 5 year old ... asked me why they are making fun of Native Americans .."", ""@USER @USER @USER I will take shit that didn't happen for $ 100"", ""@USER @USER @USER No .. he actually in the gifted program and reads on second grade level . ... and he knows Kansas City is in Missouri""]} Dataset size statistics Train Test 5000 1800 Project Objective Our project objective is to learn from the Training dataset and predict the labels of Test dataset (SARCASM or NOT_SARCASM). Setup and Usage Instructions Software Dependencies: 1) Python==3.8.3 2) nltk==3.5 3) pandas==1.0.5 4) scikit_learn==0.23.2 Setup and Usage Instructions: 1) conda create -n ""project_demo"" python=3.8.3 2) conda activate project_demo 3) git clone https://github.com/subhasishb-coder/CourseProject.git 4) cd CourseProject 5) pip install nltk==3.5 6) pip install pandas==1.0.5 7) pip install scikit_learn==0.23.2 8) cd code 9) python TestClassficationCompetion_Sarcasm_Detection.py Software Usage Tutorial Link: https://mediaspace.illinois.edu/media/t/1_xwb0wmzt"
https://github.com/surajbisht1809/CourseProject	"Team AmazingS3 Progress Report * Our team is Amazing S3 * Following are the team members o Sithira S Serasinghe sithira2@illinois.edu o Santosh Kore kore3@illinois.edu o Suraj Bisht surajb2@illinois.edu (Team Leader) * Project: Text classification competition Task completed: Team tried various models i.e. LSTM, sequence, RNN, Spacy, Naive, Embedding, Logistic regression, RandomForest, SVC and BERT After performing pre-processing and data cleaning with BERT model our team have crossed baseline, following is the current view Pending task: Team is currently working to optimization model i.e. using different BERT variant, tuning parameters and including context Challenges: After working on various model, BERT is working well for current assignment. BERT model requires heavy computing resources hence on desktop building and training model takes around three hrs. Our team is looking for some resources specially GPUs for tuning parameters and optimizing model Project Proposal Our team is Amazing S3 Following are the team members Sithira S Serasinghe sithira2@illinois.edu Santosh Kore kore3@illinois.edu Suraj Bisht surajb2@illinois.edu (Team Leader) We are joining Text classification competition Prior experience in Keras, TensorFlow and PyTorch frameworks Python will be used for this competition CS410 - Text Classification Competition Overview We participated in the Text Classification Competition for Sarcasm Detection in Tweets. Our team beat the baseline (0.723) and achieved an F1 score of 0.7542963307013469. The code can be used for training a preprocessing the given dataset (train.jsonl and test.jsonl) and train a BERT model. The usage of our solution can be found in the ""Source Code Walkthrough section"". Our Team: AmazingS3 Suraj Bisht surajb2@illinois.edu (Team Leader) Sithira Serasinghe sithira2@illinois.edu Santosh Kore kore3@illinois.edu Source Code Walkthrough 1. Prerequisites Anaconda 1.9.12 Python 3.8.3 PyTorch 1.7.0 Transformers 3.0.0 2. Install dependencies Make sure to run this program in an Ananconda environment (i.e. Conda console). This has been tested on *nix and Windows systems. 1. Libs ```bash pip install tweet-preprocessor textblob wordsegment contractions tqdm ```` 2. Download TextBlob corpora bash python -m textblob.download_corpora 3. Install PyTorch & Transformers bash conda install pytorch torchvision torchaudio cpuonly -c pytorch transformers If it complains that the transformers lib's not installed, try this command: bash conda install -c conda-forge transformers 3. Usage First, cd src and run the following commands, tl;dr bash python clean.py && python train.py && python eval.py This will preprocess, train and generate the answer.txt model which can be then submitted to the grader for evaluation. Description of each step: 1. Clean the dataset python clean.py Train the model python train.py Once the model is trained it will create an input/model.bin file which saves our model to a binary file. We can later load this file (in the evaluation step) to make predictions. Make predictions & create the answer.txt file python eval.py The answer.txt file is created at the output folder. The following section describes each of these steps in-depth. Data Cleaning / Preprocessing We perform data cleaning steps for both train.jsonl and test.jsonl so that they are normalized for training and evaluation purposes. The algorithm for cleaning the data is as follows: For each tweet: 1. Append all context to become one sentence and prefix it to the response. 2. Fix the tweet if it has special characters to support better expansion of contractions. 3. Remove all digits from the tweets. 4. Remove <URL> and @USER as they do not add any value. 5. Convert all tweets to lowercase. 6. Use NLTK's tweet processor to remove emojis, URLs, smileys, and '@' mentions 7. Do hashtag segmentation to expand any hashtags to words. 8. Expand contracted words. 9. Remove all special symbols. 10. Perform lemmatization on the words. Model Training A model can be built and trained with the provided parameters by issuing a python train.py command. The following steps are run in sequence during the model training. 1. Read in the train.csv from the prior step. 2. Training dataset (5000 records) is split into training and validation as 80:20 ratio. 3. Feed in the parameters to the model. 4. Perform model training for the given number of epochs. 5. Calculate validation accuracy for each run and save the best model as a bin file Tuning the model The following can be considered as parameters that could be optimized to achieve a better result. src/config.py python DEVICE = ""cpu"" # If you have CUDA GPU, change this to 'cuda' MAX_LEN = 256 # Max length of the tokens in a given document EPOCHS = 5 # Number of epochs to train the model for BERT_PATH = ""bert-base-uncased"" # Our base BERT model. Can plug in different models such as bert-large-uncased TRAIN_BATCH_SIZE = 8 # Size of the training dataset batch VALID_BATCH_SIZE = 4 # Size of the validation dataset batch src/train.py python L25: test_size=0.15 # Size of the validation dataset L69: optimizer = AdamW(optimizer_parameters, lr=2e-5) # A different optimizer can be plugging or a learning rate can be defined here L71: num_warmup_steps=2 # No. of warmup steps that need to run before the actual training step src/model.py python L13: nn.Dropout(0.1) # Configure the dropout value Evaluation of the model A high-level view of the sequence of operations run during the evaluation step is as follows. Load the test.csv file from the data transformation step. Load the best performing model from the training step. Perform predictions for each test tweet (1800 total records) Generate answer.txt that will be submitted to the grader to the ""output"" folder. Contributions of the team members Suraj Bisht surajb2@illinois.edu (Team Leader) Improve the initial coding workflow (Google Colab, Local setup etc.). Investigating Sequential model, Logistic Regression, SVC etc. Investigating bert-base-uncased model. Investigating data preprocessing options. Hyperparameter tuning to improve the current model. Sithira Serasinghe sithira2@illinois.edu Setting up the initial workflow. Investigating LSTM/BiDirectional LSTM, Random Forest etc. Investigating various data preprocessing options. Investigating bert-base-uncased model. Hyperparameter tuning to improve the current model. Santosh Kore kore3@illinois.edu Improve the initial coding workflow (Google Colab, Local setup etc.). Investigating Sequential models, SimpleRNN, CNN etc. Investigating bert-large-uncased model. Investigating data preprocessing options. Hyperparameter tuning to improve the current model. Future Enhancements Cleaning data further with different methods. Optimizing BERT model parameters and trying different BERT model (eg. RoBERTa) Re-use some of the tried models and optimizing to beat F1 scores. Extract Emoji's to add more meaning to the sentiments of the tweets. Data augmentation steps to prevent overfitting. Try an ensemble of models (eg. BERT + VLaD etc. ) Run our model on different test data and compare results against state-of-art. References/Credits The usage of BERT model is inspired by https://github.com/abhishekkrthakur/bert-sentiment"
https://github.com/tgw4uiuc/CourseProject	"metapy: (experimental) Python bindings for MeTA diff - Author tgw4's note regarding attribution: I created the OS - specific sections of this, and the note regarding python 2.7 and - 3.4-3.7 being easier to use, the rest was created by the - previous tutorial author(s) who created https://github.com/meta-toolkit/metapy/blob/master/README.md. This project provides Python (2.7 and 3.x are supported) bindings for the MeTA toolkit. They are still very much under construction, but the goal is to make it seamless to use MeTA's components within any Python application (e.g., a Django or Flask web app). This project is made possible by the excellent pybind11 library. Outline Generic Instructions OS Specific Instructions Chromebook Ubuntu 20.04LTS CentOS 8.2.2004 Windows 10 Note that metapy will be much easier to install in python verisons 2.7 or 3.4 through 3.7, while later versions require compiling all the source code. Generic setup notes (the easy way) ```bash Ensure your pip is up to date pip install --upgrade pip install metapy! pip install metapy ``` This should work on Linux, OS X, and Windows with pretty much any recent Python version >= 2.7. On Linux, make sure to update your pip to version 8.1 (or newer) so you can install from a binary package---this will save you a lot of time. Compiling it yourself (the hard way) You will, of course, need Python installed. You will also need its headers to be installed as well, so look for a python-dev or similar package for your system. Beyond that, you'll of course need to satisfy the requirements for building MeTA itself. This repository should have everything you need to get started. You should ensure that you've fetched all of the submodules first, though: bash git submodule update --init --recursive Once that's done, you should be able to build the library like so: bash mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release make You can force building against a specific version of Python if you happen to have multiple versions installed by specifying -DMETAPY_PYTHON_VERSION=x.y when invoking cmake. The module should be written to metapy.so in the build directory. OS Specific Instructions Chromebook Setup Recently released Chromebooks (in the last couple of years as of December 2020) have a option to run a linux beta. Metapy can be installed on these Chromebooks. Check here for a list of compatible Chromebooks: https://www.chromium.org/chromium-os/chrome-os-systems-supporting-linux If your Chromebook is compatible, see this page to turn on the linux beta mode: Chomebook Linux Beta Mode Once you have Linux Beta installed and working, these are the steps to install metapy: Open a linux terminal then: ``` update package lists sudo apt-get update Install python3.7 sudo apt-get install python3.7 install pip sudo apt-get install python3-pip update pip pip3 install --upgrade pip install metapy pip3 install metapy ``` That's it, metapy is now installed. Ubuntu Setup Version 20.04 LTS Ubuntu 20.04 LTS comes wiht python 3.8 installed, which will not make it easy to install metapy. The easy way: (using a previous version of python) Install python version 3.4 through 3.7 from python.org. Make python3.7 your active version. See the instructions here for help switching versions: https://linuxconfig.org/ubuntu-20-04-python-version-switch-manager Now install pip: ``` install pip sudo apt-get install python3-pip update pip pip3 install --upgrade pip install metapy pip3 install metapy ``` That's it, metapy is now installed! The hard way: (sticking with python 3.8) First lets update the list of packages available to install: ``` update package lists sudo apt update install pip sudo apt install python3-pip update pip pip3 install --upgrade pip make an install file directory mkdir metapy cd metapy pip3 download metapy ls (to see the filename it downloaded .. should be metapy(something).tar.gz tar -xvf metapy-0.2.13.tar.gz (or whatever your filename is you downloaded above) ls (to see the name of the directory it created) cd (directory name from above, will be metapy-(something)) cd deps mkdir icu-61.1 now download the ""icu4c-61_1-src.tgz"" file from https://github.com/unicode-org/icu/releases/tag/release-61-1 and copy it to the icu-61.1 directory you created above. move back up to the directory above the meta directory cd .. cd .. cd .. when you 'ls' you should see ""metapy"" as one of the subdirectories install metapy pip3 install metapy (this will takeawhile as the metapy package is compiled). ``` Once its completed, that's it, metapy is now installed. CentOS Setup Version 8.2.2004 Centos is quite easy, as python 3.6 and pip are installed by default. All you need to do is: Make sure your user account is in the ""sudoers"" group, see this tutorial here: https://linuxize.com/post/how-to-add-user-to-sudoers-in-centos/ Then to install metapy: sudo pip3 install --upgrade pip sudo pip3 install metapy That's it, you now have metapy installed! Windows 10 Setup First, be sure to have python version 2.7 or 3.4-3.7 installed. To check if python is installed, open a command prompt window an type: `` check if proper version of python is installed python --version `` If it is not, download it from python.org (be sure to grab one of the versions mentioned above, not necessarily the latest version.) Once python is installed, check if pip is installed: ``` check for pip pip --version ``` If pip is not installed, get it from https://pypi.org/project/pip/. Once pip is installed, upgrade it to the latest version, and install metapy: ``` upgrade pip pip install --upgrade pip install metapy pip install metapy ``` That's it, you now have metapy installed! MeTA: ModErn Text Analysis Please visit our web page for information and tutorials about MeTA! Build Status (by branch) master: develop: diff - Author tgw4's note regarding attribution: I created the - following OS specific sections of this tutorial: - Chromebook - Ubuntu 20.04 LTS - CentOS 8.2.2004 - And also added the icu4c and xlocale.h info to the ""Generic Setup Notes"" as well as some general editing and reordering. - The rest was created by the previous tutorial author(s) who created https://github.com/meta-toolkit/metapy/blob/master/README.md. Outline Intro Documentation Tutorials Citing Project Setup Generic Setup Notes Mac OS X Chromebook Ubuntu Arch Linux Fedora CentOS EWS/EngrIT (this is UIUC-specific) Windows Intro MeTA is a modern C++ data sciences toolkit featuring text tokenization, including deep semantic features like parse trees inverted and forward indexes with compression and various caching strategies a collection of ranking functions for searching the indexes topic models classification algorithms graph algorithms language models CRF implementation (POS-tagging, shallow parsing) wrappers for liblinear and libsvm (including libsvm dataset parsers) UTF8 support for analysis on various languages multithreaded algorithms Documentation Doxygen documentation can be found here. Tutorials We have walkthroughs for a few different parts of MeTA on the MeTA homepage. Citing If you used MeTA in your research, we would greatly appreciate a citation for our ACL demo paper: latex @InProceedings{meta-toolkit, author = {Massung, Sean and Geigle, Chase and Zhai, Cheng{X}iang}, title = {{MeTA: A Unified Toolkit for Text Retrieval and Analysis}}, booktitle = {Proceedings of ACL-2016 System Demonstrations}, month = {August}, year = {2016}, address = {Berlin, Germany}, publisher = {Association for Computational Linguistics}, pages = {91--96}, url = {http://anthology.aclweb.org/P16-4016} } Project setup Generic Setup Notes There are rules for clean, tidy, and doc. After you run the cmake command once, you will be able to just run make as usual when you're developing---it'll detect when the CMakeLists.txt file has changed and rebuild Makefiles if it needs to. To compile in debug mode, just replace Release with Debug in the appropriate cmake command for your OS above and rebuild using make after. Note: as of December 2020, the icu4c repository moved from icu-porject.org to github.com/unicode-org/icu. MeTa requires icu4c to compile, so you will likely need to download it manually to allow MeTa to successfully compile. The Ubuntu 20.04 LTS, Centos 8.2.2004, and Chromebook sections were updated in December 2020, and address this specifically, but for other OS versions, as a general guide the process is (do this before you do the final ""make"" step): `Download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory where you've downloaded the MeTa source. Copy it to the meta/deps/icu-58.2/ directory (create those deps and icu-58.2 directories if they don't exist yet.) One other thing that may cause issues in newer OS distributions: The MeTa source uses xlocale.h, which is no longer included in many newer OS distributions. We can use locale.h instead, so this will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Don't hesitate to reach out on the forum if you encounter problems getting set up. We routinely build with a wide variety of compilers and operating systems through our continuous integration setups (travis-ci for Linux and OS X and Appveyor for Windows), so we can be fairly certain that things should build on nearly all major platforms. Mac OS X Build Guide Mac OS X 10.6 or higher is required. You may have success with 10.5, but this is not tested. You will need to have homebrew installed, as well as the Command Line Tools for Xcode (homebrew requires these as well, and it will prompt for them during install, or you can install them with xcode-select --install on recent versions of OS X). Once you have homebrew installed, run the following commands to get the dependencies for MeTA: bash brew update brew install cmake jemalloc lzlib icu4c To get started, run the following commands: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=clang++ cmake ../ -DCMAKE_BUILD_TYPE=Release -DICU_ROOT=/usr/local/opt/icu4c make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Chromebook Build Guide Recently released Chromebooks (in the last couple of years as of December 2020) have a option to run a linux beta. MeTa can be built on these Chromebooks. Check here for a list of compatible Chromebooks: https://www.chromium.org/chromium-os/chrome-os-systems-supporting-linux If your Chromebook is compatible, see this page to turn on the linux beta mode: Chomebook Linux Beta Mode Once you have Linux Beta installed and working, these are the steps to install Meta: ``` Need gcc/g++-7, gcc/g++-8 or later will not work sudo apt-get update sudo apt-get install gcc-7 g++-7 next update the system and get needed files sudo apt-get update sudo apt-get install software-properties-common install dependencies sudo apt-get install cmake libicu-dev git libjemalloc-dev zlib1g-dev ``` Now we need to make sure that we use gcc/g++-7 and not the newer version that is installed by default on the system. (You may want to remove these links after you are finished successfully building MeTa). ``` sudo ln -s /usr/bin/gcc-7 /usr/local/bin/gcc sudo ln -s /usr/bin/g++-7 /usr/local/bin/g++ ``` Now quit and restart the linux beta terminal window. Next lets download the MeTa files: ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project cd .. (make sure you are in the meta/build/ directory) cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu Build Guide The directions here depend greatly on your installed version of Ubuntu. To check what version you are on, run the following command: bash cat /etc/issue Based on what you see, you should proceed with one of the following guides: Ubuntu 20.04 LTS Build Guide Older Ubuntu Versions (no longer supported, instructions remain for reference purposes) - Ubuntu 15.10 Build Guide - Ubuntu 14.04 LTS Build Guide - Ubuntu 12.04 LTS Build Guide Ubuntu 20.04 LTS Build Guide Update the list of available packages, and then install gcc-7, g++-7 and other prerequisites. sudo apt update sudo apt install gcc-7 sudo apt install g++-7 sudo apt install git cmake make libjemalloc-dev zlib1g-dev Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project, make sure we're using gcc/g++ version 7 cmake ../ -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER=gcc-7 -DCMAKE_CXX_COMPILER=g++-7 make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 15.10 Build Guide Ubuntu's non-LTS desktop offering in 15.10 has enough modern software in its repositories to build MeTA without much trouble. To install the dependencies, run the following commands. bash apt update apt install g++ git cmake make libjemalloc-dev zlib1g-dev Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 14.04 LTS Build Guide Ubuntu 14.04 has a recent enough GCC for building MeTA, but we'll need to add a ppa for a more recent version of CMake. Start by running the following commands to install the dependencies for MeTA. ```bash this might take a while sudo apt-get update sudo apt-get install software-properties-common add the ppa for cmake sudo add-apt-repository ppa:george-edison55/cmake-3.x sudo apt-get update install dependencies sudo apt-get install g++ cmake libicu-dev git libjemalloc-dev zlib1g-dev ``` Once the dependencies are all installed, you should double check your versions by running the following commands. bash g++ --version should output g++ (Ubuntu 4.8.2-19ubuntu1) 4.8.2 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should output cmake version 3.2.2 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 12.04 LTS Build Guide Building on Ubuntu 12.04 LTS requires more work than its more up-to-date 14.04 sister, but it can be done relatively easily. You will, however, need to install a newer C++ compiler from a ppa, and switch to it in order to build meta. We will also need to install a newer CMake version than is natively available. Start by running the following commands to get the dependencies that we will need for building MeTA. ```bash this might take a while sudo apt-get update sudo apt-get install python-software-properties add the ppa that contains an updated g++ sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update this will probably take a while sudo apt-get install g++ g++-4.8 git make wget libjemalloc-dev zlib1g-dev wget http://www.cmake.org/files/v3.2/cmake-3.2.0-Linux-x86_64.sh sudo sh cmake-3.2.0-Linux-x86_64.sh --prefix=/usr/local ``` During CMake installation, you should agree to the license and then say ""n"" to including the subdirectory. You should be able to run the following commands and see the following output: bash g++-4.8 --version should print g++-4.8 (Ubuntu 4.8.1-2ubuntu1~12.04) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash /usr/local/bin/cmake --version should print cmake version 3.2.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project (set C and CXX flags to use the gcc/g++ 7 compiler we installed earlier. CXX=g++-4.8 /usr/local/bin/cmake ../ -DCMAKE_BUILD_TYPE=Release -D CMAKE_C_COMPILER=gcc-7 -D CMAKE_CXX_COMPILER=g++-7 make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Arch Linux Build Guide Arch Linux consistently has the most up to date packages due to its rolling release setup, so it's often the easiest platform to get set up on. To install the dependencies, run the following commands. bash sudo pacman -Sy sudo pacman -S clang cmake git icu libc++ make jemalloc zlib Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=clang++ cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Fedora Build Guide This has been tested with Fedora 22+ (the oldest currently supported Fedora as of the time of writing). You may have success with earlier versions, but this is not tested. (If you're on an older version of Fedora, use yum instead of dnf for the commands given below.) To get started, install some dependencies: ```bash These may be already installed sudo dnf install make git wget gcc-c++ jemalloc-devel cmake zlib-devel ``` You should be able to run the following commands and see the following output: bash g++ --version should print g++ (GCC) 5.3.1 20151207 (Red Hat 5.3.1-2) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should print cmake version 3.3.2 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system with the following command: bash ./unit-test --reporter=spec CentOS Build Guide CentOS 8.2.2004 The first step is to install gcc/g++ 7.5.0 (or any version between 4.8.5 and 7.5.0, inclusive). MeTa won't compile properly with gcc 8.0 or higher. Once that is installed, this is how to setup MeTa: sudo yum install git cmake make libjemalloc-dev zlib1g-dev ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Older versions of CentOS: MeTA can be built in CentOS 7 and above. CentOS 7 comes with a recent enough compiler (GCC 4.8.5), but too old a version of CMake. We'll thus install the compiler and related libraries from the package manager and install our own more recent cmake ourselves. ```bash install build dependencies (this will probably take a while) sudo yum install gcc gcc-c++ git make wget zlib-devel epel-release sudo yum install jemalloc-devel wget http://www.cmake.org/files/v3.2/cmake-3.2.0-Linux-x86_64.sh sudo sh cmake-3.2.0-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir ``` You should be able to run the following commands and see the following output: bash g++ --version should print g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash /usr/local/bin/cmake --version should print cmake version 3.2.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project /usr/local/bin/cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. EWS/EngrIT Build Guide Note: Please don't do this if you are able to get MeTA working in any other possible way, as the EWS filesystem has a habit of being unbearably slow and increasing compile times by several orders of magnitude. For example, comparing the cmake, make, and unit-test steps on my desktop vs. EWS gives the following: | system | cmake time | make time | unit-test time | | -------------- | ----------- | ----------- | ---------------- | | my desktop | 0m7.523s | 2m30.715s | 0m36.631s | | EWS | 1m28s | 11m28.473s | 1m25.326s | If you are on a machine managed by Engineering IT at UIUC, you should follow this guide. These systems have software that is much too old for building MeTA, but EngrIT has been kind enough to package updated versions of research software as modules. The modules provided for GCC and CMake are recent enough to build MeTA, so it is actually mostly straightforward. To set up your dependencies (you will need to do this every time you log back in to the system), run the following commands: bash module load gcc module load cmake/3.5.0 Once you have done this, double check your versions by running the following commands. bash g++ --version should output g++ (GCC) 5.3.0 Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should output cmake version 3.5.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). If your versions are correct, you should be ready to build. To get started, run the following commands: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=which g++ CC=which gcc cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Windows Build Guide MeTA can be built on Windows using the MinGW-w64 toolchain with gcc. We strongly recommend using MSYS2 as this makes fetching the compiler and related libraries significantly easier than it would be otherwise, and it tends to have very up-to-date packages relative to other similar MinGW distributions. Note: If you find yourself confused or lost by the instructions below, please refer to our visual setup guide for Windows which includes screenshots for every step, including updating MSYS2 and the MinGW-w64 toolchain. To start, download the installer for MSYS2 from the linked website and follow the instructions on that page. Once you've got it installed, you should use the MinGW shell to start a new terminal, in which you should run the following commands to download dependencies and related software needed for building: bash pacman -Syu git make patch mingw-w64-x86_64-{gcc,cmake,icu,jemalloc,zlib} --force (the --force is needed to work around a bug with the latest MSYS2 installer as of the time of writing.) Then, exit the shell and launch the ""MinGW-w64 Win64"" shell. You can obtain the toolkit and get started with: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake .. -G ""MSYS Makefiles"" -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Thomas Wrighttgw4@illinois.eduCS410 Progress UpdateThe project chosen was: 2.1 Meta Toolkit - Enhance available tutorials for installing and using the tool on different platforms.1) Which tasks have been completed?To ensure that nothing is left over from previous installations and make sure the tutorials will work starting from zero, multiple spare PCs were set up with SSD drives to allow quick, fresh installations of operating systems and the MeTa code. 3 PCs have thus been set up to allow working on them in parallel, so what while one is busy installing or compiling, work can be done on another. The latest versions of two popular Linux distribution (CentOS 8.2.2004 and Ubuntu 20.04 LTS) and the latest version of Windows 10 (October 2020 release) were downloaded, USB installation media created, and the OSes installed. MeTa was successfully installed on both of the above Linux versions after significant troubleshooting (see section #3 below). Metapy and pytoml were successfully installed on CentOS. Troubleshooting is still underway to try to get it working on Ubuntu.Detailed notes were taken on the installing and troubleshooting efforts (and the discovered workarounds) so far, which will be turned into HTML tutorials for the MeTa toolkit webpage.2) Which tasks are pending?The next task which is pending is to further troubleshoot and find a way to get metapy to install on Ubuntu. There have been significant challenges to doing this, see part 3 below for details. Another task that is pending is to try to installing MeTa and metapy on Windows. With all of the problems discovered trying to get it working on the Linux versions, it was not possible to get to the Windows attempt yet, but hopefully in the next several days that will be possible. Next, as mentioned in the project proposal, investigation will be done into whether MeTa and/or metapy can be installed on a Chromebook. Given all of the issues uncovered in just getting it to work on newer versions of Linux where it had worked before, it is looking less likely to be able to get it working on a Chromebook, but a good attempt will be made to see if its workable.Then HTML versions of the installation notes will need to be created and placed on the project Github site, and following that the installation instructions will be followed one more time from scratch to ensure they work correctly.Finally, the final project writeup/documentation will need to be completed. 3) Are you facing any challenges?Yes, several challenges have risen causing significant challenges related to installing MeTa and metapy. MeTa/metapy don't seem to have been maintained recently. In the case of MeTa, I would estimate around 5 years, given that the existing tutorials refer to Ubuntu version 14.04 LTS and GCC version 4.8.5, which were released 6 and 5 years ago respectively. Metapy seems tobe more recently maintained, within the past couple of years. Quite a few things have changed in those past several years that make it difficult to install MeTa and metapy. First, the source files for MeTa and metapy both download and compile versions of the unicode utility icu4c (version 58-2 for MeTa, 61-1 for metapy). Icu4c is no longer hosted at the site given in the sourcecode (icu-project.org), the source code is now hosted on Github instead. I needed to go through the error logs from the compiler to see where it was calling that, and then adjust the source files to point to the new Github location. This affected MeTa on both CentOS and Ubuntu, but only metapy on Ubuntu. Metapy for CentOS appears to use a pre-compiled version, but for Ubuntu it wants to compile the source code to install it. I am still working on changing the source code to fix this (it is not as easy to work with the source code for metapy, as the pip installer tries to do it all in one operation, I'm working on how to break this into a download step and then a compile/installation step so that the source code can be modified in-between those two steps. This code repository change appears to have been a recent change (in the last month or two), as I used metapy on Ubuntu 20.04 LTS to do MP2.2 for the class, and it installed with no issues then, but won't install now on a fresh installation of Ubuntu 20.04 LTS. Next, the MeTa source code includes xlocale.h. This was part of the glibc library, but was removed a few years ago, and is not included in current Linux distributions. In troubleshooting this, it was determined that for CentOS, it needed to be pointed to /usr/include/bits/types/__locale_t.h instead, while for Ubuntu, it should be redirected to /usr/include/locale.h.Finally, MeTa's source code is not compatible with the GCC 8 (CentOS) and GCC 9 (Ubuntu) versions that are part of the modernLinux distributions. The way that operator overloading is handled changed between gcc 7 and 8, and since the MeTa sourcecode uses this, it will not compile. GCC 7 had to be installed and used to compile MeTa instead of the version 8 or 9 that came with the OSes. This was easier done on Ubuntu than on CentOS. On Ubuntu, it was possible to point the package installer apt-get to use an archive site and install a GCC 7 package relatively easily. For CentOS, there did not seem to be any equivalent archive for the yum package installer used by that OS. The source code for GCC 7 had to be downloaded and compiled from scratch to install it. Given the enormous size and complexity of GCC, this took several hours to compile. Hours spent so far:Installing SSDs and RAM in PCs: 1 hourDownloading install ISOs and creating install USB sticks for several OSes: 1 hourInstalling OSes: 1 hourInstalling MeTa: general installation testing: 1 hourInstalling and Troubleshooting MeTa installation: icu4u repository change: 3 hoursInstalling and Troubleshooting MeTa installation: xlocale.h fix: 1 hourInstalling and Troubleshooting MeTa installation: GCC version issues fix: 6 hoursInstalling metapy/troubleshooting on Ubuntu: 2 hours so far (ongoing)Writing this summary report: 2 hoursEstimated time spent so far: 18 hours Thomas Wrighttgw4@illinois.eduCS410 Project Documentaon and Final ReportPart 1 - Documentaon1. Installaon instrucons - This project was quite different from most of the others. It was not about coding, but rather toimprove the MeTA and metapy tutorials. Therefore, there is no code to install. Rather, the product ofthis project was to produce installaon instrucons for MeTA and metapy for various operangsystems. These instrucons have been created in the form of github .md files, so they can betransferred to the MeTa and metapy github sites to replace or augment the tutorial instruconsalready there.The locaon of these .md tutorial files are in the directory here: h(c)ps://github.com/tgw4uiuc/CourseProject The files are MeTA_tutorial.md and metapy_tutorial.md. Click on either of these files to open them and display the tutorial instrucons.2. Source Code -This project doesn't have formal source code, per se. That was not the goal of this project, creang tutorial instrucons was the goal. Thus, the deliverables are the .md files, which take the place of the 'code' deliverable. ""The documentaon is the code, and the code is the documentaon.""Please note that this project was to improve the exisng tutorials, so much of the older tutorial informaon is sll there. For the purposes of the ""plagarism"" queson on part 6 of the grading rubric,I am only claiming the CentOS 8.2.2004, Ubuntu 20.04 LTS, Chromebook and some notes in the general setup secons as my own for the MeTA instrucons, and the CentOS 8.2.2004, Ubuntu 20.04 LTS, Chromebook and Windows 10 instrucons in the metapy tutorial as my own. The rest of the documentaon is from the previous author(s) of the tutorials.3. SoLware usage tutorial -There is a link to the usage video in the video_link.md file in the h(c)ps://github.com/tgw4uiuc/CourseProject directory. Click that file to see the link to the video.Part 2 - Project ReportProject Goal:The goal of this project was to improve the exisng tutorials for installing MeTA and metapy on various operang systems. The exisng tutorials were outdated and hadn't been updated in several years, and as a consequence, I found many obstacles that had to be overcome to install MeTA and metapy, as many changes had occurred since then that had prevented easy installaon. In most cases,those problems were overcome, and both MeTA and metapy were installed successfully. In addion, since recently produced Chromebooks can now have a beta feature that allows them to use a linux shell, I wanted to invesgate the possibility of ge<<ng MeTA and metapy working on a Chromebook. This was also achieved successfully.Project Results:MeTA was successfully installed on these OSes: (all newer versions of OSes already in the tutorial, or a whole new OS in the case of the Chromebooks):ChromeOSUbuntu 20.04 LTSCentOS 8.2.2004Metapy was successfully installed on the above OSes as well, and also confirmed that is it sll installable under Win10, with some caveats (python versions 2.7 or 3.4-3.7). The only one that failed was installing MeTA on Windows 10. I tried the msys2 method as described inthe exisng tutorial, trying different versions of gcc/g++ and much debugging me, but could not get itto successfully compile. I also tried cygwin, as it is another linux-like environment similar to msys2, but I could not get it to install there either. I had already spent more than half a day trying to get it to work there, and had to give up on that part of the project, as there was no more me for further experimentaon there.Overall, I would say the project was quite successful, and has provided working tutorials for several updated and new OSes that will allow people to get MeTA successfully installed. I would have liked to do tutorials for even more OS versions, but with all of the unexpected problems that were found and the troubleshoong me that was required to find workarounds for them, there was no me to do any others. As it was, just doing these took far more me than the required 20 hours for the project.Problems Encountered:There were several problems encountered during the project related to the age of the MeTA and metapy packages and the evoluon of newer OS versions since the MeTA and metapy packages were last updated. Those problems are as follows:1.gcc/g++ incompability - The way that gcc/g++ handle operator overflow was changed between gcc/g++ versions 7 and 8. This causes problems for the MeTa source code, and it will not successfully compile on versions 8 and above. Thus earlier versions of the compilers must be installed to successfully compile the code.2.xlocale.h no longer supported. This .h file does not exist on many newer linux versions, and thus the MeTA code will error out when trying to compile as it looks for this file. The fix for this is to point xlocale.h to locale.h, which provides the funconality for MeTA to compile successfully.3.The icu4c source repository has moved from icu-project.org to github.com/unicode-org. This causes both the MeTA and metapy code to fail because the make process can no longer download the required source file. The fix for this is to either download the file manually and copy it into the proper directory in the build files (which is what is done in the tutorials) or to edit the makefiles to point them to the new repository. 4.Metapy is harder to install on python version 3.8 and higher, as .whl ('wheel') preprocessed code packages are available for python version 2.7 and 3.4 through 3.7, but not 3.8 and up. These 'wheels' make installaon easy, as they do not need to be compiled from scratch, just downloaded and installed. When installing on python 3.8 and higher, they need to be compiled from the source code, which runs into the icu4c repository problem noted above.Suggesons for improvement of MeTA and metapy (and possible project ideas for future students):There are several things that would greatly improve the usability of MeTA and metapy, especially for new users. These ideas might make good project oportunies for future students to CS410, especially if they have experience with c++ and makefiles/cmake. These would be addressing the problems noted above.1.Update for modern versions of gcc/g++. Update the code to fix the incompabilies with versions of the compiler ranging from version 8.0 and up. 2.Switch the code from using xlocale.h to locale.h. From what I found researching xlocale vs. locale.h online, there is no funconal difference between them for MeTA's needs, but this should of course be confirmed further by the project team.3.Point the code/makefiles to the new icu4c source repository. Change the references from theold icu-project.org repository to the new github.com/unicode-org repository. 4.For metapy, create new 'wheel' .whl packages for python 3.8 and up. This will greatly simplify installaon for new users.Time Spent:Installing SSDs and RAM in test PCs: 1 hourDownloading install ISOs and creang install USB scks for several OSes: 1 hourInstalling OSes: 1 hourInstalling MeTA: general installaon tesng: 1 hourInstalling and Troubleshoong MeTA installaon: icu4u repository change: 3 hoursInstalling and Troubleshoong MeTA installaon: xlocale.h fix: 1 hourInstalling and Troubleshoong MeTa installaon: GCC version issues fix: 6 hoursInstalling metapy/troubleshoong on Ubuntu: 3 hoursInstalling metapy/troubleshoong on CentOS:  1/2  hourInvesgang which Chromebooks I had access to could support the beta linux mode, and se<<ng that up: 1 hourInstalling and Troubleshoong MeTA on the Chromebook: 4.5 hoursInstalling and Troubleshoong metapy on the Chromebook: 1 hourClean re-install of OSes and verifying tutorials work: 5 hoursWring .md tutorial files: 4 hoursWring summary report: 2 hoursWring this final report: 3 hoursCreang, processing and uploading soLware usage video: 2 hoursTotal me spent: 40 hours (at least, some of the above are fairly conservave esmates). Thomas G Wrighttgw4@illinois.eduCS410 Fall 2020 Project ProposalOption 2: Improving a System*2.1 Meta Toolkit*Enhance available tutorials for installing and using the tool on different platformsThis project will cover improving (or creating from scratch if not currently existant) the installation/setuptutorials for MeTA-metapy on several different platforms. OSes which will be considered in improving these tutorials include Windows and multiple Linux versions (Ubuntu, RedHat, Amazon AWS Linux, and perhaps others as time allows) and hopefully ChromeOS if I can get it working (see next to last paragraph below).The current MeTA Setup Guide tutorials on meta-toolkit.org (or the meta-toolkit section on github) seem to have been done quite some time ago, judging by the fact that the latest Ubuntu version listed is 14.04 and the current version is 20.04. The existing tutorials will be reviewed and tested to be sure that they still work, correcting and enhancing where needed. Instructions for newer versions of software, such as Ubuntu 20.04 will be added, as well as instructions for other Linux distributions. Also, there seems to be very little documentation for installing/setting up metapy. This project will add tutorials for that on several OSes. These will start from the assumption of a fresh OS install, unlike what instructions seem to exist online, which are very sparse and assume you have pip, git, etc. already installed on your computer. How to get to that stage seems to be a cause of frustration for several that have had problems. These tutorials would be set up to be a link off the tutorials section ofthe main page, in a section called something like ""metapy setup guide"".All of these tutorials will be written in HTML, and will aim to stay consistent with the style of the currentpage, which is nealy all text. Some enhancements with screenshots may be added to clarify things where needed, such is currently being done in the ""visual setup guide for Windows"" tutorial currently on the site.In addition to OS types which already exist in the tutorials, I will try to see if it is possible to put metapyand/or MeTa on ChromeOS. Online searches reveal that it is possible to put python and gcc on ChromeOS, I will experiment to see if that can be extended to include MeTa or metapy. This will be a whole new OS where it hasn't been done before, which I feel will be a nice addition to the MeTa/metapy universe, especially as the popularity of Chromebooks grows.Regarding whether this fulfills the 20 hours per team member, I believe this will easily do that. This is a 1 person team, so it only needs to cover 20 hours total. To ensure that the results from the tutorials are correct and repeatable, I will be sure to use fresh OS installs so that nothing that was already on an existing system from previous installs would cause erroneous results. In the case of writing a whole new tutorial, once it is complete, it will be repeated following the tutorial exactly on a new fresh install, this takes extra time, but I feel it is necessary to be sure the tutorial is accurate. I have a few old PCs that aren't being used for anything at the moment, so this way one can be re-imaged with a fresh OS install in parallel while working with another one, so it will take some time to do that , but it won't be too excessive. By also trying to make it work on ChromeOS, I feel this will easily go over 20 hours total of work, as I expect that experiment to take quite a bit of time. Time spent will be tracked, so that if the total time is under 20 hours, more OSes/Linux versions will be added to ensure well over 20 hours of time was put into the project. Files for the CS410 Course Project: (see above file list) The software usage tutorial video is located here: (and a link is also in the video_link.md file) https://youtu.be/nbpwsQBnId0 The MeTA_tutorial.md and metapy_tutorial.md files are the main deliverables for this project, and can be viewed by clicking on them. The ""Project Final Report and Documentation.pdf"" file contains the final report as well as some suggested future fixes for MeTA and metapy, as well as some suggested MeTA and metapy improvements for following CS410 classes. Finally, there are also the ProjectProposal.pdf and ProgressReport.pdf files that were submitted previously. Video is located here: https://youtu.be/nbpwsQBnId0"
https://github.com/theRocket/CourseProject	"arXiv:1607.01759v3 [cs.CL] 9 Aug 2016BagofTricksforEfficientTextClassificationArmandJoulinEdouardGravePiotrBojanowskiTomasMikolovFacebookAIResearch{ajoulin,egrave,bojanowski,tmikolov}@fb.comAbstractThispaperexploresasimpleandefficientbaselinefortextclassification.Ourex-perimentsshowthatourfasttextclassi-fierfastTextisoftenonparwithdeeplearningclassifiersintermsofaccuracy,andmanyordersofmagnitudefasterfortrainingandevaluation.WecantrainfastTextonmorethanonebillionwordsinlessthantenminutesusingastandardmulticoreCPU,andclassifyhalfamillionsentencesamong312Kclassesinlessthanaminute.1IntroductionTextclassificationisanimportanttaskinNaturalLanguageProcessingwithmanyapplications,suchaswebsearch,informationretrieval,rankinganddocumentclassification(Deerwesteretal.,1990;PangandLee,2008).Recently,modelsbasedonneuralnetworkshavebecomeincreasinglypopular(Kim,2014;ZhangandLeCun,2015;Conneauetal.,2016).Whilethesemodelsachieveverygoodperformanceinpractice,theytendtoberelativelyslowbothattrainandtesttime,limitingtheiruseonverylargedatasets.Meanwhile,linearclassifiersareof-tenconsideredasstrongbaselinesfortextclassificationproblems(Joachims,1998;McCallumandNigam,1998;Fanetal.,2008).Despitetheirsimplicity,theyoftenobtainstate-of-the-artperformancesiftherightfeaturesareused(WangandManning,2012).Theyalsohavethepotentialtoscaletoverylargecor-pus(Agarwaletal.,2014).Inthiswork,weexplorewaystoscalethesebaselinestoverylargecorpuswithalargeoutputspace,inthecontextoftextclassification.Inspiredbytherecentworkinefficientwordrepresentationlearning(Mikolovetal.,2013;Levyetal.,2015),weshowthatlinearmodelswitharankconstraintandafastlossapproximationcantrainonabillionwordswithintenminutes,whileachievingperfor-manceonparwiththestate-of-the-art.Weevalu-atethequalityofourapproachfastText1ontwodifferenttasks,namelytagpredictionandsentimentanalysis.2ModelarchitectureAsimpleandefficientbaselineforsentenceclassificationistorepresentsentencesasbagofwords(BoW)andtrainalinearclassifier,e.g.,alogisticregressionoranSVM(Joachims,1998;Fanetal.,2008).However,linearclassifiersdonotshareparametersamongfeaturesandclasses.Thispossiblylimitstheirgeneralizationinthecontextoflargeoutputspacewheresomeclasseshaveveryfewexamples.Commonsolutionstothisproblemaretofactorizethelinearclas-sifierintolowrankmatrices(Schutze,1992;Mikolovetal.,2013)ortousemultilayerneuralnetworks(CollobertandWeston,2008;Zhangetal.,2015).Figure1showsasimplelinearmodelwithrankconstraint.ThefirstweightmatrixAisalook-uptableoverthewords.Thewordrepresentationsarethenaveragedintoatextrepresentation,whichisinturnfedtoalinearclassifier.Thetextrepresenta-1https://github.com/facebookresearch/fastTextx1x2...xN-1xNhiddenoutputFigure1:ModelarchitectureoffastTextforasentencewithNngramfeaturesx1,...,xN.Thefeaturesareembeddedandaveragedtoformthehiddenvariable.tionisanhiddenvariablewhichcanbepotentiallybereused.ThisarchitectureissimilartothecbowmodelofMikolovetal.(2013),wherethemiddlewordisreplacedbyalabel.Weusethesoftmaxfunctionftocomputetheprobabilitydistributionoverthepredefinedclasses.ForasetofNdoc-uments,thisleadstominimizingthenegativelog-likelihoodovertheclasses:-1NNn=1ynlog(f(BAxn)),wherexnisthenormalizedbagoffeaturesofthen-thdocument,ynthelabel,AandBtheweightmatri-ces.Thismodelistrainedasynchronouslyonmul-tipleCPUsusingstochasticgradientdescentandalinearlydecayinglearningrate.2.1HierarchicalsoftmaxWhenthenumberofclassesislarge,computingthelinearclassifieriscomputationallyexpensive.Moreprecisely,thecomputationalcomplexityisO(kh)wherekisthenumberofclassesandhthedi-mensionofthetextrepresentation.Inordertoim-proveourrunningtime,weuseahierarchicalsoft-max(Goodman,2001)basedontheHuffmancod-ingtree(Mikolovetal.,2013).Duringtraining,thecomputationalcomplexitydropstoO(hlog2(k)).Thehierarchicalsoftmaxisalsoadvantageousattesttimewhensearchingforthemostlikelyclass.Eachnodeisassociatedwithaprobabilitythatistheprobabilityofthepathfromtheroottothatnode.Ifthenodeisatdepthl+1withparentsn1,...,nl,itsprobabilityisP(nl+1)=li=1P(ni).Thismeansthattheprobabilityofanodeisalwayslowerthantheoneofitsparent.Exploringthetreewithadepthfirstsearchandtrackingthemaximumprobabilityamongtheleavesallowsustodiscardanybranchassociatedwithasmallprobability.Inpractice,weobserveareductionofthecomplexitytoO(hlog2(k))attesttime.Thisapproachisfur-therextendedtocomputetheT-toptargetsatthecostofO(log(T)),usingabinaryheap.2.2N-gramfeaturesBagofwordsisinvarianttowordorderbuttakingexplicitlythisorderintoaccountisoftencomputa-tionallyveryexpensive.Instead,weuseabagofn-gramsasadditionalfeaturestocapturesomepar-tialinformationaboutthelocalwordorder.Thisisveryefficientinpracticewhileachievingcompa-rableresultstomethodsthatexplicitlyusetheor-der(WangandManning,2012).Wemaintainafastandmemoryefficientmappingofthen-gramsbyusingthehashingtrick(Weinbergeretal.,2009)withthesamehash-ingfunctionasinMikolovetal.(2011)and10Mbinsifweonlyusedbigrams,and100Motherwise.3ExperimentsWeevaluatefastTextontwodifferenttasks.First,wecompareittoexistingtextclassifersontheproblemofsentimentanalysis.Then,weevaluateitscapacitytoscaletolargeoutputspaceonatagpredictiondataset.Notethatourmodelcouldbeim-plementedwiththeVowpalWabbitlibrary,2butweobserveinpractice,thatourtailoredimplementationisatleast2-5xfaster.3.1SentimentanalysisDatasetsandbaselines.Weemploythesame8datasetsandevaluationprotocolofZhangetal.(2015).Wereportthen-gramsandTFIDFbaselinesfromZhangetal.(2015),aswellasthecharacterlevelconvolutionalmodel(char-CNN)ofZhangandLeCun(2015),thecharacterbasedconvolutionrecurrentnet-work(char-CRNN)of(XiaoandCho,2016)andtheverydeepconvolutionalnetwork(VDCNN)ofConneauetal.(2016).Wealsocompare2Usingtheoptions--nn,--ngramsand--logmultiModelAGSogouDBPYelpP.YelpF.Yah.A.Amz.F.Amz.P.BoW(Zhangetal.,2015)88.892.996.692.258.068.954.690.4ngrams(Zhangetal.,2015)92.097.198.695.656.368.554.392.0ngramsTFIDF(Zhangetal.,2015)92.497.298.795.454.868.552.491.5char-CNN(ZhangandLeCun,2015)87.295.198.394.762.071.259.594.5char-CRNN(XiaoandCho,2016)91.495.298.694.561.871.759.294.1VDCNN(Conneauetal.,2016)91.396.898.795.764.773.463.095.7fastText,h=1091.593.998.193.860.472.055.891.2fastText,h=10,bigram92.596.898.695.763.972.360.294.6Table1:Testaccuracy[%]onsentimentdatasets.FastTexthasbeenrunwiththesameparametersforallthedatasets.Ithas10hiddenunitsandweevaluateitwithandwithoutbigrams.Forchar-CNN,weshowthebestreportednumberswithoutdataaugmentation.ZhangandLeCun(2015)Conneauetal.(2016)fastTextsmallchar-CNNbigchar-CNNdepth=9depth=17depth=29h=10,bigramAG1h3h24m37m51m1sSogou--25m41m56m7sDBpedia2h5h27m44m1h2sYelpP.--28m43m1h093sYelpF.--29m45m1h124sYah.A.8h1d1h1h332h5sAmz.F.2d5d2h454h207h9sAmz.P.2d5d2h454h257h10sTable2:Trainingtimeforasingleepochonsentimentanalysisdatasetscomparedtochar-CNNandVDCNN.toTangetal.(2015)followingtheirevaluationprotocol.Wereporttheirmainbaselinesaswellastheirtwoapproachesbasedonrecurrentnetworks(Conv-GRNNandLSTM-GRNN).Results.WepresenttheresultsinFigure1.Weuse10hiddenunitsandrunfastTextfor5epochswithalearningrateselectedonavalida-tionsetfrom{0.05,0.1,0.25,0.5}.Onthistask,addingbigraminformationimprovestheperfor-manceby1-4%.Overallouraccuracyisslightlybetterthanchar-CNNandchar-CRNNand,abitworsethanVDCNN.Notethatwecanincreasetheaccuracyslightlybyusingmoren-grams,forexamplewithtrigrams,theperformanceonSogougoesupto97.1%.Finally,Figure3showsthatourmethodiscompetitivewiththemethodspre-sentedinTangetal.(2015).Wetunethehyper-parametersonthevalidationsetandobservethatusingn-gramsupto5leadstothebestperfor-mance.UnlikeTangetal.(2015),fastTextdoesnotusepre-trainedwordembeddings,whichcanbeexplainedthe1%differenceinaccuracy.ModelYelp'13Yelp'14Yelp'15IMDBSVM+TF59.861.862.440.5CNN59.761.061.537.5Conv-GRNN63.765.566.042.5LSTM-GRNN65.167.167.645.3fastText64.266.266.645.2Table3:ComparisionwithTangetal.(2015).Thehyper-parametersarechosenonthevalidationset.Wereportthetestaccuracy.Trainingtime.Bothchar-CNNandVDCNNaretrainedonaNVIDIATeslaK40GPU,whileourmodelsaretrainedonaCPUusing20threads.Ta-ble2showsthatmethodsusingconvolutionsaresev-eralordersofmagnitudeslowerthanfastText.Whileitispossibletohavea10xspeedupforchar-CNNbyusingmorerecentCUDAimplemen-tationsofconvolutions,fastTexttakeslessthanaminutetotrainonthesedatasets.TheGRNNsmethodofTangetal.(2015)takesaround12hoursperepochonCPUwithasinglethread.Ourspeed-InputPredictionTagstaiyoucon2011digitals:individualsdigitalpho-tosfromtheanimeconventiontaiyoucon2011inmesa,arizona.ifyouknowthemodeland/orthecharacter,pleasecomment.#cosplay#24mm#anime#animeconvention#arizona#canon#con#convention#cos#cosplay#costume#mesa#play#taiyou#taiyoucon2012twincitiespride2012twincitiespridepa-rade#minneapolis#2012twincitiesprideparade#min-neapolis#mn#usabeagleenjoysthesnowfall#snow#2007#beagle#hillsboro#january#maddison#maddy#oregon#snowchristmas#christmas#cameraphone#mobileeuclidavenue#newyorkcity#cleveland#euclidavenueTable4:ExamplesfromthevalidationsetofYFCC100MdatasetobtainedwithfastTextwith200hiddenunitsandbigrams.Weshowafewcorrectandincorrecttagpredictions.upcomparedtoneuralnetworkbasedmethodsin-creaseswiththesizeofthedataset,goinguptoatleasta15,000xspeed-up.3.2TagpredictionDatasetandbaselines.Totestscalabilityofourapproach,furtherevaluationiscarriedontheYFCC100Mdataset(Thomeeetal.,2016)whichconsistsofalmost100Mimageswithcap-tions,titlesandtags.Wefocusonpredictingthetagsaccordingtothetitleandcaption(wedonotusetheimages).Weremovethewordsandtagsoccurringlessthan100timesandsplitthedataintoatrain,validationandtestset.Thetrainsetcontains91,188,648examples(1.5Btokens).Thevalidationhas930,497examplesandthetestset543,424.Thevocabularysizeis297,141andthereare312,116uniquetags.Wewillreleaseascriptthatrecreatesthisdatasetsothatournumberscouldbereproduced.Wereportprecisionat1.Weconsiderafrequency-basedbaselinewhichpredictsthemostfrequenttag.Wealsocom-parewithTagspace(Westonetal.,2014),whichisatagpredictionmodelsimilartoours,butbasedontheWsabiemodelofWestonetal.(2011).WhiletheTagspacemodelisdescribedusingconvolutions,weconsiderthelinearversion,whichachievescom-parableperformancebutismuchfaster.Resultsandtrainingtime.Table5presentsacomparisonoffastTextandthebaselines.WerunfastTextfor5epochsandcompareittoTagspacefortwosizesofthehiddenlayer,i.e.,50Modelprec@1RunningtimeTrainTestFreq.baseline2.2--Tagspace,h=5030.13h86hTagspace,h=20035.65h3215hfastText,h=5031.26m4048sfastText,h=50,bigram36.77m4750sfastText,h=20041.110m341m29fastText,h=200,bigram46.113m381m37Table5:Prec@1onthetestsetfortagpredictiononYFCC100M.Wealsoreportthetrainingtimeandtesttime.Testtimeisreportedforasinglethread,whiletraininguses20threadsforbothmodels.and200.Bothmodelsachieveasimilarperfor-mancewithasmallhiddenlayer,butaddingbi-gramsgivesusasignificantboostinaccuracy.Attesttime,Tagspaceneedstocomputethescoresforalltheclasseswhichmakesitrelativelyslow,whileourfastinferencegivesasignificantspeed-upwhenthenumberofclassesislarge(morethan300Khere).Overall,wearemorethananorderofmag-nitudefastertoobtainmodelwithabetterquality.Thespeedupofthetestphaseisevenmoresignifi-cant(a600xspeedup).Table4showssomequali-tativeexamples.4DiscussionandconclusionInthiswork,weproposeasimplebaselinemethodfortextclassification.Unlikeunsupervisedlytrainedwordvectorsfromword2vec,ourwordfeaturescanbeaveragedtogethertoformgoodsentencerepre-sentations.Inseveraltasks,fastTextobtainsper-formanceonparwithrecentlyproposedmethodsin-spiredbydeeplearning,whilebeingmuchfaster.Althoughdeepneuralnetworkshaveintheorymuchhigherrepresentationalpowerthanshallowmodels,itisnotclearifsimpletextclassificationproblemssuchassentimentanalysisaretherightonestoeval-uatethem.Wewillpublishourcodesothattheresearchcommunitycaneasilybuildontopofourwork.Acknowledgement.WethankGabrielSynnaeve,Herv'eG'egou,JasonWestonandL'eonBottoufortheirhelpandcomments.WealsothankAlexisCon-neau,DuyuTangandZichaoZhangforprovidinguswithinformationabouttheirmethods.References[Agarwaletal.2014]AlekhAgarwal,OlivierChapelle,MiroslavDud'ik,andJohnLangford.2014.Areliableeffectiveterascalelinearlearningsystem.JMLR.[CollobertandWeston2008]RonanCollobertandJasonWeston.2008.Aunifiedarchitecturefornaturallan-guageprocessing:Deepneuralnetworkswithmulti-tasklearning.InICML.[Conneauetal.2016]AlexisConneau,HolgerSchwenk,Lo""icBarrault,andYannLecun.2016.Verydeepcon-volutionalnetworksfornaturallanguageprocessing.arXivpreprintarXiv:1606.01781.[Deerwesteretal.1990]ScottDeerwester,SusanTDu-mais,GeorgeWFurnas,ThomasKLandauer,andRichardHarshman.1990.Indexingbylatentsemanticanalysis.JournaloftheAmericansocietyforinforma-tionscience.[Fanetal.2008]Rong-EnFan,Kai-WeiChang,Cho-JuiHsieh,Xiang-RuiWang,andChih-JenLin.2008.Li-blinear:Alibraryforlargelinearclassification.JMLR.[Goodman2001]JoshuaGoodman.2001.Classesforfastmaximumentropytraining.InICASSP.[Joachims1998]ThorstenJoachims.1998.Textcatego-rizationwithsupportvectormachines:Learningwithmanyrelevantfeatures.Springer.[Kim2014]YoonKim.2014.Convolutionalneuralnet-worksforsentenceclassification.InEMNLP.[Levyetal.2015]OmerLevy,YoavGoldberg,andIdoDagan.2015.Improvingdistributionalsimilaritywithlessonslearnedfromwordembeddings.TACL.[McCallumandNigam1998]AndrewMcCallumandKa-malNigam.1998.Acomparisonofeventmodelsfornaivebayestextclassification.InAAAIworkshoponlearningfortextcategorization.[Mikolovetal.2011]Tom'aVsMikolov,AnoopDeoras,DanielPovey,Luk'aVsBurget,andJanVCernock`y.2011.Strategiesfortraininglargescaleneuralnetworklan-guagemodels.InWorkshoponAutomaticSpeechRecognitionandUnderstanding.IEEE.[Mikolovetal.2013]TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.2013.Efficientestimationofwordrepresentationsinvectorspace.arXivpreprintarXiv:1301.3781.[PangandLee2008]BoPangandLillianLee.2008.Opinionminingandsentimentanalysis.Foundationsandtrendsininformationretrieval.[Schutze1992]HinrichSchutze.1992.Dimensionsofmeaning.InSupercomputing.[Tangetal.2015]DuyuTang,BingQin,andTingLiu.2015.Documentmodelingwithgatedrecurrentneuralnetworkforsentimentclassification.InEMNLP.[Thomeeetal.2016]BartThomee,DavidAShamma,GeraldFriedland,BenjaminElizalde,KarlNi,Dou-glasPoland,DamianBorth,andLi-JiaLi.2016.Yfcc100m:Thenewdatainmultimediaresearch.vol-ume59,pages64-73.ACM.[WangandManning2012]SidaWangandChristopherDManning.2012.Baselinesandbigrams:Simple,goodsentimentandtopicclassification.InACL.[Weinbergeretal.2009]KilianWeinberger,AnirbanDas-gupta,JohnLangford,AlexSmola,andJoshAtten-berg.2009.Featurehashingforlargescalemultitasklearning.InICML.[Westonetal.2011]JasonWeston,SamyBengio,andNicolasUsunier.2011.Wsabie:Scalinguptolargevocabularyimageannotation.InIJCAI.[Westonetal.2014]JasonWeston,SumitChopra,andKeithAdams.2014.#tagspace:Semanticembed-dingsfromhashtags.InEMNLP.[XiaoandCho2016]YijunXiaoandKyunghyunCho.2016.Efficientcharacter-leveldocumentclassificationbycombiningconvolutionandrecurrentlayers.arXivpreprintarXiv:1602.00367.[ZhangandLeCun2015]XiangZhangandYannLeCun.2015.Textunderstandingfromscratch.arXivpreprintarXiv:1502.01710.[Zhangetal.2015]XiangZhang,JunboZhao,andYannLeCun.2015.Character-levelconvolutionalnetworksfortextclassification.InNIPS. CS 410 Final Project Gluon NLP with MXNet on AWS Sagemaker Training for Yelp Sentiment Analysis Text Information Systems Fall 2020 Ryan Rickerts (ryanjr3) https://github.com/theRocket/CourseProject CS 410 FINAL PROJECTUIUC MCS-DS1CS 410 Final Project Gluon NLP with MXNet on AWS Sagemaker Yelp Sentiment Analysis Motivation State-of-the-art deep learning models in natural language processing (NLP) are fascinating to read about and try to understand for a graduate-level computer science student. It is also possible for a hobbyist or student to obtain pre-trained models from these research breakthroughs and perform inference using the meager hardware at their disposal while working and studying from home (i.e. no access to lab clusters at an institution). Perhaps a deeper and more satisfying approach for investigating this multi-layered architecture with broad areas of application is to train a dataset by oneOs self, much like raising your own puppy to learn preferred tricks. However, time and compute resources can be an intimidating constraint in these compute-intensive algorithms. For this project, I aimed to Thnd a relatively cutting edge approach in NLP where the training can be replicated by a hobbyist developer. The Gluon Project aims to meet this need. According to this blog post:  Symptom: Natural language processing papers are difThcult to reproduce. The quality of open source implementations available on Github varies a lot, and maintainers can stop supporting the projects.  GluonNLP prescription: Reproduction of latest research results. Frequent updates of the reproduction code, which comes with training scripts, hyper-parameters, runtime logs etc. 1A broad description from the GluonNLP website is as follows: https://medium.com/apache-mxnet/gluonnlp-deep-learning-toolkit-for-natural-language-1processing-98e684131c8aCS 410 FINAL PROJECTUIUC MCS-DS2GluonNLP provides implementations of the state-of-the-art (SOTA) deep learning models in NLP, and build blocks for text data pipelines and models. It is designed for engineers, researchers, and students to fast prototype research ideas and products based on these models. 2From their model zoo, we selected the Text ClassiThcation example, which trains the FastText classiThcation model on the Yelp review dataset. This is a binary classiThcation dataset (positive vs. negative sentiment), and we aim to replicate their validation accuracy score of 94% in a reasonable time frame and cost. For optimum training times, we turned to NVIDIA Tesla V100 GPUs available on Amazon Web Services (or AWS) Sagemaker instances. These are available on-demand and can be powered up just for training times to keep costs at a minimum. We investigate adapting the FastText scripts provided by the Gluon project to the AWS environment, such as loading required Jupyter kernels and dependent libraries for this algorithm, reading and writing data from S3 buckets, and of course keeping compute regions and permissions (or roles) in good order. https://nlp.gluon.ai/#about-gluonnlp2CS 410 FINAL PROJECTUIUC MCS-DS3Fast-text Word N-gram This model is a slight variation of the one published by the Facebook AI Research team in 2016 in a paper called OBag of tricks for efThcient text classiThcationO by Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. The aim of this approach, which they call fastText or a Olibrary for efThcient text classiThcation and representation learning,O is also to increase training speed over the dominant deep learning algorithms of the day. The abstract states: Our experiments show that our fast text classiTher fastText is often on par with deep learning classiThers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among ~312K classes in less than a minute. 3The Gluon-provided code used in this project, found at text_classiThcation/fasttext_word_ngram.py in the repo, does not reference any code from the fastText project. It uses only gluonnlp and mxnet python libraries, so it appears to be a complete rewrite of the approach. It is modiThed here to meet the requirements of running on AWS infrastructure, which we cover in the next section, and becomes text_classiThcation/fasttext_word_ngram_aws.py in the Github repo. Apache MXNet is an open-source deep learning framework that is well supported on AWS and is rivaled by TensorFlow and PyTorch in popularity and performance on NLP. The advantage of MXNet (short for mix net) is the broad language support of Python, R, C++, Julia, and Scala. It also scales well: OAnother advantage is that the models built using MXNet are portable such that they can Tht in small amounts of memory. So, once your model is trained and tested, it can be easily deployed to mobile devices or connected systems. MXNets are scalable to be https://arxiv.org/abs/1607.017593CS 410 FINAL PROJECTUIUC MCS-DS4used on multiple machines and GPU simultaneously. This is why Amazon has chosen this framework for its deep learning web services.O 4In this code, the Gluon authors create two classes: Y=a MeanPoolingLayer N a block for mean pooling of encoder features N which takes a gluon.HybridBlock as a parameter. Y=a FastTextClassiThcationModelNthe trained embeddings layerNwhich takes a HybridBlock as a parameter. The rest of the logic centers around argument parsing to the main function, the creation of n-grams (defaults to 1 and recommended <=2 for large datasets), data loading, preprocessing, labeling, and evaluating accuracy of the training against a test dataset. Scripts to fetch the the training and test datasets from Google Drive are hosted at fastTextOs Github repo, but these proved problematic given the response time of 5the downloads. I manually fetched the Yelp Review Polarity set only, then modiThed the code to normalize the text. This modiThed bash script is at text_classiThcation/data_fetch.sh in the project repo, and the CSV Thles are placed in the data/ sub-directory. Once this was prepared, we can start running the python script locally with the recommended parameters as follows: python fasttext_word_ngram.py --input data/yelp_review_polarity.train \ --output data/yelp_review_polarity.gluon \ --validation data/yelp_review_polarity.test \ --ngrams 1 --epochs 10 --lr 0.1 --emsize 100 This set of parameters is notably missing the ONgpu=0O ssag for the local run only. This is because my laptop, a 2019 MacBook Pro with a 6-Core Intel i7 CPU, has a AMD Radeon Pro 5300M graphics card that does not implement CUDA architecture. This job will run on the CPU instead, and while threading does not appear to be implemented, according to MXNet documentation: https://analyticsindiamag.com/mxnet-tutorial-complete-guide-with-hands-on-implementation-of-deep-learning-4framework/ https://github.com/facebookresearch/fastText/blob/master/classiThcation-example.sh5CS 410 FINAL PROJECTUIUC MCS-DS5OFor a CPU, MXNet will allocate data on main memory, and try to use all CPU cores as possible, even if there is more than one CPU socket.O 6The Activity Monitor on the laptop shows the CPU being pegged at 100% and no use of the GPU: With 35000 batches conThgured to run on 464402 vocabulary words and 560000 sentences, the Thrst attempt never even completed a single epoch. I quit the job after 8 hours of processing time and only 17000 batches completed. So now we move this workload to the GPUs hosted on the cloud! AWS Sagemaker and NVIDIA Tesla V100 GPUs I implement this training job using high-end P3 AWS Sagemaker instances. 7According to the table of instance sizes listed at the bottom of the above linked page, the cheapest instance offered - p3.2xlarge - provides one Tesla V100 GPU with 16GB of GPU memory for $3.07/hr on demand. A helpful reference were these published benchmarks of NVIDIA GPUs provided in TensorFLOPs, which are units of ssoating-point arithmetic performance aimed at NVIDIA GPU hardware called Tensor Cores: OA new, specialized Tensor Core unit was introduced with OVoltaO generation GPUs. It combines a multiply of two FP16 units (into a full precision product) with a FP32 accumulate operationNthe exact operations used in Deep Learning Training https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/getting-started/crash-course/6-6use_gpus.html https://aws.amazon.com/ec2/instance-types/p3/7CS 410 FINAL PROJECTUIUC MCS-DS6computation. NVIDIA is now measuring GPUs with Tensor Cores by a new deep learning performance metric: a new unit called TensorTFLOPS.O 8According to that metric, the Tesla V100 GPU rates around 112-125 TensorTFLOPS (exact Thgure depending on the use of PCI-Express or SXM2 SKU interfaces). For comparison, the maximum known deep learning performance at any precision of the Tesla K80 is 5.6 TFLOPS for FP32. This GPU is provided on the P2 Sagemaker instances, and for 1 GPU on the p2.xlarge instance size, the cost is $0.90/hr. If we can attain a 20x performance increase on our training job for approx. 4x compute resource cost, that seems like a great win! The clock time used by the p3.2xlarge instance and costs incurred for several training runs is discussed in the Results sub-section below. Data Workssow The Thrst step on AWS Services is to select a region for all our work, in this case US East (Ohio) or us-east-2. This is important for several reasons: one, the data transferred between Amazon S3 buckets and Amazon EC2 instances in the same Region and account is relatively straightforward and free, and our training data sizes are signiThcant (where I live in a remote area, it took 30 minutes to upload due to low bandwidth). Two, the cost of the P3 instances (also signiThcant) are determined by region, and we want to accurately estimate our costs for this workload to avoid a surprise bill. Furthermore, I had to request access to these highly specialized machines on my account via a support ticket, and their staff response time was not immediate. On the Thrst attempt, they enabled a different region than I had uploaded the training data, so I had to sync it across to another S3 bucket, now called sagemaker-cs410-Thnalproj and pictured below. Sagemaker has an S3 Uploader library available for outputting the model parameters (a local Thle save is Thrst required Thrst) and returning the training resultsNnet.paramsN to the S3 bucket under a yelp_sentiment_polarity.gluon directory, as pictured below. These parameters will be needed for later inference use cases outside this JupyterNotebook instance, so they must ofssoaded for access. https://www.microway.com/knowledge-center-articles/comparison-of-nvidia-geforce-gpus-and-nvidia-tesla-gpus/8CS 410 FINAL PROJECTUIUC MCS-DS7def save_model(net, output_file): file_name = ""net.params"" # local version net.save_parameters(file_name) S3Uploader.upload(file_name,output_file) Since we established this bucket in the same region, the S3 Uploader utility only requires the bucket name to perform this work. That variable was conThgured in the Thrst JupyterNotebook cell, with: bucket = 'sagemaker-cs410-finalproj' Although you can also use a default bucket for the session, which is initialized like this: bucket = Session().default_bucket() For streaming lines from the data in S3 to the data parsing method, we used a library called smart_open which simulates reading in data from a local Thle in an 9iterable function such as a for-in loop, like: for line in open(filename): tokens = line.split(',', 1) labels.append(tokens[0].strip()) data.append(tokens[1].strip()) return labels, data https://pypi.org/project/smart-open/9CS 410 FINAL PROJECTUIUC MCS-DS8This proved very effective and the data was parsed quickly into labels and normalized text data for the training run. Training Results The one Tesla V100 GPU performing text classiThcation on our p3.2xlarge instance exceeded expectations and churned out training and test results for 10 Epochs in under 20 mins. Furthermore, the same accuracy of 94% was achieved as mentioned on the Gluon page where we sourced this code. SGD performed slightly better than Adam as an optimizer, and this is discussed in the Further Work section below. Jupyter notebook run 1 with `adam` as optimizer, 10 Epochs Y=Highest Test Accuracy: 0.9401578947368421 (Epoch 8) Y=Final Test Accuracy: 0.939921052631579, Test Loss: 0.17803387705344548 (Epoch 10) Jupyter notebook run 2 with `sgd` as optimizer, 10 Epochs Y=Highest Test Accuracy: 0.9403157894736842 (Epoch 8) Y=Final Test Accuracy: 0.9400526315789474, Test Loss: 0.17815197125596924 (Epoch 10) Jupyter notebook run 3 with `sgd` as optimizer, 25 Epochs Y=Highest Test Accuracy: 0.9403157894736842 (Epoch 8) Y=Final Test Accuracy: 0.9397105263157894, Test Loss: 0.17758843273002864 (Epoch 25) Y=Note, the accuracy above was reached at Epoch 18 and remained stable. CS 410 FINAL PROJECTUIUC MCS-DS9To reiterate, my MacBook Pro CPU (ctx=cpu(0)) never completed a single epoch after 8 hours of run time, and the V100 GPU (ctx=gpu(0)) Thnished one epoch in less than 2 minutes! The entire job run of 10 epochs required only 17 minutes total, which is very satisfying. The costs incurred for several hours of experimenting with the AWS SageMaker JupyterNotebook environment, plus the three training runs above (and a few failed ones when parameter output to Thle was not conThgured correctly) remained under $10. So we were able to capture reliable and conclusive training work at the expected cost of around $3/hr! The Cost Explorer returns this data analysis by service typically the next day. Importantly, I shut off my P3 EC2 instance whenever I was not using it, and I deleted other experimental instances, such as P2, when I moved onto another solution. Not performing this cleanup task is a painful way to discover an exorbitant bill from AWS at the end of the month! CS 410 FINAL PROJECTUIUC MCS-DS10Further Work 1) The Gluon team notes that they simpliThed the model somewhat for training speed as follows: OWe have added dropout to the Thnal layer, and the optimizer is changed from OsgdO to OadamO These are made for demo purposes and we can get very good numbers with original settings too, but a complete async sgd with batch size = 1, might be very slow for training using a GPU.O 10I was able to pass OsgdO (or Stochastic Gradient Descent) as an optimizer parameter and get slightly better testing accuracy over OadamO (0.94032 vs. .94016) with no noticeable decrease in training speed. I did not remove the dropout, increase the size of n-grams, nor change the batch size, so these could be restored to the original fastText algorithm for more benchmarking on this particular GPU instance. 2) Rather than Thring up JupyterLab in our P3 instance and copy/pasting in the custom algorithm for our training job, Amazon SageMaker can interact with a Docker container based on a pre-built image. This would be a better practice for continued use of this approach, since it can easily be deployed into different regions and scaled up or down on different EC2 instances for more or less GPU power. 11This workssow is covered in more detail here, which was also an early inspiration for this project (although they use GluonCV): Deploying custom models built with Gluon and Apache MXNet on Amazon SageMaker OWhen you build models with the Apache MXNet deep learning framework, you can take advantage of the expansive model zoo provided by GluonCV to quickly train state-of-the-art computer vision algorithms for image and video processing. A typical development environment for training consists of a Jupyter notebook hosted on a https://nlp.gluon.ai/model_zoo/text_classiThcation/index.html10 https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html11CS 410 FINAL PROJECTUIUC MCS-DS11compute instance conThgured by the operating data scientist. To make sure this environment is replicated during use in production, the environment is wrapped inside a Docker container, which is launched and scaled according to the expected load.O 123) Elastic Inference MXNet on Amazon SageMaker has support for Elastic Inference, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to accelerator_type to your deploy call. predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, accelerator_type='ml.eia1.medium') I felt great enjoyment in this implementing this project and learned a lot about NLP using AWS Sagemaker GPUs. I look forward to experimenting more with Gluon and MXNet to obtain rapid training times on state-of-the-art deep learning algorithms. https://aws.amazon.com/blogs/machine-learning/deploying-custom-models-built-with-gluon-and-apache-12mxnet-on-amazon-sagemaker/CS 410 FINAL PROJECTUIUC MCS-DS12 Training FastText ""Bag of Tricks"" using Gluon & MXNet on AWS GPUs (ryanjr3) CS 410 - Text Information Systems (MCS-DS at UIUC) Final Project Report: Gluon NLP with MXNet on AWS Sagemaker (PDF) AWS Services presentation (YouTube) The project formerly known as BERT & ERNIE (benchmarking on Google Cloud TPUs): Please see the October README commit for the original project proposal. Training times and cloud compute costs for this model were discovered to be too prohibitive for our project timeline and resources. Gluon for NLP and MXNet The aim of my project is to become better acquainted with the Gluon API for Natural Language Processing (NLP). GluonNLP provides implementations of the state-of-the-art (SOTA) deep learning models in NLP, and build blocks for text data pipelines and models. It is designed for engineers, researchers, and students to fast prototype research ideas and products based on these models. For example, it can easily provide the cosine similarity of two word vectors with the following simple python function, cos_similarity(): ``` import mxnet as mx import gluonnlp as nlp glove = nlp.embedding.create('glove', source='glove.6B.50d') def cos_similarity(embedding, word1, word2): vec1, vec2 = embedding[word1], embedding[word2] return mx.nd.dot(vec1, vec2) / (vec1.norm() * vec2.norm()) print('Similarity between ""baby"" and ""infant"": ', cos_similarity(glove, 'baby', 'infant').asnumpy()[0]) ``` The Gluon API provides user-friendly access to the Apache MXNet library for Deep Learning, which advertises itself as being a ""truly open source deep learning framework suited for flexible research prototyping and production."" The Amazon Web Services (AWS) Sagemaker instances support MXNet running on Python 3.6 with the conda_mxnet_p36 kernel selected for the Jupyter Notebook. Gluon NLP dependencies are easily added to the notebook by running a cell with: !pip install gluonnlp AWS Sagemaker DL performance on NVIDIA GPUs I plan to implement the training job using high-end P3 AWS Sagemaker instances to benchmark rapid training of models using python v3.6. According to the table of instance sizes listed at the bottom of the above linked page, the cheapest instance offered - p3.2xlarge - provides 1 Tesla V100 GPU with 16GB of GPU memory for $3.07/hr on demand. We have published benchmarks of NVIDIA GPUs provided in TensorFLOPs, which are units of floating-point arithmetic performance aimed at NVIDIA GPU hardware called Tensor Cores: A new, specialized Tensor Core unit was introduced with ""Volta"" generation GPUs. It combines a multiply of two FP16 units (into a full precision product) with a FP32 accumulate operation--the exact operations used in Deep Learning Training computation. NVIDIA is now measuring GPUs with Tensor Cores by a new deep learning performance metric: a new unit called TensorTFLOPS. According to that metric, the Tesla V100 GPU rates around 112-125 TensorTFLOPS (exact figure depending on the use of PCI-Express or SXM2 SKU interfaces). For comparison, the maximum known deep learning performance at any precision of the Tesla K80 is 5.6 TFLOPS for FP32. This GPU is provided on the P2 Sagemaker instances, and for 1 GPU on the p2.xlarge instance size, the cost is $0.90/hr. If we can attain a 20x performance increase on our training job for approx. 4x compute resource cost, that seems like a great win! I also compare running this training job on my Macbook Pro CPU - a 2.6 GHz 6-Core Intel Core i7. Since the included AMD Radeon Pro 5300M graphics card does not implement CUDA architecture, we must deleted the following option when running the training job: --gpu=0. Otherwise, this signals the index number of the GPU to use. Since we selected AWS instances with only one GPU, this flag will always be zero. FastText ""Bag of Tricks"" and Yelp Sentiment Classification The primary influence for this project was an entry hosted at nlp.gluon.ai for Text Classification called Fast-text Word N-gram. It leverages the fastText python library used for ""efficient text classification and representation learning"" and developed at Facebook research. The paper was also published by the Facebook AI Research team in 2016 and is called Bag of Tricks for Efficient Text Classification (the full PDF is included in this repo). They claim in the abstract: Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. In this project, I begin by training the model on AWS using the Yelp Sentiment (binary classification) data to establish a workflow. Once the architecture is in place and proven to achieve timely results, we can expand into the other datasets. Each are manually uploaded to S3 buckets to make them accessible to our Sagemaker instance, rather than using the script provided by fastText (although we do use their text normalization function). Output from AWS Sagemaker: Jupyter notebook run 1 with adam as optimizer, 10 Epochs - Highest Test Accuracy: 0.9401578947368421 (Epoch 8) - Final Test Accuracy: 0.939921052631579, Test Loss: 0.17803387705344548 (Epoch 10) Jupyter notebook run 2 with sgd as optimizer, 10 Epochs - Highest Test Accuracy: 0.9403157894736842 (Epoch 8) - Final Test Accuracy: 0.9400526315789474, Test Loss: 0.17815197125596924 (Epoch 10) Jupyter notebook run 3 with sgd as optimizer, 25 Epochs - Highest Test Accuracy: 0.9403157894736842 (Epoch 8) - Final Test Accuracy: 0.9397105263157894, Test Loss: 0.17758843273002864 (Epoch 25) - Note, the accuracy above was reached at Epoch 18 and remained stable. Other Resources: Deploying custom models built with Gluon and Apache MXNet on Amazon SageMaker Use MXNet with the SageMaker Python SDK SageMaker MXNet Inference Toolkit Another possible compute resource are the AWS Deep Learning containers. Amazon claims that: AWS DL Containers include AWSoptimizations and improvements to the latest versions of popular frameworks, like TensorFlow, PyTorch, and Apache MXNet, and libraries to deliver the highest performance for training and inference in the cloud. For example, AWS TensorFlow optimizations allow models to train up to twice as fast through significantly improved GPU scaling."
https://github.com/tmm-ai/CourseProject	"CS 410: Final Project Progress Report: Sarcasm Detection in Twitter Posts Date: November 29, 2020 Team: Tardy Slackers Progress Thus Far: We have a decent ""first draft"" python code that is working with the provided training set. For the input of the model, we are combining the response and context tweet. We then use a huggingface tokenizer to get the tokens, masks, and special token objects. We started working with the RoBERTa model that was developed by Google. Since we were working with the free version of Google Colab, we froze all the parameters to use as embeddings and then ran it with another LSTM layer, which means we are only changing LSTM weights at this point, to fit the model into memory. The results of this locally got a .8 F1 but on the official test set, we only got around .65 F1 which is about 0.08 below the baseline score. Remaining Tasks Upgrade to a paid plan of Google Colab GPU so we get more processing capability Run our program with the full BERT/ROBERTA model, and possibly other models. Experiment with different values for epochs, multiple GPUs, various learning rates, stacking models together, and using external data to see what delivers better/best performance to reach the baseline threshold. Current Challenges The familiarity and comfort with Google Colab, BERT/ROBERTA model, and NLP programming tasks such as this project vary significantly across team members. Those will less familiarity will be focused on getting more fully up to speed in the coming weeks and adjusting our model to achieve SOTA. CS410 Final Project: Text Classification / Twitter Sarcasm Team: The Tardy Slackers Wei Dai:  weidai6@illinois.edu Michael Huang:  mh54@illinois.edu Tom McNulty:  tmcnu3@illinois.edu Demo video / code walkthrough :   https://youtu.be/OUu71EapO5U An overview of the function of the code (i.e., what it does and what it can be used for). The function of the code provided is text classification. Specifically, the model is meant for classifying whether a response tweet given its context is sarcastic or not. While this code can be used for classifying any type of text into any number of classifications with some minor adjustments, the code has been specifically designed to fit the text in the provided testing and training files. The program takes in the provided 3-part Twitter interactions made up of 1 tweet that is a potentially sarcastic response and two tweets that occurred just before the response. These two tweets are collectively the ""context"". We use the RoBERTa pretraining language model. RoBERTa builds on the original BERT program which stands for Bidirectional Encoder Representations from Transformers, or BERT, is a revolutionary self-supervised pretrained architecture that has learned to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters in BERT, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT. After reading in and preparing the data, we tokenize the data, train the RoBERTa model, do a prediction on the validation set, prepare the test input, perform a prediction on the test set and then prepare the output, which is a text document simply listing out where each line in the test file is sarcasm or not. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Code: All code is labeled with comments for easy understanding. We extensively use the HuggingFace library for tokenization and model loading. We use Tensorflow to train the model. Data Preparation: 1.Have train.jsonl and test.jsonl in folder called data in present working directory 2.We used an 80-20 train-val split 3.We simply combined the context together and appended it to the response tweet and used that as a string, using only the first 128 tokens in our model as max input sequence length Training: 1.Tokenize data with huggingface tokenizer to get encodings, then put in a tensorflow dataset for training 2.Start training from pretrained roberta-large and finetune with our dataset for 3 epochs with Adam optimizer with learning rate of 2e-5 with batch size of 16. Validation and Test results: 1.Predict on validation and get f1 score, then predict on test set and write to answer.txt file and upload to leaderboard 2.Model achieves .84 F1 score on validation set(internally) and .78396226 F1 on test set / leaderboard(under tmmai) Experimentation: For the parameters, we adjusted the max length for tokenization, training encodings and validation encodings. We tried lengths of 64, 80, 128, 150, 256. We also experimented with adjusted the learning rates from 2e-5 up and down just slightly, such as 1e-5 and 3e-5 but these met with poor results. We also tried Bert-large, but this gave worse results than roberta-large. Maxlength of 128 for tokenization, training and validation with a learning rate of 2e-5 delivered the best results. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run the software, whichever is applicable. Two ways to run code: as python scripts or on google colab as notebook For running python scripts: 1.Clone the repo 2.Cd into repo and run pip install --upgrade -r requirements.txt, make sure you are using python3 3.Run python cs410_FINAL.py for training and testing. If you want to simply test on validation data and test data, then run python cs410_FINAL_EVAL.py 4.Both codes will evaluate on validation set and print f1 score 5.It will also write a data/answer.txt file with predictions on tests 6.If you use the training script, it will also save the model in huggingface hf format as data/roberta_model. In this folder, it will contain the tensorflow .h5 model 7.If using validation script, download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2 . Place these files in a folder called roberta_model within the data subdirectory i.e. data/roberta_model For running the colab notebook: 1.Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2 . 2.Get our train.jsonl and test.jsonl files and place them in your google drive in subdirectory called data i.e. drive/MyDrive/data/train.jsonl 3.Run through our notebook, https://colab.research.google.com/drive/1zcMMw8xe6vh9rMPlBB_i-HZGrxsk5UJj#scrollTo=Qbr6a4LYcl_w , but skip training step. Instead, load our model into your drive and run on validation and test set. Then, submit answer.txt from last cell to livedatalab. Link to the demonstration video / code walkthrough:  https://youtu.be/OUu71EapO5U Brief description of the contribution of each team member in case of a multi-person team. Wei Dai: Researched ideas, attempted to develop code in parallel to the team. Studied theories under BERT and RoBERTa. Actively discuss questions with other team members. Learned tensorflow, transformers and other libraries. Tried to discover parameters following Michael and Tom's draft. Michael Huang: Researched ideas/models and determined RoBERTa was likely our best model to use. Developed the best working code of the team, trained the RoBERTa model to beat baseline, and provided good resources for the team to learn more about implementing BERT and RoBERTa. Also created python script to run model and contributed heavily to documentation showing how to run software. Tom McNulty: Team captain, set up meetings, drafted the first drafts of all deliverables and requested team members to edit. Researched initial ideas and models, tried to develop a decent draft of text classification code in parallel with other team members but Michael developed the superior draft. Extensively experimented with various hyper parameter and Colab settings to get to optimal results. Contributed heavily to final documentation. Citations / Resources RoBERTa model:  https://arxiv.org/abs/1907.11692 BERT model:  https://www.aclweb.org/anthology/N19-1423/ Huggingface blog / libraries:  https://huggingface.co/transformers/model_doc/roberta.html Keras:  https://keras.io/api/ ** A lot of code utilizes huggingface and keras api's CS410 Final Project Proposal 10/25/2020 Team name: The Tardy Slackers What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Tom McNulty - Captain NetID: tmcnu3 tmcnu3@illinois.edu Wei Dai NetID: weidai6 weidai6@illinois.edu Michael Huang NetID: mh54 mh54@illinois.edu Which competition do you plan to join? The text classification competition - identifying sarcasm on Twitter Are you prepared to learn state-of-the-art neural network classifiers?: Yes - of course! Name some neural classifiers and deep learning frameworks that you may have heard of. We have done some research to identify good classifiers, but have not chosen exactly which one(s) we will actually use yet. Those we have heard of are Transformer-based models such as BERT, ALBERT, RoBERTa, XLNet, UniLM, Multi-Task DNN LSTMs, GRU, RNN Word Embeddings i.e. word2vec, glove, ELMO CNNs Describe any relevant prior experience with such methods Michael has experience with using these architectures. In undergrad, he participated in research projects that utilized LSTMs for NER tasks and also CNNs for some computer vision research. Also at work, he utilizes many transformer-based models to solve text-classification tasks as well as language generation tasks. (Side note: Although he does have this experience, he still finds this project appealing as it allows him to focus on improving other areas he's been meaning to work on such as writing good software, scripts, and documentation that makes it easy to install, use, and maintain) Wei has experience with data collection. At work, she utilizes many tools such as Json, goose-extractor to web scrape and crawl to build datasets for further use. Which programming language do you plan to use? - Python, most likely with TensorFlow for modeling Additional If we are able to achieve state-of-the-art performance in a reasonable amount of time, we would like to expand the capabilities of our program to do one or more of the following: Identify a rate of sarcasm or create a sarcasm score on the Twitter accounts of some well-known people such as comedians, politicians, flamboyant business people, etc. The score would be based on the person's entire tweeting history - or as much as we can get a hold of. Develop a capability to determine metaphors / idioms based on comparing figurative and literal interpretations of tweets. These concepts were presented at a recent UIUC presentation by Suma Bhat who is an Assistant Professor in ECE at Illinois https://mediaspace.illinois.edu/media/t/1_0db6ad18 Deploy our model with a REST API, so that it is easily interactable and usable by others, and it would also make for a cool demo Text Classification Competition: Twitter Sarcasm Detection Evaluation Please look at ""CS410 Final Project: Text Classification / Twitter Sarcasm.pdf"" for all documentation deliverables and if you want details about our code and training. This README.md is a brief summary on how to evaluate our model results. This assumes you are running our model and not training your own model to validate our results, since training your own model may give different results due to changes in initialization. More details about how to evaluate model results if anything is not clear may be present in pdf mentioned above. We provided our answer.txt in this repo. If you add this to livedatalab, you can confirm our rank on the leaderboard (under username tmmai). If you want to evaluate our results by having the model predict on the test set, then there are two options you can take: run our notebook on google colab or clone this repo and run our python scripts. Run on Google Colab Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2. Get our train.jsonl and test.jsonl files and place them in your google drive in subdirectory called data i.e. drive/MyDrive/data/train.jsonl Run through our notebook, https://colab.research.google.com/drive/1zcMMw8xe6vh9rMPlBB_i-HZGrxsk5UJj#scrollTo=Qbr6a4LYcl_w, but skip training step. Instead, load our model into your drive and run on validation and test set. Then, submit answer.txt from last cell to livedatalab. Demo video / code walkthrough of Colab verion: https://youtu.be/OUu71EapO5U Run with python scripts Clone this repo and cd into it Run pip install --upgrade -r requirements.txt (make sure you are using python3) Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2. Place these files in a folder called roberta_model within the data subdirectory i.e. data/roberta_model Run python cs410_FINAL_EVAL.py. This code will evaluate on validation set and print f1 score. It will also write in the data folder a answer.txt file with predictions on tests"
https://github.com/toskaushik/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tpjwm/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/varunhari2020/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/vickyli99/CourseProject	Progress Report We have implemented the 5.1 mentioned in the paper, which includes importing data set downloaded from the website, converting word into lower cases, removing punctuations, stop words, and stemming each word to its root. Our remaining task is to implement the rest of the functions from the paper, including 5.2 - 5.4, and get the result from our program by the end of this project. The formulas mentioned in the paper are a bit confusing to us, and we will make sure we can understand the math behind it so that we can finish this project. CourseProject Course Project for CS 410: Reproducing a paper, Latent Aspect Rating Analysis without Aspect Keyword Supervision. Paper link: https://www.cs.virginia.edu/~hw5x/paper/p618.pdf Stage 1 By Nov 29 Reproduce of Step 5.1 file: test.py First we remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects). Then we convert all the words into lower cases and remove punctuations and stop words. In vocab.txt we write vocabulary appearance based on reviews. If a word appears several times in the same review, it would only be counted as once. We then filtered out words that have less than ten occurrences. Step 5.2 In Progress Documentation 1.Overview This project consists of tasks of preprocessing data and implementing LARA functions. We get the data from http://timan.cs.uiuc.edu/ downloads.html and we focused on TripAdvisor data for this project. 2.Programming Language and Packages Python 3.X Packages: numpy, scipy, math, re, random, nltk 3.Implementation Clean.py This is the python program for preprocessing the data, we did the following for this part: 1) remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects); 2) convert all the words into lower cases; and 3) removing punctuations, stop words, and the terms occurring in less than 10 reviews in the collection. Lara.py This is the main program we implemented all the functions for building this LARA model. In this program, we implemented function such as update_mu, update_beta, E_step, M_step etc. Load.py This is the python program we have to load our data and build our vocabulary. 4.Project Members Ziyuan Wei (ziyuan3@illinois.edu) Xinyi He (xinyihe4@illinois.edu) Weijiang Li (wl13@illinois.edu) Dingsen Shi (dingsen2@illinois.edu) Qunyu Shen (qunyus2@illinois.edu) We decided to collaborate with another team, led by Xinyi He, half way through the project since we met some challenges when understanding the methods used in the paper. Then we splitted our tasks between two groups, our group (led by Weijiang Li, collaborating with Ziyuan Wei) focuses on the implementation of preprocessing and EM steps in the building up the model, and the other group contributed to the rest of the functions such as negative likelihood, and another main part of this project is the implementation of bootstrap. Team members from both team worked hard to try to get the code done based on the method description from the paper. When implementing EM steps, we separate the procedure into two functions, e-step() and m-step(), before adopting a new function runEM() to combine and output the previous data. 5. Video Link Here is the link to the demo video on mediaspace: https://mediaspace.illinois.edu/media/t/1_fo2gtfej The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/violetta-ta/CourseProject	"Final project - Proposal Team information: Team Name: The West Coasters Team Members Y= Marina Polupanova - marinap2 [Captain] Y= Tirthankar Bhakta - tbhakta2 Y= Savita Manghnani - savitam2 Selected competition: Option4 - Text Classification competition State and goals of the project: Our team has almost no experience with any of the neural networks and machine learning frameworks. For this program we intend to explore on following techniques and Machine Learning frameworks: Y= Random Forest Y= Logistic Regression Y= Support Vector Machine Y= Recurrent Neural Network Machine Learning Frameworks Y= Tensorflow Tensoflow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that let researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. The system is general enough to be applicable in a wide variety of other domains, as well. We will pick up one ML model in the TensorFlow library for our classification task. Y= Keras The Keras neural networks library supports both convolutional and recurrent networks that are capable of running on either TensorFlow or Theano. Keras deep learning framework was built to provide a simplistic interface for quick prototyping by constructing active neural networks that can work with TensorFlow. In a nutshell, Keras is lightweight, easy-to-use, and has a minimalist approach. These are the very reasons as to why Keras is a part of TensorFlow's core API. The primary usage of Keras is in classification, text generation, and summarization, tagging, translation along with speech recognition, and others. Y= Scikit-learn Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. It is licensed under a permissive simplified BSD license and is distributed under many Linux distributions. We are planning to use the Scikit's robust set of algorithm for: SS Regression: Fitting logistic regression models. For Logistic regression we will as well try GLMnet library, and will submit the results of the library which will look more competitive. SS Decision Trees: Tree induction and pruning for both classification and regression tasks SS SVMs: for learning decision boundaries Programming language: Y= Python CS 410: Project Progress Report The West Coasters | 28-Nov-2020 Text Classification Competition Team: The West Coasters 2020 Project Progress Report CS410: Text Information Systems Final Project: Text Classification Competition D Sarcasm detection Date: Saturday, Nov. 29, 2020 Team members: * Marina Polupanova ( University of Illinois at Urbana-Champaign) - marinap2@illinois.edu * Savita Manghnani ( University of Illinois at Urbana-Champaign) - savitam2@illinois.edu * Tirthankar Bhakta ( University of Illinois at Urbana-Champaign) - tbhakta2@illinois.edu Approaches considered for 'Sarcasm Detection': We have considered the following ideas to implement text classification in order to detect Sarcasm. 1. Text classification and sarcasm detection with CNN: We want to try a model, which can be made with parallel convolutional neural networks, where different kernel sizes used in parallel convolutional layers. This gives a multichannel input of text, that in fact uses different n-gram sizes. The network type was chosen, as over all the tutorials, it gave one of the best result on the text data, and also, logically, it should grab maximum information having in mind using different n-ngrams. The neural network will be built using TensorFlow and Keras libraries. 2. Text classification and sarcasm detection with BERT: The idea here is to build a classifier with BERT. BERT or Bidirectional Encoder Representations from Transformers is considered as a choice to do text classification as it can do compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after. We plan to use Tensorflow and Keras libraries to create the basic machine learning models and then pre-train the dataset with BERT, fine-tune the model and use it on the test data. 3. Text classification and sarcasm detection with DNN: Tensorflow library contains multiple estimators which can be used without building the complex models. Here I tried one such pre-made estimator, DNNClassifier, available in tensorflow python library for the classification task. DNNClassifier is an Tensorflow implementation of Deep Neural Network model. It is capable of accepting text in its raw format than required preprocessing to convert in numbers. Paired with Adagard, a gradient based optimization, DNNClassifier provides high recall. 4. Text classification and sarcasm detection with Bidirectional LSTM Long Short Term Memory networks - usually just called ""LSTMs"" - are a special kind of RNN, capable of learning long-term dependencies. The classification task requires the model to learn from various language usages, tones in the sentences, etc. The amount of learning for this supervised task requires, neural network needs to memorize the decisions from short term to long term. LSTM, as the name suggests fits the requirements very well. As there is no pre-estimator or readily available model available for LSTM in Tensorflow, requires creation of neural network model using Tensorflow library. Implementation & Current Status: 1. Text classification and sarcasm detection with CNN: The implementation steps for the model include following: a) Import required libraries - Tensorflow, Keras, Numpy, Pandas, Json, re, string b) Load the 'train.jsonl' and 'test.jsonl' file and read the file c) Create the train and validation labels and feature arrays from 'train.jsonl', and test label and feature arrays from 'test.jsonl'. d) Convert all the resulting arrays to tensors e) Pre-process all the feature train, test, and validation tensors to get a numeric input to Embedding layer and CNN. f) Create a model with Embedding and CNN layers g) Use model to train the data h) To evaluate the model i) To test the model on test data, and check the results vs Leaderboard. Status: Currently, the steps up to ""e"" are passed, now we are in the process of model creation and training. Challenges: There was a challenge to load the data from jsonl files to tensors, and it was multiple time failing as it was the first our experience with Pandas and Tensorflow data structures. After some efforts, the expected input was received. Current challenge is to create the Embedding/CNN of a structure, which will count towards the highest score. Possible issue can be that we have chosen to merge ""response"" and ""context"" in one variable, but so far we haven't yet understood how to make it a multichannel tensor. 2. Text classification and sarcasm detection with BERT: The idea is to use Tensorflow and Keras to create a ML model and use BERT pre-training models to perform pre-training on the data, fine tune it, optimize the output and create the final model. Here are the implementation steps taken into consideration: j) Import required libraries - Tensorflow, Keras, Numpy, Pandas, Jsonlines, Tensorflow-Hub k) Load the 'train.jsonl' file and read the file l) Create the train dataset from 'train.jsonl'. m) Split the dataset into train and validation datasets n) Load models from Tensorflow Hub o) Choose BERT models and determine the best fit to fine-tune p) Use BERT models to Pre-train the data q) Build own model by combining BERT with a classifier r) Train a model, including the preprocessing module, BERT encoder, data, and classifier s) Use an Optimizer like Adaptive Moments to fine-tune the model t) Run the test data and analyze data. Status: Currently, we are in the process of creating the dataset to train the model. The BERT models for pre-training and training are selected and also the model is built. Challenges: Using Tensorflow-Hub was giving an error - ""ImportError: cannot import name 'feature_column_v2' from 'tensorflow-hub"". Resolved the error by setting up a new environment and installing Tensorflow ver. 1.14. The current challenge is to create the expected dataset that BERT encoder and classifier can understand. 3. Text classification and sarcasm detection with DNN Current State: DNNClassifier is a pre-made estimator available in tensorflow library. The classifier is capable of supporting multiple classes for categorization. Implementation steps: a) Import required libraries, as it's a pre-made estimator not many libraries are required for making it work. Libraries it requires are tensorflow, numpy, pandas, sklearn, tensorflow_hub. b) Load the training dataset and split it into training and validation datasets as Pandas dataframes. c) Use TFHub sentence encoder to encode the datasets from Pandas dataframes. d) Created embeddings from above step are fed to DNNClassifier for training and evaluation. e) Use the trained model, categorize the test data set and generate answer.txt file. The latest submission with DNNClassifier, LiveLab reports F1 measure of 0.7073684210526316. Challenges: One of the challenge I faced with the approach was to embed text data to the classifier. Next steps: Train the model with ""Attention"" to improve on the performance. 4. Text classification and sarcasm detection with Bidirectional LSTM Current State: Unlike DNNClassifier where one does not have to build a model, LSTM requires creation of a neural network model. To create the model, we use tensorflow keras APIs. The generic APIs accept the data set in numeric format, thus preprocessing of input will not just involve cleaning of data but also converting data from text to numeric representation. Following steps were performed to achieve the task: a) Import required libraries like tensorflow, numpy, sklearn, TextVectorization, keras layers and losses. b) Load the training data set and split the data set into training and validation data sets. c) Preprocess data set to remove unwanted characters and words; convert the data set into numeric format as expected by keras APIs. d) Build neural network model with different layers including bidirectional LSTM. e) Train and evaluate the model. f) When satisfied with evaluation, use the model to predict for test data set. The latest submission with this model resulted in F1 measure of 0.6917372881355932. Challenges: Once the DNNClassifier implementation was done, this implementation did not take much time. The only challenge I faced was to vectorize the text in format understood by keras APIs. Next Steps: Train the model with ""Attention"" to improve on the performance. CourseProject The project for Text Classification Competition from the team ""The West Coasters"": Tirthankar Bhakta tbhakta2@illinois.edu Marina Polupanova marinap2@illinois.edu Savita Manghnani savitam2@illinois.edu We planed to classify tweets to predict the ones which can be qualified as sarcasm using following methods: * Random Forest * Logistic Regression * Support Vector Machine * Recurrent Neural Network Instead, after feedback on the Project Proposal submission, we used following methods: * LSTM * DNN Classifier with DAN sentence encoder (hit the LiveDataLab threshold). Please note, that in the demo video we have told that we use Transformer sentence encoder, but in fact the one which hit the performance baseline was DAN sentence encoder, and it is posted in the dnn.py file. * Multi-channel CNN model * BERT Project Tutorial: https://mediaspace.illinois.edu/media/1_uscvryhp Project Documentation file: Documentation/ProjectDocumentation.pdf Installation instructions: Documentation/Installation_and_execution_instructions.txt The account on the LiveDataLab for submitting the ""answers.txt"" was ""marina_polupanova""."
https://github.com/vkreiden/CourseProject	"Progress Report: Applying ML to log analysis for anomalies detection Progress made so far: * Deepening knowledge in the area. The technology review in the related area as well as some video-courses on NLP and Leveraging NLP and Word Embeddings in ML. * Dependencies for the project. Python libraries: spaCy (text parsing), pandas, numpy (data manipulation), gensim (word2vec embedding), sklearn (ML models, validations, ...) * Installing and configuring dev env. * Breaking down the tasks and creating the backlog. Backlog: * (in background) Data acquisition. Work to obtain real log files * Skeleton. Build a skeleton of the steps w/stubs (+ some tests) * 1st implementation. Implement word2vec embedding w/small synthetic log files; kFold of the first ML algorithm (TBD); implement perf. reporting (e.g. accuracy, runtime) * Refactor to OOP. Should enable an easy extension at least with a different ML algorithm; (optionally embeddings as well) * Extend the implementation with at least one more model. Ideally should be able to run a comparison of models (details TBD) * Run the tool on the real log files. Run and compare models performance * Documentation. [Optional] * Dockerization. For more convenient tool usage * Non-functional requirements. Look at the performance optimization * Hyperparameters tuning. Challenges so far: * Real data acquisition. If no real data achieved the fallback would be to use a synthetically generated dataset * Env tech issues. Some issue w/scapy corpus load - troubleshooting in progress... Tool: Applying ML to log analysis for anomalies detection Team (C) valeryk2 (Valery Kreidenko) What is the function of the tool The tool is a thematic continuation of the technology review. The tool will allow to train and evaluate performance of different machine learning models on the datasets of the labeled log files. Who will benefit from such a tool SREs, data scientists and researchers who would like to experiment and test different ML models for anomalies detection from the log files. Does this kind of tools already exist Some work has been already done in this area to support the research brought in the technology review though not sure what exactly since the code isnOt publicly available. What existing resources can be used WeOll use the approach which was researched and suggested by the authors of https://hal.laas.fr/hal-01576291/document. WeOll try to acquire some real existing datasets from the real system. What techniques/ algorithms will be used The current plan is: - to use word2vec or other NLP technique(s) to map text data into the ML models consumable data - weOll implement at least 2 supervised ML methods - per performance evaluation o for effectiveness weOll use F-measure and maybe others o for efficiency weOll compute model training and classification times for models comparison - for validation we plan to implement 10-fold or other method - finally, weOll try to apply OOP design patterns to make this tool extensible for easy plugging of more ML models, etc. - implementation language: Python + its ML libraries How the usefulness of the tool will be demonstrated WeOll run at least 2 models of the tool on the same data and weOll evaluate and compare the relative performance in terms of the effectiveness and efficiency. We hope to demo this tool on the real dataset, otherwise weOll synthesize some. Timeline This is estimated as a ~3 weeks project. CourseProject This is an #UIUC #MCS_DS Text Information Systems course final project. The developed tool is based on the technology review. Table of contents Functionality Implementation details Usage Usage presentation Functionality This tool provides a functionality of assessing performance as well as performing comparison of supervised learning classifiers when applied to the log files or chunks of log files for anomalies detection. It is mostly based on the approach described in this paper. At high level, the tool receives 2 lists of text files as input - one dataset corresponds to the system in error state (e.g. under stress, performance issues, security incidents, etc.) and the second set corresponds to a benign work of the system. Each file represents one data item (document) in one of the two lists thus the dataset items are prelabeled with either ""fail"" or ""normal"" labels. After the data is read and preprocessed, an NLP method is applied to transform each data item (log file) into a vector space. The received matrix along with labels is fed into each of of the supervised machine learning models to evaluate the performance. The output of the tool run is a scatter chart displaying each tested classifier over 2 dimensions - classification accuracy vs. runtime. The tool is implemented in python (see Implementation details) and runs using the configuration provided in the ini format config file (see Usage). Implementation details General The tool is implemented in python and uses a number of standard python modules for data consumption and processing as well as specialized modules for applying NLP, ML methods and plotting the results as detailed below. Reading the configuration The standard configparser is used to read the following configuration items form the configuration file file: locations of the two sets of files, preprocessing directives on how to optionally drop the ""standard"" prefix, a list of classifiers to assess along with the K-fold test set size parameter. There's also a file format parameter which used when plotted into a file. Datasets and preprocessing The expected datasets are two sets of log files (one corresponding to a normal state of the system and the other one during abnormal work). Each file contains a number of log messages. There are two preprocessing steps which are performed on the input data using standard string manipulation functions: 1 drop a standard prefix of a log message using delimeters 2 remove non-alphanumerical characters NLP To prepare the data for ML algorithms we start with applying the word2vec word embedding to vectorize the words. We train the word2vec model on the vocabulary generated from all the words appearing in the log messages. We use gensim library to build the word2vec model over default number of dimensions (100) of the vector space. To calculate the positions of the files in the vector space we use a centroid approach twice: we first calculate an average postion vector per log message over all the mesage words vectors. Then we find a position of each file in the vector space by calculating the average of all the file's log messages vectors. ML estimators assessment and comparison We use scikit-learn library to build and evaluate ML models. Specifically, we use K-fold validation. When enumerating a list of the fully-qualified class names of the estimators the reflection is used to instantiate an instance of each. We capture accuracy and a model evaluation time measurement for plotting the results at the next step. Results presentation We use a matplotlib module to plot a scatter chart presenting the results of the run. X axe shows the classification accuracy vs. model evaluation runtime on the Y axe. Usage describes two ways of using the tools. When the tool is run on the host OS the chart will be displayed onto the display. This may not be easily achieved if the tool is run in the container. In this case the tool will save the chart into figures folder in the format specified by user in the config. Testing datasets We provide two sample datasets ""perfect split"" and ""mix"". These datasets are mostly synthetic. They were generated from the real httpd error log files in the following way: the notice level messages are separated into the ""notice"" files while error level messages are separated into the ""error"" files. ""Perfect split"" dataset includes strictly separated notice and error files while the ""mix"" contains a ~70%-30% mixes of two kinds. This allows to see the implemented approach works from the perspective of expected accuracy: with the perfect split all the classifiers are 100% accurate while when we use a mixed dataset the accuracy drops below 100%. Perfect split dataset Mix dataset Usage We prepared and tested two running configurations below. Clone this project and cd into it. Docker (recommended) Build docker container: docker build -t ad_app . (On Linux/ Mac) Run: docker run -d --mount type=bind,src=""$(pwd)""/config,target=/config \ --mount type=bind,src=""$(pwd)""/datasets,target=/datasets \ --mount type=bind,src=""$(pwd)""/figures,target=/figures ad_app (On Windows) substitute ""$(pwd)"" in the above command with the current directory path Using pip package manager in virtual environment python -m ensurepip --default-pip python -m venv test_ad_app (On Linux/ Mac): source test_ad_app/bin/activate (On Windows): test_ad_app\Scripts\activate pip install -r requirements.txt Using conda package manager Configure: conda update conda conda install -c anaconda gensim scikit-learn conda install -c conda-forge matplotlib conda update -y smart_open Run: python anomaly_detector.py Usage presentation recording"
https://github.com/vs27-illinois/CourseProject	"RecipeFinder Software code submission with documentation by Team ""TasteBuddies"" Vijayaragavan Selvaraj (VS27) - Team Leader Sathyanarayanan Gokarnesan (SG53) Karthika Gopalakrishnan (KG24) 1 |  Page 1. Application - Live Demo link Please see the live demo of the ""RecipeFinder"" application running at the following location. http://54.81.52.188/ 2. Overview ""RecipeFinder"" is a web-based tool built using novel information retrieval techniques to search popular recipes. It searches recipes based on the ingredients of the recipes. When the user searches with an ingredient name, it lists the top 20 recipes that contain the given ingredient. For each of the recipes in the search results, users can also see recipe details including the cooking directions, other ingredients of the recipe, nutritional values, timings, and ratings of the recipe. Users are also recommended with 4 similar recipes based on the nutritional value. Content based similarity approach has been used for this project. 3. Architecture and Source Code Following is the high-level architecture of ""RecipeFinder"".  Front end Angular Web app 2 |  Page Source code for the ""Recipe Finder"" application is present in the following github location: https://github.com/vs27-illinois/CourseProject.git Source code for our project can be broadly categorized into three sections. BackEnd FrontEnd Infrastructure Backend: Following are the main files in the backend system. 1. Indexer.py This python file is responsible for taking the ""Recipe Finder"" dataset, parsing the csv file for different fields, creating indexes on different fields, and writing the same in an index document. We used Apache PyLucene to index the dataset. The indexed dataset contains 49,698 records in total. We used the dataset from Kaggle at the following location for our project: https://www.kaggle.com/elisaxxygao/foodrecsysv1?select=raw-data_recipe.csv Following are the columns in the index. 3 |  Page S.No Column Index Type 1 id IndexOptions.DOCS 2 name IndexOptions.NONE 3 image IndexOptions.NONE 4 avg_rating IndexOptions.NONE 5 total_reviews IndexOptions.NONE 6 ingredients IndexOptions.DOCS_AND_FREQS_AND_POSITIONS 7 time_taken IndexOptions.NONE 8 nutrition IndexOptions.NONE 9 calories IndexOptions.NONE We used MMapDirectory to load the index files and indexed the ingredients and recipe id field by using EnglishAnalyzer since the dataset is in English. We stored the rest of the fields in the indexed document. Following are some of the main code snippets of indexer.py. 4 |  Page 10 carbohydrates IndexOptions.NONE 11 protein IndexOptions.NONE 12 fat IndexOptions.NONE 2. Retriever.py Following are the functionalities of retriever.py. #It is responsible for taking the ""ingredient"" input from the user, searching the index for 20 popular recipes based on the ingredient, converting the results (list of recipes) into json format to be rendered in the UI. #It is responsible for taking the recipe id from the user to provide a detailed view of the recipe. #It also returns the list of top 4 recipes based on similar nutritional value for the recipe which the user wants to see the details. Following are the API calls involved. 5 |  Page S.No End Point Method Output 1 http://<ipaddress>/recipe/search/{ingredient} Ex: http://54.81.52.188/recipe/search/chocolate GET [ { ""avg_rating"": 4.34615373611, ""calories"": 274.0809, ""carbohydrates"": 45.78727, ""fat"": 10.5331, ""id"": 32482, ""image"": ""https://images.media-allrecipes.com/userphotos/250x250/710487.jpg"", ""ingredients"": [ ""<strong>chocolate</strong> chips"", ""powdered <strong>chocolate</strong> drink mix"", ""<strong>chocolate</strong> syrup"", ""scoops <strong>chocolate</strong> ice cream"", ""milk"", ""ice"" ], ""name"": ""Chocolate Surprise Milkshake"", ""protein"": 3.052425, ""total_reviews"": 17 },... ] 2 http://<ipaddress> /recipe/details/{recipeId} Ex: http://54.81.52.188/recipe/details/220725 GET { ""avg_rating"": 4.26016616821, ""calories"": 123.5964, ""carbohydrates"": 19.74722, ""directions"": [ ""Prepare the cake mix according to package directions using any of the recommended pan sizes. When cake is done, crumble while warm into a large bowl, and stir in the frosting until well blended."", ""Melt chocolate coating in a glass bowl in the microwave, or in a metal bowl over a pan of simmering water, stirring occasionally until smooth."", ""Use a melon baller or small scoop to form balls of the chocolate cake mixture. Dip the balls in 6 |  Page chocolate using a toothpick or fork to hold them. Place on waxed paper to set."" ], ""fat"": 5.188236, ""id"": 67656, ""image"": ""http://images.media-allrecipes.com/userphotos/720x405/599097.jpg"", ""ingredients"": [ ""chocolate cake mix"", ""prepared chocolate frosting"", ""bar chocolate flavored confectioners coating"" ], ""name"": ""Cake Balls"", ""nutrition"": { ""calcium"": { ""amount"": 27.23583, ""displayValue"": ""27"", ""hasCompleteData"": true, ""name"": ""Calcium"", ""percentDailyValue"": ""3"", ""unit"": ""mg"" }, ""calories"": { ""amount"": 123.5964, ""displayValue"": ""124"", ""hasCompleteData"": true, ""name"": ""Calories"", ""percentDailyValue"": ""6"", ""unit"": ""kcal"" },.. }, ""protein"": 1.122792, ""time_taken"": [ ""Prep"", ""40 m"", ""Cook"", ""30 m"", ""Ready In"", ""3 h 10 m"" ], ""total_reviews"": 1867 } 3 http://<ipaddress>/recipe/recommend/{recipeId} Ex: http://54.81.52.188/recipe/recommend/220725 GET [ { ""avg_rating"": 3.5, ""calories"": 124.4117, ""carbohydrates"": 19.98162, ""fat"": 4.989575, ""id"": 15463, ""image"": ""http://images.media-allrecipes.com/userphotos/720x405/1115423.jpg"", ""ingredients"": [ ""canola oil"", ""honey"", ""packed brown sugar"", ""egg whites"", ""vanilla extract"", ""water"", ""wheat flour"", ""all-purpose flour"", ""baking powder"", ""salt"", ""ground cinnamon"", ""semisweet chocolate chips"" We maintained the IndexSearcher object as a global variable to boost the performance of the search. We used Lucene Highlighter to highlight the search terms in the search results in the website. We maintained a global variable of pandas dataframe with all the records that is the L2 normalized form of nutritional values that we used in the recommendation service. The recommendation is done based on the nutritional values of calories and macronutrients (i.e., protein, carbohydrates and fat) by calculating euclidean distance between the record of the given recipe id and the other records. Following are some of the main code snippets for retriever.py. 7 |  Page ], ""name"": ""No Cholesterol Chocolate Chip"", ""protein"": 1.515088, ""total_reviews"": 15 }, 8 |  Page Frontend: The frontend of the code to display the search results and recommended recipes are based on Angular 11. It makes http GET requests to the backend to retrieve the search results, recipe details and recommended recipes. The frontend is a single page application and the code is located in the ""src/app"" folder. We used the Angular Material module as a base to build the UI. Infrastructure: Following are the infrastructure related files. 1. DockerFile We used Docker to containerize our application and used the following image as the base to install PyLucene:  https://hub.docker.com/r/coady/pylucene . The Dockerfile in the project folder copies all the required backend and frontend files and deploys them in a standalone container. 2. startup.sh This shell script file is responsible for creating the docker image (recipefinder:1.0) and running the image as a docker container. 9 |  Page 4. Setup and Installation Instructions Following are the technologies used in the project. *Python 3.9.0 *Apache PyLucene 8.6.1 *Flask 1.1.2 *Angular 11.0 *Docker 2.5 Some of the python packages used are: *Numpy 1.19.4 *Pandas 1.1.5 *scikit-learn 0.23.2 *sklearn *scipy 1.5.4 Angular modules used: *angular-material *ng-bootstrap Since we used Docker, the project can be installed in either a local environment or on any cloud instances. Following are the steps: 1. Install and set up Docker. 2. Clone the project from the github location  https://github.com/vs27-illinois/CourseProject.git 3. Open the shell script and run the following command: sh startup.sh 4. If the application is running on a cloud environment, enable the http port (80) in the host machine, so that the Flask application running in the docker container can be exposed to the internet. Sample image below. 10 |  Page 5. Open your favorite browser and go to http://127.0.0.1/ (if the app is running in a local environment) or http://<ipaddress>/ (if the app is running in a cloud environment). 5. Snapshots Following are the snapshots of the ""Recipe Finder"" application. 11 |  Page 12 |  Page 6. Further Improvements Within the given timeframe, we implemented all the functionalities that we have initially proposed for this project. We even fairly optimized the response time of the APIs by improving the performance of the search and recommendation service of the application. As a future enhancement, the performance of the recommendation service can be improved (currently it takes ~20 seconds to provide results). Moreover, we attempted to modify the recommendation service to use other fields like ingredients and faced memory limitations in our EC2 instance (we used free tier). It can also be tried as a further development of this project. 7. References https://lucene.apache.org/core/8_6_1/ https://docs.scipy.org/doc/scipy/reference/spatial.distance.html https://www.kaggle.com/elisaxxygao/foodrecsysv1 https://github.com/coady/docker https://material.angular.io/ 13 |  Page 8. Contribution of Team Members Vijayaragavan Selvaraj (VS27) - Team Leader *Retriever (Search) *Docker *Angular Sathyanarayanan Gokarnesan (SG53) *Indexer *EC2 setup *Angular Karthika Gopalakrishnan (KG24) *Retriever (Recommendation service) *Documentation *Presentation 14 |  Page Project Progress Report: Recipe Finder Team Tastebuddies, Net IDs: vs27, kg24, sg53 1) Which tasks have been completed? Installed Pylucene using Docker in the local environment and did a test run in EC2. Development of python code completed for the following: Parsing and Cleansing of recipe dataset. Indexing and Storing of recipe dataset. Enhanced performance of indexing using Pandas. While implementing the indexer, we evaluated different indexing options and field types like multivalued fields given in the Lucene library. Retriever to search the recipes based on given 'Ingredient' query term. While implementing the retriever, we evaluated different query parser options and analyzers to search the data using a single term or a phrase. Development of API's using Flask. 2) Which tasks are pending? Development of web pages using Polymer is in progress. (vs27, expected to be completed by 12/02) Development of the Content Based Recommender system is in progress. (kg24, expected to be completed by 12/03) Deployment of Pylucene in EC2 using Docker. (sg53, expected to be completed by 12/04) Deployment of API's and web pages in a web server in EC2. (vs27, expected to be completed by 12/05) Documentation and presentation. (expected to be completed by 12/10) Planning to do the final submission by 12/10. 3) Are you facing any challenges? In our project proposal, we mentioned that we would be using Apache Solr to index and store the data. However, we later found that Apache PyLucene is sufficient for our use case and used it for indexing and storing the dataset. We have faced a lot of issues in installing Pylucene in the local environment as well as EC2. As a solution to those issues, we found an open source docker based Pylucene repository that helped us in automating the installation of Pylucene. We are not facing any challenges as of now. Project Proposal: Recipe Finder WhatarethenamesandNetIDsofallyourteammembers?Whoisthecaptain?Thecaptain will have more administrative duties than team members. -Team: Tastebuddies -Vijayaragavan Selvaraj (vs27) - Team Lead -Karthika Gopalakrishnan (kg24) -Sathyanarayanan Gokarnesan (sg53) W hatisyourfreetopic?Pleasegiveadetaileddescription.Whatisthetask?Whyisit importantorinteresting?Whatisyourplannedapproach?Whattools,systemsor datasetsareinvolved?Whatistheexpectedoutcome?Howareyougoingtoevaluate your work ? -Free Topic:   Recipe Finder -Description:Itisasearchtooltofindthemostpopularrecipesbasedonaningredient. Foreachrecipeinthesearchresult,theuserscanseethecookingdirections,nutritional value and a list of recommended recipes. -Task:WearegoingtousethenovelInformationRetrievaltechniquestobuilda web-basedsearchenginethatallowsuserssearchfortherecipesbasedontheir preferredingredient,andarecommendationservicethatsuggestsusersalistofsimilar recipes based on a combination of ingredients and nutritional value. -Motivation: Allrecipes.comisarecipecurationwebsiteandhascuratedoverthousands ofrecipesfromallovertheworld.OurultimategoalistodevelopaRecipeFinderto enhancetherecipesearchprocessbyretrievingtop20popularrecipesusingthe ingrediententeredbytheuserandrecommendsimilarrecipesforeachrecipeviewedby the user. -Approach:WearegoingtouseApacheSolrasthebackendlayerforthesearch.Wewill indexthedatainSolrandfinetunetheparametersforbettersearchexperience.Forthe recommendersystem,wearegoingtoadoptContent-basedFilteringmechanismand use the columns, nutritional value and ingredients, to suggest similar recipes to the user. -Dataset:   Allrecipes.com dataset from kaggle -Outcome:Awebsitewhereuserswouldbeabletoenteraningredientinasearchbarand thesystemwouldreturnthetop20popularrecipesandalongwithrecommendationsof similar recipes for each recipe in the top 20 list. -Evaluation:Userneedstoenteraningredient(eg.potato)inasearchbarandthesystem wouldlistthetop20popularrecipesfortheingredient.Iftheusersselectarecipefrom the20recipes,theycanseethecookingdirectionsandnutritionvaluefortherecipe.In additiontothat,theuserswouldalsoseealistofsimilarrecipesthathavesimilar ingredients and better nutritional value. Which programming language do you plan to use? -Python, Apache Solr, Scikits (learn, surprise), Pandas, Numpy, HTML, Javascript, CSS Project Proposal: Recipe Finder Pleasejustifythattheworkloadofyourtopicisatleast20*Nhours,Nbeingthetotalnumber ofstudentsinyourteam.Youmaylistthemaintaskstobecompleted,andtheestimatedtime cost for each task. Ourestimatefortheprojectwouldbe80hoursapproximatelyandbelowisthebreakupofthe workload for the project. -Oct 26 - Nov 9: Import Dataset in Apache Solr (~20 hours) -Setup Solr in EC2 -Data Cleansing & Ingestion -Fine Tuning and Performance Analysis -Nov 10 - Nov 22: Recipe Recommendation Model (~30 hours) -DesignContent-basedFilteringSystemtorecommendsimilarrecipestotheuser based on the ingredient and nutritional value -Performance Analysis and Fine Tuning of the Model -Nov 23 - Dec 4: Web Application (~25 hours) -Design Backend and Frontend for the Web application -Scenario 1: -Input: User enters Ingredient in search bar -Output: Top 20 recipes that has the ingredient -Scenario 2: -Input: User opens a recipe from the Top 20 list -Output:CookingDirections,NutritionalValueandRecommendationsof similar recipes -Dec 5 - Dec 8: Documentation & Presentation Preparation (~5 hours) Recipe Finder Cooking has become a survival skill for everyone now and every day, people search for recipes using their preferred ingredient. AllRecipes.com is a website which has curated thousands of recipes from all over the world in different cuisines and cooking methods. Our RecipeFinder application will be an enhancement to the existing system and helps their users to search for recipes using their preferred ingredient and get recommended recipes based on the ingredient and nutritional value."
https://github.com/wdchild/CourseProject	"Thursday, December 3, 2020Course Project: Final Report W. Daniel Child Notes on Setup To run this code on your own computer you will need to install Stanza, chromedriver, search engine parser, and BeautifulSoup. Installing chromedriver. You must also have the correct version of chromedriver according to your OS. I can only speak for my own system, but if using this code on a Mac with Catalina (MacOS 10.15.x), then it is critical that you have the appropriate version of chromedriver installed in the folder where the Python code is running. Also, because Catalina has strict developer recognition requirements, you may have to give the driver permission to run. Under System Preferences / Security, you must explicitly bless chromedriver. For details https://stackoverflow.com/questions/60362018/macos-catalinav-10-15-3-error-chromedriver-cannot-be-opened-because-the-de for details. The version of chromedriver I am uploading to github may not be the one you need for your own computer. Installing Stanza. Theoretically, installation is simple enough using pip. Simply run >>> pip install stanzaAlthough Stanza can be installed using Anaconda for Python 3.7 or earlier, conda installs will not work if using Python 3.8. (I inadvertently tried running stanza from Python 3.7 within conda after a pip install and promptly received missing resource errors. So if you use Anaconda regularly and want to use Stanza, you should install it as follows: >>> conda install -c stanfordnlp stanzaTaking this step resolves error messages involving missing resources. Search Engine Parser. It appears that Google has discontinued free options for using their search API. Another option has been developed for Python, however, and it seems to work perfectly well. So I have used that as my starting point. You can install the search engine parser as follows: Warren Daniel ChildProgress Report of 19Thursday, December 3, 2020>>> pip install search-engine-parserThis search-engine-parser supports different search engines, including Google, Yahoo, and (allegedly) Bing. However, during tests, Bing failed miserably, flagging searches as possibly illegal, so I chose not to use it. I did compare Yahoo and Google for the query `Zi ' (literally ""black characters"" but also the name of a flower, in Japanese) and found that they had very different results. Accordingly, I wrote code that made it possible to use either or both of those engines. Project Objective As delineated in the project proposal, the purpose of this project is to develop an application that can help Japanese translators search for contextual information on rare Japanese patent terms and expressions, and to show those terms and expressions in the context where they are used. When one searches for rare patent terms, search engines routinely return irrelevant pages, and one has to hunt for relevant information out of a large number of retrieved urls. From a translator's perspective, it would be extremely helpful to have concise term reports that contain extracts showing exactly where the expression is used. As a bonus, this project also includes automatically generated syntactic analyses of the extracts that are retrieved. Why Is Finding Terms Problematic In Japanese patents, one frequently encounters obscure terms that are not readily found in dictionaries. In addition, because Japanese does not partition words by spaces, parsers routinely make mistakes in determining word boundaries. In the case of rare terms, the tendency is to break a longer term into something shorter. As a result, search engines like Google frequently return text retrieval results that do not in fact contain the full term in question. For example, imagine you want to find an expression that contained the characters ABCDE. As it turns out, if BC is considered to be a word, and if DE is considered to be a word, then a typical browser search is liable to return urls that match BC, DE, or even BCDE, but not the full expression ABCDE. Such returned urls are often unhelpful. Because so many pages end up being off topic, the translator is frequently forced to browse a large number of irrelevant pages before finding one that contains the term of interest. The purpose of this project is to make it easy to locate pages that actually contain the full term in Warren Daniel ChildProgress Report of 29Thursday, December 3, 2020question (ABCDE) and to then use Stanza's NLP parser to analyze passages that actually contain the data. Application Structure There are three main pieces of code: analyzer.py scraper.py search.pyNow, search.py is basically a main function where one can adjust parameters before running a search and conducting an analysis. The other files  namely, scraper.py and analyzer.py, implement classes that respectively perform the scraping and analysis methods. Running the Application The analyzer and scraper files serve to implement Analyzer and Scraper classes that I created. Creating classes in this way made it possible for me to encapsulate nearly all of the functionality that is needed for this project. This makes running the code is extremely simple. To run the application, you simply need to set three parameters or variables within search.py: (1) the expression to search (e.g. target = Zi ) (2) whether you want to use a Google or Yahoo engine for the search (3) the maximum depth of the search The expression here should be Japanese. I have provided a number of expressions for testing purposes directly into the code. To run one program, you simply need to copy the correct variable names into the class constructor: scraper = Scraper(target = <target variable name here>, max_depth = <set depth value here>, engine = <OgoogleO, OyahooO, or ObothO (default)>)The search engine parser returns a page of results that one would see on a normal browser. Because I am using a headless browser, one page of results corresponds to approximately 8 - Warren Daniel ChildProgress Report of 39Thursday, December 3, 202010 urls. The max_depth variable indicates how many pages of urls you want to explore (a depth of 5 would correspond to approximately 50 urls). To avoid long runtimes during project exploration, you may want to keep this to 1. Operation Once these parameters have been set, the line scraper.run_search()will run code to search each page for the target expression. There is a lot that goes on behind the scenes. Scraper operations can be summarized as follows: until max depth is reached or while no matches are foundEprocess a set of pages byobtaining the html (OsoupO) for each pagesearching for the expression on each pagecollecting text passages where the expression is foundThe biggest issue here was cleaning up the ""soup"" produced by BeautifulSoup. Most sources recommend ""decompose"" but it turned out to not be that helpful. A more productive approach was to regular expressions to find if the expression was found, and to indicate which element contained the target expression, and then return that element's text. The results will indicate whether a given url has matches, as follows: Url 0 has matches for Chu Lai Xing  a Chu Lai Xing  Url 1 has no matches for Chu Lai Xing  Url 2 has matches for Chu Lai Xing  h3 2014.12.02 Chu Lai Xing toChu Lai Gao noWei i p Tu Mu Jian Zhu Yong Yu ni, [Chu Lai Xing ] to[Chu Lai Gao ] gaarimasuga, kono2tsuhatotemoyokuSi taYan Xie desuga, Yi Wei haYi narimasu.  Warren Daniel ChildProgress Report of 49Thursday, December 3, 2020Syntactic Analysis After the max depth has been reached and all urls have been explored, then operations are conducted by the analyzer, whose main function is to utilize Stanza functions. The analyzer obtains the data that was stored by the scraper under its ""results"" instance variable and applies Stanza parsing and analysis to that data. This analysis shows that the word in question is similar to another but has a different meaning. 2014.12.02 CD NUM Chu Lai  VV VERB Xing  NN NOUN to PS ADP Chu Lai Gao  NN NOUN no PN ADP Wei i NN NOUN Tu Mu  NN NOUN  SYM SYM Jian Zhu  NN NOUN Yong Yu  NN NOUN ni PS ADP ,  SYM PUNCT [ SYM PUNCT Chu Lai Xing  NN NOUN ]  SYM PUNCT to PQ ADP [ SYM PUNCT Chu Lai Gao  NN NOUN ]  SYM PUNCT ga PS ADP ari VV VERB masu AV AUX ga PC SCONJ ,  SYM PUNCT kono JR DET 2 CD NUM tsu XSC NOUN ha PK ADP totemo RB ADV yoku RB ADV Si  VV VERB Warren Daniel ChildProgress Report of 59Thursday, December 3, 2020ta AV AUX Yan Xie  NN NOUN desu AV AUX ga PC SCONJ ,  SYM PUNCT Yi Wei  NN NOUN ha PK ADP Yi nari VV VERB masu AV AUX .  SYM PUNCT Parts of speech are indicated in the analysis. Because the default printout is rather long, I have used a more tabular approach then the default approach used by Stanza At the end, the extracted text and Stanza analysis are written to file, and if all goes well, the program terminates. Project Evaluation Being able to get clean text containing the target expression is a huge win. The analysis from Stanza (shown above on this page) is helpful as well. The project has been successful in that it does what it is supposed to do. Avoiding Occasional Pitfalls * Chromedriver Errors Twenty hours is not enough time to develop a truly robust parsing function. Things sometimes go wrong when scraping. The page may no longer exist, or it may have an anti-scraping function embedded in it. Such issues seem to throw an error within chromedriver itself. Here is an example of what can happen: Traceback (most recent call last): File ""search.py"", line 29, in <module> scraper.run_search() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 106, in run_search self.process_page_set(self.current_page_set) Warren Daniel ChildProgress Report of 69Thursday, December 3, 2020 File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 99, in process_page_set self.get_soup(retrieved_urls[url_num]) File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 64, in get_soup self.driver.get(url) (ENTERING GOOGLE CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 333, in get self.execute(Command.GET, {'url': url}) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute self.error_handler.check_response(response) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response raise exception_class(message, screen, stacktrace) selenium.common.exceptions.WebDriverException: Message: unknown error: net::ERR_CONNECTION_CLOSED (Session info: headless chrome=87.0.4280.67) There is not much I can do about errors internal to chromedriver, so to handle the issue more gracefully, I wrapped the driver's get(url) function in a try-except block. If html retrieval fails, then the program will move on to the next url. This seems to have solved the problem, though many more hours of testing would be needed to make sure every conceivable problem could be anticipated. * Stanza Errors It turns out that Stanford's Stanza wrapper for CoreNLP is also buggy. Consider this error: Traceback (most recent call last): File ""search.py"", line 44, in <module> analyzer.analyze_data() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/analyzer.py"", line 24, in analyze_data doc = self.nlp(d) (ENTERING STANZA CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__ doc = self.process(doc) Warren Daniel ChildProgress Report of 79Thursday, December 3, 2020 File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process doc = self.processors[processor_name].process(doc) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/pos_processor.py"", line 30, in process sort_during_eval=True) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 48, in __init__ self.data = self.chunk_batches(data) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 150, in chunk_batches (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data]) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/common/data.py"", line 39, in sort_all return sorted_all[2:], sorted_all[1] IndexError: list index out of range This seems to have occurred because Stanza got confused when parsing a very long text passage. To circumvent such errors, I similarly employed a try-except block. * File Write Errors Because the data that is written to file for future analysis is based on Stanza, analogous errors can appear when writing to file. *** WRITING DATA TO FILE: Chu Lai Gao  analysis.txt *** Traceback (most recent call last): File ""search.py"", line 45, in <module> analyzer.write_analysis() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/analyzer.py"", line 41, in write_analysis doc = self.nlp(d) (ENTERING STANZA CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__ doc = self.process(doc) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process doc = self.processors[processor_name].process(doc) Warren Daniel ChildProgress Report of 89Thursday, December 3, 2020 File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/pos_processor.py"", line 30, in process sort_during_eval=True) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 48, in __init__ self.data = self.chunk_batches(data) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/data.py"", line 150, in chunk_batches (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data]) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/common/data.py"", line 39, in sort_all return sorted_all[2:], sorted_all[1] IndexError: list index out of range Once again, to make sure that the program exits gracefully, I used a try-except block. Assessment of the Analysis Being able to quickly isolate instances where a particular expression is used, while bypassing web pages that contain portions of the expression but not the entire expression, is highly useful. The Stanza analysis is somewhat erratic: given two different text passages, a portion of the expression may sometimes be interpreted as a verb, and elsewhere as a noun, even though the usage is exactly the same in both contexts. Clearly more work needs to be done on Stanza's end. Future Avenues of Development I have already gone well over 30 hours on this project, but if I had more time I would add functions to check for definitions specifically, as well for English translations of the terms. I would also like to explore other Japanese morphological analyzers to see if they perform better. Still, in its current form, this project does what it is supposed to, and should provide a solid foundation for people who want to develop a more robust and function-filled application.Warren Daniel ChildProgress Report of 99 Captain: Warren D. Child (only member) UIUC ID: wchild2 Email: wchild2@illinois.edu PLEASE NOTE: My original CMT link had an incorrect email address and is invalid. The email above is correct. Topic: Japanese Patent Term Retrieval: Identifying the Immediate Context in Which Japanese Patent Terms Are Used for Linguistic Comparison Language: Python Other tools: Google Programmable Search Engine Kuromoji (Japanese Morphological Analyzer) Stanza (Python Wrapper for Stanford Core NLP) Abstract: Japanese patents use a large number of fairly esoteric terms and expressions that are not found in standard dictionaries, and that can therefore be tricky to translate. Such terms are a headache for patent translators, and typically one needs to look at a number of documents where the word is employed to understand its significance. This project seeks to build on top of the standard text retrieval capacities of a major search engine like Google and add functionality that will make it easy to quickly compare sentences containing such a term. Providing laser-focused contextual usage should prove beneficial for technical translators. Discussion: Unlike normal text retrieval, where one is looking for topic relevance in documents, here one is looking for the correct usage of terms in context. For example, in normal text retrieval, the presence of a term within the title might suggest a higher level of relevance. But from the standpoint of understanding how a word is used in context, it is actual sentences that one hopes to find. The topic and the term are not necessarily that closely related. Difficulties: Obviously, one difficulty will be with accurately parsing words in Japanese, which does not employ spaces to parse separate words. Japanese kanji (hiragana and katakana) can sometimes help to indicate word boundaries, though as often as not, hiragana is used in place of kanji. Equally troublesome is the tendency to sometimes use hiragana (or even katakana) in place of characters, or to use variant characters for the same character. I will be researching ways to address this issue. It is unclear how successful Kuromoji will be with more esoteric patent terms. If it does not work or does not allow one to supplement their dictionaries, I may have to use another tool (Stanford's Stanza being a possible alternative). Added Functionality: Taking the search results from the initial search, I will be parsing the retrieved documents to identify context sentences where the critical term occurs. Those sentences will be retrieved in a single document for comparison purposes. Search results that do not have the requisite vocabulary, or only have it in a non-sentence, will be eliminated from the results. Success Benchmarks: If I am able to routinely prepare clean sets of sample sentences for problematic Japanese patent-specific terms, I will consider this successful. Often, search results return a potentially useful document, but because of document length, it is hard to see where the relevant portion or portions are. Having cleanly parsed relevant sample sentences would be a great time-saver for translators and other linguists. Hours Expended: Between having to learn the Kuromoji API, possibly Stanza, features of the Google Programmable Search Engine, and writing scripts to parse the relevant parts of the returned documents should take more than 20 hours. Note: I discussed this topic proposal with the professor, and the idea of adding functionality to Google was actually his suggestion. I am open to more suggestions on how to make this a useful tool for persons like myself who work as translators. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Saturday, November 28, 2020Course Project Progress Report W. Daniel Child Project Objective As delineated in the project proposal, my goal is to develop a suite of search functions that make it easier to search for rare Japanese patent terms and to show those terms in context. With rare terms like this, search engines often return irrelevant pages, and one has to hunt for the immediate context in question. From a translator's perspective, it would be extremely helpful to have concise reports on term contexts to understand how the terms are used. The following is a report on what I have accomplished so far. Setting Up Search Engine Capabilities It appears that Google has discontinued free options for using their search API. Another option has been developed for Python, however, and it seems to work perfectly well. So I am using that as my starting point. So I installed the search engine parser as follows: pip install search-engine-parser It turns out that this search-engine-parser supports different search engines, including Google, Yahoo, and (allegedly) Bing. However, Bing failed miserably, flagging searches as possibly illegal, and so I didn't use it. I did compare Yahoo and Google for the query `Zi ' (""black characters"" in Japanese) and found that they had very different results. Google Top 3 'https://www.google.com/url?q=http://verdure.tyanoyu.net/kuromoji.html&sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAZ6BAgHEAE&usg=AOvVaw1Bg6RJJuLCKaDfYsL1sHWf' 'https://www.google.com/url?q=http://www.jugemusha.com/jumoku-zz-kuromoji.htm&sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAd6BAgIEAE&usg=AOvVaw17dFX2w6L1A6Nug5SQGJT6' Warren Daniel ChildProgress Report of 13Saturday, November 28, 2020'https://www.google.com/url?q=https://www.nakagawa-masashichi.jp/shop/g/g4547639507716/&sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAh6BAgJEAE&usg=AOvVaw2JVwx-Z7pwG8sEWU7Vb9Su' Yahoo Top 3 'https://ja.wikipedia.org/wiki/%25E3%2582%25AF%25E3%2583%25AD%25E3%2583%25A2%25E3%2582%25B8' 'https://www.weblio.jp/content/%25E9%25BB%2592%25E6%2596%2587%25E5%25AD%2597' 'https://search.rakuten.co.jp/search/mall/%25E9%25BB%2592%25E6%2596%2587%25E5%25AD%2597/' Spot-checking the different websites, I actually found some of the Yahoo websites more helpful. I have therefore decided to incorporate both search engines into the patent search system. Parser Once you have a list of web pages, the next step is, of course, to be able to parse these individual pages. I am currently developing the parser (based on BeautifulSoup) to look for the target terms. Since the point is to understand rare terms from context, I need to see sentences and not just headings or titles, which will not be of much help from a context standpoint. I am also going to develop a capability that is sensitive to the possibility of the term being defined, and I am also entertaining the possibility of looking for cases where the term is actually translated into English. Text Analysis and Issues Being Faced I have already tested Stanza (Stanford's Python wrapper for Stanford CoreNLP) and it is looks to be capable of handling Japanese so long as you install it correctly (you need to pay attention to whether you are using vanilla Python or Python via an Anaconda environment) and utilize the right module. Given its inability to properly parse some fairly straightforward terms, however, it is equally clear that the terms I will be testing against will not tend to be in the Stanza dictionaries, meaning that they will not be parsed correctly. Warren Daniel ChildProgress Report of 23Saturday, November 28, 2020That may not matter. I will be able to recognize such terms as appearing in consecutive lemmas. Stanza has a function that enables you to parse sentences, so once I have identified the surrounding context, I should be to pull the context that I am looking for. I have not yet decided whether to it will be necessary to incorporate alternative Japanese tokenizers such as Juman or Kuromoji. What Remains to Be Done Obviously, I need to continue working on the parser, and to make it successfully pulls out the information I need from each of the web pages returned by the search engine parser. Stanza sentence identification needs to be smoothly integrated so that it goes to work on the data extracted by the web parser. Once this has been done, I want to demonstrate how many websites could be bypassed by using this system, and how much easier it is for a translator like myself to find contextual information about rare terms. The final step will be to generate clean reports based on the data that I have extracted.Warren Daniel ChildProgress Report of 33"
https://github.com/wfcwfcwfcwfc/CourseProject	"CS 410 Course Project - Final Report Source code and test set predictions Code: https://github.com/wfcwfcwfcwfc/CourseProject/blob/main/cs410-classification-contest.ipynb Test set predictions: https://github.com/wfcwfcwfcwfc/CourseProject/blob/main/cs410-classification-contest-result.txt Explain your model, and how you perform the training. Describe your experiments with other methods that you may have tried and any hyperparameter tuning. The classifier uses BERT transformer deep learning framework at its core. BERT is a recent NLP framework based on Transformer and self-attention architecture. It serves the ""encoder"" in the transformer model and is widely adopted in text generation and text classification. Training a BERT model generally has two stages: pre-training and fine-tuning. Pre-training aims at providing BERT a general understanding of a language. This step builds the embeddings and trains the parameters. Fine-tuning is optimizing BERT for certain specific tasks. Pre-training requires large language corpus and tremendous computing power. A common practice is to use existing pre-trained model and fine-tuning for the specific task. In this scenario, I use ""bert-large-cased-whole-word-masking"" from hugging-face as pre-trained model, then fine-tuned with the training data provided. After fine-tuning, the model is capable to perform predictions. The performance with default parameters beats the baseline. I also explored other pre-trained models like ""distilbert-base-uncased"", ""roberta-base"", ""xlnet-base-cased"". They all have smaller number of parameters compared to ""bert-large-cased-whole-word-masking"". The performance is good on training set but does not pass the baseline in test data. On engineering side, the model was implemented with PyTorch and deployed on Microsoft Azure ML Studio. It provided convenient middleware for ML tasks for easy deployment and prototyping. All the fine-tuning was done on a single compute, GPU instance and running time is less than 10 minutes. Compute GPU instance pre-installed with CUDA 10.1. All code is contained in the notebook shown in the beginning of this document. I used the ""NLP Best Practices"" library as well as ""NLP Utilities"" library to build the classifier. These libraries also provided example templates which is referenced in this project. Demo and Tutorial Environment Setup Sign up for Microsoft Azure. Create a subscription that allows you to use GPU instances. Student email get $100 free credit. Create resource group and ML workspace In the created workspace, launch ML studio. Create GPU compute instance and start. Upload notebook file, data file and clone the NLP Library. Update Conda: conda update -n base -c defaults conda Generate conda env config: cd nlp-recipes python tools/generate_conda_file.py --gpu Open nlp_gpu.yaml, update pytorch version from 1.4.0 to 1.5.1 Create Anaconda env conda env create -n nlp_gpu -f nlp_gpu.yaml conda activate nlp_gpu Register this virtual env to notebook python -m ipykernel install --user --name nlp_gpu --display-name ""Python (nlp_gpu)"" The environment setup is complete at this point. Running the notebook Open notebook and set kernel as 'nlp_gpu'. Run the notebook, and the result shows up in ""answer.txt"" Intermediate output can be seen in the notebook. Performance passed the baseline. CS 410 Course Project Progress Report Classification Competition - Twitter Sarcasm Detection Which tasks have been completed? Overall Design, implementation and testing. BERT was decided as the core language model. Using pre-training then fine-tuning is the primary strategy. PyTorch as the implementation framework. Microsoft Azure ML platform as the deployment environment. Which tasks are pending? Fine-tuning data to beat the baseline. Using context data to improve the scores. Are you facing any challenges? No CS 410 Project Proposal Fengchao Wang, fw9@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Fengchao Wang - Captain 2. Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection 3. 1. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. N/A 2. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Yes 3. Name some neural classifiers and deep learning frameworks that you may have heard of. RNN, LSTM, GAN, CNN. Tensorflow, Keras, PyTorch 4. Describe any relevant prior experience with such methods Used LSTM model to predict stock price and performed operational metric anomaly detection. Implemented with Facebook's prophet package. 4. Which programming language do you plan to use? Python CourseProject Feel free to reach out if any explnation needed. Slack: fw9 Final Report Code Result Demo Video"
https://github.com/williamdlupo/CourseProject	PolitiTweet Application Suite PolitiTweet is an application suite that mines US politician tweets, builds a Lucene Index and provides a web based application for querying and displaying results. The deployed, working appliation s located at: https://polititweetui.azurewebsites.net TweetMiner - A C# Azure function that runs at 12pm EST to extract and store the previous 24 hours of tweets of US politicians. Leverages the Twitter API and stores tweets in an Azure database. Repo Url: https://github.com/williamdlupo/TweetMiner SearchApi - A Java Azure function application that builds and queries a Lucene Index built upon tweets stored in an Azure database. Repo Url: https://github.com/williamdlupo/SearchApi PolitiTweet-UI - A .Net Core 3 MVC application that accepts user queries, sends queries to deployed SearchApi and displays matching tweets.
https://github.com/woshinisenbaba/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/xiang-wang2020/CourseProject	"1) Progress made thus far The website is deployed. Front-end is near Thnished. The database is built. It is based on university, location, name, and other information. 2) Remaining tasks The backend and the search function. 3) Any challenges/issues being faced. Based on the TAOs instruction, I have narrowed my goal into Oreplicate ExpertSearch,O and I would like to make sure if this is sufficient. I Thnd my peers say the course TA could provide the existing data. I wonder if this is true. Project ProposalI. Team InformationThis will be an individual team.Project Name: IndividualProject Topic: 2.2 ExpertSearch SystemTeam Member: Xiang Wang (xiangw3@illinois.edu)Project Leader/coordinator: Xiang WangII. DetailsBrief idea: I have chosen to work on the ExpertSearch System. I would like to write a my own version of the ExpertSearch System, and the system will have functions includes: a website supporting search functions, a database Thlled with crawled data from testing university, and a new function to support automatically crawling data from a given university.Usefulness demonstration: If anyone ever need to quickly look up a professor, this website will save them time from search themselves, and the new function will make the whole website live, because it expands over time and will be improved every time a user use it.About tools: I would like to use python for backend, and mysql for database. I do not have an idea about search algorithm yet, but I will have it Thgured out soon.How to show my function work as expected: My plan includes adding a new function, and I crawl some data (university faculty info) as my test data and test them against my function.About time: I think building the website, from front end to back end will take at least 15 hours, build the new function will take 10 hours, and testing and other stuff will take 10 hours. Together they are about 35 hours.About timeline: By week 12: Have the website ready. By week 14: Have the new function ready. Video Demo https://drive.google.com/file/d/1X6JIXAFP2zzAArOhJZVI2y-YqfIeZ-SR/view?usp=sharing How to set up: download the package to your local computer open a terminal and move into the folder run ""pip install -r packages.txt"" run python start.py go to http://127.0.0.1:5000/ in your browser What does it do: The goal is to replicate ExpertSearch System, a course project built by former CS410 students (Original Link: http://timan102.cs.illinois.edu/expertsearch//#). The essential function is to search and display ranked information of faculties from many universities. The algorithm behind ExpertSearch is relevance ranker. In this version, I used cosine similarity to rank the documents. The documents are turned into vectors, following the guidance of tf*idf algorithm, and ranked based on how similar they are from the query. Meanwhile, you are able to set up the must-include and must-exclude keywords and set up the number of results you want to see. Introduce my files: Most of the data cleaning and processing work is in process.py. start.py handles the backend, and templates/index.html handles the frontend. The search result will be displayed by templates/result. html. faculties.py includes 999 lines of crawled data from different universities, and every line is one section/person. testcase.txt includes some sample testcases for the grader."
https://github.com/xw23/CourseProject	"Software Implementation: The implementation for this program comes in two parts. We decided to use a Naive Bayes methodology, which starts with a training method and then uses that data to predict classifications onto a testing dataset. We use a bag of words model in which we consider each tweet to be represented as a bag of independent words, meaning that we will ignore the position that the words appear and only deal with the frequency that they appear. Using a unigram model, the program first calls the training function found in training.py, which goes through the training data given and creates a bag of words model. Because each tweet is pre labeled as either sarcasm or not sarcasm, we can treat each tweet as a list of words using the NLTK TweetTokenizer. This method returns a list of sarcastic words and their frequencies, a list of non-sarcastic words and their frequencies, as well as totals for both sarcastic and non sarcastic words. After that, during the classification phase, the program calculates the probability for each tweet to be sarcastic or non-sarcastic based on the probabilities that were developed from the training set. The program develops a posterior probability for each tweet, and based on which is higher, classifies the tweet as either sarcastic or non-sarcastic. In order to avoid zero probabilities, we have a smoothing parameter in place, which has been optimized to give the best results. Installation and running: To install the software, first clone this git repository: https://github.com/shail4221/Classification-Competition.git Once finished, you can run the program inside by running python classify.py. This will train the data on the train.jsonl file, run the classification on the test.jsonl file, and output the results into answer.txt Description of contribution: Both members worked together mostly equally, with Xuechen focusing primarily on the training side of the program and Shail focusing on the classification side. Both team members worked together to refine and test the program for accuracy. Team25ProgressReportTeamName:Team25TeamMembers:XuechenWang(xuechen5),ShailDesai(shailrd2)ProjectTopic:ReproducingAPaper*Subtopic:Patternannotation*Papertoberecreated:Latentaspectratinganalysiswithoutaspectkeywordsupervision(byHongningWang,YueLu,andChengXiangZhai.2011)Progressmadethusfar:Weretrievedthedataset,andlookedinanddidpreliminaryprocessingofthedataset.Wereadthroughtheresearchpaperandmadesomenotes,andcommunicatedourideasabouthowtobuildtheLARAMmodel.Remainingtasks:WritepythonscriptstobuildtheLARAMmodel.Compareourresultswiththeauthors'results.Writeandsubmitourprojectreport.Challenges/issuesfaced:WearehavingahardtimetounderstandandimplementthefunctionsformininglatenttopicsandtheLARAfunction. Team25ProjectProposalTeamName:Team25TeamMembers:XuechenWang(xuechen5),ShailDesai(shailrd2)Captain:XuechenWang(xuechen5)ProjectTopic:ReproducingAPaper*Subtopic:Patternannotation*Papertoberecreated:Latentaspectratinganalysiswithoutaspectkeywordsupervision(byHongningWang,YueLu,andChengXiangZhai.2011)Programminglanguagetouse:PythonTheDatasetsusedinthepaperforevaluation:canbefoundinhttp://sifaka.cs.uiuc.edu/~wang296/Data/index.html CourseProject Software Implementation: The implementation for this program comes in two parts. We decided to use a Naive Bayes methodology, which starts with a training method and then uses that data to predict classifications onto a testing dataset. We use a bag of words model in which we consider each tweet to be represented as a bag of independent words, meaning that we will ignore the position that the words appear and only deal with the frequency that they appear. Using a unigram model, the program first calls the training function found in train.py, which goes through the training data given and creates a bag of words model. Because each tweet is pre-labeled as either ""SARCASM"" or ""NOT_SARCASM"", we can treat each tweet as a list of words using the NLTK TweetTokenizer. This method returns a list of sarcastic words and their frequencies, a list of non-sarcastic words and their frequencies, as well as totals for both sarcastic and non sarcastic words. After that, during the classification phase, the program calculates the probability for each tweet to be ""SARCASM"" or ""NOT_SARCASM"" based on the probabilities that were developed from the training set. The program develops a posterior probability for each tweet, and based on which is higher, classifies the tweet as either ""SARCASM""or ""NOT_SARCASM"". In order to avoid zero probabilities, we have a smoothing parameter in place, which has been optimized to give the best results. Installation and running: To install the software, first clone this git repository: https://github.com/shail4221/Classification-Competition.git Once finished, you can run the program inside by running python classify.py. This will train the data on the train.jsonl file, run the classification on the test.jsonl file, and output the results into answer.txt The video demo for running the code could be found at: https://drive.google.com/file/d/14abGsfp8Gjn4iRPimjK375e4egvLOwgn/view?usp=sharing Description of contribution: Both members worked together mostly equally, with Xuechen focusing primarily on the training side of the program and Shail focusing on the classification side. Both team members worked together to refine and test the program for accuracy. The training file is train.py, the classification file is classify.py, and the output file is answer.txt. The link to video demo is also in VideoLink.txt. Thank you."
https://github.com/yangyangsquare/CourseProject	ClassificationCompetition:TwitterSarcasmDetectionCS410FinalProjectDocumentationYangYangyangy19@illinois.eduAbstractSarcasmdetectionisaspecificcaseofsentimentanalysiswhereinsteadofdetectingasentimentinthewholespectrum,thefocusisonsarcasm.InthisClassificationCompetition,thetaskistodetectsarcasmincontextualTwittertext.InordertobeatthebaselineF1scoreandimprovetheperformance,themainmodelusedinthisprojectisoneoftheState-of-the-ArtNLPmodels,BERT.Iadapttheoff-the-shelfBERTclassifiermodelbyHug-gingface,modifyandexpandtheuseoffine-tuningforotherBERT-basedmodels.Furthermore,IinvestigatetheBERTmodelperformancewhencontextinformationisusedindifferentmanners.Theresultinterestinglyshowsthatdoingthisspecifictaskasasentencepairclassificationoutperformsitasanormaltextclassification.1.IntroductionWiththegrowingroleofsocialmediaacrosstheworld,Sarcasmintweetshasraisedmoreattention.Thus,howtouseNLPmodelstoefficientlydetectSarcasmintweetsalsohasbeenahottopicinbothacademiaandindustry.Inthisproject,IfirstuseacoupleofBERT-basedpre-trainedmodels,suchasBERT,ALBERT,DistilBERTandSqueezeBERT,tounderstandlanguageandbeatthebaselineperformance.ThenIstarttolookatthemethodsofutilizingContextinformationintweets.SarcasmDetection,fromthetopicnameitself,soundslikeaverytypicalbinarytextclassification.SincewehaveContextinformationtogetherwithResponse,IuseContextsentence(s)inthreedifferentmethodstorunthehyperparametertuningunderBERTpre-trainedmodel.2.ApproachOverall,Ifollowtheproceduresbelowtofine-tuneandimprovethemodelperformanceinthisproject:(1)AdaptBERTclassifierfromGoogleResearchandBERTexamplebyHuggingfacetransformers[6];(2)ModifyBERTmodelcodetomakeitapplicableforotherBERT-basedmodels;(3)Loadthetrainandtestdatasetandsplitoriginaltraindatasetin80:20fortrainandvalidation;(4)Runpre-trainedmodelsfromBERT,ALBERT,DistilBERT,SqueezeBERTandXLNetwithsamehyperpa-rametersettingandcompareperformances;(5)CompareBERTmodelperformanceswithdifferentContextmethods;(6)Fine-tunehyperparametersforBERTmodelwithbestContextmethod.2.1.BERTClassifierAdaptionandExpansionInspiredbyRajapakse[3]usingoneoftheState-of-the-ArtNLPmodel,BERT,IfirstlookintoGoogle'soriginalBERTpaper[7],andthennoticetheoff-the-shelfBERTclassifierfromGoogleResearchGitHub[1].WhilesomeTensorFlowbertpackageshaverevisionissuesandhaven'tbeensolvedforawhile.SoIswitchsomefunctionstosimilaronesinPyTorchtofixincompatibleissuesinthecode,luckilybecauseNLPresearchersfromHuggingfacehavedevelopedaPyTorchversionofBERT.1ThenextstepistoexpandtheBERTclassifierforotherBERT-basedmodelfine-tuningformodelperformancecomparison.ThankstoPyTorchAutoClassesmodel,itcanautomaticallyrecognizethearchitecturefrompre-trainedmodelidtoextracttokenizerandconfigurationfileaccordingly.Inordertomakethetrainingportioncompatible,Idigintootherfourmodels'SequenceClassificationfunctionsonHuggingfaceTransformersDocumentationwebsite[2].ItturnsoutthatnotallSequenceClassificationfunctionsareabletoacceptsameinputs:TheSequenceClassificationfunctionsforBERT,ALBERTandSqueezeBERTalltakeinputids,attentionmaskandtokentypeidstokens.WhiletheSequenceClassificationfunctionsforDistilBERTandXLNetbothcanonlyreadinputidsandattentionmasktokens,notokentypeidstokens.SoIneedtodifferentiatethesetwosituationsandsupplydifferentinputs.2.2.BERT-basedModelsAfteradaptionandmodification,thecodecanruntofine-tuneotherpre-trainedmodelslikeBERT,ALBERT,SqueezeBERT,DistilBERTandXLNet.Tobeginwithabasiccomparison,Igothroughthepre-trainedmodelshubandlisthostedbyhuggingface[4].Basedonthemodeldescriptionandcommunityresults,Iselectfollowingmodelsfromthesefivearchitectures:1.BERT:Atransformersmodelpre-trainedonalargecorpusofEnglishdatainaself-supervisedfashion.bert-base-uncasedisthemostpopularBERTmodelandtrainedonlowercasedEnglishtext.2.ALBERT:AliteBERTforself-supervisedlearningoflanguagerepresentations.Itusesrepeatinglayerswhichresultsinasmallmemoryfootprint,howeverthecomputationalcostremainssimilartoaBERT-likearchitecturewiththesamenumberofhiddenlayersasithastoiteratethroughthesamenumberof(repeating)layers.albert-base-v2istrainedonALBERTbasemodelwithnodropout,additionaltrainingdataandlongertraining.3.DistilBERT:Atransformersmodel,smallerandfasterthanBERT,whichwaspretrainedonthesamecorpusinaself-supervisedfashion,usingtheBERTbasemodelasateacher.distilbert-base-uncasedisdistilledfrombert-base-uncasedcheckpoint.4.SqueezeBERT:AbidirectionaltransformersimilartotheBERTmodel.Thekeydifferencebe-tweentheBERTarchitectureandtheSqueezeBERTarchitectureisthatSqueezeBERTusesgroupedconvolutionsinsteadoffully-connectedlayersfortheQ,K,VandFFNlayers.squeezebert-mnli-headlessisthesqueezebert-uncasedmodelfinetunedonMNLIsentencepairclassificationtaskwithdistillationfromelectra-base.Thispre-trainedmodelisspecificallyrecommendedonHuggingfaceSqueezeBERTsite[5]forbestresultswhenfine-tuningonsequenceclassificationtasks.5.XLNet:AnextensionoftheTransformer-XLmodelpre-trainedusinganautoregressivemethodtolearnbidirectionalcontextsbymaximizingtheexpectedlikelihoodoverallpermutationsoftheinputsequencefactorizationorder.xlnet-base-casedistheXLNetbaseEnglishmodel.2ArchitectureModelIDModelSizeHiddenLayer(L)HiddenSize(H)AttentionHeads(A)ParametersBERTbert-base-uncased1276812110MALBERTalbert-base-v212(repeating)7681211MDistilBERTdistilbert-base-uncased67681266MSqueezeBERTsqueezebert-mnli-headless127681251MXLNetxlnet-base-cased1276812110MTable1:ModelSizeSummaryofSelectedPre-trainedModelsThesefivepre-trainedmodelsareselectedfromeachofthearchitecturebecausetheyareeitherthemostefficientorbestforthetaskamongallinthearchitecture.Table1showsamodelsizesummaryofthepre-trainedmodelsIuseintheproject.2.3.MethodstoUseContextInfoWhenadaptingBERTclassifier,InoticethattheInputExampleclasshastwodifferentstringattributestextaandtextbforsequencetext.Apparently,Responseinformationalwaysgoesintotexta.SoIcanimplementContextinformationinthreedifferentmethods:Method1-UseNoContextInfo:OnlyuseResponseinformationintextaandignoreContextinforma-tion.Method2-ConcatenateContextwithResponse(usedin2.2):ConcatenateContextstringafterResponsestringintextaandrunitasanormaltextclassificationtask.Method3-UseContextasSeparateSentenceInfo:UseResponseinformationintextaandContextinformationintextb.NowwhenrunningSequenceClassification,itisactuallyasequencepairclassification.2.4.BERTModelHyperparameterFine-tuningForhyperparameterfine-tuning,Isweepthebatchsizeinf8;16;32g,learningrateinf210 CS 410 Course Project Progress Report Yang Yang (NetID: yangy19) Project Topic: Text Classification Competition 1. Which tasks have been completed? I have implemented pre-trained BERT models from PyTorch on Google Colab for Twitter Sarcasm Detection dataset and beat the baseline. 2. Which tasks are pending? I am finalizing the documented source code with explanations. Then I will focus on creating the tutorial demo. 3. Are you facing any challenges? It was taking too much time when I was trying to train the model on my laptop (CPU) due to size of the dataset. But it got fixed after I switched to Google Colab (GPU). CS410CourseProjectProposalProjectTopic:TextClassificationCompetition1.WhatarethenamesandNetIDsofallyourteammembers?Whoisthecaptain?IndividualTeamName:Yang,YangNetID:yangy192.Whichcompetitiondoyouplantojoin?TextClassificationCompetition3.Ifyouchoosetheclassificationcompetition,areyoupreparedtolearnstate-of-the-artneuralnetworkclassifiers?Namesomeneuralclassifiersanddeeplearningframeworksthatyoumayhaveheardof.DescribeanyrelevantpriorexperiencewithsuchmethodsNeuralNetworkClassifiers:FullyConnectedNeuralNetworksCNN(ConvolutionalNeuralNetworks)RNN(RecurrentNeuralNetworks)LSTM(LongShort-TermMemory)BERT(BidirectionalEncoderRepresentationsfromTransformers)Attention-BasedNetworksGraph-BasedNetworksDeepLearningFrameworks:PyTorchTensorFlowKerasCaffeCNTKMXNetDL4JIdon'thavemuchpracticalpriorexperiencewiththemethodsmentionedabove.Iamreadytolearnmorestate-of-the-artneuralnetworkclassifiers.4.Whichprogramminglanguagedoyouplantouse?IwillusePythonforthisprojectcompetition. Text Classification Competition - Twitter Scarcasm Detection This task is to detect sarcasm from contextual tweets and beat the baseline performance of F1 = 0.723. Voiced Presentation (Demo) Link https://mediaspace.illinois.edu/media/t/1_685r9kih Setup for prediction generation on test dataset Please open the Twitter_Scarcasm_Detection_Source_Code.ipynb file from Google Colab. (Link directly to Colab) Go to Runtime -> Change runtime type, and make sure it has GPU selected as Hardware accelerator and High-RAM as Runtime shape. Go to Runtime -> Run all. It takes approximately 5 minutes to complete. Before you download the answer.txt, you can also look at the validation F1 score, which is usually ~0.83. You can use the Table of contents toolbar on the left to navigate to section 7. Evaluation. Use the Files toolbar on the left, go to outputs -> Twitter_Sarcasm_Detection, and you should be able to see answer.txt. Final Result on test dataset F1 = 0.7626858 More Details Please review the project documentation file, which includes all models I have tried and three different methods I used for Context text string. It also covers model performance comparison and different method comparison specification for this task.
https://github.com/yhui288/CourseProject	"Documentation 1. Project Submission * Project Code: https://github.com/yhui288/ClassificationCompetition * Presentation Video: https://drive.google.com/file/d/1WlLuUBaauCe_yQBEOWXbl8YxxzWSVEQF/view?usp=sharing 2. Overview of The Function of The Project Our team participated in the Text Classification Competition. This competition aims to predict if tweets with given context are sarcasm. We are given training data with labels. The goal is to correctly predict if tweets in test data are sarcasm. Team Member: Changcheng Fu (cf7), Jiaying Li (jl63), Yanghui Pang (yanghui2) Leaderboard Name: Yanghui Pang Rank: ~55 Project File Structure (Download our project here): * train.py/train.ipynb: the program we train the classifier and predict the results for test data * answer.txt: the file storing predicted results for test data * saved_weight.pt: the saved model (the file is too large, we didn't push it to github) 3. Code Implementation * The process of training (what train.py/train.ipynb does): 1) Install packages and import necessary packages 2) Preprocessing Data: Data Loading + Data cleaning 3) Import the pre-trained BERT model and tokenizer 4) Tokenize the data 5) Create train/validate dataloaders 6) Initialize a BERT model, optimizer and loss function 7) Define functions for training and evaluating 8) Start training the model (also save the best model) 9) Predict the test data with the saved model * The models we tried to beat the baseline: First, we try to use traditional models from sklearn to access this problem. We tried Naive Bayes, SVM, LogisticRegression, RandomForestClassifier, KNeighborsClassifier and SGDClassifier from sklearn. Among all these classifiers, the SGDClassifier performed the best which has the highest f1 among them, but the score still cannot beat the baseline. Then, we analyzed the data we have and decided to train a convolutional neural net with pooling, but still found the result is not ideal. We also tried to use fine-tune BERT model with linear layer and dropout layer, but we found that the loss rate is relatively high. Finally, we tried the pre-trained BERT basic model from huggingface. By referencing the research papers, we adjusted the some hyperparameters such as learning rate and the number of epochs, and also tried various loss function. This pre-trained BERT model with adjusted parameters give us prediction that could beat the baseline. 4. Usage of Software (how to run our code) * Run Locally (With python version 3.8) pip3 install -r requirements.txt python3 train.py Or * Run on Colab (Tutorial for running our code on Colab) 1) Upload ""train.ipynb"" from our project 2) Create ""data"" folder, and upload ""train.jsonl"" and ""test.jsonl"" to ""data"" folder 3) Run the code block step by step * The trained model will be saved in ""saved_weight.pt""; Predicted results for test data will be saved in ""answer.txt"" 5. Division of Labor * Changcheng Fu: Tried training a neutral network with CNN and pooling layer; Tried BERT with linear and dropout layer; Tried a pretrained BERT basic model, achieved the best performance and beat the baseline; Wrote the progress report. * Jiaying Li: Tried built-in modules from sklearn such as RandomForestClassifier, KNeighborsClassifier, LogisticRegression; Wrote documentation; Presentation. * Yanghui Pang: Wrote the proposal; Tried built-in modules from sklearn such as Naive Bayes, SVM, SGDClassifier; Revised documentation; Presentation. CS 410 Progress Report 1. Progress made thus far We have already implemented the BERT-based pre-trained model using python and then using linear transformation with train and validation set to fine-tuned the model for text classification. We've already finished the majority of our code but got F1 score of 70.14, which is below the baseline. 2. Remaining tasks We still need to make improvements to our code: adding CNN or doing data cleaning for our training and test set. In addition, we still need to make full documentation for our code and optimize our code if possible after beating the baseline. Also, we need to make our own tutorial video at the end to show our functionality and code. 3. Any challenges/issues being faced. Currently, we can not simply improve our F1 score by adding epoch or simply changing hyperparameter like learning rate, l2 regulation rate, or batch size to improve our performance. As a result, we need more efficient and effective method to increase our model's precision and recall. CS 410 Final Project Proposal - Team1 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Yanghui Pang (yanghui2) Member: Changcheng Fu (cf7), Jiaying Li (jl63) 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We choose the text classification competition, and we are prepared to learn state-of-the-art neural network classifiers. Some neural classifiers that we heard of are the MLP classfier and Keras Neural Network Classifier. We also heard of deep learning frameworks such as PyTorch, TensorFlow. However, none of us have any relevant prior experience with such methods except mp assignments written in CS440: Artificial Intelligence. 4. Which programming language do you plan to use? Python CourseProject - Text Classification Competition Team Member: Changcheng Fu (cf7), Jiaying Li (jl63), Yanghui Pang (yanghui2) Code Submission Find it in ""Code"" directory, or Click here to download our project Project Code: containing the code for training model, saved model, predicted results - train.ipynb/train.py: model training (instruction to run it can be found in ""Documentation.pdf"") - answer.txt: predicted reuslts - saved_weights.pt: the saved model (the file is too large, we didn't push it to github) Presentation Video Click here to watch the presentation video Documentation Find it in ""Documentation.pdf"". Containing the overview of the project, description of our code, instruction to run code, what we did, the division fo labor File Structure Documentation.pdf Proposal.pdf Progress Report.pdf"
https://github.com/yil7/CourseProject	"Text Classification Competition The main goal of this project is to develop a learning system that predicts the label of response on Tweet to be 'SARCASM' or 'NOT SARCASM' while optionally using the context by learning from responses labeled already. The trained model should achieve a fa score above 0.723 on the test data. We trained a machine learning model first, which is XGBoost as our baseline model. It can only achieve a 0.65 f1 score and overfit a lot. Therefore, we have been focused on deep learning models and tuning the parameter after more than 140 times, and we finally beat the baseline. We used the Convolutional Neural Network (CNN) to accomplish this text classification task and achieve an accuracy of 0.7247, which beats the baseline of 0.723. CNN is a class of deep neural network that can take in an input, assign importance to various objects in the input, and be able to differentiate one from the other. It can automatically learn a large number of filters in parallel specific to a training dataset under the constraints of a particular predictive modeling problem. We utilized Python and Jupyter Notebook to develop our learning system. The libraries related were Json, Pandas, String, Nltk, Numpy, Matplotlib, Sklearn, Re, Keras, GENISM, etc. The steps are as follows: Load data: We loaded the data line by line and converted the raw train.jsonl data to a data frame called ""reviews"". Clean and pre-process the data: We made a clean function to remove URL text such as http, @, #, and any numbers. The information from ""response"" and ""context"" was combined into a new column: ""text"". Then we tokenized the texts by using NLTK's word_tokenize so that a sentence is divided into single words. We also turned all letters to lower case. Next, we added two new columns to the 'reviews' data frame to prepare for the binary classification. Prepare Tain and test sets: There were 5000 objects to train the system and 1800 objects to test. We built training vocabulary and got maximum training sentence length (73) and the total number of words in the training set (12526). Load Google News Word2Vec model and trained word embeddings: After that, we loaded the Google News Word2Vec model and trained our word embeddings. Tokenize and Pad sequences: We assigned an integer to each word and put that integer in a list. We padded the sentences so that all training sentences had the same input shape (50). We got embeddings from the Google News Word2Vec model and saved them corresponding to the sequence integer assigned to each word. If there were no embeddings, a random vector was saved for that word. Define CNN: the content of 'text' as a sequence was passed to a CNN. The embeddings matrix was passed to embedding_layer. We applied five different filter sizes to each content and GlobalMaxPooling1D layers to each layer. Outputs were concatenated. Dropout layer - Dense layer - Dropout layer - final Dense layer was applied. We also printed a summary of all the layers with corresponding output shapes. Train CNN: The number of epochs and batch size we utilized were 3 and 80, which means our model will loop around and learn three times, and eighty data will be viewed at a single time. Since the training dataset was small, we took this relatively small number of epochs to avoid overfitting. Since sarcasm is hard to detect, we also lower the confidence level from 0.5 to 0.38 during the inference phase, which means if the classifier is 38 percent sure this sentence is sarcastic, then this sentence will be predicted as sarcasm. Test: We used the model to predict the label of data in the test set and get an accuracy of 0.7247. The CNN system could be utilized on other tasks, such as image recognition, Electromyography (EMG) recognition, which relates to identifying human intention to control assistive devices, video analysis, drug discovery, health risk assessment, and biomarkers of aging discovery, etc. Except for the CNN model, we have also tried another deep learning model: BERT (Bidirectional Encoder Representations from Transformers) model, though it didn't beat the baseline score. Bert model is a pre-trained model, and it typically has several advantages over other NLP models, including quicker training, smaller data size requirement, and higher prediction result. It is widely used in text feature extraction and text classification tasks. However, it performed well in our validation datasets but had inferior prediction results on the test data, which was surprising. To run a BERT model, we utilized the free GPU offered by Google Colab. The libraries we mainly used were TensorFlow, Torch, and Transformers package from Hugging Face library. We used the same data cleaning approaches as the CNN model and did not remove the stop words. We then used the pretrained Bert tokenizer method to convert the response into tokens. Each response was split into tokens of the same length. The shorter response text would be padded with token zero, and we added attention masks to differentiate the padded tokens from the true tokens. The maximum response size was 85, and we set the fixed size to be 120 in case the size of test text data is larger. We split the 5000 training data into 80% training data and 20% test data. The classification model we chose is BertForSequenceClassification. We tried to tune the hyperparameters such as batch size, learning rate, and the number of epochs. However, the best-tuned parameters made the test prediction results worse. In the end, we used batch size = 16, learning rate lr = 2e-5, eps = 1e-8 and epochs = 2. The metric we used to evaluate was the f1 score. When validating the model results from training, the f1 score we had was around 80%. Finally, we did the same procedure to the test data before modeling, and as mentioned above, the results were surprisingly not satisfactory. Data cleaning techniques and tuning parameters also didn't improve the results. Future attempts might include trying different BERT models and padding choices. Reference: CNN: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7 BERT: https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=DEfSbAA4QHas 1. What we have done? We imported twelve necessary packages. We loaded data line by line and convert the raw train.jsonl data to a data frame called ""reviews"". Next, we made a clean function to remove URL text such as http, @, #, and any numbers. The information from ""response"" and ""context"" was combined into a new column: ""text"". We processed the information from ""text"" with our clean function. We also turned the ""label"" to 0-1 from ""SARCASM/NOT_SARCASM"". Then we separated both information from ""text"" and ""label"" into two sets: ""test"" set with 20% of raw data and ""train"" set with 80% of raw data for validation test. We transformed information from text train set and text test to vector with TfidfVectorizer (stopword-'English', range of words to be extracted is 1-3). We then applied the XG Boost model and find out the training accuracy is 0.96125 and test accuracy is 0.665. We also loaded raw test.jsonl data in to a data frame called ""test"" to do the predicted test and processed it with in the same way. We applied the XG Boost model to predict the label and convert 0-1 to SARCASM/NOT_SARCASM. 2. Remaining task We will try to find out whether more pre-processing could be done. The parameters of vectorizer may still be adjusted. We will also try to build a deep learning model - LSTM to better improve the accuracy of our model. 3. Challenges The accuracy of train set is good enough whereas the accuracy of test set is much lower. That means we might overfit. In your project proposal, please answer the following questions: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Which competition do you plan to join? If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Which programming language do you plan to use? 1. Team Members Wenxin Fang wenxinf2 Shengyi Wang shengyi4 (Captain) Yi Li yil7 2. Text classification competition 3. Baseline model: Logistic regression, Naive Bayes Our deep learning framework: LSTM Deep learning We have some experience in implementing and training logistic regression on a toy dataset. However, we never train and test this model on text data before. We haven't had some experience in Naive Bayes and LSTM deep learning framework. But we are thrilled to learn more about these machine learning approaches and look forward to seeing how it performs on the tweet dataset. 4. Python CS 410 Final Project: Classification Competition Preparation Datasets used are test.jsonl and train.jsonl in the data folder. Required Packages scikit learn numpy xgboost Pytorch matplotlib seaborn nltk json pickle pandas re math tensorflow sys string gensim os keras Baseline Model All the work done within the jupyter notebook Text Classification baseline.ipynb CNN Model All the work done within the jupyter notebook CNN-final.ipynb BERT Model All the work done within the jupyter notebook BERT.ipynb Documentation Documentation.docx Video Illustration https://youtu.be/UJcDlOYOAZY Reference CNN: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7 BERT: https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=DEfSbAA4QHas"
https://github.com/yiz9/CourseProject	"ACross-CollectionMixtureModelforComparativeTextMiningChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbanaChampaignAtulyaVelivelliDepartmentofElectricalandComputerEngineeringUniversityofIllinoisatUrbanaChampaignBeiYuGraduateSchoolofLibraryandInformationScienceUniversityofIllinoisatUrbanaChampaignABSTRACTInthispaper,wede neandstudyanoveltextminingproblem,whichwerefertoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcom-monthemesacrossallcollectionsaswellassummarizethesimilarityanddi erencesofthesecollectionsalongeachcom-montheme.Thisgeneralproblemsubsumesmanyinterest-ingapplications,includingbusinessintelligenceandopinionsummarization.Weproposeagenerativeprobabilisticmix-turemodelforcomparativetextmining.Themodelsimul-taneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.ThemodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algo-rithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.CategoriesandSubjectDescriptors:H.3.3[Informa-tionSearchandRetrieval]:TextMiningGeneralTerms:AlgorithmsKeywords:Comparativetextmining,mixturemodels,clus-tering1.INTRODUCTIONTextminingisconcernedwithextractingknowledgeandpatternsfromtext[5,6].Whiletherehasbeenmuchre-searchintextmining,mostexistingresearchisfocusedononesinglecollectionoftext.Thegoalsareoftentoextractbasicsemanticunitssuchasnamedentities,toextractrela-tionsbetweeninformationunits,ortoextracttopicthemes.Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Tocopyotherwise,torepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.KDD'04,August22-25,2004,Seattle,Washington,USA.Copyright2004ACM1-58113-888-1/04/0008...$5.00.Inthispaper,westudyanovelproblemoftextminingre-ferredtoasComparativeTextMining(CTM).Givenasetofcomparabletextcollections,thetaskofcomparativetextminingistodiscoveranylatentcommonthemesacrossallcollectionsaswellassummarizethesimilarityanddi er-encesofthesecollectionsalongeachcommontheme.Specif-ically,thetaskinvolves:(1)discoveringthedi erentcom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Theneedforcomparativetextminingexistsinmanydi erentapplica-tions,includingbusinessintelligence,summarizingreviewsofsimilarproducts,andcomparingdi erentopinionsaboutacommontopicingeneral.Inthispaper,westudytheCTMproblemandproposeagenerativeprobabilisticmixturemodelforCTM.Themodelsimultaneouslyperformscross-collectionclusteringandwithin-collectionclustering,andcanbeappliedtoanarbitrarysetofcomparabletextcollections.Themixturemodelisbasedoncomponentmultinomialdistributionmodels,eachcharacterizingadi erenttheme.Thecommonthemesandcollection-speci cthemesareexplicitlymodeled.Thepro-posedmodelcanbeestimatedecientlyusingtheExpectation-Maximization(EM)algorithm.Weevaluatethemodelontwodi erenttextdatasets(i.e.,anewsarticledatasetandalaptopreviewdataset),andcompareitwithabaselineclusteringmethodalsobasedonamixturemodel.Experimentresultsshowthatthemodelisquitee ectiveindiscoveringthelatentcommonthemesacrosscollectionsandperformssigni cantlybetterthanourbaselinemixturemodel.Therestofthepaperisorganizedasfollows.InSection2,webrie yintroducetheproblemofCTM.Wethenpresentabaselinesimplemixturemodelandanewcross-collectionmixturemodelinSection3andSection4.WediscusstheexperimentresultsinSection5.2.COMPARATIVETEXTMINING2.1AmotivatingexampleWiththepopularityofe-commerce,onlinecustomereval-uationsarebecomingwidelyprovidedbyonlinestoresandthird-partywebsites.Pioneerslikeamazon.comandepin-ions.comhaveaccumulatedlargeamountsofcustomerinputincludingreviews,comments,recommendationsandadvice,etc.Forexample,thenumberofreviewsinepinions.comismorethanonemillion[4].Givenaproduct,therecouldbeuptohundredsofreviews,whichisimpossibleforthereaderstogothrough.Itisthusdesirabletosummarizeacollectionofreviewsforacertaintypeofproductsinordertoprovidethereadersthemostsalientfeedbacksfromthepeers.Forreviewsummarization,themostimportanttaskistoidentifydi erentsemanticaspectsofaproductthatthereviewersmentionedandtogrouptheopinionsaccord-ingtotheseaspectstoshowsimilaritiesanddi erencesintheopinions.Forexample,supposewehavereviewsofthreedi erentbrandsoflaptops(Dell,IBM,andApple),andwewanttosummarizethereviews.Ausefulsummarywouldbeatab-ularrepresentationoftheopinionsasshowninTable1,inwhicheachrowrepresentsoneaspect(subtopic)anddi er-entcolumnscorrespondtodi erentopinions.Table1:AtabularsummarySubtopicsDellIBMAppleBatterylifelongenoughshortshortMemorygoodbadgoodSpeedslowfastfastItis,ofcourse,verydicult,ifnotimpossibletopro-ducesuchatablecompletelyautomatically.However,wecanachievealessambitiousgoal{identifyingthesemanticaspectsandidentifyingthecommonandspeci ccharacter-isticsofeachproductinanunsupervisedway.Thisisaconcreteexampleofcomparativetextmining.2.2ThegeneralproblemTheexampleaboveisonlyoneofthemanypossibleappli-cationsofcomparativetextmining.Ingeneral,thetaskofcomparativetextmininginvolves:(1)discoveringthecom-monthemesacrossallthecollections;(2)foreachdiscoveredtheme,characterizewhatisincommonamongallthecol-lectionsandwhatisuniquetoeachcollection.Itisveryhardtopreciselyde newhatathemeis,butitcorrespondsroughlytoatopicorsubtopic.Thegranularityofthemesisapplication-speci c.CTMisafundamentaltaskinex-ploratorytextanalysis.Inadditiontoopinioncomparisonandsummarization,ithasmanyotherapplications,suchasbusinessintelligence(comparingdi erentcompanies),cus-tomerrelationshipmanagement(comparingdi erentgroupsofcustomers),andsemanticintegrationoftext(comparingcomponenttextcollections).CTMischallenginginseveralways:(1)Itisacompletelyunsupervisedlearningtask;notrainingdataisavailable.(ItisforthesamereasonthatCTMcanbeveryusefulformanydi erentpurposes{itmakesminimumassumptionsaboutthecollectionsandinprinciplewecancompareanyarbitrarypartitionoftext.)(2)Weneedtoidentifythemesacrossdi erentcollections,whichismorechallengingthanidentifyingtopicthemesinonesinglecollection.(3)Thetaskinvolvesadiscriminationcomponent{foreachdiscov-eredtheme,wealsowanttoidentifytheuniqueinformationspeci ctoeachcollection.Suchadiscriminationtaskisdif- cultgiventhatwedonothavetrainingdata.Inaway,CTMgoesbeyondtheregularone-collectiontextminingbyrequiringan\alignment""ofmultiplecollectionsbasedoncommonthemes.Sincenotrainingdataisavailable,ingeneral,wemustrelyonunsupervisedlearningmethods,suchasclustering,toperformCTM.Inthispaper,westudyhowtouseprob-abilisticmixturemodelstoperformCTM.Belowwe rstdescribeasimplemixturemodelforclustering,whichrepre-sentsastraightforwardapplicationofanexistingtextmin-ingmethod,andthenpresentamoresophisticatedmixturemodelspeci callydesignedforCTM.3.CLUSTERINGWITHASIMPLEMIXTUREMODELqqqqqFigure1:TheSimpleMixtureModelAnaivesolutiontoCTMistotreatthemultiplecollec-tionsasonesinglecollectionandperformclustering.Ourhopeisthatsomeclusterswouldrepresentthecommonthemesacrossthecollections,whilesomeotherswouldrep-resentthemesspeci ctoonecollection(seeFigure1).Wenowpresentasimplemultinomialmixturemodelforclus-teringanarbitrarycollectionofdocuments,inwhichweassumethereareklatentcommonthemesinallcollections,andeachischaracterizedbyamultinomialworddistribu-tion(alsocalledaunigramlanguagemodel).Adocumentisregardedasasampleofamixturemodelwiththesethememodelsascomponents.We tsuchamixturemodeltotheunionofallthetextcollectionswehave,andtheobtainedcomponentmultinomialmodelscanbeusedtoanalyzethecommonthemesanddi erencesamongthecollections.Formally,letC=fC1;C2;:::;Cmgbemcomparablecol-lectionsofdocuments.Let1;:::;kbekthemeunigramlanguagemodelsandBbethebackgroundmodelforallthecollections.Adocumentdisregardedasasampleofthefollowingmixturemodel(basedonwordgeneration).pd(w)=Bp(wjB)+(1 We will briefly address the 4 questions posted on piazza in this document. Note that this is not our project report, for a detailed report of our work, please see the Project Report file. An overview of the function of the code (i.e., what it does and what it can be used for) The python scripts in the datasets folders are used to obtain the two datasets. Using our own work in MP2, we were able to retrieve text data from news articles reporting COVID-19 and SARS. The python scripts in the models and analysis folder are used to generate and run the models. Our inputs are the datasets we obtained. Our outputs are clusters of words and their probabilities. They are presented both in the Project Report file and the models and analysis folder. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Please watch the demo provided in the video presentation. The use of our code is straight forward. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Please watch the demo provided in the video presentation. The use of our code is straight forward. Brief description of the contribution of each team member in case of a multi-person team. Each team member contributed equally to the program. We had multiple group meetings to assign tasks and made sure that everyone was on the same page. Dongni Yang: Studied the paper, obtained the COVID-19 dataset, obtained the SARS dataset, generated and improved the models, ran the model. The main contributor to the demo of our source code. Zhaoyuan Yang: Studied the paper, helped to generate the models, analyzed the results, and provided conclusion. The main contributor to the Project Report. Yi Zhou: Studied the paper, explained the EM algorithm used to improve the CCMix model, helped the running of the model, administrative tasks (CMT, Github, presentations, etc.) Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients Progress Report Group Members: Yi Zhou Zhaoyuan Yang Dongni Yang The project of our group focuses on reproducing the results of the paper ""A cross-collection mixture model for comparative text mining"" by Zhai and his co-workers. In the paper, Zhai used his model to examine two sets of news data, one on the Iraq War and the other on the Afghanistan war. Specifically, the Iraq war news excerpts were a combination of 30 articles from the CNN, BBC websites over 2013-2014. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC websites for one year starting from Nov. 2001. To examine the robustness of the model, we decided to use different news data for our project. The Covid-19 pandemic is an event that we believe is both appropriate and comparable. With the help of our work and experiences from MP2, we extracted 35 articles from CNN, BBC, and HUFFPOST websites to form our dataset. (Work Completed) We have also begun to input the modeling formulas into our scripts. The large number of unfamiliar concepts and variables have caused us some problems. We also found some of the formulas difficult to understand, but we believe we can fix this problem by further studying the paper and doing research on the related topics. (Challenges Encountered) Our next steps would be to finish the modeling and run the dataset. Plotting and analyzing the data may involve the usage of R Markdown. After analyzing the data, we will check if our results correspond to the conclusions from the paper. (Work Pending) To view our work in detail, please take a look at the ""Progress Report"" folder. Project Report Our project aims to reproduce the results in the paper by Zhai and coworkers, ""A cross-collection mixture model for comparative text mining"". In this paper, the author developed an efficient text mining model, the Cross-Collection Mixture Model (CCMix), using the Expectation-Maximization (EM) algorithm for Comparative Text Mining (CTM). The simple mixture model was originally used as a naive solution to CTM. This method treats the multiple collections as one single collection and performs clustering. The CCMix Model, however, uses latent common themes as well as a potentially different set of collection-specific themes for each collection. These component models directly correspond to all the information we are interested in discovering. In the paper, the authors used new articles from BBC and CNN as datasets to test this model. The comparison was taken between results from news on the Iraq War and news on the Afghanistan War. Unfortunately, we are unable to obtain the same dataset for this project. Instead, we chose the COVID-19 pandemic and the SARS outbreak in 2003 as the comparing events. We extracted 35 news articles from CNN, BBC, and HUFFPOST to form our dataset. To build the model, we used an open-source code on GitHub as the basis, with added features on theme clustering to let it behave as similarly in the paper. The paper suggested that the CCMix model can help reveal many interesting common aspects between the Iraq War and the Afghanistan War that the SimpMix model failed to do. The results presented by SimpMix were less meaningful, making it hard for people to extract useful information from the common words. In contrast, we were able to gather useful information from the CCMix results. For example, from cluster 5 we knew that the news about both events mentioned the diplomatic role played by the United Nations, and from cluster 4 we knew that there were Monday briefings by an official spokesman of a political administration during both wars. Our attempt in comparing the SimpMix and CCMix model also led to meaningful results. We set lB = 0:95 for SimpMix and set lb = 0:9, lc = 0:25 for CCMix; in both cases, the number of clusters is fixed to 5. These parameters are exactly the same as in the paper. The results are tabulated below: SimpMix Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients Although there were some interesting themes from the SimpMix results (for instance, cluster 2 mentioned the lockdown policies in Britain, and cluster 3 indicates the government's effort in inventing vaccines), we could not find common themes to both events. It appears that cluster 1 to 3 solely described themes on COVID-19 and cluster 4 and 5 only covered the SARS pandemic. The CCMix results gave us more information on the similarities between SARS and COVID-19. In cluster 1, ""vaccine"", ""firms"" and ""animals"" suggest that there had been attempts for animal trials on vaccines in both cases. In cluster 4, we were able to tell that both pandemics involved an outburst of infected cases in Hong Kong, which allowed the events to draw global attention. The common theme captured in cluster 5 simply told us that a lot of patients went to the hospitals because of the diseases. The term ""dr"" also implied that there are large portions of interviews and advice of doctors in the news articles. Overall, our reproduction work using CCmix model gave us similar results as in the paper. However, it was evident that there were aspects that our model failed to do. For example, words such as ""coffee"" and ""beans"" that were unrelated to our topic appear in the common theme words section in our results. One possible explanation is that we falsely collected words in the advertisement into our dataset. Since our new articles mainly came from BBC News, the article websites may be displaying the same advertisement on coffee which let ""coffee"" being the most popular theme word. Lacking information on specific events was another drawback of our research. In the paper, the authors were able to gather detailed information on the themes (for example, the name of the spokesman, the name of the commander, etc.). Our results, however, only contained general aspects of the events. We were unable to figure out what exactly happened by just looking at the common theme words. This might be caused by the inherent difference of our dataset compared to the one in the paper. As the geographic scale and timescale are much larger in the case of the pandemic than the war, the focus on the news articles tends to be diverse. Therefore, it can be hard for a word that is only important to a specific event to stand out in the common theme words list in our case. One possible solution we can think of is to set a lower lb value to let uncommon words more favored that set a higher lc to make the collection more collection-specific. Project Report Our project aims to reproduce the results in the paper by Zhai and coworkers, ""A cross-collection mixture model for comparative text mining"". In this paper, the author developed an efficient text mining model, the Cross-Collection Mixture Model (CCMix), using the Expectation-Maximization (EM) algorithm for Comparative Text Mining (CTM). The simple mixture model was originally used as a naive solution to CTM. This method treats the multiple collections as one single collection and performs clustering. The CCMix Model, however, uses latent common themes as well as a potentially different set of collection-specific themes for each collection. These component models directly correspond to all the information we are interested in discovering. In the paper, the authors used new articles from BBC and CNN as datasets to test this model. The comparison was taken between results from news on the Iraq War and news on the Afghanistan War. Unfortunately, we are unable to obtain the same dataset for this project. Instead, we chose the COVID-19 pandemic and the SARS outbreak in 2003 as the comparing events. We extracted 35 news articles from CNN, BBC, and HUFFPOST to form our dataset. To build the model, we used an open-source code on GitHub as the basis, with added features on theme clustering to let it behave as similarly in the paper. The paper suggested that the CCMix model can help reveal many interesting common aspects between the Iraq War and the Afghanistan War that the SimpMix model failed to do. The results presented by SimpMix were less meaningful, making it hard for people to extract useful information from the common words. In contrast, we were able to gather useful information from the CCMix results. For example, from cluster 5 we knew that the news about both events mentioned the diplomatic role played by the United Nations, and from cluster 4 we knew that there were Monday briefings by an official spokesman of a political administration during both wars. Our attempt in comparing the SimpMix and CCMix model also led to meaningful results. We set lB = 0:95 for SimpMix and set lb = 0:9, lc = 0:25 for CCMix; in both cases, the number of clusters is fixed to 5. These parameters are exactly the same as in the paper. The results are tabulated below: SimpMix Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients Although there were some interesting themes from the SimpMix results (for instance, cluster 2 mentioned the lockdown policies in Britain, and cluster 3 indicates the government's effort in inventing vaccines), we could not find common themes to both events. It appears that cluster 1 to 3 solely described themes on COVID-19 and cluster 4 and 5 only covered the SARS pandemic. The CCMix results gave us more information on the similarities between SARS and COVID-19. In cluster 1, ""vaccine"", ""firms"" and ""animals"" suggest that there had been attempts for animal trials on vaccines in both cases. In cluster 4, we were able to tell that both pandemics involved an outburst of infected cases in Hong Kong, which allowed the events to draw global attention. The common theme captured in cluster 5 simply told us that a lot of patients went to the hospitals because of the diseases. The term ""dr"" also implied that there are large portions of interviews and advice of doctors in the news articles. Overall, our reproduction work using CCmix model gave us similar results as in the paper. However, it was evident that there were aspects that our model failed to do. For example, words such as ""coffee"" and ""beans"" that were unrelated to our topic appear in the common theme words section in our results. One possible explanation is that we falsely collected words in the advertisement into our dataset. Since our new articles mainly came from BBC News, the article websites may be displaying the same advertisement on coffee which let ""coffee"" being the most popular theme word. Lacking information on specific events was another drawback of our research. In the paper, the authors were able to gather detailed information on the themes (for example, the name of the spokesman, the name of the commander, etc.). Our results, however, only contained general aspects of the events. We were unable to figure out what exactly happened by just looking at the common theme words. This might be caused by the inherent difference of our dataset compared to the one in the paper. As the geographic scale and timescale are much larger in the case of the pandemic than the war, the focus on the news articles tends to be diverse. Therefore, it can be hard for a word that is only important to a specific event to stand out in the common theme words list in our case. One possible solution we can think of is to set a lower lb value to let uncommon words more favored that set a higher lc to make the collection more collection-specific. Reproducing A Paper: Contextual text mining The team captain is Yi Zhou. Netid is yiz9. Team members are: Yi Zhou. Netid is yiz9. (Captain) Zhaoyuan Yang. Netid is zy7. Dongni Yang. Netid is dongniy2. Our chosen paper is A CrossCollection Mixture Model for Comparative Text Mining. We are going to use python in this project. We can't obtain the exact same dataset used in the paper. However, we can use other datasets to achieve similar goals. CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Progress Report updated. (Used part of our work from MP2. I hope that's ok.) Project completed. Make sure you check out our Project Report and our Project Presentation, which includes a demo of our source code. Source code for each part can be found in each folder. We used our own works in MPs and also some online resources. The references can be found in the main branch. Thank you for grading! If needed, please contact us through the emails provided on the CMT. Just added an addition to our documentation, which answers the 4 questions on Piazza. Thank you for Grading!"
https://github.com/yuanchung1987/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yukuo78/CourseProject	Monday, November 30, 2020Progress Report Subject: Reproducing paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. 1)Progress made thus far Finished reading this paper and the original paper Thrst paper. Acquired the datasets needed for the paper experiments. 2) Remaining tasks The remaining tasks would be actually understand the described model and try to reproduce the algorithm and experiment in time, hopefully. 3) Any challenges/issues being faced. Since previously IOm quite busy with some other study in parallel with this course. Except for keeping up with the lesson and MPs, and also the taking exam, I hadnOt have enough time to work on the project, or to join a team. So IOm working alone now. This is also why I chose this reproducing paper as project, since I Thgured this requires least creativity thus effort, and might get some help from classmates and TAs. 1 Sunday, October 25, 2020Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. kuoyu2 2. Which paper have you chosen? Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. 3. Which programming language do you plan to use? Python 4 Can you obtain the datasets used in the paper for evaluation? Maybe not all. 5. If you answer OnoO to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? I belive I can with some help from TAs. 6. If you answer OnoO to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?1 CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Please check the docs directory for all progress report, documentation and video presentation.
https://github.com/yunfeim2/CourseProject	CourseProject This project is about reproducing the results for latent aspect analysis. The source code is from the paper provided with the data. Software installation first we should install the nltk tool using the command: import nltk nltk.download() The project will be base on a mix of java and python run the project first we should generator the key work for our latent aspect analysis by using the code python key_generator.py the stopwords.txt is for the stop work that will like occur in all kinds of documents. Therefore we ignore the exsitence of the those words to produce a better keyword collection. After that, download the NLP from https://opennlp.apache.org/ to install and put the enviromental variable to path to the folder you installed Next, we should using java platform to run the analyse.java under src and the final result of the hotel data will be listed under vectors folder in vector_CHI_4000.dat presentation vedio : https://mediaspace.illinois.edu/media/1_64gzbvp4 Sources http://sifaka.cs.uiuc.edu/~wang296/Data/index.html http://sifaka.cs.uiuc.edu/~wang296/ https://www.cs.virginia.edu/~hw5x/paper/p618.pdf
https://github.com/yunhezhang/CourseProject	CourseProject Final report - https://github.com/yunhezhang/CourseProject/blob/main/doc/final_report.pdf. Demo is at https://github.com/yunhezhang/CourseProject/blob/main/demo.mp4. Directory structure: doc - documentations src - source code for the SVM model and web crawler trainingData - training data for the SVM model
https://github.com/yuranw3/CourseProject	"CourseProject Team-RPDD Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Group Name: Team-RPDD Presentation Link https://mediaspace.illinois.edu/media/t/1_fylki6t8 Team Name: Team-RPDD Team Member: Yuran Wang netid: yuranw3  [captain] Hongru Wang netid:hongru2 Zhengyu Li netid: zhengyu6 1) Progress made thus far We've read the papers.We have started writing code. We have not finished the coding session and still in progress. 2) Remaining tasks We will debug our program soon once we finish it. 3) Any challenges/issues being faced We have not encountered any specific challenges yet. Documentation Team:  Team-RPDD Team member:Yuran Wang (netid:yuranw3) [captain] Zhengyu Li (netid:zhengyu6) Hongru Wang (netid:hongru2) Project:  Reproducing A Paper(Latent aspect rating analysis) 1)An overview of the functions of the code: a)Create vocabulary from the dataset. b)Use stemmer on the dataset to create a vocabulary corpus of the reviews, which will be used for aspect mining and rating. c)Use BootStrapping to mine aspects(stored in\Data\Seeds\hotel_bootstrapping.dat ), use regression to calculate weight for each aspect. d)Calculate rating per minded aspects. e)The results are in Data/Vectors/vector_CHI_4000.dat"" 2)Implementation documentation: a)Acknowledgment: The authors of the paper released the codes for LARA on his personal website( http://sifaka.cs.uiuc.edu/~wang296/ ) . We modified the original version of the code for our projects. b)Review dataset sources:  http://sifaka.cs.uiuc.edu/~wang296/Data/index.html c)Setup instructions: i)Add colt.jar, concurrent.jar, opennlp-maxent-3.0.1-incubating.jar, opennlp-tools-1.5.1-incubating.jar, and JRE System Library[Java-SE-1.7] to the libraries of Java Build Path. Then run the ""Analyzer"" file. 3)Usage documentations: a)In order to run the code, the user needs to import tools from opennlp.  You can import the core toolkit directly from Maven. Then run mvn install.  The detailed dolumentation can be found here  https://github.com/apache/opennlp . b)It requires tools from NTLK c)In order to run the code, the user also needs Python3 and Java. 4)Work distribution: a)We worked together so everyone on the team participated. 5)Final report:  The authors of the paper developed the LARAM to effectively solve the problem of LARA, including automatically identifying meaningful topical aspects, inferring interesting differences in aspect ratings within reviews, and modeling users' preferences with the inferred relative emphasis on different aspects. Such detailed analysis of opinions at the level of topical aspects enabled by LARAM can support multiple application tasks, including aspect opinion summarization, ranking of entities based on aspect ratings, and analysis of reviewers rating behavior. Our project borrowed the ideas from the author's work about LARA, and our work is mainly based on aspects-mining. 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team member1:Yuran Wang (netid:yuranw3) [captain] Team member2:Zhengyu Li (netid:zhengyu6) Team member3:Hongru Wang (netid:hongru2) 2.Which paper have you chosen? We choose option 1: Reproducing a paper as our project. And we choose paper Latent aspect rating analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3.Which programming language do you plan to use? Python 4.Can you obtain the datasets used in the paper for evaluation? Yes"
https://github.com/yy228731/CourseProject	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/zachdick93/CourseProject	"CODE DOCUMENTATION Project Directory Structure: The code implemented for this project is in the LARA directory. In this directory you will find the Makefile that is used to compile and build this code. If the project has been built you will also see the object files (e.g. main.o), and the executable prog.exe. The src directory contains the source files for this project, and the include directory includes the header files. The file src/main.cpp is used to test the classes implemented in this project. The main class in this project is the LRR class. The code for this class can be found in the src/lara directory. In src/optimization is an implementation of the LBFGS optimization algorithm (https://en.wikipedia.org/wiki/Limited-memory_BFGS for more information). The directories src/utilities and src/algebra have some utility like functions used to perform frequently used calculations throughout the rest of the code. To learn more about the theoretical aspects of the implemented functionality in this project I recommend reading the document titled manual.pdf. Building and Running: This code was developed on Windows 10 and compiled using the MINGW64 implementation of the g++ compiler tools. There is no guarantee that this will work on other operating systems, but I would expect that it will work with any g++ version on any OS. To build this code open a command prompt, go into the LARA, and run the command make all. Once the project has built you will see an executable named prog.exe. To run this code, using that same command prompt window, type in the command ./prog.exe. Testing the Code There is a test function for the LRR functionality in the main.cpp folder (see image below). This function initializes the model with the same parameters used in the document manual.pdf which was provided by the original project implementation. Then this function calls the EMEst function passing a file of vectors, also from the original project. At the end of the EM estimation step, this function will save the predictions in LARA/Data/Results/prediction.dat and it will save the model, which can be loaded as a 5th parameter to the constructor, in LARA/Data/Model/model_hotel.dat. CS410 Course Project: Implement LRR for MeTA By Zachariah Dick Project Description Implement the aspect segmentation and Latent Rating Regression model in C++ from the paper ""latent aspect rating analysis on review text data: a rating regression approach"". Developed with the intent to be easily ported into the MeTA codebase. I was able to use the original Java project code as a reference for this. So, ultimately the project was rewriting the Java code in C++. Development Environment OS: Windows 10 Build Tools: MINGW64 implementation of g++. IDE: Visual Studio Code In This Repository The C++ code can be found in the LARA directory. The original Java code can be found in the OldJavaLARAProject directory. The project directory structure is the same between the two. Src: contains the source code files. Includes: contains header files. Data: includes the data input and output files used by the project. The next slide has a picture of the directory tree structure. Challenges The MeTA project had too much technical debt for me to build it locally on my machine without many errors. The original LARA Java Code project files were not recognizable by my IntelliJ IDE, so I was not able to use it as a debugging tool for my C++ implementation. The Java code referenced some jar file Java specific libraries that I was not able to replace properly. One was a linear algebra library that I replaced by implementing the functionality. State of the Project I can build and run the program, but the results are not recognizable by the ""topics.py"" script in the data file, which is used to print out the top M topics. With some investigation it is due to the f value and gnorm values being insanely large as they are going through the ""optimizer"" code. Example of the values on next slide displayed in picture of the output from a run of the program (e.g. gnorm printed as ""inf""). Building the Code Ensure you have a version of MINGW64 installed with g++. Open a command prompt and go to the CourseProject/LARA directory and run the command ""make all"" (or whatever make-like command your version of MINGW64 uses for running makefiles). The program will build into the executable ""prog.exe"". See next slide for example picture. Running the Code There is a test function in ./LARA/src/main.cpp call lrr_test(), where you can modify the parameters for this function. You will find a document in this code called ""manual.pdf"" which comes from the original java project and contains more detail about the Lara implementation in these projects. Unfortunately running the C++ project does not currently produce any meaningful results. You can see the saved model, and predictions in the ./LARA/Data directory. See next slide for example picture. Next Steps (if I had more time!) Upgrade the Java project so that I can run it as a reference of comparison to the C++ project. Upgrade the MeTA project so that it can be built with updated tools. Replace the algebra implementation with an optimized library, maybe a Boost library. Replace the LBFGS code with an updated optimizer quasi-newton algorithm. Develop a dataset with expected output for each function using the what has been output by the Java code. References Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903 Progress Report Which tasks have been completed? I currently have set up my development environment using VSCode and by following the setup instructions for the MeTA project. I have reviewed the MeTA project and the LARA project from the paper. Due to being given the location of the data and code from the paper I have decided to convert the Java project from the paper to C++ and to hopefully be able to recreate the results from the paper using the same dataset. I have begun converting the Java code to C++. Which tasks are pending? I need to finish converting the Java code to C++. I also need to setup the C++ repo as a CMake project. I then need to complete the documentation and presentation. Are you facing any challenges? I attempted to build the MeTA package, but it failed. I even debugged it and updated it a bit, but the technical debt of that project seems to have built up to a point where it would not be worth continuing with that. Another challenge that will likely present itself soon is in finding a C++ equivalent for each of the Java libraries used in the paper. Project Proposal Team Captain: Zachariah Dick, NetID: zdick2 (One-person team) Topic: Improving a System: MeTA Toolkit Sub-Topic: Add text mining functions to the MeTA toolkit Main Proposal: I propose implementing the aspect segmentation and Latent Rating Regression model (LRR) in C++ from the paper ""latent aspect rating analysis on review text data: a rating regression approach"" mentioned in the project topics document. This functionality will be developed with the intent to be easily ported into the MeTA codebase. Due to time constraints and the fact that I am a one person team I do not intend on fully integrating this functionality into the meta repository, but instead applying an MIT license to this repository and leaving it public to allow for future integration into the MeTA project. Task List Estimate: Setup development environment based on that of the meta-toolkit/meta. Review the structure of the MeTA repository to gather needed namespaces, data structures, testing setup, and other necessary information to ensure compatibility in my setup. Dissect and Analyze the mentioned paper to develop the pseudo code for the implementation and fully understand the algorithm. Gather testing datasets and process them using python tools that have been used in MPs. Use metapy wherever possible to ensure compatibility with MeTA. The link to the dataset used in the paper is no longer valid so I will need to put together my own test data. Write the functions needed to do the implementation. Attempt to maintain the same pattern used for functions in MeTA repository for ease of portability. Write automated tests for the functionality developed. Continuous development of documentation. Create demo presentation. Deliverables: The deliverables for this project are the source code implementing functionality for latent aspect rating analysis, an automated test suite to demonstrate the functionality, usage documentation, and any other project requirements listed in the project overview. Resources to be Used: Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903 meta-toolkit. https://github.com/meta-toolkit/meta Constraint-freeImplementationofLRRHongningWangDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbanaIL,61801USAwang296@illinois.edu1OverviewInthisconstraint-freeimplementationoflatentratingregression(LRR)[Wangetal.,2010],auxiliaryvariablesareintroducedtogetridofconstraintsintheoriginalLRRmodel,whereaspectweights shouldbepositiveandsumuptoone.ThenewimplementationperformscloselyastheoriginalLRRmodelandcanbeusedasareplacementofit.2LatentRatingRegressionModelIntheworkofLatentAspectRatingAnalysis(LARA)[Wangetal.,2010],weassumeineachreview:1)theoverallratingistheweightedsumoftheindividualaspectratings;2)theaspectratingscanbepredictedbythewordsassociatedwitheachaspect.Intuitively,wecanformalizetheseassumptionsasfollows:p(td|;S;;)=p( d|;S)p(rd| TdSd;)d dwhererdistheoverallratingforreviewd,Sd={Sd1;Sd2;:::;Sdk}arethepredictedratingsforeachaspectwhereSdi= Tiwi,and disthecorrespondinginferredaspectweight;isthestandarddeviationofoverallratingprediction.IntheoriginalLRRmodel,werequirei; di>=0andki=1 di=1.Theconstrainton dgreatlyincreasesthecomputationalcomplexity.Toavoidsolvingaconstraintoptimizationproblem,weintroduceasetofauxiliaryvariables{^ d1;^ d2;:::;^ dk}foreachreviewd,andset di=exp(^ di)kj=1exp(^ dj):(1)Furthermore,wewillassume^ diisdrawnfromN(;S)ratherthan diasintheoriginalLRRmodel.SimilartrickcanbeappliedontheaspectratingSditoavoidnegativepredictedratings:Sdi=exp( Tiwi):(2)1HONGNINGWANGIMPLEMENTATIONOFLRR3EMUpdatingFormulasThecomplete-datalog-likelihoodfunctionforthenewlyderivedproblemisverysimilarasthatintheoriginalLRRmodel:L(rd;^ d;Sd;;S;2; )=-log2-( TdSd-rd)22-logS-(^ d-)TS CourseProject ATTENTION: The presentation slides have a voice narration added to it, so make sure to open in powerpoint and click start presentation! The document ""CODE DOCUMENTATION.docx"" has the documentation to go with this project. The document ""manual.pdf"" is the original document provided with the original Java source code from the previous implementation of this LRR model."
https://github.com/zen030/CourseProject	"Project Progress Report Project Topic: BERT Sentiment Analysis to Detect Twitter Sarcasm Project Team Member Name: Zainal Hakim NetID: zainalh2 Result (answer.txt) Progress status: 100% (Completed) The project scores (beating the baseline): F1-Score Recall Precision Project Result 0.757905138339921 0.8522222222222222 0.6823843416370107 Baseline 0.723 0.723 0.723 Software Code Progress status: 100% (Completed) Source code files: Training and Evaluation (link) Evaluation (for DEMO) (link) Documentation Progress status: 100% (Completed) The document files: Proposal (link) Project Documentation (link) Demo Video Progress status: 100% (Completed) Demo video URL: https://www.youtube.com/watch?v=PsYn2lUWpQg Challenges in The Project To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project. Project Progress Report Project Topic: BERT Sentiment Analysis to Detect Twitter Sarcasm Project Team Member * Name: Zainal Hakim * NetID: zainalh2 Result (answer.txt) * Progress status: 100% (Completed) * The project scores (beating the baseline): F1-Score Recall Precision Project Result 0.757905138339921 0.8522222222222222 0.6823843416370107 Baseline 0.723 0.723 0.723 Software Code * Progress status: 100% (Completed) * Source code files: o Training and Evaluation (link) o Evaluation (for DEMO) (link) Documentation * Progress status: 100% (Completed) * The document files: o Proposal (link) o Project Documentation (link) Demo Video * Progress status: 100% (Completed) * Demo video URL: https://www.youtube.com/watch?v=PsYn2lUWpQg Challenges in The Project * To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. * It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. * Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project. BERT Sentiment Analysis to Detect Twitter Sarcasm (Naive Approach) Zainal Hakim zainalh2@illinois.edu Table of content A. Introduction 3 B. Bidirectional Encoder Representations from Transformers (BERT) 3 C. Dataset Description 4 D. The Naive Approach 4 E. The Model, Training, and Evaluation 5 F. The Software Code 6 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review 6 1. Colab Configuration 6 2. Mounting Google Drive to Colab session (To save result files) 9 3. The Main Python Class 10 4. Training and Evaluation experiments 13 5. Save the result files to Google Drive 14 F.2. DEMO_Model_Evaluation.ipynb Code Review 15 1. The first 3-steps are already explained in detail in the previous section 15 2. Preparing the encoded testing dataset and data loader 15 3. Run the evaluation batch iteration 16 4. Generate the 'anwer.txt' file 17 5. Post 'answer.txt' to LiveDataLab for scoring 17 G. Result and Conclusion 18 Reference 18 Appendix 19 1. answer.txt 19 Introduction Sarcasm is a form of figurative language that implies a negative sentiment while displaying a positive sentiment on the surface (Joshi et al., 2017). I present a Naive approach to detect Twitter tweet sarcasm sentiment using a transformers-based pre-trained model that considers only the response tweet. This approach completely ignores the context of the response tweet to train the model. The model uses a transformer encoder to generate the embedding representation for the response. The model is trained and evaluated on the given training and testing datasets. My best performance model gives an F1-score of 75.79%, beating the Classification Competition baseline score after four epoch iterations (epoch # 4). ""C. Dataset Description"" section of this document explains further the response and context relationship. Important files in the project: Documented software code Training and evaluation (link) Evaluation of a trained-model for demo purpose (link) Best performance testing set predictions (answer.txt) (link) Best performance trained-model (link) Training dataset (link) Testing dataset (link) Bidirectional Encoder Representations from Transformers (BERT) This project uses BERT, a transformer-based technique for Natural Language Processing pre-training developed by a team in Google. The original English language BERT model comes with two pre-trained model types: Model Type Layer Hidden Head Parameter Corpus Word Base 12 768 12 110 M 800 M Large 24 1024 16 340 M 2.500 M Table 1: BERT original model types BERT Large model essentially has better computing leverage than the base model. Google team trained the large model using a larger corpus word size than the base model. The large model is expected to perform better than the base model in most of the NLP tasks such as sentiment analysis. Original BERT paper is available here (link). Dataset Description There are two Twitter tweet datasets available for this project: Training dataset: a labeled dataset to train the model Testing dataset: tweet with a unique ID to evaluate the trained-model For the training dataset, each line contains a JSON object with the following columns: label: SARCASM or NOT_SARCASM response: the classified tweet context: the conversation context of the response For the testing/evaluation dataset, each line contains a JSON object with the following columns: id: unique identifier for the sample response: the tweet to be classified context: the conversation context of the response Training Dataset Testing Dataset 5000 lines 1800 lines Table 2: Dataset size statistics A more detailed dataset description is available in the project competition Github repository (link). The Naive Approach I hypothesize the context does not always support the sentiment of a response. Context can have an opposing effect on the sentiment of a response. I hypothesize there are 2 types of context: A Positive context is a context that supports the sentiment of a response. A Negative context is a context that does not support the sentiment of a response. Sentiment Negative Context Sentiment Negative Context Fig.1: Illustration of context reduces sentiment quality Positive Context S e n t i m e n t Positive Context S e n t i m e n t Fig.2: Illustration of context increase sentiment quality It is critical to utilize the context to support the response's sentiment. For this project, I consider only the sentiment-labeled response to training the model, and I completely ignore the context. I call this a Naive approach. In the future project, I can use advanced machine learning techniques to utilize response and context to train the model by selectively reconstruct the context to support the sentiment of a response. The Model, Training, and Evaluation In this project, I use the datasets to train and evaluate BERT Large uncased and base uncased models. I use the original BERT paper as a reference (A.3 Fine-tuning Procedure) to choose hyperparameters for my experiments. The hyperparameters in my experiments are: Learning rate: 2e-5 Batch size: 5 (considering memory size) Epochs: 4 iterations Epsilon: 1e-8 Random seed value: 17 BERT model can handle a text with a maximum of 512 characters. If the input text is more than 512 characters, the model truncates the text to 512 characters. Response text in training and testing datasets is less than 512 characters, in this case, we are guaranteed to consider all words in the response text to train and evaluate the model. The Python source code below prints the response maximum characters for training and testing datasets. Fig.3: Source code to check maximum training and evaluation response characters length Response Max. Chars Training Dataset 315 Testing Dataset 310 Table 3: Maximum response characters length The Software Code For the software code of this project, I implemented two Google Colab Notebooks: NAIVE_BERT_sentiment_analysis.ipynb: Training and evaluation notebook (link) DEMO_Model_Evaluation.ipynb: Evaluation of selected trained-model notebook (for DEMO purpose) (link) I use the Google Colab PRO environment to implement and test the software code. Introduction about Google Colab is available here link. Software code uses the following main Python libraries: Numpy 1.18.5 Pytorch 1.7.0+cu101 Huggingface Transformers 3.5.0 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review This notebook trains and evaluates the BERT Large uncased and base uncased models using the provided datasets. In the end, the trained-model and evaluation results are copied and stored in the project Google Drive folder. Colab Configuration Install Python modules required for the notebook. Copy train.jsonl and test.jsonl files from Google Drive to Colab session I have already copied train.jsonl and test.jsonl files to a Google Drive account created for this project. The files are shared with the public. The following code will copy the files from Google Drive to the Colab session. The source code above will prompt a URL. Click the URL, it will prompt the Google account login page. Select the Google account to run the notebook: Click the ""Allow"" button to allow Google Cloud SDK to access the Google account. That finally prompts the verification code. Copy the code and paste it in the ""Enter verification code"" text box. Press ""Enter"" That will copy the training and testing datasets to the Colab session! We are ready to train and evaluate the model using the datasets. Mounting Google Drive to Colab session (To save result files) To save the result files (the trained-model and answer.txt) to Google Drive, we need to mount Google Drive to the Colab session. In this project, I mount the Google Drive directory to the './content/uiuc' folder. At the end of this notebook code execution, the Google Drive mounted folder in the Colab session will look like the following: Mounted Google Drive in Colab Session where to keep result files permanently The Main Python Class This project implements a Python class, BERT_Model, that handles the following tasks: Read the dataset from JSONL files into a list of JSON Convert list of JSON to Pandas DataFrame Create the BERT Model Run the training and save the model for each epoch Evaluate the model and store the result into a file Below are the class signatures: For details please check the source code here link. The source code comment describes what each step does. The main algorithm of the training: Create a BERT tokenizer. We use the tokenizer to encode the text and prepare the dataset. Create a BERT model using the chosen hyperparameter. Create AdamW optimizer (link), it is used to calculate the convergence in the model. It is considered the fastest convergence algorithm. Create a data loader. The data loader main task is passing dataset batch to model. It will make sure all the text in the dataset is processed by the model. Set the model to train mode. Iterate the data loader to pass the dataset batch to the model until the training is completed. After the training is completed, save the trained model to a file. The main algorithm of testing: Load the model file to the memory and set the model to evaluation mode. Create the tokenizer, encode the input data for testing, and prepare the dataset. Create the data loader to handle the evaluation batch. Run the evaluation. One of the most important outputs from the model is logits. In this project, logits is an array of two elements. The model is configured to have the first element of logits represents SARCASM, and the second element represents NOT_SARCASM. We use the NumPy argmax function to return the index of the maximum value in logits. If the maximum value is the first element, the function returns the index array of 0 (SARCASM) If the maximum value is the second element, the function returns the index array of 1 (NOT_SARCASM) The final step is to write the output to a file (answer.txt) Training and Evaluation experiments To test my Naive hypothesis, I run experiments with the same hyperparameters on two BERT models. Experiment-1: BERT base uncased Experiment-2: BERT LARGE uncased Both experiments code above will generate result files in the Colab session folder below: Save the result files to Google Drive Colab deletes result files when the session ends. We need to store the files permanently in other locations, in this project I use Google Drive. The code below will copy the result files to the project Google Drive folder (the Google Drive folder has been mounted in the earlier step). In this project, I save the epoch # 4 model file only. We use the files stored in the project Google Drive folder to run a project demo. The next section will illustrate how to use the trained-model file to evaluate the testing dataset. F.2. DEMO_Model_Evaluation.ipynb Code Review In the demo notebook, I demonstrate how to generate 'answer.txt' from the BERT Large uncased trained model stored in the project Google Drive folder. The trained-model is available to the public here link. With this model, we will reproduce the evaluation result which is available to the public here link. The demo video is available here link. The main algorithm in the notebook demo: The first 3-steps are already explained in detail in the previous section Colab configuration (Python modules import and installation) Copy the trained-model and testing dataset files from Google Drive to the Colab session. We need to authorize Colab to access the Google account described in the previous section. Prepare Panda DataFrame for the testing dataset. Preparing the encoded testing dataset and data loader The main steps: Create the BERT tokenizer to encode the testing dataset. Create the data loader to run the evaluation in batches. Preparing the Python objects for the evaluation Run the evaluation batch iteration The main steps: Set the device to GPU, if applicable. Load the trained model from file to memory. Set the model to evaluation mode. Data loader iteration to pass text to evaluate in batches to the model. The model output logits (how to process logits is described in the previous section) Generate the 'anwer.txt' file Generate the sentiment output to the screen and 'answer.txt' file. The previous section describes the source code snippet. Post 'answer.txt' to LiveDataLab for scoring For F1, precision, and recall scores evaluation, I post the 'answer.txt' to LiveDataLab. Leaderboard snapshot on 03-Nov-2020 Result and Conclusion In summary, the testing (evaluation) results from BERT Large and base models: Model F1-Score Recall Precision BERT Large uncased 0.757905138339921 0.8522222222222222 0.6823843416370107 BERT Base uncased 0.7458777885548012 0.8544444444444445 0.6617900172117039 Surprisingly, the base model performs almost as good as the large model. In this project, I did try to use different trained model such as RoBERTa and XLNet (and different hyperparameters), but I could not produce a result higher than BERT Large uncased score. The project scope as proposed in the project proposal is to explore BERT hence, I am reporting the result for BERT models only. In the future, I would like to explore more on the following topics: To use advanced machine learning techniques to explore other hyperparameters for BERT models. To utilize both the context and response to training the models. I hypothesize that the context can be used as an additional dataset to train the model. To explore another model, such as RoBERTa and XLNet. Reference Aditya Joshi, Pushpak Bhattacharyya, and Mark J. Car- man. 2017. Automatic Sarcasm Detection: A Survey. ACM Computing Surveys, 50(5):1-22. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Appendix answer.txt twitter_1,NOT_SARCASM twitter_2,SARCASM twitter_3,SARCASM twitter_4,NOT_SARCASM twitter_5,SARCASM twitter_6,SARCASM twitter_7,NOT_SARCASM twitter_8,SARCASM twitter_9,NOT_SARCASM twitter_10,SARCASM twitter_11,NOT_SARCASM twitter_12,SARCASM twitter_13,SARCASM twitter_14,NOT_SARCASM twitter_15,SARCASM twitter_16,SARCASM twitter_17,SARCASM twitter_18,SARCASM twitter_19,SARCASM twitter_20,NOT_SARCASM twitter_21,NOT_SARCASM twitter_22,SARCASM twitter_23,NOT_SARCASM twitter_24,SARCASM twitter_25,SARCASM twitter_26,SARCASM twitter_27,NOT_SARCASM twitter_28,NOT_SARCASM twitter_29,SARCASM twitter_30,NOT_SARCASM twitter_31,SARCASM twitter_32,NOT_SARCASM twitter_33,NOT_SARCASM twitter_34,SARCASM twitter_35,NOT_SARCASM twitter_36,SARCASM twitter_37,SARCASM twitter_38,SARCASM twitter_39,SARCASM twitter_40,SARCASM twitter_41,SARCASM twitter_42,NOT_SARCASM twitter_43,SARCASM twitter_44,NOT_SARCASM twitter_45,SARCASM twitter_46,NOT_SARCASM twitter_47,SARCASM twitter_48,SARCASM twitter_49,NOT_SARCASM twitter_50,SARCASM twitter_51,NOT_SARCASM twitter_52,NOT_SARCASM twitter_53,SARCASM twitter_54,SARCASM twitter_55,SARCASM twitter_56,SARCASM twitter_57,NOT_SARCASM twitter_58,NOT_SARCASM twitter_59,SARCASM twitter_60,SARCASM twitter_61,NOT_SARCASM twitter_62,SARCASM twitter_63,SARCASM twitter_64,SARCASM twitter_65,SARCASM twitter_66,NOT_SARCASM twitter_67,SARCASM twitter_68,NOT_SARCASM twitter_69,SARCASM twitter_70,SARCASM twitter_71,NOT_SARCASM twitter_72,SARCASM twitter_73,SARCASM twitter_74,SARCASM twitter_75,NOT_SARCASM twitter_76,NOT_SARCASM twitter_77,SARCASM twitter_78,SARCASM twitter_79,NOT_SARCASM twitter_80,SARCASM twitter_81,NOT_SARCASM twitter_82,NOT_SARCASM twitter_83,SARCASM twitter_84,NOT_SARCASM twitter_85,SARCASM twitter_86,SARCASM twitter_87,NOT_SARCASM twitter_88,SARCASM twitter_89,SARCASM twitter_90,NOT_SARCASM twitter_91,NOT_SARCASM twitter_92,SARCASM twitter_93,SARCASM twitter_94,SARCASM twitter_95,SARCASM twitter_96,SARCASM twitter_97,SARCASM twitter_98,NOT_SARCASM twitter_99,NOT_SARCASM twitter_100,NOT_SARCASM twitter_101,SARCASM twitter_102,SARCASM twitter_103,NOT_SARCASM twitter_104,NOT_SARCASM twitter_105,SARCASM twitter_106,SARCASM twitter_107,NOT_SARCASM twitter_108,NOT_SARCASM twitter_109,SARCASM twitter_110,SARCASM twitter_111,SARCASM twitter_112,SARCASM twitter_113,SARCASM twitter_114,SARCASM twitter_115,SARCASM twitter_116,NOT_SARCASM twitter_117,NOT_SARCASM twitter_118,SARCASM twitter_119,NOT_SARCASM twitter_120,NOT_SARCASM twitter_121,SARCASM twitter_122,SARCASM twitter_123,NOT_SARCASM twitter_124,SARCASM twitter_125,NOT_SARCASM twitter_126,NOT_SARCASM twitter_127,SARCASM twitter_128,NOT_SARCASM twitter_129,NOT_SARCASM twitter_130,SARCASM twitter_131,NOT_SARCASM twitter_132,SARCASM twitter_133,NOT_SARCASM twitter_134,NOT_SARCASM twitter_135,SARCASM twitter_136,NOT_SARCASM twitter_137,SARCASM twitter_138,NOT_SARCASM twitter_139,NOT_SARCASM twitter_140,SARCASM twitter_141,SARCASM twitter_142,SARCASM twitter_143,SARCASM twitter_144,SARCASM twitter_145,SARCASM twitter_146,SARCASM twitter_147,SARCASM twitter_148,SARCASM twitter_149,SARCASM twitter_150,SARCASM twitter_151,SARCASM twitter_152,NOT_SARCASM twitter_153,NOT_SARCASM twitter_154,SARCASM twitter_155,SARCASM twitter_156,NOT_SARCASM twitter_157,SARCASM twitter_158,SARCASM twitter_159,SARCASM twitter_160,SARCASM twitter_161,NOT_SARCASM twitter_162,SARCASM twitter_163,SARCASM twitter_164,SARCASM twitter_165,SARCASM twitter_166,SARCASM twitter_167,SARCASM twitter_168,SARCASM twitter_169,NOT_SARCASM twitter_170,NOT_SARCASM twitter_171,NOT_SARCASM twitter_172,NOT_SARCASM twitter_173,SARCASM twitter_174,SARCASM twitter_175,SARCASM twitter_176,SARCASM twitter_177,SARCASM twitter_178,NOT_SARCASM twitter_179,SARCASM twitter_180,SARCASM twitter_181,SARCASM twitter_182,NOT_SARCASM twitter_183,SARCASM twitter_184,SARCASM twitter_185,NOT_SARCASM twitter_186,SARCASM twitter_187,SARCASM twitter_188,SARCASM twitter_189,NOT_SARCASM twitter_190,NOT_SARCASM twitter_191,NOT_SARCASM twitter_192,NOT_SARCASM twitter_193,SARCASM twitter_194,NOT_SARCASM twitter_195,NOT_SARCASM twitter_196,SARCASM twitter_197,NOT_SARCASM twitter_198,SARCASM twitter_199,SARCASM twitter_200,SARCASM twitter_201,SARCASM twitter_202,SARCASM twitter_203,NOT_SARCASM twitter_204,NOT_SARCASM twitter_205,SARCASM twitter_206,NOT_SARCASM twitter_207,SARCASM twitter_208,SARCASM twitter_209,SARCASM twitter_210,SARCASM twitter_211,SARCASM twitter_212,NOT_SARCASM twitter_213,NOT_SARCASM twitter_214,NOT_SARCASM twitter_215,NOT_SARCASM twitter_216,SARCASM twitter_217,SARCASM twitter_218,SARCASM twitter_219,SARCASM twitter_220,SARCASM twitter_221,SARCASM twitter_222,NOT_SARCASM twitter_223,NOT_SARCASM twitter_224,SARCASM twitter_225,NOT_SARCASM twitter_226,SARCASM twitter_227,NOT_SARCASM twitter_228,NOT_SARCASM twitter_229,SARCASM twitter_230,NOT_SARCASM twitter_231,SARCASM twitter_232,SARCASM twitter_233,SARCASM twitter_234,NOT_SARCASM twitter_235,SARCASM twitter_236,NOT_SARCASM twitter_237,SARCASM twitter_238,NOT_SARCASM twitter_239,SARCASM twitter_240,SARCASM twitter_241,SARCASM twitter_242,SARCASM twitter_243,NOT_SARCASM twitter_244,NOT_SARCASM twitter_245,NOT_SARCASM twitter_246,NOT_SARCASM twitter_247,SARCASM twitter_248,SARCASM twitter_249,SARCASM twitter_250,SARCASM twitter_251,SARCASM twitter_252,SARCASM twitter_253,NOT_SARCASM twitter_254,NOT_SARCASM twitter_255,SARCASM twitter_256,NOT_SARCASM twitter_257,NOT_SARCASM twitter_258,SARCASM twitter_259,SARCASM twitter_260,SARCASM twitter_261,NOT_SARCASM twitter_262,SARCASM twitter_263,NOT_SARCASM twitter_264,SARCASM twitter_265,SARCASM twitter_266,SARCASM twitter_267,SARCASM twitter_268,NOT_SARCASM twitter_269,SARCASM twitter_270,NOT_SARCASM twitter_271,NOT_SARCASM twitter_272,NOT_SARCASM twitter_273,NOT_SARCASM twitter_274,SARCASM twitter_275,SARCASM twitter_276,NOT_SARCASM twitter_277,NOT_SARCASM twitter_278,SARCASM twitter_279,SARCASM twitter_280,NOT_SARCASM twitter_281,SARCASM twitter_282,NOT_SARCASM twitter_283,SARCASM twitter_284,SARCASM twitter_285,NOT_SARCASM twitter_286,NOT_SARCASM twitter_287,NOT_SARCASM twitter_288,NOT_SARCASM twitter_289,NOT_SARCASM twitter_290,SARCASM twitter_291,SARCASM twitter_292,SARCASM twitter_293,SARCASM twitter_294,SARCASM twitter_295,NOT_SARCASM twitter_296,SARCASM twitter_297,SARCASM twitter_298,SARCASM twitter_299,SARCASM twitter_300,NOT_SARCASM twitter_301,SARCASM twitter_302,NOT_SARCASM twitter_303,SARCASM twitter_304,SARCASM twitter_305,NOT_SARCASM twitter_306,SARCASM twitter_307,NOT_SARCASM twitter_308,SARCASM twitter_309,SARCASM twitter_310,NOT_SARCASM twitter_311,SARCASM twitter_312,NOT_SARCASM twitter_313,SARCASM twitter_314,NOT_SARCASM twitter_315,SARCASM twitter_316,SARCASM twitter_317,NOT_SARCASM twitter_318,NOT_SARCASM twitter_319,SARCASM twitter_320,SARCASM twitter_321,NOT_SARCASM twitter_322,NOT_SARCASM twitter_323,NOT_SARCASM twitter_324,SARCASM twitter_325,NOT_SARCASM twitter_326,NOT_SARCASM twitter_327,SARCASM twitter_328,NOT_SARCASM twitter_329,SARCASM twitter_330,SARCASM twitter_331,SARCASM twitter_332,SARCASM twitter_333,SARCASM twitter_334,NOT_SARCASM twitter_335,NOT_SARCASM twitter_336,SARCASM twitter_337,NOT_SARCASM twitter_338,SARCASM twitter_339,NOT_SARCASM twitter_340,SARCASM twitter_341,NOT_SARCASM twitter_342,SARCASM twitter_343,SARCASM twitter_344,NOT_SARCASM twitter_345,SARCASM twitter_346,NOT_SARCASM twitter_347,SARCASM twitter_348,SARCASM twitter_349,SARCASM twitter_350,SARCASM twitter_351,SARCASM twitter_352,SARCASM twitter_353,NOT_SARCASM twitter_354,SARCASM twitter_355,SARCASM twitter_356,SARCASM twitter_357,SARCASM twitter_358,NOT_SARCASM twitter_359,SARCASM twitter_360,NOT_SARCASM twitter_361,SARCASM twitter_362,SARCASM twitter_363,SARCASM twitter_364,SARCASM twitter_365,NOT_SARCASM twitter_366,SARCASM twitter_367,SARCASM twitter_368,SARCASM twitter_369,NOT_SARCASM twitter_370,NOT_SARCASM twitter_371,SARCASM twitter_372,SARCASM twitter_373,SARCASM twitter_374,NOT_SARCASM twitter_375,SARCASM twitter_376,NOT_SARCASM twitter_377,SARCASM twitter_378,NOT_SARCASM twitter_379,NOT_SARCASM twitter_380,SARCASM twitter_381,SARCASM twitter_382,NOT_SARCASM twitter_383,SARCASM twitter_384,NOT_SARCASM twitter_385,NOT_SARCASM twitter_386,SARCASM twitter_387,SARCASM twitter_388,SARCASM twitter_389,NOT_SARCASM twitter_390,NOT_SARCASM twitter_391,SARCASM twitter_392,SARCASM twitter_393,NOT_SARCASM twitter_394,SARCASM twitter_395,SARCASM twitter_396,SARCASM twitter_397,NOT_SARCASM twitter_398,NOT_SARCASM twitter_399,SARCASM twitter_400,SARCASM twitter_401,SARCASM twitter_402,NOT_SARCASM twitter_403,SARCASM twitter_404,NOT_SARCASM twitter_405,NOT_SARCASM twitter_406,SARCASM twitter_407,SARCASM twitter_408,SARCASM twitter_409,SARCASM twitter_410,SARCASM twitter_411,SARCASM twitter_412,NOT_SARCASM twitter_413,SARCASM twitter_414,SARCASM twitter_415,SARCASM twitter_416,SARCASM twitter_417,SARCASM twitter_418,SARCASM twitter_419,NOT_SARCASM twitter_420,NOT_SARCASM twitter_421,SARCASM twitter_422,NOT_SARCASM twitter_423,SARCASM twitter_424,SARCASM twitter_425,SARCASM twitter_426,NOT_SARCASM twitter_427,SARCASM twitter_428,SARCASM twitter_429,NOT_SARCASM twitter_430,SARCASM twitter_431,SARCASM twitter_432,SARCASM twitter_433,SARCASM twitter_434,SARCASM twitter_435,NOT_SARCASM twitter_436,SARCASM twitter_437,SARCASM twitter_438,NOT_SARCASM twitter_439,NOT_SARCASM twitter_440,SARCASM twitter_441,SARCASM twitter_442,SARCASM twitter_443,SARCASM twitter_444,SARCASM twitter_445,SARCASM twitter_446,SARCASM twitter_447,SARCASM twitter_448,SARCASM twitter_449,SARCASM twitter_450,NOT_SARCASM twitter_451,NOT_SARCASM twitter_452,NOT_SARCASM twitter_453,SARCASM twitter_454,SARCASM twitter_455,NOT_SARCASM twitter_456,NOT_SARCASM twitter_457,SARCASM twitter_458,SARCASM twitter_459,SARCASM twitter_460,SARCASM twitter_461,SARCASM twitter_462,SARCASM twitter_463,SARCASM twitter_464,SARCASM twitter_465,NOT_SARCASM twitter_466,SARCASM twitter_467,NOT_SARCASM twitter_468,SARCASM twitter_469,SARCASM twitter_470,SARCASM twitter_471,SARCASM twitter_472,SARCASM twitter_473,NOT_SARCASM twitter_474,SARCASM twitter_475,NOT_SARCASM twitter_476,SARCASM twitter_477,NOT_SARCASM twitter_478,SARCASM twitter_479,SARCASM twitter_480,SARCASM twitter_481,NOT_SARCASM twitter_482,NOT_SARCASM twitter_483,NOT_SARCASM twitter_484,SARCASM twitter_485,NOT_SARCASM twitter_486,SARCASM twitter_487,NOT_SARCASM twitter_488,SARCASM twitter_489,SARCASM twitter_490,NOT_SARCASM twitter_491,NOT_SARCASM twitter_492,NOT_SARCASM twitter_493,SARCASM twitter_494,SARCASM twitter_495,SARCASM twitter_496,SARCASM twitter_497,NOT_SARCASM twitter_498,SARCASM twitter_499,SARCASM twitter_500,SARCASM twitter_501,SARCASM twitter_502,SARCASM twitter_503,SARCASM twitter_504,SARCASM twitter_505,SARCASM twitter_506,SARCASM twitter_507,SARCASM twitter_508,NOT_SARCASM twitter_509,NOT_SARCASM twitter_510,SARCASM twitter_511,NOT_SARCASM twitter_512,NOT_SARCASM twitter_513,SARCASM twitter_514,SARCASM twitter_515,NOT_SARCASM twitter_516,NOT_SARCASM twitter_517,SARCASM twitter_518,SARCASM twitter_519,NOT_SARCASM twitter_520,SARCASM twitter_521,SARCASM twitter_522,SARCASM twitter_523,NOT_SARCASM twitter_524,SARCASM twitter_525,SARCASM twitter_526,SARCASM twitter_527,SARCASM twitter_528,SARCASM twitter_529,NOT_SARCASM twitter_530,NOT_SARCASM twitter_531,NOT_SARCASM twitter_532,SARCASM twitter_533,NOT_SARCASM twitter_534,SARCASM twitter_535,SARCASM twitter_536,SARCASM twitter_537,NOT_SARCASM twitter_538,NOT_SARCASM twitter_539,SARCASM twitter_540,SARCASM twitter_541,SARCASM twitter_542,SARCASM twitter_543,NOT_SARCASM twitter_544,NOT_SARCASM twitter_545,SARCASM twitter_546,SARCASM twitter_547,SARCASM twitter_548,NOT_SARCASM twitter_549,NOT_SARCASM twitter_550,SARCASM twitter_551,SARCASM twitter_552,NOT_SARCASM twitter_553,SARCASM twitter_554,SARCASM twitter_555,NOT_SARCASM twitter_556,SARCASM twitter_557,SARCASM twitter_558,SARCASM twitter_559,SARCASM twitter_560,SARCASM twitter_561,NOT_SARCASM twitter_562,SARCASM twitter_563,SARCASM twitter_564,SARCASM twitter_565,SARCASM twitter_566,SARCASM twitter_567,SARCASM twitter_568,SARCASM twitter_569,NOT_SARCASM twitter_570,SARCASM twitter_571,SARCASM twitter_572,SARCASM twitter_573,SARCASM twitter_574,SARCASM twitter_575,NOT_SARCASM twitter_576,SARCASM twitter_577,SARCASM twitter_578,NOT_SARCASM twitter_579,NOT_SARCASM twitter_580,NOT_SARCASM twitter_581,NOT_SARCASM twitter_582,NOT_SARCASM twitter_583,SARCASM twitter_584,SARCASM twitter_585,SARCASM twitter_586,SARCASM twitter_587,SARCASM twitter_588,SARCASM twitter_589,NOT_SARCASM twitter_590,NOT_SARCASM twitter_591,SARCASM twitter_592,SARCASM twitter_593,NOT_SARCASM twitter_594,SARCASM twitter_595,SARCASM twitter_596,NOT_SARCASM twitter_597,NOT_SARCASM twitter_598,SARCASM twitter_599,NOT_SARCASM twitter_600,SARCASM twitter_601,NOT_SARCASM twitter_602,SARCASM twitter_603,NOT_SARCASM twitter_604,SARCASM twitter_605,NOT_SARCASM twitter_606,NOT_SARCASM twitter_607,SARCASM twitter_608,SARCASM twitter_609,NOT_SARCASM twitter_610,NOT_SARCASM twitter_611,NOT_SARCASM twitter_612,SARCASM twitter_613,SARCASM twitter_614,NOT_SARCASM twitter_615,NOT_SARCASM twitter_616,SARCASM twitter_617,SARCASM twitter_618,NOT_SARCASM twitter_619,SARCASM twitter_620,NOT_SARCASM twitter_621,SARCASM twitter_622,NOT_SARCASM twitter_623,SARCASM twitter_624,NOT_SARCASM twitter_625,SARCASM twitter_626,SARCASM twitter_627,NOT_SARCASM twitter_628,NOT_SARCASM twitter_629,SARCASM twitter_630,SARCASM twitter_631,NOT_SARCASM twitter_632,SARCASM twitter_633,NOT_SARCASM twitter_634,SARCASM twitter_635,SARCASM twitter_636,NOT_SARCASM twitter_637,SARCASM twitter_638,NOT_SARCASM twitter_639,SARCASM twitter_640,NOT_SARCASM twitter_641,NOT_SARCASM twitter_642,SARCASM twitter_643,SARCASM twitter_644,SARCASM twitter_645,NOT_SARCASM twitter_646,NOT_SARCASM twitter_647,NOT_SARCASM twitter_648,NOT_SARCASM twitter_649,SARCASM twitter_650,NOT_SARCASM twitter_651,SARCASM twitter_652,NOT_SARCASM twitter_653,NOT_SARCASM twitter_654,SARCASM twitter_655,NOT_SARCASM twitter_656,NOT_SARCASM twitter_657,SARCASM twitter_658,NOT_SARCASM twitter_659,SARCASM twitter_660,SARCASM twitter_661,SARCASM twitter_662,SARCASM twitter_663,NOT_SARCASM twitter_664,SARCASM twitter_665,NOT_SARCASM twitter_666,NOT_SARCASM twitter_667,SARCASM twitter_668,SARCASM twitter_669,SARCASM twitter_670,NOT_SARCASM twitter_671,SARCASM twitter_672,SARCASM twitter_673,SARCASM twitter_674,SARCASM twitter_675,SARCASM twitter_676,NOT_SARCASM twitter_677,NOT_SARCASM twitter_678,SARCASM twitter_679,SARCASM twitter_680,NOT_SARCASM twitter_681,NOT_SARCASM twitter_682,SARCASM twitter_683,NOT_SARCASM twitter_684,NOT_SARCASM twitter_685,SARCASM twitter_686,SARCASM twitter_687,NOT_SARCASM twitter_688,SARCASM twitter_689,SARCASM twitter_690,SARCASM twitter_691,NOT_SARCASM twitter_692,SARCASM twitter_693,SARCASM twitter_694,NOT_SARCASM twitter_695,SARCASM twitter_696,NOT_SARCASM twitter_697,SARCASM twitter_698,NOT_SARCASM twitter_699,NOT_SARCASM twitter_700,NOT_SARCASM twitter_701,SARCASM twitter_702,NOT_SARCASM twitter_703,SARCASM twitter_704,SARCASM twitter_705,NOT_SARCASM twitter_706,SARCASM twitter_707,NOT_SARCASM twitter_708,SARCASM twitter_709,NOT_SARCASM twitter_710,SARCASM twitter_711,SARCASM twitter_712,SARCASM twitter_713,SARCASM twitter_714,SARCASM twitter_715,SARCASM twitter_716,SARCASM twitter_717,SARCASM twitter_718,SARCASM twitter_719,NOT_SARCASM twitter_720,NOT_SARCASM twitter_721,SARCASM twitter_722,NOT_SARCASM twitter_723,SARCASM twitter_724,SARCASM twitter_725,SARCASM twitter_726,NOT_SARCASM twitter_727,NOT_SARCASM twitter_728,SARCASM twitter_729,SARCASM twitter_730,SARCASM twitter_731,NOT_SARCASM twitter_732,SARCASM twitter_733,NOT_SARCASM twitter_734,NOT_SARCASM twitter_735,SARCASM twitter_736,SARCASM twitter_737,SARCASM twitter_738,SARCASM twitter_739,SARCASM twitter_740,SARCASM twitter_741,SARCASM twitter_742,SARCASM twitter_743,SARCASM twitter_744,NOT_SARCASM twitter_745,NOT_SARCASM twitter_746,SARCASM twitter_747,SARCASM twitter_748,NOT_SARCASM twitter_749,NOT_SARCASM twitter_750,NOT_SARCASM twitter_751,SARCASM twitter_752,NOT_SARCASM twitter_753,SARCASM twitter_754,SARCASM twitter_755,NOT_SARCASM twitter_756,SARCASM twitter_757,NOT_SARCASM twitter_758,NOT_SARCASM twitter_759,SARCASM twitter_760,SARCASM twitter_761,NOT_SARCASM twitter_762,SARCASM twitter_763,NOT_SARCASM twitter_764,SARCASM twitter_765,SARCASM twitter_766,SARCASM twitter_767,SARCASM twitter_768,NOT_SARCASM twitter_769,NOT_SARCASM twitter_770,NOT_SARCASM twitter_771,SARCASM twitter_772,NOT_SARCASM twitter_773,SARCASM twitter_774,NOT_SARCASM twitter_775,SARCASM twitter_776,SARCASM twitter_777,NOT_SARCASM twitter_778,SARCASM twitter_779,NOT_SARCASM twitter_780,SARCASM twitter_781,NOT_SARCASM twitter_782,NOT_SARCASM twitter_783,SARCASM twitter_784,SARCASM twitter_785,NOT_SARCASM twitter_786,SARCASM twitter_787,SARCASM twitter_788,NOT_SARCASM twitter_789,SARCASM twitter_790,SARCASM twitter_791,NOT_SARCASM twitter_792,NOT_SARCASM twitter_793,SARCASM twitter_794,NOT_SARCASM twitter_795,SARCASM twitter_796,SARCASM twitter_797,SARCASM twitter_798,SARCASM twitter_799,SARCASM twitter_800,NOT_SARCASM twitter_801,NOT_SARCASM twitter_802,NOT_SARCASM twitter_803,SARCASM twitter_804,NOT_SARCASM twitter_805,SARCASM twitter_806,SARCASM twitter_807,NOT_SARCASM twitter_808,SARCASM twitter_809,SARCASM twitter_810,NOT_SARCASM twitter_811,SARCASM twitter_812,SARCASM twitter_813,SARCASM twitter_814,SARCASM twitter_815,SARCASM twitter_816,SARCASM twitter_817,SARCASM twitter_818,NOT_SARCASM twitter_819,SARCASM twitter_820,NOT_SARCASM twitter_821,NOT_SARCASM twitter_822,NOT_SARCASM twitter_823,SARCASM twitter_824,NOT_SARCASM twitter_825,NOT_SARCASM twitter_826,SARCASM twitter_827,SARCASM twitter_828,NOT_SARCASM twitter_829,SARCASM twitter_830,SARCASM twitter_831,NOT_SARCASM twitter_832,NOT_SARCASM twitter_833,NOT_SARCASM twitter_834,NOT_SARCASM twitter_835,SARCASM twitter_836,SARCASM twitter_837,SARCASM twitter_838,SARCASM twitter_839,SARCASM twitter_840,SARCASM twitter_841,SARCASM twitter_842,SARCASM twitter_843,SARCASM twitter_844,SARCASM twitter_845,NOT_SARCASM twitter_846,NOT_SARCASM twitter_847,SARCASM twitter_848,NOT_SARCASM twitter_849,NOT_SARCASM twitter_850,SARCASM twitter_851,SARCASM twitter_852,NOT_SARCASM twitter_853,NOT_SARCASM twitter_854,NOT_SARCASM twitter_855,NOT_SARCASM twitter_856,SARCASM twitter_857,SARCASM twitter_858,SARCASM twitter_859,NOT_SARCASM twitter_860,NOT_SARCASM twitter_861,NOT_SARCASM twitter_862,NOT_SARCASM twitter_863,SARCASM twitter_864,NOT_SARCASM twitter_865,SARCASM twitter_866,SARCASM twitter_867,SARCASM twitter_868,SARCASM twitter_869,SARCASM twitter_870,SARCASM twitter_871,SARCASM twitter_872,NOT_SARCASM twitter_873,SARCASM twitter_874,NOT_SARCASM twitter_875,NOT_SARCASM twitter_876,NOT_SARCASM twitter_877,NOT_SARCASM twitter_878,SARCASM twitter_879,NOT_SARCASM twitter_880,SARCASM twitter_881,NOT_SARCASM twitter_882,NOT_SARCASM twitter_883,SARCASM twitter_884,SARCASM twitter_885,SARCASM twitter_886,SARCASM twitter_887,NOT_SARCASM twitter_888,SARCASM twitter_889,NOT_SARCASM twitter_890,SARCASM twitter_891,SARCASM twitter_892,NOT_SARCASM twitter_893,SARCASM twitter_894,NOT_SARCASM twitter_895,SARCASM twitter_896,NOT_SARCASM twitter_897,NOT_SARCASM twitter_898,SARCASM twitter_899,SARCASM twitter_900,NOT_SARCASM twitter_901,SARCASM twitter_902,SARCASM twitter_903,SARCASM twitter_904,SARCASM twitter_905,SARCASM twitter_906,SARCASM twitter_907,SARCASM twitter_908,SARCASM twitter_909,SARCASM twitter_910,NOT_SARCASM twitter_911,NOT_SARCASM twitter_912,NOT_SARCASM twitter_913,SARCASM twitter_914,SARCASM twitter_915,SARCASM twitter_916,SARCASM twitter_917,SARCASM twitter_918,NOT_SARCASM twitter_919,SARCASM twitter_920,NOT_SARCASM twitter_921,SARCASM twitter_922,SARCASM twitter_923,NOT_SARCASM twitter_924,SARCASM twitter_925,NOT_SARCASM twitter_926,NOT_SARCASM twitter_927,NOT_SARCASM twitter_928,NOT_SARCASM twitter_929,NOT_SARCASM twitter_930,NOT_SARCASM twitter_931,SARCASM twitter_932,SARCASM twitter_933,NOT_SARCASM twitter_934,NOT_SARCASM twitter_935,SARCASM twitter_936,SARCASM twitter_937,SARCASM twitter_938,SARCASM twitter_939,SARCASM twitter_940,SARCASM twitter_941,SARCASM twitter_942,NOT_SARCASM twitter_943,NOT_SARCASM twitter_944,SARCASM twitter_945,SARCASM twitter_946,SARCASM twitter_947,NOT_SARCASM twitter_948,SARCASM twitter_949,SARCASM twitter_950,SARCASM twitter_951,NOT_SARCASM twitter_952,SARCASM twitter_953,NOT_SARCASM twitter_954,SARCASM twitter_955,SARCASM twitter_956,SARCASM twitter_957,SARCASM twitter_958,SARCASM twitter_959,NOT_SARCASM twitter_960,SARCASM twitter_961,SARCASM twitter_962,SARCASM twitter_963,NOT_SARCASM twitter_964,NOT_SARCASM twitter_965,SARCASM twitter_966,SARCASM twitter_967,SARCASM twitter_968,SARCASM twitter_969,SARCASM twitter_970,SARCASM twitter_971,SARCASM twitter_972,NOT_SARCASM twitter_973,SARCASM twitter_974,NOT_SARCASM twitter_975,SARCASM twitter_976,NOT_SARCASM twitter_977,SARCASM twitter_978,SARCASM twitter_979,SARCASM twitter_980,SARCASM twitter_981,SARCASM twitter_982,NOT_SARCASM twitter_983,SARCASM twitter_984,SARCASM twitter_985,SARCASM twitter_986,NOT_SARCASM twitter_987,NOT_SARCASM twitter_988,SARCASM twitter_989,NOT_SARCASM twitter_990,NOT_SARCASM twitter_991,SARCASM twitter_992,SARCASM twitter_993,SARCASM twitter_994,SARCASM twitter_995,SARCASM twitter_996,SARCASM twitter_997,SARCASM twitter_998,SARCASM twitter_999,NOT_SARCASM twitter_1000,NOT_SARCASM twitter_1001,NOT_SARCASM twitter_1002,SARCASM twitter_1003,SARCASM twitter_1004,SARCASM twitter_1005,SARCASM twitter_1006,SARCASM twitter_1007,SARCASM twitter_1008,NOT_SARCASM twitter_1009,NOT_SARCASM twitter_1010,SARCASM twitter_1011,NOT_SARCASM twitter_1012,SARCASM twitter_1013,SARCASM twitter_1014,SARCASM twitter_1015,NOT_SARCASM twitter_1016,NOT_SARCASM twitter_1017,NOT_SARCASM twitter_1018,SARCASM twitter_1019,NOT_SARCASM twitter_1020,SARCASM twitter_1021,NOT_SARCASM twitter_1022,NOT_SARCASM twitter_1023,SARCASM twitter_1024,SARCASM twitter_1025,NOT_SARCASM twitter_1026,NOT_SARCASM twitter_1027,SARCASM twitter_1028,SARCASM twitter_1029,NOT_SARCASM twitter_1030,SARCASM twitter_1031,NOT_SARCASM twitter_1032,NOT_SARCASM twitter_1033,NOT_SARCASM twitter_1034,SARCASM twitter_1035,NOT_SARCASM twitter_1036,SARCASM twitter_1037,NOT_SARCASM twitter_1038,SARCASM twitter_1039,SARCASM twitter_1040,SARCASM twitter_1041,NOT_SARCASM twitter_1042,SARCASM twitter_1043,SARCASM twitter_1044,NOT_SARCASM twitter_1045,SARCASM twitter_1046,SARCASM twitter_1047,NOT_SARCASM twitter_1048,SARCASM twitter_1049,NOT_SARCASM twitter_1050,SARCASM twitter_1051,NOT_SARCASM twitter_1052,NOT_SARCASM twitter_1053,SARCASM twitter_1054,SARCASM twitter_1055,NOT_SARCASM twitter_1056,SARCASM twitter_1057,NOT_SARCASM twitter_1058,SARCASM twitter_1059,SARCASM twitter_1060,NOT_SARCASM twitter_1061,SARCASM twitter_1062,SARCASM twitter_1063,NOT_SARCASM twitter_1064,NOT_SARCASM twitter_1065,SARCASM twitter_1066,NOT_SARCASM twitter_1067,SARCASM twitter_1068,NOT_SARCASM twitter_1069,SARCASM twitter_1070,SARCASM twitter_1071,SARCASM twitter_1072,NOT_SARCASM twitter_1073,NOT_SARCASM twitter_1074,SARCASM twitter_1075,NOT_SARCASM twitter_1076,SARCASM twitter_1077,NOT_SARCASM twitter_1078,SARCASM twitter_1079,SARCASM twitter_1080,SARCASM twitter_1081,SARCASM twitter_1082,NOT_SARCASM twitter_1083,SARCASM twitter_1084,SARCASM twitter_1085,SARCASM twitter_1086,SARCASM twitter_1087,NOT_SARCASM twitter_1088,NOT_SARCASM twitter_1089,NOT_SARCASM twitter_1090,NOT_SARCASM twitter_1091,SARCASM twitter_1092,SARCASM twitter_1093,NOT_SARCASM twitter_1094,NOT_SARCASM twitter_1095,SARCASM twitter_1096,NOT_SARCASM twitter_1097,NOT_SARCASM twitter_1098,NOT_SARCASM twitter_1099,NOT_SARCASM twitter_1100,SARCASM twitter_1101,SARCASM twitter_1102,SARCASM twitter_1103,SARCASM twitter_1104,SARCASM twitter_1105,SARCASM twitter_1106,NOT_SARCASM twitter_1107,SARCASM twitter_1108,SARCASM twitter_1109,SARCASM twitter_1110,NOT_SARCASM twitter_1111,SARCASM twitter_1112,SARCASM twitter_1113,SARCASM twitter_1114,NOT_SARCASM twitter_1115,SARCASM twitter_1116,SARCASM twitter_1117,NOT_SARCASM twitter_1118,NOT_SARCASM twitter_1119,NOT_SARCASM twitter_1120,SARCASM twitter_1121,SARCASM twitter_1122,NOT_SARCASM twitter_1123,SARCASM twitter_1124,SARCASM twitter_1125,SARCASM twitter_1126,NOT_SARCASM twitter_1127,NOT_SARCASM twitter_1128,NOT_SARCASM twitter_1129,NOT_SARCASM twitter_1130,SARCASM twitter_1131,SARCASM twitter_1132,NOT_SARCASM twitter_1133,SARCASM twitter_1134,NOT_SARCASM twitter_1135,NOT_SARCASM twitter_1136,SARCASM twitter_1137,SARCASM twitter_1138,NOT_SARCASM twitter_1139,SARCASM twitter_1140,NOT_SARCASM twitter_1141,SARCASM twitter_1142,SARCASM twitter_1143,SARCASM twitter_1144,SARCASM twitter_1145,SARCASM twitter_1146,NOT_SARCASM twitter_1147,SARCASM twitter_1148,NOT_SARCASM twitter_1149,SARCASM twitter_1150,NOT_SARCASM twitter_1151,NOT_SARCASM twitter_1152,SARCASM twitter_1153,NOT_SARCASM twitter_1154,NOT_SARCASM twitter_1155,SARCASM twitter_1156,SARCASM twitter_1157,SARCASM twitter_1158,SARCASM twitter_1159,SARCASM twitter_1160,NOT_SARCASM twitter_1161,NOT_SARCASM twitter_1162,SARCASM twitter_1163,NOT_SARCASM twitter_1164,SARCASM twitter_1165,NOT_SARCASM twitter_1166,SARCASM twitter_1167,SARCASM twitter_1168,NOT_SARCASM twitter_1169,SARCASM twitter_1170,SARCASM twitter_1171,SARCASM twitter_1172,SARCASM twitter_1173,SARCASM twitter_1174,SARCASM twitter_1175,SARCASM twitter_1176,NOT_SARCASM twitter_1177,NOT_SARCASM twitter_1178,SARCASM twitter_1179,NOT_SARCASM twitter_1180,SARCASM twitter_1181,NOT_SARCASM twitter_1182,NOT_SARCASM twitter_1183,SARCASM twitter_1184,SARCASM twitter_1185,NOT_SARCASM twitter_1186,SARCASM twitter_1187,SARCASM twitter_1188,SARCASM twitter_1189,SARCASM twitter_1190,SARCASM twitter_1191,SARCASM twitter_1192,SARCASM twitter_1193,NOT_SARCASM twitter_1194,NOT_SARCASM twitter_1195,NOT_SARCASM twitter_1196,NOT_SARCASM twitter_1197,NOT_SARCASM twitter_1198,SARCASM twitter_1199,SARCASM twitter_1200,SARCASM twitter_1201,NOT_SARCASM twitter_1202,SARCASM twitter_1203,NOT_SARCASM twitter_1204,NOT_SARCASM twitter_1205,NOT_SARCASM twitter_1206,NOT_SARCASM twitter_1207,SARCASM twitter_1208,SARCASM twitter_1209,NOT_SARCASM twitter_1210,SARCASM twitter_1211,SARCASM twitter_1212,SARCASM twitter_1213,NOT_SARCASM twitter_1214,SARCASM twitter_1215,SARCASM twitter_1216,NOT_SARCASM twitter_1217,NOT_SARCASM twitter_1218,NOT_SARCASM twitter_1219,SARCASM twitter_1220,NOT_SARCASM twitter_1221,SARCASM twitter_1222,SARCASM twitter_1223,NOT_SARCASM twitter_1224,NOT_SARCASM twitter_1225,SARCASM twitter_1226,SARCASM twitter_1227,SARCASM twitter_1228,SARCASM twitter_1229,SARCASM twitter_1230,SARCASM twitter_1231,SARCASM twitter_1232,SARCASM twitter_1233,SARCASM twitter_1234,SARCASM twitter_1235,SARCASM twitter_1236,SARCASM twitter_1237,NOT_SARCASM twitter_1238,SARCASM twitter_1239,NOT_SARCASM twitter_1240,SARCASM twitter_1241,NOT_SARCASM twitter_1242,SARCASM twitter_1243,SARCASM twitter_1244,NOT_SARCASM twitter_1245,NOT_SARCASM twitter_1246,NOT_SARCASM twitter_1247,SARCASM twitter_1248,SARCASM twitter_1249,NOT_SARCASM twitter_1250,NOT_SARCASM twitter_1251,SARCASM twitter_1252,SARCASM twitter_1253,NOT_SARCASM twitter_1254,SARCASM twitter_1255,SARCASM twitter_1256,NOT_SARCASM twitter_1257,SARCASM twitter_1258,NOT_SARCASM twitter_1259,NOT_SARCASM twitter_1260,SARCASM twitter_1261,SARCASM twitter_1262,SARCASM twitter_1263,SARCASM twitter_1264,SARCASM twitter_1265,NOT_SARCASM twitter_1266,NOT_SARCASM twitter_1267,SARCASM twitter_1268,SARCASM twitter_1269,NOT_SARCASM twitter_1270,SARCASM twitter_1271,SARCASM twitter_1272,SARCASM twitter_1273,NOT_SARCASM twitter_1274,SARCASM twitter_1275,SARCASM twitter_1276,NOT_SARCASM twitter_1277,NOT_SARCASM twitter_1278,NOT_SARCASM twitter_1279,NOT_SARCASM twitter_1280,SARCASM twitter_1281,NOT_SARCASM twitter_1282,NOT_SARCASM twitter_1283,SARCASM twitter_1284,NOT_SARCASM twitter_1285,SARCASM twitter_1286,NOT_SARCASM twitter_1287,NOT_SARCASM twitter_1288,SARCASM twitter_1289,SARCASM twitter_1290,SARCASM twitter_1291,SARCASM twitter_1292,SARCASM twitter_1293,SARCASM twitter_1294,NOT_SARCASM twitter_1295,SARCASM twitter_1296,SARCASM twitter_1297,SARCASM twitter_1298,SARCASM twitter_1299,SARCASM twitter_1300,SARCASM twitter_1301,SARCASM twitter_1302,SARCASM twitter_1303,NOT_SARCASM twitter_1304,NOT_SARCASM twitter_1305,SARCASM twitter_1306,NOT_SARCASM twitter_1307,SARCASM twitter_1308,SARCASM twitter_1309,SARCASM twitter_1310,SARCASM twitter_1311,SARCASM twitter_1312,SARCASM twitter_1313,NOT_SARCASM twitter_1314,SARCASM twitter_1315,SARCASM twitter_1316,SARCASM twitter_1317,SARCASM twitter_1318,NOT_SARCASM twitter_1319,NOT_SARCASM twitter_1320,SARCASM twitter_1321,SARCASM twitter_1322,SARCASM twitter_1323,SARCASM twitter_1324,SARCASM twitter_1325,SARCASM twitter_1326,NOT_SARCASM twitter_1327,NOT_SARCASM twitter_1328,NOT_SARCASM twitter_1329,SARCASM twitter_1330,SARCASM twitter_1331,SARCASM twitter_1332,SARCASM twitter_1333,NOT_SARCASM twitter_1334,SARCASM twitter_1335,NOT_SARCASM twitter_1336,NOT_SARCASM twitter_1337,SARCASM twitter_1338,NOT_SARCASM twitter_1339,SARCASM twitter_1340,SARCASM twitter_1341,SARCASM twitter_1342,SARCASM twitter_1343,SARCASM twitter_1344,SARCASM twitter_1345,NOT_SARCASM twitter_1346,SARCASM twitter_1347,SARCASM twitter_1348,SARCASM twitter_1349,SARCASM twitter_1350,SARCASM twitter_1351,SARCASM twitter_1352,NOT_SARCASM twitter_1353,SARCASM twitter_1354,NOT_SARCASM twitter_1355,SARCASM twitter_1356,SARCASM twitter_1357,SARCASM twitter_1358,SARCASM twitter_1359,SARCASM twitter_1360,SARCASM twitter_1361,SARCASM twitter_1362,NOT_SARCASM twitter_1363,SARCASM twitter_1364,NOT_SARCASM twitter_1365,SARCASM twitter_1366,SARCASM twitter_1367,SARCASM twitter_1368,SARCASM twitter_1369,NOT_SARCASM twitter_1370,NOT_SARCASM twitter_1371,NOT_SARCASM twitter_1372,NOT_SARCASM twitter_1373,NOT_SARCASM twitter_1374,SARCASM twitter_1375,NOT_SARCASM twitter_1376,SARCASM twitter_1377,SARCASM twitter_1378,NOT_SARCASM twitter_1379,SARCASM twitter_1380,SARCASM twitter_1381,SARCASM twitter_1382,SARCASM twitter_1383,SARCASM twitter_1384,NOT_SARCASM twitter_1385,SARCASM twitter_1386,NOT_SARCASM twitter_1387,SARCASM twitter_1388,SARCASM twitter_1389,SARCASM twitter_1390,SARCASM twitter_1391,SARCASM twitter_1392,SARCASM twitter_1393,SARCASM twitter_1394,NOT_SARCASM twitter_1395,SARCASM twitter_1396,NOT_SARCASM twitter_1397,SARCASM twitter_1398,SARCASM twitter_1399,SARCASM twitter_1400,NOT_SARCASM twitter_1401,SARCASM twitter_1402,SARCASM twitter_1403,SARCASM twitter_1404,SARCASM twitter_1405,NOT_SARCASM twitter_1406,NOT_SARCASM twitter_1407,SARCASM twitter_1408,SARCASM twitter_1409,SARCASM twitter_1410,SARCASM twitter_1411,SARCASM twitter_1412,NOT_SARCASM twitter_1413,NOT_SARCASM twitter_1414,SARCASM twitter_1415,SARCASM twitter_1416,SARCASM twitter_1417,SARCASM twitter_1418,NOT_SARCASM twitter_1419,NOT_SARCASM twitter_1420,NOT_SARCASM twitter_1421,NOT_SARCASM twitter_1422,NOT_SARCASM twitter_1423,SARCASM twitter_1424,SARCASM twitter_1425,SARCASM twitter_1426,NOT_SARCASM twitter_1427,SARCASM twitter_1428,SARCASM twitter_1429,SARCASM twitter_1430,NOT_SARCASM twitter_1431,NOT_SARCASM twitter_1432,SARCASM twitter_1433,NOT_SARCASM twitter_1434,NOT_SARCASM twitter_1435,SARCASM twitter_1436,SARCASM twitter_1437,SARCASM twitter_1438,SARCASM twitter_1439,SARCASM twitter_1440,NOT_SARCASM twitter_1441,SARCASM twitter_1442,NOT_SARCASM twitter_1443,NOT_SARCASM twitter_1444,NOT_SARCASM twitter_1445,NOT_SARCASM twitter_1446,NOT_SARCASM twitter_1447,SARCASM twitter_1448,SARCASM twitter_1449,SARCASM twitter_1450,SARCASM twitter_1451,SARCASM twitter_1452,SARCASM twitter_1453,SARCASM twitter_1454,SARCASM twitter_1455,SARCASM twitter_1456,SARCASM twitter_1457,NOT_SARCASM twitter_1458,NOT_SARCASM twitter_1459,NOT_SARCASM twitter_1460,SARCASM twitter_1461,SARCASM twitter_1462,SARCASM twitter_1463,SARCASM twitter_1464,SARCASM twitter_1465,SARCASM twitter_1466,SARCASM twitter_1467,NOT_SARCASM twitter_1468,SARCASM twitter_1469,SARCASM twitter_1470,SARCASM twitter_1471,SARCASM twitter_1472,SARCASM twitter_1473,NOT_SARCASM twitter_1474,NOT_SARCASM twitter_1475,SARCASM twitter_1476,SARCASM twitter_1477,SARCASM twitter_1478,NOT_SARCASM twitter_1479,SARCASM twitter_1480,NOT_SARCASM twitter_1481,SARCASM twitter_1482,NOT_SARCASM twitter_1483,SARCASM twitter_1484,SARCASM twitter_1485,NOT_SARCASM twitter_1486,SARCASM twitter_1487,SARCASM twitter_1488,SARCASM twitter_1489,SARCASM twitter_1490,SARCASM twitter_1491,NOT_SARCASM twitter_1492,SARCASM twitter_1493,SARCASM twitter_1494,SARCASM twitter_1495,NOT_SARCASM twitter_1496,SARCASM twitter_1497,NOT_SARCASM twitter_1498,SARCASM twitter_1499,NOT_SARCASM twitter_1500,NOT_SARCASM twitter_1501,NOT_SARCASM twitter_1502,NOT_SARCASM twitter_1503,NOT_SARCASM twitter_1504,SARCASM twitter_1505,NOT_SARCASM twitter_1506,SARCASM twitter_1507,SARCASM twitter_1508,SARCASM twitter_1509,SARCASM twitter_1510,SARCASM twitter_1511,SARCASM twitter_1512,SARCASM twitter_1513,SARCASM twitter_1514,SARCASM twitter_1515,NOT_SARCASM twitter_1516,SARCASM twitter_1517,NOT_SARCASM twitter_1518,SARCASM twitter_1519,NOT_SARCASM twitter_1520,SARCASM twitter_1521,SARCASM twitter_1522,SARCASM twitter_1523,SARCASM twitter_1524,NOT_SARCASM twitter_1525,SARCASM twitter_1526,NOT_SARCASM twitter_1527,SARCASM twitter_1528,SARCASM twitter_1529,SARCASM twitter_1530,SARCASM twitter_1531,NOT_SARCASM twitter_1532,SARCASM twitter_1533,SARCASM twitter_1534,SARCASM twitter_1535,SARCASM twitter_1536,NOT_SARCASM twitter_1537,SARCASM twitter_1538,SARCASM twitter_1539,SARCASM twitter_1540,SARCASM twitter_1541,SARCASM twitter_1542,SARCASM twitter_1543,NOT_SARCASM twitter_1544,SARCASM twitter_1545,NOT_SARCASM twitter_1546,SARCASM twitter_1547,SARCASM twitter_1548,NOT_SARCASM twitter_1549,SARCASM twitter_1550,SARCASM twitter_1551,SARCASM twitter_1552,SARCASM twitter_1553,SARCASM twitter_1554,SARCASM twitter_1555,NOT_SARCASM twitter_1556,SARCASM twitter_1557,SARCASM twitter_1558,SARCASM twitter_1559,SARCASM twitter_1560,SARCASM twitter_1561,SARCASM twitter_1562,SARCASM twitter_1563,NOT_SARCASM twitter_1564,SARCASM twitter_1565,SARCASM twitter_1566,NOT_SARCASM twitter_1567,SARCASM twitter_1568,NOT_SARCASM twitter_1569,NOT_SARCASM twitter_1570,SARCASM twitter_1571,SARCASM twitter_1572,SARCASM twitter_1573,NOT_SARCASM twitter_1574,NOT_SARCASM twitter_1575,NOT_SARCASM twitter_1576,SARCASM twitter_1577,SARCASM twitter_1578,SARCASM twitter_1579,SARCASM twitter_1580,NOT_SARCASM twitter_1581,SARCASM twitter_1582,NOT_SARCASM twitter_1583,NOT_SARCASM twitter_1584,NOT_SARCASM twitter_1585,SARCASM twitter_1586,SARCASM twitter_1587,NOT_SARCASM twitter_1588,SARCASM twitter_1589,NOT_SARCASM twitter_1590,NOT_SARCASM twitter_1591,SARCASM twitter_1592,NOT_SARCASM twitter_1593,NOT_SARCASM twitter_1594,SARCASM twitter_1595,NOT_SARCASM twitter_1596,SARCASM twitter_1597,SARCASM twitter_1598,NOT_SARCASM twitter_1599,SARCASM twitter_1600,SARCASM twitter_1601,SARCASM twitter_1602,NOT_SARCASM twitter_1603,NOT_SARCASM twitter_1604,SARCASM twitter_1605,SARCASM twitter_1606,SARCASM twitter_1607,SARCASM twitter_1608,NOT_SARCASM twitter_1609,SARCASM twitter_1610,SARCASM twitter_1611,SARCASM twitter_1612,NOT_SARCASM twitter_1613,SARCASM twitter_1614,NOT_SARCASM twitter_1615,SARCASM twitter_1616,SARCASM twitter_1617,SARCASM twitter_1618,NOT_SARCASM twitter_1619,NOT_SARCASM twitter_1620,SARCASM twitter_1621,SARCASM twitter_1622,NOT_SARCASM twitter_1623,NOT_SARCASM twitter_1624,NOT_SARCASM twitter_1625,SARCASM twitter_1626,SARCASM twitter_1627,SARCASM twitter_1628,NOT_SARCASM twitter_1629,SARCASM twitter_1630,NOT_SARCASM twitter_1631,NOT_SARCASM twitter_1632,SARCASM twitter_1633,SARCASM twitter_1634,SARCASM twitter_1635,SARCASM twitter_1636,SARCASM twitter_1637,NOT_SARCASM twitter_1638,SARCASM twitter_1639,SARCASM twitter_1640,SARCASM twitter_1641,NOT_SARCASM twitter_1642,SARCASM twitter_1643,SARCASM twitter_1644,SARCASM twitter_1645,SARCASM twitter_1646,SARCASM twitter_1647,NOT_SARCASM twitter_1648,NOT_SARCASM twitter_1649,SARCASM twitter_1650,NOT_SARCASM twitter_1651,NOT_SARCASM twitter_1652,SARCASM twitter_1653,SARCASM twitter_1654,SARCASM twitter_1655,SARCASM twitter_1656,SARCASM twitter_1657,SARCASM twitter_1658,NOT_SARCASM twitter_1659,SARCASM twitter_1660,SARCASM twitter_1661,NOT_SARCASM twitter_1662,SARCASM twitter_1663,NOT_SARCASM twitter_1664,SARCASM twitter_1665,SARCASM twitter_1666,SARCASM twitter_1667,NOT_SARCASM twitter_1668,SARCASM twitter_1669,NOT_SARCASM twitter_1670,SARCASM twitter_1671,SARCASM twitter_1672,SARCASM twitter_1673,NOT_SARCASM twitter_1674,SARCASM twitter_1675,SARCASM twitter_1676,SARCASM twitter_1677,SARCASM twitter_1678,NOT_SARCASM twitter_1679,SARCASM twitter_1680,SARCASM twitter_1681,NOT_SARCASM twitter_1682,SARCASM twitter_1683,SARCASM twitter_1684,SARCASM twitter_1685,SARCASM twitter_1686,SARCASM twitter_1687,SARCASM twitter_1688,NOT_SARCASM twitter_1689,NOT_SARCASM twitter_1690,NOT_SARCASM twitter_1691,SARCASM twitter_1692,SARCASM twitter_1693,SARCASM twitter_1694,NOT_SARCASM twitter_1695,NOT_SARCASM twitter_1696,SARCASM twitter_1697,SARCASM twitter_1698,NOT_SARCASM twitter_1699,SARCASM twitter_1700,NOT_SARCASM twitter_1701,SARCASM twitter_1702,SARCASM twitter_1703,SARCASM twitter_1704,NOT_SARCASM twitter_1705,SARCASM twitter_1706,SARCASM twitter_1707,NOT_SARCASM twitter_1708,SARCASM twitter_1709,NOT_SARCASM twitter_1710,SARCASM twitter_1711,SARCASM twitter_1712,NOT_SARCASM twitter_1713,SARCASM twitter_1714,NOT_SARCASM twitter_1715,NOT_SARCASM twitter_1716,SARCASM twitter_1717,NOT_SARCASM twitter_1718,NOT_SARCASM twitter_1719,NOT_SARCASM twitter_1720,SARCASM twitter_1721,SARCASM twitter_1722,NOT_SARCASM twitter_1723,SARCASM twitter_1724,SARCASM twitter_1725,NOT_SARCASM twitter_1726,SARCASM twitter_1727,NOT_SARCASM twitter_1728,SARCASM twitter_1729,NOT_SARCASM twitter_1730,SARCASM twitter_1731,SARCASM twitter_1732,SARCASM twitter_1733,SARCASM twitter_1734,NOT_SARCASM twitter_1735,SARCASM twitter_1736,SARCASM twitter_1737,SARCASM twitter_1738,SARCASM twitter_1739,NOT_SARCASM twitter_1740,SARCASM twitter_1741,SARCASM twitter_1742,NOT_SARCASM twitter_1743,NOT_SARCASM twitter_1744,SARCASM twitter_1745,SARCASM twitter_1746,SARCASM twitter_1747,SARCASM twitter_1748,SARCASM twitter_1749,SARCASM twitter_1750,NOT_SARCASM twitter_1751,SARCASM twitter_1752,SARCASM twitter_1753,NOT_SARCASM twitter_1754,SARCASM twitter_1755,NOT_SARCASM twitter_1756,SARCASM twitter_1757,NOT_SARCASM twitter_1758,SARCASM twitter_1759,SARCASM twitter_1760,SARCASM twitter_1761,SARCASM twitter_1762,SARCASM twitter_1763,NOT_SARCASM twitter_1764,NOT_SARCASM twitter_1765,SARCASM twitter_1766,SARCASM twitter_1767,NOT_SARCASM twitter_1768,SARCASM twitter_1769,SARCASM twitter_1770,SARCASM twitter_1771,SARCASM twitter_1772,SARCASM twitter_1773,NOT_SARCASM twitter_1774,SARCASM twitter_1775,NOT_SARCASM twitter_1776,SARCASM twitter_1777,NOT_SARCASM twitter_1778,SARCASM twitter_1779,NOT_SARCASM twitter_1780,SARCASM twitter_1781,NOT_SARCASM twitter_1782,SARCASM twitter_1783,SARCASM twitter_1784,SARCASM twitter_1785,SARCASM twitter_1786,SARCASM twitter_1787,NOT_SARCASM twitter_1788,SARCASM twitter_1789,NOT_SARCASM twitter_1790,SARCASM twitter_1791,NOT_SARCASM twitter_1792,SARCASM twitter_1793,NOT_SARCASM twitter_1794,SARCASM twitter_1795,SARCASM twitter_1796,NOT_SARCASM twitter_1797,SARCASM twitter_1798,NOT_SARCASM twitter_1799,NOT_SARCASM twitter_1800,NOT_SARCASM BERT Sentiment Analysis to Detect Twitter Sarcasm (Naive Approach) Zainal Hakim zainalh2@illinois.edu Table of content A. Introduction 3 B. Bidirectional Encoder Representations from Transformers (BERT) 3 C. Dataset Description 4 D. The Naive Approach 4 E. The Model, Training, and Evaluation 5 F. The Software Code 6 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review 6 1. Colab Configuration 6 2. Mounting Google Drive to Colab session (To save result files) 9 3. The Main Python Class 10 4. Training and Evaluation experiments 13 5. Save the result files to Google Drive 14 F.2. DEMO_Model_Evaluation.ipynb Code Review 15 1. The first 3-steps are already explained in detail in the previous section 15 2. Preparing the encoded testing dataset and data loader 15 3. Run the evaluation batch iteration 16 4. Generate the 'anwer.txt' file 17 5. Post 'answer.txt' to LiveDataLab for scoring 18 G. Result and Conclusion 18 Reference 19 Appendix 19 1. answer.txt 19 A. Introduction Sarcasm is a form of figurative language that implies a negative sentiment while displaying a positive sentiment on the surface (Joshi et al., 2017). I present a Naive approach to detect Twitter tweet sarcasm sentiment using a transformers-based pre-trained model that considers only the response tweet. This approach completely ignores the context of the response tweet to train the model. The model uses a transformer encoder to generate the embedding representation for the response. The model is trained and evaluated on the given training and testing datasets. My best performance model gives an F1-score of 75.79%, beating the Classification Competition baseline score after four epoch iterations (epoch # 4). ""C. Dataset Description"" section of this document explains further the response and context relationship. Important files in the project: 1. Documented software code * Training and evaluation (link) * Evaluation of a trained-model for demo purpose (link) 2. Best performance testing set predictions (answer.txt) (link) 3. Best performance trained-model (link) 4. Training dataset (link) 5. Testing dataset (link) B. Bidirectional Encoder Representations from Transformers (BERT) This project uses BERT, a transformer-based technique for Natural Language Processing pre-training developed by a team in Google. The original English language BERT model comes with two pre-trained model types: Model Type Layer Hidden Head Parameter Corpus Word Base 12 768 12 110 M 800 M Large 24 1024 16 340 M 2.500 M Table 1: BERT original model types BERT Large model essentially has better computing leverage than the base model. Google team trained the large model using a larger corpus word size than the base model. The large model is expected to perform better than the base model in most of the NLP tasks such as sentiment analysis. Original BERT paper is available here (link). C. Dataset Description There are two Twitter tweet datasets available for this project: 1. Training dataset: a labeled dataset to train the model 2. Testing dataset: tweet with a unique ID to evaluate the trained-model For the training dataset, each line contains a JSON object with the following columns: * label: SARCASM or NOT_SARCASM * response: the classified tweet * context: the conversation context of the response For the testing/evaluation dataset, each line contains a JSON object with the following columns: * id: unique identifier for the sample * response: the tweet to be classified * context: the conversation context of the response Training Dataset Testing Dataset 5000 lines 1800 lines Table 2: Dataset size statistics A more detailed dataset description is available in the project competition Github repository (link). D. The Naive Approach I hypothesize the context does not always support the sentiment of a response. Context can have an opposing effect on the sentiment of a response. I hypothesize there are 2 types of context: 1. A Positive context is a context that supports the sentiment of a response. 2. A Negative context is a context that does not support the sentiment of a response. Fig.1: Illustration of context reduces sentiment quality Sentiment Negative Context Fig.2: Illustration of context increase sentiment quality It is critical to utilize the context to support the response's sentiment. For this project, I consider only the sentiment-labeled response to training the model, and I completely ignore the context. I call this a Naive approach. In the future project, I can use advanced machine learning techniques to utilize response and context to train the model by selectively reconstruct the context to support the sentiment of a response. E. The Model, Training, and Evaluation In this project, I use the datasets to train and evaluate BERT Large uncased and base uncased models. I use the original BERT paper as a reference (A.3 Fine-tuning Procedure) to choose hyperparameters for my experiments. The hyperparameters in my experiments are: - Learning rate: 2e-5 - Batch size: 5 (considering memory size) - Epochs: 4 iterations - Epsilon: 1e-8 - Random seed value: 17 BERT model can handle a text with a maximum of 512 characters. If the input text is more than 512 characters, the model truncates the text to 512 characters. Response text in training and testing datasets is less than 512 characters, in this case, we are guaranteed to consider all words in the response text to train and evaluate the model. The Python source code below prints the response maximum characters for training and testing datasets. Positive Context S e n t i m e n t Fig.3: Source code to check maximum training and evaluation response characters length Response Max. Chars Training Dataset 315 Testing Dataset 310 Table 3: Maximum response characters length F. The Software Code For the software code of this project, I implemented two Google Colab Notebooks: 1. NAIVE_BERT_sentiment_analysis.ipynb: Training and evaluation notebook (link) 2. DEMO_Model_Evaluation.ipynb: Evaluation of selected trained-model notebook (for DEMO purpose) (link) I use the Google Colab PRO environment to implement and test the software code. Introduction about Google Colab is available here link. Software code uses the following main Python libraries: - Numpy 1.18.5 - Pytorch 1.7.0+cu101 - Huggingface Transformers 3.5.0 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review This notebook trains and evaluates the BERT Large uncased and base uncased models using the provided datasets. In the end, the trained-model and evaluation results are copied and stored in the project Google Drive folder. 1. Colab Configuration a. Install Python modules required for the notebook. b. Copy train.jsonl and test.jsonl files from Google Drive to Colab session I have already copied train.jsonl and test.jsonl files to a Google Drive account created for this project. The files are shared with the public. The following code will copy the files from Google Drive to the Colab session. The source code above will prompt a URL. Click the URL, it will prompt the Google account login page. Select the Google account to run the notebook: Click the ""Allow"" button to allow Google Cloud SDK to access the Google account. That finally prompts the verification code. Copy the code and paste it in the ""Enter verification code"" text box. Press ""Enter"" That will copy the training and testing datasets to the Colab session! We are ready to train and evaluate the model using the datasets. 2. Mounting Google Drive to Colab session (To save result files) To save the result files (the trained-model and answer.txt) to Google Drive, we need to mount Google Drive to the Colab session. In this project, I mount the Google Drive directory to the './content/uiuc' folder. At the end of this notebook code execution, the Google Drive mounted folder in the Colab session will look like the following: Mounted Google Drive in Colab Session where to keep result files permanently 3. The Main Python Class This project implements a Python class, BERT_Model, that handles the following tasks: o Read the dataset from JSONL files into a list of JSON o Convert list of JSON to Pandas DataFrame o Create the BERT Model o Run the training and save the model for each epoch o Evaluate the model and store the result into a file Below are the class signatures: For details please check the source code here link. The source code comment describes what each step does. The main algorithm of the training: 1. Create a BERT tokenizer. We use the tokenizer to encode the text and prepare the dataset. 2. Create a BERT model using the chosen hyperparameter. 3. Create AdamW optimizer (link), it is used to calculate the convergence in the model. It is considered the fastest convergence algorithm. 4. Create a data loader. The data loader main task is passing dataset batch to model. It will make sure all the text in the dataset is processed by the model. 5. Set the model to train mode. Iterate the data loader to pass the dataset batch to the model until the training is completed. 6. After the training is completed, save the trained model to a file. The main algorithm of testing: 1. Load the model file to the memory and set the model to evaluation mode. 2. Create the tokenizer, encode the input data for testing, and prepare the dataset. 3. Create the data loader to handle the evaluation batch. 4. Run the evaluation. One of the most important outputs from the model is logits. In this project, logits is an array of two elements. The model is configured to have the first element of logits represents SARCASM, and the second element represents NOT_SARCASM. 5. We use the NumPy argmax function to return the index of the maximum value in logits. o If the maximum value is the first element, the function returns the index array of 0 (SARCASM) o If the maximum value is the second element, the function returns the index array of 1 (NOT_SARCASM) 6. The final step is to write the output to a file (answer.txt) 4. Training and Evaluation experiments To test my Naive hypothesis, I run experiments with the same hyperparameters on two BERT models. 1. Experiment-1: BERT base uncased 2. Experiment-2: BERT LARGE uncased Both experiments code above will generate result files in the Colab session folder below: 5. Save the result files to Google Drive Colab deletes result files when the session ends. We need to store the files permanently in other locations, in this project I use Google Drive. The code below will copy the result files to the project Google Drive folder (the Google Drive folder has been mounted in the earlier step). In this project, I save the epoch # 4 model file only. We use the files stored in the project Google Drive folder to run a project demo. The next section will illustrate how to use the trained-model file to evaluate the testing dataset. F.2. DEMO_Model_Evaluation.ipynb Code Review In the demo notebook, I demonstrate how to generate 'answer.txt' from the BERT Large uncased trained model stored in the project Google Drive folder. The trained-model is available to the public here link. With this model, we will reproduce the evaluation result which is available to the public here link. The demo video is available here link. The main algorithm in the notebook demo: 1. The first 3-steps are already explained in detail in the previous section * Colab configuration (Python modules import and installation) * Copy the trained-model and testing dataset files from Google Drive to the Colab session. We need to authorize Colab to access the Google account described in the previous section. * Prepare Panda DataFrame for the testing dataset. 2. Preparing the encoded testing dataset and data loader The main steps: * Create the BERT tokenizer to encode the testing dataset. * Create the data loader to run the evaluation in batches. Preparing the Python objects for the evaluation 3. Run the evaluation batch iteration The main steps: * Set the device to GPU, if applicable. * Load the trained model from file to memory. * Set the model to evaluation mode. * Data loader iteration to pass text to evaluate in batches to the model. * The model output logits (how to process logits is described in the previous section) 4. Generate the 'anwer.txt' file Generate the sentiment output to the screen and 'answer.txt' file. The previous section describes the source code snippet. 5. Post 'answer.txt' to LiveDataLab for scoring For F1, precision, and recall scores evaluation, I post the 'answer.txt' to LiveDataLab. Leaderboard snapshot on 03-Nov-2020 G. Result and Conclusion In summary, the testing (evaluation) results from BERT Large and base models: Model F1-Score Recall Precision BERT Large uncased 0.757905138339921 0.8522222222222222 0.6823843416370107 BERT Base uncased 0.7458777885548012 0.8544444444444445 0.6617900172117039 Surprisingly, the base model performs almost as good as the large model. In this project, I did try to use different trained model such as RoBERTa and XLNet (and different hyperparameters), but I could not produce a result higher than BERT Large uncased score. The project scope as proposed in the project proposal is to explore BERT hence, I am reporting the result for BERT models only. In the future, I would like to explore more on the following topics: * To use advanced machine learning techniques to explore other hyperparameters for BERT models. * To utilize both the context and response to training the models. I hypothesize that the context can be used as an additional dataset to train the model. * To explore another model, such as RoBERTa and XLNet. Reference 1. Aditya Joshi, Pushpak Bhattacharyya, and Mark J. Car- man. 2017. Automatic Sarcasm Detection: A Survey. ACM Computing Surveys, 50(5):1-22. 2. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Appendix 1. answer.txt twitter_1,NOT_SARCASM twitter_2,SARCASM twitter_3,SARCASM twitter_4,NOT_SARCASM twitter_5,SARCASM twitter_6,SARCASM twitter_7,NOT_SARCASM twitter_8,SARCASM twitter_9,NOT_SARCASM twitter_10,SARCASM twitter_11,NOT_SARCASM twitter_12,SARCASM twitter_13,SARCASM twitter_14,NOT_SARCASM twitter_15,SARCASM twitter_16,SARCASM twitter_17,SARCASM twitter_18,SARCASM twitter_19,SARCASM twitter_20,NOT_SARCASM twitter_21,NOT_SARCASM twitter_22,SARCASM twitter_23,NOT_SARCASM twitter_24,SARCASM twitter_25,SARCASM twitter_26,SARCASM twitter_27,NOT_SARCASM twitter_28,NOT_SARCASM twitter_29,SARCASM twitter_30,NOT_SARCASM twitter_31,SARCASM twitter_32,NOT_SARCASM twitter_33,NOT_SARCASM twitter_34,SARCASM twitter_35,NOT_SARCASM twitter_36,SARCASM twitter_37,SARCASM twitter_38,SARCASM twitter_39,SARCASM twitter_40,SARCASM twitter_41,SARCASM twitter_42,NOT_SARCASM twitter_43,SARCASM twitter_44,NOT_SARCASM twitter_45,SARCASM twitter_46,NOT_SARCASM twitter_47,SARCASM twitter_48,SARCASM twitter_49,NOT_SARCASM twitter_50,SARCASM twitter_51,NOT_SARCASM twitter_52,NOT_SARCASM twitter_53,SARCASM twitter_54,SARCASM twitter_55,SARCASM twitter_56,SARCASM twitter_57,NOT_SARCASM twitter_58,NOT_SARCASM twitter_59,SARCASM twitter_60,SARCASM twitter_61,NOT_SARCASM twitter_62,SARCASM twitter_63,SARCASM twitter_64,SARCASM twitter_65,SARCASM twitter_66,NOT_SARCASM twitter_67,SARCASM twitter_68,NOT_SARCASM twitter_69,SARCASM twitter_70,SARCASM twitter_71,NOT_SARCASM twitter_72,SARCASM twitter_73,SARCASM twitter_74,SARCASM twitter_75,NOT_SARCASM twitter_76,NOT_SARCASM twitter_77,SARCASM twitter_78,SARCASM twitter_79,NOT_SARCASM twitter_80,SARCASM twitter_81,NOT_SARCASM twitter_82,NOT_SARCASM twitter_83,SARCASM twitter_84,NOT_SARCASM twitter_85,SARCASM twitter_86,SARCASM twitter_87,NOT_SARCASM twitter_88,SARCASM twitter_89,SARCASM twitter_90,NOT_SARCASM twitter_91,NOT_SARCASM twitter_92,SARCASM twitter_93,SARCASM twitter_94,SARCASM twitter_95,SARCASM twitter_96,SARCASM twitter_97,SARCASM twitter_98,NOT_SARCASM twitter_99,NOT_SARCASM twitter_100,NOT_SARCASM twitter_101,SARCASM twitter_102,SARCASM twitter_103,NOT_SARCASM twitter_104,NOT_SARCASM twitter_105,SARCASM twitter_106,SARCASM twitter_107,NOT_SARCASM twitter_108,NOT_SARCASM twitter_109,SARCASM twitter_110,SARCASM twitter_111,SARCASM twitter_112,SARCASM twitter_113,SARCASM twitter_114,SARCASM twitter_115,SARCASM twitter_116,NOT_SARCASM twitter_117,NOT_SARCASM twitter_118,SARCASM twitter_119,NOT_SARCASM twitter_120,NOT_SARCASM twitter_121,SARCASM twitter_122,SARCASM twitter_123,NOT_SARCASM twitter_124,SARCASM twitter_125,NOT_SARCASM twitter_126,NOT_SARCASM twitter_127,SARCASM twitter_128,NOT_SARCASM twitter_129,NOT_SARCASM twitter_130,SARCASM twitter_131,NOT_SARCASM twitter_132,SARCASM twitter_133,NOT_SARCASM twitter_134,NOT_SARCASM twitter_135,SARCASM twitter_136,NOT_SARCASM twitter_137,SARCASM twitter_138,NOT_SARCASM twitter_139,NOT_SARCASM twitter_140,SARCASM twitter_141,SARCASM twitter_142,SARCASM twitter_143,SARCASM twitter_144,SARCASM twitter_145,SARCASM twitter_146,SARCASM twitter_147,SARCASM twitter_148,SARCASM twitter_149,SARCASM twitter_150,SARCASM twitter_151,SARCASM twitter_152,NOT_SARCASM twitter_153,NOT_SARCASM twitter_154,SARCASM twitter_155,SARCASM twitter_156,NOT_SARCASM twitter_157,SARCASM twitter_158,SARCASM twitter_159,SARCASM twitter_160,SARCASM twitter_161,NOT_SARCASM twitter_162,SARCASM twitter_163,SARCASM twitter_164,SARCASM twitter_165,SARCASM twitter_166,SARCASM twitter_167,SARCASM twitter_168,SARCASM twitter_169,NOT_SARCASM twitter_170,NOT_SARCASM twitter_171,NOT_SARCASM twitter_172,NOT_SARCASM twitter_173,SARCASM twitter_174,SARCASM twitter_175,SARCASM twitter_176,SARCASM twitter_177,SARCASM twitter_178,NOT_SARCASM twitter_179,SARCASM twitter_180,SARCASM twitter_181,SARCASM twitter_182,NOT_SARCASM twitter_183,SARCASM twitter_184,SARCASM twitter_185,NOT_SARCASM twitter_186,SARCASM twitter_187,SARCASM twitter_188,SARCASM twitter_189,NOT_SARCASM twitter_190,NOT_SARCASM twitter_191,NOT_SARCASM twitter_192,NOT_SARCASM twitter_193,SARCASM twitter_194,NOT_SARCASM twitter_195,NOT_SARCASM twitter_196,SARCASM twitter_197,NOT_SARCASM twitter_198,SARCASM twitter_199,SARCASM twitter_200,SARCASM twitter_201,SARCASM twitter_202,SARCASM twitter_203,NOT_SARCASM twitter_204,NOT_SARCASM twitter_205,SARCASM twitter_206,NOT_SARCASM twitter_207,SARCASM twitter_208,SARCASM twitter_209,SARCASM twitter_210,SARCASM twitter_211,SARCASM twitter_212,NOT_SARCASM twitter_213,NOT_SARCASM twitter_214,NOT_SARCASM twitter_215,NOT_SARCASM twitter_216,SARCASM twitter_217,SARCASM twitter_218,SARCASM twitter_219,SARCASM twitter_220,SARCASM twitter_221,SARCASM twitter_222,NOT_SARCASM twitter_223,NOT_SARCASM twitter_224,SARCASM twitter_225,NOT_SARCASM twitter_226,SARCASM twitter_227,NOT_SARCASM twitter_228,NOT_SARCASM twitter_229,SARCASM twitter_230,NOT_SARCASM twitter_231,SARCASM twitter_232,SARCASM twitter_233,SARCASM twitter_234,NOT_SARCASM twitter_235,SARCASM twitter_236,NOT_SARCASM twitter_237,SARCASM twitter_238,NOT_SARCASM twitter_239,SARCASM twitter_240,SARCASM twitter_241,SARCASM twitter_242,SARCASM twitter_243,NOT_SARCASM twitter_244,NOT_SARCASM twitter_245,NOT_SARCASM twitter_246,NOT_SARCASM twitter_247,SARCASM twitter_248,SARCASM twitter_249,SARCASM twitter_250,SARCASM twitter_251,SARCASM twitter_252,SARCASM twitter_253,NOT_SARCASM twitter_254,NOT_SARCASM twitter_255,SARCASM twitter_256,NOT_SARCASM twitter_257,NOT_SARCASM twitter_258,SARCASM twitter_259,SARCASM twitter_260,SARCASM twitter_261,NOT_SARCASM twitter_262,SARCASM twitter_263,NOT_SARCASM twitter_264,SARCASM twitter_265,SARCASM twitter_266,SARCASM twitter_267,SARCASM twitter_268,NOT_SARCASM twitter_269,SARCASM twitter_270,NOT_SARCASM twitter_271,NOT_SARCASM twitter_272,NOT_SARCASM twitter_273,NOT_SARCASM twitter_274,SARCASM twitter_275,SARCASM twitter_276,NOT_SARCASM twitter_277,NOT_SARCASM twitter_278,SARCASM twitter_279,SARCASM twitter_280,NOT_SARCASM twitter_281,SARCASM twitter_282,NOT_SARCASM twitter_283,SARCASM twitter_284,SARCASM twitter_285,NOT_SARCASM twitter_286,NOT_SARCASM twitter_287,NOT_SARCASM twitter_288,NOT_SARCASM twitter_289,NOT_SARCASM twitter_290,SARCASM twitter_291,SARCASM twitter_292,SARCASM twitter_293,SARCASM twitter_294,SARCASM twitter_295,NOT_SARCASM twitter_296,SARCASM twitter_297,SARCASM twitter_298,SARCASM twitter_299,SARCASM twitter_300,NOT_SARCASM twitter_301,SARCASM twitter_302,NOT_SARCASM twitter_303,SARCASM twitter_304,SARCASM twitter_305,NOT_SARCASM twitter_306,SARCASM twitter_307,NOT_SARCASM twitter_308,SARCASM twitter_309,SARCASM twitter_310,NOT_SARCASM twitter_311,SARCASM twitter_312,NOT_SARCASM twitter_313,SARCASM twitter_314,NOT_SARCASM twitter_315,SARCASM twitter_316,SARCASM twitter_317,NOT_SARCASM twitter_318,NOT_SARCASM twitter_319,SARCASM twitter_320,SARCASM twitter_321,NOT_SARCASM twitter_322,NOT_SARCASM twitter_323,NOT_SARCASM twitter_324,SARCASM twitter_325,NOT_SARCASM twitter_326,NOT_SARCASM twitter_327,SARCASM twitter_328,NOT_SARCASM twitter_329,SARCASM twitter_330,SARCASM twitter_331,SARCASM twitter_332,SARCASM twitter_333,SARCASM twitter_334,NOT_SARCASM twitter_335,NOT_SARCASM twitter_336,SARCASM twitter_337,NOT_SARCASM twitter_338,SARCASM twitter_339,NOT_SARCASM twitter_340,SARCASM twitter_341,NOT_SARCASM twitter_342,SARCASM twitter_343,SARCASM twitter_344,NOT_SARCASM twitter_345,SARCASM twitter_346,NOT_SARCASM twitter_347,SARCASM twitter_348,SARCASM twitter_349,SARCASM twitter_350,SARCASM twitter_351,SARCASM twitter_352,SARCASM twitter_353,NOT_SARCASM twitter_354,SARCASM twitter_355,SARCASM twitter_356,SARCASM twitter_357,SARCASM twitter_358,NOT_SARCASM twitter_359,SARCASM twitter_360,NOT_SARCASM twitter_361,SARCASM twitter_362,SARCASM twitter_363,SARCASM twitter_364,SARCASM twitter_365,NOT_SARCASM twitter_366,SARCASM twitter_367,SARCASM twitter_368,SARCASM twitter_369,NOT_SARCASM twitter_370,NOT_SARCASM twitter_371,SARCASM twitter_372,SARCASM twitter_373,SARCASM twitter_374,NOT_SARCASM twitter_375,SARCASM twitter_376,NOT_SARCASM twitter_377,SARCASM twitter_378,NOT_SARCASM twitter_379,NOT_SARCASM twitter_380,SARCASM twitter_381,SARCASM twitter_382,NOT_SARCASM twitter_383,SARCASM twitter_384,NOT_SARCASM twitter_385,NOT_SARCASM twitter_386,SARCASM twitter_387,SARCASM twitter_388,SARCASM twitter_389,NOT_SARCASM twitter_390,NOT_SARCASM twitter_391,SARCASM twitter_392,SARCASM twitter_393,NOT_SARCASM twitter_394,SARCASM twitter_395,SARCASM twitter_396,SARCASM twitter_397,NOT_SARCASM twitter_398,NOT_SARCASM twitter_399,SARCASM twitter_400,SARCASM twitter_401,SARCASM twitter_402,NOT_SARCASM twitter_403,SARCASM twitter_404,NOT_SARCASM twitter_405,NOT_SARCASM twitter_406,SARCASM twitter_407,SARCASM twitter_408,SARCASM twitter_409,SARCASM twitter_410,SARCASM twitter_411,SARCASM twitter_412,NOT_SARCASM twitter_413,SARCASM twitter_414,SARCASM twitter_415,SARCASM twitter_416,SARCASM twitter_417,SARCASM twitter_418,SARCASM twitter_419,NOT_SARCASM twitter_420,NOT_SARCASM twitter_421,SARCASM twitter_422,NOT_SARCASM twitter_423,SARCASM twitter_424,SARCASM twitter_425,SARCASM twitter_426,NOT_SARCASM twitter_427,SARCASM twitter_428,SARCASM twitter_429,NOT_SARCASM twitter_430,SARCASM twitter_431,SARCASM twitter_432,SARCASM twitter_433,SARCASM twitter_434,SARCASM twitter_435,NOT_SARCASM twitter_436,SARCASM twitter_437,SARCASM twitter_438,NOT_SARCASM twitter_439,NOT_SARCASM twitter_440,SARCASM twitter_441,SARCASM twitter_442,SARCASM twitter_443,SARCASM twitter_444,SARCASM twitter_445,SARCASM twitter_446,SARCASM twitter_447,SARCASM twitter_448,SARCASM twitter_449,SARCASM twitter_450,NOT_SARCASM twitter_451,NOT_SARCASM twitter_452,NOT_SARCASM twitter_453,SARCASM twitter_454,SARCASM twitter_455,NOT_SARCASM twitter_456,NOT_SARCASM twitter_457,SARCASM twitter_458,SARCASM twitter_459,SARCASM twitter_460,SARCASM twitter_461,SARCASM twitter_462,SARCASM twitter_463,SARCASM twitter_464,SARCASM twitter_465,NOT_SARCASM twitter_466,SARCASM twitter_467,NOT_SARCASM twitter_468,SARCASM twitter_469,SARCASM twitter_470,SARCASM twitter_471,SARCASM twitter_472,SARCASM twitter_473,NOT_SARCASM twitter_474,SARCASM twitter_475,NOT_SARCASM twitter_476,SARCASM twitter_477,NOT_SARCASM twitter_478,SARCASM twitter_479,SARCASM twitter_480,SARCASM twitter_481,NOT_SARCASM twitter_482,NOT_SARCASM twitter_483,NOT_SARCASM twitter_484,SARCASM twitter_485,NOT_SARCASM twitter_486,SARCASM twitter_487,NOT_SARCASM twitter_488,SARCASM twitter_489,SARCASM twitter_490,NOT_SARCASM twitter_491,NOT_SARCASM twitter_492,NOT_SARCASM twitter_493,SARCASM twitter_494,SARCASM twitter_495,SARCASM twitter_496,SARCASM twitter_497,NOT_SARCASM twitter_498,SARCASM twitter_499,SARCASM twitter_500,SARCASM twitter_501,SARCASM twitter_502,SARCASM twitter_503,SARCASM twitter_504,SARCASM twitter_505,SARCASM twitter_506,SARCASM twitter_507,SARCASM twitter_508,NOT_SARCASM twitter_509,NOT_SARCASM twitter_510,SARCASM twitter_511,NOT_SARCASM twitter_512,NOT_SARCASM twitter_513,SARCASM twitter_514,SARCASM twitter_515,NOT_SARCASM twitter_516,NOT_SARCASM twitter_517,SARCASM twitter_518,SARCASM twitter_519,NOT_SARCASM twitter_520,SARCASM twitter_521,SARCASM twitter_522,SARCASM twitter_523,NOT_SARCASM twitter_524,SARCASM twitter_525,SARCASM twitter_526,SARCASM twitter_527,SARCASM twitter_528,SARCASM twitter_529,NOT_SARCASM twitter_530,NOT_SARCASM twitter_531,NOT_SARCASM twitter_532,SARCASM twitter_533,NOT_SARCASM twitter_534,SARCASM twitter_535,SARCASM twitter_536,SARCASM twitter_537,NOT_SARCASM twitter_538,NOT_SARCASM twitter_539,SARCASM twitter_540,SARCASM twitter_541,SARCASM twitter_542,SARCASM twitter_543,NOT_SARCASM twitter_544,NOT_SARCASM twitter_545,SARCASM twitter_546,SARCASM twitter_547,SARCASM twitter_548,NOT_SARCASM twitter_549,NOT_SARCASM twitter_550,SARCASM twitter_551,SARCASM twitter_552,NOT_SARCASM twitter_553,SARCASM twitter_554,SARCASM twitter_555,NOT_SARCASM twitter_556,SARCASM twitter_557,SARCASM twitter_558,SARCASM twitter_559,SARCASM twitter_560,SARCASM twitter_561,NOT_SARCASM twitter_562,SARCASM twitter_563,SARCASM twitter_564,SARCASM twitter_565,SARCASM twitter_566,SARCASM twitter_567,SARCASM twitter_568,SARCASM twitter_569,NOT_SARCASM twitter_570,SARCASM twitter_571,SARCASM twitter_572,SARCASM twitter_573,SARCASM twitter_574,SARCASM twitter_575,NOT_SARCASM twitter_576,SARCASM twitter_577,SARCASM twitter_578,NOT_SARCASM twitter_579,NOT_SARCASM twitter_580,NOT_SARCASM twitter_581,NOT_SARCASM twitter_582,NOT_SARCASM twitter_583,SARCASM twitter_584,SARCASM twitter_585,SARCASM twitter_586,SARCASM twitter_587,SARCASM twitter_588,SARCASM twitter_589,NOT_SARCASM twitter_590,NOT_SARCASM twitter_591,SARCASM twitter_592,SARCASM twitter_593,NOT_SARCASM twitter_594,SARCASM twitter_595,SARCASM twitter_596,NOT_SARCASM twitter_597,NOT_SARCASM twitter_598,SARCASM twitter_599,NOT_SARCASM twitter_600,SARCASM twitter_601,NOT_SARCASM twitter_602,SARCASM twitter_603,NOT_SARCASM twitter_604,SARCASM twitter_605,NOT_SARCASM twitter_606,NOT_SARCASM twitter_607,SARCASM twitter_608,SARCASM twitter_609,NOT_SARCASM twitter_610,NOT_SARCASM twitter_611,NOT_SARCASM twitter_612,SARCASM twitter_613,SARCASM twitter_614,NOT_SARCASM twitter_615,NOT_SARCASM twitter_616,SARCASM twitter_617,SARCASM twitter_618,NOT_SARCASM twitter_619,SARCASM twitter_620,NOT_SARCASM twitter_621,SARCASM twitter_622,NOT_SARCASM twitter_623,SARCASM twitter_624,NOT_SARCASM twitter_625,SARCASM twitter_626,SARCASM twitter_627,NOT_SARCASM twitter_628,NOT_SARCASM twitter_629,SARCASM twitter_630,SARCASM twitter_631,NOT_SARCASM twitter_632,SARCASM twitter_633,NOT_SARCASM twitter_634,SARCASM twitter_635,SARCASM twitter_636,NOT_SARCASM twitter_637,SARCASM twitter_638,NOT_SARCASM twitter_639,SARCASM twitter_640,NOT_SARCASM twitter_641,NOT_SARCASM twitter_642,SARCASM twitter_643,SARCASM twitter_644,SARCASM twitter_645,NOT_SARCASM twitter_646,NOT_SARCASM twitter_647,NOT_SARCASM twitter_648,NOT_SARCASM twitter_649,SARCASM twitter_650,NOT_SARCASM twitter_651,SARCASM twitter_652,NOT_SARCASM twitter_653,NOT_SARCASM twitter_654,SARCASM twitter_655,NOT_SARCASM twitter_656,NOT_SARCASM twitter_657,SARCASM twitter_658,NOT_SARCASM twitter_659,SARCASM twitter_660,SARCASM twitter_661,SARCASM twitter_662,SARCASM twitter_663,NOT_SARCASM twitter_664,SARCASM twitter_665,NOT_SARCASM twitter_666,NOT_SARCASM twitter_667,SARCASM twitter_668,SARCASM twitter_669,SARCASM twitter_670,NOT_SARCASM twitter_671,SARCASM twitter_672,SARCASM twitter_673,SARCASM twitter_674,SARCASM twitter_675,SARCASM twitter_676,NOT_SARCASM twitter_677,NOT_SARCASM twitter_678,SARCASM twitter_679,SARCASM twitter_680,NOT_SARCASM twitter_681,NOT_SARCASM twitter_682,SARCASM twitter_683,NOT_SARCASM twitter_684,NOT_SARCASM twitter_685,SARCASM twitter_686,SARCASM twitter_687,NOT_SARCASM twitter_688,SARCASM twitter_689,SARCASM twitter_690,SARCASM twitter_691,NOT_SARCASM twitter_692,SARCASM twitter_693,SARCASM twitter_694,NOT_SARCASM twitter_695,SARCASM twitter_696,NOT_SARCASM twitter_697,SARCASM twitter_698,NOT_SARCASM twitter_699,NOT_SARCASM twitter_700,NOT_SARCASM twitter_701,SARCASM twitter_702,NOT_SARCASM twitter_703,SARCASM twitter_704,SARCASM twitter_705,NOT_SARCASM twitter_706,SARCASM twitter_707,NOT_SARCASM twitter_708,SARCASM twitter_709,NOT_SARCASM twitter_710,SARCASM twitter_711,SARCASM twitter_712,SARCASM twitter_713,SARCASM twitter_714,SARCASM twitter_715,SARCASM twitter_716,SARCASM twitter_717,SARCASM twitter_718,SARCASM twitter_719,NOT_SARCASM twitter_720,NOT_SARCASM twitter_721,SARCASM twitter_722,NOT_SARCASM twitter_723,SARCASM twitter_724,SARCASM twitter_725,SARCASM twitter_726,NOT_SARCASM twitter_727,NOT_SARCASM twitter_728,SARCASM twitter_729,SARCASM twitter_730,SARCASM twitter_731,NOT_SARCASM twitter_732,SARCASM twitter_733,NOT_SARCASM twitter_734,NOT_SARCASM twitter_735,SARCASM twitter_736,SARCASM twitter_737,SARCASM twitter_738,SARCASM twitter_739,SARCASM twitter_740,SARCASM twitter_741,SARCASM twitter_742,SARCASM twitter_743,SARCASM twitter_744,NOT_SARCASM twitter_745,NOT_SARCASM twitter_746,SARCASM twitter_747,SARCASM twitter_748,NOT_SARCASM twitter_749,NOT_SARCASM twitter_750,NOT_SARCASM twitter_751,SARCASM twitter_752,NOT_SARCASM twitter_753,SARCASM twitter_754,SARCASM twitter_755,NOT_SARCASM twitter_756,SARCASM twitter_757,NOT_SARCASM twitter_758,NOT_SARCASM twitter_759,SARCASM twitter_760,SARCASM twitter_761,NOT_SARCASM twitter_762,SARCASM twitter_763,NOT_SARCASM twitter_764,SARCASM twitter_765,SARCASM twitter_766,SARCASM twitter_767,SARCASM twitter_768,NOT_SARCASM twitter_769,NOT_SARCASM twitter_770,NOT_SARCASM twitter_771,SARCASM twitter_772,NOT_SARCASM twitter_773,SARCASM twitter_774,NOT_SARCASM twitter_775,SARCASM twitter_776,SARCASM twitter_777,NOT_SARCASM twitter_778,SARCASM twitter_779,NOT_SARCASM twitter_780,SARCASM twitter_781,NOT_SARCASM twitter_782,NOT_SARCASM twitter_783,SARCASM twitter_784,SARCASM twitter_785,NOT_SARCASM twitter_786,SARCASM twitter_787,SARCASM twitter_788,NOT_SARCASM twitter_789,SARCASM twitter_790,SARCASM twitter_791,NOT_SARCASM twitter_792,NOT_SARCASM twitter_793,SARCASM twitter_794,NOT_SARCASM twitter_795,SARCASM twitter_796,SARCASM twitter_797,SARCASM twitter_798,SARCASM twitter_799,SARCASM twitter_800,NOT_SARCASM twitter_801,NOT_SARCASM twitter_802,NOT_SARCASM twitter_803,SARCASM twitter_804,NOT_SARCASM twitter_805,SARCASM twitter_806,SARCASM twitter_807,NOT_SARCASM twitter_808,SARCASM twitter_809,SARCASM twitter_810,NOT_SARCASM twitter_811,SARCASM twitter_812,SARCASM twitter_813,SARCASM twitter_814,SARCASM twitter_815,SARCASM twitter_816,SARCASM twitter_817,SARCASM twitter_818,NOT_SARCASM twitter_819,SARCASM twitter_820,NOT_SARCASM twitter_821,NOT_SARCASM twitter_822,NOT_SARCASM twitter_823,SARCASM twitter_824,NOT_SARCASM twitter_825,NOT_SARCASM twitter_826,SARCASM twitter_827,SARCASM twitter_828,NOT_SARCASM twitter_829,SARCASM twitter_830,SARCASM twitter_831,NOT_SARCASM twitter_832,NOT_SARCASM twitter_833,NOT_SARCASM twitter_834,NOT_SARCASM twitter_835,SARCASM twitter_836,SARCASM twitter_837,SARCASM twitter_838,SARCASM twitter_839,SARCASM twitter_840,SARCASM twitter_841,SARCASM twitter_842,SARCASM twitter_843,SARCASM twitter_844,SARCASM twitter_845,NOT_SARCASM twitter_846,NOT_SARCASM twitter_847,SARCASM twitter_848,NOT_SARCASM twitter_849,NOT_SARCASM twitter_850,SARCASM twitter_851,SARCASM twitter_852,NOT_SARCASM twitter_853,NOT_SARCASM twitter_854,NOT_SARCASM twitter_855,NOT_SARCASM twitter_856,SARCASM twitter_857,SARCASM twitter_858,SARCASM twitter_859,NOT_SARCASM twitter_860,NOT_SARCASM twitter_861,NOT_SARCASM twitter_862,NOT_SARCASM twitter_863,SARCASM twitter_864,NOT_SARCASM twitter_865,SARCASM twitter_866,SARCASM twitter_867,SARCASM twitter_868,SARCASM twitter_869,SARCASM twitter_870,SARCASM twitter_871,SARCASM twitter_872,NOT_SARCASM twitter_873,SARCASM twitter_874,NOT_SARCASM twitter_875,NOT_SARCASM twitter_876,NOT_SARCASM twitter_877,NOT_SARCASM twitter_878,SARCASM twitter_879,NOT_SARCASM twitter_880,SARCASM twitter_881,NOT_SARCASM twitter_882,NOT_SARCASM twitter_883,SARCASM twitter_884,SARCASM twitter_885,SARCASM twitter_886,SARCASM twitter_887,NOT_SARCASM twitter_888,SARCASM twitter_889,NOT_SARCASM twitter_890,SARCASM twitter_891,SARCASM twitter_892,NOT_SARCASM twitter_893,SARCASM twitter_894,NOT_SARCASM twitter_895,SARCASM twitter_896,NOT_SARCASM twitter_897,NOT_SARCASM twitter_898,SARCASM twitter_899,SARCASM twitter_900,NOT_SARCASM twitter_901,SARCASM twitter_902,SARCASM twitter_903,SARCASM twitter_904,SARCASM twitter_905,SARCASM twitter_906,SARCASM twitter_907,SARCASM twitter_908,SARCASM twitter_909,SARCASM twitter_910,NOT_SARCASM twitter_911,NOT_SARCASM twitter_912,NOT_SARCASM twitter_913,SARCASM twitter_914,SARCASM twitter_915,SARCASM twitter_916,SARCASM twitter_917,SARCASM twitter_918,NOT_SARCASM twitter_919,SARCASM twitter_920,NOT_SARCASM twitter_921,SARCASM twitter_922,SARCASM twitter_923,NOT_SARCASM twitter_924,SARCASM twitter_925,NOT_SARCASM twitter_926,NOT_SARCASM twitter_927,NOT_SARCASM twitter_928,NOT_SARCASM twitter_929,NOT_SARCASM twitter_930,NOT_SARCASM twitter_931,SARCASM twitter_932,SARCASM twitter_933,NOT_SARCASM twitter_934,NOT_SARCASM twitter_935,SARCASM twitter_936,SARCASM twitter_937,SARCASM twitter_938,SARCASM twitter_939,SARCASM twitter_940,SARCASM twitter_941,SARCASM twitter_942,NOT_SARCASM twitter_943,NOT_SARCASM twitter_944,SARCASM twitter_945,SARCASM twitter_946,SARCASM twitter_947,NOT_SARCASM twitter_948,SARCASM twitter_949,SARCASM twitter_950,SARCASM twitter_951,NOT_SARCASM twitter_952,SARCASM twitter_953,NOT_SARCASM twitter_954,SARCASM twitter_955,SARCASM twitter_956,SARCASM twitter_957,SARCASM twitter_958,SARCASM twitter_959,NOT_SARCASM twitter_960,SARCASM twitter_961,SARCASM twitter_962,SARCASM twitter_963,NOT_SARCASM twitter_964,NOT_SARCASM twitter_965,SARCASM twitter_966,SARCASM twitter_967,SARCASM twitter_968,SARCASM twitter_969,SARCASM twitter_970,SARCASM twitter_971,SARCASM twitter_972,NOT_SARCASM twitter_973,SARCASM twitter_974,NOT_SARCASM twitter_975,SARCASM twitter_976,NOT_SARCASM twitter_977,SARCASM twitter_978,SARCASM twitter_979,SARCASM twitter_980,SARCASM twitter_981,SARCASM twitter_982,NOT_SARCASM twitter_983,SARCASM twitter_984,SARCASM twitter_985,SARCASM twitter_986,NOT_SARCASM twitter_987,NOT_SARCASM twitter_988,SARCASM twitter_989,NOT_SARCASM twitter_990,NOT_SARCASM twitter_991,SARCASM twitter_992,SARCASM twitter_993,SARCASM twitter_994,SARCASM twitter_995,SARCASM twitter_996,SARCASM twitter_997,SARCASM twitter_998,SARCASM twitter_999,NOT_SARCASM twitter_1000,NOT_SARCASM twitter_1001,NOT_SARCASM twitter_1002,SARCASM twitter_1003,SARCASM twitter_1004,SARCASM twitter_1005,SARCASM twitter_1006,SARCASM twitter_1007,SARCASM twitter_1008,NOT_SARCASM twitter_1009,NOT_SARCASM twitter_1010,SARCASM twitter_1011,NOT_SARCASM twitter_1012,SARCASM twitter_1013,SARCASM twitter_1014,SARCASM twitter_1015,NOT_SARCASM twitter_1016,NOT_SARCASM twitter_1017,NOT_SARCASM twitter_1018,SARCASM twitter_1019,NOT_SARCASM twitter_1020,SARCASM twitter_1021,NOT_SARCASM twitter_1022,NOT_SARCASM twitter_1023,SARCASM twitter_1024,SARCASM twitter_1025,NOT_SARCASM twitter_1026,NOT_SARCASM twitter_1027,SARCASM twitter_1028,SARCASM twitter_1029,NOT_SARCASM twitter_1030,SARCASM twitter_1031,NOT_SARCASM twitter_1032,NOT_SARCASM twitter_1033,NOT_SARCASM twitter_1034,SARCASM twitter_1035,NOT_SARCASM twitter_1036,SARCASM twitter_1037,NOT_SARCASM twitter_1038,SARCASM twitter_1039,SARCASM twitter_1040,SARCASM twitter_1041,NOT_SARCASM twitter_1042,SARCASM twitter_1043,SARCASM twitter_1044,NOT_SARCASM twitter_1045,SARCASM twitter_1046,SARCASM twitter_1047,NOT_SARCASM twitter_1048,SARCASM twitter_1049,NOT_SARCASM twitter_1050,SARCASM twitter_1051,NOT_SARCASM twitter_1052,NOT_SARCASM twitter_1053,SARCASM twitter_1054,SARCASM twitter_1055,NOT_SARCASM twitter_1056,SARCASM twitter_1057,NOT_SARCASM twitter_1058,SARCASM twitter_1059,SARCASM twitter_1060,NOT_SARCASM twitter_1061,SARCASM twitter_1062,SARCASM twitter_1063,NOT_SARCASM twitter_1064,NOT_SARCASM twitter_1065,SARCASM twitter_1066,NOT_SARCASM twitter_1067,SARCASM twitter_1068,NOT_SARCASM twitter_1069,SARCASM twitter_1070,SARCASM twitter_1071,SARCASM twitter_1072,NOT_SARCASM twitter_1073,NOT_SARCASM twitter_1074,SARCASM twitter_1075,NOT_SARCASM twitter_1076,SARCASM twitter_1077,NOT_SARCASM twitter_1078,SARCASM twitter_1079,SARCASM twitter_1080,SARCASM twitter_1081,SARCASM twitter_1082,NOT_SARCASM twitter_1083,SARCASM twitter_1084,SARCASM twitter_1085,SARCASM twitter_1086,SARCASM twitter_1087,NOT_SARCASM twitter_1088,NOT_SARCASM twitter_1089,NOT_SARCASM twitter_1090,NOT_SARCASM twitter_1091,SARCASM twitter_1092,SARCASM twitter_1093,NOT_SARCASM twitter_1094,NOT_SARCASM twitter_1095,SARCASM twitter_1096,NOT_SARCASM twitter_1097,NOT_SARCASM twitter_1098,NOT_SARCASM twitter_1099,NOT_SARCASM twitter_1100,SARCASM twitter_1101,SARCASM twitter_1102,SARCASM twitter_1103,SARCASM twitter_1104,SARCASM twitter_1105,SARCASM twitter_1106,NOT_SARCASM twitter_1107,SARCASM twitter_1108,SARCASM twitter_1109,SARCASM twitter_1110,NOT_SARCASM twitter_1111,SARCASM twitter_1112,SARCASM twitter_1113,SARCASM twitter_1114,NOT_SARCASM twitter_1115,SARCASM twitter_1116,SARCASM twitter_1117,NOT_SARCASM twitter_1118,NOT_SARCASM twitter_1119,NOT_SARCASM twitter_1120,SARCASM twitter_1121,SARCASM twitter_1122,NOT_SARCASM twitter_1123,SARCASM twitter_1124,SARCASM twitter_1125,SARCASM twitter_1126,NOT_SARCASM twitter_1127,NOT_SARCASM twitter_1128,NOT_SARCASM twitter_1129,NOT_SARCASM twitter_1130,SARCASM twitter_1131,SARCASM twitter_1132,NOT_SARCASM twitter_1133,SARCASM twitter_1134,NOT_SARCASM twitter_1135,NOT_SARCASM twitter_1136,SARCASM twitter_1137,SARCASM twitter_1138,NOT_SARCASM twitter_1139,SARCASM twitter_1140,NOT_SARCASM twitter_1141,SARCASM twitter_1142,SARCASM twitter_1143,SARCASM twitter_1144,SARCASM twitter_1145,SARCASM twitter_1146,NOT_SARCASM twitter_1147,SARCASM twitter_1148,NOT_SARCASM twitter_1149,SARCASM twitter_1150,NOT_SARCASM twitter_1151,NOT_SARCASM twitter_1152,SARCASM twitter_1153,NOT_SARCASM twitter_1154,NOT_SARCASM twitter_1155,SARCASM twitter_1156,SARCASM twitter_1157,SARCASM twitter_1158,SARCASM twitter_1159,SARCASM twitter_1160,NOT_SARCASM twitter_1161,NOT_SARCASM twitter_1162,SARCASM twitter_1163,NOT_SARCASM twitter_1164,SARCASM twitter_1165,NOT_SARCASM twitter_1166,SARCASM twitter_1167,SARCASM twitter_1168,NOT_SARCASM twitter_1169,SARCASM twitter_1170,SARCASM twitter_1171,SARCASM twitter_1172,SARCASM twitter_1173,SARCASM twitter_1174,SARCASM twitter_1175,SARCASM twitter_1176,NOT_SARCASM twitter_1177,NOT_SARCASM twitter_1178,SARCASM twitter_1179,NOT_SARCASM twitter_1180,SARCASM twitter_1181,NOT_SARCASM twitter_1182,NOT_SARCASM twitter_1183,SARCASM twitter_1184,SARCASM twitter_1185,NOT_SARCASM twitter_1186,SARCASM twitter_1187,SARCASM twitter_1188,SARCASM twitter_1189,SARCASM twitter_1190,SARCASM twitter_1191,SARCASM twitter_1192,SARCASM twitter_1193,NOT_SARCASM twitter_1194,NOT_SARCASM twitter_1195,NOT_SARCASM twitter_1196,NOT_SARCASM twitter_1197,NOT_SARCASM twitter_1198,SARCASM twitter_1199,SARCASM twitter_1200,SARCASM twitter_1201,NOT_SARCASM twitter_1202,SARCASM twitter_1203,NOT_SARCASM twitter_1204,NOT_SARCASM twitter_1205,NOT_SARCASM twitter_1206,NOT_SARCASM twitter_1207,SARCASM twitter_1208,SARCASM twitter_1209,NOT_SARCASM twitter_1210,SARCASM twitter_1211,SARCASM twitter_1212,SARCASM twitter_1213,NOT_SARCASM twitter_1214,SARCASM twitter_1215,SARCASM twitter_1216,NOT_SARCASM twitter_1217,NOT_SARCASM twitter_1218,NOT_SARCASM twitter_1219,SARCASM twitter_1220,NOT_SARCASM twitter_1221,SARCASM twitter_1222,SARCASM twitter_1223,NOT_SARCASM twitter_1224,NOT_SARCASM twitter_1225,SARCASM twitter_1226,SARCASM twitter_1227,SARCASM twitter_1228,SARCASM twitter_1229,SARCASM twitter_1230,SARCASM twitter_1231,SARCASM twitter_1232,SARCASM twitter_1233,SARCASM twitter_1234,SARCASM twitter_1235,SARCASM twitter_1236,SARCASM twitter_1237,NOT_SARCASM twitter_1238,SARCASM twitter_1239,NOT_SARCASM twitter_1240,SARCASM twitter_1241,NOT_SARCASM twitter_1242,SARCASM twitter_1243,SARCASM twitter_1244,NOT_SARCASM twitter_1245,NOT_SARCASM twitter_1246,NOT_SARCASM twitter_1247,SARCASM twitter_1248,SARCASM twitter_1249,NOT_SARCASM twitter_1250,NOT_SARCASM twitter_1251,SARCASM twitter_1252,SARCASM twitter_1253,NOT_SARCASM twitter_1254,SARCASM twitter_1255,SARCASM twitter_1256,NOT_SARCASM twitter_1257,SARCASM twitter_1258,NOT_SARCASM twitter_1259,NOT_SARCASM twitter_1260,SARCASM twitter_1261,SARCASM twitter_1262,SARCASM twitter_1263,SARCASM twitter_1264,SARCASM twitter_1265,NOT_SARCASM twitter_1266,NOT_SARCASM twitter_1267,SARCASM twitter_1268,SARCASM twitter_1269,NOT_SARCASM twitter_1270,SARCASM twitter_1271,SARCASM twitter_1272,SARCASM twitter_1273,NOT_SARCASM twitter_1274,SARCASM twitter_1275,SARCASM twitter_1276,NOT_SARCASM twitter_1277,NOT_SARCASM twitter_1278,NOT_SARCASM twitter_1279,NOT_SARCASM twitter_1280,SARCASM twitter_1281,NOT_SARCASM twitter_1282,NOT_SARCASM twitter_1283,SARCASM twitter_1284,NOT_SARCASM twitter_1285,SARCASM twitter_1286,NOT_SARCASM twitter_1287,NOT_SARCASM twitter_1288,SARCASM twitter_1289,SARCASM twitter_1290,SARCASM twitter_1291,SARCASM twitter_1292,SARCASM twitter_1293,SARCASM twitter_1294,NOT_SARCASM twitter_1295,SARCASM twitter_1296,SARCASM twitter_1297,SARCASM twitter_1298,SARCASM twitter_1299,SARCASM twitter_1300,SARCASM twitter_1301,SARCASM twitter_1302,SARCASM twitter_1303,NOT_SARCASM twitter_1304,NOT_SARCASM twitter_1305,SARCASM twitter_1306,NOT_SARCASM twitter_1307,SARCASM twitter_1308,SARCASM twitter_1309,SARCASM twitter_1310,SARCASM twitter_1311,SARCASM twitter_1312,SARCASM twitter_1313,NOT_SARCASM twitter_1314,SARCASM twitter_1315,SARCASM twitter_1316,SARCASM twitter_1317,SARCASM twitter_1318,NOT_SARCASM twitter_1319,NOT_SARCASM twitter_1320,SARCASM twitter_1321,SARCASM twitter_1322,SARCASM twitter_1323,SARCASM twitter_1324,SARCASM twitter_1325,SARCASM twitter_1326,NOT_SARCASM twitter_1327,NOT_SARCASM twitter_1328,NOT_SARCASM twitter_1329,SARCASM twitter_1330,SARCASM twitter_1331,SARCASM twitter_1332,SARCASM twitter_1333,NOT_SARCASM twitter_1334,SARCASM twitter_1335,NOT_SARCASM twitter_1336,NOT_SARCASM twitter_1337,SARCASM twitter_1338,NOT_SARCASM twitter_1339,SARCASM twitter_1340,SARCASM twitter_1341,SARCASM twitter_1342,SARCASM twitter_1343,SARCASM twitter_1344,SARCASM twitter_1345,NOT_SARCASM twitter_1346,SARCASM twitter_1347,SARCASM twitter_1348,SARCASM twitter_1349,SARCASM twitter_1350,SARCASM twitter_1351,SARCASM twitter_1352,NOT_SARCASM twitter_1353,SARCASM twitter_1354,NOT_SARCASM twitter_1355,SARCASM twitter_1356,SARCASM twitter_1357,SARCASM twitter_1358,SARCASM twitter_1359,SARCASM twitter_1360,SARCASM twitter_1361,SARCASM twitter_1362,NOT_SARCASM twitter_1363,SARCASM twitter_1364,NOT_SARCASM twitter_1365,SARCASM twitter_1366,SARCASM twitter_1367,SARCASM twitter_1368,SARCASM twitter_1369,NOT_SARCASM twitter_1370,NOT_SARCASM twitter_1371,NOT_SARCASM twitter_1372,NOT_SARCASM twitter_1373,NOT_SARCASM twitter_1374,SARCASM twitter_1375,NOT_SARCASM twitter_1376,SARCASM twitter_1377,SARCASM twitter_1378,NOT_SARCASM twitter_1379,SARCASM twitter_1380,SARCASM twitter_1381,SARCASM twitter_1382,SARCASM twitter_1383,SARCASM twitter_1384,NOT_SARCASM twitter_1385,SARCASM twitter_1386,NOT_SARCASM twitter_1387,SARCASM twitter_1388,SARCASM twitter_1389,SARCASM twitter_1390,SARCASM twitter_1391,SARCASM twitter_1392,SARCASM twitter_1393,SARCASM twitter_1394,NOT_SARCASM twitter_1395,SARCASM twitter_1396,NOT_SARCASM twitter_1397,SARCASM twitter_1398,SARCASM twitter_1399,SARCASM twitter_1400,NOT_SARCASM twitter_1401,SARCASM twitter_1402,SARCASM twitter_1403,SARCASM twitter_1404,SARCASM twitter_1405,NOT_SARCASM twitter_1406,NOT_SARCASM twitter_1407,SARCASM twitter_1408,SARCASM twitter_1409,SARCASM twitter_1410,SARCASM twitter_1411,SARCASM twitter_1412,NOT_SARCASM twitter_1413,NOT_SARCASM twitter_1414,SARCASM twitter_1415,SARCASM twitter_1416,SARCASM twitter_1417,SARCASM twitter_1418,NOT_SARCASM twitter_1419,NOT_SARCASM twitter_1420,NOT_SARCASM twitter_1421,NOT_SARCASM twitter_1422,NOT_SARCASM twitter_1423,SARCASM twitter_1424,SARCASM twitter_1425,SARCASM twitter_1426,NOT_SARCASM twitter_1427,SARCASM twitter_1428,SARCASM twitter_1429,SARCASM twitter_1430,NOT_SARCASM twitter_1431,NOT_SARCASM twitter_1432,SARCASM twitter_1433,NOT_SARCASM twitter_1434,NOT_SARCASM twitter_1435,SARCASM twitter_1436,SARCASM twitter_1437,SARCASM twitter_1438,SARCASM twitter_1439,SARCASM twitter_1440,NOT_SARCASM twitter_1441,SARCASM twitter_1442,NOT_SARCASM twitter_1443,NOT_SARCASM twitter_1444,NOT_SARCASM twitter_1445,NOT_SARCASM twitter_1446,NOT_SARCASM twitter_1447,SARCASM twitter_1448,SARCASM twitter_1449,SARCASM twitter_1450,SARCASM twitter_1451,SARCASM twitter_1452,SARCASM twitter_1453,SARCASM twitter_1454,SARCASM twitter_1455,SARCASM twitter_1456,SARCASM twitter_1457,NOT_SARCASM twitter_1458,NOT_SARCASM twitter_1459,NOT_SARCASM twitter_1460,SARCASM twitter_1461,SARCASM twitter_1462,SARCASM twitter_1463,SARCASM twitter_1464,SARCASM twitter_1465,SARCASM twitter_1466,SARCASM twitter_1467,NOT_SARCASM twitter_1468,SARCASM twitter_1469,SARCASM twitter_1470,SARCASM twitter_1471,SARCASM twitter_1472,SARCASM twitter_1473,NOT_SARCASM twitter_1474,NOT_SARCASM twitter_1475,SARCASM twitter_1476,SARCASM twitter_1477,SARCASM twitter_1478,NOT_SARCASM twitter_1479,SARCASM twitter_1480,NOT_SARCASM twitter_1481,SARCASM twitter_1482,NOT_SARCASM twitter_1483,SARCASM twitter_1484,SARCASM twitter_1485,NOT_SARCASM twitter_1486,SARCASM twitter_1487,SARCASM twitter_1488,SARCASM twitter_1489,SARCASM twitter_1490,SARCASM twitter_1491,NOT_SARCASM twitter_1492,SARCASM twitter_1493,SARCASM twitter_1494,SARCASM twitter_1495,NOT_SARCASM twitter_1496,SARCASM twitter_1497,NOT_SARCASM twitter_1498,SARCASM twitter_1499,NOT_SARCASM twitter_1500,NOT_SARCASM twitter_1501,NOT_SARCASM twitter_1502,NOT_SARCASM twitter_1503,NOT_SARCASM twitter_1504,SARCASM twitter_1505,NOT_SARCASM twitter_1506,SARCASM twitter_1507,SARCASM twitter_1508,SARCASM twitter_1509,SARCASM twitter_1510,SARCASM twitter_1511,SARCASM twitter_1512,SARCASM twitter_1513,SARCASM twitter_1514,SARCASM twitter_1515,NOT_SARCASM twitter_1516,SARCASM twitter_1517,NOT_SARCASM twitter_1518,SARCASM twitter_1519,NOT_SARCASM twitter_1520,SARCASM twitter_1521,SARCASM twitter_1522,SARCASM twitter_1523,SARCASM twitter_1524,NOT_SARCASM twitter_1525,SARCASM twitter_1526,NOT_SARCASM twitter_1527,SARCASM twitter_1528,SARCASM twitter_1529,SARCASM twitter_1530,SARCASM twitter_1531,NOT_SARCASM twitter_1532,SARCASM twitter_1533,SARCASM twitter_1534,SARCASM twitter_1535,SARCASM twitter_1536,NOT_SARCASM twitter_1537,SARCASM twitter_1538,SARCASM twitter_1539,SARCASM twitter_1540,SARCASM twitter_1541,SARCASM twitter_1542,SARCASM twitter_1543,NOT_SARCASM twitter_1544,SARCASM twitter_1545,NOT_SARCASM twitter_1546,SARCASM twitter_1547,SARCASM twitter_1548,NOT_SARCASM twitter_1549,SARCASM twitter_1550,SARCASM twitter_1551,SARCASM twitter_1552,SARCASM twitter_1553,SARCASM twitter_1554,SARCASM twitter_1555,NOT_SARCASM twitter_1556,SARCASM twitter_1557,SARCASM twitter_1558,SARCASM twitter_1559,SARCASM twitter_1560,SARCASM twitter_1561,SARCASM twitter_1562,SARCASM twitter_1563,NOT_SARCASM twitter_1564,SARCASM twitter_1565,SARCASM twitter_1566,NOT_SARCASM twitter_1567,SARCASM twitter_1568,NOT_SARCASM twitter_1569,NOT_SARCASM twitter_1570,SARCASM twitter_1571,SARCASM twitter_1572,SARCASM twitter_1573,NOT_SARCASM twitter_1574,NOT_SARCASM twitter_1575,NOT_SARCASM twitter_1576,SARCASM twitter_1577,SARCASM twitter_1578,SARCASM twitter_1579,SARCASM twitter_1580,NOT_SARCASM twitter_1581,SARCASM twitter_1582,NOT_SARCASM twitter_1583,NOT_SARCASM twitter_1584,NOT_SARCASM twitter_1585,SARCASM twitter_1586,SARCASM twitter_1587,NOT_SARCASM twitter_1588,SARCASM twitter_1589,NOT_SARCASM twitter_1590,NOT_SARCASM twitter_1591,SARCASM twitter_1592,NOT_SARCASM twitter_1593,NOT_SARCASM twitter_1594,SARCASM twitter_1595,NOT_SARCASM twitter_1596,SARCASM twitter_1597,SARCASM twitter_1598,NOT_SARCASM twitter_1599,SARCASM twitter_1600,SARCASM twitter_1601,SARCASM twitter_1602,NOT_SARCASM twitter_1603,NOT_SARCASM twitter_1604,SARCASM twitter_1605,SARCASM twitter_1606,SARCASM twitter_1607,SARCASM twitter_1608,NOT_SARCASM twitter_1609,SARCASM twitter_1610,SARCASM twitter_1611,SARCASM twitter_1612,NOT_SARCASM twitter_1613,SARCASM twitter_1614,NOT_SARCASM twitter_1615,SARCASM twitter_1616,SARCASM twitter_1617,SARCASM twitter_1618,NOT_SARCASM twitter_1619,NOT_SARCASM twitter_1620,SARCASM twitter_1621,SARCASM twitter_1622,NOT_SARCASM twitter_1623,NOT_SARCASM twitter_1624,NOT_SARCASM twitter_1625,SARCASM twitter_1626,SARCASM twitter_1627,SARCASM twitter_1628,NOT_SARCASM twitter_1629,SARCASM twitter_1630,NOT_SARCASM twitter_1631,NOT_SARCASM twitter_1632,SARCASM twitter_1633,SARCASM twitter_1634,SARCASM twitter_1635,SARCASM twitter_1636,SARCASM twitter_1637,NOT_SARCASM twitter_1638,SARCASM twitter_1639,SARCASM twitter_1640,SARCASM twitter_1641,NOT_SARCASM twitter_1642,SARCASM twitter_1643,SARCASM twitter_1644,SARCASM twitter_1645,SARCASM twitter_1646,SARCASM twitter_1647,NOT_SARCASM twitter_1648,NOT_SARCASM twitter_1649,SARCASM twitter_1650,NOT_SARCASM twitter_1651,NOT_SARCASM twitter_1652,SARCASM twitter_1653,SARCASM twitter_1654,SARCASM twitter_1655,SARCASM twitter_1656,SARCASM twitter_1657,SARCASM twitter_1658,NOT_SARCASM twitter_1659,SARCASM twitter_1660,SARCASM twitter_1661,NOT_SARCASM twitter_1662,SARCASM twitter_1663,NOT_SARCASM twitter_1664,SARCASM twitter_1665,SARCASM twitter_1666,SARCASM twitter_1667,NOT_SARCASM twitter_1668,SARCASM twitter_1669,NOT_SARCASM twitter_1670,SARCASM twitter_1671,SARCASM twitter_1672,SARCASM twitter_1673,NOT_SARCASM twitter_1674,SARCASM twitter_1675,SARCASM twitter_1676,SARCASM twitter_1677,SARCASM twitter_1678,NOT_SARCASM twitter_1679,SARCASM twitter_1680,SARCASM twitter_1681,NOT_SARCASM twitter_1682,SARCASM twitter_1683,SARCASM twitter_1684,SARCASM twitter_1685,SARCASM twitter_1686,SARCASM twitter_1687,SARCASM twitter_1688,NOT_SARCASM twitter_1689,NOT_SARCASM twitter_1690,NOT_SARCASM twitter_1691,SARCASM twitter_1692,SARCASM twitter_1693,SARCASM twitter_1694,NOT_SARCASM twitter_1695,NOT_SARCASM twitter_1696,SARCASM twitter_1697,SARCASM twitter_1698,NOT_SARCASM twitter_1699,SARCASM twitter_1700,NOT_SARCASM twitter_1701,SARCASM twitter_1702,SARCASM twitter_1703,SARCASM twitter_1704,NOT_SARCASM twitter_1705,SARCASM twitter_1706,SARCASM twitter_1707,NOT_SARCASM twitter_1708,SARCASM twitter_1709,NOT_SARCASM twitter_1710,SARCASM twitter_1711,SARCASM twitter_1712,NOT_SARCASM twitter_1713,SARCASM twitter_1714,NOT_SARCASM twitter_1715,NOT_SARCASM twitter_1716,SARCASM twitter_1717,NOT_SARCASM twitter_1718,NOT_SARCASM twitter_1719,NOT_SARCASM twitter_1720,SARCASM twitter_1721,SARCASM twitter_1722,NOT_SARCASM twitter_1723,SARCASM twitter_1724,SARCASM twitter_1725,NOT_SARCASM twitter_1726,SARCASM twitter_1727,NOT_SARCASM twitter_1728,SARCASM twitter_1729,NOT_SARCASM twitter_1730,SARCASM twitter_1731,SARCASM twitter_1732,SARCASM twitter_1733,SARCASM twitter_1734,NOT_SARCASM twitter_1735,SARCASM twitter_1736,SARCASM twitter_1737,SARCASM twitter_1738,SARCASM twitter_1739,NOT_SARCASM twitter_1740,SARCASM twitter_1741,SARCASM twitter_1742,NOT_SARCASM twitter_1743,NOT_SARCASM twitter_1744,SARCASM twitter_1745,SARCASM twitter_1746,SARCASM twitter_1747,SARCASM twitter_1748,SARCASM twitter_1749,SARCASM twitter_1750,NOT_SARCASM twitter_1751,SARCASM twitter_1752,SARCASM twitter_1753,NOT_SARCASM twitter_1754,SARCASM twitter_1755,NOT_SARCASM twitter_1756,SARCASM twitter_1757,NOT_SARCASM twitter_1758,SARCASM twitter_1759,SARCASM twitter_1760,SARCASM twitter_1761,SARCASM twitter_1762,SARCASM twitter_1763,NOT_SARCASM twitter_1764,NOT_SARCASM twitter_1765,SARCASM twitter_1766,SARCASM twitter_1767,NOT_SARCASM twitter_1768,SARCASM twitter_1769,SARCASM twitter_1770,SARCASM twitter_1771,SARCASM twitter_1772,SARCASM twitter_1773,NOT_SARCASM twitter_1774,SARCASM twitter_1775,NOT_SARCASM twitter_1776,SARCASM twitter_1777,NOT_SARCASM twitter_1778,SARCASM twitter_1779,NOT_SARCASM twitter_1780,SARCASM twitter_1781,NOT_SARCASM twitter_1782,SARCASM twitter_1783,SARCASM twitter_1784,SARCASM twitter_1785,SARCASM twitter_1786,SARCASM twitter_1787,NOT_SARCASM twitter_1788,SARCASM twitter_1789,NOT_SARCASM twitter_1790,SARCASM twitter_1791,NOT_SARCASM twitter_1792,SARCASM twitter_1793,NOT_SARCASM twitter_1794,SARCASM twitter_1795,SARCASM twitter_1796,NOT_SARCASM twitter_1797,SARCASM twitter_1798,NOT_SARCASM twitter_1799,NOT_SARCASM twitter_1800,NOT_SARCASM BERT Sentiment Analysis to Detect Twitter Sarcasm 1. Project Team Member Y= Name: Zainal Hakim Y= NetID: zainalh2 2. Project Topic The project topic falls under the Text Classification Competition option. The main goals of this project: 1. To explore sentiment analysis using a state-of-the-art method 2. To beat the baseline score using the given training and sample datasets 3. Sentiment Classifier using BERT Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art pre-training Natural Language Processing (NLP) model developed by Google. In this project, sentiment analysis uses BERT to detect sarcasm in Twitter tweets. 4. Programming Language The project uses Python 3.8 programming language to implement software code. 5. Previous Experience with BERT I have no previous experience with BERT nor with Deep Learning. I have a little experience with the Python programming language and the Pandas library. Hint: Download the PDF files to view it 1. Project Result Summary The project score outperforms the baseline score on the project competition Leaderboard. - The project F1 score: 0.757905138339921 - The baseline F1 score: 0.723 2. Project Team Member Name: Zainal Hakim NetID: zainalh2 3. Project Topic The project topic falls under the Text Classification Competition option. The main goals of this project: 1. To explore sentiment analysis using a state-of-the-art method 2. To beat the baseline score using the given training and sample datasets 4. Sentiment Classifier using BERT Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art pre-training Natural Language Processing (NLP) model developed by Google. In this project, sentiment analysis uses BERT to detect sarcasm in Twitter tweets. 5. Programming Language & Library The project uses: - Python 3.8 programming language to implement software code. - Huggingface library: https://huggingface.co 6. Previous Experience with BERT I have no previous experience with BERT nor with Deep Learning. I have a little experience with the Python programming language and the Pandas library. 7. Important File(s) The FINAL trained models: 1. BERT LARGE uncased model: https://drive.google.com/file/d/1EMcBXsFPqOVg4w_-Nob4ebWA0qTr9SLQ/view?usp=sharing 2. BERT Base uncased model: https://drive.google.com/file/d/1--_k6QVpRIV3HtP-PzWjm9066ebtmA8S/view?usp=sharing The hyperparameters in my experiments are: - Learning rate: 2e-5 - Batch size: 5 (considering memory size) - Epochs: 4 iterations - Epsilon: 1e-8 - Random seed value: 17 8. Demo Demo video is available: - Here https://drive.google.com/file/d/1PAmInsMvXlgkB3jZFt9qRu-SbtsoIQBJ/view?usp=sharing - or here https://www.youtube.com/watch?v=PsYn2lUWpQg 9. Challenges To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project."
