project_url	file_name	file_text
https://github.com/rm36/CourseProject	ProjectProgress.pdf	Project Progress 1. Which tasks have been completed? Researching and parsing dataset - Done Implementing a sentiment analysis algorithm - 50% done Creating a website with a video playback and student commenting tool - 50% done Integrating the algorithm in the website - 50% done 2. Which tasks are pending? Training, iterating and fine-tuning - pending Creating a website with a video playback and student commenting tool - 50% left Integrating the algorithm in the website - 50% left Creating the presentation slides and demo 3. Are you facing any challenges? No unexpected challenges yet. Although I don't have much web experience so I'm learning a lot here :) 4. Plus anything specifically mentioned in the reviews to cover. Thank you for the reviews, I added the file with more details.
https://github.com/rm36/CourseProject	ProjectProposal.pdf	Abstract MOOCs will come with an integrated course chat, for a more integrated cohesive system where students may post questions in real time (as people sometimes do in twitch or youtube live), and they are linked to the video lecture's timestamp. This can help them communicate with other students, e.g. as a student, seeing a classmate's emoji could help them feel less isolated, feel understood and optionally reading through students' concerns about the material is enlightening, without having to open another tool. As a teacher, this can provide valuable feedback to improve. This project will analyze real-time video-synced students' comments on any given lecture to provide insight on which phrases are confusing, helpful or make students feel in any given way to incorporate into the current communication or future lectures. Project Proposal questions 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rodrigo Mendoza rm36@illinois.edu Only me 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Intelligent Learning Platform - Student Sentiment Analysis I'll implement a system that will let students comment at any time in the video and a system which analyzes the students' comments in real-time to provide sentiment analysis. I'll use word-based robust sentiment analysis tools for the analysis, and a web-based portal with the video playing for students at their own pace. 3. Briefly describe any datasets, algorithms or techniques you plan to use I'll use sentiment analysis datasets, which have positive or negative reviews associated with them and will use word-based sentiment analysis tools for analysis, then display the aggregated predicted strength of each comment to the teacher. I'm also planning to provide a way to get my own dataset info. 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? I will evaluate the system in two ways: 1. using a test portion of the dataset not used in training to measure how close the system predicts the comment individually, and 2. Testing the aggregated comments by doing a mix of different comments, all positive, all negative and half and half. I will use mostly python. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Researching and parsing dataset - 2 hours Implementing a sentiment analysis algorithm - 6 hours Training, iterating and fine-tuning - 4 hours Creating a website with a video playback and student commenting tool - 8 hours Integrating the algorithm in the website - 3 hours Creating the presentation slides and demo - 2 hours
https://github.com/rm36/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Abstract: MOOCs will come with an integrated course chat, for a more integrated cohesive system where students may post questions in real time (as people sometimes do in twitch or youtube live), and they are linked to the video lecture's timestamp. This can help them communicate with other students, e.g. as a student, seeing a classmate's emoji could help them feel less isolated, feel understood and optionally reading through students concerns about the material is enlightening, without having to open another tool. As a teacher, this can provide valuable feedback to improve. This project will analyze real-time video-synced students' comments on any given lecture to provide insight on which phrases are confusing, helpful or make students feel in any given way to incorporate into the current communication or future lectures.
https://github.com/AlexZurski/CourseProject	Project Progress.docx	Alexander Zurawski azuraws2 CS 410 11/15/2021 CS410 Final Project Progress Report Which tasks have been completed? The main thing I did was rescope the project. Instead of probing a lot of free Coursera classes, I will just be using the 3 I have access to. Instead of designing a crawler, I manually collected the data myself. I have also chosen to design a scoring function to be able to test the effectiveness of the different iterations of the ranking function. To do this, I will use the weekly quiz questions to design a set of queries that will be split into a testing and training group. After which I will rank the documents that are relevant as 1 and not relevant as 0. The rest of the project will continue as normal. In terms of steps taken so far, the data has been collected. Which tasks are pending? Conversion of data files into a bag of words representation Create query set Rank the data files in terms of relevance to query Designing a ranking function Designing a scoring function Iterate on ranking function to optimize performance Are you facing any challenges? Originally, I thought the data would be in the PPT file format, but it is in PDF format. I think this will be overall easier for the conversion. Practical Statistical Learning is missing lectures from Week 5 onward. An entire week's worth of slides are all in one document rather than split into individual lecture videos. I may split them up or just keep them as is. I think keeping exclusively to the 3 classes I have could overfit the data. I hope splitting testing and training queries will avoid that. Response to specific feedback Meta-Reviewer: I chose the three courses (CS410: Text Information Systems, CS598: Foundations of Data Curation, and CS598: Practical Statistical Learning) because of representation. The courses have some level of overlap, such as the EM algorithm, while also having a lot of unique information. I think it is an ideal test set. Reviewer 1 & 4: I haven't thought of implementing a UI, I am focusing on the ranking and scoring. If I have time to build something pretty, I will. For now, it will run in the command line and output one or more data file. I can connect the data file to a URL that links to the lecture the data file is connected to. Reviewer 3: I expected most people doing this project to focus on the transcript, so in a 'real-world' roll out, I would expect to mix multiple techniques to find the best clips. Great suggestion on using the quizzes to find queries, I am going to use that!
https://github.com/AlexZurski/CourseProject	Project Progress.pdf	Alexander Zurawski azuraws2 CS 410 11/15/2021 CS410 Final Project Progress Report * Which tasks have been completed? The main thing I did was rescope the project. Instead of probing a lot of free Coursera classes, I will just be using the 3 I have access to. Instead of designing a crawler, I manually collected the data myself. I have also chosen to design a scoring function to be able to test the effectiveness of the different iterations of the ranking function. To do this, I will use the weekly quiz questions to design a set of queries that will be split into a testing and training group. After which I will rank the documents that are relevant as 1 and not relevant as 0. The rest of the project will continue as normal. In terms of steps taken so far, the data has been collected. * Which tasks are pending? o Conversion of data files into a bag of words representation o Create query set o Rank the data files in terms of relevance to query o Designing a ranking function o Designing a scoring function o Iterate on ranking function to optimize performance * Are you facing any challenges? o Originally, I thought the data would be in the PPT file format, but it is in PDF format. I think this will be overall easier for the conversion. o Practical Statistical Learning is missing lectures from Week 5 onward. An entire week's worth of slides are all in one document rather than split into individual lecture videos. I may split them up or just keep them as is. o I think keeping exclusively to the 3 classes I have could overfit the data. I hope splitting testing and training queries will avoid that. * Response to specific feedback Meta-Reviewer: I chose the three courses (CS410: Text Information Systems, CS598: Foundations of Data Curation, and CS598: Practical Statistical Learning) because of representation. The courses have some level of overlap, such as the EM algorithm, while also having a lot of unique information. I think it is an ideal test set. Reviewer 1 & 4: I haven't thought of implementing a UI, I am focusing on the ranking and scoring. If I have time to build something pretty, I will. For now, it will run in the command line and output one or more data file. I can connect the data file to a URL that links to the lecture the data file is connected to. Reviewer 3: I expected most people doing this project to focus on the transcript, so in a 'real-world' roll out, I would expect to mix multiple techniques to find the best clips. Great suggestion on using the quizzes to find queries, I am going to use that!
https://github.com/AlexZurski/CourseProject	Proposal.docx	I, Alexander Zurawski (azuraws2), am doing the project solo, and will be the captain. My chosen topic is Intelligent Learning Platform, specifically Video Segment Search. I have found this to be a problem often myself. I will see a topic in my notes and have to search an entire week's lecture videos to find the relevant 5-minute segment. The designed system will be a search engine that will find relevant text data based on a query, which is a large topic of discussion in the first half of the course. The datasets I will be using are the three sets of lecture videos I have access to: CS410: Text Information Systems, CS598: Foundations of Data Curation, and CS: Practical Statistical Learning. I will be using a crawler to find webpages and I will use a ranking function such as BM25 to evaluate results. I will plan on using free Coursera courses to test the efficacy of the built system. To be more specific, I will make dummy accounts, enroll in 3-4 free courses ranging from very similar, 4 computer science courses, to not similar, 4 courses of 4 different subjects, and run the program. Since I won't be able to get a full understanding of each free course I enroll in, I will have to make ad hoc judgements. I plan on working in Python. The tasks of the projects are as follows: Build a crawler that starts at a user's Coursera home page and can find every web page tied to a lecture Use the crawler to extract the power-point / slides for each lecture Convert the power-point / slide of each lecture into text data Use a ranking algorithm to retrieve the 'best' lecture videos Stretch goals if the above is not 20 hours: Recommend the time stamp that the lecture slide appears Train, test, and tune on larger data sets to see the effects of scalability
https://github.com/AlexZurski/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mnhuoi/CourseProject	progress_report.pdf	team m22 project coordinator/leader Michael Neill Hartman mnh5@illinois.edu Progress Report custom scraper / information aggregator The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? I have generated prototypes for a few typical use-cases: * I can now query if items are in stock at amazon or newegg given the url of the item * Partial: I can now pull the top 10 news headlines from google news * Partial: I can now pull crypto prices and exchange rates from coinmarketcap * Partial: I can now list all images contained on a site, with some sites * Partial: I have started prototyping the configuration file format * Partial: I have started prototyping the HTML/Javascript interface 2) Which tasks are pending? * pulling crypto prices and exchange rates from sites other than coinmarketcap * pulling the top 10 news headlines for news sites other than google news * General list all images contained on a site * Finalized configuration file format * Working HTML/Javascript interface 3) Are you facing any challenges? Realizing that many sites interpret an HTTP GET request from a browser differently than a plain HTTP GET request and identifying how to make them the same took some time. I have been having a hard time attempting to make generic implementations across multiple sites. It is not very difficult to generate a scraping and parsing method for each site, but more difficult to generate one that works across dissimilar sites. While I don't need to create fully generic implementations to fulfill my use cases, it is only natural to look for ways to do so while prototyping. Some sites use content delivery networks with very odd resource names. This causes a lot of problems trying to identify all the images on a site. Amazon suggests similar items when an item is out of stock. This makes it more difficult to determine if an item is in stock or not. This took some time to resolve. Many of the sites which have crypto prices and exchange rates make parsing the values extremely difficult, but offer an api. I have only found one site that I could reasonably create a method to parse the values from.
https://github.com/mnhuoi/CourseProject	Project_Proposal_and_Team_Formation_m22_custom_scraper_information_aggregator.pdf	team m22 project coordinator/leader Michael Neill Hartman mnh5@illinois.edu Free Topics What is your free topic? custom scraper / information aggregator Please give a detailed description. What is the task? People spend a lot of time opening a bunch of different websites, reading them looking for specific data. This proposal is for a custom scraper / information aggregator that will scrape target websites for specific info, adding the results to a single all in one webpage. It will take a collection of urls each with its own set of rules or targets and use them to populate a correlated div on the the one_page. Additional features that I might have time to add: * exporting results as datafile * monitor for changes, track changes on subsequent pulls * automatically place new changes at the top of the page * build api to access to allow interacting with result contents programmatically * advanced analysis on the scraped content * comparison between results from different sites using configured rules/comparators Why is it important or interesting? I would use this myself. I am sure a lot of people would like to save time with such a tool. There are lots of very specific applications of this, but I have not found a general one. What is your planned approach? I will prototype with a few typical use-cases: * pulling stock or crypto prices and exchange rates * pulling the top 10 news headlines for a given news site * tracking if items are in stock at amazon or newegg * tracking all images contained on a site By implementing these use cases I expect to cover many possible similar use cases. What tools, systems or datasets are involved? I plan on wrapping the javascript in an html file that will be opened in a browser, that will read in a configuration file in the same directory as the html file. The configuration file will contain the url : rules/targets, and any other configurations What is the expected outcome? I expect that with 20+ hours I will have a working webpage that can be opened in a browser and work as intended with a given configuration of urls and rules/targets for at least two use cases. How are you going to evaluate your work? I will use the code and see if it works, if it gets to that point I will take suggestions for additional use-cases and test that it works with those as well. Which programming language do you plan to use? I will use javascript. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. As I am working as a solo team, I believe the core implementation of this custom scraper / information aggregator will easily take 20 hours. If it takes less time, I will continue adding additional features and testing additional use cases.
https://github.com/mnhuoi/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Hejingze/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mebespencer/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mthatt/CourseProject	Proposal.pdf	Track: Free Topics Them: Free Topics Title: NLP Analysis of Cryptocurrency Group: The project will be completed by me, Mihir Thatte (Net ID: mthatt2). Summary: The project aims to rank most relevant cryptocurrencies being discussed across a variety of forums using sentiment analysis. It will be built mainly in python with a React/JS front end. The project will build upon my existing code which collects counts of how many times currencies were mentioned. The web scraping, front end development, designing and implementing a sentiment analysis + recommender system, and Docker packaging is expected to take at least 20 hours.
https://github.com/mthatt/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/johnkz2/CourseProject	410ProjectProposal.docx	Project Team Members: Captain - John Zhou (netid: johnkz2) My free topic is sentiment analysis of NBA twitter posts. The task is to categorize tweets based on teams/players and then use sentiment analysis to see if the public opinion of them is positive or negative. I'm interested in this task because I enjoy following basketball and I think basketball fans on twitter have pretty strong views and opinions on their favorite teams/player. My planned approach is to gather tweets on a given keyword using the twitter API and then run sentiment analysis to determine the trends behind those tweets. I want to visualize the sentiment in some manner but I'm still not sure how I should approach that maybe through a webapp using React. The expected outcome is that if teams are winning and doing well they will have a general positive sentiment, while the opposite is expected for teams that are losing. I think popular teams such as the Lakers will have very divisive tweets with a lot of fans but also haters with negative sentiment. Python Since I'm working alone I think the workload will easily be at least 20 hours. 5 hours - Getting familiar with twitter API and gathering tweets on team/players. 5 hours - Run sentiment analysis and evaluate results. 10 hours - Create webapp or visualization of data in user accessible manner.
https://github.com/johnkz2/CourseProject	CS410 Project Progress Report.pdf	CS410 Project Progress Report 1) Which tasks have been completed? First, I would like to mention that I have decided to focus the direction of my project a little bit more than my proposal explained. I want to use the twitter sentiment about teams to predict how well the team are currently playing for example their current standings. I can compare the results from the sentiment analysis to the actual standings of teams to see if I was able to accurately predict it. I have used with the Tweepy library in Python to grab tweets about different teams using team specific queries and cleaned up the tweets to make performing sentiment analysis easier. 2) Which tasks are pending? I have started doing some experimenting with sentiment analysis of a few tweets but this is still in progress. I am unsure if I will continue with the idea of making a webapp as it depends on the amount of time I have. If I don't make the webapp I will still make a report documenting the results of my work. 3) Are you facing any challenges? One of the limitations with the Tweepy API is that it only pulls tweets from the most recent 7 days. I've found that this does not really give a great indication of the overall performance of a team throughout the season so I may have to find another way to scrape tweets that is not limited to a recent timeframe.
https://github.com/johnkz2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Scott-Huang/CourseProject	Progress_Report.pdf	"Progress Report Group: NLP Member: Mingwei Huang (mingwei6) Nov 14th, 2021 Completed: * Text collection and text preprocessing * Topic modeling for the dataset * A word2vec model for word similarity (may not be used) * Algorithm for segmenting paragraphs based on the topic distribution of each sentence * Also, uncommented code is pushed to the repo Ongoing: * model evaluation (may need to manually label 10-20 lectures) * Evaluate how the model performs on segmenting paragraphs * (Maybe) evaluate topic modeling since I have multiple models (which needs word2vec) * May also incorporate the data here Future work: * Try paragraph segmentation by measuring sentence similarities with Sci-BERT * (Maybe) Summarize each paragraph * baseline - BM25 * evaluation (may need manual labeling too) * still Sci-BERT on keyword extraction Problem: * The bag-of-word performs too much worse than an average human, and the result is barely useful. * I suspect the usage of a transformer will not improve the performance a lot even though it can take the context into consideration. Paragraph segmentation requires an extremely fine understanding of the text which is even hard for an average human, and this level of difficulty is too hard for supervised learning, let alone the unsupervised one. Example result of segmentation of baseline approach using topic difference of each sentence: number segmented pieces: 15 segmented text: [""This lecture is about the syntagmatic relation discovery. An entropy. In this lecture, we're going to continue talking about word Association mining. In particular, we can talk about how to discover syntagmatic relations. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. By definition, Syntagmatic relations hold between words that have correlated Co occurrences. That means when we see one word occurs in the context, we tend to see the occurrence of the other word. So take a more specific example, here we can ask the question whenever eats occurs, but other words also tend to occur. Now looking at the sentence is on the left. We see some words that might occur together with eats like a cat, dog or fish is right."", 'But if I take them out and if you look at the right side where we only show eats and some other words. The question that is, can you predict what other words occur?', 'To the left or to the right.', 'Right, so this would force us to think about what other words are associated with eats.', ""If they are associated with eats, they tend to occur in the context of eats. So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment. Right here we can ask the question about the world W is present or absent in this segment. Now, what's interesting is that some words are actually easier for it, in other words. If you take a look at the three words shown here, meet, the and Unicorn. Which one do you think it is easier to predict? Now, if you think about it for a moment, you might conclude that. The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence. Unicorn is also relatively easy. Because Unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence."", ""But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately. But it may also not occur in the segment. So now let's start this problem more formally. Alright, so the problem can be formally defined as predicting the value of a binary random variable. Here we denoted by X sub w, w denotes a word. So this random variable is associated with precisely one word. When the value of the variable is 1, it means this word is present. When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1. Because a word is either present or absent in the segment. There's no other choice. So the intuition we discussed earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction would be."", ""Now the question is, how does one quantitatively measure the randomness of a random variable like X sub w, how in general, can we quantify the randomness of a variable? And that's why we need a measure called entropy. And this is a measure introduced in information theory to measure the randomness of X. There is also some connection with the information here, but that's beyond the scope of this course. So for our purpose we just treat the entropy function as a function defined on a random variable."", ""In this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values."", ""Now the function form looks like this. There's a sum over all the possible values for this random variable inside the sum, for each value we have a product of the probability that the random variable equals this value and log of this probability. And note that there is also an negative sign there. Now, entropy in general is not negative and that can be mathematically proved. So if we expand this sum will see the equation looks like a second one I explicitly plugged in the two values zero and one. And sometimes when we have 0 log of 0, we would generally find that as zero because log of 0 is undefined. So this is the entropy function and this function will give a different value for different distributions of this random variable. And this clear it clearly depends on the probability that the random variable taking a value of one or zero. If we plotted his function against the probability that the random variable is equal to 1 and then the function looks like this. At the two ends, That means when the probability of X = 1 is very small or very large, then the entropy function has a lower value when it's .5 in the middle that it reaches the maximum. Now, if we plot the function against the probability that the X is taking a value of 0 and the function would show exactly the same curve here. And you can imagine why and so that's because the two probabilities are symmetric and completely symmetric. So an interesting question."", 'You could think about in general here is for what kind of X? Does the entropy reached maximum or minimum and we can in particular think about some special cases.', ""For example, in one case we might have a random variable that always takes the value of one, the probability is one or there is a random variable that Is equally likely taking a value of 1 or 0. In this case, the probability that X = 1 is .5. Now, which one has a higher entropy? It's easier to look at the problem by thinking of simple example. Using coin tossing, so when we think about the random experiment like a tossing a coin, it gives us a random variable that can represent the result. It can be head or tail, so we can define a random variable X sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing. So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head hotel equally likely, so the two probabilities would be, 1/2 right so both will have both equal to 1/2. Another extreme case is completely biased coin, where the coin always shows up as head, so it's a completely biased coin."", ""Now let's think about the entropies in the two cases, and if you plug in these values you can see the entropies, would be as follows for a fair coin"", ""we see the entropy reaches its maximum, that's one. For the completely biased coin we see is 0 and that intuitively makes a lot of sense because a fair coin is most difficult to predict whereas a completely biased coin is very easy to predict that we can always say it's a head because it is a head all the time so they can be shown on the curve as follows. So the fair coin corresponds to the middle point, or it's very uncertain. The completely biased coin corresponds to the end point. We have a probability of 1.0 and the entropy is 0."", ""So now let's see how we can use entropy for word prediction. Now the problem, let's think about our problem right, still predicted whether W is present or absolutely in this segment. Again, think about the three words. Particularly, think about their entropies. Now we can assume high entropy words are harder to predict. And so we will now have quantitative way to tell us which word is harder to predict. Now if you look at the three words, meat, the and Unicorn again."", ""An we clearly would expect the meat to have a high entropy, then the OR Unicorn. In fact, if you look at the entropy of the, It's close to 0, because it occurs everywhere. So, it's like a completed biased coin, therefore the entropy is 0.""]"
https://github.com/Scott-Huang/CourseProject	Project_Proposal.pdf	Project Proposal Team NLP 1. Name and NetID: Mingwei Huang (mingwei6) 2. Topic: Intelligent Learning Platform, the Smartmoocs problem, where I want to improve the topic labeling of summarizing the transcripts. This course also introduces text clustering and text categorization which is related to this problem. 3. Data, Algorithms/Techniques: The datasets are those transcripts and manually summarized related concepts of each lecture. I prefer some end-to-end model but it should be almost impossible to build one since I do not have much labeled data. So, instead, I will try some pipeline approach: segregate the texts first and then summarize them and it is at least doable in an unsupervised way. 4. See if the performance makes sense and is useful. The result is a little bit hard to be evaluated since there is no such correct answer nor a range of acceptable answers. But it is still possible to use the manually summarized related concepts of each lecture to get some rough precision and recall in this case, but the majority of evaluation would be done manually. And I will firstly start with a baseline approach using bag-of-words/n-grams to see if the result is at least working or not, and then develop/use some models to compare with the baseline performance. 5. I think the data collection & preprocessing, implementation of the baseline approach along with the evaluation will take around 8-10 hours. The remaining time will be spent to develop/explore models that can achieve some SOTA result, or at least obviously improve the performance.
https://github.com/Scott-Huang/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/gsklyr/CourseProject	Progres Report.pdf	UIUC | CS410 | Fall 2021 | Project Progress Report Team: Gleb Sklyr Student ID: gsklyr2 System Extension :: NLTK 15th November 2021 Completed Tasks * Final testing corpus selection - 1,500 product reviews with 3 data columns and 2 target columns: The dataset includes 1-5 star rating, review title, and review text while the goal is to create a positive/negative/neutral sentiment column as well as a review subject (specific complaint possibly) column. * Familiarity with the toolkit, its available libraries. * Basic setup code to preprocess the dataset into quality tokens using NLTK only. * Code/tests/documentation of the preprocessing work. Pending Tasks * Implement several appropriate sentiment analysis strategies on the clean, tokenized data. * Select several comparison metrics. * Analyze the strength and weaknesses of each model. * Testing, documentation. * Package and propose an addition to the NLTK development community. Challenges * Still, the main part of the project is to be completed. * Only ground prep work is complete. Additional Comments * Someone pointed out in the reviews that I had my estimated work sum up to less than 20 hours. I meant to add hours to tasks 1 and 3 (1 hour each).
https://github.com/gsklyr/CourseProject	Project proposal.pdf	"UIUC CS410 Fall 2021 Final Project Proposal Team: Gleb Sklyr Student ID: gsklyr2 System Extension::NLTK OVERVIEW Developed 20 years ago, NLTK continues to evolve as a suite of libraries that support natural language processing techniques discussed in class, includes wrapper code for 3rd party libraries, and a growing support community. Being open-source, free, platform inclusive, and thoroughly documented, NLTK serves as a great starting point for people interested in NLP, and supports developers and researchers worldwide. Supporting this tool is beneficial for the global advancement of NLP. GOALS 1. The main goal for this project will be to extend the sentiment analysis capability of NLTK. 2. Specifically, I would like to add a module for feature-based classification (e.g. using the Customer Product Reviews). This feature is listed under ""Next Steps"" for sentiment analysis wiki of NLTK's public Github account. 3. Several corpi are available to verify the added feature. I will probably be fitting my project to a specific topic (most likely product reviews) and so the assessment of my work will have to use an appropriate dataset. SPECIFICATIONS - Programming Language: Python 3 - Development Guidelines: https://github.com/nltk/nltk/blob/develop/CONTRIBUTING.md MILESTONES (approx. 20 hours) - [4 hours] Corpus selection: finding an annotated dataset which I will be using throughout the process to develop and evaluate my code. - [9 hours] Functional code development: implementation of sentiment analysis algorithms on top of the existing NLTK libraries. - [5 hours] Non-functional code: setting up tests, unit tests, and evaluation metrics as well as documentation and report writing. https://github.com/gsklyr/CourseProject.git"
https://github.com/gsklyr/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/TakumiDawn/Congress-Financial-Disclosures-Sentiment-Analysis-Project	progressReport.pdf	1) Progress made thus far: I have completed the first two tasks, with a small portion of the second part data visualization to be improved. a. senatestockwatcher(https://senatestockwatcher.com/api) and housestockwatcher(https://housestockwatcher.com/api) APIs designed by Tim Carambat (https://ko-fi.com/rambat),get the disclosed trading data from the APIs, clean and pre-process the data into tabular forms b. make data visualization and Sentiment Analysis about their buy or sale actions in different industries or the whole market 2) Remaining tasks c. get the S&P 500 and Nasdaq index data from Yahoo Finance API (https://www.yahoofinanceapi.com/) d. correlates them by various similarity matrices, make analysis about the disclosed information and the market index data e. conclude the bullish or bearish trends about the disclosed trades in different periods 3) Any challenges/issues being faced Figuring out how to use the API to retrieve the data is more challenging and took more time than I expected. Also, I will assume the pending tasks take more time than expected because cleaning data after retrieval is not an easy task. However, it's quite promising to me to see how the results will be useful.
https://github.com/TakumiDawn/Congress-Financial-Disclosures-Sentiment-Analysis-Project	proposal.docx	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. The project team will be a team of just myself, and I will be the captain. NetId: feiyang3, Name: Takumi Li What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems, or datasets are involved? What is the expected outcome? How are you going to evaluate your work? The topic of the project is Congress Financial Disclosures Sentiment Analysis. I was inspired by the news that ""Four senators sold stocks before coronavirus threat crashed market"" (https://thehill.com/homenews/senate/488593-four-senators-sold-stocks-before-coronavirus-threat-crashed-market), I would like to develop a tool to extract useful information from trades made by senators and representatives, to improve the data transparency into Congress and make insider trading less likely. It's crucially important because it helps the public to understand if and how politicians benefit from the information they know before the public, which is the purpose of financial disclosures. I plan to take advantage of the senatestockwatcher(https://senatestockwatcher.com/api) and housestockwatcher(https://housestockwatcher.com/api) APIs designed by Tim Carambat (https://ko-fi.com/rambat), get the disclosed trading data from the APIs and pre-process the data into tabular forms, make data visualization and Sentiment Analysis about their buy or sale actions in different industries or the whole market, get the S&P 500 and Nasdaq index data from Yahoo Finance API (https://www.yahoofinanceapi.com/), and correlates them by various similarity matrices. Due to the limits of the disclosed information (no specific trading price, volumes, and trading time, but only trading volume ranges and dates), I do not expect to develop trading algorithms based on disclosed information. However, I would expect to see the general bullish or bearish trends of the disclosed trading. To evaluate the work, I would like to consider the process of sentiment analysis about the trading actions and concluding the bullish or bearish trends of some specific periods (day/week/month) as the most important milestones. I do not consider finding out the correlation between the disclosed information and the S&P 500 and Nasdaq as an evaluation measure because there may or may not be an actual correlation. Finding the specific relationship would probably require extensive analysis and open research work. Which programming language do you plan to use? Python will be the primary programming language to be used, and JavaScript may also be used. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed and the estimated time cost for each task. Takumi Li, feiyang3, will complete all the following tasks. senatestockwatcher(https://senatestockwatcher.com/api) and housestockwatcher(https://housestockwatcher.com/api) APIs designed by Tim Carambat (https://ko-fi.com/rambat), get the disclosed trading data from the APIs, clean and pre-process the data into tabular forms hours make data visualization and Sentiment Analysis about their buy or sale actions in different industries or the whole market hours get the S&P 500 and Nasdaq index data from Yahoo Finance API (https://www.yahoofinanceapi.com/) hours correlates them by various similarity matrices, analyze the disclosed information and the market index data hours conclude the bullish or bearish trends about the disclosed trades in different periods 4 hours"
https://github.com/TakumiDawn/Congress-Financial-Disclosures-Sentiment-Analysis-Project	proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. The project team will be a team of just myself, and I will be the captain. NetId: feiyang3, Name: Takumi Li 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems, or datasets are involved? What is the expected outcome? How are you going to evaluate your work? The topic of the project is Congress Financial Disclosures Sentiment Analysis. I was inspired by the news that ""Four senators sold stocks before coronavirus threat crashed market"" (https://thehill.com/homenews/senate/488593-four-senators-sold-stocks-before- coronavirus-threat-crashed-market), I would like to develop a tool to extract useful information from trades made by senators and representatives, to improve the data transparency into Congress and make insider trading less likely. It's crucially important because it helps the public to understand if and how politicians benefit from the information they know before the public, which is the purpose of financial disclosures. I plan to take advantage of the senatestockwatcher(https://senatestockwatcher.com/api) and housestockwatcher(https://housestockwatcher.com/api) APIs designed by Tim Carambat (https://ko-fi.com/rambat), get the disclosed trading data from the APIs and pre-process the data into tabular forms, make data visualization and Sentiment Analysis about their buy or sale actions in different industries or the whole market, get the S&P 500 and Nasdaq index data from Yahoo Finance API (https://www.yahoofinanceapi.com/), and correlates them by various similarity matrices. Due to the limits of the disclosed information (no specific trading price, volumes, and trading time, but only trading volume ranges and dates), I do not expect to develop trading algorithms based on disclosed information. However, I would expect to see the general bullish or bearish trends of the disclosed trading. To evaluate the work, I would like to consider the process of sentiment analysis about the trading actions and concluding the bullish or bearish trends of some specific periods (day/week/month) as the most important milestones. I do not consider finding out the correlation between the disclosed information and the S&P 500 and Nasdaq as an evaluation measure because there may or may not be an actual correlation. Finding the specific relationship would probably require extensive analysis and open research work. 3. Which programming language do you plan to use? Python will be the primary programming language to be used, and JavaScript may also be used. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed and the estimated time cost for each task. Takumi Li, feiyang3, will complete all the following tasks. a. senatestockwatcher(https://senatestockwatcher.com/api) and housestockwatcher(https://housestockwatcher.com/api) APIs designed by Tim Carambat (https://ko-fi.com/rambat), get the disclosed trading data from the APIs, clean and pre- process the data into tabular forms 3 hours b. make data visualization and Sentiment Analysis about their buy or sale actions in different industries or the whole market 4 hours c. get the S&P 500 and Nasdaq index data from Yahoo Finance API (https://www.yahoofinanceapi.com/) 3 hours d. correlates them by various similarity matrices, analyze the disclosed information and the market index data 4 hours e. conclude the bullish or bearish trends about the disclosed trades in different periods 4 hours"
https://github.com/TakumiDawn/Congress-Financial-Disclosures-Sentiment-Analysis-Project	README.md	Congress Financial Disclosures Sentiment Analysis Project Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Please refer to the proposal PDF to learn about this project.
https://github.com/realLongjiLi/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/abot3/CourseProject	AbotProgressReport.pdf	Course Project Report Text Clustering/Classification Plugin for Web Server Team: Abot, botelho3@illinois.edu Progress made thus far As of writing this report the first couple of tasks have been completed or are in progress. The dataset has been ingested and cleaned. The basic webserver setup and build scripts have been completed. Some basic web pages are being served but need to be hooked up to the dataset views. Preliminary research on the Clustering Algorithm has been done based on the Literature Review. The Gensim library will be used to build the corpus and tokenize/filter the dataset. Gensim's built in topic modeling algorithms will be evaluated, if they do not achieve the expected level of accuracy Labelled Latent Dirichlet Analysis (LLDA) may be used. Documentation hasn't been started and will be done at the end of the project. Task Hours Required Progress (Hrs) Finished Collection + Cleaning of Dataset 6 6 Y Design of Classification or Clustering Algorithm 10 2 N Coding of Plugin/Framework Implementing Algorithm in Flask 10 3 N Designing basic web-interface to Demo results on Example dataset. 6 3 N Documentation 2 0 N Remaining Tasks Task Hours Required Progress (Hrs) Remaining Design of Classification or Clustering Algorithm 10 2 8 Coding of Plugin/Framework Implementing Algorithm in Flask 10 3 7 Designing basic web-interface to Demo results on Example dataset. 6 3 3 Documentation 2 0 2 Any challenges/issues being faced A few risks have been identified at this point, related to the dataset quality and topic modeling algorithm. The dataset initially proposed as part of the project is a Kaggle dataset of recipe reviews. This dataset has been cleaned and ingested and is easy to work with in Pandas + Gensim. Unfortunately the volume of text in the recipe instructions and recipe description may not be detailed enough to produce high-quality clustering via LSA/LDA. If the desired result can't be achieved it may be possible to use a more advanced topic modeling algorithm e.g. Labelled Latent Dirichlet Analysis (LLDA). Alternatively the topic modeling algorithm could be applied to a dataset with a higher volume of text e.g. the BBC News Article dataset. This will require more work as the data ingestion component of the project will need to be rewritten. The topic modeling algorithm research has been done. Ideally LSA or LDA will be sufficient. Libraries exist implementing both algorithms that appear to be performant. However, based on the research, topic modeling algorithms on large corpuses can be computationally and memory intensive. Frameworks claim to implement tunable versions in which the hidden variables, and topic matrices can be computed with a fixed amount of resources. If not due to the hardware limitations of the developers in the group a creative solution may need to be found to reduce the system requirements. There are a number of workarounds but they all come with a penalty of time and complexity. For example comput constraints could be removed by renting compute time from Amazon Web Services or DigitalOcean, the corpus could be randomly sampled or batched to reduce the size of the matrices, or the libraries implemented LSA/LDA can be re-written or customized for better performance.
https://github.com/abot3/CourseProject	AbotProjectProposal.pdf	Text Clustering/Classification Plugin for Web Server Team: Abot Aaron Botelho (botelho3@illinois.edu) Importance With the growing popularity of websites serving user generated content, discovery of interesting content among the millions of hours of video or millions of pages of written posts and reviews becomes difficult. The growth of forums like Reddit, design websites like Pinterest, interest based websites like Cooking.com, or reviews on any of the major retailer websites like Amazon.com have been explosive. Many such large sites rely on their large community of users to categorize their content. E.g Reddit with subreddits, Amazon with product categories, Cooking.com by cuisine, ingredients or author. Smaller websites without the engineering resources to build such categorization systems are at a disadvantage. Without a large community of users to manually organize or classify contents into subcategories for users to navigate, or to tag/like content allowing classification models or user-similarity based recommender systems to be built. Operators of niche websites continue to lag behind their larger peers in offering these advanced features. Without such features users of smaller sites may become frustrated with the inconvenience and inability to find content of interest. A vicious cycle. To remedy this issue of lack of investment relative to their larger peers. Smaller developers and webmasters often rely on open source tools, e.g Apache Lucene or ElasticSearch to provide search functionality to their website. In my experience, smaller websites I use often have search capability thanks to Lucene or ElasticSearch, but are often lacking when it comes to content classification. Task Summary I plan to develop a content discovery and/or classification plugin for a common webserver to help remedy this functionality gap. As a proof of concept, I'll implement either content discovery/clustering or classification functionality for generic user generated documents e.g. forum threads, reviews, or recipes. Automated content discovery may be done using PLSA or clustering via a simple mixture model. Classification functionality may be implemented with a Naive Bayes classifier or Logistic Regression. Tools & Languages: Language: Python Tools: * Webserver - Flask (Python) * Numpy/Scipy * May use other numerics packages for e.g classification Datasets: * Dataset of user generated content (Kaggle, Scraped etc.) * Amazon reviews * Film/Game reviews * Interest forums posts * Recipes/Food blogs. * E.g. https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions/vers ion/2?select=RAW_recipes.csv Approach The first step is to find an example dataset on which to run the proposed clustering or classification algorithm. For classification there is the additional challenge of finding a dataset with tags or category labels included so that manual classification is not necessary. The dataset may also need to be cleaned (null removal, drop unnecessary columns) and split into a test, train set. Once the dataset for the demo has been selected, work can begin on coding the algorithm needed to perform the classification or clustering of the dataset. For classification, we propose Logistic Regression or a Naive Bayes Classifier. For clustering, PLSA or a Simple Mixture Model. For either task, the algorithms implemented will be the same as those covered in the course. The algorithm will be tested on the dataset until we are satisfied with the accuracy. With the final accurate model developed, the code needed to clean the dataset, read the dataset, initialize model parameters, iteratively or analytically solve the model, and define the result format will be formalized into a Flask plugin. The plugin is expected to consist of a few Python source files, some config files, an example dataset, and documentation. Lastly, in order to be able to demonstrate the results of our plugin a simple web interface will be developed to filter the dataset based on the clustering or classification results. This will likely be text-based and will focus on returning the most important results if the dataset is large. If the first 3 steps take longer than expected we may forgo the web-interface in favor of a command-line python interface. Outcome & Evaluation The expected result is to have a working Python plugin class that can be used to perform clustering/classification of user generated data, and provide the result to an application running the Flask web server. In the case of clustering the validity of the result can be judged manually. The top 10 or 20 clusters by importance or size look plausibly related when inspected by a human then the plugin will be considered accurate. If no pattern can be discerned then the plugin is not useful. For classification tasks the user-generated dataset will be annotated with a class label or tag. The standard precision and recall methods can be used to verify the accuracy of the plugin. The data will be split into a test and train set to prevent overfitting. In addition a human should manually spot-check some results as plausible. Workload Task Hours Hours Cumulative Collection + Cleaning of Dataset 6 6 Design of Classification or Clustering Algorithm 10 16 Coding of Plugin/Framework Implementing Algorithm in Flask 10 26 Designing basic web-interface to Demo results on Example dataset. 6 32 Documentation 2 34
https://github.com/abot3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Overview Installation Run export FLASK_APP=app/flask_app.py export FLASK_ENV=development cd src/server flask init-db flask run --eager-loading Project Layout
https://github.com/Anthony-S4/CourseProject	CS410 Project Progress Report.pdf	CS410 Project - Team Reddit Recommenders 1) Which tasks have been completed? We have divided the project tasks among group members. Kimberly and Miguel will work on scraping and indexing the text data from various subreddits. Ethan and Anthony will work on implementing the content-based filtering algorithm. Nico and Miguel will work on creating the user interface for the recommendation system. 2) Which tasks are pending? The group has many tasks pending, namely to implement the subgroups' tasks and begin to join the parts together. In our last group meeting we discussed specific logistical issues such as the number of subreddits to analyze and which similarity functions to use. We are now working on implementing our tasks in subgroups and communicating any difficulties with the team so that we can assist each other when needed. 3) Are you facing any challenges? The group's initial challenge was getting all of the group members to meet at the same time, but we have settled on future group meeting times to continue communicating our progress. We anticipate facing additional challenges, as a project of this scope is new to many of the group members.
https://github.com/Anthony-S4/CourseProject	CS410 Project Proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Reddit Recommenders Kimberly Martin kjmarti2@illinois.edu Ethan Choi ethansc2@illinois.edu Anthony Safo as95@illinois.edu (team coordinator) Nico Calderon nac7@illinois.edu Riano Miguel Paulo Garcia mriano2@illinois.edu 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen intelligent browsing and plan to extend the functionality of Reddit by scraping and ranking subreddit text to create a subreddit recommendation system. Subreddit recommendation is a problem because there are so many subreddits that it would be challenging for a user to find the most relevant subreddit pertaining to a search topic. This project relates to the class because it will use information retrieval techniques to add intelligence to the browsing capabilities of Reddit. 3. Briefly describe any datasets, algorithms or techniques you plan to use We plan to use text data from numerous subreddits as well as a ranking function like BM25 to produce a recommended subreddit based on the user's search topic. 4. How will you demonstrate that your approach will work as expected? We will demonstrate that the approach works as expected by showing the search results from our system and comparing it to the regular search results from Reddit. 5. Which programming language do you plan to use? We plan to use Python. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Scraping & indexing subreddit data 20hrs Implementing ranking function based on user search topic 30hrs Creation of Chrome extension 25hrs Testing & analyzing recommendation results 25hrs
https://github.com/Anthony-S4/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mrprice3/CourseProject	CS 410 Project Proposal.docx	Michael Price (team captain) - mrprice3@illinois.edu The topic I have chosen for my course project is to create a Chrome extension that will highlight the most important sections of a webpage. This is a problem because a lot of webpages contain tons of information that is not important to the users; it would be beneficial for users to see the important content of each webpage upon arrival. This is relevant to the class as I will need to use text analysis to determine what is important text. To do this, I must implement a recommender system to find and recommend the most important content of a webpage to the user. I will demonstrate that my extension will work by compiling several different webpages, determining their important sections, and making sure the highlighted sections match what I have selected. I will implement this Chrome extension with HTML, CSS, and JavaScript. I believe that this will take more than 20 hours to complete as it will require me to implement text analysis as well as all the front-end components.
https://github.com/mrprice3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/erikl2/CourseProject	Progress_Report.pdf	CS 410 Project Progress Report My name is Erik Larsen, my NetID is erikl2 and I am the team captain for team Wolf. Progress made: So far, I have explored different datasets and models that will allow me to summarize book reviews. I found some promising pieces in the HuggingFace Transformers library, including a dataset of Amazon reviews that I've begun experimenting with. Remaining tasks: I still need to get the summarizer model working and to implement the user interface (command line or web frontend). Challenges: I ran into some initial problems since developer keys are no longer being issued for the GoodReads API. It seems data for certain APIs will be difficult to obtain, so I am also trying to use already existing datasets.
https://github.com/erikl2/CourseProject	Project_Proposal.pdf	CS 410 Project Proposal My name is Erik Larsen, my NetID is erikl2 and I am the team captain for team Wolf. My free topic will be to create a book review summarizer and recommender system. I love books and I am always interested in finding good new books to read on various topics. I plan to use various APIs to access book and book review information, such as Google Books, GoodReads, and perhaps others such as Amazon and LitHub. I will initially develop a command line interface and then attempt to create a web frontend as well. I may utilize Huggingface's Transformers library to achieve the review summarization. I will evaluate my work by comparing review summaries to complete reviews for certain books, and comparing the recommender system to existing systems such as Amazon and GoodReads. I plan to use Python for the backend and Flask, and possibly JavaScript libraries such as React for the frontend. I estimate the following task workloads: - Testing API functionality for this purpose: 3+ hours - Writing backend code to summarize and recommend books: 15+ hours - Writing frontend code to provide a user interface: 10+ hours
https://github.com/erikl2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/godsell3/CourseProject	Progress Report.pdf	Progress Made: * Partitioned slides into portions by topic/content Remaining Tasks: * Access source code * Retrieve and rank relevant slides for slide portions * Link relevant slides to slide partitions * Create topic index and group slides by shared topic * Test the system and update accordingly Challenges/Issues Being Faced: * EducationalWeb was down, making it difficult to view its source code
https://github.com/godsell3/CourseProject	Project Proposal.pdf	Team Members Ani Pirosmanishvili, anip2 Audrey Godsell, godsell3 (captain) Olivia Park, opark3 Project Topic and Relevance Within Theme 2 (Intelligent Learning Platform), we have chosen the topic of extending the existing learning platform EducationalWeb. More specifically, we will improve the linking between slides on EducationalWeb. Currently, slides are linked by general topics covered in the slide. We aim to increase the functionality of EducationalWeb by allowing individual topics mentioned in bullet points on each slide to be connected to other slides that address and expand on those specific topics. Doing this solves an existing problem, because students using EducationalWeb to review CS 410 may gain clarity and deeper understanding of a difficult concept with access to related content that may not be immediately apparent by viewing only one bullet point in the current slide. Our proposed improvement to EducationalWeb would allow the student to easily find content related to this specific portion of the slide. This project relates to the theme of an Intelligent Learning Platform because it will add intelligence to an existing learning platform, EducationalWeb. Finding related content to specific concepts on class slides is a text retrieval problem, which is what this class is all about. Datasets, Algorithms, and Techniques We will reference EducationalWeb itself strongly for this project, but other datasets may also prove useful. The Project Topics document provided for the project links to a ConceptView spreadsheet complete with course topics and which week they appear in. This will be useful to use in order to provide a starting point for our text retrieval of certain subjects. We may use ranking functions like BM25 to determine which slides are relevant to the particular point in question. Validation and Coding Language We will demonstrate that relevant slides are linked conveniently to the bullet points or concepts included in the currently viewed slide. We plan to use python to implement our code. Workload Justification Reviewing EducationalWeb structure and base code (3~5 hours) Partitioning slides into portions by topic/content (10~12 hours) Retrieving and ranking relevant slides for slide portions (12~15 hours) Linking relevant slides to slide partitions (10~12 hours) Creating topic index and grouping slides by shared topic (10~12 hours) User testing/feedback and updating system accordingly (12~15 hours) Total time = 57~71 hours
https://github.com/godsell3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/avinashnathan1/CourseProject	CS 410 Final Project Progress Report.pdf	CS 410 Final Project Progress Report Predicting Song Genre Based On Lyrics 1. Team Members Aaron Aftab (aaronaa2) - Team Captain Avinash Nathan (anathan4) 2. Progress Made By leveraging the Spotify API (https://developer.spotify.com/documentation/), we have been able to pull data regarding songs and their genres. Since Spotify songs are not specifically associated with genres, we used the first-listed genre associated with the corresponding artist. In order to get lyrics data, we used the Genius API (https://docs.genius.com/) to access relevant lyrics pages and scrape the desired text. Now, we have a full dataset containing songs, their associated genre, and their lyrics. 3. Remaining Tasks To complete this project, we will need to create the pipeline for modifying and processing our lyrics data (stopwords, stemming, etc.) and then test different models with our separate lyrics data set to find which is the most accurate. Then, we will build the frontend allowing a user to input a set of lyrics and receive a genre categorization. 4. Issues Faced We have not encountered any major issues yet - data collection via the Spotify API has been smooth and we have been able to successfully generate a varied dataset, as we desired. We initially ran into an issue when we realized that the Spotify API did not provide lyrics, but we were able to find another API (Genius) to allow us to do this.
https://github.com/avinashnathan1/CourseProject	CS 410 Final Project Proposal.pdf	CS 410 Final Project Predicting Song Genre Based On Lyrics 1. Team Members Aaron Aftab (aaronaa2) - Team Captain Avinash Nathan (anathan4) 2. Topic The topic of our project is to develop an NLP model that will allow us to predict the genre of a given song based on its lyrics. Music can fall into a variety of genres, such as pop, hip-hop, R&B, country, EDM, and classical. Many music listeners prefer specific genres over others, and artists will often specialize in a certain genre in order to explore it deeply and deliver cohesive albums. A tool allowing users to query a specific song and evaluate its genre based on lyrical content would be useful for artists to validate that their lyrics match the paradigm for their desired genre and confirm that they have created music that will resonate with the target audience. We intend to gather song lyrics as well as their corresponding genre designations using Spotify's API - this will give us a training dataset that will allow us to associate specific genres with specific songs. We will then develop a user interface in which someone can input a text file containing song lyrics and receive a judgment of the genre that these lyrics are most strongly associated with. We will evaluate the success of our project by judging how well our model is able to predict the genre of a song compared with its designated genre on Spotify. 3. Programming Language We intend to use Python to develop both the model and the UI for this project. 4. Workload/Timeline Gathering song and genre data using Spotify API - 3 hrs/person Building and training prediction model - 6 hrs/person Developing UI and application for lyrical text input and corresponding judgment - 7 hrs/person Creating presentation/demonstration - 4 hrs/person
https://github.com/avinashnathan1/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/roy-liu/CourseProject	progressreport.md	Progress Report So far the project has proven to be somewhat more difficult than expected. I have accomplished the following so far: - Parse PDFs into a string - Hook up API for arXiv - Search for papers by author. - Basic UI to search and parse Due to challenges in running my own server to get elsevier running due to CORS issues I have opted to do the same thing but using arXiv papers only. This is because they have a comprehensive system for APIs. However, their return objects are in XML which takes time to convert into JSONs. Now that I have both summaries for all papers that an author has written and the data for any given PDF, I will start looking at the performance differences between algorithms and choose a more optimal one. That should finalize the project and should be on good track after. Challenges The biggest challenge is getting the chrome extension to work the way I want it due to having service workers running weirdly. After a few hours of messing around and debugging I have achieved my goal of getting it to work properly. What's next All that is left is implementing the actual algorithms and generating results. Since I already have the results, I need to compare the algorithms for relevance.
https://github.com/roy-liu/CourseProject	README.md	Progress Report So far the project has proven to be somewhat more difficult than expected. I have accomplished the following so far: - Parse PDFs into a string - Hook up API for arXiv - Search for papers by author. - Basic UI to search and parse Due to challenges in running my own server to get elsevier running due to CORS issues I have opted to do the same thing but using arXiv papers only. This is because they have a comprehensive system for APIs. However, their return objects are in XML which takes time to convert into JSONs. Now that I have both summaries for all papers that an author has written and the data for any given PDF, I will start looking at the performance differences between algorithms and choose a more optimal one. That should finalize the project and should be on good track after. Challenges The biggest challenge is getting the chrome extension to work the way I want it due to having service workers running weirdly. After a few hours of messing around and debugging I have achieved my goal of getting it to work properly. What's next All that is left is implementing the actual algorithms and generating results. Since I already have the results, I need to compare the algorithms for relevance. CourseProject Team Members Just me, royliu2! Topic I have chosen to create a chrome extension that helps you parse research papers and find other papers that the authors have authored in order of relevance. This is related to ranking relevance of different data. The datasets that I will be using will be coming through API keys from dev.elsevier.com. We will query for data that the author has published and rank based on paper relevance. I plan on using native javascript to parse the data, and will demonstrate that it works by finding individuals with papers published in different fields. If the relevance is valid, then fields differing from the current paper will not be selected. This will likely take more than 20 hours for the follwing reasons: - Parse PDFs through JS (3 hours) - Hook up API properly (2 hours) - Find relevant information and using the valid algiorithms (10 hours) - Testing different tradeoffs (2 hours) - Debugging/Optimizations (5 hours)
https://github.com/akashrpatel621/CourseProject	CS 410 Project Proposal.pdf	CS 410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Akash Patel - apate400 - Captain * Rushil Thakkar - rthakk20 * Ebubechukwu (George) Obetta 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? * Stock market sentimental analysis to predict stock appreciation and depreciation based on tweets on Twitter for a specific stock. This will be done by training a BERT model with test data with past tweets that are labeled as positive or negative based on the sentiment.. The tweets that will be run through the sentiment analysis will be gathered through the Twitter API. The overall goal of our project is to have a user type the name of a stock, and then we gather the current tweets for it and let them know if it has a positive or negative sentiment and if the stock will go up or down based on that. * We plan to use the stock market tweets dataset from the kaggle website here: https://www.kaggle.com/utkarshxy/stock-markettweets-lexicon-data. * We plan to use the following Twitter api endpoint to search for tweets about a stock: https://api.twitter.com/2/tweets/search/recent?query= * Once the model is trained then we will use it to classify and count the number of positive and negative tweets for a stock for a specific period of time. Then we will predict if the stock will appreciate or depreciate the next day based on this data. * To evaluate our work, we can predict the sentiment for a stock and see if the prediction we gave stands true or false. 3. Which programming language do you plan to use? * We plan to use HTML, CSS, and JavaScript for the front end and python for the backend and running the sentiment analysis models. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Learn BERT model & implement it - 30 hrs * Front End - 15 hrs * API Development & Integration - 10 hrs * Debugging & helping teammates with the problems in their respective tasks - 5 hrs
https://github.com/akashrpatel621/CourseProject	Project Progress Report.pdf	CS 410 Progress Report Task Completed Pending Challenges Front-end website Built initial layout of text, search bar and a 'call-to-action' button. Code in HTML and styled using bootstrap CSS. Make button press trigger a sequence of events that will start with a call to the sentimental analysis model and which eventually ends with output for the user. Making sure the layout was formatted correctly using the bootstrap CSS. Styling nested components of the webpage was difficult at first. API (internal) Built API with Python and Flask. Completed basic framework for API and tested simple route. Code in api.py Create a get request route that will call the sentimental analysis model and return if the stock price will increase or decrease. While building the API, I needed to understand how to integrate flask and python in the implementation. Backend model Got the training and testing data from kaggle, understood it, cleaned it up for training the model Applied for the Twitter API access and am waiting to hear back Code in model.py Have to train the model and test it to tune the parameters to improve accuracy Have to set up the pipeline for getting all tweets from Twitter API for that day Understanding the data was a challenge at beginning and then also researching about various sentimental analysis models and deciding one model to use was a challenge
https://github.com/akashrpatel621/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/eckels/CourseProject	CS_410_Project_Progress_Report.pdf	CS 410 Project Progress Report 1 CS 410 Project Progress Report Team Name: Money Evan Eckels (Captain) - eeckels2@illinois.edu Bhargav Yadavalli - bhargavy@illinois.edu Preston Chao - preston7@illinois.edu Chosen System: System-Extension LiveDataLab LiveDataLab Subtopic: More friendly UI (web-interface) 1) Which tasks have been completed? Initial Data Collection - 5 hours total Define success metrics - 1 hour Collecting initial metrics of current LiveDataLab UI - 4 hours total Test on existing users - 2 hours Test on new users - 2 hours User Research - 15 hours total Conduct user interviews - 4 hours total New users - 2 hours Existing users - 2 hours We have basically done some interviews and defined our goals for the project. We still need to do the second half of our user research where we synthesize all our information gathered from the interviews. We interviewed 5 people in each group (new and existing). 2) Which tasks are pending? User Research - 15 hours total Audience analysis - 2 hours Identify scenarios & use cases - 0.5 hours Create user flow diagrams - 0.5 hours Create wireframes - 8 hours total Desktop - 4 hours Mobile - 4 hours High-fidelity mockups - 18 hours total Create high-fidelity Desktop mockups - 12 hours Create high-fidelity Mobile mockups - 6 hours Test high-fidelity mockups - 8 hours total Collecting new success metrics of proposed LiveDataLab UI - 4 hours Test on new users - 2 hours Test on existing students - 2 hours Repeat for A/B testing to try different options - 4 hours for each different option (we will assume 2 options) Implement designs - 40 hours Implement designs in React - 30 hours Add CSS - 8 hours CS 410 Project Progress Report 2 Revise/create graphics - 2 hours Still have all our implementation tasks and mockup tasks left. We have only gotten through some of the user research tasks. 3) Are you facing any challenges? No issues so far, the only thing that might be useful is having a way to see how the current site performs with something like user metrics or analytics, but that likely doesn't exist.
https://github.com/eckels/CourseProject	CS_410_Project_Proposal.pdf	CS 410 Project Proposal 1 CS 410 Project Proposal Team Name: Money Evan Eckels (Captain) - eeckels2@illinois.edu Bhargav Yadavalli - bhargavy@illinois.edu Preston Chao - preston7@illinois.edu Chosen System: System-Extension LiveDataLab LiveDataLab Subtopic: More friendly UI (web-interface) Project Summary: For our project we are going to be improving the UI of LiveDataLab so that students are better able to find, create, and complete their MPs within the class of CS410. Right now, the UI gets the job done. But we could definitely make it more user friendly for students to complete their tasks faster and with less confusion. By making the UI more clear and user friendly, we anticipate that there will be a higher completion rate of MPs and also less questions to course staff when it comes to completing MPs or finding information within LiveDataLab. On top of that, it will be more delightful for students to look at when they work on MPs. An optimized UI would definitely improve LiveDataLabs effectiveness. Datasets, algorithms, and techniques: We do not plan to use any algorithms or datasets for this project since it focuses on UI/UX design. We will be using some design techniques to help us build the best UI for the project. These include: wireframes, high-fidelity mockups, A/B testing, user testing, user interviews, audience analysis, user flow diagrams, user research, information architecture analysis, interaction design, graphic design, typography design, mobile design, and color theory and design. We will also be using different coding techniques to code and implement our resulting designs in React on LiveDataLab. Most will revolve around front-end techniques such as CSS layout structuring, React component design, and other techniques that we hope to discover while we are completing our high-fidelity mockups that result from our user research and wireframes. Defining Success (improving a function): To prove that our work is actually improving LiveDataLab, we are going to be doing a lot of initial interviews with existing and new users (students) to identify pain points and any areas of confusion they may have when using LiveDataLab. We ultimately want to make it easier to complete tasks related to MPs for students, so we will be measuring base metrics for us to improve with our project. These metrics include success rate of completing a task like cloning an MP, the average time it takes to complete that task, overall happiness with the experience (survey-based). These metrics relate to if students can successfully clone MPs and how quickly. We will measure similar things for tasks like linking a GitHub, completion of non-coding MP's (validating datasets), and checking submission details and leaderboards. Once we have all these metrics, we can compare them to the results of the same experiments we run with our improved designs. If we find they improve the experience, we will implement the designs in React. Using LiveDataLab (utilizing the system): LiveDataLab's front end is made in React, so we will be contributing to the GitHub repo so that we can add our changes if necessary. Programming Languages: HTML, React, JavaScript, SCSS/SASS/CSS Anticipated Work (3 students): Initial Data Collection - 5 hours total Define success metrics - 1 hour Collecting initial metrics of current LiveDataLab UI - 4 hours total CS 410 Project Proposal 2 Test on existing users - 2 hours Test on new users - 2 hours User Research - 15 hours total Conduct user interviews - 4 hours total New users - 2 hours Existing users - 2 hours Audience analysis - 2 hours Identify scenarios & use cases - 0.5 hours Create user flow diagrams - 0.5 hours Create wireframes - 8 hours total Desktop - 4 hours Mobile - 4 hours High-fidelity mockups - 18 hours total Create high-fidelity Desktop mockups - 12 hours Create high-fidelity Mobile mockups - 6 hours Test high-fidelity mockups - 8 hours total Collecting new success metrics of proposed LiveDataLab UI - 4 hours Test on new users - 2 hours Test on existing students - 2 hours Repeat for A/B testing to try different options - 4 hours for each different option (we will assume 2 options) Implement designs - 40 hours Implement designs in React - 30 hours Add CSS - 8 hours Revise/create graphics - 2 hours Total: 86 hours of work
https://github.com/eckels/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mpolsgrove/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Jiaqicao257/team_aplus_project	cs410_progress_report.pdf	Progress Report At this time, we have scraped some reviews from Amazon and converted them into formats which are better for processing. We have compared different Natural Languages Processing packages used frequently in Python such as NLTK and SpaCy. After the comparison, we decided to use NLTK for POS tagging in our case. Also, we learned about sentiment analyzer in the NLTK package, and will use it to perform sentiment analysis. Moreover, we had several basic ideas in regards to the UI/UX design for the front-end part. Next week we will work on tokenization and stemming, as well as performing sentiment analysis and topic analysis on the reviews. We will visualize our user interface design and start to implement necessary elements. In the next two weeks, we will build the front-end component and make it as an extension to the browser. So far our work progresses without any challenge since the scraping technique was introduced in one of our previous MPs. Tokenization and analysis may also be simple to implement, but front-end implementation may be a tough task to do since we have not made browser extensions before. Also, we might need to figure out what python packages for sentiment analysis we should use in our case because different packages have different performance.
https://github.com/Jiaqicao257/team_aplus_project	CS410_project.pdf	"Project proposal Team members: Jiaqi Cao (jiaqic7), Naifu Zheng (naifuz2), Yige Feng (yigef2), Yuxin Wang (yuxinw5) Captain: Jiaqi Cao (jiaqic7) Topic chosen: Intelligence Browsing - Chrome extension to summarize Amazon product review using topic analysis and sentiment analysis. Why is it a problem? There are summaries about the product reviews on Amazon, but only for the products with more than hundreds of reviews. However, when browsing the product review summaries, there are some inaccuracies in the summaries. For example, there was a summary with the tag ""even though"", and another two redundant summaries of ""easy to set"" and ""easy to set up"", which can be simplified and improved. How does it relate to the theme and to the class? The summary tags are going to be simplified by using the text retrieval techniques that we learned in this class such as stemming. Dataset: Amazon Customer Reviews Dataset Algorithms: Tokenization, stemming, POS tagging Feasibility Description: Customers of Amazon provide comments, either positive or negative, on the products. Our extension performs topic analysis on the comments and captures keywords from them. For each key aspect, the extension performs sentiment analysis on the keywords and identifies whether customers like or dislike the product. Also, there might be a minority of people who make highly biased comments on the products with high subjectivity, and it is part of our goal to differentiate between objective and subjective comments. Programming languages: Javascript, Python Tasks: Module Estimated time Scrape and crawl product reviews from a target url ~10hrs Data cleaning ~5hrs Tokenization and stemming ~5hrs Topic analysis and sentiment analysis ~10hrs Make the module as an extension ~15hrs Frontend & user interface ~20hrs Integration of each parts ~15hrs"
https://github.com/Jiaqicao257/team_aplus_project	README.md	Team_aplus's Project for CS410 Team Members: Jiaqi Cao, Yuxin Wang, Naifu Zheng, Yige Feng
https://github.com/jennz0/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/GiuseppeIII/CS410CourseProject	Project Report.docx	Which tasks have been completed? No tasks have currently been completed. Which tasks are pending? All tasks are pending. Are you facing any challenges? Due to time constraints have not had the opportunity to start working on the project yet, but will have lots of time to make progress in the immediate future.
https://github.com/GiuseppeIII/CS410CourseProject	Proposal.docx	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. What system have you chosen? Which subtopic(s) under the system? If it is not listed above, how is it related to the class? Briefly describe any datasets, algorithms or techniques you plan to use If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Which programming language do you plan to use? Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Proposal Joseph Babbo, jbabbo2 ExpertSearch System; Automatically crawling faculty webpages Use the MP2 signup sheet as a dataset for directory page URLs alongside crawled miscellaneous university website pages, to get non-directory page URLs. This data would be used to create a university directory page classifier. Additionally, as time allows, a similar thing could be done with faculty webpage URL. These classifiers will then be wrapped in a frontend, where when the top page of a university is pointed to, the faculty and directory pages are correctly returned. To show that this new classifier works as expected our dataset will be split into training and test sets. Additionally, once the wrapper is completed the function can be demonstrated by simply pointing it to a university website. The code will not communicate with or utilize the ExpertSearch system. However, it will be written in a way where that it could hopefully in the future be easily integrated into the system. Python Scrapping Relevant Data - 5 hrs. Creating Functioning Classifiers to Identify Pages - 15 hrs. Wrapper to point at University URL and Return Correct Pages - 5 hrs.
https://github.com/GiuseppeIII/CS410CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/karthik63/ChromeEntity	Project Progress Report.pdf	Project Progress Report Karhik Venkat Ramanan kv16@illinois.edu 1) Which tasks have been completed? 1. I've Implemented a prototype for the extension. 2. I've identified the entity linker to use 3. I've identified the knowledge base to use 2) Which tasks are pending? 1. Including context from the web page using TF-IDF. 2. Completing the extension. 3. Including nearby entity view 3) Are you facing any challenges? I'll have to confirm with the TAs that I don't have to report any metrics for this project. This is more of a tool for search than a new IR model.
https://github.com/karthik63/ChromeEntity	Proposal.pdf	"CS410 Project Proposal Karthik Venkat Ramanan kv16@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? My netid is kv16. I'm the only member of my team. My team name is ""peak"". 2. What topic have you chosen? In my project, I aim to develop a chrome extension for easier search. I'd like to address the specific problem where, while browsing a website say a Youtube comment section, we encounter a word or phrase that we'd like to look up. This involves copy-pasting the word to a search engine and navigating to the article we want, which is often quite low on the search results because it is missing the specific context from the website we were browsing. My extension will use the local context of the word/phrase as well as the context of the web page to perform entity linking. It will then display the results of the lookup from a Knowledge Base / Search Engine in a window on the original web page without the user having to open a new tab. Another goal of mine is to encourage ""linked thinking"". My browser extension will also display neighboring entities and relations from the knowledge graph from which the entity is extracted and also use KG embeddings to perform a nearest-neighbor search to identify related entities. 3. Briefly describe any datasets, algorithms, or techniques you plan to use? I plan on using statistical IR techniques like BM25 and PL2. I'll be using knowledge bases like Wikidata and DBpedia to extract relevant information. I'll also be using KG embedding techniques like ComplEx, TransE, etc. 4. How will you demonstrate that your approach will work as expected? By delivering a working browser extension. 5. Which programming language do you plan to use? The back-end will be in Python using Django. The front end will be in Javascript and Bootstrap. 6. Please justify that the workload of your topic is at least 20*N hours The core tasks to be completed are- Task Time Commitment Implementing entity linking on the back-end 10 hours Implementing the front-end 10 hours Displaying graph data from the KG 10 hours KG nearest neighbors search 10 hours"
https://github.com/karthik63/ChromeEntity	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/shreyarao/CourseProject	CS 410 Final Project Proposal.docx	Intelligent Browsing Tool for News and Current Events Team members/Captain: Shreya Rao (surao2) The topic I have chosen is intelligent browsing for news websites. Often times, when someone visits a new site and reads an article, they want to do further research on the topic they just read about or get multiple perspectives on that topic. If there is no way for them to do that easily, they might never learn more about the topic other than the one article they read. I want to create a browser extension that allows the reader of an article to see the top related links to that topic and be able to click on them for more information. This is related to the class because it involves improving existing browsing functionality. It makes accessing information about current events easier and more impactful. I will demonstrate that the extension works by showing that its functionality works on a popular news site. Upon clicking on a news article the reader will be able to see links to the top search results for that topic. It will be a Chrome extension created with HTML, CSS, and Javascript. The back end will be the python web framework Flask. I plan to scrape the webpage and analyze its text for a main topic. Once the topic is identified I will use it as a query in a search engine and return the top results to show in the browser extension. The workload for this project is more than 20 hours of work because the planning (~5 hours), creation of back end logic (~8 hours), creation of front end (~5 hours), and testing (~8 hours) combined adds up to more than 20 hours.
https://github.com/shreyarao/CourseProject	CS 410 Project Progress Report.pdf	CS 410 Project Progress Report Title: Intelligent Browsing Tool for News and Current Events Which tasks have been completed? At this point, the main task I have completed in this project is the planning and identification of which technologies to use in the project. Because I am making a browser extension, I completed tutorials on making a Chrome extension using HTML, CSS, and javascript. Additionally, I did a Flask tutorial to understand how I would build the framework for the python back end. Which tasks are pending? Now that I have the framework for how to build the extension, the next steps are to put everything together by fully developing the front and back end of the extension to meet my specific goals. Additionally, I need to test the extension on various news sites to make sure it works. Are you facing any challenges? The only challenge I am facing is the volume of development work ahead of me. However, I have the resources required to complete the project to the best of my ability.
https://github.com/shreyarao/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ryan-berry-72/CourseProject	progress_report.pdf	"Ryan Berry Nov 15th, 2021 CS410 - Text Information Systems Progress Report Tasks as originally stated: 1. Front-end: 5 hrs a. Webpage development: 5 hrs 2. Back-end: 19+ hrs a. API communication: 3 hrs b. Parameter generation: 6 hrs c. Sonic Pi code generation 10+ hrs Task Progress: 1. Which tasks have been completed? a. Most of my progress so far has been dedicated toward the largest task (2c) b. I've spend 7 hrs on task 2c and 1 hr on task 1a 2. Which tasks are pending? a. I have not begun progress on tasks 2a and 2b 3. Are you facing any challenges? a. My major challenge as of right now is figuring out how to leverage Sonic Pi to make musical beats that actually sound cool b. Learning Sonic Pi is similar to learning a new programming language i. It does have great documentation and sample code built-in to the UI, which is very helpful c. I am trying to construct a beat template i. Once I have the beat template completed, I will be able to plug in my interfacing parameters to dictate things like the beats per minute, the pitch of notes, and the sounds behind the instrument effects 4. Response to reviewer question(s) a. Are you planning to generate some test cases yourself in end to test the system? i. Yes - this is an important factor that I should focus on, so thank you for the suggestion ii. This project's success is heavily influenced by the end result (the musical output), so it will be very important for me to be constantly tweaking and testing against a defined set of text input iii. As I stated in my proposal, ""I love you"" and ""I hate you"" will be two of these test cases iv. I also plan to use some basic sentences like ""The man saw the dog chasing the boy around the playground"" as well as more complex writing like song lyrics 1. It will be interesting to compare how the beat behind certain songs can compare to the beat that this system will generate based on the song's lyrics"
https://github.com/ryan-berry-72/CourseProject	proposal.pdf	"Group Name: FB90 (Flyers by 90) Group Member(s): Ryan Berry (rpberry2) Group Captain: Ryan Berry Topic: Text to Music: Combining Sentiment Analysis with Sonic Pi Description: This project will explore the relationship between written text and music. Given a text input, the goal is to produce a musical output based on various interfacing parameters. These parameters, which will be mined from the text input, may include: positive/negative sentiment, fact/opinion sentiment, word length distribution, and POS-tagging. Once the parameters are derived, a ""musical output"" will be generated. The musical output will actually take on the form of a coding language/tool called Sonic Pi (https://sonic- pi.net/). Sonic Pi, which is a code-based music creation and performance tool, provides a powerful interface for generating music - think Python meets GarageBand. Upon completion of this project, a user will be able to navigate to a JavaScript-based webpage, input one or more sentences into a text box, and receive a block of code which can be copied/pasted into Sonic Pi for listening. On the server side, a Python-based, Flask API will receive the text input, derive the interfacing parameters using Python NLP libraries, then generate and respond with executable Sonic Pi code. Success of this project can be measured by how unique and relevant the musical beats are in conjunction with the text input. This is an empirical analysis, for which the user will have to decide how well the generated beats represent the input text. E.g. does the sentence, ""I love you"" produce a distinct and meaningful beat compared to the sentence ""I hate you""? Programming Languages: Python, JavaScript, Sonic Pi Workload Justification: 1. Front-end: 5 hrs a. Webpage development: 5 hrs 2. Back-end: 19+ hrs a. API communication: 3 hrs b. Parameter generation: 6 hrs c. Sonic Pi code generation 10+ hrs"
https://github.com/ryan-berry-72/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/diane630/CourseProject	Progress Report.pdf	"Progress Report [Free Topic] Search Engine for Movie Adaptations of Books 1. Which tasks have been completed? a Filter unrelated information and crawl useful data from IMDB and Good . Reads: for movies: url, title, writers, short description, top cast, storyline for books: url, title, writers, description, characters b. Install Elasticsearch with Docker, set up container, store movie and book documents in database and create index c. Writing testing queries using Elasticsearch Query DSL, especially ""query string"" for retrieving search results and ""more like this query"" for retrieving similar documents, use Kibana dev tool to view query result 2. Which tasks are pending? a. Build the front end, plan to use Flask templates for simplicity b. Enhance the accuracy of recommendation system by customizing ranking of ""more like this query"" c. Evaluate retrieving results against a test set with known book-to-movie relationship 3. Are you facing any challenges? a. Docker and ElasticSearch are new to me. So, I spent some time learning about configurations and Elasticsearch DSL. b. There's no publicly available book-to-movie adaptation dataset. I have to crawl data from either user-created IMDB lists or IMDB search results with keyword ""adaptation"". Data cleaning and manual labelling is required."
https://github.com/diane630/CourseProject	Project Proposal.pdf	Project Proposal [Free Topic] Search Engine for Movie Adaptations of Books 1. Team Daiyun Xu (net id: daiyunx2) - working individually 2. Description My topic is a search engine for book-to-movie or movie-to-book adaptations. This is interesting in that it is highly possible that people who like a movie will also want to read the book the movie is based on, and vice versa. So, the basic idea is building a recommendation system on top of a classic search engine to recommend books to a movie query or movies to a book query. For datasets, I will crawl movie data from IMDB and book data from Good Reads. Depending on the complexity of data cleaning, I may or may not filter out those original non-adapted movies/books since they are not of interest in this project. Then, I can borrow some ideas from ExpertSearch System to build a classic search engine, which allows users to choose either movie or book as the query category. Lastly, BM25 or other content-based similarity algorithms can be used to find the top relevant documents from the other category for recommendation. My expected outcome is when querying a novel-adapted movie, at the very least the original novel should show up among the top few recommendations. Other valid recommendations would be books from the same series/ themes or books written by the same writer, etc. To evaluate it's working, I can test it against a dataset with known book-to-movie relationships. 3. Languages Python, Javascript, HTML/CSS 4. Workload estimation 1. crawl data from IMDB and Good Reads and data cleaning (5~10h) 2. build the movie/book search engine based on the idea of ExpertSearch System (5~10h) 3. recommendation system implementation after finishing my technology review on the same topic (5~10h) 4. performance review and documentation (5~10h) Total workload should be at least 20 hours.
https://github.com/diane630/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/IEnjoyEatingCookies/CourseProject	Progress Report.pdf	1) Which tasks have been completed? We have mostly completed task 1) Web scraping data/course from Coursera. We used beautiful soup to pull the html files, grab links and headers and parse to create a list of all the Coursera pages (including videos). As well, we parse through the pages to grab the actual document from the pages. We are also working on task 2) Search Engine Algorithm , using the documents (reading pages only currently) and using base bm25 (changeable as we test other algorithms) as our search engine algorithm. 2) Which tasks are pending? We have not started on 3) Video Search Algorithm or 4) Recommender based Algorithm. For 3) we are planning on similarly using a base bm25 model to parse through the video contents (since the transcript is available). We have not really thought much about 4) yet. 3) Are you facing any challenges? One of the biggest challenges is actually implementing the algorithms and connecting it with an interface which we will have to use later on in our final product
https://github.com/IEnjoyEatingCookies/CourseProject	Project Proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Daniel Zhang - dlzhang2 Shrey Patel - shreysp3 Apaar Bhatnagar - apaarb2 Sarang Mohaniraj - sarangm2 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have decided to create a search engine for Coursera to allow students to search through an entire course about topics/key words. This will allow students to quickly find the materials (text and video) they need for a specific topic they want to learn about. As well, it allows students to go to the part of videos that are most relevant to the topic they want to know more about. This relates to the theme since it helps students quickly find learning materials and relates to the class since we are creating a search engine. 3. Briefly describe any datasets, algorithms or techniques you plan to use Coursera will be searched as the dataset. We will use ranking functions and/or pagerank for text retrieval and possibly a recommender system for suggesting topics close to what a user asks for. 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? We will plan tests for our search engine (makeshift dataset or live coursera data). We are planning to use Python. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. We have 4 main components for our project: 1 - Web scraping data/course from Coursera 10 2 - Search Engine Algorithm 30 3 - Video Search Algorithm 20 4 - Recommender based Algorithm 30
https://github.com/IEnjoyEatingCookies/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tobyzl2/CourseProject	progress_report.pdf	Which tasks have been completed? I have completed the task of creating a dataset for the paper recommendation system. To do this, I utilized the Arxiv API to obtain the HTML formatted paper responses. I then parse these responses using beautiful soup and regex libraries. My current dataset contains 8 topics within the computer science field with about 100 papers for each. I've also started researching which model to use and am working on my implementation on Pytorch. I am currently using a transformer encoder architecture with a linear classifier and training the entire model from top to bottom. Which tasks are pending? Tasks that are still pending include keyword matching, writing the API, and evaluating the classification system. Are you facing any challenges? I'm currently trying to debug some issues with padding and masking since the inputs can be of various lengths with batching. I am also noticing that the loss is not converging smoothly when running it with a batch size of 1.
https://github.com/tobyzl2/CourseProject	proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Toby Liang NetId: tobyzl2 Captain: Toby Liang 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? For my free topic, I will be creating a recommendation system for research papers. In this task, the user will submit a research paper that interests them and the recommendation system will find a set of relevant papers from a corpus of research papers that is most relevant. This task is important because users who perform literature reviews need to find papers that are similar in topic and researchers can sometimes realize that a paper is not so relevant in the middle of reading a paper. Thus, this tool can save lots of time for researchers as it can automatically compare the content between different papers. My planned approach is to create a dataset using the arxiv api where the data will be the abstracts of the papers and the labels will be the arxiv category. Using this dataset, I will then train a classifier that takes in the paper abstract and predicts an arxiv category. At inference, papers with the same category will be considered for recommendation and ranked using term overlap. To write my classifier, I will utilize the Pytorch library. The expected outcome is a ranked listing of relevant papers that are predicted to be in the same arxiv category. To evaluate my work, I will create a test set to evaluate my classification model using precision, recall, and F1 score. Finally, I will create an API for this tool using Flask so that developers can have easy access to this tool. 3. Which programming language do you plan to use? I plan to use Python since many of the APIs and libraries I plan to use are written for Python. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Manual Data Collection: 4 Hours Writing Model and Training: 6 Hours Writing Keyword Matching and Ranking: 4 Hours System Evaluations: 3 Hours Writing Flask API: 3 Hours
https://github.com/tobyzl2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ardelysti/CourseProject	CS410 Progress Report.pdf	CS 410 Project Progress Report Team ayo: Ardel Kardi (pnkardi2), Putra Firmansyah (pdf2) Tasks: 1. Creating function for recommendations 2. Front-end design or whatever 3. Back-end for database to store user information 1) Which tasks have been completed? 1. We have gained the data that we need to perform content-based filtering for a movie recommender system. We have also started to work on designing the recommending algorithm. The algorithm will use the dataset we have found to find similarities to movies we will provide as input to the algorithm and will output the top 5 most similar movies as a recommendation. 2. Created Movies and User schema to store data, now just need to test out populating it for the actual implementation. 3. Base functionality templates created (pop up window to contain the recommender view) 2) Which tasks are pending: 1. Chrome extension manifest needs updating for potentially more dependencies 2. Creating the payload js script to be injected into the chrome webpage to feed the extension the movie details the user wants to bookmark and for our database 3. Need to create scripts to put inserts into the database when adding new movies into the bookmark extension and have it correspond with the user ID. 4. Still need to finish up the recommending algorithm. 3) Are you facing any challenges: 1. Inserting a button to IMDb's website to emulate the bookmarking functionality to feed the movie data into our database 2. Neither of us have any experience with chrome extensions so we stumble across a lot of the framework problem and structure
https://github.com/ardelysti/CourseProject	CS410 proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Ardel Kardi (pnkardi2) - Captain Putra Firmansyah (pdf2) 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen intelligent browsing and specifically, we will make an intelligent bookmark for movies that can be appended by many different other users (in real life would be like a friend group) and recommend movies based on the choices. The recommendation will be based on similarity between synopsis of movies and recommend a movie. It is a problem because currently, there are not many well-known technologies that can be used by many users on a common platform, in this case on chrome, to pick their favorite movies and recommend movies that they can add to the bookmark and, say, be watched on a movie night. It relates to the theme as it personalizes browsing for people so that they can find more relevant movies to watch. It relates to the class as we are implementing a recommendation system which is a push retrieval system which is a big section of the class. 3. Briefly describe any datasets, algorithms or techniques you plan to use The technique that we plan to use is content-based filtering. In order to do this, we plan to find similar movies based on the list of movies that we have. Specifically, we will use the movie synopsis which will act as our dataset that can be taken from IMDB and find similar movies that will be recommended to the bookmark. We plan to use a mixture model estimation with 4. How will you demonstrate that your approach will work as expected? We will generate offline metrics such as F-score or MAP from relevance judgements sessions where the movies returned as similar will be scored by the judges (the two of us) as either relevant or not-relevant. 5. Which programming language do you plan to use? We plan to use Python as that is what we are most familiar with and also how we have done all our MPs in. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Total: 48 hours * Create data scraper from all movies on IMDB: 2 hours * NLP and text representation: 10 hours * Word association mining and analysis: 8 hours * Topic mining and analysis: 3 hours * Opinion mining and sentiment analysis: 5 hours * Recommender system: 10 hours * Frontend: 10 hours
https://github.com/ardelysti/CourseProject	README.md	Movie bookmarking and recommendation system on synopsis similarity Intelligent Browsing By: Ardel Kardi (pnkardi2) and Putra Firmansyah (pdf2)
https://github.com/yuanchung1987/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/DrLucky2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/emeraldrains/CourseProject	ProgressReport.pdf	Progress Report Completed tasks: Working on the web-crawler Pending Tasks: Building the website Challenges: Tweaking the web-crawler and filtering the results with an appropriate clustering algorithm; finding relevant websites; displaying the results intuitively.
https://github.com/emeraldrains/CourseProject	ProjectProposal.pdf	"Team: Arete Members: Andrew Chandra; arc11 (arc11@illinois.edu) Topic: Vertical search with a focused crawler on topics relevant to climate change mitigation by public and private organizations. Task: Build a simplified search engine/crawler/web app that can gather and analyze current approaches to reducing carbon dioxide in the atmosphere; ranks most relevant scraped documents based on a search term (e.g., ""coal power plants"", ""solar energy percentage"", ""nuclear reactor"", ""carbontech""); produce data visualizations from scraped documents found on government and business websites; shows groupings of highly related documents; we can evaluate relevance with feedback. Technology: Python, Javascript Workload: 10h front-end, 20h+ back-end"
https://github.com/emeraldrains/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/SathishRama/CourseProject	Progress Report.pdf	CS410 Fall 2021 Project Progress @14-Nov-2021 Project Title : Retrieving & ranking customer communication/complaints for optimal routing & resolution of customer concerns # of team members : 1 Team Name : KMX Captain : Sathish Rama Team Member : Sathish Rama NetID : sbrama2 Status Update: Identified a public dataset on Kaggle with 5000+ customer complaints. There were lot of invalid rows so did the cleaning. The dataset doesn't have the complaints by product line so working on extracting product lines based on the notes in the complaint. Blockers : None Task Id Task Description Approx. Efforts in hours Status 1 Identify publicly available customer concerns dataset that can be used for this project 1 Completed 2 Define/design retrieval strategy - query terms & ranking rules based on the dataset ( say based on product lines, categories, define common query terms, based on product lines define ranking rules etc ) 3 In Progress 3 Dataset cleanup, feature engineering - remove duplicates, blanks, invalid rows etc 4 Completed 4 Develop the core retrieval & ranking algorithm using query terms and ranking rules. Expected Outcome: Retrieve top complaints based on query & ranking rules and display in console. 10 5 Evaluation : Label ( query & ranks ) set of complaints and use this as reference to evaluate the output on non labelled set of complaints 2 6 Project Documentation & Submission 4 7 Stretch Goal ( if time permits ) : Create a conversational complaint search & retrieval interface to key in query & display ranked results as response using IBM Watson chat or AWS tools. Host the core retrieval & ranking algorithm on cloud as function and involve it from IBM Watson chat. 10-20 (Optional )
https://github.com/SathishRama/CourseProject	Project_Proposal.docx	CS410 Fall 2021 Project Proposal Project Title : Retrieving & ranking customer communication/complaints for optimal routing & resolution of customer concerns # of team members : 1 Team Name : KMX Captain : Sathish Rama Team Member : Sathish Rama NetID : sbrama2 Free Topic: Retrieving & ranking customer concerns/complaints using text retrieval and ranking techniques in a enterprise setup to assign the customer concern/complaint for resolution to right customer care team/associate there by addressing the complaints in timely manner given customer care team capacity constraints. Across several organizations, one of the challenges is to optimize the customer care team productivity by identifying & resolving customer complaints in timely fashion. The challenges faced in today's setup are complaints don't get the needed attention and are routed to incorrect care teams or a customer needs to call multiple times and talk to several customer care associates to eventually get right resolution. Building a end to end complaints handling/management system needs large effort, so in this project I will focus on demonstrating retrieval using query terms and ranking complaints so top ranked complaints can be picked up first for resolution. Task Id Task Description Approx. Efforts in hours 1 Identify publicly available customer concerns dataset that can be used for this project 1 2 Define/design retrieval strategy - query terms & ranking rules based on the dataset ( say based on product lines, categories, define common query terms, based on product lines define ranking rules etc ) 3 3 Dataset cleanup, feature engineering - remove duplicates, blanks, invalid rows etc 4 4 Develop the core retrieval & ranking algorithm using query terms and ranking rules. Expected Outcome: Retrieve top complaints based on query & ranking rules and display in console. 10 5 Evaluation : Label ( query & ranks ) set of complaints and use this as reference to evaluate the output on non labelled set of complaints 2 6 Project Documentation & Submission 4 7 Stretch Goal ( if time permits ) : Create a conversational complaint search & retrieval interface to key in query & display ranked results as response using IBM Watson chat or AWS tools. Host the core retrieval & ranking algorithm on cloud as function and involve it from IBM Watson chat. 10-20 (Optional ) Programming Language : Core Retrieval & ranking : python, metapy Conversation AI : IBM Watson & AWS Lambda ( optional; this is a stretch goal )
https://github.com/SathishRama/CourseProject	README.md	CourseProject This project is part of the CS410 Fall21 course. Project Topic Retrieving & ranking customer communication/complaints for optimal routing & resolution of customer concerns
https://github.com/MinsooKim15/CS410_QuestionAnswering	CS 410 Project Proposal.pdf	"Proposal CS410 - team SM 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? - Building question and answering modules based on campuswire. - Campuswire search service only takes exact matched word for search. For example, if we type in ""homework"" we can't find the post talks about ""assignment"". New module will use word embedding which will catch the similarity of ""homework"" and ""assignment"". Furthermore, student might type in sentenced question to find posts that can answer their question. - Crawled data from capuswire will be used for question and answering. - Outcome will be simple web demo, user types in the question and get a list of answers. - 1) working demo of question and answering 2) compare query satisfaction with current search engince of campuswire. 3. Which programming language do you plan to use? - Python, Javascript 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. List of main tasks - Crawling the capuswire posts and answers. - 8H - Building question and answering module that retrieve most similar posts that answers the query(=question) - 25H or more - Building Web demo components(backend api and frontend web etc..) - 7H Name NetId Captain Minsoo Kim minsoo4 O Suejung Kang suejung2"
https://github.com/MinsooKim15/CS410_QuestionAnswering	CS410 Project Progress.pdf	"Building question and answering module for improving campuswire search 1. Which tasks have been completed? Tasks Notes Implement crawling function and cleaning up dataset We have finalized our crawling function and dataset. We fetched over 1500 posts from campuswire for development and testing. Code and data uploaded here Building question and answering module based on campuswire data We have completed building a question and answering module. It is based on Q-Q(User Question - Campuswire Questions) similarity matching which finds similar questions and gives answers from those posts. Code and the note are uploaded here. You can see the detailed notes about the flow of the executors and the sample results. 2. Which tasks are pending? Tasks Notes Improving QA module We are thinking about ways to improve the QA. Following is what we are considering * Using the answers of a post * Our QA is currently only using the first answer of a post. * There are several answers in a post. Moreover, answer threads are composed of the following questions and answers. * We are examining how to choose the answers from those threads. * Additional Training * We used the pre-trained Sentence Bert model for sentence embedding. * Considering additional training based on campus wire posts to improve the results. * The similarity between the document with long sentences and the one with short sentences. * (+) Additionally, we are thinking about which adapting BM25 ranking function for reranking the QA result will give the meaningful output. Run crawler on schedule to fetch newly added data. Currently crawler fetch data from scratch every time. We are considering making a QA demo so data should be up to date. Building web demo Making a simple web demo which users can type in the question and get top-N answers. It may directly call the rest-api which our QA module provides. 3. Are you facing any challenges? - No challenges until now. but we are considering several topics to improve the QA module(in the ""Improving QA module"" row). And those might be challenging topics for us to solve until the final submission."
https://github.com/MinsooKim15/CS410_QuestionAnswering	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. requirement python 3.8 pip install -r requirement.txt base sentence embedding model bert-base-nli-mean-tokens(https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens) Indexing Search With REST API query ``` python app.py -t query data/crawling/example.json Preprocess@34012[L]:ready and listening SentenceSplitter@34012[L]:ready and listening SentenceKoBART@34012[L]:ready and listening DocVecIndexer@34012[L]:ready and listening KeyValIndexer@34012[L]:ready and listening gateway@34012[L]:ready and listening Flow@34012[I]: Flow is ready to use!  Protocol: GRPC  Local access: 0.0.0.0:60808  Private network: 192.168.35.52:60808 Please type a sentence: assignment search from DocVectorIndexer Ta-DahdY""(r), here are what we found for: unable to find onboarding course 10 0(0.51). TA Office hours-21st August. Same here, I'm unable to join as well. : Below is the reply from MCS Support ""My apologies. TA Office Hours don't start until next week (first week of class). The Live Event you're referring to was created by mistake and has been deleted since. Sorry for the confusion."" 1(0.50). Can't find onboarding course. Log in to coursera organization account (UIUC) and you should see it under My courses : It turns out my account was not linked yet, but MCS helped resend the link to do so. 2(0.38). Conditional Entropy. I figured it out: I believe the Conditional Entropy slide is incorrect: the example conditional entropy calculation on this slide does not agree with the formula on the next slide. In other words, the log of each conditional probability needs to be multiplied by the joint probability NOT the conditional probability. (Or you could multiply the log of the conditional probability by the conditional probability and the marginal probability of the conditioning event, but that just seems complicated. : How can the probability of event A occurring be less than the probability of A occurring and B occurring? I'm not sure but take a look at: https://brilliant.org/wiki/conditional-probability-distribution/ 3(0.38). Can't sign up for LiveDataLab. Think this was answered in the #6. They should get it Monday. : LiveDataLab is a complicated cloud-based infrastructure, which we are still working on setting up. We hope to complete the setup ASAP and will post a note here once it's available. You don't need to do anything for now. LiveDataLab is used for MPs, and the hard deadline for MP1 is sometime in Oct. 4(0.37). Any luck or tips for installing Python3.5 on newer MacOS?. I have attempted these steps and am still unable to install and run python on my machine. I hav MacOS Big Sur. Is it possible for me to get help from a TA in office hours? If so, when would be the best time? : Paul, MeTA should work with Python 3.x.... did you try just using the default Python you have? Are you running into trouble installing MeTA then?"
https://github.com/ruikang2/BookmarkSearchEngine	Progress Report.docx	Progress Report Progress made thus far Worked on the front-end part of the chrome extension Remaining tasks Backend server - collecting the dataset based on user's bookmark collections Applying BM25 retrieval function Challenges/issues being faced The actual amount of work might be too much for a one-member group.
https://github.com/ruikang2/BookmarkSearchEngine	Proj Proposal.docx	Members: Ruikang Zhao (netid: ruikang2) Topic: Chrome extension: bookmark search engine Chrome users could search for websites among their saved bookmarks Dataset: users' bookmark set Demonstration: demo the usage of the extension with a demo Programming languages: Java, JavaScript Workload: algorithm design + Chrome extension implementation + testing >= 20hrs
https://github.com/ruikang2/BookmarkSearchEngine	README.md	Personal Bookmark Search Engine This is a Chrome extension that help Chrome users to find frequently used webpages among their saved bookmarks.
https://github.com/li-951/CourseProject	410 progress report.pdf	CS 410 Project Progress Report Topic: Data Set Creation Team: kms Kim Li -- kimli2 Kevin Tzeng -- ktzeng2 Shreyas Chandrashekaran -- svc3 1) Which tasks have been completed? a) We have built a basic web crawler that is able to parse through pages and find the respective sender, recipients, subject line, and email content. We are also able to format the raw data into a CSV file, where further sorting can be applied. 2) Which tasks are pending? a) Our current web crawler is too generic and is creating a data set that contains a lot of extraneous information, especially because there is no selection criteria other than the content being an email. There are many duplicate emails or emails with overlapping content, which we would ideally like to minimize in our actual data set. Our first goal is to organize all of our results into a smaller collection that still accurately represents all of the data that was initially scraped. From there, we can actually rank the emails based on importance for our actual data set. We also need to solidify how the dataset is being ranked, since there is currently no strict guideline for what makes an email important compared to spam. This specification will also help with developing algorithms in order to sort important emails from spam. 3) Are you facing any challenges? a) No major challenges, just working on limiting our data set, since the web crawler is returning a few thousand emails, which is too large and tedious for manual ranking.
https://github.com/li-951/CourseProject	410 project.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Kim Li -- kimli2 (captain) b. Kevin Tzeng -- ktzeng2 c. Shreyas Chandrashekaran -- svc3 2. What is the type of your project: Is it Data Set Creation or Leaderboard Competition Creation? a. Data Set Creation 3. If your project is Data Set Creation, what is the novelty of your data set as compared with all the existing data sets? Which of the existing datasets is the closet to yours? What new task can your new data set be used to evaluate? How do you plan to create the data set? a. Our dataset will be focusing on email importance and ranking them on a scale from most important to spam emails. There are a lot of methods for spam and fraud email detection which is helpful to remove unnecessary emails, but it is more beneficial for users to be able to sort through the most relevant email first, in order from most to least important. There exists email datasets already, but they are specific to a particular topic and all relevant to the user, meaning that they have already filtered out the unimportant content. Through our large range of emails, algorithms can use our data set to evaluate how good they are at analyzing the text data and highlighting key words and phrases that the email recipient may believe to be important. We plan to create the data set by combining a selection across multiple datasets, so that there is a large variety within our new dataset. We will be analyzing the number of recipients, word semantics, and other email specific attributes such as attachments or email signatures for ranking.
https://github.com/li-951/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team name: kms Team members: Kim Li (kimli2), Kevin Tzeng (ktzeng2), Shreyas Chandrashekaran (svc3)
https://github.com/VAkarsh20/CS410CourseProject	CS 410 - Project Progress Report.pdf	Team Members Captain: Akarsh Vankayalapati (akarshv2) Other Members: John Armgardt (johnra2), Ian Goodwin (img3), Spencer Sullivan-Hayes (sjs7), Aditya Mansharamani (adityam5) Which tasks have been completed? We have decided how the data will be obtained to receive the data. The two data sources will be the IMDb API and the Wikipedia API. The IMDb API will return characteristics about the movie and the Wikipedia API will return movie scores for Rotten Tomatoes and Metacritic. Also, we have started to work on the workflow of the project and what parts will work with each other. Furthermore, we have divided out each person's role on the team. The roles are as follows: * Akarsh Vankayalapati: Akarsh is acting as the project manager for the project as he is delegating tasks out to the other members, designing the project, working on all the progress reports, and making sure to help out with other members if any help is needed. Also, Akarsh will work on miscellaneous programming tasks when needed. * John Armgardt: John is in charge of making API calls to the IMDb API to get data regarding the movie title, genre, actors, and IMDb score and the Wikipedia API to scrape data regarding the movie's Rotten Tomatoes and Metacritic scores. * Aditya Mansharamani: Adithya is working on developing the model to make recommendations based on data received from the IMDb API and the Wikipedia API. This data will then be given to Ian to be displayed in the front-end. * Spencer Sullivan-Hayes: Spencer will work with John to create the script of what is needed from Wikipedia to extract the right data from the text. Also, Spencer will work with Adithya to help create the recommendation model. * Ian Goodwin: Ian is working on the front-end for the project as he makes sure work from the other members is being displayed how it should be. Ian has also worked on obtaining API keys, like the IMDb API key. Which tasks are pending? Tasks pending are finalizing the workflow of the project and finalizing which languages and technologies we will be using. Once this task is finalized, we can work on the software code submission of the project. Our group plans on working on this aspect in the coming week with some work over break as well. The final week will be testing the programming implementation, creating the documentation for the software code, and finalizing the tutorial presentation. Are you facing any challenges? The only challenge we have encountered has been finding a time where the entire group can meet to discuss their progress. As Thanksgiving break is coming soon, this problem should be mitigated as our group will be freer to meet and work on the project.
https://github.com/VAkarsh20/CS410CourseProject	CS 410 - Project Proposal.pdf	"Project Proposal 1. Team Members a. Captain: Akarsh Vankayalapati (akarshv2) b. Other Members: John Armgardt (johnra2), Ian Goodwin (img3), Spencer Sullivan-Hayes (sjs7), Aditya Mansharamani (adityam5) 2. The topic we chose was an Intelligent Browsing Chrome extension that based on searching for a movie on Google will return a list of movies based on similar critic site ratings. This extension will aggregate scores from Rotten Tomatoes, Metacritic, and IMDb to generate an overall rating for a movie. Then, the extension will find the movies with the closest overall rating to the searched movie. This is a problem because users often feel overwhelmed by the surplus of movies available to them, coined ""analysis by paralysis,"" so it will benefit users to have a way to have movies recommended to them based on searching for a specific movie. This relates to the theme and the class because it involves the pull method of browsing and document ranking based on closest matches from the searched movie and similar movies. 3. The databases we will be using are Imdb API and web scraping packages like BeautifulSoup and Selenium to get data. Doing this, we can dynamically extract data for movies to build our database, where data will be coded into an internal SQL database. The algorithm we will use is a derivation of the vector space model to compare movies with the closest movies in the vector space. 4. We will demonstrate our approach will work as expected by using a testing set that we will use to see if the movies returned were the same as what was expected and by gaining user feedback on how well they would use a tool like this to help them choose what movies to watch next. 5. The programming language we plan to use is Javascript with the possibility of having Python for back-end functionality. Because we will be developing a Chrome extension, we will be using Javascript because of the capabilities it has for this function. Moreover, because we have some members of the team who are more familiar with Python, we will try to incorporate their skills for more of the backend functions. 6. Our team is made up of 5 members, meaning there will be at least 100 hours of work to be done. Moreover, the main tasks to be completed will be creating the front-end interface and extension, creating the database and calling the APIs, creating the ranking function, and making sure all parts of the project are connected. Because of this, we will split the project into the following split: 1 member working on the front-end (20 hours), 1 member working on creating the database and calling the APIs (20 hours), 2 members working on the ranking function (40 hours), and 1 member working on connecting all the parts and testing (20 hours). Since there are many different parts involved with the project, we believe this is the best split and will take at least 100 hours to complete."
https://github.com/VAkarsh20/CS410CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/JiningSong/CourseProject	Progress report.pdf	1) Which tasks have been completed? * Research on Elasticsearch * Study on Elasticsearch applications * Investigated in current implementations for Elasticsearch 2) Which tasks are pending? * Research on other state-of-the-art ranking models * Implement them within Elasticsearch 3) Are you facing any challenges? * Not so far
https://github.com/JiningSong/CourseProject	proposal.pdf	Extend the Elasticsearch to support more ranking models Jining Song (captain) - jinings2@illinois.edu I've chosen to extend the Elasticsearch to support more ranking models and evaluate the performance of each model with the professor information dataset which we created at the beginning of the semester. Elasticsearch is one of the most popular and modern way to index large amount of document, enabling creating large scale searching applications. The service itself and the ranking models which I'll be working on are highly related to this class. Currently, the available similarity modules (ranking models) on Elasticsearch are BM25, DFR, DFI, IB, LM Dirichlet, and LM Jelinek Mercer. In this proposed study, I'll be implementing at least two more state-of-the-art ranking models through the 'scripted similarity' functionality provided by Elasticsearch. I'll use the professor information dataset to verify the integration and test the performance of the two new ranking models. I'll also compare the two new ranking models with the rest of pre-defined models. This project will be implemented in python or JavaScript and the expected workload for this project is 20 hours (This will be a single person team).
https://github.com/JiningSong/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ycchang4/CourseProject	Progress Report.pdf	"1.Progress Made 1. Data Collection We wrote a Python script to convert the given .vtt text data to a collection of documents where the transcript of each lecture is represented by a document. 2. PLSA We tried applying the PLSA algorithm to the collection of transcripts. We selected the top m topics for each transcript and chose the top n words to represent each topic. However, the results were not very good because it's difficult to identify concepts from the topic word distributions. This might also be due to the fact that the top topics for each document did not capture the concepts very well, since concepts like ""text analytics"" may occur only a few times in a transcript. We will later implement LDA to see if a background model will make any difference. 3. RAKE-NLTK We also tried using the RAKE-NLTK Python library to extract the keyphrases in each lecture transcript. Some results were good compared with the given human annotated dataset (""unigram language model"" appears in both our result and the human labeling for lecture 20 of text analytics course): However, it seems that besides noun phrases, RAKE-NLTK tends to capture subject-verb-object phrases like ""might bias towards using one topic"" in the first image. This leads to the lack of relevant concepts in the top keyphrases returned by the algorithm. 2. Remaining Tasks 1. Develop a model to predict the number of topics in a lecture. 2. Develop ways to interpret LDA word distributions. 3. Develop models to separate prerequisite and actual topic. 4. Write a python library that is easy to use 5. Comment the code. 6. Write the documentation 7. Resolve the challenges. 3.Challenges and Issues 1. How to interpret the topic word distributions in PLSA/LDA as concepts. 2. How to make good use of the timestamps in a .vtt file. Also, maybe explore the structural information in a lecture transcript (for example, words like ""first"", ""also"", ""next"" may suggest a switch in topic)."
https://github.com/ycchang4/CourseProject	Project_proposal.pdf	"Project Proposal Group Members: Grace Chang (ycchang4) - Captain Bohan Liu (bohan3) Yipeng Yang (yipengy2) Leo Yang (junjiey3) 1.What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Chosen Topic: Concept View. Given a MOOC lecture, preferably its text transcript. We want to identify the topics covered in the lecture by generating words and phrases to represent the topics. Meanwhile, our desired output would focus on the content of the lecture and exclude related, but uncovered concepts. The difficulty in this problem is to clearly identify all the concepts in a lecture and exclude the uncovered topics. The choice and representation of topics is an interesting problem to solve. This project has close relation to the theme of the class. It's within the scope of text analysis. 2. Briefly describe any datasets, algorithms or techniques you plan to use We plan to use the PLSA/LDA algorithm to extract major topics from the transcript and select words with the largest probabilities in each topic to represent the concepts. We also plan to devise an algorithm to distinguish between concepts taught in this class and those mentioned as prerequisites. It may be implemented by retrieving some background knowledge model from the Internet and then comparing the concepts extracted from the video with the background model. We can add the taught concepts in each video to a set/tree-like data structure so that for future videos in the same course we can easily identify the concepts that have already been taught. Another idea is that we can extract certain features (e.g. patterns like ""why do we use <concept>"") to to perform a binary classification between newly taught concepts and prerequisites. We will run our algorithm on the transcripts of the two moocs of CS410 and will use the given human annotated dataset for evaluation. 3. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? We will calculate the precision and recall of the top K results of our algorithm compared with the given dataset, assuming that the human annotations are reliable. We can compare the precision/recall of different techniques against the baseline, which is simply extracting the top concepts using LDA. Also, we can go through a specific video to demonstrate whether the results actually make sense from a student's perspective. We plan to use Python for this project. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Task & Estimated Time Cost: Implement LDA and fine tune parameters (5-10h) Find a background language model (5-10h) Devise algorithm to distinguish between newly taught concepts and prerequisites (25-30h) Devise algorithm to store previous concepts (10-15h) Fetch all transcripts and evaluate using given dataset (5-10h) Improve algorithm (20-30h)"
https://github.com/ycchang4/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/k72564/CourseProject	Project Progress Report.pdf	Project Progress Report Which Tasks Have Been Completed?  Built a list of the unique characteristics of faculty directory pages and faculty webpages URLs to help in identifying them  Implemented a function that classifies a page as directory vs. non-directory  Created a list of the most common attributes that do not appear in faculty directory pages and faculty webpages URLs to avoid crawling them  Implemented a function that classifies a URL as a faculty webpage or not Which Tasks are Pending?  Implement a crawler function that gets the list of the faculty webpages  Test the added functionality, which is a crawler that automatically crawls a faculty directory webpage  Create project documentation Any challenges faced? There have been no challenges faced.
https://github.com/k72564/CourseProject	Project Proposal.pdf	Project Proposal What are the names and NetIDs of all your team members? Who is the captain? Individual Project: Kamal Mahmood (mahmood3) - Captain What system have you chosen? Which subtopic(s) under the system? System: System Extension Sub-Topic: Automatically Crawling Faculty Webpages Briefly describe any datasets, algorithms or techniques you plan to use. I plan to use the dataset of the faculty directory pages and faculty webpage URLs from MP2. I plan to use the text classification technique. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? I plan to add a new function. I will demonstrate it by giving a tutorial presentation of the added functionality. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. I plan to make my crawler tool be its own system. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list main tasks to be completed and estimated time for each task. Estimated Main Tasks Estimated Time Faculty Directory Pages Text Classification 6 hours Faculty Webpage URLs Text Classification 5 hours Build Crawling Function 7 hours Progress Report 2 hours Testing Code 4 hours Project Documentation 4 hours Prepare and Edit Tutorial Presentation 4 hours Total: 32 hours
https://github.com/k72564/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/coronatsai/CourseProject	CS 410 Project Proposal.pdf	CS 410 Project Proposal Team Team Corona Tsai | cstsai2@illinois.edu Sentiment Analysis on Tweets about LoL In the past year, the popularity of online gaming seems to have risen in response to global conditions. Friends and family around me have gotten into playing games such as Valorant and League of Legends. In addition to playing for fun, there are professional teams and tournaments organized around these games. Recently, my brother has been keeping me in the loop about the LoL Worlds tournament and how there has been a lot of Twitter drama surrounding various players and teams who are participating. I thought it might be interesting to try to 'predict' the outcome of a game based on sentiment of tweets leading up to and during the game. I will be scraping tweets about a specific game and determining the performance of each player of the two teams through analyzing for positive/negative sentiments in tweets. Then, based on which team overall had the more positive sentiments, that would be the team predicted to win. In order to read tweets from the Twitter API, I will be leveraging a Python library called Tweepy. The dataset would be the tweets collected in the period of time surrounding and/or during a particular game that mention the game, the teams playing, and/or the players. The expected outcome would be to showcase how the viewers perceived each team/player's performances during a game and to hopefully see a correlation between positive sentiments and a team winning. Since I anticipate seeing a correlation between positive sentiments and winning a game, I will evaluate my work by comparing the ratio of positive/negative sentiments of each team and compare that ratio to the pre-match win/loss predictions (from Microsoft Sports). I plan to use Python. Workload Project Tasks Time estimate (hrs) Learn how to use Tweepy 2-3 Scraping Tweets & Cleaning data 5-8 Sentiment Analysis & Predictions 8-12 Troubleshooting & Testing 5-8 Demo & Final Report 4-8
https://github.com/coronatsai/CourseProject	CS410 Project Progress Report.pdf	CS410 Project Progress Report 1) Which tasks have been completed? Currently, the completed tasks include learning how to use praw, the reddit API, as well as scraping the reddit threads for data and cleaning the data. The original plan was to scrape and use tweets from Twitter, but due to some challenges (mentioned under q3), I switched over to the reddit platform instead. 2) Which tasks are pending? Pending tasks include data cleaning, performing sentiment analysis and testing. 3) Are you facing any challenges? One of the challenges I faced was due to language. As I was scraping tweets regarding an international competition featuring a number of non-English speaking nations, understandably, a large number of tweets were in foreign languages. While there are a number of translation apps out there (Google Translate, etc.) from experience, they are not very accurate. While removing all the 'foreign' language tweets from my data could be an option, instead, I opted to swap to scraping data from Reddit instead. While there are users across the world on that platform, the platform tend to feature mostly English discussions.
https://github.com/coronatsai/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/XueyangLiOSU/CourseProject	410 Project Proposal.pdf	"CS410 Course Project Proposal Team name: Noobs Team members: Jiarui Zhang (jiaruiz2), Xueyang Li (xl112)(C), Ziyang Zhan(ziyangz6) Topic: (Free topic) Sentiment analysis for Chatbot. We will implement sentiment analysis and train a simple Chatbot based on the result. This Chatbot targets daily conversation. Example scenario: ""I'm so upset"" -- ""I'm sorry to hear that, do you want some food?"". Task: The purpose of this project is to integrate what we've learned in this course and develop a simple Chatbot featuring the function of sentiment analysis, which has the ability to identify the user's sentiment based on user inputs and give a proper response that simulates real life chatting. Important/Interesting thing: Given the current COVID situation and the nature of Computer Science, it's not surprising that we sometimes feel lonelier and maybe more depressed than ever. We need friends we can talk to, someone we can relate to, someone that knows our emotions, interests and feelings. That someone isn't always available. Therefore, we hope to use the knowledge and techniques we've learned in this course, such as word identification and sentiment analysis, to build a chatbot that's able to provide a similar experience. Planned approach: We will first implement sentiment analysis based on daily chat conversations records from social media, such as twitter or facebook. After this, we will train multiple models by which we can predict the sentiment of a sentence. Then, we will use the model with the best performance to implement a simple chatbot which can detect the sentiment of a sentence and reply based on sentiment. Involved tools: We will use Python Notebook/Pycharm as our coding platform. The main libraries we will use are sklearn, NLTK, Metapy, operator, and numpy. The dataset we will use is ""Sentiment140 dataset with 1.6 million tweets"" from Kaggle. Expected outcome: A Chatbot with the ability to catch users' emotion and communicate with users (daily chatting). Evaluation principle: The rate of Chatbot correctly understanding users' emotion and giving proper corresponding responses. Programming language: Python Main tasks and time commitment: Environment setup 3 Data cleaning and processing 6 Multiple Model training 15 Model selection(based on F1, recall, precision....) 6 Classification and prediction with selected model 10 Classification and prediction test 6 Chatbot - front-end UI 18 Chatbot - backend interaction with trained model 20 Chatbot test 6 Report and documentation 6 Total 96* *Some advanced features may be removed if we do not have enough time, to better meet the 20*N hours, as we only have 3 team members."
https://github.com/XueyangLiOSU/CourseProject	ProjectProgressReport.pdf	CS410 - Project Progress Report Team Noobs - Allen Zhang (jiaruiz2), Xueyang Li(xl112)(C), Ziyang Zhan (ziyangz6) Main tasks and Time committed: Tasks Hours Status Environment setup 3 Completed Data cleaning and processing 6 Completed Multiple Model training 15 In Progress Model selection(based on F1, recall, precision....) 6 Not Started Classification and prediction with selected model 10 In Progress Classification and prediction test 6 Not Started Chatbot - front-end UI 18 In Progress Chatbot - backend interaction with trained model 20 In Progress Chatbot test 6 Not Started Report and documentation 6 Not Started Total 96* - 1. Which tasks have been completed? a. Environment setup: We set up our development environment, based on our own preferences. Two of our team members are using VS code and one of our members chose to use Jupyter Notebook. b. Data cleaning and processing: We cleaned the dataset by deleting meaningless null data records and some other useless characters in text such as urls and numbers. 2. Which tasks are pending? a. Multiple Model training: we are currently transforming the data and training our models. b. Model selection: Since the models are still being trained, the selection cannot be started right now. c. Classification and prediction with selected model: Not started, the reason is the same as 2a. d. Chatbot - front-end UI: We are currently designing and building the chatbot UI e. Chatbot - backend interaction with trained model: Since the models are still being trained, we are trying to first construct a working chatbot with dummy data f. Chatbot test: Since the Chatbot hasn't been fully established yet, we cannot test it right now. g. Report and documentation: This should be waited until all other tasks are finished. 3. Are you facing any challenges a. No prior experience in implementing Chatbot, which proves more challenging than originally thought b. Integration of different components of the project can be tricky c. Since we are trying to enable the auto-reply function in Chatbot, the knowledge base from one single database may not be enough. We may have to explore more datasets to accomplish this function.
https://github.com/XueyangLiOSU/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/deep-finders/CourseProject	Deep Finders - CS 410 Project Progress Report.pdf	Deep Finders - Progress report CS 410 Course Project | Theme: Intelligent Browsing Team Members Diego Carreno (lead) diegoac3 diegoac3@illinois.edu Jason Ho jasonho4 jasonho4@illinois.edu Chris Kabat cjkabat2 cjkabat2@illinois.edu Robbie Li robbiel2 robbiel2@illinois.edu Progress made thus far The team has done progress in a few areas for the project: * Research. We have analyzed several ways to parse HTML documents so that we can tokenize text content as text portions of different lengths (ie. paragraphs, individual sentences, group of sentences) in a robust manner for websites of multiple formats. This tokenization is needed to divide text into sub pieces to create a pseudo collection of documents from which we'll retrieve the most relevant pieces for a given user query. This research has pointed us to the use of Python-Goose as a way to reliably retrieve text data from websites of any format as long as the library can find article-looking text on the site. We have also done research to find best Javascript libraries to use to highlight passages as we show the user the most relevant results for their input query. This has pointed us to the use of mark.js as a highlighting library we are planning to use in the project. * Frontend: MVP of chrome extension. We have created the first version of the Chrome Extension for the project. The chrome extension is capable of searching for passages of text from a user query, and can also pass raw HTML data to a backend parsing and ranking system that has been deployed in an Azure function. It can also perform initial highlighting tasks using mark.js for test passages. * Backend: System and API design. We have designed the first high-level version of the system, including API schemas for the Chrome extension (FE) to communicate with the Azure function (BE), having the Chrome extension send raw HTML data to the parsing and ranking Azure function, and having the Azure function reply back to the Chrome extension with ranked text passages for a given user query. Query results are stored in an Azure Cosmos DB database to allow parameter tuning given feedback. * Backend: Parsing. We have implemented an Azure function in Python that utilizes Python-Goose to parse HTML text information into text passages of different length (eg. sentences, paragraphs, and groups of sentences). * Backend: Ranking. We have implemented an Azure function in Python that utilizes the rank-bm25 library to produce ranked passages of text for a given user query. * Backend: Parameter tuning. We have implemented a parameter tuning script to vary k1 and b parameters given the limited feedback data we have. Remaining tasks * Frontend: Highlight top ranked passages in websites. We are able to highlight text passages now on websites, but we still need to highlight the top ranked passages that are returned from our Azure BM25 ranking function. * Frontend / Backend: Testing in sites of different formats. We still need to systematically test our end to end solution across different websites to assess how robust it is and how well it behaves with different page formats. * Backend: Optimize BM25 parameters. We need to further optimize current BM25 parameters using the parameter tuning script in our backend and through executing a feedback collection workflow we plan to run internally within our 4 team members. * [Stretch] Backend / Frontend: Capture users' feedback on results to fine tune parameters and/or develop a different model. As a stretch goal we want to capture general feedback on results with every potential user to fine tune parameters. This might require modification of our parameter tuning script as well as implementing a mechanism in the UI to allow users to intuitively input their feedback to run Cranfield evaluations (or a similar method) to assess and fine tune results. Any challenges/issues being faced * Websites of different formats. Although more testing across websites of different formats is still needed, from initial tests we can see that being able to handle websites of different formats might pose a challenge since python goose might not always work well, in particular for websites that do not contain article-looking text information. * API connection. We had some challenges in integrating the Frontend (Chrome extension) with the Backend (Azure parsing and ranking functions) as calls to the Azure function would return errors due to an API schema mismatch. Upon performing tests we have found the issues and managed to solve them and our Frontend and Backend can now connect to each other.
https://github.com/deep-finders/CourseProject	Deep Finders - CS 410 Project Proposal.pdf	Deep Finders CS 410 Course Project | Theme: Intelligent Browsing Team Members Diego Carreno (lead) diegoac3 diegoac3@illinois.edu Jason Ho jasonho4 jasonho4@illinois.edu Chris Kabat cjkabat2 cjkabat2@illinois.edu Robbie Li robbiel2 robbiel2@illinois.edu Problem Statement and Relevance Search engines excel at surfacing results to users' queries. However, after the user has clicked into a returned result, they lose context on what parts of the document match their original query. Generally, users use the native browser find (i.e. CTRL + F or Cmd + F) and cycle through matched results to find relevant passages in the document. However, there are 2 downsides of this approach that can lead to user frustration. First, longer documents, such as technical documents, can provide too many matches than are useful to the user (ex. 100+), requiring the user to make multiple queries. Second, providing results based on exact matches could potentially miss on key pieces of information in the document that the user isn't aware of. This topic relates to the intelligent browsing theme because it seeks to provide additional intelligence to the user in their search experience after they have selected a document from the initial search results. It relates to topics covered in class because it is an information retrieval problem and presents an opportunity to leverage some of the techniques we learned in class to provide a better search experience. Project Goals Provide a mechanism for users to search over a document using BM25 by providing a ranked list of passages in the document that are most relevant to the user's query. The user will be able to click on each search result and be taken directly to that passage in the document. Technical Approach We plan to develop a Google Chrome extension that allows a user to enter in a query. The extension will do some preliminary parsing of the page (i.e., removing noise and HTML tags) and send the data to a backend service that will index the page, separate the page into separate passages, and use the BM25 algorithm to provide a list of ranked results. The Chrome extension will provide a UI for the user to select and navigate to the passages on the original document. A stretch goal for this project is to set up a feedback mechanism to inform us of the efficacy of our model and help to tune our model's parameters (ex. k and b values). We plan to use metapy or a similar library (ex. nltk, spaCy, etc.) to provide basic functionality such as stop tokenization, word removal, NGram analysis, etc. We will use Python for the backend and JavaScript (along with HTML and CSS) for the browser extension. We plan on hosting the backend on Microsoft Azure using Azure functions. Testing and Validation We will initially tune the model based on a selection of documents that we think are good use cases for this extension, and the 4 members will provide relevance feedback on these documents as training data. We will test and validate this extension by collecting feedback based on how a user interacts with the results, and asking fellow classmates to also use the extension to provide us with more data points. Workload Estimation Breakdown 80 hours (4 members, 20 hours each) broken down as: * Research - 10 hours * 10 hours to identify text retrieval approaches to use and for overall system design * Backend - 30 hours * 8 hours to set up database in cloud platform * 8 hours to implement ranking algorithm API * 8 hours to implement web scraper * 6 hours to implement feedback collection system * Frontend - 30 hours * 30 hours to implement chrome extension * QA and testing - 10 hours * 8 hours for overall QA and system evaluation * 2 hours tuning model assumptions (i.e., k and b values for BM25)
https://github.com/deep-finders/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/khylecalimlim/CourseProject	CS410_Project_Proposal.pdf	Name: Khyle Calimlim NetID: khylejc2 For the CS410 Project I (Khyle) will be the only member of the team, and so will be the captain. Also, my full name and NetID are listed above. The topic I chose is Intelligent browsing, and I plan on implementing searching any given web page using a common retrieval function like BM25. This is one of the given example projects, and, as stated in the project description, this extension solves the problem of modern web page find/search requiring exact keyword matches in order to give the user a successful search. This then obviously relates to the theme of the class because the entire first section of the class is about text retrieval and the algorithms (like BM25) that are applicable to solve this problem. The course content and the given project example already listed some of the things I plan to use in order to implement this project. One of the main things given already is the BM25 algorithm or any analogous retrieval function algorithm. The techniques I could use would include indexing the page. In order to demonstrate that my approach will work as expected, I will simply use my extension on a given web page using at least two queries. One query with an exact keyword match and another without an exact keyword match. This will then hopefully return similar results. The programming language that I plan to use is JavaScript since I intend on creating my extension for google chrome. Since I am working by myself the workload for this project needs to be 20 hours which I believe to be under the amount of time it will take me to code this project. I will need to do some research into how to properly create a google chrome extension, and then implement indexing the page as well as the retrieval function. Furthermore, I would also have to find some way to receive a user query, and then leverage my extension to display the results in some manner. Given that I'm working alone, I am willing to concede that this seems to be a lot of work, and I may not create the best working extension; however, I believe that I can accomplish everything that I've listed out. If things go poorly then I find that concessions can easily be made such as using a suboptimal but easier to code retrieval function, and having a very simplistic frontend/result display.
https://github.com/khylecalimlim/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yasiss/CourseProject-GoodreadsScraper	FinalProject_ProgressReport_GoodReadsScraper.pdf	"1 Yasaman Sabersheikh yasaman2@illinois.edu CS410 - Fall 2021 - Text Transformation Systems Final Project - 1 person team Project Progress Report Goodreads Scraper SUMMARY Goodreads is a free website on books that allows individuals to freely search for books and reviews. Users can sign up and generate library and reading lists. https://www.goodreads.com/ Finding books that are award winners are not easy on the website. Goodreads has a link for Reader's Choice awards but other than that it is hard to find other literary awards such as The Man Booker Prize or Giller Prize. In this project I intend to write a web scraper to find the books that award winners. The result can be used for book recommendation, book clubs or a blog post. PROJECT GOALS 1. Create a scraper to scrape the data in Goodreads website 2. Identify Award Winner books/authors SPECIFICATIONS * Python programming language for scripting * Selenium to retrieve the Html source code * BeautifulSoup to extract book-data from the Html source code 2 PROGRESS * Researched what is on the web, best practices * Learned Selenium and BeautifulSoup in detail * Accessed ""new releases"" webpage through Google Chrome * Using Selenium and Python Code to sign in to GoodReads website, by code, automatically. * Grab all the ""new releases"" for last 12 months and categorized their information in Dictionary and DataFrame. TASKS TO COMPLETE * Go Deeper in the tree and scrape information inside the URL of each book, looking for the word Prize or Award * 30% is complete CHALLENGES * As a one-person team, it was hard for me to start, as I could not brainstorm with anyone. I went through lots of materials to define my starting point. * I started big, I wanted to find all the prize winner books on the website and all the best reviews. As the data is in different places on the website and I was dealing with massive amount of data I realized, it is not a 20h project if I consider both best reviewed and prize winners. I narrowed down the scope and decided to start small and add to it, as professor suggested. So, I started with prize winners of the year. * When I was trying to scrape the html pages with BeautifulSoup, because of the structure of the pages and their html, I could not get the data I needed easily. It took me much longer than what I anticipated to get the data and load a Dictionary and DataFrame with it."
https://github.com/yasiss/CourseProject-GoodreadsScraper	FinalProject_Proposal_GoodReadsScraper.pdf	1 Yasaman Sabersheikh yasaman2@illinois.edu CS410 - Fall 2021 - Text Transformation Systems Final Project - 1 person team Goodreads Scraper OVERVIEW Goodreads is a free website on books that allows individuals to freely search for books and reviews. Users can sign up and generate library and reading lists. https://www.goodreads.com/ Finding books that are award winners are not easy on the website. Goodreads has a link for Reader's Choice awards but other than that it is hard to find other literary awards such as The Man Booker Prize or Giller Prize. In this project I intend to write a web scraper to find the books that award winners or have a high rating. The result can be used for book recommendation, book clubs or an annual blog post. GOALS 1. Create a scraper to scrape the data in Goodreads website 2. Can categorize books under the same reward category by year 3. Add the ratings to each book in rewards categories as well 4. Create a Category of highest rating books by year. e.g., 2021 rating 4 and above 5. Creating an HTML page to present the data SPECIFICATIONS * Python programming language for scripting * Selenium to retrieve the Html source code * BeautifulSoup to extract book-data from the Html source code
https://github.com/yasiss/CourseProject-GoodreadsScraper	README.md	CourseProject The initial set up of this repository was forked from CS410Assignments/CourseProject. This repository was created as place holder for UIUC CS 410 - Text Transformation Systems, Fall 2021, Final Project. I will write a web scaper with Python to scrape Goodreads website, using Selenium and BeautifulSoup
https://github.com/anumehaagrawal/CourseProject	Course Project Progress Report #1.pdf	Project Progress Report #1 Names & NetID Jhinuk Barman, jbarman2 anumeha2 Anumeha Agrawal Devang Aggarwal; devanga3 Team Captain: Anumeha Agrawal Main Task Time Cost Research stock websites to scrape 5 hours Collect web scraping data/urls 6 hours Scrape the websites 15 hours Analyze the scraped data 15 hours Gather our findings from the data 6 hours Create visualization in extension of our results 15 hours Run test cases on the chrome extension 5 hours Completed 1) Research Stock websites to scrape: We are scraping the CNN Finance website 2) Collect Web Scraping data/urls: We have scraped the top 10 most active stocks of the day using beautiful soup in python. 3) Analyze the scraped data: We have analyzed the data in a jupyter notebook with the help of pandas and matplotlib 4) Gather our findings from the data: We have analyzed the trends in the stocks in our jupyter notebook In Progress 1) Scrape the websites: We are working on scraping certain stock pages for companies itself so that when a user is on that page they can hover to our extension and find relevant values 2) Create Visualization in extension of our results: We are working on still linking our data to the extension through javascript and then visualizing the data in our extension. Currently, we have a brief skeleton of the extension ready. 3) Run test cases on extension: This is work in progress. We will do this once we have our extension fully up and running. Challenges The main challenge we are facing right now is how to read in our data into the extension and then visualize the data. We are looking at options and researching the best way to do this. We hope to achieve this by our next progress report.
https://github.com/anumehaagrawal/CourseProject	CS410 Project Proposal.pdf	Project Proposal Names & NetID Jhinuk Barman, jbarman2 anumeha2 Anumeha Agrawal Devang Aggarwal; devanga3 Team Captain: Anumeha Agrawal Theme Intelligent Browsing Topic Create a Google Chrome Extension Extension Idea Our goal is to create a Google Chrome extension which shows useful information for the top x trending stocks and cryptocurrencies for the day. Everyday the extension gets refreshed, before the market opens, with the latest trending for the day. We hope to achieve this by scraping web pages like Yahoo Finance, CNN Finance etc. We will then use this information that we have scraped to show useful data and visualizations like - latest news on that company, trading volumes, trading price, volatility etc. We hope to go one step deeper and look at the sentiment for the stocks (how many % of analysts think the stock should be held, bought and sold). Questions: 1. What system have you chosen? Which subtopic(s) under the system? a. If it is not listed above, how is it related to the class? We have chosen Intelligent Browsing and we will be developing a Chrome Extension. 2. Briefly describe any datasets, algorithms or techniques you plan to use Datasets/websites: https://www.cnbc.com/, moneycontrol, yahoo finance, the motley fool, stock APIs such as rapidAPI https://developer.chrome.com/docs/extensions/mv3/getstarted/ This is the basic tutorial that we would follow to develop the chrome extension https://usefulangle.com/post/339/chrome-extension-create-page-scraper - This is the technique that we would follow in order to develop a web scrapingh extension. We will first gather the URLs for different stocks from websites mentioned above. We will then analyse the elements on the pages to extract useful information from the DOM. We will then use this information to show important tables and visualizations about different stocks. As an extension to this, we will also attempt to find sentiments for the stocks given the data we have extracted. 3. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? a. In order to ensure our chrome extension runs as expected, we will run the extension on the chrome browser. We will also run some test cases to ensure that the values of our results are accurate according to our expectations. We plan to use testing libraries such as Selenium. 4. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly a. Our code will interact with the system by scraping data from various web pages and collecting key information on stock and cryptocurrency data. Using this data we will produce a simple, convenient and clear visual that shows the top trending stocks. We will use various metrics and generate a score from the articles to rank the top trending stocks. Users can simply download and click on the extension to view this daily information. 5. Which programming language do you plan to use? a. We plan to use JavaScript to develop the chrome extension 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Task Time Cost Research stock websites to scrape 5 hours Collect web scraping data/urls 6 hours Scrape the websites 15 hours Analyze the scraped data 15 hours Gather our findings from the data 6 hours Create visualization in extension of our results 15 hours Run test cases on the chrome extension 5 hours N = 3 group members Total estimated time: 67 hours ~ 20*N = 20*3 = 60 hours
https://github.com/anumehaagrawal/CourseProject	README.md	CS410 group project for JADE Project Name - Chrome extension for stocks Jhinuk Barman, jbarman2 Anumeha Agrawal anumeha2 Devang Aggarwal; devanga3
https://github.com/LongN22/CourseProject	Progress_Report.pdf	Members: -Thien Binh Dinh (tbdinh2) - Long Nguyen (lbn2) Progress Report 1. Progress made thus far - We have tested and deployed the chrome extension front-end, where users can paste the link in. - The program can scrape the data from the link, parse as txt files and be sent to our Python RestAPI. 2. Remaining Tasks - Build the vocabulary for good and negative reviews - Determine the best model to use to determine if the review is positive or negative toward the restaurant and if it is somewhat biased. - Implement our model as a REST API 3. Challenges - Design the front-end for users, as of now it is just a text box. We have little knowledge of html and css, so this is a big challenge for us. - We are thinking of using probability model to find the biased review, however we are still unsure how to use this method to find biased reviews.
https://github.com/LongN22/CourseProject	Project_Proposal.pdf	"Google Chrome Extension for Yelp Restaurant Reviews Goal Implement a chrome extension that uses sentiment analysis for Yelp restaurant reviews. It will help users summarize the overall reviews of the selected restaurant without having them go through the reviews. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Long Nguyen (lbn2) Member(s): Thien Binh Dinh (tbdinh2) Briefly describe any datasets, algorithms or techniques you plan to use We plan on taking the rule-based approach for natural language processing. We will define two lists of polarized words (eg. ""delicious"" represents good sentiment, ""gross"" represents bad sentiment) to represent each sentiment. Each individual review will either be marked as ""positive"", ""negative"", or ""neutral"" depending on the number of positive and negative words that appear in their review. The goal of the extension is to count the number of positive, negative, and neutral reviews based on this rule-based approach. How will you demonstrate that your approach will work as expected? We will navigate to any restaurant page on Yelp. Instead of reading the reviews, we would run the extension, which would list the number of positive, negative, and neutral reviews Which programming language do you plan to use? JavaScript, HTML, CSS Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Instead of splitting time, or tasks, we plan on working together vigorously through each task to maximize our efficiency. Task 1: Figure out the list of polarized words for both and positive sentiment Task 2: Implement techniques to scan through each review, searching for keywords. Also consider new rules like limiting the number of keywords a sentence can have. Task 3: Design basic UI for extension to present results of the search"
https://github.com/LongN22/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ttt-77/CourseProject	Project Progress Report.pdf	Which tasks have been completed? (a) We have finished the program to extract the title, the summary and the link of Google search results. These data is beneficial to analyze whether the specific search result is relative. (b) We have finished learning the basic knowledge of the web browser extension. Based on the tutorial, we have built a simple extension. The user can input relative topic keyword and irrelative topic keyword and the extension can read the information. This lays a foundation of the connection between front-end and back-end. Which tasks are pending? Because we have two people, next we will divide our project into two parts. One person is responsible for designing an algorithm for our task. Another person will setup the server and build up the internet communication between the extension and the server. Are you facing any challenges? Everything goes well. We have not met any challenge until now.
https://github.com/ttt-77/CourseProject	Proposal.pdf	"Optimize browsing result with samples from user Team members Kerui Zhu (captain), netid: keruiz2 Tengjun Jin, netid: tengjun2 Introduction We chose ""Intelligent Browsing"" as our topic. Because when we browse using the web search engines, the results usually contain some irrelevant information because of the ambiguity in the query. For example, when we search ""python"", it could mean either a kind of programming language or a kind of animal. We want to make the browsing more efficient by enabling users to select some wanted or unwanted results from the returned results on a page through clicking so that the extension can filter information based on the feedback. It is relevant to our class because we try to develop a text retrieval technique, which is a main focus of the class. Resources We plan to use some of the ""clicked query-document pairs"" datasets to build our dataset. This kind of datasets usually contain thousands of query-document pairs collected from web logs. In each sample, there is a query statement provided by the user, and a set of documents retrieved by the search engine based on the query, and each document has a ""clicked"" value showing how many times this document was clicked by the user who makes this query. Training and inference We plan to make use of the collected datasets to build our own dataset. First, we will cluster the queries based on the word in the query using the tf-idf algorithm. For each cluster, we collect the document with the top clicked value for each query and consider these documents as topically similar. Then, for any two clusters of queries, we combine the documents as the pool for retrieval. We sample two or three documents from each set as the pseudo user feedback and try to retrieve documents from the pool. The result will be evaluated both automatically and manually. To have a better demonstration, we will also use the extension in the browser in live and show the filtering result of our model. Technical skills We plan to build a chrome browser extension in this project. To do that, we need to use Python to analyze data in the backend and use html, CSS and JavaScript to show the frontend for the extension. Workload table Steps Time required Step 1: create some strategies to collect training data based on existing datasets. 15 hours Step 2: design a model to analyze the data 15 hours Step 3: train the model based on the collected data 20 hours Step 4: learn relevant knowledge about writing web browser extension 10 hours Step 5: setup the server and build up the internet communication between the extension and the server 15 hours Step 6: manually evaluate the result of the model 20 hours"
https://github.com/ttt-77/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. RetrieveMaster Kerui Zhu (Captain) Tengjun Jin Proposal https://github.com/ttt-77/CourseProject/blob/main/Proposal.pdf Project Process report https://github.com/ttt-77/CourseProject/blob/main/Project%20Progress%20Report.pdf
https://github.com/braydenturner/CourseProject	Bae Area Progress Report - CS410.pdf	"CS 410 Final Project Progress Report Team ""Bae Area"" Free Topic Improved Media Content Search Team Members Brayden Turner - brturne2@illinois.edu - Captain Joshua M. Smith - jms28@illinois.edu Progress Made So far, we have completed the following tasks: * Set up the environment and work through initial build issues: we are using the MeTAPy library for tokenization and n-gram creation, which requires an earlier version of Python, and we are committing code in Jupyter. We tested using an online collaboration notebook but abandoned this due to the size of our dataset. We have landed on self-hosting Jupyter notebooks on Python 3.6.8 and using git for syncing progress. * Dataset download: we found a data set of 10,000 top music artists and successfully implemented the Lyrics Genius API to download the lyrics for all songs by these artists. This code is using the Multiprocess library to speed up downloads. * Early tokenization and pre-processing: we have begun to implement MeTAPy, creating functions to process the lyrics in our dataset file. We have discovered that we may require some additional pre-processing prior to tokenization, and are currently working on the strategy here. Remaining Tasks The following core tasks remain: * Implement a workflow for fuzzy relations between words: Ideally, we would like our search to be able to account for paradigmatic relations and rank related words to what the query contains, not just exact matches. * Implement the search model, e.g. PLSA to retrieve documents with relevant words * Evaluate search performance * Write final report with conclusions * If time allows: create a web frontend for the application Unknowns Our original proposal scoped the project as both music lyrics and movie summaries, whether we are able to easily implement movie search will depend on how adaptable our existing code is to a different dataset of movies. If it proves too difficult this may fall out of scope."
https://github.com/braydenturner/CourseProject	Bae Area Project Proposal - CS410.pdf	"CS 410 Final Project Proposal Team ""Bae Area"" Free Topic Improved Media Content Search Team Members Brayden Turner - brturne2@illinois.edu - Captain Joshua M. Smith - jms28@illinois.edu Description We've all tried to remember a song or movie but all we have is a rough description of what the song or movie is about. Current search tools for music and movies are limited to metadata such as titles, artist, actors, but not the general sentiment of lyrics+e content, and using the lyrics and movie synopsis, return a ranked list of content that fits the description given. This can also be extended to add tags to content like ""Song from Super Bowl 40 Halftime"" or ""Rolling Stone top 10 movies list"" to enhance descriptive search. We plan to use Python libraries (lyricsgenius and imdbpy) to retrieve information from Genius and IMDB (respectively) to build our indexes and run our models on. We expect to be able to give a description of a media entity and retrieve back a ranked list of entities matching the description. We plan to take a number of entities with notable plots (e.g. Lord of the Rings, Star Wars, etc.) and build queries we know should return them. This will give us a test set of queries + entities that we expect. Using that we can get precision and recall (are all 6 of the Lord of the Rings Movies in the opt 10?). Resources and Tools * Lyrics / song data - https://pypi.org/project/lyricsgenius/ * Movie / Synopsis data - https://imdbpy.github.io * MeTA (inverted index) Programming Language Python Task Breakdown For two members, we estimate 2*20 = 40 hours of work. 1 We anticipate the following tasks: * Set up work environment and infrastructure (3 hours) * Build a dataset based on music or movie databases using available Python libraries and APIs (5 hours) * Build a set of relevance judgements and tests for evaluation (4 hours) * Construct an inverted index to use for searching the dataset (5 hours) * Create a search engine over the dataset (5 hours) * Implement the model, e.g. Naive Bayes, PLSA (10-15 hours) * Evaluate search performance (5 hours) * Write our final report with conclusions (5 hours) * If time allows: create web frontend for application (5 hours) 2"
https://github.com/braydenturner/CourseProject	README.md	Bae Area - Final Project Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/dani-richmond/CourseProject	CS410 Project Progress Report.pdf	Detecting Word Issues in Course Transcripts: Progress Report Team: The Palindromers Theme: Free Topics Date: Nov 15, 2021 1) Which tasks have been completed? * Started exploring pre-trained neural language models such as BERT, BERT variants, ELMo, GPT and the likes. We are planning to use ALBERT for this task, because it is significantly lighter on resource requirements compared to BERT * In case we train our own model instead of using a pre-trained neural language model, determined an appropriate corpus for training a bigram language model - Wikipedia pages in bulk * For evaluation, created sample testing documents to evaluate performance with and without word issues * Created a script to iterate through course transcripts and store in a format that can be fed into a language model 2) Which tasks are pending? * Explore ALBERT and learn how to fine-tune it to fit our project goal * Build a python application based on ALBERT * Use sample documents to test our project code and analyze it * Train bigram language model in case pre-trained neural language model is not a viable option from resource or implementation point of view * Write final script to utilize the trained model to parse through the actual course transcripts * Perform manual review of random sampling of course transcripts to determine effectiveness, tweak model parameters and thresholds if needed 3) Are you facing any challenges? * Pre-trained neural networks like BERT are the current state-of-the-art but they are resource intensive * Understanding / unpacking how to utilize ALBERT * Determining how best to output the exact place in the transcript that a flagged word is from, such that it is easy for human to know where in the transcript to find the erroneous words to be fixed (instead of having to search the whole transcript)
https://github.com/dani-richmond/CourseProject	CS410 Project Proposal.pdf	Detecting Word Issues in Course Transcripts Team: The Palindromers Theme: Free Topics Date: Oct 24, 2021 Team Composition First Name Last Name NetID Email Role Catherine Parker cph7 cph7@illinois.edu Team member Danielle Richmond dcr4 dcr4@illinois.edu Team captain Scott Downey scottmd3 scottmd3@illinois.edu Team member Zixiang Li zixiang9 zixiang9@illinois.edu Team member Jharna Aggarwal jharnaa2 jharnaa2@illinois.edu Team member Project Description For those with hearing impairment or for whom English is a second language, the transcripts provided for Coursera videos are a vital component of learning the material. Thus when there are incorrect words at key points in explanations it can make learning the material more difficult. Our goal is to build a method for detecting and flagging words that are potentially incorrect so that human auditors can more easily fix them. We will use Python as our coding language for this project. Dr. Zhai has offered to provide us with the video transcripts for CS410. We will then build a system with two main components: * A bigram language model built using a larger corpus to determine the likelihood that two words would actually be next to each other. * A unigram mixed language model built using the course content and a mixture of a background language model to determine how likely it is for a particular word to show up in the course transcript collection at all. Both of these will produce scores and if their combined score is above a certain threshold then we will add the following to a list: the course transcript's name, the word in question, and the word's location in the body of text. In order to evaluate our work we will manually review a random sample of both flagged and non-flagged course transcripts to confirm that the majority of flagged transcripts due in fact have words to be corrected and that the majority of non-flagged transcripts do not have any word issues. We will also explore using pre-trained neural language models to do basic flagging of grammatical errors and typos. Task & Time Estimates Time Estimate Task 12 Explore pre-trained neural language models and determine how best they could integrate with and enhance what our system does 2 Create sample documents for testing with and without word issues 3 Determine appropriate corpus for bigram language model online 15 Build and test bigram language model on sample documents 3 Determine appropriate corpus to use for unigram mixed language model's background model 20 Build and test unigram mixed language model on sample documents and determine best mixture between course content model and background model 15 Test & determine appropriate threshold for bigram model and unigram models separately and combined 15 Write final script to utilize both models to parse through the actual course transcripts 15 Perform manual review of random sampling of course transcripts to determine effectiveness, tweak model parameters and thresholds if needed 100
https://github.com/dani-richmond/CourseProject	README.md	CourseProject for The Palindromers Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/preetiamin/CourseProject	Project Proposal  Zoomalysis.pdf	Zoomalayis Project Proposal CS 410 - Fall 2021 Project Team: Team Lead: Preeti Amin (netid: preetia2) Members: Kris Gallagher (netid: kmg8) and Sunnie Wang (netid: danfeng3) Project Description: With remote work and education becoming more of a norm rather than the exception these days, online meetings have become an important part of life. Our project entails extracting topics from online meetings. The goal is to analyze text transcripts from meetings and extract a handful of main topics discussed during the meeting. We, specifically, plan on applying topic modeling to Zoom transcripts. We will develop our topic modeling algorithm using a similar meeting dataset and then apply it to Zoom transcripts. AMI Corpus is one such dataset consisting of 100 hours of meeting recordings, we will only be utilizing the text transcripts from the dataset - English only. We plan on developing the code in Python with open source libraries such as NLTK and Gensim. We will evaluate different algorithms to determine the best approach. We plan on evaluating our algorithm manually by curating and testing a dataset of about 10-20 zoom meetings, taken from mediaspace.illinois.edu, Youtube and other resources. Project Timeline/Workload: Define project (10/18-10/24) - 4 hrs per team member (12 hrs total) * Form team, define project and scope, conduct initial research on datasets, libraries etc. Project Implementation (10/25-11/28) - 16 hrs per team member (52 hrs total) * Finalize datasets, libraries to be used to extract topics - 8 hrs * Compare pros and cons of various algorithms for topic modeling - 4 hrs * Implementation of the selected algorithm in Python - 20 hrs * Design a basic interface to input meeting data and extract topics - 4 hrs * Project alignment meetings - 4 hrs per team member - 12 hrs total * Curate test data from online videos (mediaspace.illinois.edu/Youtube) - 4hrs Project Evaluation and Documentation (11/29-12/5) - 4 hrs per team member (20 hrs total) * Test model - 10 hrs * Refine workflow/user experience - 4 hrs * Create project presentation and documentation - 6 hrs
https://github.com/preetiamin/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/davegii/CourseProject	CS 410 Group Project.docx	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Tony Tumba - netid: ttumba2 Davy Ji - netid: davyji2 - Captain Pranith Bottu - netid: pbottu2 Ron Basak - netid: basak4 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic will be sentimental analysis in social media. We plan to analyze user sentiment about any particular topic based on examining their text from their social media posts. This project seems interesting since it would provide us an insight of what people are thinking about for any given topic ranging from the news to entertainment. Our planned approach would be to decide which social media platform(s) we would like to use. Although we have not chosen one in particular, we intend to use either Twitter, Facebook, or YouTube comments. After choosing which platform, we would then learn how to use the API and any given constraints of certain actions provided by the API. We will then use Python and its libraries to gather datasets. There is a large amount of research we will do to narrow down the exact tools and datasets our project would benefit the most from. Then we will use python to analyze the dataset and group them by topic. Our expected outcome is given a tweet, or facebook post, our program will be able to output a few words of emotion that the poster/tweeter feels. To evaluate our work, we will also look at the output generated by our model and compare it to what we thought the output should have been. Which programming language do you plan to use? Python3 Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. We have 4 group members, which means 20*4 = 80 hours of work. Our main tasks involve the following: Researching how to implement sentiment analysis and examples of it in today's world - 10 hours Researching APIs and other polling software to scrape public social media text - 10 hours Researching various types of social media API, mainly twitter, facebook, instagram - 10 hours Researching the problems with social media scraping and any violations that may need to taken into account when doing sentimental analysis - 10 hours Designing structure of various functions to receive text data - 10 hours Designing an algorithm that can do the social media sentiment analysis we want - 20 hours Debugging any issues as they come up (modifying parameters and optimizing) - 10 hours If time permits: Analyzing the results to think of key takeaways and modify algorithm if need be to get better results - 5 hours Miscellaneous: Make any improvements or keep developing other features - 5 hours
https://github.com/davegii/CourseProject	Progress_Report.pdf	Pranith Bottu (pbottu2) Tony Tumba (ttumba2) Davy Ji (davyji2) Ron Basak (basak4) CS410 Final Project Progress Report Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed?  We researched common sentiment analysis techniques and libraries that are commonly used. Specifically, we found two libraries for our use: textBlob and vaderSentiment.  To determine which library to use, we first created two text files, each consisting of 30 sentences, having sentences that have positive and negative sentiment. These were generated ourselves since we know for sure if our analysis is correct.  As part of testing these libraries along with the text files, experimenting with the parameters and threshold was necessary to improve our results. In conclusion, using textBlob gave us 60 - 70% accuracy while using vaderSentiment gave us approximately 85% accuracy as shown in the source code, sentiment_experiment.py  Researching various types of social media API, mainly twitter, facebook, instagram.  We have decided to use Twitter's social media API. Twitter was chosen due to its short tweets that are easier to analyse due to its character limit. Compared to Facebook posts or youtube comments, Twitter seems like an optimal social media choice. As for API usage, Twitter also is undergoing an API change, with its API v2, a lot of the bulkier parts of v1 have been updated and will be much easier to use. 2) Which tasks are pending?  We still need to research the problems with social media scraping and any API constraints that may need to be taken into account when doing sentimental analysis. This will be one of the next steps we take in our research/planning process.  Since we have recently decided to use Twitter's API, we would need to make an account via the generated token keys. Once we have this, we could further experiment with the API functions and limitations.  Also, we will need to research what APIs and other polling software we will use to scrape public social media text. Finding tools that will be able to make our life easier will be essential, so making sure we find beneficial APIS/software will be important. This step as well as the designing process will determine how smooth the implementation will go.  Finally, we still need to decide on how to design various functions to receive text data. This will be one of our most vital steps since how well we plan this will decide how smoothly the process of implementing it will go. We have begun this process already, but still need to finish it up and make sure it is done well. 3) Are you facing any challenges?  As with any project utilizing other people's data, there are some privacy concerns regarding how that data is utilized. While there are Terms & Conditions that users agree to with signing up, we're not sure how far that covers our project's goal. As a result, the first challenge we faced (but managed to solve now) was ensuring the legality of how we're using the data.  One challenge that we have is improving the accuracy of our sentiment analysis and robust testing of our model. Though we have a relatively high accuracy of 85%, we should certainly increase the number of sentences in our textfiles to reinforce our accuracy. Additionally, it is possible that the parameters and threshold we have chosen are not ideal. In order to improve our accuracy, we could keep experimenting or have a deeper understanding of the statistics and implementation behind the scenes.  The second challenge we're working on right now is which API is the most beneficial for polling/scraping social media text. We've narrowed down the list to a couple that seem good but have yet to finalize on one. Our main goal is to pick the one that makes our life the easiest and provides the most functionality. We'll likely have a solution for this soon.  The last challenge we're working through is designing the structure of the various functions to receive text data. Our group is currently split on which algorithm would be best for executing our goal. As a result, the overall workflow is not yet finalized. We've started coding out small parts to test if our ideas work and plan to wrap this up later this week.
https://github.com/davegii/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/nanyiyang/CourseProject	Project Progress Report.pdf	Progress Report - Chicken and Rice Group Members: Nanyi Yang (nanyiry2) Ivan Zhong (ninghan2) Cody Wang (yaohuiw2) 1) Which tasks have been completed? Currently we have completed a barebones web scraper similar to the one in MP2.1. However, this scraper currently does not utilize Selenium, and so it is not very robust and is not able to scrape websites that render their content using Javascript. As for our search engine, we have implemented a general structure using the BM25 model from metapy, similar to MP2.2. However, our current implementation could not load any corpus data as we need to use the corpus that we collected from our scrapper. We will incorporate the BM25 model with our collected corpus from our database in future steps. 2) Which tasks are pending? Some next steps for the web scraper will be to figure out how to use Selenium with a headless browser to simulate a user. We will also potentially need to figure out how to store scraped data in some sort of database. For the tasks on the search engine, we are currently choosing what ranker to apply and how to train the ranker. We are thinking about utilizing metapy.index.OkapiBM25(k1, b, k3) but we are still confused on how to optimize the coefficient that needs to be inserted in. One way of optimization that we've thought about is to apply the algorithm of Support Vector Machine which takes each pair of (k1, b, k3) as input vector and outputs true/false as return value for us to adjust it. 3) Are you facing any challenges? On the web scraper, we are running into a few issues with the way the content of the page is delivered. Because we want the web scraper to be robust, we will probably need to use Selenium and a headless browser to read text data that is rendered with Javascript. Besides the challenge above, we are also worried about the actual implementation steps of the optimization algorithm that we thought of. We should find a line/hyperplane to do the classification steps, but we recently have no ideas on how to set it.
https://github.com/nanyiyang/CourseProject	Project Proposal.pdf	Project Proposal - Chicken and Rice Group Members Information Name NetID Email Coordinator Nanyi Yang nanyiry2 nanyiry2@illinois.edu  Ivan Zhong ninghan2 ninghan2@illinois.edu Cody Wang yaohuiw2 yaohuiw2@illinois.edu Chosen Topic and Why We have chosen the intelligent browsing theme and specifically will try to create a way to intelligently browse faculty pages in order to reduce the amount of browsing/time needed to find a relevant professor for interesting research topics and Master's programs. Algorithms & Dataset For the search algorithm, we plan to use BM25. For the datasets we will use, the web scraper will act as a way to build our own datasets. Initially, we will try to have functionality that scarpes and searches only the current page, but if there is time, we can extend it to the scraper storign results to a database and the search engine searching that database. This way, users will be able to search various professors/programs at different institutions. Demonstration To demonstrate that our program will work, we can use the Chrome extension on faculty websites that have not been tested before in addition to those that we have already tested. This will demonstrate that our scraper is robust enough to deal with various ways of rendering text on websites. For the search engine, we can also build a simple user interface to allow users to search the documents based on user-specified parameters. Programming Language We will be using Python, and probably the beautifulSoup and Selenium for the web scraper Workload 3 Members - 60 Hours * Scraper * Chrome extension - 10 hours * Robust scraper using Selenium - 5-10 hours (considering one of the MPs was something similar, so it may take less time) * Potentially a database that stores all the scraped data, which removes the need for repeated scrapping - 10 hours - Potentially deploy database? * Search Engine * 30 hours * Integration * 5-15 hours
https://github.com/nanyiyang/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/nstruong13/CourseProject	CS410 Project Proposal.pdf	"Theme 1: Intelligent Browsing Description Most people who browse the Internet do so through popular browsers, such as Google Chrome, Mozilla Firefox, or Safari. Indeed, we complete many of our tasks by using a browser, thus how well those browsers support our tasks may significantly affect our productivity. The current browsers are great for quickly finding or saving information, but they lack ""intelligence"". Adding more intelligence to such browsers can turn a browser into a personal intelligent assistant that can broadly impact many users, potentially transforming how they access information on the entire Web. The goal of this track is for groups to build on top of existing browsers using the information retrieval techniques learned in this course. One straightforward way to extend browser functionality is to develop browser extensions (primarily written in JavaScript). Some extension-based project examples include: 1. Index the current page and allow users to search over the page using a common retrieval function, such as BM25 (the current search capabilities are limited to exact keyword match) 2. Scrape and index Campuswire pages and Coursera pages in order to link questions to content, and vice versa (you can think about how in general you might leverage a browser extension to help linking the scattered educational content such as Coursera lecture videos, textbooks, and relevant discussion on the Web) 3. Create a collective bookmarking, question-answering, or annotation system among specified groups of users a. As an example, see Fermat's Library The above examples are meant to illustrate the problem domain; groups are free to propose a topic within this track that isn't in the above list, especially if it solves a well-known problem or shortcoming of current browser systems. Students are also encouraged to coordinate group work: e.g. one group could focus on the front-end design and another group could focus on the back-end server. This coordination would allow groups to collectively solve problems beyond the scope of a single group. The following links may be helpful to get started: 1. Chrome extensions 2. Mozilla Extensions 3. Safari Extensions Requirements If you choose this theme, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * David Ho - davidsh3 * Ben - bhyang2 * Nicholas - ntruong3 (Captain) * Jun - jmzhong2 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? * Intelligent browsing system that takes topic keywords as input, scrapes web for relevant documents and generates inverted index of most frequent relevant words. * In research settings where a new project is embarked upon, a researcher might only have a general knowledge of a topic and is familiar with only a limited scope of keywords. Currently, such researchers would query upon keywords he is familiar with in order to browse and pull less-familiar keywords related to the research project. The researcher would then combine the familiar and less-familiar keywords to create more effective queries. This intelligent browsing project seeks to generate statistical visualizations relevant to a limited-keyword query, in order to help the researcher more quickly and more easily discover keywords that would help generate an effective query. The intelligent browsing program would take in some known keywords as input, scrape all docs and create an inverted index of the most frequent relevant words that appear in the docs resulting from the input query, and then generate statistical visualizations of those most frequent relevant words. 3. Briefly describe any datasets, algorithms or techniques you plan to use * MeTa * Okapi BM25 * 4. How will you demonstrate that your approach will work as expected? * Demo video, screenshots * 5. Which programming language do you plan to use? * Javascript (Chrome extension) * Python 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Chrome Extension * 40 hours * Web Scraper * 20 hours * Inverted Index * 20 hours At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why."
https://github.com/nstruong13/CourseProject	Progress Doc.pdf	1. Which tasks have been completed? * Web Scraping using Beautiful Soup * First page of Google Search * Chrome Extension basic layout 2. Which tasks are pending? * Text Mining using MetaPY to analysis scraped data * Statistical analysis and data visualizations * Chrome extension integration with the extracted information * Presentation and report 3. Are you facing any challenges? * None of us have a lot of experience building Chrome Extensions. It took a lot of research and learning to figure out how to build it. * We are still trying to figure out how to minimize the wait time on the user end when we are doing the web scraping. Currently, we plan to create a MVP where we just scrape the first page of Google. As nice future improvements, we can try to minimize wait time while scraping more information/pages. * Deciding on the best way to represent the analyzed data, from design aesthetics to model selection.
https://github.com/nstruong13/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/LifeBringer/CourseProject	README.md	"Project Proposal I am in a single person team, my NetID is ralda2 and I am the team captain. My free topic will be on building a transformer-based sentiment search. This is an expansion on my work on building a Question Answering system during my technology review. I have yet to select a specific query to address. However, I wish to build as flexible a model as I can first. I will then try to focus on a query that is widely used for my final implementation. The transformer-based sentiment model will be pegged against a unigram based approach and compared. My target is to at least beat the performance of a unigram based model. I plan to use python along with a prebuilt transformer image, ex. BERT lite. The exact type of transformer used will be dependent on hardware constraints. Transformers are known for requiring large amounts of memory to train. Retraining an existing model and turning it into an inference model will be the best approach. I estimate that the project will take me 26 hours solo. I will first spend 4 hours reviewing preprocessing for NLP, attention models, language classification, and existing methods using TensorFlow. I will then try to build a sentiment model with long text classification with BERT. I will then spend 4-5 hours adding Named Entity Recognition (NER), QA, and figuring out the metrics for language. I will then spend 5-6 hours creating a Open-Domain QA system with haystack. A large amount of time will be spent creating a quality data set for training and testing. I will try using large batch sizes to generate the final inference model. The last few hours will be spent building the semantic search engine. Progress Report 1. Which tasks have been completed? The tasks below are based on my current comprehension of NLP. The path followed will vary based on each individual. - [x] Setup CUDA via Collab and SSHed in via VS Code. This is my working environment. - [x] NLP and Transformers - Reviewed Word Vectors, RNNs, Long Short-Term Memory, Encoder-Decoder Attention, Self-Attention, Multi-head Attention, Positional Encoding, and Transformer Heads - [Github] NLP Progress on Sentiment Analysis - Baselines and Bigrams: Simple, Good Sentiment and Topic Classification Wang and Manning 2012 - [x] Preprocessing for NLP - [x] Attention - Went over examples of Dot-Product Attention applications - Reviewed self, bidirectional, multihead and scaled dot-product attention models. - Attention Is All You Need Vaswani et al. 2017 - Effective Approaches to Attention-based Neural Machine Translation Luong et al. 2015 - [x] Language Classification - Explored prebuilt flair models, looked at tokenization and special tokenization for BERT. - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. 2019 - [x] Building an initial Sentiment Model with TensorFlow and Transformer - Used the Kaggle API to download a dataset, built a dataset, shuffled, batched, split, and saved it. - Revisiting Low-Resource Neural Machine Translation: A Case Study Sennrich and Zhang 2019 2. Which tasks are pending? [ ] Long Text Classification with BERT [ ] Named Entity Recognition (NER) - spaCy [ ] Question and Answering - SQuAD [ ] Understanding the metrics, applying ROUGE to Q&A [ ] Develop a Reader-Retrieval QA with Haystack [ ] Create an Open-Domain QA with Haystack [ ] Pre-Train the transformer model with Masked-Language Modelling [ ] Setup the Next Sentence Prediction (NSP) Pre-Training Loop and DataLoader [ ] Review the Sentence Transformer [ ] Build final web application 3. Are you facing any challenges? I have being going over the notes of Jacob Eisenstein, called ""Natural language Processing"". I found this book daunting when I first read through it, however after going through Text Information Systems it's become a more enjoyable read. New topics are tough to digest, however there is content from Medium through to youtube that provide a quick high-level overview. I have started processing the data, it is very easy to get into too much detail as there are plenty of interesting papers that dwell into alternative approaches."
https://github.com/urviawasthi/CourseProject	CS 410 Project Progress Report (1).pdf	"1) Which tasks have been completed? We finished programming Latent Dirichlet Allocation for topic extraction. We preprocessed the text from a test dataset (transcripts of one of the lecture videos) to remove stopwords, perform tokenization, lematizing, and stemming words, and developed a bag of words model that we performed LDA on and found keywords that would allow the user to interpret the topic. However, the results of the LDA just gave us a probabilistic bag of words. For example, for Lecture 1.4, Overview of Text Retrieval Methods, we got the following keywords: Words: 0.042*""model"" + 0.041*""document"" + 0.024*""function"" + 0.021*""queri"" + 0.018*""word"" + 0.015*""retriev"" + 0.015*""rank"" + 0.015*""lectur"" + 0.014*""differ"" + 0.013*""score"" 2) Which tasks are pending? Currently, the output of LDA does not help us to uncover key points of topic change in the lecture. We are planning on attending office hours to ask about how we can use the results of LDA to determine the placement of a topic in a lecture, or if there might be a better approach in order to determine this. We are currently looking into utilizing cosine similarity; if we know that a lecture covers X topics, we can divide the lecture into X segments based on the differences between text blocks. Then, we can run LDA on the separate segments for a clearer representation for each segment. 3) Are you facing any challenges? Our biggest challenge right now is finding a good approach to segment the lectures based on topic transitions. However, since we have some ideas in mind, we can solve this by attending office hours and asking a TA for clarity."
https://github.com/urviawasthi/CourseProject	CS 410 Project Proposal.pdf	Requirements If you choose this theme, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Team member 1: Urvi Awasthi i. Netid: urvia2 b. Team member 2: Anupam Ojha i. Netid: anupamo2 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? a. We have chosen to improve on an intelligent learning platform for our course project, specifically Smartmoocs. The avenue of improvement that we have chosen for this project is to explore how to better segment lectures based on topic transitions. We plan on segmenting the lecture based on topic transitions that we will manually detect. The intent of this project is to allow the user to directly jump to the topic at hand in the video without coursing through the rest of the lecture. This relates to the overarching theme, intelligent learning platforms, because we are addressing a main area of improvement for the existing platform, Smartmoocs, by leveraging what we have learned in this class, such as topic mining and analysis. 3. Briefly describe any datasets, algorithms or techniques you plan to use a. We plan on using the existing CS 410 lecture transcripts as the datasets that we will be performing topic mining on. In order to do this, we will ask our TAs how we can access the code for the existing platform, Smartmoocs, which we expect already has these datasets available to it. Once we have gotten any APIs / open source code exposing the current topic mining done for the CS 410 lectures on Smartmoocs, we plan to use Latent Dirichlet Allocation for topic extraction. Specifically, we will be preprocessing the raw text to remove stopwords, perform tokenization, lematizing, and stemming words. We plan to use existing libraries such as NLTK and gensim libraries for this preprocessing. Then, we can develop a bag of words model that we can utilize the Latent Dirichlet Allocation algorithm on, to finally extract the most relevant topics for the lecture. 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use a. We will be building our application on top of the existing Smartmoocs platform, so demonstrating that our application can segment videos based on topic detection will be enough to show that our approach works as expected. b. We plan to use Python for the duration of this course project. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Environment setup: retrieve code for the existing platform, Smartmoocs. Setting up a github repository for the final project. (10 hours) b. Programming the algorithm for topic detection (10 hours) c. Segmenting video based on topics and time stamps detected (5 hours) d. Updating UI to show segmented time stamps (5 hours) e. Testing (10 hours) f. Creating the project demo (3 hours)
https://github.com/urviawasthi/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/gdeb2/CourseProject	CS410ProjectProgressReport.pdf	Pc Uda: Ta Ca C: I P: N Sad: Updated Project proposal document to implement the review comments. Place the data into proper data structures Develop model on train dataset Identification of the dataset Anomal / Outlier Detection Eplorator Data Analsis Clean, Parse and fi class imbalance So far no challenges are being faced. (Code can be viewed under Project folder) Steps done so far for clean, parse and fi class imbalance for all reviews: 1. Tokeniation 2. Remove punctuation 3. Remove stop words 4. Lemmatiation 5. Make lower case Initial feature etraction steps for all reviews: 1. Word count 2. Character count 3. Capital letters count 4. Digit count 5. Punctuation count 6. Sentiment Score Eplorator Data Analsis: Sampling data from the input file Dc  U Lab Va: OR - Original Review CG - Computer Generated
https://github.com/gdeb2/CourseProject	CS410ProjectProposal.pdf	Pojec Pooal: Team Claifie 1. What are the names and NetIDs of all our team members? Who is the captain? The captain will have more administrative duties than team members.  Gargi Deb, gdeb2, Team Captain  Ambarish Tripathi, at37  Sudhir Koundina Nagesh, sudhirk2  Gaathri Coimbatore Ramachandran, gc24 2. What topic have ou chosen? Wh is it a problem? How does it relate to the theme and to the class?  We have chosen a project related to tet classification (Free Topic)  In the past few ears, there has been an increase of fake reviews being made across man sites. With the rise of e-commerce, man buers make their decision of buing a product or purchasing a service based on the reviews that the read. To reduce the number of fake reviews, the must be first detected. Using a tet classification algorithm, this project is intended to classif fake and genuine reviews.  This project will utilie a ver relevant concept related to the class of tet classification. 3. Briefl describe an datasets, algorithms or techniques ou plan to use  Dataset we plan on using: https://osf.io/tue9/  This dataset contains 20K genuine reviews and 20K fake reviews  We plan of using this dataset and appling a ML algorithm to classif each review as either genuine or fake 4. How will ou demonstrate that our approach will work as epected?  As part of the dataset that we use since each review is classified as either genuine or fake we can compute the accurac from the model with the actual classified values to evaluate the performance of the model. 5. Which programming language do ou plan to use?  Plan on using pthon 6. Please justif that the workload of our topic is at least 20*N hours, N being the total number of students in our team. You ma list the main tasks to be completed, and the estimated time cost for each task. We epect this project to take approimatel 90 hours to complete. The breakdown is as follows:  Identification of the dataset (5%)  Eplorator Data Analsis: (25%)  Clean, Parse and fi class imbalance (15%)  Place the data into proper data structures (10%)  Develop model on train dataset (30%)  This will include evaluating and tuning on each step to improve the accurac  Identif important features to etract as part of the model  Anomal / Outlier Detection (15%)
https://github.com/gdeb2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/shefali4/CourseProject	CS 410 Final Project Progress Report.pdf	"Mounika Dadi - mdadi2@illinois.edu Shefali Sharma - shefali4@illinois.edu - Captain Ashwini Sarvepalli - ashwini6@illinois.edu Chi Zhang - chiz16@illinois.edu CS 410 Final Project Progress Report Suggestions from Reviewers: * How do you judge if the text information in that website is needed by user? In other words, how to judge if we need to memorize that website? Because sometimes we enter a website, we look through it but will find it is not what we need. * We decided that users can select when they want to save a website by directly interacting with the chrome extension and saving a website. * For each website, do you just memorize keywords or memorize other text information? * Rather than memorization we meant memoization of the website, so that previous information found would not be necessary to search through again. We would use keywords to find text information. From these prompts and working on the project, we thought of some changes to the overall project that we talk about in completed tasks. 1) Which tasks have been completed? We were able to successfully complete the first two tasks of the final project. First we all began by researching how to make a chrome extension trying to follow simple tutorials from the official google websites and other examples. We realized the easiest way to create this extension would be to use the React framework. Then we spent a lot of time learning the basics of React and how to set up a React project as we all have limited prior knowledge. Following the basic creation, we added a title and button to our extension. We added one of the main functionalities to our extension by displaying a list of URLs of the current website that a user is on to the list each time the + button is clicked. Upon doing more research and planning our timeline, we as a group decided to change the project slightly to guarantee we finished our project by our deadline. A lot of time was spent by each of us in regards to setting up and learning a new language, and now we would like to change our chrome extension to serve as a ""smart bookmark"". Through this extension, users will still be able to add and save URLs/links but we would like to add a sentence summary to each of the user's entries using NLP. Through the text analysis we will be able to extract the title of the document, and include a snippet of information that would give the main information/keywords from the given url article. Also, if time permits, we will try to implement a filter search bar that would find past saved articles. 2) Which tasks are pending? - Being able to store the information of the list of URLs selected by the user ~5 hours - Being able to extract and generate a sentence summary: ~15 hours - Including a filter search bar to search among the bookmarked pages: ~15 hours 3) Are you facing any challenges? Initially, we all needed to take a few hours to become familiar with React as we all have very limited prior knowledge. We also all have never built a chrome extension, so becoming familiar with everything was a slight challenge as well."
https://github.com/shefali4/CourseProject	CS 410 Final Project Proposal.pdf	Mounika Dadi - mdadi2@illinois.edu Shefali Sharma - shefali4@illinois.edu - Captain Ashwini Sarvepalli - ashwini6@illinois.edu Chi Zhang - chiz16@illinois.edu CS 410 Final Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Mounika Dadi - mdadi2@illinois.edu Shefali Sharma - shefali4@illinois.edu - Captain Ashwini Sarvepalli - ashwini6@illinois.edu Chi Zhang - chiz16@illinois.edu 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Topic - Intelligent Browsing Specific Topic - Chrome Extension Functionality - Extensions that keeps track of document search and the information found within for future reference when users search that topic again. Users can select the document that they would want to return to. Users often tend to search for information repetitively, and with this extension we can easily find the same information again without scrolling through. Ultimately we're memoizing the text information found in documents, so users can go back and find what they previously searched. It related to the theme of the class because we would be performing text analysis on the documents that users select from when they first activate the extension. 3. Briefly describe any datasets, algorithms or techniques you plan to use Text Analysis, so we can find keywords so the topic is recognized. We've used and learned several algorithms in class and we'll try to implement one of them. We could also gather data from the original Google Search Query and implement page rank on the marked documents in accordance to the repeated query of the user. 4. How will you demonstrate that your approach will work as expected? We will make sure to build a detailed timeline and divide the work evenly to make sure we are making progress and going to office hours if we run into issues along the way. 5. Which programming language do you plan to use? Mostly python to implement this, and potentially java 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. - Setting up the environment for a chrome extension/learning where to start: ~5 hours - Being able to store data about the types of searches the user makes: ~15 hours - Being able to scrape documents and analyze what the main topics and keywords are: ~30 hours - Being able to recommend results connected to the users needs: ~15 hours - Being able to rank the documents recommended to the user: ~15 hours
https://github.com/shefali4/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jaeroong/CourseProject	cs410-project-progress.pdf	1) Which tasks have been completed? I have researched about python libraries for collecting business news. I am testing this libriary https://github.com/datahub-ir/yahoo_finance_news_data_collection I have looked for a free api to collect historical stock data. I will be using yahoo finance api to collect prices. I have been researching more about how to do analysis. 2) Which tasks are pending? Continue learn more about sentiment analysis techniques and the implementation. Put it all together. 3) Are you facing any challenges? A challenge I face is organizing data and keeping track of news and subsequent stock performance. Sentiment analysis is a topic is that being covered in the recent lectures, so I am trying to get more familiar with it.
https://github.com/jaeroong/CourseProject	cs410-project-proposal-team-dk.docx	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Name: DK Team Member: David Kim Email: davidk11@illinois.edu This is a single member team What is your free topic? Please give a detailed description. What is the task? The topic is related to text analysis on the news of certain stocks. This program will gather news on some publicly traded stocks to be tested. Sources could be from financial news sites, such as Yahoo Finance. Then the program would perform sentiment analysis on the news and determine if it is positive or negative. Further analysis could be done if time allows. Then the system will test if the stock's subsequently performed accordingly, that is, test if it correlates the positive or negative sentiment. Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? I've been interested in the stock market, so this is an opportunity build something useful using the knowledge I've gained in this course. I plan to gather news and I am going to use NLTK library. Data is going to be news on some publicly traded companies. If there's some statistically significance between news sentiment and stock performance, then this program would be considered useful. Which programming language do you plan to use? python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Program to collect news source. 6 hrs. Program to analyze the news collected. 10 hrs. Test and analysis on the price and sentiment. 6 hrs. Project Description This project is related to the subject matter that I am interested in.
https://github.com/jaeroong/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ratanbajpai/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/guoyuhan717/CourseProject	CS 410 Project Progress Report.pdf	CS 410 Project Progress Report Team GYZ: Yuhan Guo(yuhang4), Qi Zeng(qizeng2), Haoduo Yan(haoduoy2) 1) Which tasks have been completed? We separate our tasks into three parts: framework, data, and functionality. Each group member will be responsible for one part. We have finished the user interface design and created the basic structure and files for chrome extension implementation. In addition, we have also implemented the I/O and functionality design(see below). 2) Which tasks are pending? Framework: Chrome extension implementation Data: Web scraping Data cleaning Functionality: Named Entity Recognition: Given a sentence, return the identified entity lists (position and type tuple). This function will be implemented by integrating bert-base-NER (https://huggingface.co/dslim/bert-base-NER), a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. Entity Linking: Given an entity, return its unique identity in Wikipedia if it existed. This function will be mainly based on matching. Entity Extension: Given a linked entity with a wiki identifier, return related entities. This function serves as the core function for this extension since it can return extra knowledge. We plan to explore co-occurring entities (entities shown in the same wiki page) and entities with high relevance (ranked with semantics-based ranking methods). 3) Are you facing any challenges? We are all new to chrome extension implementation and have little experience in JavaScript. The design of the chrome extension is one challenge for us, but we have found some useful tutorials online which can be used as guidelines. Another challenge is the web scraping for a passage. We want to convert the passage into a list of words. We are working on extracting critical information and annotating multiple entities within a text, and the main difficulty is to label various entities with accuracy.
https://github.com/guoyuhan717/CourseProject	CS 410 Project Proposal.pdf	CS 410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team GYZ: Yuhan Guo (yuhang4) --- captain Qi Zeng (qizeng2) Haoduo Yan (haoduoy2) 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Theme: Intelligent Browsing Topic: Chrome extension for entity-based knowledge extension Motivation: During the webpage reading process, out-of-(user)-vocabulary words are easily encountered even for a well-educated and erudite reader. Inferring the word meaning from the context is not always reliable, especially for some less skilled readers. Jumping out of the current page to conduct searching may disrupt the reading pace and decrease the reading efficiency. And most importantly, readers may be misled by the wrong impression of the particular novel words that they assume they know but actually do not. Therefore, we propose to build a Chrome extension that enables knowledge extension for reading. This tool is supposed to help identify the named entity in the current webpage and then provide related links and abstract as reference. It is closely related to Theme 1 because it is an intelligent system for less educated people, including those young or eager to learn. It is closely related to the class because it involves the basic text processing techniques and searching techniques. 3. Briefly describe any datasets, algorithms or techniques you plan to use Datasets: webpages (primarily academic articles) Techniques: NER, Key Sentence Identification 4. How will you demonstrate that your approach will work as expected? We will demonstrate our functionalities on a specific webpage to prove that our approach works as expected. 5. Which programming language do you plan to use? JavaScript and Python. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Chrome extension implementation: 10h User interface design: 5h Web scraping: 5h Data cleaning: 5h Named-entity recognition/Key Sentence Identification: 15h Searching based on the recognition: 10h Filtering based on relevance: 10h
https://github.com/guoyuhan717/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/sujay170nanj/CourseProject	PROPOSAL.pdf	CS 410 Team 68++ - Andrew Son (captain), andrew28@illinois.edu - Sujay Nanjannavar, sujaypn2@illinois.edu Abstract Our team will produce a Google Chrome extension that links job posting pages to potential salary and interview information that can be found online about that job. This allows those looking at a job listing to very quickly determine whether they are interested in the job or if they would be able to successfully complete the interview, without even needing to do a separate search. This will be done by scraping websites where previous applicants & employees report information about their interview process and job offer. For the scope of this project, the focus of the extension would be on CS-related positions (Software Engineer Intern, IT Intern, Data Scientist, etc.) and would use information from websites such as Glassdoor and levels.fyi. Datasets * Glassdoor * levels.fyi * Reddit (if time permits) Goals Run the Chrome Extension on a large sample Programming Language Because Chrome Extensions are written in JavaScript, we will use JavaScript and Python as our back-end for the webscraping to be performed. Work Estimation Task Time (hours) Become acclimated with JavaScript 5 Create browser extension UI 10 Write scraping code for salary 10 Write scraping code for interview information 20
https://github.com/sujay170nanj/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ritikd2/CourseProject	CS410_progress_report.pdf	CS 410: Project Proposal Causal analysis of stock prices vs. news (Free Topic) ritikd2 November 15, 2021 Members Ritik Dutta - ritikd2 Progress Report 1. Which tasks have been completed? Fetching data from a fnancial news website. I found a data dump of news headlines from a news aggregator (https://pulse.zerodha.com/). Additionally, I've written a script to fetch live data from the website after regular time intervals 2. Which tasks are pending? (a) Data cleaning, formatting - 4 hours (b) Check for causal relations - 15 hours (c) Evaluate results, prepare report - 5 hours 3. Are you facing any challenges? None at the moment 1
https://github.com/ritikd2/CourseProject	CS410_project_proposal.pdf	CS 410: Project Proposal Causal analysis of stock prices vs. news (Free Topic) ritikd2 October 24, 2021 Members Ritik Dutta - ritikd2 Topic The goal of this project is to implement a system that explores if there exists a causal relation between news on popular fnancial market websites and stock prices. Relations between stock prices and news cycles are of particu- lar interest to those that engage in equities trading. Being able to identify trends early on might allow traders to allocate capital in ways that maximizes their gains. I plan to implement the following: 1. Fetch news data from popular fnancial websites. I plan to collect this by scraping news aggregators like Zerodha Pulse 2. Fetch closing prices of stocks from the National Stock Exchange of India 3. Implement the Granger causality test to determine if there exists a causal relation between news data and the corresponding stock price 4. Explore building a predictive model should a correlation between the two exist The project I'm proposing is similar to this project from the previous iteration of the course but difers in the following major ways: 1. The previous team used a fxed 5-day lag to check to test their hypothesis. A fxed time-lag can be restrictive, since diferent kinds of news can afect prices at diferent speeds. I plan to explore a much broader window of time lag for any possible correlations 2. The previous team used tweets as the data source. However, parsing for good tweets can be difcult, and they have a limited reach compared to news websites, which can thus infuence a much larger audience. I plan to rely on headlines and content scraped from fnancial news websites instead. 3. Equities might often show correlations amongst each other, which was not explored by the previous team. Furthermore, sometimes general news can cause overall changes in fnancial markets. I plan to explore this as well. The project will be implemented using Python. Since I'm working on this project alone, the expected workload has to be at least 20 hours. I expect the following distribution across tasks: 1. Data scraping - 5 hours 2. Data cleaning, formatting - 4 hours 3. Check for causal relations - 15 hours 4. Evaluate results, prepare report - 5 hours All updates will be pushed to this Github repo. 1
https://github.com/ritikd2/CourseProject	README.md	Causal analysis of stock prices vs news Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/shiyao3/CourseProject	Project Progress Report.pdf	CS 410 Project Progress Report Members Shi Yao Liu (shiyao3) (Captain) Xinrui Zhu (xinruiz4) Ryan Cedzo (cedzo2) Summary Up to this point, the initial UI and setup of the extension have been completed. The project is being completed on schedule so far and is on track to be finished in time before the end of the semester. The prerequisites for meeting all the objectives outlined in the proposal have all been met, the next step is to create the implementation and evaluate its performance. Challenges Currently, there have not been any challenges preventing the completion of the project up to this point. In addition, the team has discussed the project roadmap and clarified the requirements. In the following steps, we do not foresee any major blocking issues in the near-future. Design The following figure is a screenshot of the current design of the extension. The figure was generated from the source code. It is an initial draft and is subject to change in the future as more work is done as the project progresses. Feedback and constructive criticism from peer-review is welcomed and greatly appreciated. Code Link to source code: https://github.com/cedzo2/BrowserExtension The code contains the initial layout of the extension. It currently has no functional code, its purpose is to give us a starting point to discuss implementation. Instead of implementing algorithms from scratch, we agreed that it would be much wiser to use existing libraries. All necessary files are compiled into the extension folder except for the javascript file, which will be used to perform the necessary tasks for the extension. This file includes the code used for scraping relevant links and making the necessary calculations. Once completed, the javascript file can then be linked to the manifest.json file. Data Set Nine Wikipedia pages have been chosen to evaluate the performance of this extension. For each of the nine pages, three query terms have been picked for testing. The team manually scraped the pages for links relevant to the corresponding query terms. Links not in the list are assumed to be irrelevant. Evaluation A method of evaluating the effectiveness of the system has been proposed and reviewed by the team. As with the design, this may change as more work is added and requirements are redefined when necessary. The method for evaluation is as follows: * For every term, manually find the relevant links in the page they were sourced from. * Compute the F1 measure (for each page) for the default search tool in the browser. * Compute the F1 measure (for each page) for the intelligent browsing extension. * With the aggregated F1 measures, perform a statistical significance test to see if the intelligent browsing extension makes a difference. * If the data does indicate a difference, decide if it was more effective than the exact text match. This can only be done empirically (information retrieval is meant to serve humans, thus it is up to humans to make the final call). Ideally, it would be the users who decide but given the timeline and resource constraints, the decision must be made by the developers. Feedback and constructive criticism from peer-review is welcomed and greatly appreciated. Next Steps The next step is to pick a BM25 algorithm for implementation followed by testing and evaluation. Tuning may be necessary and debugging the extension itself is definitely needed. The interface may need to be improved depending on peer feedback and the opinions of the team. The next step in UI design is to figure out how to present the results. Additional features to enhance usage and performance may be required as well. The prerequisites for meeting all the objectives outlined in the proposal have all been defined and set up, the next step is to create the implementation and evaluate its performance.
https://github.com/shiyao3/CourseProject	Project Proposal.pdf	CS 410 Project Proposal Intelligent Web Page Information Retrieval and Link Browsing (Intelligent Browsing Track) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Shi Yao Liu (shiyao3) (Captain) Xinrui Zhu (xinruiz4) Ryan Cedzo (cedzo2) 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Topic: Create a Chrome extension that indexes the current page, scrape the page for hyperlinks and recommend sections and links from the page that may be relevant to the query. Problem: Current search capabilities are only limited to exact keywords. In addition, only relevant text results from the current page are returned. By retrieving relevant links from the page in addition to text from BM25, we can provide higher-quality information to the user as well as a starting point for web browsing. Relevance: Concepts directly from the course will be applied to a real-life problem. BM25 will be used for ranking, limited web scraping and crawling will be used to extract link information. Methods of augmenting the BM25 algorithm discussed in lectures will also be explored and applied if applicable. 3. Briefly describe any datasets, algorithms or techniques you plan to use Dataset: Development, testing, and evaluation will be done on a set of Wikipedia pages. The reason is Wikipedia contains a rich amount of text information, a wide knowledge base, and various links to other sites. This provides us with a large amount of high-quality data, making it an ideal proving ground. Algorithm: BM25 algorithm. Augmentation techniques and variations introduced in lectures may also be applied if it improves performance. Techniques: Segregation page contents, use of hyperlinks, using link descriptions, web scraping, exploring variations of the BM25 algorithm. 4. How will you demonstrate that your approach will work as expected? Decide on a few Wikipedia pages with a high volume of text data and links to external sources (for example, the UIUC Wikipedia page). As discussed in lectures, empirical evaluation is always required to some degree. There are three people in the group, each individual selects a page and ranks the links and sections on the page based on relevance to a sample query. The algorithm is then run on the three pages with the same queries. The ranking from the algorithm can then be compared to the ranking we provided. 5. Which programming language do you plan to use? HTML, CSS, JavaScript 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. This project involves several major components which take considerable amounts of time to be able to obtain accurate results. They are broken down as follows: Total 60+ hours Dataset: Dataset selection and manual processing. This step involves a lot of manual work. We need to come up with queries and rank page sections and links for each query and split pages logically based on what we expect the user to be interested in. (~10 hours) Implementation: UI/UX (interface design, use cases, result presentation), ranking algorithm, web scraping, exploring variations of the BM25 algorithm. This is where the majority of the workload is, the actual implementation of the project. (40+ hours) Evaluation: Performance metrics, comparisons, calculating scores. Are the scores reasonable? Do the results make sense? Is it what the user would expect? (10+ hours).
https://github.com/shiyao3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/arvindsankar/CourseProject	README.md	"CourseProject Sentiment Analysis is used to identify the sentiments of the text source. Very commonly, Tweets are used in collecting lots of data for performing sentiment analysis. They can help us understand the opinions and perspectives of various people about a particular topic. The task is to capture the sentiments of the data by developing a ML pipeline and then evaluate the results of the model. We will use the Twitter Historical Tweet APIs [1] to provide a dataset of tweets on a show like ""Squid Game"" for a definitive timeline. After collecting a substantial dataset, we will hand label the tweet data and mark it as positive or negative sentiment. We can then do analysis and preprocess the tweets to prepare for a specific model. The model created can be validated and then evaluated with F1 Score (precision and recall). We will evaluate the data on streaming data via the Twitter Streaming APIs [2]. As tweets for a topic appear in real time, the model will evaluate each tweet as positive or negative. After the tweets stream, we can hand label them and evaluate the accuracy of our model. If we choose to evaluate multiple models, we can measure evaluation speed as another metric. This task is interesting since it could allow people to measure audience sentiment in real time. If a show is airing, producers of the show can use this tool to understand what the audience thinks of each scene. This would give them a level of insight that would make subsequent episodes more engaging for the audience. Dataset: We will be using Twitter data for the project Tools: For streaming, scrapping and stemming Python can be used for data in real time. Evaluation: We will evaluate using Precision/Recall measures."
https://github.com/arvindsankar/CourseProject	Text Info Systems - Progress Report.pdf	Progress Report: Sentiment Analysis on Real-Time Streaming Tweets Team Ninja Team Members: Captain-asankar2@ 1) Kunika Sood: kunikas2@ 2) Arvind Sankar: asankar2@ Tasks that have been completed: 1. Collect historic tweet data Create Twitter developer account, learn which APIs we need, etc. ~5 hours 2. Fetch tweets relevant to topic at a defined time interval Write code that uses API to fetch tweets relevant to a pre defined topic at a predefined time interval ~2 hours We have written code to download tweets and metadata of tweets and have built a small dataset for our models to train on. Which tasks are pending? Started performing various pre-processing steps on the dataset to remove urls, punctuations and converting data to lowercase for better generalization. Task Details Total Hours Hours used 1. Data Preprocessing Label Training Data ~3 hours 2. Data Analysis and Preprocessing to Prepare for Specific Model Apply stemming, tokenization, stopword removal, etc. ~8 hours ~2 hours 3. Split data as Training and Test Subset Split labeled data as Training and Test set in N samples (70/30 ratio) ~1 hour 4. Model Creation with Training Train models ~ 5 hours 5. Model Validation with Validate the models with test data ~ 6 hours Test Data 6. Run Experiments on Streaming data Write code that fetches data / applies preprocessing / sends to the model / records result ~5 hours 7. Evaluate the results of the model on Streaming data. Label Testing Data ~1 hour 8. Presentation and demo video Record a demo of model running evaluations on a live twitter topic. ~4 hours Are you facing any challenges? One challenge we have faced is the limitation of the Twitter API. Since we are using the developer account to retrieve tweets, we are only able to retrieve tweets from the last 7 days. This presents an issue since we were planning on training models on TV shows that air each week, but with this approach we would only get one week's worth of data. Instead, we pivoted to training on a TV show that airs daily (Jeopardy) to collect sufficient data. We have already collected one weeks worth of data with our current tooling. We can collect another week's worth of data starting next Saturday.
https://github.com/arvindsankar/CourseProject	Text Info Systems - Project Proposal.pdf	"Free Topic : Sentiment Analysis on Real-Time Streaming Tweets Team Ninja Team Members: Captain-asankar2@ 1) Kunika Sood: kunikas2@ 2) Arvind Sankar: asankar2@ What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our Free topic is Sentiment Analysis. Sentiment Analysis is used to identify the sentiments of the text source. Very commonly, Tweets are used in collecting lots of data for performing sentiment analysis. They can help us understand the opinions and perspectives of various people about a particular topic. The task is to capture the sentiments of the data by developing a ML pipeline and then evaluate the results of the model. We will use the Twitter Historical Tweet APIs [1] to provide a dataset of tweets on a show like ""Squid Game"" for a definitive timeline. After collecting a substantial dataset, we will hand label the tweet data and mark it as positive or negative sentiment. We can then do analysis and preprocess the tweets to prepare for a specific model. The model created can be validated and then evaluated with F1 Score (precision and recall). We will evaluate the data on streaming data via the Twitter Streaming APIs [2]. As tweets for a topic appear in real time, the model will evaluate each tweet as positive or negative. After the tweets stream, we can hand label them and evaluate the accuracy of our model. If we choose to evaluate multiple models, we can measure evaluation speed as another metric. This task is interesting since it could allow people to measure audience sentiment in real time. If a show is airing, producers of the show can use this tool to understand what the audience thinks of each scene. This would give them a level of insight that would make subsequent episodes more engaging for the audience. Dataset: We will be using Twitter data for the project Tools: For streaming, scrapping and stemming Python can be used for data in real time. Evaluation: We will evaluate using Precision/Recall measures. Which programming language do you plan to use? We are planning to use Python. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Justification of Workload: 20 X 2 = 40 hours Phase Task Estimated Time 1. Collect historic tweet data Create Twitter developer account, learn which APIs we need, etc. ~5 hours 2. Fetch tweets relevant to topic at a defined time interval Write code that uses API to fetch tweets relevant to a pre defined topic at a predefined time interval ~2 hours 3. Data Preprocessing Label Training Data ~3 hours 4. Data Analysis and Preprocessing to Prepare for Specific Model Apply stemming, tokenization, stopword removal, etc. ~8 hours 5. Split data as Training and Test Subset Split labeled data as Training and Test set in N samples (70/30 ratio) ~1 hour 6. Model Creation with Training Train models ~ 5 hours 7. Model Validation with Test Data Validate the models with test data ~ 6 hours 8. Run Experiments on Streaming data Write code that fetches data / applies preprocessing / sends to the model / records result ~5 hours 9. Evaluate the results of the model on Streaming data. Label Testing Data ~1 hour 10. Presentation and demo video Record a demo of model running evaluations on a live twitter topic. ~4 hours Total ~40 hours"
https://github.com/tpjwm/CourseProject	CS410_Project_Proposal.pdf	Team SIGSEGV 1. Our members: a. Dimitar Pendurkov, NetID: dimitar3 b. Michal Trzupek, NetID: michalt2 c. Oscar Burba, NetID: oburba2 The team captain is Dimitar Pendurkov 2. Our project will be under Theme 3: System Extension. The subtopic is 3.1 MeTA Toolkit. 3. We plan on making Metapy compatible with recent versions of Python and integrating/adding a ranking function. 4. We will demonstrate the compatibility by installing and running Metapy scripts with the latest python version and we will demonstrate the ranking function by running the added function on the same datasets through our implementation and another implementation and comparing results. 5. Our code will communicate with the system by updating its source code directly. 6. We plan on using python 7. Extending Metapy to work with newest versions of python is hard to gauge workload wise but it is assumed to be non-trivial as its large presence but lack of updates from the creators / other users. The other task is implementing a ranking algorithm which requires planning, research, and testing. If the time spent is less than 20*N (in our case 60) hours total, we can always implement more functionality and possibly integrate a toolkit into Metapy such as NLTK.
https://github.com/tpjwm/CourseProject	Final Project Update.pdf	"Completed so far for Task 1) Making metapy compatible with recent versions of Python. This is the error you get when you try to build metapy with a recent version of python (in this case 3.8.10). The makefile is attempting to download the ICU file specified from ""http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz"", but it fails multiple times with a different MD5 hash each time: The offending makefile dependency is found in metapy/deps/meta/CMakeLists.txt I followed this link and it redirected to the download page for all versions of ICU. I went to the github page instead and found a more recent release with the same format and used that instead. So the updated makefile with the link and new MD5 hash which looks like this: Now when making the build with python 3.8+, the build succeeds. Testing it on MP1 example shows that metapy works on the newest version now: ISSUES: I tried to fork meta and metapy and have a working updated repository, but ran into a lot of trouble with git submodules. Locally when I make these changes meta works, but I kept running into an error with a missing function when uploading these changes to github forks of meta and metapy and connecting them. I believe that this a problem with metapy using a developmental branch of meta (one that is more recently updated) so when I try to link my meta fork to metapy through git submodules, it causes additional errors. My solution to this was to upload a zip file of my local version of metapy to a github repository and link it as a submodule of our CourseProject Fork. Pending Tasks: * Currently, this fix only works with Ubuntu Linux 20.04 LTS. Pending time constraints we can also try to figure out a way to make metapy compatible with newer versions of python on the macOS and Windows operating systems. * Further testing with other MPs and with other versions of python. Completed so far for Task 2) Addition of other ranker functions using Metapy This task was more research oriented, and one of the original goals of our project was to investigate different retrieval functions and implement another ranker function that is not included in the current version of metapy. Currently, the existing ranking functions in meta are the following: Absolute Discount, Dirichlet Prior, Jelinek-Mercer, KL-Divergence, LM Ranker, Okapi-BM25, and Pivoted Length. We were curious about how different the outputs look like based on the ranker function, as each ranker function has its trade-offs. The goal of implementing a new ranker in metapy was to give more options to metapy users to rank text documents, and to find out which ranker function is overall the best one. After doing some research, we discovered several different variants of the already existing Okapi-BM25 function. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7148026/) For reference, the standard Okapi-BM25 function is One of these functions is BM25+, which introduces a new free parameter. BM25+ was originally developed as an extension to BM25. One of the disadvantages of BM25 is that the component of TF normalization by doc length is not properly lower-bounded, which results in long documents (which may not contain the query term) being scored unfairly as having the same relevancy as shorter docs that do not contain the query term at all. The free parameter (denoted as d) is usually set to 1.0 in the absence of training data. Theoretically, BM25+ should score documents with slightly more fairness as a result of the parameter. We implemented BM25+ using python. In order to do simple testing of BM25+, we used the skeleton of writing our own ranker and cranfield datasets provided from MP2.2. For now, we decided to keep it simple just to make sure if our implementation would work. The BM25+ code consists of the following: Running the standard version of Okapi-BM25 yields the following: Note: BM25 parameters were passed in with default values (k1=1.2,b=0.75,k3 = 500) MAP = ~0.2551186 Resultant output from BM25+ ranker. In our code, we set the free parameter to 1.0. MAP = ~0.24279 Using R, we used the significance test to find the p-value. Since the p-value is < 0.05, there is a significant difference between the average precisions of the two functions, illustrating just how significant a seemingly small change in the function is and how influential the free parameter of BM25+ is. ISSUES: * I ran into issues uploading the code to our final project repository, thus, you may be able not see the actual code just yet, although a fix is in progress. Pending Tasks: * Introducing more BM25 variants (such as BM25L)/introducing other ranker functions and comparing the different results to find if there is a better ranker for the current dataset. * Testing the functions using custom query datasets instead of cranfield to check different outputs using documents of various lengths. * If time permits, we could implement BM25+ within MeTa itself using C++ instead of python, thus enabling future users to simply call BM25+ like so: metapy.index.BM25plus(k1, b, k3, free_parameter) Resources: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7148026/"
https://github.com/tpjwm/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Update: 11/15/2021 Added an update for the final project, denoted as 'Final Project Update', detailing our progress on the project, tasks we still need to complete, and any issues we are currently facing.
https://github.com/rtb-illinois/CourseProject	progress.pdf	"Final Project Progress Report Ryan Baker (captain; NetID: rtb2) -- rtb2@illinois.edu Which tasks have been completed? Which tasks are pending? For convenience in reference, the original proposal includes the following tasks: Task Expected Time Scrape the Blizzard Hearthstone API and build a simple DB of the cards COMPLETE (4 hours) Build an inverted index with information from the card titles and text from the card body 5 hours Build an API (and a UI if time permits) through which a user can submit their current cards, enter search keywords and ask for recommendations 5-10 hours Build an evaluator with known existing decks 5 hours The first task to scrape the Blizzard Hearthstone API and build a simple DB of the cards has been completed. As part of this, I created two different python scripts, both checked into the Github project repository. One script handles the download of cards from the Blizzard Hearthstone API. This required signing up for the developer API and creating an OAuth client to handle requests with the Blizzard API. The cards can then be queried and are returned as paginated results, which must be collected with multiple queries to the Blizzard API. The second script utilizes the first script to gather a list of cards. It then creates a SQLite DB, with a simple table for the cards that includes all text based fields from the card details in their raw form. It populates this DB with all of the cards returned from the Blizzard Hearthstone API. The remaining steps are yet to be completed. What challenges are being faced? There are currently no significant challenges being faced. Some of the work from the first step did reveal some extra details that will need to be handled. The next step will be to create an inverted index through which a user could search through the cards based on keywords. Based on preliminary investigation into the text fields, and documentation from the Blizzard Hearthstone API, I will need to combine several different text based fields associated with each card into a single ""document"" per card. Then, I will also need to do some pre-processing of each card to remove some markdown and HTML based tagging that adds cruft to the card texts."
https://github.com/rtb-illinois/CourseProject	proposal.pdf	Final Project Proposal Ryan Baker (captain; NetID: rtb2) -- rtb2@illinois.edu The topic of my final project will be to create an information retrieval system for completing decks of the online card game Hearthstone. The game consists of various modes, with the primary mode being games between individual players that each have a deck of 30 cards. The cards have various costs, abilities, and classes. It is common for players to begin building a new deck with a specific group of cards in mind that does not fill up the 30 card deck. Another common situation is that a player will have 30 cards in mind but the player does not own one or more of the cards. In these scenarios, the player needs to fill their deck with other cards. There are a huge number of cards in the game and it can be hard to find cards that synergize well with the rest of the deck. The game has an auto-complete feature, but it doesn't give the player the ability to pick from a set of recommended cards, it simply just adds cards to the player's deck. The goal of this project is to create a system that will recommend different cards for deck completion, among which the player can choose for themself. Implementation and Evaluation This project will be implemented in Python. The datasets that will be involved include the complete list of cards available in the game and a list of completed decks. These are available from the Blizzard API. There are also a variety of websites where players can share their own decks, such as HearthPwn and Tempo Storm. It is very difficult to evaluate a given deck because of the high complexity of synergies that exist within the game. So, the system will be evaluated by comparing the final deck chosen by the user to known existing decks and evaluating whether that existing deck is well known or not, and if not, how many of the missing cards were filled with cards that match a known existing deck. One method for doing so will be to remove cards from known well-performing decks and test whether the system is able to reconstruct that deck. Outline of Tasks Task Expected Time Scrape the Blizzard Hearthstone API and build a simple DB of the cards 5 hours Build an inverted index with information from the card titles and text from the card body 5 hours Build an API (and a UI if time permits) through which a user can submit their current cards, enter search keywords and ask for recommendations 5-10 hours Build an evaluator with known existing decks 5 hours
https://github.com/rtb-illinois/CourseProject	README.md	CS 410 Course Project Ryan Baker (NetId: rtb2) rtb2@illinois.edu This repo is the final project for Ryan Baker for CS 410, Fall 2021 at the University of Illinois at Urbana-Champaign. The repo currently includes: - Project Proposal (proposal.pdf) - Project Progress Report (progress.pdf) - utils Directory (code for scraping cards from the Blizzard Hearthstone API and building a raw DB of all cards using SQLite3)
https://github.com/stanleymho/uiuc-cs410-final-project	Final-Project-Progress-Report.pdf	CS 410: Final Project Progress Report Brand Sentiment on Twitter using Sentiment Analysis Team Stanley (Fall 2021) 1. Which tasks have been completed? I have completed the following tasks so far: a) Write a program to generate a dataset by collecting the tweets from Twitter API for a given brand over a period of time. b) Write a program to submit the dataset to Amazon Comprehend for sentiment analysis. 2. Which tasks are pending? a) Create a sentiment trend graph based on the result of the sentiment analysis for data visualization. (4+ hours) b) Collect recent brand's news or events. (1+ hours) c) Analyze the sentiment trend graph, and link any sentiment shift with the recent news or events. (3+ hours) 3. Are you facing any challenges? There were a few challenges I encountered: a) Twitter API v2 is not ready for prime time Twitter currently supports two API versions: v1.1 and the new v2 in early access. I started off with the new v2 API under the impression that v2 should be superior. Unfortunately, it turned out the v2 API is still in early access and it lacked some of the functionalities I needed. After spending a few hours with the v2 API, I switched back to use the v1.1 API. b) Query abilities and rate limiting with Standard Developer Account My Twitter developer account is standard (instead of premium or academic research), and I encountered several restrictions: i) For standard developer accounts, Twitter focuses only on relevance and not completeness in search results. This means that some tweets could be missing from search results. ii) The standard developer account has a limit which only allows querying the tweets in the past 7 days. iii) Each query request could return up to 100 tweets, but there is a rate limit allowing only 180 requests (with user authentication) in a 15-min window. iv) There is also a monthly tweet cap usage which allows only 500,000 tweets to be pulled. All these issues make it difficult to retrieve large amounts of tweets over a long period of time for sentiment analysis for this project. To mitigate these, I decided to focus on performing sentiment analysis on the popular tweets in the past 7 days for this project. c) Truncated text in tweets Inspecting the retrieved tweets apparently revealed that many tweets were truncated. It turned out that Twitter originally supported only 140 characters in tweets, and later extended the support to 280 characters for certain languages. The Twitter API returned the tweet in compatibility mode which truncates the text by default. Changing the query to extended mode fixed the issue. d) Tweets in different languages Twitter is a global service available in over 200 countries, and the tweets are written by the users all over the world in various languages. To analyze the tweets, this presents a challenge as the tweet might not be written in a language which sentiment analysis would understand. For simplicity, I decided to only analyze the tweets in English for this project. e) Retweet After analyzing the tweets, it became apparent that many Twitter users didn't write their own tweets. Instead, they often favorited or retweeted what others had written. This presents another challenge as many tweets returned from Twitter had identical text. This is not ideal as each of these tweets with identical text is counted towards our developer account's rate limit and monthly usage cap. Hence, I decided to retrieve only the most popular tweets, instead of all the tweets. Retrieving the most popular tweets would return each tweet once with a retweeted count, and this helps address the concerns around rate limit and monthly usage cap. That said, retrieving only the most popular tweets means we would ignore the long tail of unpopular tweets created by many individuals, and this could affect the overall accuracy of the brand sentiment we would like to determine in this project. f) Cost of public cloud's NLP service for sentiment analysis It turns out that it is cost prohibitive to perform sentiment analysis on large volumes of text with public cloud's NLP service. Take Amazon Comprend for example, it charges $0.0001 per unit (up to 10M units) which each unit is 100 characters. Simply analyzing 10,000 tweets each with 200 characters alone would cost $2. There are 500 millions tweets put up on average every day on Twitter. Even if 0.01% of the tweets are relevant to our brand for sentiment analysis, that's 50,000 tweets per day. Hence, instead of performing sentiment analysis on all the relevant tweets over a period of time, I decided to scale back and only analyze the popular relevant tweets, as the total number of popular relevant tweets is several orders of magnitude less than the total number of individual relevant tweets, and the cost would be much more affordable for this project. 4. Plus anything specifically mentioned in the reviews to cover. N/A
https://github.com/stanleymho/uiuc-cs410-final-project	Final-Project-Proposal.pdf	CS 410: Final Project Proposal Team Stanley (Fall 2021) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Stanley Ho NetID: smho2@illinois.edu Team: Stanley 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free topic is Brand Sentiment on Twitter using Sentiment Analysis. More specifically, sentiment analysis can capture the market or customer sentiment towards a brand. Companies can use this information to better understand their audiences' reactions to the brand's news or marketing campaigns, and to further enhance the brand. Investors or traders can also leverage this information to determine whether they should long or short their positions in the stock behind the brand. The task is to perform sentiment analysis on the Twitter tweets related to a given brand over a period of time, and create a sentiment trend graph which visualizes the sentiment towards the brand. My approach is as follows: a. Generate a dataset using the Twitter API to collect the tweets related to a given brand over a period of time. b. Perform sentiment analysis on the dataset using one of the public cloud's NLP services (e.g. AWS Comprehend, Google's Natural Language API, etc.). c. Based on the result from the sentimental analysis, create a sentiment trend graph for the brand. d. To evaluate the result, I plan to pick a brand which had some significant sentiment shift over the past few months. I also plan to collect significant news or events related to the brand over the same period of time. By analyzing the sentiment trend graph, we should be able to link any significant shift in the graph with the news or events. 3. Which programming language do you plan to use? Java or Golang. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Write a program to generate a dataset by collecting the tweets from Twitter API for a given brand over a period of time. (5+ hours) b. Write a program to submit the dataset to one of the public cloud's NLP service for sentimental analysis. (7+ hours) c. Create a sentiment trend graph based on the result of the sentimental analysis for data visualization. (4+ hours) d. Collect recent brand's news or events. (1+ hours) e. Analyze the sentiment trend graph, and link any sentiment shift with the recent news or events. (3+ hours)
https://github.com/stanleymho/uiuc-cs410-final-project	README.md	"CS 410 Final Project - Brand Sentiment on Twitter using Sentiment Analysis (Fall 2021) Sentiment analysis can capture the market or customer sentiment towards a brand. Companies can use this information to better understand their audiences' reactions to the brand's news or marketing campaigns, and to further enhance the brand. Investors or traders can also leverage this information to determine whether they should long or short their positions in the stock behind the brand. This project is to perform sentiment analysis on the Twitter tweets related to a given brand over a period of time, and create a sentiment trend graph to visualize the sentiment towards the brand. Tools There are several tools developed for this project: 1. tweetscollect for collecting the tweets for a topic from Twitter for the past 7 days into a dataset. 2. sentimentalyze for performing sentiment analysis on the dataset. 3. TBD for creating a sentiment trend graph based on the analyzed data in the dataset. Prerequisites There are several prerequisites for building and running the tools: 1. You will need to install Go 1.17. 2. If you don't have a Twitter developer account, apply one. Once you have the account, you will need to create a Bearer Token for authentication. Please see How to generate from the developer portal. 3. If you don't have an AWS account, apply one. Once you have the account, you will need to create an access key for programmatic access. The access key consists of an access key ID and a secret access key. 1. tweetscollect 1.1. Description tweetscollect is a tool for collecting the tweets for a topic from Twitter for the past 7 days into a dataset. The topic could be one or more words. If the word contains special characters, e.g. $, the character must be escaped, i.e. \$. 1.2. Implementation tweetscollect uses the Twitter's standard search API to query against a mixture of the recent and popular tweets for the past 7 days for a given topic. Each tweet in the returned result is then reduced to the mininal, and it includes the date, text, language, favorite count, and retweeted count. Each API call returns limited number of tweets, and multiple paginations are involved in order to collect all the tweets across 7 days. After all the tweets are collected, they are written out to a file in json format. The json schema of the content in the output file is as follows: { ""$schema"": ""http://json-schema.org/draft-07/schema#"", ""type"": ""object"", ""properties"": { ""data"": { ""type"": ""array"", ""items"": [ { ""type"": ""object"", ""properties"": { ""date"": { ""type"": ""string"" }, ""text"": { ""type"": ""string"" }, ""lang"": { ""type"": ""string"" }, ""favorite"": { ""type"": ""integer"" }, ""retweet"": { ""type"": ""integer"" } }, ""required"": [ ""date"", ""text"", ""lang"", ""favorite"", ""retweet"" ] } ] } }, ""required"": [ ""data"" ] } 1.3. Usage To run tweetscollect, you must have the bearer token from a Twitter developer account. Collecting tweets for the past 7-days involves retrieving tens of thousands of tweets from Twitter, and it will takes a few minutes for the tool to run to completion. Please be patient! Notice that the Twitter developer account has rate limit on the maximum number of requests allowed in a 15-minutes time window, and collecting the tweets for one topic alone might get very close to the limit. Hence, in order to use the tool successfully, please run the tool at most once in a 15-minutes time window. ``` Build tweetscollect into an executable. $ go build ./cmd/tweetscollect/... Run tweetscollect to collect tweets for a topic, by using the bearer token from your Twitter developer account, and write the collected tweets to file. $ ./tweetscollect -b """" -t """" -o Collecting tweets from Twitter on topic """" ... { ""date"": ""2021-11-16T00:46:05Z"", ""text"": ""JUST IN: Ohio Attorney General sues Facebook (Meta) for securities fraud."", ""lang"": ""en"", ""favorite"": 1077, ""retweet"": 328 } ... Writing collected tweets to ... Done. ``` 2. sentimentalyze 2.1. Description sentimentalyze is a tool for performing sentiment analysis over the dataset with tweets which tweetscollect has collected. 2.2. Implementation sentimentalyze first normalizes all the tweets from the dataset, as there are many retweets and dedupling the tweets could significantly reduce the unique number of tweets for sentiment analysis. Afterwards, sentimentalyze sends the unique tweets to Amazon Comprehend in batches to determine the sentiment. After all the unique tweets have been analyzed, sentimentalyze would reprocess each of the original tweets from the dataset, identify its associated unique tweet and sentiment, and eventually write out the original tweets along with their sentiment to a file in json format. The json schema of the content in the output file is as follows: { ""$schema"": ""http://json-schema.org/draft-07/schema#"", ""type"": ""object"", ""properties"": { ""data"": { ""type"": ""array"", ""items"": [ { ""type"": ""object"", ""properties"": { ""date"": { ""type"": ""string"" }, ""text"": { ""type"": ""string"" }, ""lang"": { ""type"": ""string"" }, ""favorite"": { ""type"": ""integer"" }, ""retweet"": { ""type"": ""integer"" }, ""sentiment"": { ""type"": ""string"" } }, ""required"": [ ""date"", ""text"", ""lang"", ""favorite"", ""retweet"", ""sentiment"" ] } ] } }, ""required"": [ ""data"" ] } 2.3. Usage To run sentimentalyze, you must have the access key ID and secret access key from an AWS account. Performing sentiment analysis involves sending all the tweets to Amazon Comprehend in multiple batches to process, and it will takes 15 to 20 minutes for the tool to run to completion. Please be patient! Please be aware that since sentimentalyze will use Amazon Comprehend from the AWS account, the AWS account will be charged for usage. On average, each run involves between 40,000 to 60,000 tweets, and that's approximately 15,000 to 25,000 unique tweets which costs $1.5 to $2.5 to perform a single run of sentiment analysis. ``` Build sentimentalyze into an executable. $ go build ./cmd/sentimentalyze/... Run sentimentalyze to perform sentiment analysis on the tweets in the input file, using the access key ID and secret access key from the AWS account. $ ./sentimentalyze -i -o -a -s [-r ] Reading 40655 tweets from ../tweetscollect/tweets.json ... Normalizing 40655 tweets into 13089 unique tweets ... Performing sentiment analysis on the unique tweets ... { ""text"": ""Insight into the big rebrand.. \nvia \n#Facebook #AR #VR #meta #Metaverse #virtualworlds\n\nhttps://t.co/bMQ4hhKlTW"", ""sentiment"": ""NEUTRAL"" } { ""text"": ""Facebook owner Meta has opened up more about the amount of bullying and harassment on its platforms amid pressure to increase transparency\n\n:"", ""sentiment"": ""NEUTRAL"" } ... Writing tweets with analyzed sentiment to tweets-sentiment.json ... Done. ``` 3. TBD TBD for creating a sentiment trend graph."
https://github.com/edhadgu/r-soccer-Analyzer	CS 410 Final Project Proposal.pdf	CS 410 Project Proposal Members/Captain: Elias Hadgu NetID: eliash2 Topic: r/soccer scraper and analyzer Problem: Reddit can be an intimidating website for newer users due to the amount of text in and frequency of posts. Someone who is new to the website or someone who just wants to know information about soccer quickly could benefit from a condensed analysis of the subreddit. This project relates to the theme of the class because it involves processing large amounts of text, doing analysis on it, and returning relevant information to a user. Tools: I will be using PRAW (Python Reddit API Wrapper) to access reddit posts and comments as well as the BM25 retrieval function to find relevant reddit posts/comments based on a user's query. Testing: I will know if my implementation works because the information that will be returned to the user (Best player of the month, Fan favorite of the month, Goal of the month, etc..) are things that I would know because I follow the subreddit and the sport closely. If a player that was injured wins player of the month or if a goal that was scored last month wins goal of the month, I will know there was a problem. I could also try making a small test dataset with my own data to see if what I want to happen is actually occurring. Programming Language: Python for backend and JS for frontend Workload (Overestimates but I included more that 20 hours if it takes less time than I think): * Reading Documentation on/learning PRAW - 2hr * Create UI for user to input and view response - 10hr * I am pretty inexperienced in Js so this might take me longer than normal * Hopefully the UI will be able to show the video of the goal of the month or a picture of the fan favorite of the month instead of just linking to the reddit post * Goal of the Day/Month/Year - 1hr * Fan favorite of the Day/Month/Year (how to define favorite) - 3hr * Best Player of the Day/Month/Year (how to define best) - 5hr * Most Popular Team of the Day/Month/Year - 1hr * Top User of the Day/Month/Year - 1hr * Clip of the Day/Month/Year (non-goal) - 2hr * Top Reporter of the Day/Month/Year - 1hr
https://github.com/edhadgu/r-soccer-Analyzer	CS 410 Progress Report.pdf	"CS 410 Progress Report (eliash2) So far, I have... * Researched the PRAW library (the main library I will be using) and all of its functions * Read Flask documentation and used that for my frontend instead of NodeJS. * Created a simple UI where the user can choose which lets the user pick what they want to see and the time interval to check for * Connected the front and back end so that the user's choice affects the api call * The user's input is actually passed to the backend, the UI isn't just for show Pending tasks * I know what functions I need to call, I just need to find a better way to display them (videos, images, news articles, etc) * There is also an issue of the function call using the wrong parameters which I need to fix * The PRAWlibrary has its own search function but I might override it with my own that uses Okapi BM25 * Adding functionalities such as a number of posts to show and how to figure out how to define qualities like ""favorite"" and ""popular"" Challenges * I'm having a tough time with having functions in different folders without getting Flask errors which makes modularizing code a bit difficult. Feedback * My reviews stated that I should be more focused on what I want to do and be more specific so I will try to prioritize the definition of qualifiers because it is specific and will require knowledge gained from this course to implement."
https://github.com/edhadgu/r-soccer-Analyzer	README.md	r/soccer analyzer by Elias Hadgu Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yihaotan/CourseProject	CS410_Progress_Report.pdf	Topic: Crypto Sentiment Analysis Team: HODL NetID: yihaoht2@illinois.edu Progress Report Below is the progress made in the project: - [Done] Researched and identified subreddit with the highest traffic for analysis I used the subreddit r/Bitcoin, as it has the most number of members as compared to the other subreddit. c/Bitcoin currently has 1.1 million members. - [Done] Establish Reddit instance via PRAW API Created a Reddit app to obtain client id and client secret. - [Done] Retrieved submission IDs in subreddit r/Bitcoin I had to use 2 different APIs to obtain submissions and comments by date. Pushshift for filtering submissions by date, and PRAW for obtaining comments. This took around 5 hours. For the first step, I pulled a maximum of 1000 submission IDs per day between 2021-10-01 and 2021-10-31 The dataset contains ~3096 submissions. Each submission includes the ID and title. - [Done] Retrieved all comments of relevant posts into a dataset For every submission ID in the dataset, I pulled the top 10 related comments as well. The dataset contains ~12172 comments. Each comment includes the ID and body. - [In progress] Preprocess comments Have written the submissions / comments per day into each file with the following format bitcoin_subreddit_YYYY_MM_DD.txt The next step is to remove stop words and emojis in the text file Remaining Tasks - Apply sentiment analysis model (VADER) - Obtain results and tweak model - Visualize results with plotly - Project demo and documentation Challenges - Had to use 2 different APIs in order to obtain post via dates - Takes time to retrieve all the posts and comments
https://github.com/yihaotan/CourseProject	CS410_Project_Proposal.pdf	CS410 P P 1.      NID     ?    ? T         . G N: HODL N:  H T NID: ihah2@iii.ed 2.     ? P    .    ?      ?     ?  ,     ?     ? H       ? M          B   R     .         ,          . T     ,      R   2021,               .   ,        . I               . M      /    B  R,   ,     B    2021-10-01  2021-10-31. M       /     B    /CC  /B  2021-10-01  2021-10-31  PRA. T      ADER,  -      ,        / . T        / . I ,           B           B. T    B  ,        . I               , I     /     ADER  . S     HODL, D H . A, I   P        B  . T    I     PRA  PSAPI   R , ADER     P  . T     R  /        2021-10-01  2021-10-31. M      B          2021-10-01  2021-10-31. B     , I      B        B, N  B. I           B  B I,          . I ,            . 3.        ? P 4. P           20*N , N         .         ,        . A I        , I          : T H  R      2 E  R   API 2 I      /   R 5 P  2 A     (ADER) 5 O      2 C  2 D    5
https://github.com/yihaotan/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/17JamesH/CourseProject	Progress report.docx	Project Progress Report Team Member Task Progress Remaining Task Risk James He Web crawling from Redfin.com Worked on redfin web crawler and created a script that gathers all of the text data from a specific house's url Code is attached. Must update the script so that it automatically finds a list of houses that are available in a specific area on the redfin website, and then output all of the script data into a useful data file It is hard to decide what kind of data to keep as text, because a lot of it is also numerical data that might not be as useful in text processing. Xinyuan Chong Front end website develop Created an initial version of website with basic function for house searching. Screenshot of website is attached. Update website to call data crawled from Redfin.com and display it for user JavaScript is new to me. Need learn and implement at the same time. Hao Yan A back-end application that will serve the house data Set up a serverless application using AWS API Gateway and Lambda that runs API endpoints to fetch house data when requested. Choose a type of database to store the house data. Create tables in that database and populate them. It's hard to decide whether to use a relational or non-relational database. This decision may make a big difference in performance. Screenshot of Code???
https://github.com/17JamesH/CourseProject	Progress report.pdf	Project Progress Report Team Member Task Progress Remaining Task Risk James He Web crawling from Redfin.com Worked on redfin web crawler and created a script that gathers all of the text data from a specific house's url Code is attached. Must update the script so that it automatically finds a list of houses that are available in a specific area on the redfin website, and then output all of the script data into a useful data file It is hard to decide what kind of data to keep as text, because a lot of it is also numerical data that might not be as useful in text processing. Xinyuan Chong Front end website develop Created an initial version of website with basic function for house searching. Screenshot of website is attached. Update website to call data crawled from Redfin.com and display it for user JavaScript is new to me. Need learn and implement at the same time. Hao Yan A back-end application that will serve the house data Set up a serverless application using AWS API Gateway and Lambda that runs API endpoints to fetch house data when requested. Choose a type of database to store the house data. Create tables in that database and populate them. It's hard to decide whether to use a relational or non-relational database. This decision may make a big difference in performance.
https://github.com/17JamesH/CourseProject	Project_Proposal.docx	Answer: Team Information: Captain: James He Team Member: Hao Yan Team Member: Xinyuan Chong (xchong2) Topic Information: Topic: House Recommendation Engine Currently, the most commonly used house hunting website is Redfin.com. When users enter the interested location (city or zip code), all the houses for sale in the area will be provided. Users can also refine their search by price, lot size, bedroom number, etc. However, the search result is not ranked. Basically, users need to go through the result one by one. Our recommendation engine is targeted to provide more precise search result based on users' input. For example, user A thinks price is the most important quality due to limited budget, or user B cares more about bathroom numbers. Our engine will add weight and rank the houses based on these input from users and finally provide users the best matching houses. The top 5(10?) best matching houses will be provided to users to check. It can save a lot of time for users going through every houses on Redfin.com, which sometimes can be hundreds for hot area. The information of houses for sale will be crawled from Redfin.com and form a dataset. Based on users' input of the importance for different qualities of house, different weights will be added on each houses and eventually rank the houses. The dataset created by crawling from Redfin.com Top 5(10?) best matching houses Evaluation search will be defined Python, HTML, CSS, JavaScript...? The work will be divided into 3 portions: (1) Front End Interface: User can input the qualities of the house they think more important and also view the top matching search result (30h); (2) Back End: Web crawler and ranking function to create the dataset and add weight on houses based on users' input (30h). (3) Integration: Collect the user input from Front End Interface and send to Back End. Get the ranked house information and share to Front End to display to users (25h).
https://github.com/17JamesH/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chenjundongted/CourseProject	Progress Report.pdf	"Progress Report 1) Which tasks have been completed? We have completed the code to scrape news content from ""www.bbc.com/news"" and ""www.cnn.com"". For each site, we created two separate files to store the information in each day. The first file stores the news headlines and news URLs in each single line. The second file stores the news headlines and their article contents. This facilitates the annotation work and provides the sources for users to trace where the news from. We also provide documentation for the data files. 2) Which tasks are pending? We still haven't completed all the code to get information from other websites. We also need to manually annotate the news dataset we create. 3) Are you facing any challenges? When getting the URL of each news, it is a challenge to filter out non-text news URLs, such as live streaming and video URLs depending on how that news websites are structured. Most news websites use dynamic structures and requires user authentication for accessing their contents, which increases more difficulty for scraping their news articles."
https://github.com/chenjundongted/CourseProject	Propasal.pdf	Proposal Team GGWP Q: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Yueyi Gao (yueyig2); Ted Chen (jundong2) as captain Q: What is the type of your project: Is it Data Set Creation or Leaderboard Competition Creation? Dataset creation. Q: If your project is Data Set Creation, what is the novelty of your data set as compared with all the existing data sets? Which of the existing data sets is the closest to yours? What new task can your new data set be used to evaluate? How do you plan to create the data set? Our group aims to create a new dataset including major news that can be used for text mining tasks such as semantics analysis and search engine evaluation. This dataset includes most of the recent news from some major news sites and we, as the human judges, will annotate the topics of each article piece from the dataset. There are some existing datasets focusing on the collection of research articles and social media posts. However, for a more comprehensive evaluation of the text system, a dataset including all the aspects from technology, finance, fashion, and lifestyle should be included as part of the evaluation system. This dataset is mainly focused on the evaluation of the developing text mining system though it can also be used for semantic analysis, sentiment analysis, and topic mining analysis. We plan to develop some web crawlers to fetch the news from some major news sites [See the lists below]. We may also consider building a data pipeline for stream processing the data if time permits. https://www.cnn.com/ ; https://www.bbc.com/ ; https://www.bloomberg.com/ https://www.foxnews.com/ ; https://www.newyorker.com/ ; https://news.yahoo.com/ https://www.npr.org/ ; https://www.theatlantic.com/ ; https://www.nbcnews.com/ https://www.theguardian.com/us ; https://www.reuters.com/ ; https://www.vox.com/ https://www.washingtonpost.com/ ; https://www.nationalreview.com/ https://www.nytimes.com/;
https://github.com/chenjundongted/CourseProject	README.md	"CourseProject Documentation for bbcScript.py & bbcNews There will be two files generated in a day for the news in ""https://www.bbc.com/news"". The naming format will therefore be ""[NEWS SITE]#[MONTH][DAY].txt"" and ""[NEWS SITE]#[MONTH][DAY]urls.txt"". For example, two files generated in 11/15 should be ""bbc#1115.txt"" and ""bbc#1115urls.txt"" according to the naming format. Format of ""bbc#1115.txt"": Each news article will be put into one single line in txt file. The starting sentence of the line is the headline of the news, while the rest of the sentences are those in main article. Format of ""bbc#1115urls.txt"": Each news article will be put into one single line in txt file. The starting sentence of the line is the headline of the news, while the next sentence is the url of the news."
https://github.com/georgezhang2232/CourseProject	CS410 Final Project Proposal.pdf	CS410 Final Project Proposal 1. Project topic: Scholar Information Retrieval Team name: MZ5 Team member: Meng Zhang (NetID: mengz5) 2. My project name is Scholar Information Retrieval, and the subject topic is related to both information retrieval and text classification. The goal of this project is to helper students to find useful background information about a professor or scholar when they want to compare different University department or choose a PhD advisor. The background information of scholars includes their email, education background, affiliation, position, research interests, citations, top publications, awards, even homepages and bios if exists. By using this project, students do not need to search for this information online by hands, instead students can just provide the name of the scholar or professor and the project will search, filter, classify the information and return it to user, which saves a lot of time for user, especially when they want to search for several advisors. There are two main tasks in this project. First one is to scrapy the text information from the web, and the second task it to classify and filter the text information. For scarpy part, I plan to write Python script using bs4 and request to retrieve the information from several source like Google Scholar, ORCID, University faculty homepage. For the text classification, I plan to annotate some training data and then use that to train a Random Forest Classifier. Besides, also run a data cleaning before the training and do some post processing after prediction to improve the accuracy. The tools will be Python package for scraping like bs4, request, regex, and package for text classification and machine learning like scikit-learn. The expect outcome should be a system which can return information with a high accuracy and recall. I will use accuracy, precision, recall or F1 score to evaluate the project. 3. Python3 4. Text scrapy: 6 hours Data pre-process and post process: 6 hours Annotate training data: 2 hours Build and tune text classifier: 6 hours
https://github.com/georgezhang2232/CourseProject	Project Progress Report.pdf	Project Progress Report Currently, I have finished the implementation of web scraper for scraping scholar information from both Google Scholar and ORCID using Request and bs4 library from Python. It works pretty well so far that given a scholar name or URL it can retrieve all related information and I already test it with several examples and no bugs are found right now. Besides, I implement a rule-based web scraper which can scrapy scholar's homepage given the name of a scholar using regular expression. I also finished some part of text pre-processing pipeline using NLTK library which will be used to process the text data we scraped and send it to the model. The pending tasks are remaining part of data pre-processing pipeline, construction of the Random Forest Model for text classification, and also the annotation of training dataset. I plan to transfer the text data into feature vector using TfidfVectorizer and feed into the Random Forest model. The classifier will classify the text data scraped and processed into four different categories: Education history, Research Interest, Bio, Award info, and the annotation of training dataset will be finished by hand. For now, I have not encountered too many challenges, and one problem that I faced is that currently my homepage scraper is rule-based which means I am doing pattern matching for key word to detect if a website is the homepage of a scholar or not. This approach can find the correct homepage most of time, and it also makes mistakes sometimes. Since homepages has huge difference between different scholars, it is hard to come up an approach that can find the homepage of a scholar 100% correct without using machine learning.
https://github.com/georgezhang2232/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/timp33/CourseProject	cs410ProjectProgressReportTimPowell.pdf	Tim Powell CS 410 Final Project Progress Report 1) Which tasks have been completed? For my project thus far I've completed a majority of the initial planning/setup steps. I've identified the Python Reddit API Wrapper (PRAW). In order to use PRAW, I successfully generated an id and key to use the API from my application. In my application I explored PRAW's different functions in order to see what data is readily available from reddit. I also spent time looking into how this data was formatted and how I'll use it for future project steps. After playing with PRAW, I spent time dissecting the data I was able to retrieve from reddit. I was able to determine which words were occurring most frequently in each post and in the entire subreddit forum (collection of all the posts). Once finished, I researched how to preprocess the data to use the most relevant data for my classification task. I've used Regular Expressions to remove unnecessary content such as URLs from the data retrieved from reddit. In addition to this I've also identified the Natural Language Toolkit (NLTK). It's a suite of libraries that provides natural language processing functions. Thus far I've used this toolkit to further pre-process my data retrieved from reddit. I've been working on using NLTK to return all of the verbs and adjectives from my data. I'll be using all of the verbs and adjectives from the post to determine if it's negative or positive since the verbs and adjectives will best describe the post's nature. After determining how to retrieve and preprocess my data, I began to look into what model I'll use for my classification task. I found the scikit-learn library which provides access to a logistic regression model. I'll use this model to complete the sentiment analysis that my project is based on. I've also identified a dataset at the following website, https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages, which I'll use as training data for my model. 2) Which tasks are pending? Tasks still pending include trying to implement the logistic regression model from sci-kit learn. I'll need to train the model with the training data that I found, feed the data that I've retrieved from reddit into the model, and investigate the performance of the model. Afterwards, I'll need to analyze the results. If they aren't sufficient I'll need to backtrack and determine which steps need to be revisited. I'll also attempt to use another type of model from sci-kit learn and compare its performance to that of the logistic regression model. In the end, I'd like to work on displaying my findings in a way that is very easy for our peers to understand. 3) Are you facing any challenges? The main challenge I'm facing is determining how to find the subject of each post, instead of using the title of the post. I'd like to find the subject so that I can categorize each of the posts and determine which team/player is being spoken of most frequently, and then use my sentiment analysis to determine if they're being spoken of in a positive or negative way.
https://github.com/timp33/CourseProject	cs410ProjectProposalTimPowell.pdf	Tim Powell CS 410 Project Proposal Topic: Text mining of recent sports news 1. This project will be completed by myself therefore I will be the captain. My NetId is timp3. 2. My free topic is Text mining of recent sports news. My application will fetch text data from an online source, clean the data, classify the data, and use the classifications to make predictions. The source of the text data will be posts from one of my favorite websites, reddit. The source could be expanded to any website whose API provides easy access to its webpage contents. I'll use common approaches to clean the data such as removing stop words. Once cleaned, I'll further analyze the data in order to classify each post as positive or negative news. With these classifications I'll make predictions about the performance of players/teams in their upcoming games/seasons. This topic is interesting to me because I would like to study the correlation between team/player performance and the public opinion. I would like to see if the general opinion from outside the actual team reflects the performance of the team/players. I'll also look at what team/player is being mentioned most frequently and determine if this factors into the performance prediction. The tools involved include VS Code so that I can write and debug the source code. The data set involved will be the text data that is pulled from reddit. To be more specific, I'll be looking at subreddit forums that are specific to certain sports/leagues/teams/etc. If you are unfamiliar with reddit, it's a large collection of different forums. Each forum has its own topic and members. The expected outcome is an application that will use a general public opinion regarding a team/player to predict their future performance. I'll evaluate my work in real time by taking the predictions and comparing them to future performances. I'll also evaluate the application by using old data sets. This will include old text data as input and old performance results that I can compare to the output of the application. 3. I plan to use python to create this application. 4. In order to complete this project I'll spend 2 hours determining how to retrieve and format the data from reddit, I'll spend 4 hours determining how to clean all of the data, I'll spend 6 hours determining how to classify the data which will include trying different approaches, I'll spend 4 hours testing/debugging the application with an old data set, and I'll spend the final 4 hours optimizing the application and expanding it to accept other text data sources.
https://github.com/timp33/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/cotcat/CourseProject	02-Project Progress Report.pdf	Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? Task 1, Pure fundamental research for 2 hrs, task 2, Program environment initialization with a few basic feature testing for both basic demonstrations for 1 hour, and task 3, Feasibility study with coding and testing for secondary features for 2 hrs have been completed. I would also mark about 5 hrs progress for task 4, Realizing the basic function, and 1 hrs progress for task 5, adapting the function to a browser. 2) Which tasks are pending? Task 4, Realizing the basic function for 10 hrs remaining, task 5, adapting the function to a browser for 1-4 hrs, and task 6, final stage demo preparation for 5 hrs are still pending. 3) Are you facing any challenges? I am new to Chrome extension development. A popular Chrome Extension Tutorial offered by Codevolution from Youtube and studying some open-sourced projects might be helpful helped me significantly. However, there are still many challenges. A few open-sourced projects haven't been updated for 5 to 6 years and not functioning. It can be either I didn't implement them properly or simply lack of maintenance. For this issue, I would recommend saving time by checking the open-sourced projects is sting working. If it is not working, put the open-sourced project not working to a group. I may review projects in the group later if I can't get the necessary help from other places. To search words with a similar meaning, I might need to implement some database or dictionary. There are two issues. First, I need to find the proper dataset I need or generate by myself. Second, so far, I can only implement the dataset into JavaScript codes. Considering the overall quality of the program and the time, I should at least start with some small dataset and only expand the dataset when there is a lot of time left. Last but not least, the project proposal's plan is a bit challenging to keep tracking of the project. With my current understanding of the project, I will create more sub-tasks for the pending tasks. Then, click the Submit button here.
https://github.com/cotcat/CourseProject	advanced find project proposal.pdf	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. My name is Changjun Tang, my NetIDs is ct25. I am the captain of my team, called Team C. What topic have you chosen? Why is it a problem? How does it relate to the theme and the class? My topic is advanced find. The typical page finds only match characters one by one. Although it is handy in many cases, especially in lengthy documentation, we are not always looking for the exact string every time. For example, a verb has grammatical tense. When the user tries to search the word ""eat,"" they might want to search ""ate,"" ""eaten,"" etc. To be more specific, ideally, this project should realize when a user tries to find ""eats an apple,"" it will discover text containing ""eat apples,"" ""ate the apple,"" etc. This project should inherit many technics about the search engine and adapt them to my project. Ideally, it will even get the result of frequency-inverse document frequency from keywords ""TF-IDF."" Briefly describe any datasets, algorithms, or techniques you plan to use How will you demonstrate that your approach will work as expected? This project plan to use public datasets, like a dictionary for grammatical tense, search engine algorithms, like term frequency-inverse document frequency(TF-IDF) for text retrieval, etc. The basic demonstration should realize the advanced find in developing environment, for example, under a command-line interface(CLI). For secondary features, it is best to adapt the function to a browser extension for broader users and a better user experience. Which programming language do you plan to use? For basic demonstration, this project plans to use the most popular programing language, Python. However, this project will primarily use JavaScript for secondary features to adapt the function to a browser extension. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed and the estimated time cost for each task. Checkpoints with estimated time cost for each task: 1. Pure fundamental research for 2 hrs 2. Program environment initialization with a few basic feature testing for both basic demonstrations for 1 hour 3. Feasibility study with coding and testing for secondary features for 2 hrs 4. Realized the basic function for 15 hrs 5. (secondary) adapt the function to a browser for 2-5 hrs 6. final stage demo preparation for 5 hrs"
https://github.com/cotcat/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/matthewmechtly/CourseProject	Project_Progress_Report_CS410-Team_Discovery_Channel.pdf	"CS410 - BM25 Chrome Extension - Project Progress Report Sean Enright (seanre2) Bhaveshkumar Manivannan (bm12) Matthew Mechtly (mechtly2) 1) Which tasks have been completed? * (~20 hr) Two out of the three members of the team have never written a single line of Javascript or done any work with browser extensions. Accordingly, significant time must be invested by these members to learn this language. * Completed * The learning curve was rather steep--especially for learning browser extension architecture, but both members in question are now reasonably competent with JavaScript. * (~10 hr) The other part of the back-end will actually be calculating the BM25 score for each document and doing so in a quick manner at runtime. * Completed and Implemented * BM25 has been implemented entirely in Javascript, and it seems to work correctly. * (~10 hr) Time will likely be needed to optimize both the BM25 and document-parsing algorithms to fully take advantage of the limited browser resources. * Completed and Implemented * The algorithm runs practically instantly for any typical or reasonably sized query (<15 words) 2) Which tasks are pending? * (~15 hr) We must build a front-end interface with which the users can interact, input keywords, cycle through the various pseudo-documents, and adjust the parameters of BM25. * Already Completed and Implemented: * There exists a field in which the user inputs text, as well as a search button to kick off the BM25-calculating engine. * In order to enable the user to cycle back and forth through the most-relevant pseudo-documents, ""Next"" and ""Previous"" buttons have been provided and implemented correctly. * In Progress * While not technically part of our promised deliverable, a keyboard shortcut to automate the opening of the Chrome extension will be enabled in order to improve usability. * To allow more flexibility in the BM25 algorithm itself, A user-input field for the 'b' and 'k_1' parameters will be integrated. * (~15 hr) Extending from the front-end interface, the back-end that actually performs the text retrieval calculations must be constructed as well. A significant portion of this back-end development will involve the creation of a robust DOM-parsing engine, capable of dividing the web pages into appropriate pseudo-documents. * Already Completed and Implemented * Parsing Engine for the range of HTML DOMs is completed and implemented. * Pending * Need to develop an independent parsing engine for PDFs and integrate that with the HTML parsing engine code already implemented. * (~10 hr) Refinement / Testing of the extension based on feedback from tests on a broad range of web pages. * Pending * We need to verify that our implementation of BM25 in Javascript is correct; therefore, we must perform validation of our algorithm by evaluating test documents. * To ensure that this extension also runs on systems with low computing resources, we will perform testing of our extension on such a system. 3) Are you facing any challenges? * Currently, our biggest challenge is ensuring that our document parser also functions with PDFs, given that they are in entirely different formats by definition than HTMLs. * In order to implement the flexibility of users being able to input different 'b' and 'k_1' values, we need to integrate multiple, independent event listeners within the content.js script. This is proving surprisingly difficult. * Our final challenge is a bit more philosophical and marketing-related. Once BM25 is implemented correctly, we need to evaluate what the strongest use cases are while browsing: casual browsing or long-document searching research?"
https://github.com/matthewmechtly/CourseProject	Project_Proposal_CS410-Team_Discovery_Channel.pdf	"Intelligent Browsing: Text Retrieval for Long Webpages Team Discovery Channel - Project Proposal CS410 Team Members: * Sean Enright; seanre2@illinois.edu * Bhavesh Manivannan; bm12@illinois.edu * Matthew Mechtly; mechtly2@illinois.edu (project coordinator) As a team, we have selected ""Theme 1: Intelligent Browsing"" for our CS410 Course Project. Within this very broad theme, we will create a chrome browser extension that performs text retrieval on a given webpage. While this problem sounds similar to the problem that is often solved by simply executing a ""find-in-page"" search, it is distinct. Often, finding the most relevant part of a lengthy web page is infeasible. For example, pressing ""ctrl + f"" and typing in a keyword isn't particularly helpful when several hundred results are returned. Additionally, if we want to find certain parts of a long web page where a particular term coincides with a separate term, this cannot be done with the built in ""find-in-page"" search functionality. Therefore, we will create a browser extension that allows users to type in 'n' different keywords and returns the most relevant <div> within the webpage. This relates to the theme of the class since we will be developing a user-querying tool that facilitates faster text retrieval. No datasets will be required to complete this assignment. However, we will be employing BM25 to search through the various pseudo-documents (where each pseudo-document is a subsection of the web page on which the browser extension is run) and return the most relevant pseudo-document. After the most relevant pseudo-document is found, the user will be transported there, after which the user can cycle to the next most relevant pseudo-document. To demonstrate that our approach will work as expected, we will demonstrate the browser extension's ability to identify the most relevant pseudo documents on lengthy web pages already out in the wild of the internet. For the application we want to tackle, we don't have much choice of language: we will write the browser extension in Javascript exclusively. Anticipated Workload: (~20 hr) Two out of the three members of the team have never written a single line of Javascript or done any work with browser extensions. Accordingly, significant time must be invested by these members to learn this language. (~15 hr) We must build a front-end with which the users can interact, input keywords, cycle through the various pseudo-documents, and adjust the parameters of BM25. (~15 hr) Extending from the front-end interface, the back-end that actually performs the text retrieval calculations must be constructed as well. A significant portion of this back-end development will involve the creation of a robust DOM-parsing engine, capable of dividing the web pages into appropriate pseudo-documents. (~10 hr) The other part of the back-end will actually be calculating the BM25 score for each document and doing so in a quick manner at runtime. (~10 hr) Time will likely be needed to optimize both the BM25 and document-parsing algorithms to fully take advantage of the limited browser resources. (~10 hr) Refinement of the extension based on feedback from tests on a broad range of web pages."
https://github.com/matthewmechtly/CourseProject	README.md	CourseProject The Project Proposal is located in Project_Proposal_CS410-Team_Discovery_Channel.pdf As of 10/24/2021, this is the only work that has been done on this project. Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Sinba7/CourseProject	CS410 Project Proposal.pdf	Project Proposal 1.1 Team Information Name Ningyuan Zhang Jacky Sa Sharanya Balaji NetID nz13* Captain mjsa2 sbalaji3 1.2 Topic Introduction Our topic is Fake news detection. The goal is to develop an RNN model and various model improvements such as GRU, Transformers, LSTM to predict the fakeness of a news article. Importance of the topic: With the availability of news exploding across social media, it is very important to filter out fake news to prevent their damage on society. We believe that deep learning techniques can leverage the sequence of words to model the likelihood of a news being fake. This relates to the topic of text analysis and classification as taught in the class. Approaches: The planned approach is to preprocess the text, use general word embeddings, and start with a general RNN model architecture. We will then add complexity and improve the model to address any memory issues, such as using LSTM, GRU and transformers. After training, evaluating and refining the model, we may develop a simple app to let users enter news articles and our app will detect its fakeness. Tools/Systems/Datasets: Python/Pytorch/BERT/Transformers Fake News labeled dataset from kaggle: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset Expected Outcome: Well-trained RNN model to detect fakeness of any given news and an app to leverage the model Model Evaluation: Precision, Recall, AUC Score, Confusion Matrix Programming language: Python 1.3 Workload Justification We have 3 people => 3x20 = 60 hours. Main task: 1. Exploratory data analysis To have an overview of how the dataset looks, including n-gram analysis for text attributes, distribution analysis for targets(balanced or not). 5 Hours 2. Feature Engineering: In this case most of the work would be text preprocessing and text embedding. We will explore several general pre-trained word embedding models and compare the performance. 10 Hours 3. Model Architecture: We will develop a simple RNN model and gradually increase the model complexity(LSTM, GRU, BERT, Transformers). 25 Hours 4. Modeling Training and Tuning Parameters: 15 Hours 5. Model Evaluation: Choose different metrics to compare the performance of different architectures on this dataset and finally choose the best one. 10 hours 6. Extension to Real Applications:( if have time) Build a simple app to help detect the fakeness of a user inputed news, alongside other potential UI features. 10 Hours ~75 hours total
https://github.com/Sinba7/CourseProject	Progress Report.pdf	CS410 Project Progress Report Tasks completed: * Exploratory data analysis * Analyzed distribution of words/data * Cleaned data * Split data into test and training set * We plan on developing a bidirectional RNN to analyze the fakeness of news articles, and have discussed other RNN methods such as GRU Tasks remaining: * Develop RNN models and its variations * Analyze model results/ conclusions * Develop application to use the model o We plan on creating a simple website to let users enter news articles and we will return the fakeness of the news article Challenges: * Finding a good embedding layer for the training words in the data to use in the RNN model. * We studied and analyzed Word2vec as an option.
https://github.com/Sinba7/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kb-rahul/CourseProject	Progress Report.pdf	Completed Tasks: 1. I have identified the issue specific twitter handles to be - Pampers, Tide and Gillette (@pampers, @tide and @Gillette) as specific brands to work with as they have a lot of customer interaction typically about their complaints regarding the product that they are using. 2. I have explored different twitter scraping tools such as taspinar/twitterscraper, bisguzar/twitter-scraper and TWINT. I found that TWINT suited my need the best as it was accessible through the command line interface and provided all the necessary flexibility. 3. I am working on cleaning the scraped tweets so that I can eliminate spam and inappropriate tweets Pending Tasks: 1. Once the data is cleaned, I need to split the data into train and test components. 2. Evaluate the best deep learning vector space embedding UKPLab/sentence-transformers 3. I need to index the cleaned data into annoy (fast indexer for retrieving the nearest neighbour in vector space) using the vector space embedding chosen in step 2 4. Report the train and test accuracy of the system. Challenges: 1. Twitter response api changes affect the tool and cause it to break. I need to figure out a mechanism to scrape the data in bulk and store it in the system. 2. Choosing the correct vector space embedding optimization might require time than initially estimated.
https://github.com/kb-rahul/CourseProject	Project Proposal.pdf	Project Proposal - Curated FAQs CS410 Student Details: Name Fnu Kishan Borule Rahul NetId fnuk2@illinois.edu Team members Working Alone Captain Fnu Kishan Borule Rahul Team Name NFSMW Project Name Curated FAQs Task: Build a system to retrieve Frequently Asked Questions (FAQs) that users have about certain consumer products (Like Pampers -Diaper Brand, Tide - Detergent etc) from twitter and rank them to provide the relevant resolution to the problem. We often see that consumer product goods (CPG) companies are active on their social media sites (like twitter) for answering the queries from customers. Building a system to automatically retrieve the nearest problem that people have in mind with the resolution to the problem will have multiple points of impact. We can see that customers can effectively resolve their query with no wait-time from the company's representative to answer the query and moreover, the agent will have less questions to answer and henceforth, be more effective in their job of serving the customers better. I will describe the system's implementation in the next section. I plan to use python as the programming language for implementation. Further, I assume that I will require 40 hrs of quality time to complete this project. Current assumption on the tools required: * Twitter Scraper - https://github.com/taspinar/twitterscraper * Sentence Embedder - https://github.com/UKPLab/sentence-transformers * Indexer (To create data index) - Faiss or Annoy * Display the working prototype on streamlit or command line Project Description: Part 1 - Data Set Creation: Identify the twitter handles where company's representatives are most active and scrape the data from these handles. We will need to further clean this data with some text mining heuristics to retain a good diversity of questions for our data set. We will split this data into two groups of train and test set (90% and 10% split respectively). The 10% of test set data is purely used for evaluation purposes. Part 2 - Exploration of Vector Space Model We need to explore and identify the deep learning based pretrained sentence embedding model available here - https://github.com/UKPLab/sentence-transformers and in the repository of hugging face. I will optimise the trade-off between the speed of model evaluation on cpu with the sentence accuracy mentioned for these models and choose a model that works the best in this scenario. Further, I will choose 3 more models that are optimised for high accuracy and reserve them for evaluation. As the evaluation is done offline, I don't have to worry about the inference time. Part 3 - Indexing Once the sentence embedding model is chosen, I will index all the questions and store the relevant mapping to answers in an open-source indexer like annoy or faiss. This will help me retrieve the relevant questions quickly. Further I will create an API for this indexer so that it can work with any system. Part 4 - Interface I will create an interface for the users to interact with the whole system. Steam-lit provides a clean web-interface for users to interact with the system. Evaluation: As mentioned in the part 2 of the project description, we will select three independent sentence embedding models and evaluate the similarity score with 10% of customer queries (held out test set) . We will report average similarity scores (with geometric mean average) retrieved from these independent models for the test set.
https://github.com/kb-rahul/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kcen14/CourseProject	CS410 Course Project Proposal - KC.pdf	CS410 Course Project Proposal: Improved Keyword Search Team Name: Blue Team 1) What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This will be an individual project by Kevin Cen, netID: kcen2. 2) What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? I have chosen Intelligent Browsing as my theme. My topic within Intelligent Browsing will be to try and improve keyword search on a specific page. This is a problem because keyword search on a webpage currently only looks for exact matches, and thus often a user will miss similar keywords or themes. Or perhaps the user does not know exactly what keyword he/she is looking for on a webpage and can only enter a related keyword and hope to find relevant matches. Thus, because current search capabilities are limited to exact keyword match, I hope to expand upon that capability by allowing users to search over the page using a common retrieval function such as BM25. This relates to the theme and class because it makes browsing more intelligent for the user and implements text/information retrieval techniques learned from this class. 3) Briefly describe any datasets, algorithms or techniques you plan to use I plan to use the BM25 retrieval function to try and expand upon the capabilities of keyword search. If time permits, I hope to use other retrieval functions that we have learned in this course. 4) How will you demonstrate that your approach will work as expected? I hope to try and demonstrate the approach works by using user feedback - specifically with me as the user making judgements on whether the search results are relevant to the query/keyword. I will test over a number of different documents/websites/topics and then judge the percentage of relevant returned results. Obviously this will take time and is not efficient over a larger scale but for the purposes of this project I believe it will be the best way to judge whether the extension is helpful or not. 5) Which programming language do you plan to use? I plan to implement this project using Javascript or Python. 6) Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I believe this course project will take me at least 20 hours, which is the expected workload. I did not have much experience coding in Javascript/Pythong prior to this class and have not worked with browser extensions before so I believe learning how to work with those extensions will take a good amount of time (10+ hours). Implementation of the BM25 algorithm will likely take another 6-8 hours and then testing and ensuring the approach works as expected will take another 4-5 hours. This adds up to at least 20 hours, and I hope to try and implement another retrieval function with which to compare BM25 if possible, which will take another 6-8 hours.
https://github.com/kcen14/CourseProject	CS410 Project Progress Report - KC.pdf	CS410 Course Project Progress Report: Improved Keyword Search Team Name: Blue Team Team Members: Kevin Cen, netID: kcen2 1) Which tasks have been completed? Progress so far has consisted of watching/reading tutorials on Javascript and implementing browser extensions in Google Chrome. While I realize that progress has not been substantial thus far in terms of implementation, I have not had sufficient time these past couple weeks to dedicate to the course project. However with fall break and the holidays coming up, I will truly have time to dedicate to finishing the course project. 2) Which tasks are pending? Implementation of the BM25 and other text retrieval functions (if time permits) in a browser extension format. This will involve coding in Javascript, debugging and ensuring smooth functionality. 3) Are you facing any challenges? The main challenge so far has been learning how to create and code a browser extension which has taken my time so far. The secondary challenge has been time over the past couple weeks, which should be overcome given upcoming fall break/holidays and the lighter course schedule in terms of lectures & quizzes.
https://github.com/kcen14/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/nmohiu6/CourseProject	Progress Report CS 410 Final Project.pdf	The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? 1: Our team has completed many tasks. The first task that we completed was creating a chrome extension. This extension is going to be used by the user, and was necessary for our project. Although this extension is very barebone and has little functionality so far, this will be used for our project. This has been done by Nabil. Next, we have figured out how to scrape the Youtube Captions. This is also necessary for our project, as we will be using these captions to compare them with the user's input. This was done by Snehal. Finally, we did a bit of research on the algorithm of sorting the results. This will be used to return the most relevant results to the user. This was done by Dhyey. 2: The main aspect that we have to do is bring everything together. Right now, our components work fine independently, but we have to make sure that we can tie everything together and make our final product work in unison. This means that we have to make sure that the chrome extension will be able to access the youtube captions, and then sort the most relevant videos after taking an input from the user. Although we have these different components working independently right now, we need to make sure that they will work in unison. 3: Our team is facing a few challenges. First, we noticed that the chrome extension can not work with Python code. We are trying to find a workaround that will make sure that our extension works correctly. Next, we have to make sure that we will have access to the youtube captions. Right now, we were having a bit of trouble with accessing these captions, but we are trying to find a solution.
https://github.com/nmohiu6/CourseProject	Project Proposal.pdf	Requirements If you choose this theme, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? 3. Briefly describe any datasets, algorithms or techniques you plan to use 4. How will you demonstrate that your approach will work as expected? 5. Which programming language do you plan to use? 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Proposal Details 1. Names: Dhyey Dixit, Nabil Mohiuddin, Snehal Somalraju NetIDs: dhyey2, nmohiu6, snehals2 Team Captain: Nabil Mohiuddin 2. We have chosen the intelligent browsing topic. The specific topic we have chosen involves creating an extension which optimizes a user's search for videos. This is done through parsing the transcript of the video and comparing it to the user search, leading to a more accurate search result. This is a problem as a user's search may lead to an undesirable video that does not meet the user's needs. This relates to the theme of the course as we are planning on using a web crawler to browse the transcript as a means to assist the user. 3. We plan to use a web crawler to extract the transcripts of each web crawler. We will store them as files and use Okapi BM25 to rank the videos. If needed, we will make adjustments to the algorithm. We will return the top 5 videos. 4. We will use Cranfield Evaluation Methodology to determine which documents should be marked relevant or not. Next, we will check the ranking algorithm against the documents that we determined are accurate. We will provide key metrics such as precision, recall, and mean average precision. 5. We plan on using the python programming language as a means to create this project. 6. The work should take 60 hours total to complete. The main tasks to complete are to build an extension, devise a web crawling algorithm to extract the transcripts, devise a ranking algorithm, displaying the relevant videos, and evaluate the accuracy. The estimated time for each of these tasks is difficult to estimate as each task has a varying difficulty; however, the first 3 tasks should take up the bulk of our time. The top three tasks could be split among our team members, while all of us could work together to complete the final two tasks.
https://github.com/nmohiu6/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/AShipway/CourseProject	Progress_Report.pdf	Aaron Shipway CS 410 Fall '21 Project Progress Report 11/15/2021 1) Which tasks have been completed? a) Established messaging between Chrome extension and native Python host to allow web scraping and clustering with Python b) Created code for scraping URLs from the results of Bing search query c) Found webbased API SMMRY.com that produces keyword lists and sentence-based summaries of web pages to use for the creation of website specific keyword lists to use for website clustering d) Created code for PLSA clustering e) Tested clustering with keyword and sentence summaries of websites 2) Which tasks are pending? a) Find better way to create keyword list for websites returned by search query - keywords returned by SMMRY API are not discriminative enough b) Learn enough Javascript to transfer URL to native Python host and present results of clustering received from Python host c) Complete Javascript based functionality described above once Javascript tutorial completed d) Complete project presentation 3) Are you facing any challenges? a) Creating a list website keywords discriminative enough for effective clustering is proving to be a much bigger challenge than anticipated. All options I've found for scraping keywords rely on uniform HTML which does not generally exist between different websites. The SMMRY API only returns sentence summaries or keyword lists for about a third to one-half of the sites returned in a search query - even if keywords were discriminative enough, many sites would be missed. Being the only person on the project team, I don't see myself having the time to create code that would create discriminative enough keyword list - if it's even a reasonable task for a project at this scale.
https://github.com/AShipway/CourseProject	Project_Proposal_CS410_F2021_AShipway.pdf	"Project Proposal CS410 Fall 2021 Team ""GroupOfOne"" Team: Aaron Shipway NetID: shipway2 Theme: Intelligent Browsing Topic: Chrome Extension that Clusters Search Results: the extension will retrieve text from a user specified number of the top ranked results of a web query and use PLSA with the retrieved text data to cluster the sites into a user specified number of topic clusters. The clustering results will be presented as distinct URL lists with a summary of most frequent key words in each cluster. When searching the web by query, we get a list of ranked results from the search engine. What follows is looking through the websites to identify pages with relevant content. Providing a cluster analysis of results will speed up the process of finding relevant pages by organizing similar pages into groups. Once a group is found by the user to be irrelevant, he can save time and skip looking at the pages in that group. This is an example of text mining - one of the two core topics of the class. Development Plan: I plan to use Chrome developer tools (APIs, etc) and Python based text retrieval tools to construct the app. Programming languages/tools will be JavaScript, Python, HTML and CSS. Projected Workload: Projected workload is > 20hours. Subtasks: Learn Chrome extension development process, Learn applicable Python based text retrieval tools, JavaScript tutorial, HTML/CSS refresher, Coding, Testing, Final presentation"
https://github.com/AShipway/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tonymuu/CourseProject	README.md	FAANGMULA Job Search Engine The project proposal is under the docs/ directory, or can be found here
https://github.com/waelmb/Results-Caching-System	README.md	CS410 Course Project: Results Caching System Installing the requirements Make sure that you have Python 3.9 or above by running the following in the command line: python --version Clone/download the repository to your local machine Using the command line, go to the main directory: cd YOURPATH\Results-Caching-System\code Create a python virual environment: python -m venv venv Activate the virtual environment: ./venv\Scripts\activate Install the required depedencies: pip install -r requirements.txt Create a copy of .env.example and rename it to .env. Then, update the credentials. Note: these instructions are on windows 10 machines. It should be similar on other systems as well. Running the code Using the command line, go to the main directory: cd YOURPATH\Results-Caching-System\code Activate the virtual environment: ./venv\Scripts\activate To run the microservice: python microservice.py Similarly, to run other individual python files: python FILENAME.py
https://github.com/waelmb/Results-Caching-System	SSW Progress Report.pdf	SSW CS410 Progress Report: Design a Results Cache for Popular Queries (System Extension) Github Link: https://github.com/waelmb/Results-Caching-System 1. Progress Made Thus Far a. Looked at health conferences, medical databases, and medical news sites to find best formatted sites with medical info - these mostly turned out to be news sites b. Scraped the sites for the latest articles and stored the articles as .txt files and to an Elasticsearch deployment for cloud usage in the future. c. Used topic modeling to see what the most popular topics were among the scraped articles 2. Remaining Tasks a. Fine tune the algorithm for identifying popular and relevant queries. i. Issue: results from topic modeling contains stop words which are irrelevant ii. Plan: combine topic modeling with keyword extraction (Yake) b. Create and maintain an Least Recently Used (LRU) cache index on Elasticsearch c. Put together the code within a web framework that can be easily deployed as a microservice d. Clean up, test, and fine tune the microservice 3. Challenges/Issues Faced Thus Far a. Integration with 1-Search to improve search performance i. We are still thinking about how to integrate them to work together in a loosely-coupled way as we aim to deploy this system as a microservice. We investigated potential frameworks that will make cloud deployment very easy, which include Flask and FastAPI, but the decision has not been made yet. ii. The original system uses REST APIs to deliver the search results to the client, but to leverage the microservice we are building in 1-Search, we might need to convert the REST API architecture to Web Sockets architecture. This will allow a stream of results to be delivered as they become available. As such, we can deliver the results from the cache first, then deliver the remaining results as they become available a few seconds later. With that said, this might be out of the scope of this project given the time, but we have not decided yet. b. Operating system variability: since the team members use different operating systems, we faced issues with code reliability, but were able to account for it at the end c. Testing for users with no access to 1-Search: since our microservice will closely run with 1-Search, we are not entirely sure how graders and other testers will be able run our code. Our code not only requires access to 1-Search source code, but also access to an Elasticsearch deployment. 4. Takeaways From Proposal Review a. The comments form the reviewers advised us to reassess our tasks and goals for this project so we can deliver what we proposed. i. One of the suggestions was to look into existing extensions or services that tackle similar problems we are trying to solve. We were unable to find such a service with all the functionalities we desired and thus decided to stick to what we have proposed originally. ii. Another reviewer stated that we may have difficulty completing the tasks due to time constraints. We have decided to utilize an LRU cache instead of storing popular queries, which will thus cut down on the time required for the project.
https://github.com/waelmb/Results-Caching-System	SSW Project Proposal.pdf	SSW CS410 Project Proposal: Design a Results Cache for Popular Queries (System Extension) Github Link: https://github.com/waelmb/Results-Caching-System 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Wael Mobeirek, wmobei2 - Captain b. Suchita Joshi, suchita3 c. Shirley Mao, smao10 2. What system have you chosen? Which subtopic(s) under the system? If it is not listed above, how is it related to the class? The system we chose is 1-Search, which is a medical search engine that is customized based on medical speciality. We are going to build an extension to this system, in the form of a results cache for popular queries to improve the search efficiency. Currently, the search takes between 5-10 seconds for every query as it needs to crawl data sources in real-time since these data sources do not allow continuous crawling. We aim to improve the search performance by caching results that are previously crawled. 3. Briefly describe any datasets, algorithms or techniques you plan to use. a. Health forums and test queries: Dr. Karl & Sophia b. Keyword extraction approaches and tools: Yake c. Use topic modeling to get most popular searches/topics from forums (compare LSA, PLSA, LDA, and Ida2Vec and see what works best - can implement with scikit-learn and PyTorch; if we have time, we can also try using a transformer-based model like BERT) d. Use LRU cache to store and get most popular results on 1-Search e. Scraper: Selenium f. Data sources: PubMed, DynaMed, UpToDate, MayoClinic 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We will be adding a function to allow caching of popular queries. There will be two parts: 1) identifying what the current popular medical query is and 2) caching the search results of that query. The function would work as expected if the query time for the current popular topic using our system is faster than 1-Search. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. We will design the cache as a separate microservice. It will run on the cloud separately in its own container. The service will cache results to an elasticsearch index, which can then be utilized by the search engine to fetch query results. The search engine will inform the service of the popular queries by REST API communication. 6. Which programming language do you plan to use? Python for the backend. We will also utilize some web technologies for the frontend, including but not limited to: HTML/CSS/Django 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Identifying current popular queries from outside sources (web crawling) - 25 hrs i. Create crawler, crawl website, and parse relevant info (~10 hrs) ii. Identify the popular and relevant queries (> 15 hrs) b. Create and maintain the index for these popular queries on elastic search - 20 hrs c. Identifying current popular queries within 1-Search - 20 hrs d. Testing and cleaning up the service to make it more user friendly - 20 hrs
https://github.com/kristina2018/CourseProject	Progress_Report.pdf	CS410 Project Progress Report November 15, 2021 Team: Topic Miner Member/Captain: Kristina Hill (kdh5) Which tasks have been completed? * [1h] Dataset acquisition and enablement, videos and transcripts from the UIUC Text Analytics Coursera course Which tasks are pending? * [3h] Segment dataset and explore topic of video segments in the current minute-long video segments * [2h] Examine paragraph breakpoints in video transcripts - explore topic within segments * [6h] Implement Gensim method of developing topic clusters * [5h] Translate results of Genesis topic segmentation into video timestamps and incorporate empirical feedback to compare timestamps with current minute-long segments. * [2h] Compare user-friendliness of timestamps created with all three methods and provide empirical feedback. Are you facing any challenges? Yes, I'm quite behind in the course. I hope to use the Thanksgiving holiday to catch up on course quizzes and this project. Wish me luck :)
https://github.com/kristina2018/CourseProject	Project_Proposal.pdf	"CS410 Project Proposal October 24, 2021 Team: Topic Miner Member/Captain: Kristina Hill (kdh5) What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? * Enhance the algorithms used in Smartmoocs: the project is hosted at smartmoocs.web.illinois.edu. * As a learner I would like to be able to revisit a lecture and jump directly to the portion of the video that addresses a certain topic (for easy revising before exams, or revisiting topics that are more complicated). * Explore better ways to segment lectures based on topic transition. At present, the lecture is segmented into uniform length 1 minute segments. A better way is to detect points of ""topic change"" in the lecture transcript and segment the lectures such that each segment discusses a different topic. Briefly describe any datasets, algorithms or techniques you plan to use. What is your planned approach? What tools, systems or datasets are involved? * Explore the Gensim toolkit in Python to develop topic clusters and segmentation points (timestamps) within each lecture on a MOOC * Utilize the videos and transcripts from the UIUC Text Analytics Coursera course Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. * [1h] Dataset acquisition and enablement * [3h] Explore topic of video segments in the current minute-long video segments * [2h] Examine paragraph breakpoints in video transcripts - explore topic within segments * [6h] Implement Gensim method of developing topic clusters * [5h] Translate results of Genesis topic segmentation into video timestamps and incorporate empirical feedback to compare timestamps with current minute-long segments. * [2h] Compare user-friendliness of timestamps created with all three methods and provide empirical feedback. * Possible exploration of additional algorithms for topic mining including the NLTK toolkit - compare algorithm performance using empirical feedback."
https://github.com/kristina2018/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/siswara/MedRecommendExt	PROGRESS REPORT.md	Progress Report Completed Tasks Chrome Extension Setup: We have been researching and implementing the bare bones chrome extension setup. Installing and running training BioBERT. Installed tensorflow version 1.15.0 to run biobert as newer version deprecate Optimizer used in biobert Utilize BC2GM, BC4CHEMD, BC5CDR-chem, BC5CDR-disease, JNLPBA, linnaeus, NCBI-disease and s800 as Corpus for Named Entity Recognition (NER) training data Register team to Azure service for Bing Search API Added Bing Search API call implementation Ongoing Tasks Implementation of Page Scraper to create word collection from page Implementation of PLSA Algorithm to grab topics/keywords & key terms from scraped information Implementation of BM25 to retrieve definitions for each retrieved word/term Parsing and ranking the result provided by Bing Search API Integrating web extension with web pages and highlighting relevant words Challenge For the Chrome extension, the background language has to be Javascript, but the library we are working with is Python based. To solve this, there are a few options including compiling the Python to Javascript or finding a Javascript based library. For biobert the learning takes a long time, the latest training trial lasts for more than 4 hours. Configuring the PLSA algorithm to a level where only specific medical keywords are considered is proving to be challenging.
https://github.com/siswara/MedRecommendExt	README.md	CS410 CourseProject : Medical Recommendation Extension Team The Recommenders Members : * Satyo Iswara (iswara2) (captain) * Molly Graton (mgraton2) * Eric Wong (etw2) Challenges Medical journals/articles contains a lot of terms that are domain specific and thus create a challenge for user to comprehend the main concepts in it. Our team wants to bridge this challenge by adding a recommender system that helps users to bridge this problem. This is relevant to the theme of the class as it extends to an intelligent browsing function and utilizes search/ranking to provide quick information to the user. Algorithms / Techniques Our team plans to use PLSA algorithm to generate several topic and significant terms from users browser current page. After topic and significant term identification, our extension will search the web (via Bing Search API) for related pages for term definitions. On the returned list, we plan to further rank the document with BM25 to present user with the most relevant information. Data Sets For this recomendation system we are planning to use the following external data sets and resources: * BioBert (https://github.com/dmis-lab/biobert) BioBert will provide us background languange model that we will use to implement our PLSA model. Since BioBert is specifically designed for biomedical text mining task, this language model fits our project challanges. Bing Search API (https://www.microsoft.com/en-us/bing/apis/bing-web-search-api) Bing Search API will provide us with a shorthand of all pages related to our query term which we plan to further rank. Demonstration Our team plans to demonstrate our program by pulling up a medical article, showing which words are highlighted, viewing related links of such words as provided by the chrome extension. The provided links would provide definition to the highlighted terms. Programing Languages This project will be implemented using : * HTML/CSS for browser UI * Javascript for the extension functionality * Python for text analysis backend Workload Estimated time spend are follows | Work Item | Hours | | --------- | ------------: | | Setting up BioBert | 6 | | Implementation of page scraper | 4 | | Implementation of PLSA algorithm | 10 | | Implementation of BM25 | 10 | | Implementation of Bing Callout | 5 | | Integrating Chrome extension styles | 20 | | Quality Testing | 10 | | Documentation | 5 | | Total | 70 |
https://github.com/pehartma/CourseProject	project_proposal.docx	"CS 410 Fall 2021 Authors: Peter Hartman, Steven Hernandez, SangJin Hyun Project Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Peter Hartman (NetID: pehartma), captain Steven Hernandez (NetID: stevenh5) SangJin Hyun (NetID: sangjin2) What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Our group project follows the Intelligent Browsing topic, by aiming to provide a browser extension for Chrome to highlight keywords in webpages and transform them into hyperlinks which the user can click to search more information on the keyword. This extension would use text retrieval techniques to rank candidate keywords and exclude stop words, thus providing a user like a researcher with a convenient method to improve research efficiency. It would also provide controls to limit the number of keywords as well as search for a keyword. Briefly describe any datasets, algorithms or techniques you plan to use. For this project, we will be using text retrieval to match user query (for keyword search) with the VSM algorithm with stop word elimination, page indexing, and text mining using Wink-NLP library for JavaScript. How will you demonstrate that your approach will work as expected? Our group will be taking on the role of user ""experts"" to judge the quality of highlighted keywords, as well as confirm that tuning controls respond as expected and that the keywords are correctly transformed into hyperlinks and can be operated with well. Which programming language do you plan to use? JavaScript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Member Task Estimated Hours Total (60 hours) Peter Hartman Program structure, beginning script, UI, display and execute module, module integration 24 24 Steven Hernandez Parsing module, computational module, validation coordination 20 20 SangJin Hyun Work on program design, parsing, computational module 20 20"
https://github.com/pehartma/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/BillyZhaohengLi/PLSAprior	cs410_midterm_report.pdf	Project Progress Report: PLSA with Prior for MetaPy Billy Li zl20@illinois.edu Department of Computer Science November 16, 2021 1 Completed tasks * Extending the PLSA model to include Prior & background model: This was a pretty straightforward step as the equations are available on lecture slides and notes provided in MP3[4]. The input fles are read in a fashion similar to SKlearn models with the training documents, labels for the training documents and testing documents read separately. The training documents and labels are used to compute the prior probabilities for each topic and the background model. * Adding model saving/loading functionality: This functionality was imple- mented using the Pickles library in Python. The users can save and load computed class variables (i.e. P(w|z)) to both resume an uncompleted E-M algorithm or save time by not having to recompute static variables such as prior probabilities and the background model. 2 Pending tasks * Integrating the PLSA model with MetaPy: This step is found to be harder than expected: the MetaPy library is written using C++ instead of Python and translated into Python using the PyBind library. This step will require both rewrit- ing the PLSA model using C++ and becoming familiar enough with the PyBind library to integrate the model into MetaPy. * Reading background/topic models from online sources: Cancelled. The dataset found at [3] is a paid dataset, and all free datasets for English word fre- quency are either samples of paid datasets or have a limit to the frequency of API calls. Simply using the overall distribution of words in all documents of all topics as proposed in [4] appears to give satisfactory results. 3 Challenges * Understanding how C++ code is translated into equivalent Python code via the PyBind library. * Implementing the PLSA model in C++ in a style that is consistent with other MetaPy functions. 1 4 Estimated progress * Extending the PLSA model to include Prior & background model: Done * Adding model saving/loading functionality: Done * Reading background/topic models from online sources: Cancelled * Integrating the model with MetaPy: 15 hours References [1] https://github.com/meta-toolkit/metapy [2] https://scikit-learn.org/0.19/datasets/twenty newsgroups.html [3] https://www.wordfrequency.info/ [4] http://times.cs.uiuc.edu/course/598f16/plsa-note.pdf 2
https://github.com/BillyZhaohengLi/PLSAprior	cs410_project_proposal.pdf	Project Proposal: PLSA with Prior for MetaPy Billy Li zl20@illinois.edu Department of Computer Science October 25, 2021 1 Team Members Billy Li (zl20@illinois.edu) 2 Topic Theme: System extension Subtopic: MeTA toolkit 3 Details This project will aim to extend the PLSA algorithm written in MP3 with the added functions of including both a background model and priors and implement this extension as a new function of the MetaPy toolkit. This extension will be implemented based on the formulations discussed in the Week 9 videos on PLSA and LDA, and we will perform a test integration using a clone of the MetaPy repository [1] locally. As the MetaPy toolkit is written in Python, this project will be implemented using Python. Some particular features we plan on including are * Reading the background model from an online source (e.g. [3]), as the large amount of text data required to construct a background model of reasonable accuracy limits the feasibility of reading fles locally. Depending on available online sources, we may also allow the user to retrieve topic models online. * Saving intermediate models. This facilitates operations such as incremental training, which allows the user to run a few more iterations of the EM algorithm should they be dissatisfed with the current model results. We will test our project using the 20newsgroups dataset [2] in sklearn, as it is a real- world dataset and is of the appropriate size (11000 documents). The workload of this project will be estimated to be around 20 hours: * Extending the PLSA model to include Prior & background model: 6 hours * Adding model saving/loading functionality: 2 hours * Reading background/topic models from online sources: 6 hours * Integrating the model with MetaPy: 6 hours 1 References [1] https://github.com/meta-toolkit/metapy [2] https://scikit-learn.org/0.19/datasets/twenty newsgroups.html [3] https://www.wordfrequency.info/ 2
https://github.com/BillyZhaohengLi/PLSAprior	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mersaults/CourseProject	progress_report.md	Mid-Project Progress Report The project has been implemented as outlined in the project proposal so far. Currently, I have implemented a web scraper using Selenium and BeautifulSoup4 which has scraped text data from roughly 450 computational linguistics papers. I plan to implement several different topic models to analyze what subfield of linguistics and what linguistic concepts are most utilized in these papers. Tasks which have been completed so far are: - Set up a web scraping algorithm - Acquired a large text corpus of computational linguistics papers - Set up project dependencies and project repository Tasks which still need to be done are as follows: - Implement a unigram, bigram, and trigram LDA topic model. - Preprocess all of the scraped text data with NLTK for ease of parsing and model quality - Aggregate data and create a data visualization for the final project writeup For this project it has been difficult to generate a generalized algorithm for web scraping. To fix this, I have implemented a series of patchwork solutions to ensure that a variety of pages are able to get scraped for building my model. Other than this, the project has been going smoothly and I am on pace to accomplish all of the project's goals by the deadline.
https://github.com/mersaults/CourseProject	proposal.docx	Names: Jackson Kennel (Captain) NetIDs: kennel2 The goal of this project is to analyze the subject matter of recently published papers in computational linguistics journals in order to identify which topics are receiving the most attention from academics. We plan to scrape the text of papers published in well regarded journals (ex. MIT's computational linguistics journal https://direct.mit.edu/coli/issue). We think that this is an important issue because there are many underloved and underexplored areas of computational linguistics, and the goal of this project is to highlight which areas which are typically neglected or could benefit from additional research interest. We believe that this particular topic is novel because computational linguistics is a rapidly growing area of interest among academics that has not received much attention until recently. Our final deliverable will be a data visualization showing topic coverage by number of papers and number of citations, and a writeup describing and analyzing the data. This will also be accompanied with a meta-analysis describing the successes and/or failures of the implemented topic model. We plan to measure success by comparing the efficacy of our model against the state of the art and the size of data generated. The estimated time scheme for the project is as follows: 8 hours - Web scraping and text processing 1 hour - Training data annotation 7 hours - Topic Model implementation and testing 5 hours - Data visualization 3 hours - Writing/creating final deliverables The tools we plan to use are as follows: Python NLTK + gensim matplotlib BeautifulSoup
https://github.com/mersaults/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/xihec3/CS410_CourseProject	progress_report.pdf	"Progress Report Topic: Chrome ""Find in webpage"" with auto-correction, stemming, and synonym detection Team members: Xiaohe Cheng (xiaohec3@illinois.edu) - Project coordinator Summary Task Estimated time Completion ratio Set up a Chrome extension that can take user input, read HTML, and control (e.g. highlight the search results) 8 hours 60% Implement and test basic auto-correction 3 hours 30% Implement and test basic stemming 3 hours 0% Implement and test basic synonym replacement 5 hours 0% Test and further improvement 5 hours 0% Current Progress * In the first half of the project, I have been focusing on building the Chrome extension. * Plain UI is ready. * Capable of reading and parsing HTML text of the current Chrome tab. * Techniques: HTML, CSS, Javascript. * Tested the auto-correction library. Based on the testing results, I think the library satisfies the needs of this project. Remaining work * Add functionality to the Chrome extension, allowing it to jump around the page and highlight text. * Implement the auto-correction, stemming, and synonym detection feature. * Test the extension. * Prepare documentation and presentation. Challenges * For security reasons, Chrome places restrictions on the usage of libraries. Need to make sure the NLP libraries satisfy the requirements. * Synonym detection seems to be the least-established task among the three. Additional tuning might be necessary."
https://github.com/xihec3/CS410_CourseProject	proposal.pdf	"Project Proposal Topic: Chrome ""Find in webpage"" with auto-correction, stemming, and synonym detection Team members: Xiaohe Cheng (xiaohec3@illinois.edu) - Project coordinator Problem statement: Search within a webpage (""Find"", Ctrl+F) is a very common use case in web browsing. Chrome, one of the most popular web browsers, provides built-in search which finds all occurrences that precisely match user input. However, user input is not always precise. Users sometimes make typing mistakes. Sometimes they want to go back to a paragraph they have read but cannot recall the exact content. Sometimes they only have a rough direction of what is desirable. To cater for this need, this project proposes to build a Chrome extension that enhances Chrome ""Find in webpage"", by adding the following features: - Auto-correction, e.g. user types ""Probablistic"", also search for ""Probabilistic"" - Stemming, e.g. user types ""Creating"", also search for ""Created"" - Synonym detection, e.g. user types ""Novel"", also search for ""Innovative"" This project is highly related to CS410, because tasks like stemming are classic NLP problems. Methodology: The plan is to use JavaScript to write a Chrome extension. Considering the limited time and resources, it is desirable to leverage existing libraries as much as possible. For the three tasks, the npm autocorrect, stemmer, and synonyms package can be good start points. If time allows, I can customize or extend the libraries. Deliverables: A working Chrome extension, with examples that demonstrate the features. Workload analysis: Task Estimated time Set up a Chrome extension that can take user input, read HTML, and control (e.g. highlight the search results) 8 hours Implement and test basic auto-correction 3 hours Implement and test basic stemming 3 hours Implement and test basic synonym replacement 5 hours Test and further improvement 5 hours"
https://github.com/xihec3/CS410_CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/LunaMystic/CourseProject	CS410 Project Proposal.pdf	Team Diet Coke Proposal Group leader: suxiang2@illinois.edu Team members: Suxian Hang: suxiang2@illinois.edu Joshua Wu: jgwu3@illionis.edu Juntong Song: juntong7@illinois.edu What topic has our group chosen: Better Search box Chrome Extension (Intelligent Browsing) Why is it a problem: Google's search engine is extremely robust and powerful, but not without flaws. Google regularly changes how it's search engine works, which can cause some frustrations for users. The abstracts are very short, and aren't always as relevant as they could be. Our group noticed that there's still a gap between utilizing search engines and actually gaining the relevant information. After the search engine prompts out the result page, it actually only provides abstracts of many webpages which potentially could provide useful information to the users. Many times, the users still have to access these websites and keep searching for the information they want manually. Unfortunately, such a process could be very painful sometimes. * Solution: Chrome provides a built-in search box function where users can search for text in the webpage. However this function is limited to exact keyword match and cannot perform any relevance search. Therefore, we can build a chrome extension to replace the built-in search box where we will use BM25 or similar approach to perform relevance search and will highlight the result in different colors based on their likelihood. By this approach, users can use this extension as an upgraded version of Chrome built-in search box. What's more, when users access a website via google search we will automatically record their query. Next, the extension will use the query to find the relevant texts in the web page and notify them to users, which will address the problem posted above. How does it relate to the theme and to the class: The main idea of our project is relevance search, which is a classic text retrieval problem and has been covered a lot in this class. We will be utilizing relevant methods that are covered in class, like BM25 or something similar, as well as providing feedback to our system through explicit feedback. Team Diet Coke What kind of datasets, algorithms, or techniques we plan to use: We plan to consider a variety of techniques for our relevance searches. We will most likely use a ranking function in the form of BM25 or some variant of it for our relevance calculations, and we can use explicit feedback from us (the user in this case) in order to provide feedback to the ranking function. We will use this feedback to adjust the parameters of the function in order to provide better results. Justification to the feasibility of our plan. What will be the expected outcome: Our team members are very confident in the success of this project. Here are the reasons: * Robust and usable system: There are many text retrieval algorithms that have been thoroughly tested and proved robust and effective. And our task has high similarities with the one that these text retrieval algorithms are dealing with. * High accuracy expected: Finding relevent informations fragment in an website is actually a simpler version of information retrieval problem. Before the information is passed in and processed by our extension, the user and search engine actually has filtered out a lot of noise. This means most of the time, the extension will be working on a relatively clean dataset. Many possible challenges in the text retrieval area will not be the case in this project. * User tolerable response time: Although, people will have higher expectations on the response time of our product, considering the difference of data volume between what regular text retrieval system and our system need to handle, efficiency would be a solvable problem in team's time limit. Programming language plan to use: For Chrome extension: React (HTML+CSS+JS) For Rest API interacted by the Chrome extension: Node.js Workload justification (20 * N hours): We have three group members for an expected workload of 60 hours. The task is very robust, and will involve developing an application using React. For every subtask we will work in a new branch and merge to master once finished. Also, we will record our tasks and time spent using trello/pull requests. Therefore, by carefully documenting our work, each of our team members can always increase or decrease workload to match the project requirements. We also can continually adjust our algorithms and provide feedback, which is a lengthy process that can take many iterations.
https://github.com/LunaMystic/CourseProject	Project Progress Report.pdf	Project Progress Report Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? Completed Tasks: Frontend (Mostly DOM interacting and UI): * Create a minimal working example of chrome extension * Obtain Query and Results from the https://google.com/search*, and store them. * Obtained the query information above when the link in the search page is accessed. * Obtain text data from the result page DOM, set up an interface for TR model. * Render result from text retrieval (currently a simple placeholder) Backend (Text retrieval processing): * Basic web scraper that pulls links from a google search and grabs words from the links * Basic implementation of BM25 using an existing library Algorithm: * A basic framework of ranking text that allows us to add in more features and text retrieval algorithms in the future. * Naive BM25 method that can return which parts in the given document are most relevant to our target query. Pending Tasks: Frontend (Mostly DOM interacting and UI): * Replace all placeholder with native-messaging between backend text retrieval model * Make the UI more aesthetic and practical (such as highlighting result) * Make User able to navigate through result via keyboard up/down * Make a Search Box which take custom query instead the google search query * Make a Settings page where user can control search parameters/threshold (optional) Backend (Text retrieval processing): * Improving the text retrieval methods (see Challenges) * Improving the web scraper to pull more accurate text data * Running the algorithm on the aforementioned text data * Combining the components together into final application Algorithm: * Explore better ranking algorithms. We are planning to include a distilled version of the BERT in our product. Challenges: As none of us have built a chrome extension before, we spent a lot of time learning Document Object Model, Chrome Extension Structure and Manifest V3. Also, we found out interacting with the DOM element, connecting between background, active tab and newly-created tab is way more ambiguous and time consuming than we expected. We are also considering how to run our Python code for scraping web pages and running our text analysis algorithm. However, we don't believe that there are straightforward methods of running python scripts dependent on libraries in browser extensions, so it is likely that we will end up using some form of native messaging. Meanwhile, although we have already built a basic BM25 model, to match elements in the web page seems to be more ambiguous than we expected and much harder to achieve an optimal result. Processing the DOM elements is tricky and it is really hard to determine the threshold of relative terms as different users might have different expectations. Those tasks won't be super hard but are expected to be time-consuming.
https://github.com/LunaMystic/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/madstommy/CourseProject	Project Progress Report.pdf	Project Progress Report Team Name: SaintsFC Solo Project by Thomas Cheal, tcheal2@illinois.edu Things that have been completed: 1. Basic Website Design 2. Website Privately Hosted on Github Pages 3. Pulling in Sites as Iframes Pending Tasks: 1. Taking User Input as Queries 2. Making Website Not Ugly 3. Forwarding User Queries to Search Engines So far the biggest challenge has been trying to pull in these different search engines as Iframes. They do not seem to want to cooperate. I am not entirely sure why or how this is happening. I have gotten a couple of the search engines to be pulled in as Iframes, so I am not too worried as to the final outcome of the project, but it would be nice to have all of the search engines that I possibly can. I am not concerned about manipulating user queries to be forwarded to search engines. It appears that all of the search engines have fairly easy and similar formats that should be easy to implement for me. The other issue is not making the website look completely ugly. I had forgotten that my strength is not interface design. Not that it particularly matters, but again it would be nice if my site was nicer to look at. I am considering using Bootstrap (a CSS framework) to help pretty up the site. This wouldn't be much extra work and it could help out tremendously. That being said this likely would impact on the loading times of my site, so I am weary against importing this framework.
https://github.com/madstommy/CourseProject	Project Proposal.pdf	Project Proposal Team Name: SaintsFC Solo Project by Thomas Cheal, tcheal2@illinois.edu The aim of my project is to create a website that enables you to search through multiple search engines at once. The point of this is to compare the results of the different search engines. Even though most top search engines most likely use similar information retrieval algorithms, it is likely that they are different. Take Google for example, a lot of their algorithm is based on open source software, but there is also a significant amount of proprietary software behind it as well. Because of this fact, it is not possible to compare the code of each search engine directly, so it is therefore more useful to compare the results of these search engines. In any case, as text retrieval is an empirically defined field, it is likely to be better to compare results regardless of whether or not we could see the source code. My project's goals would be twofold: One is to allow for the comparison of these search engines for purposes of determining the most effective one, and two is for users to be able to quickly sift through the best results of the top search engines to gather information more effectively. Most of the project will be centered around the user experience, as fulfilling those goals will lead to better experience for the more scientifically inclined individual as well. One thing that I found interesting is that this had not already been done exactly. SearchAll.net is close to what I want, but it still does not allow for easy comparison of search engines, instead it opens a new tab, and also has a bizarre fascination with using voice to search, which makes me think it may not be a trustworthy site. This website https://www.guidingtech.com/58654/multiple-search-engines-web/ lays out ways to search through multiple search engines but they either do not work or cost money (and possibly also do not work), so I think it is a worthwhile endeavor to have this functionality. The main algorithm in play on my website will be interpreting user queries and translating them into the queries that each search engine will be able to understand. I can use the url of each search engine to inject the user query, but it will need to be translated to each of the exact format that the search engines use. I do not expect this to be particularly difficult, but it could run into issues on the more advanced queries, as some preprocessing is done even before the query is sent to the server. I will likely use iframes to display the different search engines. The approach will be to shown to work by displaying the website. It should be very self evident that the website works or it doesn't. To do this, the website will be created using html, CSS, and Javascript. I do not anticipate needing a front end framework for this site, or any backend work. This project should take at least 20 hours (since I am alone). I foresee the workload being as such: 8-10 hours on website setup, 3-4 hours on dynamic selection of search engines, 3-4 hours on interpreting user queries and posting them to each search engine, 2 hours on more advanced queries, 1 hour on hosting (anticipate using Github pages), 2 hours on creating demo or anything else required for the class generally, 2 hours testing, and a blanket 5 hours for any debugging or major issues that may arise.
https://github.com/madstommy/CourseProject	README.md	"CourseProject 10/24 Project Proposal Uploaded: ""Project Proposal.pdf"" 11/15 Project Progress Report Uploaded: ""Project Progress Report.pdf"""
https://github.com/chaarud/CourseProject	progress_report.pdf	"CS 410 Text Information Systems Final Project Progress Report Which tasks have been completed? Which tasks are pending? Here's the tasks from my proposal, annotated with progress: 1. COMPLETED: Assemble and clean data set: 2-4 hours 2. IN PROGRESS: Set up scoring/search engine with the data set: 3-5 hours 3. NOT BEGUN: Augment scoring with user profile initialization: 3-5 hours 4. NOT BEGUN: Augment with thresholding: 2-4 hours 5. NOT BEGUN: Augment with feedback/updating user profile and threshold: 10-12 hours Are you facing any challenges? Deciding on a framework/toolset and understanding the state of the art when it comes to systems that are relatively easy to set up is challenging, but it's not a blocker - just taking a bit longer than expected. Specific feedback from reviews: 1. There is no clear indication on the interface that will be provided Answer: I don't have plans to build any sort of GUI. The UI will be a command line, with text input for initial profile setup, and text output for recommendations. The main reason I'm not building a GUI is that it'd probably take a lot longer than the project target time allotment. 2. Although since your current proposal doesn't incorporate new data pulls from Crunchbase, it does make me think it's more like a search engine than a recommender system. With recommender systems, there is typically a ""dynamic info source"" (Wk 6.5 slide 4) and the system has to decide whether to recommend a new item each time a new item is added. I suggest confirming with TAs as you may need to augment your project to include a web scraper that can pull fresh content from Crunchbase with some level of frequency. Answer: This is a good point. I don't think augmenting to include a web scraper is going to fit within the project target time allotment, but it's definitely something I'll plan to do if possible. If absolutely necessary, the idea of ""fresh content"" is something that could be approximated with a static dataset (eg by holding out a portion of the data and simulating fresh content)."
https://github.com/chaarud/CourseProject	project_proposal.pdf	"CS 410 Text Information Systems Final Project Proposal ""Company Similarity Recommender"" Team Member and captain (just one): Chaaru Dingankar NetID: chaarud2 Topic description The goal of the project is to create a recommendation system that will suggest companies similar to a selected company (ie, a content-based filtering system). The data used will be publicly available company description data from Crunchbase, which is a site that houses information about many companies worldwide. This is an interesting use case because it's important to be able to understand what companies are similar to others (a question that has a valuable answer whether you're a sales person, a researcher, a job seeker, or a potential investor). The approach I plan on taking is to set up a python search engine using either the MeTA or Whoosh libraries, and then augment it with the relevant systems to be able to support a content-based filtering system use case (that is, I would need to add the concept of a user profile that changes over time and is initialized a certain way). My expected outcome is being able to deliver companies that are relevant to a user's profile. To evaluate my work, I will go through the recommendation process myself and see how feedback was incorporated, and whether the recommendations make intuitive sense. Programming language I plan on using Python primarily, for the data processing and delivery of recommendations. I may also use Scala if appropriate, for peripheral operations like data cleaning or loading. Workload justification plan Here's some tasks that are part of my rough plan, and approximate time estimations for each: 1. Assemble and clean data set: 2-4 hours 2. Set up scoring/search engine with the data set: 3-5 hours 3. Augment scoring with user profile initialization: 3-5 hours 4. Augment with thresholding: 2-4 hours 5. Augment with feedback/updating user profile and threshold: 10-12 hours All together, this represents at least 20 hours of work."
https://github.com/chaarud/CourseProject	README.md	"CourseProject The project proposal pdf can be found in the file ""project_proposal.pdf"" The progress report pdf can be found in the file ""progress_report.pdf"""
https://github.com/tajshaik24/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tajshaik24/CourseProject	TeamX-ProgressReport.pdf	CS410 Project Progress Report Team Members * Taj Shaik (tajbs2@illinois.edu) - Captain * Anirban Datta (anirban2@illinois.edu) * Kautuk Khare (kkhare2@illinois.edu) Which tasks have been completed? So far, we've completed the dependency resolution for MeTa (specifically upgrading the ICU dependency). We have also built a testing suite and methodology in order for us to verify that the changes we have made will succeed with newer Python versions. Which tasks are pending? Several tasks remain for us. The first one is to continue iterating upon our changes and verify our changes using a CI tool such as Travis. We then want to release the new MeTaPy package on pip such that it is installable by the user. We also need to test that the new MeTaPy release would work on non-Unix OSes such as Windows, and to eventually execute upon our testing suite (making fixes and other changes as necessary). Are you facing any challenges? As of now, we are not facing any challenges. Plus anything specifically mentioned in the reviews to cover. N/A
https://github.com/tajshaik24/CourseProject	TeamX-ProjectProposal.pdf	CS410 Project Proposal Team Members * Taj Shaik (tajbs2@illinois.edu) - Captain * Anirban Datta (anirban2@illinois.edu) * Kautuk Khare (kkhare2@illinois.edu) Our Intended Project For our CS410 Project, we intend on tackling the problem of MeTAPy not being compatible with the latest versions of Python on two operating systems: MacOS and Linux. We would like to work on this problem due to issues we faced in this class while working on MP 1 and MP 2. Theme 3: System Extension Subtopic: 3.1 MeTA Toolkit Goal: Enhance MeTA and Metapy usability by upgrading MetaPy for the Python 3.8 and Python 3.10 & updating tutorials for installing and using the tool on different platforms (based on the refactors that will be completed). Repo for MetaPy: https://github.com/meta-toolkit/metapy Algorithms and Techniques to be Used For this project, we intend on starting with the initial MetaPy repo. This will allow us to build on the work that is currently being used and we know works, rather than building our own from scratch. We intend on using Software Engineering principles to understand how the MetaPy is currently being compiled and installed. From there, we will go through all the libraries to determine if any libraries are no longer compatible with the latest versions of Python and will find suitable replacements. We will also check and validate any external endpoints being used to fetch some source code and will update those accordingly if they are broken. Finally, we intend on using Python Packaging techniques to build a complete package that can be easily uploaded to and downloaded from pip. How we Intend on Proving Our Work In order to prove that our work has been successful, we will be using 2 tests: first, being able to successfully install MetaPy with Python 3.8 and Python 3.10 on macOS and Linux, and second, running MP 1 and 2 from CS410 with our updated MetaPy library. We require the first test to show that our library can actually run on other student's/user's computers. We will use the second test to prove that the core functionality is present in our library and that we will be easily able to complete and compare the outcomes of the MPs completed on the previous version of MetaPy and the updated one. Since we have a baseline we can use (our own assignments) and know what the expected output is, we can easily identify whether our efforts were successful or not. Programming Language to be Used We will be using Python 3.8 & 3.10 (more specifically Python 3.8.5, the most commonly used version of Python (default on Ubuntu 20.04 LTS) and Python 3.10.0, the latest version of Python). This will allow us to cover our bases and support a wide range of users. Main Tasks & Workload For this project, we expect that the majority of time will be spent on refactoring the existing code to work with Python 3.8 and Python 3.10, which will involve us removing deprecated libraries and functions. Since we will be tackling multiple OSs (Linux and MacOS), we expect the work to be that we will exceed the 60 hour minimum requirement of working on the project. The Main Tasks to be Completed: * Refactor all the source files to ensure that there is compatibility with Python 3.8 and Python 3.10 * Will modify any wrappers to the C++ code to ensure compatibility with C++ compiler * Prepare documentation on how to install the updated MetaPy on the versions of Python we intend on making MetaPy optimized for * Building the progress reports and projects reports/presentations for CS410
https://github.com/duadua9/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/iluvcomputers/CourseProject	Progress Report.pdf	"Progress Report We have roughly sectioned our project into three sections (one for each member): text collection from Coursera, fuzzy match algorithm development and implementation, and Firefox extension creation. Below is the progress for each. Text Collection Completed tasks: Research limitations of the Coursera API -- the API offers only summary information, and is therefore not sufficient for retrieving subtitle text. Corpus can be constructed by collating paginated returns from empty search requests. This is painful, but doable. Assembling timestamp chunks -- this is done by assembling the srt subtitle text data from downloaded form coursera into a corpus. Each entry in the srt is too small to provide reasonable context in search results, so they are chunked into larger documents. This can also be accomplished by hitting the search endpoint and combining the paginated results of an empty query. Pending tasks: Assembling timestamp chunks -- get results from the search endpoint rather than the downloaded srt file. This isn't critical, but it would sidestep the need to serve up the documents from a local webserver. Issues: No blockers right now. It would have been nice if the Coursera API was at all useful. Algorithm Development Completed tasks: Research of algorithms and algorithm selection - we have decided to implement an approximate string matching algorithm (rather than a ranking algorithm). The similarity measure between query and document (caption file) will be similar to Levenshtein distance, with some modifications. We have written a formula for the approximate string matching (still in progress). Pending tasks: Implementation of algorithm. Issues: Unsure of how to parse all substrings in the caption file efficiently. Extension Creation Completed tasks: Research regarding implementing a Firefox extension (also known as ""add-on""). Research regarding basic methodology for retrieving a user query and displaying a ""results"" page to hold the results of our selected algorithm. At this point, we have a basic Firefox extension which can be loaded and which provides a right-click context menu that retrieves a selected text and sends to a new ""results"" tab. Pending tasks - Integrating the retrieved query into an API or other method for obtaining search results based on that query - Displaying the results on the results page - Allowing for manually entering the input query on the results page for subsequent searches Issues - Current methodology for input query retrieval is to copy a selected text to the user's clipboard and paste it in the results tab. There is likely a more efficient solution, but the current implementation will suffice for basic functionality development."
https://github.com/iluvcomputers/CourseProject	Project Proposal.pdf	"Proposal 1. Team members and captain Name NetID Captain: Joey Gaysek jgaysek2 Naina Balepur nainab2 Brian Betancourt brianib2 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen to create a browser extension to fuzzy match Coursera subtitles. This extension would be helpful because search is presently limited to exact match, which may limit some relevant results for users. This relates to Intelligent Browsing because we are improving an existing browser (Chrome, Firefox, etc) as well as Coursera. It relates to the theme of the class because we are using our knowledge from the Text Retrieval section to retrieve relevant documents (subtitles and lecture segments) based on queries. 3. Briefly describe any datasets, algorithms or techniques you plan to use We plan to use the Coursera API to collect the subtitle chunks and video timestamps. If we encounter limitations with the Coursera API, we will continue our work on data pulled from coursera-dl instead. We will then construct a corpus with these documents. To implement fuzzy matching, or approximate string matching, we will explore several methods such as: hamming distance, levenshtein distance, ngram string match, and bk tree. For testing purposes, we will use the CS 410 course as our data source for videos and subtitles. 4. How will you demonstrate that your approach will work as expected? We will compare our match results to current algorithms. If we are able to increase the matched results while still keeping them relevant, our approach has worked as expected. For instance, searching ""dirichlet"" in cs 410 returns 16 matches, however a mistyped query of ""direchlet"" returns zero matches. This is a contrived example, but fuzzy matching can be beneficial beyond simple user errors, as subtitles can have inaccuracies and inconsistencies, making it more likely for a student to miss pertinent information with the current search implementation. 5. Which programming language do you plan to use? Work will be done primarily in Javascript, but we may work with Python as well serverside. That said, should a different language become a better choice, we will pivot. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 20*N => 20*3 => 60 hours - Team organization / proposal refinement / task assignment (5 hours) - Research limitations of api (4 hours) - Research browser extension creation for Chrome/Firefox (6 hours) - Research current state of search (4 hours) - Model Selection / Prototype - Assembling timestamp chunks of subtitles into corpus (6 hours) - Displaying results effectively (8 hours) Some or all of the following: - implement/test hamming distance (4 hours) - implement/test levenshtein distance (6 hours) - implement/test ngram string search (6 hours) - implement/test bk tree (8 hours) - Creating browser extension, integrating queries, search logic and display (20 hours) - Testing (10 hours) - Evaluation/comparison to alternatives (6 hours) => ~70-90 hours"
https://github.com/iluvcomputers/CourseProject	README.md	Fuzzy Search for Coursera Video Sibtitles Presently, Coursera search provides exact match only for queries. Due to the inaccuracies in subtitles, as well as the relative ease of causing typos, there is a clear benefit to implementing approximate sting matching, or fuzzy search. Fuzzy search will help ensure students do not miss out on critically important information.
https://github.com/tonydfth/CourseProject	progress report.pdf	CS410 Tony Jiao Nov 15th Progress Report Progress made: I have made an overall structure of my application. I finished files such as loading in the training and testing data, a testing program that outputs the accuracy after running the test data set through, an initial iteration of the Naive Bayes bigram algorithm, and a main driver for the whole application. Remaining tasks: The rest of the tasks are mainly focused on testing and tweaking the algorithm to have more accurate results. Currently, the training dataset I made only consists of around 200 reviews. This is why the test data results are not as accurate as I hope it'll be. As I input more training data in, some of the constants of the bigram algorithm might also need to be changed. Challenges/issues faced: The main challenge so far is the large amount of time spent on making the training data. I'll have to go into a game review website and manually copy and paste each review, read through it, and creating a label to determine if it's a positive or negative review. This is extremely time consuming as I plan on having thousands of reviews in my training data. I am considering making another scrapping app very quickly to automatically scrap all the reviews from a certain website from the HTML code. This is very similar to one of the MPs in this course so hopefully it won't take too long to make. This also raises another issue since I'm getting those reviews from different websites, so I'll have to change the scrapping program for each website as well.
https://github.com/tonydfth/CourseProject	Project proposal.pdf	CS410 Oct 18th Tony Jiao Project Proposal 1. I am working as a team of 1. My NetID is tonyj2 and I am the captain 2. My free topic is a sentiment analysis on video games based on reviews using classifier. I will make an application to determine if a video game is worth playing or not using the reviews it receives. Sentiment analysis will be performed on each review for the video game to decide if it is a positive or negative review. A classifier will first be trained using video game review data with pre-determined sentiment. A unigram system will be used during this process. The program will then use the classifier to dictate if a video game is worth playing or not based on the percentage of positive reviews. This is both important and interesting as video game journalist has been regarded as one of the most biased form of media in recent years. Inflated/paid rating dominant major video game media sites. Therefore, just doing basic research is no longer enough to determine if a video game is good or bad. Review written by real gamers, however, is a much more creditable source of information. Hence, rather than reading through hundreds of reviews, an application like this can effectively summarize those opinions and output the one thing we truly care about: Is the game worth playing or not? The expected outcome is the percentage of positive reviews for a specific video game. I am going to evaluate my work by comparing the sentiment determined by my program with sentiment determined by my own opinion. 3. I plan to use python. 4. Since this is a one-person team, I would need at least 20 hours. From there, around 5-8 hours should be used for writing the classifier algorithm that uses the unigram system to determine if a review is positive or negative. Then, 10-15 hours will be needed to collect data from various websites such as IGN for video game reviews, manually processing them to determine the sentiment for both training and testing data.
https://github.com/tonydfth/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Reynold-Chan/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jialen2/CourseProject	CS 410 Progress Report.docx	"Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? Scrape data from IMDB API. Decided what data we needed to subtract from the dataset, which included title, director, genre, region, average rating, main casts, and adult/non-adult title. Set up a form of movies using the data scraped from IMDB API (Movie Title : Basic Information) Set up our Mysql database and upload our data on AWS. 2) Which tasks are pending? Research on recommendation algorithms based on users' similarity Implement recommendation algorithms Boolean form of users (User : Movie Title) indicates whether (1 for yes; 0 for no) a specific user watched the movie. 3) Are you facing any challenges? We are stuck at how to quantify similarity How to define the dimension of our vector space How to position users' vectors in vector space How to calculate similarity between vectors How to find real user data to improve First way to achieve: deep scrapy to obtain information about user having watched movies or not Second way to achieve:let user use our website and label movies to ""watched"" or ""not watched"""
https://github.com/jialen2/CourseProject	CS410 Proposal.docx	1. Jiale Ning(jialen2), Li Ju(liju2), Jiawei Pei(jiaweip2) Captain: Jiale Ning(jialen2) 2. Our free topic is a movie recommendation system. It is important because movie likers can use this technique to find friends with similar movie preferences and more movies to watch. If two people have watched many movies in common, we can use collaborative filtering based on similarities to give them movie recommendations. We will use MySQL database to store user information and their preferred movie. We will probably use IMDB to get a movie list. Our expected outcome will be a platform that provides people with movie recommendations based on their preferences. We will evaluate the similarity between the recommended movies and their original watched movies. 3. Javascript, Python, SQL 4. Scraping movie list data from IMDB. 8h Create schemas in the database in MySQL and connect them to our backend code. 12h Design and implement algorithm for movie recommendation. 10h Build a frontend interface to take in user input and show the results. 10h Evaluate recommendation algorithm 6h Improve recommendation algorithm based on evaluation 10h Peer reviewed 4h Total: 60h
https://github.com/jialen2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Giang-nguyen/CourseProject	Project Progress Report.pdf	Project Update: Jobs to Skills 15 November 2021 Nguyen Hoang Giang (HG) gianghn2@illinois.edu Captain Nguyen Thu Giang (TG) tgn3@illinois.edu Member Progress Please refer to the following table to see the completed and pending tasks regarding this project. No Task Hours allocated Member in-charge Progress 1 Crawl job description 5 HG, TG Completed 2 Dataset preparation and cleaning 5 HG, TG Completed 3 Topic modelling 10 HG Starting on 15 Nov a. LDA (5) b. Plsa (5) 4 Topic clustering 10 TG Starting on 15 Nov 5 Analysis of result 5 HG, TG Starting on 21 Nov 6 Report writing 5 HG, TG Starting on 21 Nov Total hours 40 Challenges * Crawling of job descriptions has a limited number of API calls (only 1000 per day), hence we need to deploy divide-and-conquer strategy to mine a number of job descriptions each day. 1 * Job description texts require intensive cleaning due to many non-English symbols. 2
https://github.com/Giang-nguyen/CourseProject	Proposal.pdf	Project Proposal: Jobs to Skills 24 October 2021 Nguyen Hoang Giang (HG) gianghn2@illinois.edu Captain Nguyen Thu Giang (TG) tgn3@illinois.edu Member Background & Motivation With the current ubiquitous use of online job platforms as a main channel of recruitment, it is increasingly easier to identify in-demand skills. This information availability allows potential job seekers to be informed of what skills they need to master in order to gain a headstart in their career. In this project, we would propose the usage of text mining and text analytics methods to increase the efficiency of identifying relevant skills in any given job description and gain updated insights of popular skills in recent job descriptions. Task Description In this project, we are aiming to 1. Identify main skills explicitly stated in English job descriptions. 2. Identify implicit skills related to the main requirements in job descriptions. 3. Identify highly demanded skills in the market based on a collection of recent job descriptions. Planned Approach General Approach We are planning to approach the skill extraction tasks via 2 routes: 1. Topic modelling: As we assume that topics in job descriptions would mainly consist of keywords describing skills needed, topic mining is a possible approach to extract major skills explicitly stated in the job description. 2. Text clustering: a. We would identify several cluster job descriptions. The cluster would help to identify job postings having similar contents in terms of skill requirements and job scopes. 1 b. With a set of skill descriptions, we would identify relevant clusters that these job descriptions belong to, which would help in interpreting the meaning of each cluster. c. Topic modelling could also be applied to a group of job descriptions in each cluster in order to identify related skills. Dataset 1. Job description dataset: We would crawl around 2500 online job descriptions on job portals such as Linkedin, Indeed, etc. 2. Skill description: a. First, we would identify a list of top 50 common skills by mining LinkedIn and Angel (https://angel.co/skills). b. Based on skills listed in step a, we would then obtain the descriptions of each skill on Wikipedia. For example, description for project management skill could be based on the following article on Wikipedia. Algorithm The following algorithms proposed are as follows but not exclusive to: Task Text representation Algorithms Topic modelling Bag-of-word * Latent Dirichlet Allocation (LDA) * Probabilistic Latent Semantic Analysis (PLSA) Topic clustering * Word embeddings: We would explore a suitable word embedding technique such as word2vec, BERT, etc. * TF-IDF K-means clustering Systems and Programing Language We would utilise Python 3 and relevant packages as the main programming language for this project. 2 Task Package Dataset mining phantom, beautifulsoup, selenium Model development BERT, scikit-learn, gensim Expected Outcomes * A model that will take in a job description and output a number of relevant skills. * A list of trending skills as reflected by the collection of recent job descriptions that we crawl. Evaluation Methods As both supervised and unsupervised learning algorithms are used, there is no simple and direct approach to evaluate the system. However, several methods can be combined to validate the outcome. Metrics such as elbow method and silhouette analysis are used for text clustering evaluation. A small dataset including job descriptions annotated with related skills will be used to evaluate the preciseness of extracted skills. Metrics such as mean average precision (MAP) or normalized discounted cumulative gain (NDCG) can be used to validate job matching or skill rank. Work Allocation No Task Hours allocated Member in-charge 1 Crawl job description 5 HG, TG 2 Dataset preparation and cleaning 5 HG, TG 3 Topic modelling 10 HG a. LDA (5) b. Plsa (5) 4 Topic clustering 10 TG 5 Analysis of result 5 HG, TG 6 Report writing 5 HG, TG Total hours 40 3
https://github.com/Giang-nguyen/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Project name: Jobs to Skills
https://github.com/zhihaox3/CourseProject	Progress report – Group LGD.pdf	"Progress report - Group LGD 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. What is the task? Team members: Zhihao Xu (zhihaox3), Qikai Yang (qikaiy2), Hang Qi (hangq2). Captain: Zhihao Xu 2. Which tasks have been completed? First, we read the ACL 2011 paper ""Learning Word Vectors for Sentiment Analysis"" from Stanford University, and learned about the basic structure and features of the IMDb movie reviews dataset. The basic structure and characteristics of the IMDb movie reviews dataset. Our goal is to try to input movie reviews into the model and have it predict whether it is a positive or negative movie review. We have completed the pre- processing of the dataset. The dataset contains the tokenized data for our models. In addition, we have completed a Convolutional Neural Network model from scratch, and achieved a prediction accuracy of 78% for the test set. 78% accuracy on the test set. 3. Which tasks are pending? We are also trying to save sequence information to complete a Recurrent Neural Network model. We are also trying to integrate the GloVe features with the preprocessing stage. Finally, if things go well, we may try to complete the generating fake reviews model to automatically generate fake movie reviews with the model. 4. Are you facing any challenges? A major difficulty is the availability of computational resources. Cloud computing services such as Colab have time and speed constraints. This makes it take longer to complete the project because we need to change a variety of hyperparameters and run them multiple times."
https://github.com/zhihaox3/CourseProject	Project Proposal - Group LGD.pdf	Project Proposal - Group LGD 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team members: Zhihao Xu (zhihaox3), Qikai Yang (qikaiy2), Hang Qi (hangq2). Captain: Zhihao Xu 2.What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic is Sentiment Analysis for IMDB Movie Reviews. It works with the Large Movie Review Dataset. We will train models to detect and classify the sentiment of written text. More specifically, we will try to feed a movie review into a model and have it predict whether it is a positive or negative movie review. A bag of words model, Convolutional Neural Network and Recurrent Neural Network are involved in this project. We will evaluate our work with the precision rate and compare it with different methods and models. 3.Which programming language do you plan to use? We plan to use Python. 4.Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Project Setup - 20 hours Implement Model 1 - 15 hours Implement Model 2 - 15 hours Implement Model 3 - 15 hours Train - 25 hours Test and optimize - 10 hours Write reports - 10 hours
https://github.com/zhihaox3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Sushanta77/CourseProject	Project Progress.docx	CS 410 Final Project Progress 2021 Topic: Topic Modeling on Top Topics Discussed in Quora on Illinois CS Department Team: Panda5 Team Captain: Sushanta Panda Sushanta Panda NetId: Panda5 Completed: Following are the task completed so far Captured the data (manually) from the Quora's Question & Answer which was raised related to the Illinois University & related to the Computer Science Department Do some exploratory data analysis to understand the Text Data collected from Quora. Write a python code for data processing (tokenization, stemming etc.) Pending / In Progress: Following are the tasks in pipeline to be done in the coming weeks Write / Extend the python code to do the Topic Modeling Show the Top 5 / 10 Topics discussed in the Quora Write up the Presentation / Video to describe the whole process Any Challenges: No hard challenges so far
https://github.com/Sushanta77/CourseProject	Project Proposal.pdf	"CS 410 Final Project Proposal October 2021 Topic: Topic Modeling on Top Topics Discussed in Quora on Illinois CS Department Team: Panda5 Team Captain: Sushanta Panda Sushanta Panda NetId: Panda5 1. Introduction: For the final project, I choose the topic of Topic modeling from the Quora feed. I will be extracted all the Quora feed which talks about the Illinois Computer science department and stored it in a text file. This will be a manual feed and all the text will be stored as plain text. The system will then be built over python to do the Topic Modeling of identifying the Top 10 Topics. 2. Overview: The Quora is the platform where the questions and answer are being made by users. This platform was a great platform for the users to raised questions and many users responds to it based on their prior experience and will be very useful to the end user who raised the question. As part of the Project, I will be doing the Topic modeling to fetch out the Top Topics discussed in Quora about the Illinois University for Computer Science (CS) department. First, I will fetch out (manually) the Quora feed about both the questions and answers about the Illinois University's Computer Science Department and stored in the Text file. Then will have a program in Python which will do Topic modeling and fetch out the Top Topics discussed by users in the Quora. This will be present as the form of list from the Program, where I can show the Top 10 or 20 lists based on the limit set in the Program. 3. Motivation: Student often query and research about many universities before they finally apply for the university. This Topic modeling will be helpful to understand what people discussed in Quora about the Illinois University for the Computer Science Department. If let's say the Topics comes out as ""Research"" / ""Project"" / ""Student"" / ""Professor"" would give more confident to a student rather ""Hectic"" / ""Cost"" / ""Simple"". This also gives an uber picture of the department what people should discuss about. Also this program can leverage to other Question & Answer Site (or social media platform) or to other university tool 4. Work To Be Done: I need to figure it out how to do the Topic modeling in Python after the cleaning / pruning / tokenization of the Quora Feed. Also needs to research the Quora feed to see enough discussing available for the Computer Science Department for the Illinois University. 5. Evaluation: I will be manually checking all the Quora feed and do some research in the Google to verify whether the Top Topics pulled out by my program is matching and in-line with the Topic generally people are discussing. 6. Tools: * Python 3.5 * NLTK Natural Language Tool Kit * Numpy * Pandas * Scikit - Learn, Sci-py"
https://github.com/Sushanta77/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kikibean/CourseProject	Final Project Progress report.pdf	CS 410 Final Project Progress report: Sentiment Analysis on Financial Statements(Free Topics Track) Team Member: Hui Huang - huih3@illinois.edu 1) Which tasks have been completed? * Managed to download 'Risk Factors' section of 10-Ks for several tickers from SEC using sec_api Python API. * Preprocess/Clean up the dataset including removing stop words, word lemmatization * Compute Jaccard Similarity/Cosine Similarity of the 10-ks and across time using sentiment bag of words model 2) Which tasks are pending? Other neural network related models Model performance comparison by evaluating the alpha factors 3) Are you facing any challenges? One reviewer suggests me to expand either on the depth or breadth of the topic, I am trying to read any state-of-the-art technology or related papers for this topic.
https://github.com/kikibean/CourseProject	proposal.pdf	CS 410 Final Project Proposal: Sentiment Analysis on Financial Statements Team Member: Hui Huang - huih3@illinois.edu Motivation: A 10-K is a comprehensive report filed annually by a publicly-traded company about its financial performance required by SEC. Information documented in the 10-K includes the history, organizational structure, financial statements, earnings per share, and any other relevant data. As the 10-K document contain information which are not captured by the financials, such as the expectations about the future and risk measures. It's interesting to see if the text information extracted helps in investment decisions for the investors. Plan: Data: Get 10K documents from SEC website. (4h-6h as SEC has limits on the number of calls that can be made to the website) Methods: The bag-of-words model/RNN model to model the sentiments for the companies. (15h Need to get familiar with PyTorch Framework for RNN) Performance Evaluation: Back test on historical stock returns using the sentiments extracted. (3- 6h Need to build a stock return factor model) Expected outcome: Sentiments extracted is an alpha factor for the stock returns J Programming language: Python
https://github.com/kikibean/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/quickcatch/CourseProject	Progress Report.pdf	Trevor Moyer, Alex Powell Progress Report - intelligent browsing (HN = HackerNews, the site we are crawling/making extension for) Since our initial proposal, we have been working on the web crawler. When we first started making the web crawler, we were going to use javascript, but found that it would be easier to write the web crawler in python with the aid of the library beautiful soup, and then store that info from the web crawler on a back end system, with a front end handling all the requests for information. Initially we planned to do search/retrieval for articles, however, we realized that HN actually had a pretty decent search functionality already, making our initial project not too useful. So, we decided to pivot the project to do clustering on these articles and the extension will allow you to find similar articles that have been linked on HN. For the web crawler itself, we are crawling through about a year's worth of articles from the site. Within those articles, we are only gathering specific articles that we feel are more valuable to a search process, and that tend to be longer, such as github repos and news stories. In order to create a more efficient crawler, we created a blacklist functionality to not crawl websites we feel do not add much to the search such as social media postings, or youtube links. We also implemented multiprocessing to ensure that the crawler is efficient from a time perspective as well. In terms of storage required to store all the articles we are looking over, we found that the amount of articles in a year was about 10gb, so we may have to do compression of some sort in the future. However, after implementing this and running it for an hour, we got permanently banned on 1 ip for too many requests. After investigating, we realized that there was in fact an exposed API for HackerNews. Using this, we were able to fetch a specified number of articles between 2 UNIX timestamps. However, there were actually many articles posted each day and doing a GET request for each URL to get the HTML so we can get the text was taking a long time. Thus, we first retrieved all the articles for a timestamp interval and then split up the url's among different processes and they all did GET requests in parallel, which significantly improved the speed. The next step for this is to use Process pooling as it will lower the overhead of spawning num_threads processes for each day. After that, we will be able to move on to the actual clustering/front end. Overall, so far we have completed the crawler for the project, and started learning some javascript for the coming parts. Our plan for the remainder of the time is to to create the front end portion of this extension that does the queries, and then link that to the web crawler directory.
https://github.com/quickcatch/CourseProject	Project Proposal.pdf	Project Proposal - Intelligent Browsing * Team Members o Trevor Moyer - trevorm4@illinois.edu - Captain o Alex Powell - apowell7@illinois.edu * Topic o We are going to do Intelligent Browsing as our overall topic theme. In specific, we are going to apply the bm25 algorithm to hackernews (https://news.ycombinator.com/front ) in the form of a browser extension. This site at the moment does not have a great searching feature and requires a lot of manual browsing to go back in time or to figure out specific articles with keywords etc. By taking all the articles as our corpus, we will be able to build an efficient indexer and search over their archive of articles * Datasets/algorithms o We will most likely be using the BM25 algorithm for doing the retrieval. We will be using the hackernews archives as our dataset, which will require manual crawling/scraping, * Verifying that it works o To verify that it works, we will want to test it on a variety of queries, which we can easily do by using keywords from existing articles. We can compare the results to other retrieval methods as well as simply using google to see it's results. The expectation is that our extension will be able to effectively retrieve similar topics/articles to that which google returns. * Language o Since this is a browser extension, the only choice is javascript as it is quite ubiquitous for browser extensions at this point. * Time Commitment Topic Time Learn javscript frameworks for scraping 5 hours Build scraper/crawler 15 hours Implement BM25 retrieval algorithm in Js 10 hours Link scraper to bm25 retrieval algorithm while taking user input to create extension 10 hours
https://github.com/quickcatch/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kaiyuandou/CourseProject	project_progress_report.pdf	CS 410 Fall 2021 Authors: kaiyuandou Project Progress Report 1) Which tasks have been completed? Crawl text containing keyword typed by users 2) Which tasks are pending? Add ranking function to crawled text 3) Are you facing any challenges? Maybe this extension is not that useful because it only focus on resources on website rather than documents.
https://github.com/kaiyuandou/CourseProject	project_proposal.pdf	CS 410 Fall 2021 Authors: kaiyuandou Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * kaiyuandou (NetID: kdou3), captain 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? The group project follows the Intelligent Browsing topic, by aiming to provide a browser extension for Chrome to crawl related data. With this extension it will be more convenient to save data that user interests. This extension would use text retrieval techniques to rank data in the website, when user types keywords, relevant data will appear for them to choose. 3. Briefly describe any datasets, algorithms or techniques you plan to use. For this project, the group will be using text retrieval to match user query (for keyword search). 4. How will you demonstrate that your approach will work as expected? The group will test it in different situations to prove its efficiency. 5. Which programming language do you plan to use? python 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Member Task Estimated Hours Total (60 hours) kaiyuandou Extension design and programming, testing, writing report. 20 20
https://github.com/kaiyuandou/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/snehangshugithub/CourseProject	CS410 Project Progress Report.docx	Which tasks have been completed? We have extracted source code from git hub https://github.com/CS410Assignments/ExpertSearch. Created local repository and troubleshot few issues and finally we were able to run expertSearch on localhost. We have started analyzing existing code and working on auto-crawler. Which tasks are pending? First, we are planning to create a python script to automatically crawl the faculty pages and extract all the bios data. We are using faculty webpage URL from MP 2.3 dataset for positive examples and will add some negative cases as well. From this bios data dump, we will also try to add the text content extracted from the faculty research interest page if available. Next, we will work on the topic mining part. We will explore PLSA, LDA etc. algorithm to see which one is the best fit for our use case. We will work on the model creation manually in Jupyter Notebooks and will check and analyze the results to understand which model is better. Are you facing any challenges? We have faced few technical challenges while creating local repo. We are using Anaconda for our project due to some issues with Mac OS Big Sur's built-in python versions. When we enabled python 2.7 environment in Anaconda, baseline version of the expertSearch didn't work. There are few missing packages in the baseline library, and due to this we were not able to run expertSearch on localhost. we have troubleshot this issue and installed missing packages to fix this problem. We have faced few challenges with Gunicorn 19.10.0 as well and resolved these issues successfully, finally expertSearch is running on our local machine. We have another challenge as well which is more critical. We were a 2-member team, and my teammate (Indranil Guha, Net ID: iguha4) went back to India due to a family medical emergency, so we have a resource crunch now; due to this we need to descope few items to adjust the workload. We will not be able to focus on UI integration part, also UI improvement part (which we marked as optional in the original proposal). We are only focusing on auto-crawler and topic-mining implementation as standalone python code, most likely in Jupyter Notebooks as mentioned above in #2.
https://github.com/snehangshugithub/CourseProject	CS410 Project Proposal.docx	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team: JUEE Snehangshu Shankar Bhattacharjee (ssb8) - Team Captain Indranil Guha (iguha4) What system have you chosen? Which subtopic(s) under the system? Under this project, we have chosen 'System Extension' to add new function/improve ExpertSearch functionality Briefly describe any datasets, algorithms or techniques you plan to use Datasets: Faculty webpage URL from MP 2.3 dataset for positive examples, for negative example we can use some random URL online (TBD) for negative example. Algorithms: Will be exploring various topic modeling algorithm such as PLSA, LDA etc. and see what best fits. Techniques: Topic Mining, keyword extraction If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Additional features: Topic Mining Will be adding a function to create a topic model using the bios data extracted from faculty page using automatic crawler. We can also try to add the text content extracted from the faculty research interest page (if available) to add to the collection to improve the topic model outcome. Saving the top keywords per topic as the common research areas. We can maintain a dictionary to map the topics to each faculty and display the top 5 research area in the faculty information on the UI. System improvement (optional, if time permits before final submission): UI improvement Implement keyword extraction features from bios, research papers, etc. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will take the ExpertSearch baseline code from GitHub and add our own code on top of it for the following features: (Source code: https://github.com/CS410Assignments/ExpertSearch, URL: http://timan102.cs.illinois.edu/expertsearch//) Adding a script to automatically crawling the directory pages to faculty pages and extract all the bios data. Adding the function for topic mining/keyword extraction UI improvement (optional) Which programming language do you plan to use? Python JavaScript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. # Item / Task description Efforts (hours) 1 Understanding ExpertSearch baseline code, troubleshooting (as needed) 10+ 2 Automatic Crawling features for directory pages and associated faculty pages 20+ 3 Adding the function for topic mining/keyword extraction 20+ 4 UI improvement (optional, if time permits before final submission) 10+ Total 60+
https://github.com/snehangshugithub/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/arhamahmed/CourseProject	CS 410 Project Proposal.pdf	Mohammad Arham Ahmed 10/23/21 CS 410 Project Proposal 1. The team, Zetabyte, has one member/captain Mohammad Arham Ahmed (mbahmed2). 2. The project topic is a recommendation system for open source (OS) projects based on OS projects a user has liked or contributed to. The task is mining the user's preferences and interests of OS projects then recommending similar projects. The planned approach is to apply content-based filtering from the lectures as the recommendation algorithm to find projects similar to one a user likes. This is an interesting problem to solve since it can be difficult to sift through the vast number of projects available and handpick the ones that look interesting. With this recommendation system there is a significant reduction in the amount of time and effort needed for such a task, giving the user more time to understand or contribute to the recommended projects. The system is planned to have some frontend component (command line interface, or website) and a backend engine that is exposed with a simple API. The design will be iteratively improved and it is expected to use several Python libraries for the implementation such as pandas, scikit, and numpy. The dataset to be used is from Kaggle where it contains information on the top-980 starred repositories on GitHub. The outcome is expected to be application of data mining theory to a real-world use-case where the end result are recommended projects the user finds useful or beneficial. Evaluation of this recommender system will be done empirically by asking peers and volunteers to anonymously use the system then provide explicit feedback on its utility. 3. Python will be the programming language of choice for this project. 4. There are several tasks involved to implement this system: a. Data preparation: cleaning, scraping for additional data, and enrichment 5 hours b. Recommendation engine: 10 hours c. Frontend interface: 2 hours d. Analysis and testing: 3 hours e. Documentation and report: 2 hours
https://github.com/arhamahmed/CourseProject	Progress_Report.pdf	Mohammad Arham Ahmed 11/14/21 CS 410 Project Progress Report Completed Tasks The tasks listed below have been completed: * Data preparation: cleaning, scraping for additional data, and enrichment * Recommendation engine Pending Tasks The tasks listed below are pending / in progress: * Frontend interface - started * Analysis and testing - started * Documentation and report - pending Challenges One challenge was with the dataset: there do not appear to be any datasets that exist for recommendation of coding projects, making it difficult to get started with a model and come up with an evaluation strategy. A dataset with just coding project records has since been found and the intent to use implicit and explicit feedback as evaluation, but a golden-standard dataset would have been nice. Another challenge was with data enrichment: initially, there was a small learning curve in fetching additional data to improve the recommendations by querying the GitHub API using GraphQL. The final / current challenge is modelling: this is a work-in-progress, however it was not immediately obvious how to apply content-based filtering based on the lecture content. Review feedback It was suggested to use implicit feedback in addition to explicit feedback for evaluation, this will be taken into detailed consideration when the project reaches that stage. It was suggested to add more information about the dataset: it was available on Kaggle, scraped from GitHub and outlines: the owner, repository name, description, tags, last updated date, number of stars, language and URL of the top 980 most starred GitHub repositories/projects.
https://github.com/arhamahmed/CourseProject	README.md	CourseProject Open source project recommendation system Setup Run pip3 install -r requirements.yaml Create a GitHub PAT token and paste it in a new file github_pat.txt placed in the root repository folder. Running Run python3 main.py then enter a space-separated list of #s from 0 to 980 (WIP)
https://github.com/GregHarrison/CourseProject	Progress Report.docx	"CS 410 Project Progress Report Progress Made: I have implemented some of the web scraping. I can gather recipe ingredients and ratings from recipes. Remaining Tasks: I need to complete the web scraping portion of the project by solving the problems outlined in the challenges section. I will then need to complete the querying portion of the project where I will filter out recipes based on the ingredients from the user. I will then need to implement the recommendation portion, where a list of recipes will be recommended to the user. Challenges: The most difficult challenge I am facing right now is that the website I am scraping uses an infinite scroll where the user must press ""Load More"" to get more items. I have not yet figured out how to activate this and continue scraping from the webpage. Once I figure out how to do this, I assume that I will need to implement some exception handling for the case where I have reached the end of the list and the 'Load More' button is not present."
https://github.com/GregHarrison/CourseProject	Project Proposal.docx	CS410 Project Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Greg Harrison (gdh3) What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? This project will perform web scraping on allrecipes.com to create a recipe recommendation system that compares the ingredients required for a recipe to a list of ingredients that a user provides, and then recommends recipes in order of popularity. Popularity will be based on the average rating value and the number of ratings. The idea is to create a tool that will recommend recipes for a user based on the ingredients they have so they can make something without having to go shopping. The outcome will be a recipe data set containing the name of the recipe, the URL for the recipe, the list of ingredients for a recipe, the required quantity of each ingredient, the average rating of the recipe, and the number of ratings. Additionally, I will produce a tool that will compare a list of ingredients from a file with the ingredients in each recipe and produce a list of recommendations. I will make some effort to avoid recommending duplicate/comparable recipes. This may be done by limiting the number of recipes pulled into the data set, or by checking for this when providing a list of recipes to the user. Which programming language do you plan to use? I will use Python for this project. To pull data from the recipes, I will use the Beautiful Soup Python library. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Building the web scraper will likely take me 10 - 15 hours. Producing a polished tool to compare the data set of recipes from All Recipes with a list of ingredients and make recipe recommendations will probably take an additional 10 hours.
https://github.com/GregHarrison/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/liam-p-23/CourseProject	ProgressReport.pdf	Group Name: Cobra Kai Project Progress Report CS 410 1. Which tasks have been completed? I have created a list of the top 100 law schools according to the 2022 U.S. News Rankings and collected the faculty directory URL for each school on the list. Additionally, I started the data collection process and have scraped faculty information from 27 law schools for the test collection. 2. Which tasks are pending? I still need to collect faculty information from the remaining 73 law schools and then assemble the faculty information from each individual school into a single test collection. 3. Are you facing any challenges? The faculty directory of certain law schools makes the task of scraping faculty information comparatively more difficult and time consuming (e.g. when the results are paginated or when the site lacks an informative structure with labeled tables/divs). There is a similar structural issue with the page layout of certain faculty members, which requires significantly more time and effort to ensure that the scraped page is going to contribute toward building a reliable test collection.
https://github.com/liam-p-23/CourseProject	Proposal.pdf	Group Name: Cobra Kai Project Proposal CS 410 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Members: Name: William Plefka (Captain) NetID: wplefka2 2. What is the type of your project: Is it Data Set Creation or Leaderboard Competition Creation? Project Type: Data Set Creation 3. If your project is Data Set Creation, what is the novelty of your data set as compared with all the existing data sets? Which of the existing data sets is the closet to yours? What new task can your new data set be used to evaluate? How do you plan to create the data set? Project Description: The data set that I am proposing for this project will consist of faculty information from the top 100 law schools in the United States (per the 2022 U.S. News Rankings ). The data set that is closest to the proposed data set is the CS 410 MP2 faculty data set, which contains biographical faculty information from various departments/universities. Another data set that is related to this proposed project is the TREC Legal Track data set, which consists of data to assess the abilities of IR techniques being used for electronic business record retrieval for e-Discovery (the latest version of this data set was released in 2011). The novelty of the proposed data set is attributed to the focus on one academic discipline (i.e. law) and to the complete coverage of every current faculty member of a top 100 ABA-approved law school whose information is readily available via a faculty profile/website. Once completed, this new data set could be used to evaluate various text-retrieval techniques when responding to queries related to both topics in legal research and the associated legal researchers. The creation of the proposed data set will follow a process similar to that of MP2.1, however, on a larger scale compared to the task of scraping a single faculty directory and with more emphasis on generating a reliable test collection that will have actual utility in the evaluation of search algorithms.
https://github.com/liam-p-23/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/LahariP2/Movies4U	Project_Progress.md	Progress Report Lahari Pisupati, Grace Im, Shivani Ingle, Crystal Wang Tasks: Clean dataset, remove unnecessary columns - 2 hours Stop word removal in movie descriptions - 2 hours Clean overviews: stopword/punctuation removal, tokenization, lowercasing - 2 hours Write code to create a topic model - 10 hours Train the model - 2 hours Testing accuracy of topic model output - 2 hours Debugging and fixing errors for the topic model - 2 hours Implement TF-IDF ranking algorithm for search engine - 8 hours Implement Query Likelihood ranking algorithm for search engine - 8 hours Testing and comparing accuracy of both ranking models - 5 hours Write underlying Chrome Extension code - 5 hours Integrate topic model & ranking code into the Chrome Extension code - 8 hours Create UI design - 6 hours Implement UI design - 8 hours Debugging and fixing errors for the search engine - 5 hours Cleaning up code, writing comments - 5 hours 1) Which tasks have been completed? We found four different datasets that are related to IMDB movies; movies, names, rating and title principal. During the process of dataset cleaning, we decided to create a new dataset by selecting essential data from each dataset that is relevant for our project. Since our project focuses on returning movie genres and suggestions based on the inputted summary, we selected data columns such as movie title, genre, IMDB rating, and total number of votes received. We also loaded the dataset and wrote a program to remove any stop words in the movie descriptions/overviews. We further cleaned the movie descriptions data by tokenizing the text and removing punctuations. For topic modeling we decided to use Latent Dirichlet Allocation (LDA), to transform the textual data in a format that serves as an input for training the LDA model. We used the previously written function that cleans the movie data in order to create a tokenized object. Next we converted the tokenized object into a corpus and dictionary. We trained the model with the number of topics equal to the number of genres in the dataset. We finally used a visualization package pyLDAvis to visualize the topic model. 2) Which tasks are pending? As can be seen in the tasks list above, we have completed tasks 1-5, which are the core structure of the topic model, and we still have to complete tasks 6-16. These remaining tasks involve testing our topic model and fixing any bugs (we might need to try out different topic model algorithms depending on our current results), implementing the main portion of the search engine with TF-IDF and Query Likelihood, and developing our Chrome Extension application. 3) Are you facing any challenges? While implementing the topic model, we had some trouble with categorizing words that show up in multiple different topic categories. To fix this, we had to keep track of the number of occurrences of each word per topic. Then, we assigned the word to a topic for which the word occurred the most in. Another challenge we are currently facing is testing the accuracy of the topic model output given that some movies can be classified into various topics/genres. In such situations, we are trying to figure out if we will output the genre that most likely describes the movie or output all genres that the movie fits into.
https://github.com/LahariP2/Movies4U	Project_Proposal.md	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. TEAM CAPTAIN - Lahari Pisupati - lahari2 Grace Im - youjini2 Crystal Wang - cw30 Shivani Ingle - ingle3 What is your free topic? Please give a detailed description. What is the task? Our application has two features, a topic model and a search engine. Topic model: Users can input a summary of a specific movie and we will tell them the genre of the movie based on the inputted summary. Search engine: Users browsing for movie suggestions can enter queries that describe a specific movie, theme or storyline, and we will give them a ranked movie list of recommendations. Why is it important or interesting? Our application has a practical use for people looking for new movie suggestions or interested in filtering out movies based on the genre. What is your planned approach? First, we will work on parsing and cleaning the dataset. Then, we'll implement the algorithms taught in class to train a topic model that recognizes & classifies movies into different genres. We will implement a search engine system using various TF-DIF and other techniques to rank movies based on the inputted query. We will integrate the topic model and search engine with backend methods to fetch data based on user input. Finally, we will create a Chrome Extension to enable easy user usage. What tools, systems or datasets are involved? We are going to use the Chrome API to create a Chrome extension We are using the movies dataset includes 85,855 movies with attributes such as movie description, average rating, number of votes, and genre. Data has been scraped from the publicly available website https://www.imdb.com. All the movies with more than 100 votes have been scrapped as of 01/01/2020. What is the expected outcome? We expect to output the movie genre that is relevant to the user's movie summary input and rank movies based on the inputted query. How are you going to evaluate your work? We will evaluate our work by running our dataset as input and using statistical tests to ensure the results are not random. We will also have various peer reviewers test our application with different input Which programming language do you plan to use? Python, Javascript, HTML Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. *Below are the approximations for the amount of time to complete main tasks. Approximations may change as we start working on the project. Clean dataset, remove unnecessary columns - 2 hours Stop word removal in movie descriptions - 2 hours Clean overviews: stopword/punctuation removal, tokenization, lowercasing - 2 hours Write code to create a topic model - 10 hours Train the model - 2 hours Testing accuracy of topic model output - 2 hours Debugging and fixing errors for the topic model - 2 hours Implement TF-IDF ranking algorithm for search engine - 8 hours Implement Query Likelihood ranking algorithm for search engine - 8 hours Testing and comparing accuracy of both ranking models - 5 hours Write underlying Chrome Extension code - 5 hours Integrate topic model & ranking code into the Chrome Extension code - 8 hours Create UI design - 6 hours Implement UI design - 8 hours Debugging and fixing errors for the search engine - 5 hours Cleaning up code, writing comments - 5 hours
https://github.com/LahariP2/Movies4U	README.md	Movies4U
https://github.com/manuv3/cs410-project	CS410 Project Progress Report.pdf	"Progress Report What we have achieved so far:  Our project requires CS410 transcripts and slides as input corpus for extracting models and for building indexes. So we wrote a crawler (using Selenium with Python) for Coursera, which crawled successfully through the course and downloaded all transcripts and slides. This would have been very time consuming manually. We also believe that this is a reusable component and should work for other courses on Coursera. * The downloaded files are here: https://github.com/manuv3/cs410-project/tree/main/data  We were able to get to speed on using Gensim and added scripts to build corpus out of raw text. This included steps: a. Tokenization using NLTK RegexpTokenizer b. Removing stop words provided both in NLTK and Gensim c. Lemmatization using NLTK WordNetLemmatizer d. Generating and persisting VSM (Vector Space) model as dictionary e. Generating and persisting MM (Market Metrics) format based Corpus. This code was witten making use of Gensims streaming enabled APIs to achieve ""constant memory"" processing, which would be very useful with large corpus.  We built two Topic models based on LDA (Latent Dirichlet Allocation) and LSI (Latent Semantic Analysis), to compare the relative performance. The performance was computed based on ""Topic Coherence'', using ""u_mass"" score per topic. * We found that LDA was indeed providing more coherent topics. As a result, we plan to use it for our project.  Currently, we have managed to create ""somewhat"" coherent topics as shown below:  The above shown topics seem to have decent intertopic distance: All the code to our project can be found here: https://github.com/manuv3/cs410-project/tree/main/code Challenges and next steps  Some of the important technical terms have very low probability across the topics, most probably due to low TF. We want to apply some heuristics to boost their probability. Maybe we will leverage additional documents created by parsing the slides as well as the title of the lectures. This means we want to leverage context in topic modeling process.  We want to explore methods to generate meaningful phrases (bigram) out of the topics or documents. * We want to implement techniques like using NLP ""chunkers"", and ""Context Model"" discussed in the research paper by Professor C Zhai: Automatic Labeling of Multinomial Topic Models. * Professor Zhai has also suggested the use of Word2vec algorithm (based on skip gram modelling)  We still need to analyze the UI design of EducationalWeb, to design our UI component (which is basically an index of topics/concepts)."
https://github.com/manuv3/cs410-project	Proposal_final.pdf	"1. Team The names and net IDs of the team members are the following: - Manu Vinod Shesha (manuv3) (Captain) - Angus Jyu (angusfj2) - Sofia Godovykh (sofiaag5) 2. Topic The topic is ""Intelligent Topic Modeling and Index building of Course on EducationalWeb"". This loosely aligns with Theme 2: Intelligent Learning Platform, with the sub-area of ""ConceptView"". 2a. Problem Courses like ""CS-410 Text Mining and Text Analysis"" are presented on MOOCs (like Coursera/EducationalWeb) as a series of lectures, each lecture roughly discussing one main idea and some related concepts. However, the title of a lecture video may reveal underneath topics/concepts, to a small degree. As a user, it may be difficult to find out where was a topic or concept discussed in all of course lectures. The other challenge is that many times individual lectures contain more than one topic, and all the topics may not be apparent by just looking at the lecture title. 2b. Proposal We believe Topic Modeling may be one effective way to solve this challenge. Intuitively, a list of topics extracted from a particular lecture transcript, should provide a snapshot of what the lecture covers. At the same time, such a model, once built, for a Course, can act as building block for building other solutions which can ease user's learning experience. As a possible application of such a model, we want to demonstrate a topic ""index"" for the course. So, at high level, we want to: Stage 1: Discover primary topic(s) as well as secondary topics in a given lecture. Here, we want to do comparative analysis of algorithms: PLSA and LDA, to measure the relative effectiveness of these algorithms in a paradigm of scientific/technical corpus (like course transcript). This will also give us opportunity to compare effectiveness and ease of use of tools like Lemur, Gensim etc. Stage 2: Explore ways to do Automated Labeling, to create phrases (like 2-gram phrases), which define the ""concepts"" in more meaningful ways, than just ""bag of word"" representation of topics in traditional sense of Topic Modeling. For example: individual topics like ""Dirichlet"" and ""distribution"" can be more meaningful for a user if we can do a bit of semantic analysis and discover labels like ""Dirichlet distribution"". The idea is to use techniques which can improve parameters: Topic Coverage (which means that discovered topics fully represent the document) and Topic Differentiation (which means that the topics are specific enough to differentiate between two documents). Here we want to explore the techniques like using NLP ""chunkers"", and ""Context Model"". The inspiration is the research paper by Professor C Zhai: Automatic Labeling of Multinomial Topic Models. (Please note that this is an analysis/discovery goal for this project and may not necessarily apply to our final solution. At minimum, we will discuss what techniques were applied and how it impacted the topic models) Stage 3: Extend EducationalWeb UI to provide the index-like representation of the topic models, which can provide a mapping of topics to relevant lectures. This will greatly enhance the learning experience as it will be useful for the learner to quickly navigate to specific segments of lecture based on ""topic"". This idea goes beyond the basic search provided by EducationalWeb, where a user can search for a topic and this search task is treated as a simple ""bag of word"" search to give a whole list of results which may and may not be relevant for the user. Also, a traditional search on such learning platforms are not comparable to full-fledged browser, and here, the assumption is that the user has some idea on what he is searching for. Instead, we want to provide a push-kind of model, where we generate an index of relevant topics covered in a course (spread across a series of lectures). (As an option, we will try to index specific video segments, based on topics. However, this is an optional goal for this project, depending on current capability of EducationalWeb) 3. Datasets, Algorithms and Tools 3a. Datasets  CS410 course transcipts available on EducationWeb/Coursera as our Collection Corpus.  Human generated topics (by TAs and former students) for the lectures here, to evaluate our models.  Built-in Background language models, in tools like Gensim, Lemur, etc. 3b. Algorithms We plan to use PLSA (Probabilistic Latent Semantic Analysis) or LDA (Latent Dirichlet Allocation) to generate the topic models. For automated labeling, we would use the techniques like Chunking, Ngram testing, and Semantic Relevance scoring, discussed in the paper, referenced in section 2b. 3c. Tools  Gensim  EducationalWeb 3d. Language  Python  C++ 4. Approach Testing 4a. Procedure Below flow chart highlights the process: 4b. Testing We will demonstrate whether the approach works as expected or not through testing and comparisons. We will draw up a model of what the ideal index output should be by manually going through the videos and parsing out the key topics that ideally would be indexed, and then compare this ideal model to the produced model to see how close to the ideal the model gets. We intend to use the Python/C++ programming language for this testing. 5. Workload Time Estimations Our time estimations regarding the workload will be separated into three main tasks. * Task 1 [Discover primary topic(s) as well as secondary topics in a given lecture]: 15 hrs For this task, we will do some comparative analysis of PLSA and LDA, as well as different tools like Jensim, Lemur etc. to get best Topic models. * Task 2 [Automated Labeling techniques]: 20 hrs For this task, we will need to try different heuristics suggested in the paper as well as additional techniques like text scrapping from slides. like we estimate the time cost to be around 20 hours. * Task 3 [Cranfield Test Suite]: 15hrs For this task, we would build a small script, to provide a reliable, repeatable evaluation of our generated models. * Task 4 [Extend EducationalWeb UI] : 20 hrs Analyze the current UI design and add a section with index-like representation of the topic models, which can provide a mapping of topics to relevant lectures. In total, we expect the project to have a time cost of ~70 hours, which surpasses the 20*3 = 60- hours requirement. 6. References [1] http://timan102.cs.illinois.edu/explanation/ [2] https://docs.google.com/spreadsheets/d/1V_PvgwqUifq6QlmXl-0EDQutaCVErYfSDl09HJwE6W0 [3] https://radimrehurek.com/gensim/ [4] http://www-personal.umich.edu/~qmei/pub/kdd07-label.pdf"
https://github.com/manuv3/cs410-project	Proposal_v2.docx	"1. Team The names and net IDs of the team members are the following: - Manu Vinod Shesha (manuv3) (Captain) - Angus Jyu (angusfj2) - Sofia Godovykh (sofiaag5) 2. Topic The topic is ""Intelligent Topic Modeling and Index building of Course on EducationalWeb"". This loosely aligns with Theme 2: Intelligent Learning Platform, with the sub-area of ""ConceptView"". 2a. Problem Courses like ""CS-410 Text Mining and Text Analysis"" are presented on MOOCs (like Coursera/EducationalWeb) as a series of lectures, each lecture roughly discussing one main idea and some related concepts. However, the title of a lecture video may reveal underneath topics/concepts, to a small degree. As a user, it may be difficult to find out where was a topic or concept discussed in all of course lectures. The other challenge is that many times individual lectures contain more than one topic, and all the topics may not be apparent by just looking at the lecture title. 2b. Proposal We believe Topic Modeling may be one effective way to solve this challenge. Intuitively, a list of topics extracted from a particular lecture transcript, should provide a snapshot of what the lecture covers. At the same time, such a model, once built, for a Course, can act as building block for building other solutions which can ease user's learning experience. As a possible application of such a model, we want to demonstrate a topic ""index"" for the course. So, at high level, we want to: Stage 1: Discover primary topic(s) as well as secondary topics in a given lecture. Here, we want to do comparative analysis of algorithms: PLSA and LDA, to measure the relative effectiveness of these algorithms in a paradigm of scientific/technical corpus (like course transcript). This will also give us opportunity to compare effectiveness and ease of use of tools like Lemur, Gensim etc. Stage 2: Explore ways to do Automated Labeling, to create phrases (like 2-gram phrases), which define the ""concepts"" in more meaningful ways, than just ""bag of word"" representation of topics in traditional sense of Topic Modeling. For example: individual topics like ""Dirichlet"" and ""distribution"" can be more meaningful for a user if we can do a bit of semantic analysis and discover labels like ""Dirichlet distribution"". The idea is to use techniques which can improve parameters: Topic Coverage (which means that discovered topics fully represent the document) and Topic Differentiation (which means that the topics are specific enough to differentiate between two documents). Here we want to explore the techniques like using NLP ""chunkers"", and ""Context Model"". The inspiration is the research paper by Professor C Zhai: Automatic Labeling of Multinomial Topic Models. (Please note that this is an analysis/discovery goal for this project and may not necessarily apply to our final solution. At minimum, we will discuss what techniques were applied and how it impacted the topic models) Stage 3: Extend EducationalWeb UI to provide the index-like representation of the topic models, which can provide a mapping of topics to relevant lectures. This will greatly enhance the learning experience as it will be useful for the learner to quickly navigate to specific segments of lecture based on ""topic"". This idea goes beyond the basic search provided by EducationalWeb, where a user can search for a topic and this search task is treated as a simple ""bag of word"" search to give a whole list of results which may and may not be relevant for the user. Also, a traditional search on such learning platforms are not comparable to full-fledged browser, and here, the assumption is that the user has some idea on what he is searching for. Instead, we want to provide a push-kind of model, where we generate an index of relevant topics covered in a course (spread across a series of lectures). (As an option, we will try to index specific video segments, based on topics. However, this is an optional goal for this project, depending on current capability of EducationalWeb) 3. Datasets, Algorithms and Tools 3a. Datasets CS410 course transcipts available on EducationWeb/Coursera as our Collection Corpus. Human generated topics (by TAs and former students) for the lectures here, to evaluate our models. Built-in Background language models, in tools like Gensim, Lemur, etc. 3b. Algorithms We plan to use PLSA (Probabilistic Latent Semantic Analysis) or LDA (Latent Dirichlet Allocation) to generate the topic models. For automated labeling, we would use the techniques like Chunking, Ngram testing, and Semantic Relevance scoring, discussed in the paper, referenced in section 2b. 3c. Tools Gensim EducationalWeb 3d. Language Python C++ 4. Approach Testing 4a. Procedure Below flow chart highlights the process: 4b. Testing We will demonstrate whether the approach works as expected or not through testing and comparisons. We will draw up a model of what the ideal index output should be by manually going through the videos and parsing out the key topics that ideally would be indexed, and then compare this ideal model to the produced model to see how close to the ideal the model gets. We intend to use the Python/C++ programming language for this testing. 5. Workload Time Estimations Our time estimations regarding the workload will be separated into three main tasks. Task 1 [Discover primary topic(s) as well as secondary topics in a given lecture]: 15 hrs For this task, we will do some comparative analysis of PLSA and LDA, as well as different tools like Jensim, Lemur etc. to get best Topic models. Task 2 [Automated Labeling techniques]: 20 hrs For this task, we will need to try different heuristics suggested in the paper as well as additional techniques like text scrapping from slides. like we estimate the time cost to be around 20 hours. Task 3 [Cranfield Test Suite]: 15hrs For this task, we would build a small script, to provide a reliable, repeatable evaluation of our generated models. Task 4 [Extend EducationalWeb UI] : 20 hrs Analyze the current UI design and add a section with index-like representation of the topic models, which can provide a mapping of topics to relevant lectures. In total, we expect the project to have a time cost of ~70 hours, which surpasses the 20*3 = 60-hours requirement. 6. References [1] http://timan102.cs.illinois.edu/explanation/ [2] https://docs.google.com/spreadsheets/d/1V_PvgwqUifq6QlmXl-0EDQutaCVErYfSDl09HJwE6W0 [3] https://radimrehurek.com/gensim/ [4] http://www-personal.umich.edu/~qmei/pub/kdd07-label.pdf"
https://github.com/manuv3/cs410-project	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/sdgs72/CourseProject	CS-410-group-project-proposal.pdf	Team Name = SDK Github Link = https://github.com/sdgs72/CourseProject Members = Dion Hiananto(hianant2) - (Captain), Kaung Yang (kaungky2), Shyam Sridharan (shyams4) 1. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Extension to support users of Wikipedia by offering for any page visited: a summary of content, ability to ask direct questions pertaining to the page and navigated to relevant sections in the page. Many wikipedia pages are dense sources of information for users. For those visiting multiple pages, they should have the ability to obtain quick answers to their questions and summarizing content to better understand the information available from a specific page. By employing techniques built upon the material of this course we intend to create a Google Chrome extension with the ability to provide additional functionality for Google Chrome users of wikipedia pages. The extension will employ Intelligent browsing to further assist the user in information retrieval. 2. Briefly describe any datasets, algorithms or techniques you plan to use We will be using a collection of wikipedia articles as our dataset and the user's article and question as the input. We will be researching and using a type of Question Answering algorithm to process and answer user questions (IE: BERT). We will be exploring the NLTK library for our needs. 3. How will you demonstrate that your approach will work as expected? We will be doing a video demo of our application deployed locally. 4. Which programming language do you plan to use? Python / Javascript / Frontend Languages 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Research (Searching for the right library/algorithms for TLDR/Question Answering) ~ 10 hour b. Frontend(Extension) design i. Learning how to write and understand extensions how extension works ~ 5 hour ii. Front end design of extensions ~ 15 hour c. Backend i. Setup, Create API, Parsing (given a wiki link, download it and perform TLDR and QA) ~ 8 hour (Deployed locally, will need more time for deploying publicly) d. TLDR(Summarization) ~ implementation and tuning ~ 8 hours e. Question Answering(QA) ~ implementation and tuning ~ 8 hour f. Testing, debugging, reporting, administration ~ 6 hour
https://github.com/sdgs72/CourseProject	Progress_Report.pdf	Progress Report (14 November 2021) Shyam Sridharan(shyams4), Dion Hiananto(hianant2), Kaung Yang(kaungky2) Which tasks have been completed? We have researched our approach to fulfilling the question answering, extension creation, and backend creation portion of the project. We are currently weighing the option of training a BERT model implementation for the Question Answering task or going with a pre-trained model. We will be using python's flask library as our backend system that will be called by the extension via REST API. Thanks to a comment given for our project outline we began looking at using SQuAD to aid with this. We have also begun creation of the extension as well. Which tasks are pending? We still need to start working on fulfilling the summarization task as well as complete our implementation of the Question Answering and extension creation. Then we need to combine these components into a full product. Then we need to test locally to confirm that the product is working and available for use by the teaching staff. Are you facing any challenges? No hard challenges at the moment but we are still considering if we should host the extension locally or publicly and would like direction on this from course staff.
https://github.com/sdgs72/CourseProject	README.md	CourseProject Team Name = Team SDK Members: 1. shyamsri (shyams4@illinois.edu) 2. kkaungyang (kaungky2@illinois.edu) 3. sdgs72 (hianant2@illinois.edu)
https://github.com/rycao/CourseProject	CS 410 Mr. Goose Final Project Proposal.pdf	Final Project Proposal Hotel Review Sentiment Analysis Team Mr. Goose Runyu Cao (runyuc2@illinois.edu) (Captain) Weijie Wang (weijiew2@illinois.edu) Qijing Zhu(qijingz2@illinois.edu) Background Hotel reviews are one of the most important factors influencing a person's booking selection and thus have a high impact on marketing strategies. Our team has chosen to utilize sentiment analysis, a technique of extracting emotions based on the selected hotels' textual reviews and producing feedback and solutions to benefit travelers by comparing different options. A challenge we will be discussing is whether a sentiment analysis model learned on hotel reviews can process different kinds of text across domains. Implementation Planning Dataset We use the dataset from Hotel Reviews Data in Europe and separate them to be used as the training, validation, and testing data. We will perform a series of data cleaning on the dataset before training the model, such as stop word removal, word stemming. In addition, we could convert the review text to n-gram corpus as appropriate for the model of choice. Algorithm Stemming, BM25, Logistic Regression, K-Nearest Neighbor, XGBoost Tech Stack We will use Python 3.8 as our main programming language. Jupyter notebook will be used to simplify the process of presenting the data and trying out different models. Scikit learn will be used for its processing functions and classification models. Nltk will be used for its pre-defined English stopword list, stemming tool, and tokenizer. Metapy will also be used. Evaluation To evaluate the tool and demonstrate, we will show 10 reviews to users, and users are expected to mark these reviews from 1-5 based on their sentiment. After collecting users' scores (true label) and predicted scores, we will measure the average accuracy and average difference. Timeline Task Time estimate Note Dataset Selection 3 hours Background study Proposal 3 hours Dataset Cleaning 10 hours Feature engineering Coding 25 hours Debugging, Evaluation Team Meeting 6 hours Weekly 0.5-1 hour sync up * 6 weeks Progress Report 2 hours Final Report 5 hours Drafting, Proof-reading Presentation 5 hours Recording, Video editing Total  59 hours
https://github.com/rycao/CourseProject	Final Project Progress Report.pdf	Hotel Review Sentiment Analysis Team Mr. Goose Runyu Cao (runyuc2@illinois.edu) (Captain) Weijie Wang (weijiew2@illinois.edu) Qijing Zhu (qijingz2@illinois.edu) Project Progress Report Current Progress Dataset Preparation We use the dataset from Hotel Reviews Data in Europe. The data size is 515k. For the purpose of trying things out, we only took the first 10k rows. Data Cleaning We eliminated irrelevant columns, made everything into lower case, split them into words, removed English stopwords, and stemmed the words. The data was later split into training and testing dataset with rate 80%-20%. Feature Engineering (v1) We then performed 1-3 gram(s) on the positive and negative reviews and picked 200 of the most common grams as sentimental analysis features. Model Training (v1) We used the random forest regressor model as our initial model for it's robustness and ability to play a role in further feature selection. Model Evaluation (v1) We compared the model msr with baseline, and based on the data our model shows it is learning (indicating signs of learning). We also tried some of our own inputs and the results looked good. Things we learned so far We got a valuable chance to get our hands dirty on a real dataset and learned how to use Python data processing libraries like Pandas and Scikit-learn. Pending Tasks The pending tasks include further feature engineering, training other classifiers, fine-tuning classifier parameters, and comparing the performance of each classifier. A stretch goal is to build the web tooling interface for users to get the label for new comments produced by the trained model. Challenges The main challenge we faced was the large size of the dataset, 238 MB, 515k rows, which leads to exceptionally long time in computing n-grams. We can solve this challenge by computing n-grams on only a random subset of the whole dataset and use that as the initial features.
https://github.com/rycao/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Zetmas/CourseProject	410 Project Proposal.pdf	410 Team Project Proposal Team Willis Tower Zehao Miao(Captain, zehaom2), Zuhua Cao (zuhuac2), Fan Wu (fanw8) The theme we chose is Intelligent Browsing. We plan to write a Chrome extension that supports the user to classify their default bookmark list. For some users, they might have a large number of sites bookmarked but don't want to label it every time. This extension is designed to solve such problems. It will be able to sort the bookmark list with intelligence. The user will input a label list, and the extension will sort bookmarks automatically based on the given label list. Such problems are related to document classification and text mining, which is the main topic in this class. As for the dataset, we didn't find a useful one to work on, so we plan to make a sample dataset by ourselves. We will pick 7 keywords and choose the top 10-15 results from the Google search using those keywords, then form a set. We assume only those top results will be considered relevant. Our test will be based on this dataset to check the performance of our algorithm, and test the functionalities. The main algorithm we utilize is BM25. The program we designed will use each element in the label list as a query, and then the program will call BM25 to rank the documents. It will pick the top 10 results to form a folder with the corresponding keyword. For the evaluation part, we will manually label a dataset of common web pages, and perform the classification on the pages using our extension. We will then compare the results given by our extension and their correct labels and compute the accuracy of our algorithm. The programming language we plan to use should be HTML, CSS, JavaScript (React Frontend). Last, we will mainly separate our workload into three parts: frontend, algorithms and testing. [Total 20 hrs] The front end part includes two tasks: web crawling and user interface design. For the web crawling of documents, we need to find a proper strategy to crawl general web pages, and then store the crawled pages in the browser's local storage. This design will take about 8 hours, considering the fact that we spent 4 hours on MP2.1 to crawl a specific type of page, and the crawling of general pages will be more complex. For the user interface design, this will be a normal web application design, which will take about 12 hours considering the UI logic implementation and styling. [Total 21 hrs] The algorithms part has 4 tasks: algorithm choosing, coding, debugging and connecting. The first one for the algorithm should be choosing a proper algorithm for bookmarks classification, which will take about 4 hours to search and understand the current techniques for text classification. Then we need to spend about 8 hours writing the code part for this algorithm, then we should spend 4 hours debugging. After that, it will take about 5 hours to connect our algorithms with our frontend part. It will take 21 hours to complete by adding them up. [Total 20 hrs] The dataset/testing part includes two tasks: dataset forming and accuracy testing. The dataset forming task requires us to look for accessible web pages and label them with our testing classification keywords. Given the dataset size mentioned before, this task will take about 15 hours, considering the time to look for useful web pages and read/label them. The second task requires accuracy checking and fine-tuning the parameters of our algorithm, which will take about 5 hours.
https://github.com/Zetmas/CourseProject	Progress report.pdf	410 Team Progress Report Team Willis Tower Zehao Miao(Captain, zehaom2), Zuhua Cao (zuhuac2), Fan Wu (fanw8) In the proposal, we divided our work into three categories, and the corresponding progress are as follows: Frontend part: Progress made: Learned how Chrome extension works and finished the setup of the front-end code. Finished the UI design, including the keyword selection component and Add Page button. The frontend project code has been uploaded to the repo under /frontend Remaining tasks: Integrate the web crawling code to the frontend and link the algorithm. Attach the UI to the corresponding logic. Setup chrome storage methods. Challenges being faced: Possible UI freeze due to the long processing time of the algorithm. A possible optimization is needed. Algorithm part: Progress made: We've implemented a classic BM25 algorithm in JavaScript. In this model, it will have several documents and a query as the input. Then the model will help us to score each document and create an ID for them. Remaining tasks: The remaining task should be changing it to multiple queries so that it could classify every document based on their scores. Also, we need to connect our algorithm with the frontend and utilize the dataset as our input. Challenges being faced: The main challenge that we met was writing BM25 in JavaScript since we are not that familiar with this language. Dataset Testing part: Compared to the proposal, for the dataset building part, we made some changes. We decided to pick documents from the WIKI pages since it's easy to process. We choose 7 subjects as our base tag to select documents. All document URLs and tags are saved in /dataset/tag_index as reference. The document text is saved in dataset/doc.txt using beautiful soup. Code is saved in /dataset/test.ipynb Remaining tasks: Evaluation for algorithm accuracy, accuracy testing. Challenges being faced: Not sure about how to construct the evaluation part, try to find some template but don't have a clear clue yet.
https://github.com/Zetmas/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/t1tb/CourseProject	Progress Report.pdf	Progress Report Which tasks have been completed? * An effectively working Extension Functionality and User Interface * The extension now can read all text in the current tab * Support precise matching like [Ctrl F], including highlighting and scrolling * Able to jump to the target word * Support find sentences best matched with the query using BM25 * Extra Task Completed: Support Regular Expression matching. * We set up a shortcut key [CTRL+SHIFT+F] to open the extension Which tasks are pending? * We need to improve our user interface * The icon of the extension * Tune parameters like k & b * Decide if there is a better dividing unit compared with sentence, eg. paragraph. Also, sometimes, sentences are not split perfectly. The outcome may only highlight half part of the sentence. Are you facing any challenges? * We are still finding a suitable word bag to do the conversion of synonyms, nouns, verbs, and adjectives. * The extension currently running with a delay Demonstration on how the Chrome Extension works: Currently, we are able to divide contents of Web Pages into documents and run BM25 to compute scores and rank text contents while considering the entire page as a collection and divisions of it as documents. On top of that, we find that at this stage it's best to highlight desired search results with scores above properly set threshold. Users can then conveniently jump across these matching results by clicking on the up and down arrow. The current window is then automatically scrolled into orange-highlighted texts which indicate the current position or focus of the user. It's quite simple to go back and forth between these results to see which best fits users' needs. Feel free to try our extension if you want!
https://github.com/t1tb/CourseProject	Proposal.pdf	Team members Luhao Wang(Team Leader) NetID: luhaow2 Shih-Chiang Lee NetID: sclee8 Jiaxin Ying NetID: jiaxiny7 Topic Intelligent Browsing We want to design a Chrome extension to enhance browser find functionalities. The find function of browsers are among the most frequently used functions while browsing to find useful information. However, in almost all common browsers nowadays, the find capabilities still remain basic. They only support exact keyword match, and won't work even for a missing space or extra comma, which is fairly inefficient. If users do not know the exact keyword to put in, they won't be able to find matches using the find capability, which is far from our theme of intelligent browsing. Also, it would be great if users can find relevant topics they are interested in by using description of contents. For example, they may want to know the movie analysis for a certain plot, then they can describe the plot and search the matching paragraphs. Therefore, we believe introducing a Chrome extension that enables search-engine-like find capabilities will bring users a huge step closer to intelligent browsing and enhance productivity exponentially, as people rely on the Ctrl-F capability so much. The concepts learned in class, such as TF-IDF, ranking functions, text retrieval methods, and topic analysis, mixture models, EM, PLSA, can contribute to the implementation of this project to a great extent. As for techniques to achieve our goals, we want to ensure TF-IDF and document length normalization to rank. Therefore, we deem the BM25 algorithm to be the most appropriate approach for a ranking function, since BM25 is effective and efficient. We believe it is completely capable of supporting such enhanced find capabilities. Our main idea is to consider the entire page as collections and divide components of the page into documents using topic analysis. At this stage, we want to just consider each paragraph in the webpage a document for simplicity. Currently, we only consider ranking Documents that are in English. How will you demonstrate that your approach will work as expected? With properly set parameters, BM25 is shown to work well in finding relevant documents given a query. Here, we treat each paragraph as a document, and treat the whole document as the collection. In that way, we can use BM25 to find relevant paragraphs in the whole article. The document (here components of the webpage) with the highest ranking score can be empirically proved to be more relevant in some ways at least. In the worst case scenario, if our ranking algorithm doesn't return any relevant paragraphs, the results would still be as good as the basic find capabilities of browsers, as our algorithm uses bag-of-words representation to find matches for computing the score and can keep track of locations of the occurences. Thus, our approach can be proved to be better than the basic find capability of common browsers. More importantly, our approach is more user-friendly compared with the exact matching approach, since users can use any format of words and can find relevant contents by describing key ideas. Our plans for completing the project as a group: We are going to hold a meeting once a week to make sure all the group members are doing their work; and we have a google cloud to share all the workload to group members. Although we have little experience on google extension, we will still try our best to learn by ourselves and combine the knowledge we learned from lessons to complete the project. programming language JavaScript, Python, HTML Datasets, Algorithms or techniques we plan to use: 1. UI design - 10h 2. Divide a web page into several documents - 2h 3. Perform stop-word removal to query and doc - 5h 4. stemming & Lemmatization of the searching keywords and doc - 10h 5. Using BM25 to find out relative documents (actually paragraphs) - 20h 6. Finding the optimal parameters of BM25 - 15h 7. Mark the results for users - 5h
https://github.com/t1tb/CourseProject	README.md	CourseProject (Browser Extension) Group Name: Debuff Members: Luhao Wang Shih-Chiang Lee Jiaxin Ying
https://github.com/sanjibg01/CS410-project-team-trailblazers	CS410_ Progress Report - Team Trailblazers.docx	Progress Made Below is the progress made on ArXiv data set and model. All code relating to this step is in this branch of our Github repository. The steps involved included: Read in the JSON Preprocessed the text: Tokenized Removed numbers Lemmatized Removed stop words Limited to nouns (trying this for the topic modeling) Created features for modeling from the preprocessed ArXiv data Created a doc-term matrix and transformed with TF-IDF Currently using single tokens, but may n-grams as well depending on topic modeling results Created some very initial topic models, with the main goal of understanding the library I used the Python library spaCy for the text processing. This library was easy to learn how to use and very helpful, particularly for the part-of-speech tagging that let me easily limit to nouns. I then created the document-term matrix and applied the TF-IDF transform using sklearn's feature_extraction library. I am creating the topic models with sklearn's implementation of Latent Dirichlet Allocation. Below is the progress made on the Coursera lecture data set. Code related to this step is in this branch of our Github repository. Script that downloads Course transcripts (using coursera-dl Python library), parses them into JSON files and writes to Cloud storage so that all team members can access from a centralized place. Preprocessed the text using metapy: Tokenized Lower case Length filter Lemmatized Removed stop words from lemur stopwords list and manual list unique to transcript data i.e. '[MUSIC]', '[SOUND]', '[INAUDIBLE]' Created unigrams (trying for topic modeling) Remaining Tasks Read up on Latent Dirichlet Allocation to better understand the model and its parameters. Determine how to quantify the performance of the topic modeling, and use this to test parameters and pre-processing steps against one another to select the best model for the arXiv dataset. This will involve setting up a train/test splitting scheme. Create feature set for and conduct topic modeling on the coursera transcript dataset. Link the content between the data sources using the topic modeling results. Challenges/Issues For the arXiv dataset, I am currently working with a small subset of my data to build the pipeline and understand topic modeling more broadly. I'll need to expand to the full dataset, which might lead to challenges with computation time and slow down progress. Because we are matching the results from a broad-topic data set (arXiv) to a more narrow-topic data set (Text Info Sys lectures), it's possible that matches for some keywords/topics will be limited or that we'll get the same matches back across many queries.
https://github.com/sanjibg01/CS410-project-team-trailblazers	CS410_ Project Proposal - Team Trailblazers.docx	"Project Proposal What are the names and NetIDs of all your team members? Who is the captain? Sanjib Ghosh: sanjibg2@illinois.edu (Team Captain) Bo-Ryehn Chung: brchung2@illinois.edu Matt DiNauta: dinauta2@illinois.edu Pawel Zuradzki: pzurad@illinois.edu What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Our project falls under the ""Intelligent Learning Platform"" theme. Our idea is to develop a method that links Coursera course content (via lecture video transcriptions) to academic journal articles. This will enrich the Coursera course content by allowing students to see how topics being taught have appeared in scientific literature, hence more 'intelligent learning.' To accomplish this, we will mine for topics in a dataset of academic papers and a dataset of lecture video transcriptions, applying NLP techniques either directly taught in CS410 or closely related. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? We'll use the Python programming language. There are a couple approaches we will explore to demonstrate our project (we will choose one of the following): We plan to create a simple command line application as a proof-of-concept. A user can enter a search query and be returned a list of topics as results. The user then selects a topic. Upon selecting, the user will be presented with related course transcripts and papers. Create an index of topics, with references to the lecture segments related to that topic, and references to papers related to that topic. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. A list of the main tasks as we've currently conceiving of the project: Scrape or download Coursera lecture transcripts. (5 hours) Preprocess the text data: tokenize/stem, BoW with unigrams/ngrams (2 hours) Build a topic model for the Coursera lecture transcript data. We plan to explore different levels of granularity, e.g. associate the entire video with a topic, a paragraph, a sentence, an n-gram. (30 hours) Build a topic model for the academic paper data. We will mine the topics from paper abstracts, using this dataset available on Kaggle.com. (10 hours) Link the results of 2 and 3 together, implement basic search functionality, and build the command line application. If time permits, we may also add functionality that returns to the user ""related topics"" and link to the video clips relevant to the topic. (20 hours). Alternative approach for matching topics of lectures to papers: score on abstract-to-document text similarity instead of topic matches. Provide evaluation (confusion matrix, F1, etc). (20 hrs) Scoring Evaluation (15 hours): need relevant/not relevant labels and topic labels for some test queries. Could label with pseudo feedback, or explicit relevance feedback (manually label the topic and relevant/not relevant) avg precision for specific queries MAP/gMAP across multiple queries nDCG for multi-level relevance research scoring metrics for topics DevOps, Deployment (5 hours) Github repository: https://github.com/sanjibg01/CS410-project-team-trailblazers.git Command line interface (ex: argparse package) Publishing as a package for distribution and reproducibility Ex: `$ pip install git+https://github.com/user/our_project.git` Public data store for datasets (Google Drive, GitHub[?], S3)."
https://github.com/sanjibg01/CS410-project-team-trailblazers	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Jonathan-Birkey/CourseProject	CS 410 Project Progress Report.pdf	CS 410 Project Progress Report Group Name: cs-410-team-yoda 1. Which tasks have been completed? * Setup team standup schedule * Create dataset schema * Investigate and prove out reddit API * Investigate and prove out yahoo finance API * Create stock list csv file * Collect raw stock data * Collect raw reddit data * Curate stock data to fit dataset schema * Curate reddit data to fit dataset schema 2. Which tasks are pending? * So far, we have gathered stock and reddit data for the 25 stocks in the FOMO ETF and are still investigating the level of effort to capture all 500 stocks from the S&P500 * Revise dataset documentation * Fix raw reddit data encoding * Design course project deliverable * Plan course project demo 3. Are you facing any challenges? * Primary challenge revolves around the size of the dataset for all 500 stocks in the S&P500
https://github.com/Jonathan-Birkey/CourseProject	CS 410 Project Proposal.pdf	CS 410 Project Proposal Group Name: cs-410-team-yoda 1. What are the names and NetIDs of all your team members? * Jonathan Birkey, jbirke2@illinois.edu * Dang Nguyen, dangn2@illinois.edu * Mathew McDade, mmcdade2@illinois.edu * Sakshi Maheshwari, smahes20@illinois.edu 2. Who is the captain? * Jonathan Birkey 3. What is the type of your project: Is it Data Set Creation or Leaderboard Competition Creation? * Data Set Creation 4. If your project is Data Set Creation, what is the novelty of your data set as compared with all the existing data sets? * The novelty of our data set comes from the coloration of stock price change and the amount of conversation that stock is receiving on Wall Street Bets. This can help investors predict when there will be massive swings in stock price volatility. 5. Which of the existing data sets is the closet to yours? * Our data set will likely be similar to the datasets found on datahub.io/collections/stock-market-data and reddit.com/r/datasets 6. What new task can your new data set be used to evaluate? * A user will be able to train algorithms against our dataset to see if their algorithm can predict the probability of a stocks rate of change given the amount of communication that stock is receiving on the subreddit Wall Street Bets. 7. How do you plan to create the data set? * We will use reddit's API to scrap data on the Wall Street Bets subreddit for a given timeframe. This will then be curated and formatted in a meaningful way that will be stored as a .txt file. We will then scrape the web for all the stocks in the S&P500 over the same timeframe. This will also be curated and stored with the stocks rate of change for each given day. This is what will be used to test a user's output against.
https://github.com/Jonathan-Birkey/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mrcstan/CourseProject	CS410 project progress report.pdf	"CS410 Project Progress Report - Browser Extension for Intelligent Query Matching Marcus Hwai Yik Tan (leader) Omid Afshar Xue Ying Lin htan8 oafshar2 xylin2 Progress Update * Implemented the following for the frontend (Chrome extension) * Grab text nodes from webpage * Get query from user * Send http request to server with query and text nodes * Receive http response from the backend with matches and find the relevant sentences. Highlight words contributing to the match in a sentence * Order results by their ranking provided by the backend * Add next / prev buttons to navigate between result * Like/dislike buttons have been added to the extension pop-up, allowing a user to indicate whether a particular result returned by the extension for the query issued is relevant or not. This feature enables us to obtain relevance feedback to conduct measurements on how useful our extension is. * Implemented the following for the backend (Python Flask and Gensim packages) * Receive text nodes from frontend and split them into sentences, each sentence corresponding to a document for ranking * Create a mapping from sentences to text nodes * Tokenize the query and the sentences, i.e., separate them into lists of words. Remove punctuations and special characters, make the words lower case and stem the words in the process * Create bag of words representations of the query and sentences * Use BM25 to rank the sentences * Find words in each ranked sentence that are matching the query words * Return results containing the text node indices and the relative locations of the ranked sentences and the matching words in relevant text nodes * We began implementing the backend logic to keep track of the relevance feedback issued for search queries. This logic keeps track of the document and corresponding search query issued on the document to log the user's relevance judgments for each result item issued by our application for the query. This log of relevance judgments can be later used to assess the overall utility of our application. Remaining Tasks * In order to better assess the performance of our extension, we want to surface an option for the user to toggle between conducting an exact match or BM25 ranking search. We will potentially add more ranking options as well. * We will complete the implementation for recording user relevance judgements on the backend. As mentioned previously, each user's judgment will be logged with the following information: website, query, result_ranking, relevance judgement (i.e. relevant or non-relevant). * Calculate MAP * Remove text nodes that are invisible in the browser Challenges/Issues * Setting up the frontend with limited and obsolete online documentations and templates. Templates for older versions of the extensions no longer work with current extension (Manifest 3.0) * Highlighting sentences with different ranks in each text node should be done in a single pass since previous highlighting would be removed in the next pass * Text nodes that are invisible on screen but are sent to the backend. Results that include parts of the invisible text nodes will show up as one of the results in the search bar but not in the webpage * Highlighting is challenging since it involves inserting spans into the text nodes at the correct locations and changing the spans as the user goes over the results with next/previous button * Figuring out what information needed to be included in network requests from the frontend to the backend to ensure that user relevance judgments would be grouped together properly was non-trivial. Initially, we considered generating a unique ID with each user ""session"", but since this would reset each time the user closed and re-opened the extension, we opted instead to rely on the website and search query as the basis of a ""session"". * Lack of prior experience using Flask to receive and send information to the frontend. For example, figuring out how to use app.route function of Flask to communicate with the frontend * Figuring out proper way to parse and rank documents with gensim when documentation is incomplete and scattered Feedbacks from Reviewers * Feedbacks were lost when we uploaded the progress report in CMT. Here are the feedbacks that we remember of our heads * Meta-reviewer * Another group is working on ranking of job-posting. Maybe you can talk to them to split the tasks * Answer: the workload for our group is just right and we are handling it well * Reviewer 2 * Describe data structures used * Answer: standard data structures such as array and list have been used. Similarly, the standard json data format is used to pass data between the frontend and the backend."
https://github.com/mrcstan/CourseProject	CS410 project proposal.pdf	"CS410 Project Proposal - Browser Extension for Intelligent Query Matching Members: Marcus Hwai Yik Tan (leader, htan8), Omid Afshar (oafshar2), Xue Ying Lin (xylin2) Group name: Tree leaves When searching through the text contents of a webpage, users are currently limited to the browser-native functionality of using exact keyword match. In order to make the webpage search capabilities more ""intelligent"", our team is developing a Chrome extension that will allow users to query text in a document and receive ranked results. In this manner, our extension will leverage indexing, retrieval functions (BM25), and relevance judgments to enable a more intelligent search experience. On the frontend, we will need to set up 3 data structures to maintain state about the positions of text within the webpage as well as the corresponding DOM nodes which contain the text. The data structures will work as follows: an inverted index to keep track of terms, term frequencies, document frequencies, etc.; a mapping to track text, absolute position of the DOM node the text belongs to, and absolute position of the text within the DOM node; a mapping in which the keys are the absolute position within the DOM and the values are the reference to the corresponding DOM nodes. The logic of the extension consists of the following: When the extension starts up, it will iterate through the DOM nodes of the webpage's body. For each node, if it contains text, we add each piece of text as an entry into the inverted index and the text-to-DOM-node mapping. We also add each piece of text as an entry into the text-to-DOM-node mapping, and finally add the DOM node to the third mapping. When a user inputs query text, the extension will retrieve the relevant text information from the inverted index for each word (or phrase) in the query text. It will send this to the backend server using a POST request and wait for the response. When the backend server responds with the ranked results, the extension will display the results as a popup and highlight the text on the page. As previously mentioned, the documents are pushed to a local backend server implemented using the Python Flask web framework (https://www.fullstackpython.com/flask.html). The Flask package provides functions that fetch data from the backend server to Python and push data vice versa. A well-established Python package (Gensim package) is used for text analysis. As per standard practice, a vocabulary is established with the provided documents. Stop words are not included in the vocabulary. A bag of word representation is then created for the documents and the query. Ranking of the documents is done using the BM25 ranking function. If time permits, other ranking functions will be explored and provided as a choice to the user. The top N documents with the highest scores are then pushed to the backend server. The performance of the ranking is evaluated with respect to a set of webpages and queries with user-determined relevance judgements. To obtain the relevance judgements, several users excluding team members will be chosen and provided with the extension together with a predetermined set of webpages and queries. The user can mark a highlighted group of words as relevant/irrelevant. Mean average precision is used as the performance metric. Member Task Workload (hours) Research Implementation Testing Total Marcus Backend (Python and Python Flask), Integration (Chrome extension) 5 8 7 20 Omid Frontend (javascript), user relevance judgements 5 8 7 20 Xue Ying Integration, GUI (Chrome extension), user relevance judgements 5 8 7 20"
https://github.com/mrcstan/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/lynn0032/CourseProject	Project Progress Report.pdf	Project Progress Report: Classifying Party of Political Tweets Melissa Lynn For my project, I will scrape political tweets from Twitter, build a dataset of tweets labeled by political party, and train a classification algorithm to classify tweets by political party. 1. Which tasks have been completed? I have assembled a dataset (in a pandas dataframe) containing a record for every member of the United States Congress, containing the following information: (1) Their first and last names. (2) The state and/or district they represent. (3) Whether they are in the Senate of the House of Representatives. (4) Their political party. (5) Their Twitter Handle. I decided to use Tweepy to scrape Tweets from Twitter. In order to use Tweepy, I need authorization from Twitter, so I applied for a developer account. Hopefully that will approved within the next few days. In the meantime, I have written code that should scrape the tweets and store them in a dataframe, along with the additional context of the dataframe mentioned above. I have partially tested this code, and will start scraping tweets as soon as my account is approved by Twitter. 2. Which tasks are pending? Once my account is approved by Twitter, I will scrape tweets to create my dataset. Once I have a dataset of tweets, I will do the following: * Do some data preprocessing on the tweets, to create a vector as in the bag-of-words model. I would like to experiment with different options here, to see how they affect the model I train. * Exploratory data analysis. Since I think that the dataset itself will be interesting, I want to dedicate significant time to exploring the data, and trying to identify any interesting patterns. * Training classification algorithms. I will experiment with different classification algorithms and parameters, to try to produce the most accurate classifiers that I can. I think it would be particularly interesting to look at decision trees and linear models, where I can easily interpret how the models are determining a classification. Note: the code that I've written will automatically label the tweets with the political party, so I don't need to do any extra labeling of the data. 3. Are you facing any challenges? Just waiting for my authorization for a Twitter developer account to be approved. 1
https://github.com/lynn0032/CourseProject	Project Proposal.pdf	Project Proposal: Classifying Party of Political Tweets Melissa Lynn 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Melissa Lynn, NetID 655811658. I will be completing the project individually. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? For my project, I will scrape political tweets from Twitter, build a dataset of tweets labeled by political party, and train a classification algorithm to classify tweets by political party. This project is interesting because it will involve analyzing the content of tweets by politicians, giving insight into the priorities of politicians from each party and how they communicate. The project will involve the following steps: (1) Scrape tweets from the Twitter accounts of politicians. There are various blog articles available online showing how to scrape tweets. Twitter's API, Tweepy, is one possibility, though it looks like that has some limitations. (2) Clean and label gathered tweet data, to create a dataset where each line is a tweet, and each tweet is a sequence of words without any extra punctuation. This will include tokenization and eliminating stop words, to simplify the dataset and make it more suitable for the next steps. Each tweet will be labeled with the political party of the politician who posted the tweet. (3) Perform some exploratory data analysis on the dataset, to see if there are interesting patterns in which words are commonly used by politicians from each party. (4) Train classification algorithms on the dataset, to create a classification model that classifies tweets according to the political party of the poster. I have not yet gone through that part of the course, so I am not sure about the details here yet. My project will be successful if I am successfully able to construct the dataset of tweets, and if I am able to train a classifier that is somewhat successful at classifying political tweets. I think that I should certainly be able to achieve an accuracy over 50%, however I expect that I won't be able to get close to perfect accu- racy, given the nature of tweets. My guess is that somewhere around 60% to 70% would be good performance. 3. Which programming language do you plan to use? I will use Python. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Here is my anticipated breakdown of time spent on this project: (1) Scraping tweets:  4 hours. Since I haven't done Twitter scraping before, and this course was the first time I've done web scraping, I expect that it will take me some time to figure out how to use the API, and gather tweets from various politician's Twitter accounts. (2) Cleaning and labeling tweets:  8 hours. I expect that it will take a significant amount of time to clean up the data and labeling the tweets. (3) Exploratory data analysis:  3 hours. Since I think that the dataset itself will be interesting, I want to dedicate significant time to exploring the data, and trying to identify any interesting patterns. (4) Training classification algorithms:  5 hours. I will experiment with different classification algo- rithms and parameters, to try to produce the most accurate classifiers that I can. I think it would be particularly interesting to look at decision trees and linear models, where I can easily interpret how the models are determining a classification. 1
https://github.com/lynn0032/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/sgangele21/CourseProject	Progress Report.pdf	Progress Report Sahil Gangele CS410 Progress made thus far Fetched 500 reviews worth of data for the PBS App via the iTunes API Asynchronously. Creating an iOS app requires our code being multi-threaded, meaning network calls and UI tasks are done on separate threads, and are handled asynchronously. I was able to fetch the data, and store that in a local variable inside the entity where the UI of the app will live. Created a barebones UI for allowing a user to enter a query and press a button to initiate some action Created a UI for displaying a list of Reviews My Technology Review allowed me to gain a deeper understanding of Apple's NaturalLanguage Framework and how to go about making an intelligent query system on Apple Platforms. This will help my progress greatly going forward Remaining tasks Add code for being able to perform intelligent search using Apple NaturalLanguage Framework This is what my Technology Review was on, so I have a very strong understanding of how to go about this Determine measure of correctness using Cranfield methodologies Any challenges/issues being faced I'm wondering how graders will be able to build my project. It requires Xcode, and a Mac. I'll need to follow up with the graders on this.
https://github.com/sgangele21/CourseProject	Proposal.pdf	"Proposal Theme 5: Free Topics What are the names and NetIDs of all your team members? Who is the captain? The team captain will be Sahil Gangele, NetID is gangele2@illinois.edu What is your free topic? Please give a detailed description. My free topic consists of creating an iOS Application to provide useful search capabilities for an apps App Store reviews. What is the task? > The goal of this project is to create an iOS Application that allow's one to intelligently search through an apps 500 most recent iOS App Store Reviews. An app review is a review a user leaves for a specific app on the Apple App Store. When searching through a list of reviews, one can take the naive approach of simply showing reviews to users that contain a word in a query. For eg. If a query was ""Bad"", then a user would see a list of reviews that contain the word ""Bad"" in it. However, by applying NLP techniques (Word embeddings representation, nearest neighbor similarity measure etc.) we can create a search query for ""Bad"" and reviews that contain the word ""Worst"" , ""Horrible"", ""Difficult"" will be shown to the end user. This makes the search ""intelligent"". What is your planned approach? My approach is split into four parts 1. Collect, clean, and format the data of the 500 most recent iOS App Store Reviews for the PBS App 2. Create the user interface to allow a user to search the iOS App Store Reviews and bring up N results. 0 >= N <= 500 3. Use Apple's NLP tools to create an intelligent search engine that allows a user to query the list of reviews 4. Evaluate system to define ""correctness"" Why is it important or interesting? The App Store is home to one of the only sources of feedback that developers can receive for an App on Apple's Platforms (iOS, iPadOS, MacOS). Therefore, it's an important asset for listening to users' feedback to see what improvements can be made to one's app. However, with so many reviews given each day, it's hard to gather data for reviews based on a general idea. This bottleneck makes it difficult to communicate these reviews to stakeholders of an App. To help ease this process, a search engine for App Store Reviews would be helpful in gathering and communicating feedback from users to our stakeholders in order to improve the quality of our app. What tools, systems or datasets are involved? The source of the data is from a public API created by Apple: https://itunes.apple.com/ us/rss/customerreviews/page=2/id={APP-ID}/sortby=mostrecent/json which returns back a list of 50 reviews with a reference to the next 50 reviews. The max number of reviews one can receive is 500 total reviews. To create the user interface of the iOS Application, Xcode 13 will be used as my Integrated Developer Environment (IDE). UIKit / SwiftUI will be the libraries used to create the user interface, and the NaturalLanguage library will be used to perform various NLP tasks. What is the expected outcome? The expected outcome is to have a functional iOS app that at the minimum retrieves relevant reviews to the user given a query search. These results will be ranked according to which review best matches the query. The higher the rank, the higher it is on the list of results first displayed to the user. How are you going to evaluate your work? Evaluation is going to be done using the Cranfield Evaluation method. For example, to test the query ""Bad"", I will set aside about X reviews and will create relevance judgements on each of them to determine whether a review is ""Good"" or ""Bad"". Say out of 50 documents, 15 are bad. When evaluating the system, I'll only run my system on those X number of reviews, and will input the query ""Bad"". I'll then ensure that of the 50 results surfaced to the user, that the 15 ""Bad"" results are ranked ""high"" in the list. In this example, ""high"" could be the top 25 results. This process can be used for other various queries, and numbers such as X, and the threshold to determine if the relevant judged documents are ""high"" in the ranked list can all be adjusted to ensure proper evaluation. Which programming language do you plan to use? Swift 5.5 Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. Based on the planned approach: 1. Collect, clean, format the data of the 500 most recent iOS App Store Reviews for the PBS App This would take 5 hours to complete. The reason parsing this data is difficult is due to all 500 reviews not being present at once. I will have to perform multiple network calls to parse all 500 reviews. This requires threading / asynchronous work to collect all data. 2. Create the user interface to allow a user to search the iOS App Store Reviews and bring up N results. 0 >= N <= 500 This will take approximately 10 hours. My plan is to use SwiftUI, as this is a new UI framework introduced by Apple to create simple User Interfaces 3. Use Apple's NLP tools to create an intelligent search engine that allows a user to query the list of reviews This will take around 10 hours plus to complete. Apple has a video example showing how to use their NaturalLanguage library. Learning from this video, applying the knowledge, and ensuring those results display correctly in the app will be the large heavy lifting of the project 4. Evaluate system to define ""correctness"" This will take about 5 hours as creating a Cranfield Evaluation method will require me to manually judge reviews related to queries to define ""correctness"""
https://github.com/sgangele21/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/pipipiii/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/asadiku/CourseProject	CS 410 Project Progress Report.pdf	1) Which tasks have been completed? * Completed creating a backend python server in Django * Connected backend server with Chrome Extension * Created basic framework of Chrome Extension 2) Which tasks are pending? * Scraping coursera data * Implementing BM25 and integrating with server * Displaying results in extension 3) Are you facing any challenges? Nothing at the moment
https://github.com/asadiku/CourseProject	CS_410_Project_Proposal.docx	CS 410 Project Proposal: Chrome Extension for Linking User Queries to Coursera Video Segments Group Members for this project include: Albert Sadiku (NetID: asadik3, Team Captain) and Mathew Tang (NetID: mt13) For our project, we decided to choose the Intelligent Browsing topic because we wanted to provide additional functionality on top of existing web browsers. In particular, we wanted to build a Chrome extension for linking user queries to Coursera video segments. This is a problem currently because Coursera only allows you to search for exact terms when looking for a particular topic video. However, we would like to improve this by using the BM25 retrieval function on transcript data to allow for more complex searches. Instead of looking for exact search results, our extension would look at all the possible video segments (of CS410 lecture videos) and rank them in order of relevance based on the BM25 retrieval function. Then, we would show the user the most relevant video segments based on their query. This relates to the theme of Intelligent Browsing because we are improving the capabilities of the web browser via our extension. Moreover, this project relates to the class because we are using the skills and methods learned during the first half of the course--namely, document ranking and the BM25 retrieval function. To obtain the data we need for this extension, we would scrape the lecture transcript data for all of CS 410. After scraping all the data, we would need to process it and make use of the BM25 retrieval function. To demonstrate how our algorithm will work as expected, we could do example queries and see how well it is working through user input. For example, if we are able to search a term, it could return direct users to relevant transcript locations. For the scraping portion, preprocessing portion, and BM25 portion we would use Python. We would use Javascript to develop the front end for the actual Chrome extension. Workload of the project: Scraping all Coursera Lectures: 10 hours Pre-processing all data for use: 5 hours Implementing BM25: 5 hours Creating front end: 5 hours Integrate front end with backend: 7 hours Full documentation: 2 hours Testing for functionality: 3 hours Report: 3 hours Total: 20 * 2 students = 40 hours
https://github.com/asadiku/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Links used: https://docs.djangoproject.com/en/3.0/intro/tutorial01/ https://developer.chrome.com/docs/extensions/mv3/xhr/ https://medium.com/@oaishi.faria/connecting-chrome-extension-with-python-backend-912d1d0db26
https://github.com/mccho3/CourseProject	CS 410 Project Proposal.pdf	CS 410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Melissa Cho NetID: mccho3 Name: Iris Kan NetID: iriskan2 Captain: Melissa Cho 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen the track of Free Topics and the topic of using sentiment analysis on Google reviews of apartments in Urbana-Champaign, because this will make it easier for students to determine the sentiment of the reviews by other students for each apartment complex, which will aid them in their search for an apartment. This relates to the class in general, because we will be implementing a program that uses sentiment analysis to analyze Google reviews of apartments in Urbana-Champaign. 3. Briefly describe any datasets, algorithms or techniques you plan to use. We plan to use sentiment analysis in our program, as well as web scraping to collect the data of the reviews for the analysis. 4. How will you demonstrate that your approach will work as expected? It will be clear that our approach is working as expected if the sentiment analysis returns a sentiment that matches the overall number of stars of the reviews. 5. Which programming language do you plan to use? We plan to use Python. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. N: 2 People Week 10: (8 hrs/person) - Web scrape google reviews of apartments in Urbana-Champaign - Building dataframe of google reviews of apartments in Urbana-Champaign - Classify/give scores to word with positive/negative connotations - Assigning a sentiment score to each google review Week 11: (8 hrs/person) - Build sentiment analysis model - data cleaning - split data frame - create bag of words - import logistic regression - split target and independent variables - fit model on data - make predictions Week 12: (2 hrs/person) - test model's accuracy - classification report Week 13: (2 hrs/person) - Prepare for presentation
https://github.com/mccho3/CourseProject	CS410 Project Progress Report.pdf	Melissa Cho (mccho3) Iris Kan (iriskan2) CS 410 15 November 2021 Project Progress Report 1) Which tasks have been completed? We have completed all of Week 10, Week 11, and Week 12 tasks: Week 10: - Web scraping google reviews of apartments and hotels in Chicago - Building dataframe of google reviews of apartments in Urbana-Champaign - Classify/give scores to word with positive/negative connotations - Assigning a sentiment score to each google review Week 11: - Build sentiment analysis model - data cleaning - split data frame - create bag of words - import logistic regression - split target and independent variables - fit model on data - make predictions Week 12: - Testing model's accuracy - Producing classification report 2) Which tasks are pending? The pending tasks are the tasks of Week 13: - Debugging/testing model - Cleaning up code - Analyze results/form conclusions - Preparing for presentation 3) Are you facing any challenges? We initially faced a roadblock with web scraping- we could not find a good website of apartment and hotel reviews that had the sufficient information we needed to conduct our sentiment analysis. We needed reviews with a score rating (scale of stars), the text body of the review, and a one-line summary of the review. On top of that, we had trouble finding hotels and apartments that had a sufficient number of reviews to gather. However, after searching, we found the websites and web scraped the information that we needed to conduct our sentiment analysis.
https://github.com/mccho3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ShaleenMehrotra/CourseProject	Progress Report - CS 410.pdf	Progress Report - CS 410, Fall 2021 Team Name - RS4 Team Members - 1. Rudranil Debroy - rdebroy2 2. Shaleen Mehrotra - shaleen3 (Captain) 3. Shruti Kalia - skalia2 4. Sujan Das - sujan2 5. Sreyashi Das - das25 Project Topic - Free Topic - IntelliReco (An intelligent recommendation system) 1. Progress made thus far * Identified the data source to be used - Amazon Marketplace. * Crawling of some of the consumer product end URLs was done and text data was extracted from the source. * Designed the schema for storing data. * Text data stored in SQLite database to be used for different types of analysis and recommendations which are consumed from our front-end user interface. * We are in the process of building a single page React application to enable easy user interaction with the system. 2. Remaining Tasks * We plan to complete crawling of 3 Categories of consumer products, viz. Computers and Accessories, TV and Video, and Photography and Videography. * We plan to include the first 50 Most Popular items in each Category. * If time permits, we might add more categories of consumer products and related items. * We are also planning to include Walmart online consumer product data. * Perform Data Cleaning steps, if necessary. * Recommendations based on search criteria. * Perform analysis using the Concepts read in the Classroom videos (Text mining, topic extraction, ranking, cosine similarity, classifiers, etc.) and perform efficient recommendations based on user- entered criteria. * To properly display search results. * Integration and Bootstrap of all the functionalities for a finished end-user product. 3. Any challenges/issues being faced * None.
https://github.com/ShaleenMehrotra/CourseProject	Project Proposal - CS 410.docx	Project Proposal - CS 410, Fall 2021 Team Name - RS4 Team Members - Rudranil Debroy - rdebroy2 Shaleen Mehrotra - shaleen3 (Captain) Shruti Kalia - skalia2 Sujan Das - sujan2 Sreyashi Das - das25 Project Topic - Free Topic - IntelliReco (An intelligent recommendation system) Description A recommendation system which will recommend consumer products comparing various product attributes as indicated by the user. The system will have a user interface enabling easy interaction with the system. The backend recommendation system will use some of the concepts which we have learnt in this course namely - Web Crawler, Data Cleaning, NLP Text Mining, Topic Extraction, Ranking, Cosine Similarity, Classifiers etc. Task Identify and collect data. Data cleaning. Building the recommender system. Building the end user UI. What makes this topic interesting? This project helps us apply the concepts that we have learnt in this course. This intelligent recommendation system will reduce the time taken by the users to select an appropriate product based on his/her criteria. Planned approach Initial architectural design blueprint of the whole system. Identifying the necessary data sources to be used. For e.g., Amazon Marketplace, Walmart Marketplace, etc. Deal with a category/subcategory of products. Web crawling of the identified marketplaces for the identified category/subcategory. Text retrieval and cleaning. Text mining, topic extraction, ranking, cosine similarity, classifiers etc. For offline usage we may use a database. Building the UI for end user interaction. Tools and datasets to be used React, HTML, JSON, VSCode, PyCharm, JQuery. JSON product data feeds from the identified marketplaces. Evaluation criteria Manual user evaluation. Will try to build an automated one to measure the quality and productivity of the system. Programming language JavaScript, Python Time needed Approx. 120 person-hours.
https://github.com/ShaleenMehrotra/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/lowelltyner/CourseProject	ProgressReport.pdf	"Progress Report Nov 15, 2021 Completed Tasks My initial planned task for this project was ""Chrome extension development environment setup project"" and I have completed this task and a bit more. This involved: * Creating a functional ""Hello World"" Chrome extension based on Google documentation in Visual Studio Code * Finding/ installing the ""@types/chrome"" npm package which contains type definitions, to aid with development by providing JavaScript autocomplete for Chrome extension functionality. * Exploring Chrome extension documentation to get an idea for UI options for providing search term entry and search result functionality to the user. * Exploring JavaScript libraries for highlighting search results Pending Tasks Since progress is on track, my pending tasks are the remaining planned tasks which include: * Accessing visible text on a webpage, in order to apply a search term to. * Finding/incorporating a BM25 algorithm in JavaScript * How to display search results to the user in the UI Challenges I am not currently facing any blockers. I am still largely in an exploratory phase, so my biggest challenge is still to sort out details, ensuring that I haven't oversimplified any concepts that could turn out to be overly difficult."
https://github.com/lowelltyner/CourseProject	Project Proposal - CS 410 - Fall 2021.pdf	Project Proposal - CS 410 - Fall 2021 Team - Group Name: Prestige Worldwide * Lowell Tyner * ltyner@illinois.edu * Team Leader/ Captain/ Coordinator What topic have you chosen? Why is it a problem? I have chosen a project from Theme 1: Intelligent Browsing, specifically a browser extension (for Chrome) that allows intelligent searching on a page's content. Existing browser functionality of find on page only finds exact matches of the user typed term. But with an intelligent search option, using BM25 as a ranking function, more useful results can be provided to the user. For example if the user enters a typo, or enters a term similar to a word on the page. Briefly describe any datasets, algorithms or techniques I plan to index current webpage content, and apply a ranking function like BM25, matching against a user provided search term. Demonstrate that your approach will work as expected? A Chrome extension that returns some kind of BM25 ranked result against a user entered search term (UI to be determined). Programming Language JavaScript (with HTML, CSS as needed for UI), Python if feasible. 20 Hours of Work Justification 5 hours for each point (20 hours total for a team of 1). * Chrome extension development environment setup project * Access visible text on page from extension to pass it to the indexer, avoiding non-visible text that the user would not want to search on. * Find/ implement suitable BM25 algorithm in JavaScript (or see if there is a way to hook up JavaScript extension code to the Python based MeTA implementation). * UI results - how to display search results to the user (Jump to spot on page? List of matching text?) Will require investigation to see what's feasible as a POC.
https://github.com/lowelltyner/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/xiaofanliu77/CourseProject	CS410 Project Proposal.pdf	"CS410 Project Proposal Team BXB 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Xiaofan Liu / xl109@illinois.edu / Captain Boyu Pang / bpang4@illinois.edu Brittany West / bnwest2@illinois.edu 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Our topic is ""Campuswire and Coursera Query Link"", which falls under the ""Intelligent Browsing"" theme. The goal is to develop a program to present users with relevant information from Coursera related to questions that are posted on Campuswire. This program would facilitate the learning process by providing relevant ranked documents from Coursera to the user, which reduces the quantity of repeated questions on Campuswire that could quickly crowd out other questions/answers. The first problem is that due to limited searching functionality, sometimes it is difficult to find answers quickly on Coursera. The second is that there are many questions posted on Campuswire which are answered in the lecture transcripts, syllabus or reading materials, but there is no easy way to identify the exact place to go in Coursera for the answer. 3. Briefly describe any datasets, algorithms or techniques you plan to use Dataset: CS410 course data on Coursera (or a downloaded dataset that contains all information for this course) Techniques: * We will browse Campuswire and compile a list of common questions/topics/themes. * Use those themes to form the basis of our query set (i.e use queries derived from our list to search the provided Coursera data). * Preprocess the Coursera data by separating it into distinct documents. Algorithms: * For our source code: * We will explore incorporating PLSA to help improve our ranking algorithm. In our PLSA algorithm we can implement the latent dirichlet allocation. * For our ranking function, we can evaluate multiple algorithms to determine which fits our project the best. * We can evaluate our results/ranking function by manually identifying relevant documents on a set of queries, and then applying MAP. * Our output will include relevant documents with the first matching keywords of each document. 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? We will demonstrate by walking through our code and doing a test run with mock-up questions/queries for the course. We will use Python. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Our workload and tasks are listed as follows: * Brainstorming and team meetings (5 hours) * Scraping and Data cleaning, including structuring and cleaning the coursera documents, gathering queries to test our ranking function (10 hours) * Researching, implementing, fine tuning ranking functions (20 hours) * Testing ranking functions and improving them (10 hours) * Documenting code and writing project updates (5 hours) * Preparing and creating presentation (10 hours) Total hours : 60"
https://github.com/xiaofanliu77/CourseProject	ProgressReport.pdf	CS410 Project Progress Report Team BXB 1. Tasks completed: * Brainstorm meetings * Collecting, cleaning and consolidating data from the CS410 Coursera course as searchable text documents * Gathered 15 queries from Campuswire website as sample queries to test the project algorithm * Research on various ranking functions and how to apply them on our project * Rough outline/skeleton of our code in a jupyter notebook. Also set up the project github for collaboration: https://github.com/xiaofanliu77/CourseProject 2. Tasks pending: * Implementing and fine-tuning ranking functions, and getting them to return a list of ranked documents from the CS410 class on Coursera * Labeling the results from the ranking function as relevant/not relevant, and computing the MAP * Documenting code and writing project updates * Preparing and creating the presentation 3. Challenges * Getting usable data from Coursera for the CS410 course. In the end we were able to download the course files for this semester's CS410 using the tool described in this blog post: https://medium.com/@ootaki.yuuki/how-to-download-all-contents-in-coursera-cou rses-automatically-963f199be55f * Research and determining how to implement ranking functions, and how to evaluate the result (i.e. whether to use NDCG, MAP, or others)
https://github.com/xiaofanliu77/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ZacharyOldham/CourseProject	Progress Report.pdf	1 CS 410 Final Project Progress Report Topic: Stock Sentiment Analysis Using Twitter Feed Team: Porcupine Tree Team Captain: Zachary Oldham Zachary Odham NetID: zoldham2 Yogeswara Rao Lekkalapudi NetID: yrl3 Luis Mariano Ovalle Castaneda NetID: lo22 1. Which tasks are completed? a. The creation of the sentiment analysis system has been completed. This was accomplished with a recurrent neural network that was implemented using PyTorch. The system takes arbitrary labeled text strings as input, and uses them to either train or test a neural network, depending on how the data is labeled. Based on preliminary testing using data obtained from Sentiment 140, the accuracy of this classifier is about 70% when trained with 40,000 labeled tweets over 20 epochs. We will attempt to improve the performance of this system for the final product by adding additional preprocessing, and training for longer using a larger dataset. b. Testing querying using the Twitter API has been completed. We have figured out how to retrieve tweets using the API, and how to retrieve tweets based on specific keyword searches. We will use this system to retrieve the data we display to the user in the final system. 2. Which tasks are still pending? a. Identify socks to query from twitter, map company names to stock code for reverse lookup of the codes for input queries b. Build full set of documents for ranking model c. Build inverted index for the documents d. Build ranking model and evaluate e. Create interface to obtain user input and display the results to the user 3. Any challenges/issues being faced? a. Understanding twitter query parameters to fetch relevant data was tricky. Searching for a company name vs stock code returned different feeds. For our project, we are only fetching stock related feeds. b. We need to improve the performance of the sentiment analysis system. We strongly suspect this can be accomplished by adding preprocessing and training for longer with more data, but this needs to be investigated.
https://github.com/ZacharyOldham/CourseProject	Project Proposal.pdf	1 CS 410 Final Project Proposal October 2021 Topic: Stock Sentiment Analysis Using Twitter Feed Team: Porcupine Tree Team Captain: Zachary Oldham Zachary Odham NetID: zoldham2 Yogeswara Rao Lekkalapudi NetID: yrl3 Luis Mariano Ovalle Castaneda NetID: lo22 1. Introduction For the final project, we choose the topic of sentiment analysis using the Twitter feed. We will be using a Twitter API to get the documents related to a specific stock selected by the user and then using Text Access and Mining techniques learned during the course to analyze the social sentiment of the stock. This system will be beneficial for those wanting to understand the sentiment surrounding a stock prior to investing. 2. Overview Our plan is to build a system that takes as input the name of a stock. It will then build a query, likely by augmenting the stock name with the name of the company associated with that stock, though this approach may change. We will then apply the techniques that we have learned to build and optimize the query vector. Once we have a query vector, we will rank all tweets within a time window by relevance to the query vector, and then select the top k tweets. We will perform sentiment analysis on each of these tweets in order to obtain probabilities that each tweet is positive or negative. We will then have two lists, one of positive tweets and one of negative tweets. We will reorder them based on their ranking with the query vector, and some heuristic based on the popularity of the tweet. We will then present the user with the two lists of tweets with the goal of giving them an idea of how the public feels about the stock. We will also compute some heuristic sentiment score that attempt to describe the general sentiment surrounding the stock and present this to the user. 3. Motivation Social media provides a dynamic and massive amount of information that is difficult to interpret by the user. This project aims to harness this data set to give helpful information to the user for their research and help make investing decisions. Our software should quantify a specific stock user's social sentiment using three different categories; Positive, Neutral, or Negative. 4. Work To Be Done We will need to figure out how to work with the Twitter API, we need to do some investigation into what tweets we want to retrieve, we need to implement a query vector improvement system as well a ranking algorithm, we need to implement sentiment analysis which may use either the NLTK or a recurrent neural network, we need to implement a heuristic 2 ranking algorithm, and we need to implement a heuristic sentiment summary. Across the three of us, this will likely take at least 20 hours per person. 5. Evaluation We can compare our sentiment analysis against the Fidelity tool that provides a sentiment score around a stock. We can also compare the judgement against a manually generated judgement that we create by manually looking through twitter. We may also attempt to see if there is any correlation between our analysis and the performance of a stock. 6. Tools * Python 3.5 * MeTA Toolkit * Tweepy, Twitter API v1.1 * NLTK Natural Language Toolkit. * Tensorflow
https://github.com/ZacharyOldham/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jessicahuang523/CourseProject	progress_report.pdf	CS 410 Progress Report Captain: Yung Chieh Huang - ch10 Yuri Kotsk - kotsk2 Sudhendu Sharma - sharma78 Nivedita Chatterjee - nc19 In the past fe eeks our team has been setting up the development environment. We ran into some trouble hich required us to meet ith the project oner ho helped us ork through our issues. Hoever, the sstem is currentl unavailable due to some maintenance issues on the server side hich prevent us from testing our changes on the live version of the app. As this is an eisting application, understanding the eisting code base and functionalit is taking a major part of our time at this phase. As understanding the code base is an ongoing process, this task ill continue as e progress through the development and implementation phase. Belo are the main tasks e progressed so far and the challenges e are facing. Task In progress :  Understanding the eisting code base. We are specificall concentrating on the topic modelling part.  We are orking on adding ne functions to the eisting code to add topic modelling.  Parallell orking on setting up the local environment for testing our updates. Challenges :  First challenge as setting up the local environment. The project oner helped us to understand and minimie the errors. She helped us b ansering our queries about eisting set up and also guided us to focus on specific pieces of the codebase hich are more relevant to our project.  The application is scheduled for a server migration hich is preventing us from testing and completing local set up end to end.
https://github.com/jessicahuang523/CourseProject	proposal.pdf	CS 410 Pc Pa Captain: Yung Chieh Huang - ch10 Yuri Kotsk - kotsk2 Sudhendu Sharma - sharma78 Nivedita Chatterjee - nc19 For our project, e have chosen Theme 3.4: Sstem etension on the folloing unlisted Tet Mining Sstem approved b the professor and course staff: https://timan102.cs.illinois.edu/tms The sstem is related to the class because it is a tet mining and topic generation sstem for social science research that provides an eas a for researchers in social science to mine a large corpus of historical nespaper articles. Our goals for this project are to enhance this project in several as ith an overall focus on intractabilit. We ill be adding features to visualie eisting topic generation frameork that uses LDA for topic generation. If time permits e ill enhance the sstem for Seeded-LDA Algorithm for topic generation. Since the eisting code base is largel ritten in Pthon and various libraries hich etend its functionalit, e ill simpl fork the eisting code base and develop our functions and features ithin this fork. It ill take us quite some time to understand such a comple sstem but once e do it ill be quite eas to implement our features. Some of the libraries and tools hich e ill be using are flask, matplotlib and sqlite. We have 4 group members so out orkload is epected to be 80 hours: Task Estimated Time Understanding the Code Base 20 hours Topic Generation for user uploaded docs 20 hours Time Range based Topic Generation for eisting documents 20 hours Visualiation improvement (Based on suggestion from Bhava) 20 hours Seed Lda Implementation if time permits us(Optional ) 20 hours
https://github.com/jessicahuang523/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/JosephineFalso/CourseProject	Progress Report.docx	"CS410 Progress Report ""TIS Chicago"" https://github.com/JosephineFalso/CourseProject Josephine Falso Sree Alaparthi Narasimha Kethireddy Within the topic of Intelligent Learning Platform, our group is building a back-end application to integrate the instructor's lectures with the student discussions in CampusWire. Our application will highlight significant keywords in the transcription text of the Coursera videos, and the hover text will provide some additional information from the CampusWire posts related to that keyword. Our group has decided to start with the content from week 3. There were several significant course topics discussed during that week, including precision, recall, average precision, F measure, MAP, gMAP, DCG, and nDCG. There are many postings in CampusWire related to these topics from the weekly discussions and one of the MP assignments. If time allows, our group hopes to expand the scope of the project to include additional weeks of course content. The web scraping of the Coursera transcription text is in progress. Once a file of the transcription text has been generated, the Python library NLTK will be used to eliminate the words in the background language model and to generate the most popular keywords from the lectures. Although this task is pending, another teammate has extracted the data from CampusWire for a few of these keywords by hardcoding the terms. We have written a Python program to parse the json structures extracted from CampusWire and extract the topic and url of the posting. We then generated a csv file of the topic, url, and ranked relevance of the posting to the keyword. We have deployed the Coursera videos and transcription text to a new webpage for some of the videos in week 3. The tasks of highlighting the keywords and populating the hover text remain outstanding. For presentation of the final code delivery, we will create a Node.js application deployed on a Docker container. Our team members have various levels of competency with Python, so the development is progressing at an inconsistent pace. Some of the concepts in this assignment are outside of the scope of the course material and require additional research. Additionally, tasks such as highlighting of the keywords in the transcription text and display of the hover text are new concepts for the team and require additional time to implement. The reviewers gave the feedback that we may need to consider textbook data, Wikipedia data, or other datasets to populate the hover text if sufficient postings cannot be found on CampusWire. Our group hopes to expand the scope of the project to incorporate this feedback if time allows."
https://github.com/JosephineFalso/CourseProject	Project Proposal.docx	"CS410 Project Proposal Group ""TIS Chicago"" What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Josephine Falso, Team Captain, jfalso3 Sree Alaparthi, sa68 Narasimha Kethireddy, nrk6 The GitHub project URL is as follows: https://github.com/JosephineFalso/CourseProject What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen a project within the topic of ""Intelligent Learning Platform"". We will build a back-end application to identify and explain popular keywords in the Coursera lecture videos for CS410. Our application will provide a cohesive integration of the instructor's lectures with the student discussions in CampusWire. Significant keywords will be highlighted in the transcription text. When the learner hovers over a keyword, the hover text will provide some additional information from the CampusWire posts related to that keyword. This alleviates the inconvenience of navigating in and out of Coursera. By incorporating all of the relevant information directly in Coursera, the learner will have a more focused and productive learning experience. The techniques applied to this project demonstrate mastery of both text access and text mining. Briefly describe any datasets, algorithms or techniques you plan to use. To build this application, we will perform web scraping of the transcription text in the Coursera CS410 lecture videos. We will build a simple website to store this scraped text. We will use a Python NLP library such as NLTK, PKE, or spaCy for text mining. We will also perform topic analysis and keyword identification via a search of CampusWire postings to identify and associate the postings to the appropriate keywords from Coursera. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? This project will be developed in Python. We will demonstrate the approach by randomly selecting keywords and verifying that the information provided in the pop-up window matches the number of true references in CampusWire. We will limit the scope of the project to a subset of the CS410 Fall semester content. The subset will be a selection of weeks of material or a selection of course videos depending on the time that we have. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The TIS Chicago team consists of three members. The workload of sixty hours will be distributed as follows: Project Proposal, 1 hour Progress Report, 2 hours Software Code Submission, 60 hours as follows: Web Scraping of transcription text in course videos of CS410 lectures and Text Mining of keywords, 20 hours Search CampusWire feed to associate posts to keywords, 20 hours When user hovers on keywords, display CampusWire information, 20 hours Documentation, 2 hours Software Tutorial Usage Demonstration, 2 hours"
https://github.com/JosephineFalso/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/amyyq2/CourseProject	Progress Report.pdf	1) Which tasks have been completed? - Installed necessary libraries/packages such as selenium and chromedriver as needed for our project - Able to scrape youtube comments given a single youtube link - Began data cleaning process for the scraped comments - Completed Chrome extension tutorial guide - Created skeleton chrome extension that is capable of changing the background color of Youtube video titles on a Youtuber's content page. See image below as an example. - Planned out how to get links (hrefs) of Youtube videos from the Chrome extension to the comment scraper we built. 2) Which tasks are pending? - Use sentiment analysis on scraped youtube comments - Parallelize comment scraping so we can scrape multiple videos' comments at the same time - Clean comments data, filtering out emojis, empty lines, non-english comments - Connect sentiment analyzer backend to chrome extension to fetch and display each video's rating 3) Are you facing any challenges? - Time challenge - Scraping comments from a singular youtube video's comment section takes a long time. (~27 seconds for 200 comments). This is not optimal for user experience. - Connecting the Chrome Extension (Javascript) component to the sentiment analysis (Python) component -
https://github.com/amyyq2/CourseProject	Project Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Xinran Hua (xinranh2), Teresa Dong (teresad2), Christina Hu (ch35), Amy Qian (amyyq2) b. Captain: Amy Qian 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We have chosen to create a chrome extension that will analyze the sentiment of YouTube videos based on the comment section. We will color-annotate a page of videos based on the positivity/negativity on the video. For example, there will be a green box highlighting videos that are liked by video commenters and a red box around videos that are disliked by video commenters. This chrome extension will help YouTube influencers produce more videos that their followers like. It can also help viewers decide which videos to watch based on how positive the commentary is. This project relates to the theme of ""Intelligent Browsing"" and CS410: Text Information System because we have to scrape and analyze text (user commentary) to provide real-time information to the user while browsing a page of Youtube videos. 3. Briefly describe any datasets, algorithms or techniques you plan to use We will be using the comment section of Youtube videos as our dataset and we plan on using Python libraries to perform sentiment analysis. 4. How will you demonstrate that your approach will work as expected? We will analyze Youtube pages that are known to be highly disliked. Our approach works if most of the videos on those pages are highlighted red. We will do the same with pages that are known to be highly liked by commenters, and ensure these videos are highlighted green. 5. Which programming language do you plan to use? We will use Python for backend and sentiment analysis, and Javascript to create the Chrome extension 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Researching and creating a Chrome Extension - 20 hours Researching and creating sentiment analyzer - 15 hours Combine sentiment analyzer into Chrome Extension - 10 hours Analyze a Youtube video comment section using Chrome Extension and sentiment analyzer - 20 hours Display results of analysis with green/red boxes in real time on YouTube Channel page - 15 hours"
https://github.com/amyyq2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chriistinahu/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/dattatreya303/CourseProject	CS_410_TIS_Project_Progress_Report.pdf	Summary Extraction and Relevance Identification from Spotify Podcasts (Progress Report) Arijit Ghosh Chowdhury arijit2@illinois.edu Dattatreya Mohapatra (Captain) dm42@illinois.edu Gargi Balasubramaniam gargib2@illinois.edu 1 PROBLEM DESCRIPTION Given the automatic transcript of a podcast episode, the goal is to attempt the following tasks: (1) Generate meaningful summaries using multiple baseline models and compare them to identify the best performing method (2) If time permits, identify worst performing summaries and modify baseline models to address their challenges Our approach involves conducting a qualitative and qualitative evaluation of 3 techniques - TextRank [4], T5 [5] and BERT [6]. We plan to increase complexity in a step by step manner, and observe improvements in the summarization output. To this end, we hope to come up with ways to improve existing baselines after thorough analysis. 2 PROJECT PROGRESS This section outlines our progress for the project. We performed multiple pre-processing steps to generate data that can be directly fed into our models. We have trained and evaluated two baseline methods - TextRank and T5. Each step is described in detail in the following sections. 2.1 Preliminary Data Analysis The dataset contains 100,000 episodes from various podcasts across Spotify, sampled between January 1, 2019 and March 1, 2020. [2]. We have also been provided metadata pertaining to each show such as the show URI, the show name, show description, episode duration, to name a few. Table 2 outlines the details of the test dataset on which we have evlaluated our methods. 2.2 Methods We implemented TextRank and T5. Table 2 shows a summary of the quantitative results. For evaluation, we have used the commonly used automatic evaluation metric ROUGE [3]. * TextRank: [4] : The algorithm ranks sentences based on their similarity to other sentences. The basic idea imple- mented by a graph-based ranking model is that when one node links to another one, it is basically voting for that other node. The higher the number of votes that are cast for a node, the higher the relevance of the node. This is an Extractive summarization method, which out- puts sentences already present in the document. ROUGE scores for extractive methods tend to be higher due to higher similarity scores. However, a qualitative evaluation done Text Information Systems, , Fall '21', UIUC Table 1: Test Dataset Statistics Number of Episodes 1000 Average Episode Duration 35.85 Minutes Table 2: Preliminary Results Method ROUGE TextRank 0.27 T5 0.25 by human evaluators can help us adjudgde the relevance of abstractive methods better. * T5[5]: This is encoder-decoder model which has been pre- trained on a large corpus of supervised and unsupervised sequence-to-sequence tasks (like translation, summarization, question-answering etc). We have used Huggingface's1 im- plementation of the model and ran inference on the frst 15 sentences of podcast transcripts. 2.3 Remaining Tasks T5 surprisingly doesn't perform better that extractive TextRank. This could be because we have used the base model T5 which has been pre-trained on a mixture of tasks. We plant to improve upon it by fne-tuning it by re-training on the podcast transcripts and descriptions. However, we do not expect fne-tuning to address the main challenges of summarizing podcast data - length of source data, multiple speakers and noisy fller-text. We plan to address the frst and last concern by combining TextRank and T5 to perform content selection before training and inference, similar to Zheng et al's [7] approach. The second challenge is a major challenge and will be part of our future scope. We plan to implement the third method (BERT based represen- tations) and are hoping to incorporate the audio provided as future scope. We will also ask fve english speaking volunteers to score the summaries into the defned sprectrum of Bad(B) to Excellent(E). 2.4 Challenges As a future scope, we hope to leverage the audio samples provided along with the transcript. A key issue with audio is to identify diferent speakers as there may be multiple speakers in a typical podcast. We plan to explore techniques such as voice separation[1], and then process the audio further. 1https://huggingface.co/transformers/model_doc/t5.html Text Information Systems, , Fall '21', UIUC Project Proposal REFERENCES [1] Shlomo E Chazan, Lior Wolf, Eliya Nachmani, and Yossi Adi. 2021. Single channel voice separation for unknown number of speakers under reverberant and noisy settings. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 3730-3734. [2] Ann Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu, Rezvaneh Rezapour, Hamed Bonab, Maria Eskevich, Gareth Jones, Jussi Karlgren, Ben Carterette, and Rosie Jones. 2020. 100,000 Podcasts: A Spoken English Document Corpus. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), 5903-5917. https://www.aclweb.org/anthology/2020.coling-main.519 [3] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013 [4] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Barcelona, Spain, 404-411. https://aclanthology.org/W04-3252 [5] Colin Rafel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with a Unifed Text-to-Text Transformer. CoRR abs/1910.10683 (2019). arXiv:1910.10683 http://arxiv.org/abs/1910.10683 [6] Haoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang. 2019. Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). Association for Computational Linguistics, Hong Kong, China, 789-797. https://doi.org/10.18653/ v1/K19-1074 [7] Chujie Zheng, Kunpeng Zhang, Harry Jiannan Wang, and Ling Fan. 2020. A Two- Phase Approach for Abstractive Podcast Summarization. CoRR abs/2011.08291 (2020). arXiv:2011.08291 https://arxiv.org/abs/2011.08291
https://github.com/dattatreya303/CourseProject	CS_410_TIS_Project_proposal.pdf	"Summary Extraction and Relevance Identification from Spotify Podcasts (Project Proposal) Arijit Ghosh Chowdhury arijit2@illinois.edu University of Illinois, Urbana Champaign Dattatreya Mohapatra (Captain) dm42@illinois.edu University of Illinois, Urbana Champaign Gargi Balasubramaniam gargib2@illinois.edu University of Illinois, Urbana Champaign 1 INTRODUCTION With the vast amount of data circulating in the digital space, there is need to develop machine learning algorithms that can automatically shorten longer texts and create succinct summaries. Applying text summarization reduces reading time, accelerates the process of re- searching for information, and increases the amount of information that can ft in an area. Inspired by the need to understand more about spoken-content retrieval - the given scenario is especially relevant in the domain of podcasts, which have become a widely used medium of commu- nication today. In this project, we take inspiration from the TREC 2020 and 2021 Podcast Summarization Challenges [2]. The TREC 2020 Podcast track features transcript and audio datasets from Spotify - a leading platform for streaming podcasts. 2 PROBLEM DESCRIPTION Given the automatic transcript of a podcast episode, the goal is to attempt the following tasks: (1) Generate meaningful summaries using multiple baseline models (see Section 4) and compare them to identify the best performing method (2) If time permits, identify worst performing summaries and modify baseline models to address their challenges We plan to attempt task 1 as part of the scope of this project and will undertake task 2 if time permits. Our approach involves conducting a qualitative and qualitative evaluation of 3 techniques - TextRank [5], Seq2Seq Pointer Net- works [6] and BERT [7]. We plan to increase complexity in a step by step manner, and observe improvements in the summarization output. To this end, we hope to come up with ways to improve existing baselines after thorough analysis. 2.1 Motivation There has been a lot of work on text summarization and snippet ranking that involve both shallow ranking methods, as well as deep learning architectures. However, the development of most of the models was based on a critical assumption - structured text data. Podcasts present a genre of datasets where the style of text can vary from very formal to very casual depending on the speakers. Podcast transcripts also contain text from multiple sources (speak- ers), which is another aspect that existing models do account for. Text Information Systems, , Fall '21', UIUC These two reasons cascade various other consequential challenges to processing podcast transcripts using baseline models[3]. We aim to address these challenges by implementing existing models (see Section 4) and comparing the generated summaries against manual summaries. If time permits, we will also explore ways to modify existing models to work with the unique traits of podcast data. 2.2 Relevance to CS 410 Text summarization is essentially a generative modelling task. It also depends on the sentence and language features. Furthermore, in case of extractive methods, there can be multiple candidate sum- maries for a podcast transcript which have to be ranked using similarity metrics. CS410 covers all of these topics and has provided us with solid background to be able to tackle these tasks on a real- life noisy dataset. We hope to use the content of the lectures and expertise of course staf to explore a new area of application for these concepts and solve an actual problem. 3 DATA The Spotify Dataset [1] contains around 105,360 episodes from various podcasts on Spotify. The GCP Text-to-Speech API was used to generate transcriptions. In this work, we limit our scope to the transcriptions of the summaries, and do not work on audio data. The target summaries are scored from Bad (B) to Excellent (E). 4 METHODS For our case study, we use three baselines with increasing amount of complexity. * TextRank [5] : The algorithm ranks sentences based on their similarity to other sentences. The basic idea imple- mented by a graph-based ranking model is that when one node links to another one, it is basically voting for that other node. The higher the number of votes that are cast for a node, the higher the relevance of the node. * Seq2Seq Pointer-Generator Networks [6] : This abstrac- tive summarisation technique uses a hybrid pointer-generator network that can copy words from the source text via ""point- ing"", which allows accurate representation of information, while keeping the ability to produce novel words through the generator. It also uses coverage to keep track of what has already been summarized, which discourages repetition. * BERT [7] : This paper uses two-stage decoding process to leverage BERT's context modeling ability. On the frst stage, Text Information Systems, , Fall '21', UIUC Project Proposal this method generates the summary using a left-context- only-decoder. On the second stage, every word of the sum- mary is masked and the refned word is predicted one-by-one using a refne decoder. 5 IMPLEMENTATION DETAILS AND WORKLOAD All models will be trained under the same hardware and system confgurations. For this project Google Colab will be used and the neural methods will make use of the Nvidia Tesla K80 GPU. All programming will be done in Python. The chosen deep learning framework is Pytorch, and other numerical and machine learning libraries like numpy, pandas and sklearn will also be widely used. Estimated Workload : Estimated time taken for each method is 15 hours, which includes training across multiple hyperparam- eters, evaluating and documenting results. Analysing the dataset and preprocessing is estimated to take another 10 hours of efort. Another 5-10 hours are kept for collating all the results and docu- menting fnal observations and outlining practical prospects of this case study. Total estimated work hours = 65 hours. 6 EVALUATION For this project, we will use the commonly used automatic eval- uation metric ROUGE [4]. Additionally, we will use fve english speaking volunteers to score the summaries into the defned sprec- trum of Bad(B) to Excellent(E). 7 AUTHORS AND AFFILIATIONS Arijit Ghosh Chowdhury, Dattatreya Mohapatra and Gargi Balasub- ramaniam are frst year MS CS students at the University of Illinois at Urbana Champaign. Arijit has a background in NLP Research in the social media and content space. Dattatreya's research focuses on graph mining and search query understanding. Gargi has pre- vious experience with basic tools of Machine Learning and Deep Learning, with a research background in interpretable machine learning. This project will be a frst learning experience in the text domain. REFERENCES [1] Ann Clifton, Sravana Reddy, Yongze Yu, Aasish Pappu, Rezvaneh Rezapour, Hamed Bonab, Maria Eskevich, Gareth Jones, Jussi Karlgren, Ben Carterette, and Rosie Jones. 2020. 100,000 Podcasts: A Spoken English Document Corpus. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), 5903-5917. https://www.aclweb.org/anthology/2020.coling-main.519 [2] R. Jones, Ben Carteree, Ann Clion, Maria Eskevich, G. Jones, Jussi Karlgren, Aasish Pappu, S. Reddy, and Yongze Yu. 2020. TREC 2020 Podcasts Track Overview. ArXiv abs/2103.15953 (2020). [3] Rosie Jones, Hamed Zamani, Markus Schedl, Ching-Wei Chen, Sravana Reddy, Ann Clifton, Jussi Karlgren, Helia Hashemi, Aasish Pappu, Zahra Nazari, Longqi Yang, Oguz Semerci, Hugues Bouchard, and Ben Carterette. 2021. Current Challenges and Future Directions in Podcast Information Access. CoRR abs/2106.09227 (2021). arXiv:2106.09227 https://arxiv.org/abs/2106.09227 [4] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013 [5] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Barcelona, Spain, 404-411. https://aclanthology.org/W04-3252 [6] Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Vancouver, Canada, 1073- 1083. https://doi.org/10.18653/v1/P17-1099 [7] Haoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang. 2019. Pretraining-Based Natural Language Generation for Text Summarization. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). Association for Computational Linguistics, Hong Kong, China, 789-797. https://doi.org/10.18653/ v1/K19-1074"
https://github.com/dattatreya303/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/zhanz1/Course-Information-Retrieval	CS 410 Project Poposal.pdf	CS 410 Final Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - Zhan Zhang: zhanz3 (Team leader) - Shuo Wang: shuow6 - Qianmeng Chen: qc15 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems, or datasets are involved? What is the expected outcome? How are you going to evaluate your work? - We decide to create a website where users (students) can query topics to search for related courses within UIUC. We noticed that many students, especially freshmen, have a hard time looking for the classes they are interested in. There are also some other considerations like the professors, prerequisites, etc. Our plan is to add more features on Course Finding tools like including more information about professors, providing a prerequisite graph showing the course's prerequisites, if it is a prerequisite for another course, and what its follow-up courses are, allowing students to plan ahead. We will be using scrapers to extract the information about courses and professors, and build our feature on top of that. - Our expected outcome is a complete application that can help students find the right courses by giving them more information about the course so that they no longer have to jump around different pages or ask someone else about the course. Our evaluation of work will be based on how relevant the queries user entered is to the course we return to the user and if we successfully build the features or not. 3. Which programming language do you plan to use? - Python for backend, JS for front end, Flask for API 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Our team has three team members, so our total workload should be at least 60 hours. Our estimated total workload is about 66 hours. - Scraper of course information, including the course number, course name, course description, and professor. - 7 hrs - Scraper of professor information of the relevant course, including the comment and score in Ratemyprofessor. -7 hrs - Scraper of pre-requisite courses information. We want to make a dendrogram to present the result. -2 hrs - Parsing texts from the scraper and storing them in databases. - 5 hrs - Search engine for the courses. -15 hrs - Recommender for the courses, store preferences. -15hrs - Front-end website development and Flask API set up. - 15 hrs
https://github.com/zhanz1/Course-Information-Retrieval	Progress Report.pdf	Progress Report 1) Progress made thus far: 1. Find resources relevant to courses grade distribution and comments on courses and professors. https://waf.cs.illinois.edu/discovery/grade_disparity_between_sections_at_uiuc. https://uiucmcs.org/reviews 2. Write the scraper to grab some useful information from the websites, including the course number, course name, course description, and professor. 3. Write the scraper to fetch some sentimental comments and scores from Ratemyprofessor. 2) Remaining task: 1. Scraper of pre-requisite courses information, which is the necessity of our course dendrograms. 2. Parsing texts from the scraper and storing them in databases. 3. Search engine for the courses. 4. Recommender for the courses, store preferences. 5. Front-end website development and Flask API set up. 3) Any challenges/issues being face: 1. Our team members are located in different time zones. It may be difficult for us to coordinate our work and schedule our meetings. 2. After collecting the information that we need, we found it difficult to integrate what we learn from the course with the information in practice. It is overwhelming at first for us to process a large amount of information.
https://github.com/zhanz1/Course-Information-Retrieval	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rudyrath/CourseProject	CS410 Project Progress Report_RudyR.pdf	"CS410 Project Progress Report Twitter Sentiment Analysis Rudy Rath Tasks Completed The following tasks have been completed - * Creation of Twitter Developer Account * Generation of Twitter api key and access token for application * Proof of Concept of scraping Twitter data using Tweepy package * Research of NLTK features * Research of VADER and Textblob Tasks Pending The following tasks are pending - * Design decisions regarding the challenges explained in the next section. * Coding for Scraping Twitter Data - Need refinement from what was done for POC * Coding for Sentiment Analysis with NLTK and VADER - only have skeleton code so far from the examples * Project Documentation Challenges So far, I have faced the following challenges that I need to overcome in the final design and implementation - 1. While developing the POC for scraping Twitter data, I have hit the Rate Limit error as documented in https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits several times. I need to factor that in the design of the application and decide if scraping is done at run time or if should be done in advance. While doing the scraping in real time by taking the input string from the user during run time, I have found that the optimal number of tweets so far is 1500. My preference is to do the scraping in real time after user input, so I will explore this rate limit issue further. 2. Since the Twitter api key and access token are linked to my developer account and are not supposed to be shared publicly, I need to figure out how to provide instructions to run the code when I submit the project. I will probably use .gitignore to put the config file in the GitHub repo but anyone trying the run my code in their own environment will still need the config file. If I'm not able to figure out the resolution on my own, I will seek advice from course instructors. Project Proposal Meta-Reviews Feedback One question in the Meta-Reviews the project proposal was about the dataset - Whether I will be crawling for the dataset or use existing ones. The answer to that is that I plan to scrape the twitter data and build my own dataset. An example of such a data set is in the ""extracted_tweets.csv"" file that I've uploaded to the GitHub repo. The phrase for this extraction was ""Biden"""
https://github.com/rudyrath/CourseProject	CS410 Project Proposal_RudyR.pdf	CS410 Project Proposal Rudy Rath Names and Net ID Q: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. A: I'm planning to make a Solo Submission (Team of 1) for the Course Project and my details are: Name: Rudy Rath NetID: rudrar2 Captain: Rudy Project Description (Free Topic) Q: What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? A: My Free Topic is Sentiment Analysis of Twitter data Twitter is one of the most popular social networking platforms with hundreds of millions of active users. With Twitter sentiment analysis - * Businesses can discover insights their products and services to make better business decisions * We can understand public views on a variety of current topics from President Biden's Infrastructure Bill, COVID-19 Booster Shots, Dave Chapelle Netflix show etc. * And many other applications... My approach is to - * Collect Twitter data * Process the data * Perform sentiment analysis I will be using tools like NLTK and VADER (Valence Aware Dictionary and sEntiment Reasoner) The expected outcome is an easy-to-understand Visualization of the Sentiment Analysis results. I will evaluate my results by comparing my sentiment analysis results with search findings from the internet. Programming Language Q: Which programming language do you plan to use? A: Python Project Workload Q: Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. A: Since this is a solo submission, I will have to complete all the project deliverables (Project Proposal, Progress Report, Software code submission with Documentation, Software Usage tutorial presentation) by myself. Here are my estimates for the workload - * Project Research (Tools, Packages and Twitter data): 5 hours * Software Design and Proof of Concept: 5 hours * Software Development, Testing and Debugging: 10 hours * Project Documentation and Admin Tasks: 5 hours Total Estimate: 25 hours
https://github.com/rudyrath/CourseProject	README.md	CS 410 Course Project by Rudy This is the main place to submit all the course project deliverables submission, i.e.proposals, progress reports, code, final software documentation, demo presentations, etc.
https://github.com/Shivang44/CourseProject	410 Proposal.pdf	"CS 410 Project Proposal Team members: Shivang Saxena Team ""Git Good"" Intelligent Browsing Theme 10/23/201 Github Code Annotation System Current browser systems contain the ability to personalize their viewing experience through bookmarks. These bookmarks allow users to deal with an incredible amount of pages available online by selectively favoriting pages that they are interested in retrieving in the future. They can be easily added through shortcuts or by using the browser menu. However, browser bookmarks contain limitations. As bookmarks are just stored in one big list (or sub lists), searching for and retrieving the relevant bookmark can be a challenging task. Additionally, these bookmarks URLs can become outdated as web administrators update URLs on the fly. Finally, bookmarks contain limited ability for the user to personalize them, as they can only contain a name limited in size. This project proposes to solve this problem by building an annotation system specifically for Github code. Github has large amounts of code files and they can be difficult to index and track. Using bookmarks, especially, can be challenging as even for one project there can be hundreds of source code files. This project proposal proposes creating a Github Code Annotation System in the Intelligent Browsing theme. When viewing any source code file on github, users will be able to click the line and add an annotation with a note. Upon closing the browser and returning to the page, these annotations will automatically be retrieved. This could prove to be a more powerful tool than bookmarks as bookmarks do not allow users to annotate source code files with useful information when later retrieving the document. The proposed technology will be a Google Chrome extension. The chrome extension will allow users to click lines of code in github and create annotations. The annotations will be stored locally on the user's local storage and retrieved whenever the respective github codefile is opened."
https://github.com/Shivang44/CourseProject	Progress Report.pdf	"CS 410 Project Progress Report Team members: Shivang Saxena Team ""Git Good"" Intelligent Browsing Theme 11/15/2021 Progress Report 1. The following tasks have been completed a. Research how to create a chrome extension and understand all the various concepts involved b. Initialize chrome extension project c. Implement skeleton and test extension 2. The following tasks are pending a. Design architecture for annotation extension b. Design and implement annotations in HTML/CSS c. Create persistent storage and retrieval mechanism for storing annotations d. Display existing annotations for a specific github code file/SHA e. Allow user to create annotations for a specific github code file/SHA with text content f. Allow user to delete annotations g. Create presentation and write-up materials for project 3. I am not currently facing any obstacles that are blocking my progress. The main challenge so far has been wrapping my head around the structure and concepts of a chrome extension as they are very specific to chrome (background scripts, content scripts, etc) however I have made good progress."
https://github.com/Shivang44/CourseProject	README.md	"Intelligent Browsing - Github Code Annotation System Team Members Shivang Saxena Team Name Git Good Proposal Please see ""410 Proposal.pdf"" for the proposal Progress Report Please see ""Progress Report.pdf"" for the project progress report"
https://github.com/shchau2/Linked-Page-Search-Chrome-Extension	progress_report.pdf	1 Completed Tasks 1) JavaScript is used for entension frontend: A dummy client is created. There is basic UI to communicate with the Python server. The url of the current page would be sent to the server. 2) Python is used for backend server: A web scraper is created to scrap all links on given page, then store the text files on disk. 3) Java and library Lucene is used for search engine. It has two parts: Indexer and Searcher. Currently the Indexer is completed, it loads text files from disk and create inverted index for further search. 2 Pending Tasks 1) The communication chain is not completed, currently the frontend can send the url to Python server, but Python server only respond with placeholder message. Also the Python server does not make use of the Java search engine. 2) Java searcher is not completed. Once the searcher can send some meaningful messages back. The frontend UI need to be modified to display these search results. 3) A PLSA topic analyzer should be built with Java. 3 Challenges 1) I did not have JavaScript experience, so it took me some time to create the frontend elements. 2) The communication between different parts is tricky. I used WebSocket for JS-Python commu- nication and trying to use Python subprocess library to call Java binary program (still learning). 1
https://github.com/shchau2/Linked-Page-Search-Chrome-Extension	proposal.pdf	Team: NetID: shchau2, Name: Chau Siu Hung (Captain) Theme: Intelligent Browsing Topic: Linked Page Search (Chrome Extension) Description: Index all pages that the current page links to and allow users to search over the collection of pages. For example, if we run the extension on https://book.systemsapproach.org/ (table of contents of a textbook), then we can search over all chapter of the textbook, using a common retrieval function or other techniques (e.g. topic analysis). Motivation: Users would like to search over a online textbook or a blog website. Built-in tools are not always available, or are limited to exact keyword match. Google search with site:domain.com may be used. However, the linked pages may not be at the same domain (e.g. https://axisofordinary.sub- stack.com/p/the-most-counterintuitive-facts-in has links to pages with different domain). Also, we might want to control the granularity of search (i.e. considering a sentence/paragraph as a document instead of the whole web page), or add some extra functions like topic analysis. Data Sets: Online textbook, example: Interactive SICP (xuanji.appspot.com), websites mentioned above. Algorithms/techniques: Web Scraping, Retrival function (BM25) Evaluation: Relevance judged by human, compare with Google search with site:domain.com option on some simple data sets (all linked pages within same domain). Language: JavaScript, Python Workload: Chrome Extension setup/UI (5-6h), Web Scraping/Data Collection (3-4h) Indexing (3-4h) Retrievel Function (1-2h) Extra function (e.g. spliting web pages into paragraphs, topic analysis) (> 6h) 1
https://github.com/shchau2/Linked-Page-Search-Chrome-Extension	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/R2Lin/CourseProject	project _progress_report.pdf	CS410 Project Progress Report Topic: Chrome extension for Google Scholar Date:11/14/2011 Progress made 1. Following the Chrome extension developing tutorial, I've designed the user interface for the Chrome extension and made the .css, .html, and .js and manifest.json files. 2. I've implemented the function to count the independence citation numbers for each publication listed in the Google Scholar author page, in the .js file. Remaining tasks 1. From the articles that cite a publication, find the most relevant articles to the publication, make it a recommended reading list to the scholar. 2. Make a list of the authors who cited your publications, with their Google Scholar links. Challenges faced 1. Learn the steps for making a Chrome extension and make it work on my local machine. 2. Study the Google Scholar page to find the best key word to crawl for the citation counting purpose.
https://github.com/R2Lin/CourseProject	project_proposal.pdf	CS410 project proposal Topic: Chrome extension for Google Scholar Project Team member: Rongrong Lin, NetID: rlin15 Goals: This tool benefits a researcher/scholar who has a google scholar page. On a Google scholar user/scholar page, there is information about the scholar's list of publications, the citation number for each publication and etc. Based on these text contents, with the implementation of a chrome extension, I will extract further information such as: 1. From the articles that cite a publication, find the most relevant articles to the publication, make it a recommended reading list to the scholar. This uses the search engine technique and ranker function (such as BM25) in text retrieval. This functionality is useful for scholars who wants to pick some others' work that's related to his/her own research for reading and studying. 2. Make a list of the authors who cited your publications, with their Google Scholar links (for their contact info and research interests). This uses the technique of web scraping from HTML texts. This functionality is useful to help you with building connections with other scholars who share the same research interest with you and value your work (by citing them). For example, sometimes you may need recommendation letters from people who know your work but are not a colleague of you. 3. The Google Scholar page provides the citation count for each of your publication, but regardless of whether it's a self-citation (cited by yourself and your coauthors) or an independent citation. I will add a function to count the independent citations. This uses the technique of web scraping from HTML texts. This functionality is useful for scholars when they need a detailed analysis of impacts of their work. Algorithms and techniques to be used: Text retrieval, search engine, ranking function (BM25), web crawler, HTML scraping Programing languages to be used: Python, JavaScript Deliverables: * Source codes of the chrome extension * A demo showing how the tool works and how to use the tool Workload estimation: task Hours needed Chrome extension interface design, make .css, .html, .js files 6hrs Add functionality of citation counting 5hrs Add functionality of relevant reading, with Metapy toolkit from Python 7hrs Add functionality of making author list 8hrs
https://github.com/R2Lin/CourseProject	README.md	CourseProject This is the course project for CS410 Text Information Systems. This project develops a Chrome extension for Google Scholar.
https://github.com/swatinanda/CS410-Fall2021-TeamAlpha	README.md	Designing an Alert Correlation Engine using Mutual Information values The modern scaled digital transformation has made it difficult for the humans to keep up with the ephemeral state of IT workloads and processes although most of them has made significant investment on monitoring the application, infrastructure, and network. In case of any outage, these monitoring systems generate different alerts, but they do not generate them at the same time and not in sequence. It takes time for IT operations to find out the relation between these alerts and they end up creating too many tickets for different teams. We intend to correlate these alerts by collecting, analysing, and building a knowledge graph between them so that it can predict and group the future similar events that may affect availability and performance. We also want to improve the performance of the model through relevant feedback channel.
https://github.com/swatinanda/CS410-Fall2021-TeamAlpha	Team_Alpha_Project_Progress.pdf	"Designing an Alert Correlation Engine using Mutual Information values Project Progress - Dated: Nov 14th, 2021 Below is how we are progressing towards our tasks in the project. 1. Which tasks have been completed? Tasks %Complete Notes Implement scoring function 100% We have finalized our scoring function using mutual information and it has been coded for consumption for other modules. Code is uploaded here. High level notes on how do we calculate the mutual information is presented here Data cleanup / preparation (Dataset) 100% We have completed this task. It is being built with real alert monitoring scenarios based on App, Infra and Network monitoring. Some examples are * High Average response time, * High CPU usage on the Host, * Router not reachable, * Kafka Lag breached a configured threshold etc. Our utility will replay the dataset message located here to generate real time alert template data. These alarms along with their mutual information will be populated in a Graph obj and will be sent to downstream Graph database (Neo4j). This additional alarm information will help us to enrich our ""Knowledge Graph"" Create a bag of alerts 100% We have a corpus of Alert messages, Hostname and Source. This will be used to generate random alerts. Each alarm will have this 3 metadata information selected randomly from the corpus. We can create any number of dummy alarm templates through this. Associated code is here 2. Which tasks are pending? Tasks %Complete Notes Build Knowledge graph with limited datasets 10% We have completed the research related to various available frameworks for building Knowledge graph. We plan to use Neo4j for storing the graph. Chatbot will be converting the text query into Neo4j based cypher query to extract any relevant information. Plan to wrap it up by Nov 18th Develop REST API for frontend interaction Implement user interface Neo4j graph will be source of data for chatbot that is in-progress. Will club the two tasks (Rest API & User Interface) to present data into the chatbot. This task is dependent on previous task for completion. Plan to wrap it up by Nov 20th Implement Feedback loop Need to evaluate if we'll be able to accomplish it optimally. Integration, Testing & Evaluation Data for evaluation is being created as part of Dataset preparation. We will add some rules to baseline this data, against which we can evaluate/verify our algorithm output. Will start the Integration and Testing post that (tentatively by Nov 24th or 25th) 3. Are you facing any challenges? We had to do our dataset from scratch, so that is taking some time. The chatbot was our stretched goal, but we have managed to pull it in. Also, we are trying to figure out how we can accomplish the task ""Implement Feedback loop"". If we finish all tasks early will try to implement some part of feedback loop. We have some ideas, as to take user input while the user is interacting with the chatbot etc."
https://github.com/swatinanda/CS410-Fall2021-TeamAlpha	Team_Alpha_Project_Proposal.pdf	XXX-X-XXXX-XXXX-X/XX/$XX.00 (c)20XX IEEE Designing an Alert Correlation Engine using Mutual Information values Abhijit Bhadra MCS-DS University of Illinois, Urbana- Champaign abhadra2@illinois.edu Sanjeev Kumar MCS-DS University of Illinois, Urbana- Champaign sanjeev5@illinois.edu Swati Nanda (captain) MCS-DS University of Illinois, Urbana- Champaign swatin2@illinois.edu Abstract - The modern scaled digital transformation has made it difficult for the humans to keep up with the ephemeral state of IT workloads and processes although most of them has made significant investment on monitoring the application, infrastructure, and network. In case of any outage, these monitoring systems generate different alerts, but they do not generate them at the same time and not in sequence. It takes time for IT operations to find out the relation between these alerts and they end up creating too many tickets for different teams. We intend to correlate these alerts by collecting, analysing, and building a knowledge graph between them so that it can predict and group the future similar events that may affect availability and performance. We also want to improve the performance of the model through relevant feedback channel. I. INTRODUCTION Application monitoring is the process of collecting different performance metrics and log data to help developers track availability, bugs, resource use, and changes to performance in applications that affect the end- user experience. Network monitoring provides the information that network administrators need to determine, in real time, whether a network is running optimally. Infrastructure monitoring provides visibility on all the assets that are necessary to deliver and support IT services: data centres, servers, storage, and other equipment. IT operation performs their day-to-day task and mitigate error scenarios via multiple alerts generated from these monitoring systems. It empowers the users- SRE's, L1/L2, developers, who rely on these alerts to ensure health and well-being of the services and environment. Sometimes these alerts become overwhelming and create unwanted noise. Reading through each alert and analysing to get to the error pattern and identifying actionable alerts is a daunting task resulting in lengthy troubleshooting and remediation cycles and high mean time to resolution (MTTR). In this project we are trying to analyse the alerts and associate them so that we have a model which by identifying the symptoms can predict probable problems upfront so that corrective actions can be taken to prevent system snags. By effectively identifying the correlated alerts, we could reduce the alert fatigue and work on most urgent problem first, with reduced MTTRs. II. PROPOSAL One approach for solving the problems of Alert Correlation, could be to find correlated alerts on the entities(services) based on their mutual information which is dependent on their occurrence in a near time window. We will be generating a score and subsequently a knowledge graph based on that mutual information. Higher similarity score means strong correlation between the events. We should be able to present it in a graphical form or use Language Model that can be searched or suggest probable related symptoms from past learning. If time permits, we will try to incorporate the feedback to the model to improve the performance III. PROPOSED METHOD Following technical steps will have to be performed in order to develop our solution : * Prepare some fake alarm dataset based on different incidents. We might use Kafka to ingest the Alarms. * Group the alerts by entity and type over a window of 5 minutes. Associate the alerts and build a weighted knowledge graph for the alerts as vertex and occurrence count in the window as the score of association/correlation. * Create a bag of alerts using stemming, so that it can be used for symptom retrieval by the End user * Over time use the feedback from user and update knowledge graph by scoring up the positive feedback correlations * Present this correlation via some graphical representation or if time permits - we will create a chatbot to query for outage indicators IV. TASKS IDENTIFIED In order to achieve our end objective, these are the high- level tasks and estimation in hour identified * Data cleanup / preparation - 6 hours * Implement scoring function - 6 hours * Develop REST API for frontend interaction - 4 hours * Build Knowledge graph with limited datasets - 5 hours * Create a bag of alerts - 7 hours * Implement user interface - 10 hours * Implement Feedback loop - 10 hours * Integration and Testing - 8 hours * Document Preparation - 3 hours * Misc. items - 4 hours Our code will be written mainly in Java and Python V. EVALUATION METRICS We will be using Cranfield evaluation methodology for the evaluation. Dummy alerts will be generated, and we will identify their correlation manually. A relevance matrix will be generated for each test alert, and this will be used to verify the accuracy and preciseness of our algorithm
https://github.com/Pradoshks/CourseProject	C410 - TIS - Progress Report.pdf	"CS410 - Fall 2021 - Project Proposal Progress Report Team Name: Team21 Net Ids: Subudhi, Pradosh Kumar - pradosh2 Veera Sikku, Siva Prakasini - spv4 Dunor, Aaron - adunor2 Captain: Pradosh This report provides an overview of the progress on the Project that we are working on and addresses the review comments. Overall Project Plan: In addressing our email sentiment analysis project, we decomposed the project into the following smaller manageable tasks: Emails - Sentiment Annotation - Text Cleaning - Word Embedding - Model Training - Model Evaluation - Model Deployment Challenges: Finding a good email dataset set has been most challenging. We finally found customer- service-sentiment-analysis/data dataset which contains some five hundred fifty thousand emails. The dataset needs a lot of cleaning to make it usable for before performing the analysis. Tasks Completed so Far: Currently we have sourced the emails and processed them into a new data frame containing the test and the sentiment score. We used the ""SentimentIntensityAnalyser"" from NLTK library to assign sentiment categories to each email. The categories include Negative, Neutral and Positive. Next Stage: We are working on word embedding and developing the final model. Review Feedback addressed: * High level component interaction plan through a diagram * Can you clarify the novelty of this proposal? Though this is a project that is like many out there, the machine learning model created from this project could be deployed in many use cases. Unlike a tutorial on the internet that ends when the model is produced, we can release our model for reuse in applications like email clients, customer service ticketing systems and even web sites with some enhancements to the model. * Interesting project idea! Are you planning on selecting key words from each customer's complaints to study the priority? We are not building the model on select keywords. We will utilize NLTK SentimentIntensityAnalyser functionality to assign sentiments. * You are expected to have a preliminary model in the progress report: The Preliminary model can be found at the following location - https://github.com/Pradoshks/CourseProject/tree/main/Model Emails Sentiment Classification Text Cleaning Word Embedding Model Training Model Evaluation Model Deployment"
https://github.com/Pradoshks/CourseProject	C410 - TIS - Project Proposal.pdf	CS410 - Fall 2021 - Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Name: Team21 Net Ids: Subudhi, Pradosh Kumar - pradosh2 Veera Sikku, Siva Prakasini - spv4 Dunor, Aaron - adunor2 Captain: Pradosh 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Topic Chosen: Email sentiment analysis. We will be creating a process for enterprises to quickly analyze customers' sentiment about their product/ service. Problem Statement: Every customer is important to an enterprise for the business growth. To keep the customers loyal to your product/ service, we need to address their issues/ concerns on priority. By building a system which can analyze thousands of emails received every day and categorize emails based on their sentiments (e.g., customers unhappy about a certain product/ service). Relevance to the Course Subject: To achieve this goal, we will have to perform Text Mining and Analysis. 3. Briefly describe any datasets, algorithms, or techniques you plan to use Strategy: Dataset Selection: We have chosen a dataset available in Kaggle which is a collection of customer complaints from various industries. Data Source: Evaluating Customer Service https://www.kaggle.com/janiobachmann/evaluating-customer-service-sentiment- analysis/data Algorithms/ Techniques: We have considered using a Convolutional Neural Network algorithm. We will clean up the data and then convert the data to a text/ CSV format. We will develop a model to perform the sentiment analysis based on some presets. 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? We come up with step-by-step approach (Choosing the right dataset, clean up/Sort the data/Building the model based on algorithm) by splitting task to achieve the expected outcome. We will provide a demo where our model will correctly classify the sentiment of random customer emails. Programming Language: Python 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Justification: For successful completion of the project, we will have to perform the following tasks. We are estimating about seventy five hours is required to complete the project. Considering this we believe; each team member will have a minimum of twenty hours of contribution. * Analysis and research - 10 hrs. * Gather and clean data - 5 hrs. * Build machine learning model - 25 hrs. * Train and evaluate model - 12 hrs. * Team collaboration - 15 hrs. * Project documentation - 4 hrs. * Demo preparation and presentation - 4 hrs.
https://github.com/Pradoshks/CourseProject	README.md	CS410 CourseProject - Email Sentiment Analysis Topic Chosen: Email sentiment analysis. We will be creating a process for enterprises to quickly analyze customers' sentiment about their product/ service. Problem Statement: Every customer is important to an enterprise for the business growth. To keep the customers loyal to your product/ service, we need to address their issues/ concerns on priority. By building a system which can analyze thousands of emails received every day and categorize emails based on their sentiments (e.g., customers unhappy about a certain product/ service). Relevance to the Course Subject: To achieve this goal, we will have to perform Text Mining and Analysis. Strategy: Dataset Selection: We have chosen a dataset available in Kaggle which is a collection of customer complaints from various industries. Source: Evaluating Customer Service https://www.kaggle.com/janiobachmann/evaluating-customer-service-sentiment-analysis/data Algorithms/ Techniques: We are considering using a Convolutional Neural Network algorithm.
https://github.com/jasonzjc/CourseProject	CS410 Final Project Progress Report.pdf	"CS410 Final Project Progress Report Project Topic: Intelligent Learning Platform: Organize the scattered lectures into a coherent ""multimedia textbook"" and create an index Group Name: Freelancer Member: Jiecheng Zhao (jz109@illinois.edu) 1. What tasks have been completed? At this time, I have completed two tasks: literature review and keywords extraction function development. In the literature review, I have reviewed books, papers and guidance about text indexing techniques, text coherence, and ranking. I have also taken a deeper dive into the MeTA toolkit and FLASK. In the keywords extraction function, the contents are extracted from the corpus and the timestamps are discarded. MeTA is used to tokenize the contents, exclude the stop words, and get the highest occurrence of the candidate terms. 2. Which tasks are pending? The pending tasks includes: 1) Keywords and topic indexing function development 2) Webpage output development, and 3) Test and debug. 3. Are your facing any challenges? Yes. One of the major challenges I have experienced at this stage is to refine the candidate terms. 1) Some terms may stem from the same source. A stemmer needs to be used as the filter. 2) Some terms have high occurrence and are not in the stop word dictionary, but they are not quite the keywords of the course. For example, in this text information system course, compute appears many times, but does not qualify for a keyword of this specific course. One possible method is to get the background language model of general computer documents, and filter out these terms."
https://github.com/jasonzjc/CourseProject	CS410 Final Project Proposal.pdf	"CS410 Final Project Proposal Project Topic: Intelligent Learning Platform: Organize the scattered lectures into a coherent ""multimedia textbook"" and create an index Group Name: Freelancer Member: Jiecheng Zhao (jz109@illinois.edu) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. One team member Name: Jiecheng Zhao (Captain) NetID: jz109 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? The topic of this project is to organize the scattered lectures into a coherent ""multimedia textbook"" and create an index. A course typically covers many words, but only a few are key words and relates to the knowledge introduced in the course. The learner may like to quickly find the lecture or the location a specific topic is presented, or a specific key word is defined and explained. This topic relates to the text retrieval, language model, and topic analysis introduced in the text information system class. 3. Briefly describe any datasets, algorithms or techniques you plan to use The initial dataset will be used in this project is the lectures of this class (CS410 Text Information System). It is expected to extend to other classes on Coursera and other Massive Open Online Courses (MOOC) platforms. Technologies such as BM25, language model, and topic analysis (including EM algorithm) 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? The results of the initial work on CS410 will be compared with the key words listed in each week's overview part. The missing and adding key words will be manually checked. Furthermore, the effectiveness of the tool will be demonstrated on other MOOC. The programming language being used for this project will be Python. Toolboxes such as MeTA, coursera-dl, FLASK, will be used for this project. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The tasks of this project and their corresponding workload are listed in the table below. Task Workload (hours) Literature Review 2 Keywords Extraction Function Development 10 Keywords and Topic Indexing Function Development 10 Web Page Output Development 5 Test and Debug 8 Report and Demonstration 2 Total 37"
https://github.com/jasonzjc/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Data source: UIUC CS410 Text Infomraiton Systems. Scraped lecture videos and scripts are found here: https://uofi.box.com/v/cs410lecturevideodata
https://github.com/ramyan2/CourseProject	Project Proposal - Ramya Nagapudi.pdf	"1. Ramya Nagapudi: ramyasn2 (captain) 2. Free Topic: Vegan-Friendly Restaurants Recommender Task Description: I will be scraping and reviewing yelp reviews to recommend a ranked list of restaurants with the best vegan food options in the desired area (that the user inputs). Importance: As a vegan it's difficult to find restaurants on yelp that have good vegan options. Some issues with Yelp's vegan restaurants finder/recommender are the following: - When you search for ""vegan restaurants"" Yelp tends to include ""Vegetarian Friendly"" restaurants in some of the top results. Because these are two different diets, even if a restaurant is vegetarian friendly it can still have 0 vegan options. - Yelp doesn't directly indicate the quality and taste of the vegan food sold at a restaurant. To determine the taste of the vegan options I have to take the time to manually search through the various reviews in yelp with some keywords. - When you search for ""vegan restaurants"" Yelp's results are ranked in order of popularity and ratings. Most of the time, the ratings of a restaurant are reflective of the food on the entire menu (which is mostly non-vegan) instead of the few vegan options. Planned Approach: - Task 1: Take user input, which is the user's desired location, and search for all the vegan friendly restaurants in the desired location (according to Yelp) in order to gather a list of all potential vegan friendly restaurants nearby. Then for each restaurant in that list, I will scrape and review its Yelp reviews which discuss the vegan food sold. - Task 2: Implement a good search query. In this case the best search query finds the most Yelp reviews that review vegan food. It should not be an over-constrained or under-constrained query. For example it could be ""vegan"" or ""plant-based"". - Task 3: Implement a Document Selection algorithm to determine if a document is relevant or not. In this case, the collection of documents are all the yelp reviews for a restaurant and each document is a single review. Using my Document Selection system and my search query, I will retrieve a list of all the relevant documents (reviews). The relevant documents are the reviews which discuss the vegan food at the restaurant. - Task 4: Evaluate my document selection system with precision. - Task 5: With this narrowed down list of reviews, I want to identify if each review is positive or negative. To do this, I will perform sentiment analysis to determine if a review on the vegan food was positive/negative/neutral. - Task 6: Aggregate the results from the sentiment analysis to compile a ranked list (the restaurants with the most positive reviews will be ranked higher on this list). - Task 7: Compute average precision of the ranked list for evaluation. Tools and Datasets: jupyter notebook and Yelp's datasets/API Expected Outcome: The expected outcome is a ranked list of the restaurants in the area with the best vegan options. Evaluation: I can compute the average precision against an average of a few vegan peers' relevance judgements to evaluate my final ranked list. 3. Programming Language: Python 4. 20+ hour workload: - Task 1 = 2 Hours - Task 2 = 1 Hour - Task 3 = 7 Hours - Task 4 = 1 Hour - Task 5 = 9 Hours - Task 6 = 1 Hour - Task 7 = 1 Hour"
https://github.com/ramyan2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/ramyan2/CourseProject	_Progress Report - Ramya Nagapudi.pdf	"Ramya Nagapudi (ramyasn2) Free Topic: Vegan-Friendly Restaurants Recommender 1. So far I have worked on task 1 of my project proposal which is the following: Task 1: Take user input, which is the user's desired location, and search for all the vegan friendly restaurants in the desired location (according to Yelp) in order to gather a list of all potential vegan friendly restaurants nearby. Then for each restaurant in that list, I will scrape and review its Yelp reviews... To do this I built two scrapers: The first one is in Python and it scrapes Yelp data given any keyword and location. My scraper extracts the businesses that show up on Yelp's search result given a particular keyword and location. (For example: keyword = ""vegan"", location= ""chicago""). It extracts the high level details of each business in that search result such as the business name and Yelp URL. To use this scraper I would provide the keyword ""vegan"" and the location that the user provides as input to gather the list of businesses along with their Yelp URLs that show up on Yelp's search result for that particular keyword and location. The second scraper is also in Python and it uses BeautifulSoup and the web scraping API ScrapingDog to scrape public reviews for each business in the list gathered from the first scraper. For each business on the list it makes a GET request to a target URL (which is a business's Yelp URL that I get from the first scraper) to get a business's raw HTML data. BeautifulSoup parses this and from here I run a for loop to get every review for that business. 2. The tasks pending are implementing a good search query, implementing a document selection algorithm, performing sentiment analysis, and performing evaluation. 3. As of now I am not facing any challenges."
https://github.com/ottopiramuthu/CourseProject	progress-report.docx	CS410 Project Progress Report [Otto Piramuthu] NetID: obp2@illinois.edu [Free] Topic: Online Algorithms for Technology-Assisted Reviews Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? I completed the core of the project on the use of online algorithms for technology-assisted reviews (TAR) with counting process to determine the number of relevant documents (R) in the collection. This core part of the project to figure out the topic, look for related published literature, develop the algorithm, write related code, test the performance of the proposed algorithm (code) against those from three benchmark published methods, and write up the project report of about 12 pages easily took me more than 20 hours. I want to extend it by considering additional stopping criteria. I read through several related documents and have identified Horvitz-Thompson estimate to determine R. 2) Which tasks are pending? I want to write code for this selected algorithm and then incorporate this in my online learning code 3) Are you facing any challenges? Identification of the Horvitz-Thompson method to implement among the several that are in published literature was a challenge.
https://github.com/ottopiramuthu/CourseProject	project-proposal-otto-piramuthu.pdf	CS410 Project Proposal [Otto Piramuthu] NetID: obp2@illinois.edu [Free] Topic: Online Algorithms for Technology-Assisted Reviews Please give a detailed description. What is the task? Why is it important or interesting? Exhaustive manual review of documents to determine their relevancy for a given purpose is error-prone and resource intensive. This has led to the consideration of computer-aided processes for litigation support where only a small subset of the entire set of documents is manually reviewed with comparable performance as exhaustive manual review, resulting in the reduction of human-introduced error and allocated resources. As more evidence for the superiority of Technology-Assisted Reviews (TAR) becomes available, researchers and practitioners have resorted to the exploration of various methods to improve process efficiency without significant degradation in output quality. Of particular interest in this process is the decision on when to stop reviewing additional documents. I evaluate online algorithms, which are a natural fit for this purpose. What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? I have tried counting processes to determine the total number of relevant documents (R), which is necessary for online learning. The next step is to try at least one other method to determine R and incorporate this information in online algorithms. This is followed by comparison of the performance of these methods. The dataset used is from CLEF 2017 e-Health Lab Task. The hoped outcome is that the performance of online algorithms for TAR is competitive with those from existing methods. I plan to compare results from this study with that from a published study Which programming language do you plan to use? Python. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. I am doing this project by myself and have written relevant code for online algorithms with counting processes. I have written up the results so far as a paper that is about 12 pages long. You may list the main tasks to be completed, and the estimated time cost for each task. I want to identify at least one other method to determine R and incorporate that method with online algorithms. I suspect that reading papers for their relevance, selecting an algorithm for further consideration, writing code for this selected algorithm, and then incorporating this in online learning would take me at least 20 hours.
https://github.com/ottopiramuthu/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/bllguo/CourseProject	Progress Report.pdf	"Progress Report: Generating Hierarchical Topic Taxonomies over Text * Gathered dataset from my workplace. There are two levels in the hierarchy. The top level contains verticals such as ""auto,"" ""technology,"" ""health,"" and so on. There is a second level below. For example, ""VR headset"" would fall under ""technology."" The texts under ""VR headset"" are a collection of short keywords such as ""VR gear,"" ""virtual reality goggles,"" ""virtual reality games,"" and so on. * Tried to read hPLSA paper - A hierarchical model for clustering and categorizing documents, Gaussier et al. o Could not understand how the model worked after several hours, unfortunately. I feel like this idea might be beyond my current scope * Wrote several text vectorizers, including one that can use pretrained word embeddings and fine tune them on new data. o I will use these in conjunction with clustering methods like kmeans and see if this approach can generate a sensible hierarchy over my dataset given only the keywords/documents * Instead of hPLSA/hLDA, I will pivot to building hierarchical models using ""flat"" clustering models and previously mentioned text vectorizers o Perhaps even classification models could be used (if we frame every combination of topic/subtopic as a class) o My final hierarchical model implementations will form my submission"
https://github.com/bllguo/CourseProject	Project Proposal.pdf	"Members: Bill Guo (billguo2) Topic: Generating Hierarchical Topic Taxonomies over Text We've discussed topic modeling and its uses in class. However, the topic modeling approaches covered, such as LDA, do not capture topic hierarchies and subtopics. For example, in a corpus of documents, LDA might capture the topic ""gardening"" but not the more specific topic ""succulents."" Or it might identify both, but not that ""succulents"" is a subtopic nested under ""gardening."" This is an interesting problem because it comes closer to reflecting a human understanding of the world, and how different concepts are related to each. It is also a practical task. For example, I work in advertising technology, and the concept of categorizing keywords into a topic taxonomy to improve targeting and ad relevance is critical. We built a taxonomy manually - we even hire people whose whole jobs is to do this - so it would be interesting to see how to automate this process. I can gather a dataset of advertising keywords from my workplace for testing purposes. Some of this data was already labeled by hand with a topic hierarchy, so I can evaluate my outputs with it. Specifically, I am interested in: * Implementing hierarchical topic model methods. Two I found are hierarchical PLSA and hierarchical LDA. This will entail reading and implementing the relevant papers. * Adding support for different text representations, including bag-of-words and tf-idf representations as discussed in class, as well as word embeddings. o I also want to explore fine-tuning pretrained embeddings with a given corpus * Building a hierarchical modeling algorithm via successive application of flat topic modeling methods like normal PLSA or LDA, and seeing how effective that is. * Packaging the above (code for creating numeric representations of text data, training/tuning word embeddings, and running hierarchical topic models) into a Python package. * Evaluating performance on my dataset. Language: Python Workload: * Gathering dataset - 2 hr o I will not be sharing the dataset, but will report aggregate performance metrics, and possibly provide some representative examples * Writing text vectorizers - 3 hr o Should be very straightforward; budgeting some extra time for looking into word embeddings and doing transfer learning * Researching and writing hierarchical topic model implementation - 20+ hr o I think this is likely a very conservative estimate of how long it will take me to digest and code a paper. I aim to only implement one of hPLSA/hLDA, but if things go more smoothly than I predict, maybe both * Building hierarchical modeling algorithm via successive application of flat topic modeling methods - 5 hr"
https://github.com/bllguo/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/timothycheung1/CourseProject	Project Progress Report.pdf	Project Progress Report Which tasks have been complete? Research has been completed. Implementation of web crawler is underway. Which tasks are pending? The full implementation is pending. Are you facing any challenges? The main challenge I am currently facing is crawling through Yelp pages in the most efficient way. This is because my crawler only crawls one page at a time, which has a limited amount of reviews.
https://github.com/timothycheung1/CourseProject	Project Proposal.pdf	What are the names and NetIDs of all your team members? Who is the captain? - Timothy Cheung tcheung5 (captain) What system have you chosen? Which subtopic(s) under the system? - Topic: Free topic - Sub-topic: Sentiment analysis I have chosen to base my project on sentiment analysis. I will train a model based on Yelp reviews, such that different words and phrases are associated with a rating. The outcome is to develop a model that can predict a rating that a user would give based on the comment they made. In order to obtain data from yelp, I will use a web crawler. Briefly describe the datasets, algorithms or techniques you plan to use. - Web crawler: I will develop my own web crawler that can obtain the comment and ratings on any given Yelp page. I will also add an extension that will allow the crawler to identify multiple pages at a time to crawl from. For example, I may find a list of Chicago restaurants and the crawler will start crawling the pages linked to the first page of results. - Sentiment analysis model: With training, the model calculates Expected Overlap of Words in Context (EOWC) to determine a rank by comparing similarities between the comment to be predicted and the training data. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? - N/A, not adding a function How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. - Two systems: crawler, predictor - Crawler provides data for training and is only called upon when training is needed - Predictor uses stored training data Which programming language do you plan to use? - Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. - Research: 5 hours. In order to utilize the model for the given topic, I will need to do research and small test samples to examine whether and how to fit the model. - Implementation: 8 hours. Need to implement web crawler and predictor - Tuning: 2 hours. In order to maximize accuracy of prediction, I will need to do some tuning. - Documentation and miscellaneous: 5 hours. In order to prepare reports and other miscellaneous items.
https://github.com/timothycheung1/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jairmorais/CourseProject	ProjectProgress.pdf	Progress Report - CS 410 Project After search for courses that could be use in this project and could fit in two criteria open free courses where the material, books, are also free and video. It will be use the following course as data test : https://ocw.mit.edu/courses/sloan-school-of-management/15-031j-energy-decisions-markets- and-policies-spring-2012/index.htm The course has the video and video transcriptions and the material, a great set of articles, are free and can be used for the formation of the data set. The difficult of the task was to find the course were free content and free book material. The next step is finished the datasets to be used to test the similarity of the videos and the book material.
https://github.com/jairmorais/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Project Proposal Theme 2: Intelligent Learning Platform Team Members Jair Vieira Morais Filho - jairv2@illinois.edu - captain of the team Topic : Integration of the lecture videos with the content of the textbook to enable convenient switching between the two. It would be interesting to have the videos topics interact with the textbook. The textbook has more deep information than the video lectures, so for someone that wants to have a broad view of the subject, he would search in the textbook the information. The same for the text book, it would allow someone relate the information present on the text book to video content. Datasets, Algorithms or Techniques Each video has vocabulary list, list of documents is the pages of the textbook (each document is a page). We have to run text retrieve PLSA for each video vocabulary list present in each video. (Lecture 1)-Video 1- List of vocabulary run over the list of pages(documents) and find the pages with more probability to be in the video 1 topic. Programming Language - Solution It will use Python. It will use the textbook pdf split each page. Get the list of videos for each video content. Workload It will be necessary for convert the textbook to a table where each row is a page - 4hr It is necessary to get each video content to generate the list of vocabulary for each video -4hr Implement PLSA model for this proposal - 12hr Final Result The result will be a table with (video 1) is also in page 1,3 4
https://github.com/sudiptobilu/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/bhullar5/CourseProject	CS 410 Progress Report.pdf	"CS 410 Progress Report 1) Which tasks have been completed? As of now, I have completed the code that will be used to generate the dataset. The code basically scrapes the Google news page. I basically input a Covid related query plus the country and then extract the headline. The final dataset consists of a series of countries and the corresponding headline. Since I haven't really used the choropleth map feature available in Python, I wanted to implement that with respect to a mock dataset consisting of countries and their values. After generating the mock dataset, I set up code that could be reused to generate a map when I finally finish the sentiment analysis on the actual dataset. 2) Which tasks are pending? The following tasks are pending: 1) Implement sentiment analysis on the dataset (using the NLTK library) 2) After getting the values from the sentiment analysis for each country, I will display the results on the choropleth map and also showcase the vaccination rates for those countries 3) Lastly, I have to learn how to use dash, so that I can display my choropleth on a web hosted dashboard. 3) Are you facing any challenges? Yes, the biggest challenge was that I was not able to get my Twitter API use case approved by Twitter. Because of this, I had to shift my focus to implementing web scraping in Google News. I ended up scraping data from the Google News page to generate a list of headlines for each country. This is different from my original plan because initially I wanted to generate a list of tweets for each country. Other than the change in the data source and the type of data, my plan is exactly the same. I will still be applying sentiment analysis to each of the headlines. Questions from the Project Proposal: 1) Please be more specific about the output. It is not very clear now After I apply sentiment analysis to the dataset, I expect the output to be ""positive"" or ""negative"". For each country, I will average the number of positive and negative reviews to generate one value. This will then lead to a value for each country and that will be mapped to a choropleth map that reflects the values for each country. I will also add the vaccination rates for each country (from a dataset available online), so that we can see the relationship between the two. 2) The method used to generate the data set is not very clear. Are you going to do web crawling? Do you plan to save the data in what format, database or local files? I was initially going to implement the Twitter API which gives us tweets and then from the output of that API, I was going to generate a list of tweets and countries. Because my use case for the API was not approved by Twitter, I am now using web crawling techniques on the Google news page to generate a list of countries and their Covid-19 related headlines. The dataset will consist of a headline followed by the country and I will apply sentiment analysis on this. This requires no use of APIs, I am using the web crawling techniques taught to us in class. 3) What is the user input? After working on the project, I realized that no user input is needed because I am trying to do a sentiment analysis on headlines. In this use case, I already know what I have to input, I want to generate values and create visualizations to display the analysis."
https://github.com/bhullar5/CourseProject	CS 410 Project Proposal.pdf	Project Proposal CS 410 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I am choosing to do this project alone. My name is Sukhween Bhullar and my NetID is bhullar5. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? For my free topic I plan on doing sentiment analysis. Specifically, I plan on implementing the Twitter API (https://developer.twitter.com/en/docs/twitter-api ) to make specific Covi-19 related tweets, then apply sentiment analysis to those results based on that input. I find this topic to be interesting because it applies theory and text analysis to a very serious public health crisis. I want to see how Twitter highlights the sentiment around Covid-19 and vaccinations. Is the coverage positive or negative in different countries? Do Twitter users in different countries have different sentiments around the vaccine? How do those sentiments reflect the vaccinations in those regions? I will either concentrate on the global scale (by country) or stay within the United States (by state) My planned approach is as follows: Part 1) This will involve exploring the API and the structure of its results, then processing those results to create some kind of a usable dataset that consists of either a list of tweets or a list of articles and their corresponding geographic regions. In the case of a tweet, we will use the geo-tagging feature, but for the google query we will use the location used in the query itself (5 hours). This step alone is pretty time consuming and tedious because it requires a thorough understanding of the different fields that are available in the API and choosing which ones to fully leverage. Part 2) This will involve doing the actual sentiment analysis on the datasets aggregated by geographical regions. I plan on doing a geographical analysis of positive and negative sentiments on the vaccine. Part 3) This will involve displaying the results, for this I will use the Dash framework. Part 4) I will test various queries and make sure the results are displayed properly. I plan on using the NLTK library and using many of its functions, such as nltk.corpus.stopwords.words, nltk.sentiment, nltk.FreqDist and many more as I begin working on the project. I will also be using the Python Dash framework to display the results of sentiment analysis, preferable through a map that displays the results of the analysis alongside actual vaccination data for that region. The expected outcome would be to see a trend of negative sentiments when discussing vaccinations and a lower vaccination rate in that region. I would also evaluate the actual results of the sentiment analysis on a tweet by tweet basis. In the context of tweets, I will go through the specific tweets and tag them myself at first to create a test set. I will then split the data into a subset of training data. I plan on doing this with a mixture of regions and then applying that algorithm to the remaining query inputs because we can't get all possible results. 3. Which programming language do you plan to use? I plan on using Python and then using the Dash framework to create a dashboard which will essentially be used as an interface to get user input and display results accordingly. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. This project will definitely take me at the very least 20 hours. I plan on dividing the project into 3 parts. Part 1) This will involve exploring the API and the structure of its results, then processing those results to create some kind of a usable dataset that consists of a list of tweets and their corresponding geographic regions. In the case of a tweet, we will use the geo-tagging feature, but for the google query we will use the location used in the query itself (5 hours). This step alone is pretty time consuming and tedious because it requires a thorough understanding of the different fields that are available in the API and choosing which ones to fully leverage. Part 2) This will involve doing the actual sentiment analysis on the datasets aggregated by geographical regions. I plan on doing a geographical analysis of positive and negative sentiments on the vaccine or any news around Covid-19. (12 hours) Part 3) This will involve displaying the results, for this I will use the Dash framework. (10 hours) Part 4) I will test various queries and make sure the results are displayed properly. (2 hours)
https://github.com/bhullar5/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/xinqianx/CourseProject	CS 410 Progress Report.pdf	Progress Report: HyperLink Classification Group Name: TTIS 5 Captain: Zhengkai Zhang (zz68) Yuan Chung Ho (ych11) Wan Feng Cai (wfcai2) Xinqian Xiang (xinqian6) Zheng Ma (zhengma3) 1) Which tasks have been completed? For the final project, our main approach is to do the Link Classification. We defined the database, UI and API for it. Database: User table contains ID, name, categories and feedback columns. Category table contains id, name, and keywords. API: For each table, we all have to create, update, delete, getByName, and getAll to get the data from our database. In addition, we have the API that allows users to send the link and user information to be processed, and the API that receives the user feedback on our decision. UI: We have a simple UI for users to send their URL and select the user to identify the classification. Also, we have the API to create new user records in our database. For category, we didn't create any UI, but the provided API will allow us to write a script to insert the data. 2) Which tasks are pending? Data: We need some real data for categories, keywords, and users. It will require us to write a script to set up the database record, and script to fetch the data from Online. Scrape web: We need to write a method based on the input url to get all the content inside of that url. It will require work to get the content sometime recursively. Get the keywords: Based on the content we get in the URL: we will need to find the keywords in the website. Decision maker: Based on the keyword and database record, we will decide if the user is interested in the url or not. UI/API integration and improvement: There is still a lot we can do to improve our api and ui, also integrate with later compare algorithms and the scrape part. 3) Are you facing any challenges? Setup for each team member. Since our team has different backgrounds, we need to sync and get everyone on the same page. Scrape the web and the algorithm to make the decision will be a big part of work for it. Database insync, we still use local databases. Will need to spend some time to set a remote one or have a unique set up script for our local database.
https://github.com/xinqianx/CourseProject	CS 410 project proposal.pdf	Project Proposal: HyperLink Classification Group Name: TTIS 5 Captain: Zhengkai Zhang (zz68) Yuan Chung Ho (ych11) Wan Feng Cai (wfcai2) Xinqian Xiang (xinqian6) Zheng Ma (zhengma3) Abstract We receive a mass amount of information everyday, for example advertisements from email and push notifications from search engines, and it comes with a link to another page. We scan through titles and quickly make decisions whether we are interested or not. It is time consuming in searching through the interesting information. Therefore, we are encouraged to seek a way to pick out useful information to make our life more efficient. The HyperLink Classification is a project that helps users to identify whether a link they receive from email or search engine (eg: google) meets their needs. The user profile will be built based on their selected interested categories and some other helping data like relevance feedback. It will help the system identify if the link is useful or interesting to them. Scope of Work The task consists of two subtasks. The first part is to build a web based/terminal based tooling that allows an user selects their interesting categories and enters a link. The second part is to judge whether the link meets user's interest and to update the system based on users feedback or click through. We will need to build client side and server side for this tool and based on the entered hyperlink we will parsing the web page in our server side. With this parsed data from the web, we will compare and run the algorithm we learned to decide if the link fits to the current user's needs and return the result to the user. In addition, we plan to add some user feedback systems based on the user feedback or click through if the user wants to do it. We could potentially use these data to improve our algorithm for later judgements. Introduction Why is it important or interesting? There are many systems that provide recommendations or classification for different purposes, but we haven't seen any tools that help to link classification. This project is a fundamental one with potential to extend to a start-up project. It could potentially be used for helping any pages/email to check their sublink or content link, marking the qualified link with some highlights to the user and even give tooltips. It could be a very useful plugin and generate important user data for different web-pages or companies. Our planned approach 1. Build the client(web/terminal) and server with APIs and database. 2. Obtain the initial data for each category and write the script to parsing the web page. 3. Clean the data and develop algorithms to evaluate the parsed data to compare with the user database for outputting user results. 4. Make use of user's relevance feedback to update the evaluation algorithms and user database for future judgement. Methods Tools, systems and dataset We will need javascript for some front end and python for server side. We will also use scripts like beautiful soup and databases like mysql. For natural language processing, we plan to make use of some functions from Google Nature Language AI/Microsoft Cognitive Services and NLP Algorithms. We will create a user dataset based on ourselves and our friends to examine the tools and systems created. Programming languages Javascript, html, python, flask, beautiful soup, mysql/sqlite. Outcome and Evaluation Outcome of the system will be a binary judgement (yes or no) for the link for the particular user and help them decide whether the link meets their interest. Then they can make a decision whether they want to click/read the content to save their time. Users will use this tool to see if it helps to filter out the links they are interested in. It's not easy to get many users but all our team members could use the tool and give feedback. We could also make it a real web tool to get feedback from others. Process In this project, we will work as a group to build the tool. It will take over 100 hours for our team with 5 members. We will follow our processes shown below and approximate our time spending for each section. Note that the list only includes the implementation time, we will need to spend time learning the language. 1. Build the front end interface (allow user to select category, input link): ~10 hours 2. Build the server side api and database (provide api for inserting data/do classification, build the database for the user): ~20 hours 3. Scripting all potential web link and be able to evaluate the content text inside and integrate with server: ~15 hours 4. Prepare each category's data with different source and integrate with backend/front end: ~15 hours 5. Write the algorithm and integrate with server side: ~ 25 hours 6. Test and evaluate the tooling: 15 hours 7. Other optional features may need: x hours Timeline Execution Schedule Event October 25, 2021 Proposal submission November 1, 2021 Prepare data, scrape links, build api/interface November 8, 2021 Prepare data category and Implement algorithm November 16, 2021 Project Progress Report Submission November 22, 2021 Test and evaluate tool, add other features if needed November 29, 2021 Finish up project code and prepare the presentation material December 10, 2021 Project Code, Documentation and Presentation submission
https://github.com/xinqianx/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/justynlao/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/zhuxiaoyu909/CourseProject	CS410 Project Progress Report.pdf	Progress Report Team * Xiaoyu Zhu, xiaoyu23 (Captain) * Cheng Wei, chengw5 * Jiaxi Wu, jiaxiwu4 1) Which tasks have been completed? Task Status Details Research for implementation details 60% completion Found the common words data source online for the ranking function: https://www.wordfrequency.i nfo/samples.asp Found the Facebook children story datasets: http://www.thespermwhale.c om/jaseweston/babi/CBTest .tgz Clean and compile datasets 20% completion Reviewed the Facebook children story datasets and performed preliminary data cleansing. Index by sentence 10% completion Set up the environment and drafted the index script. Ranking function 10% completion Completed a preliminary ranking function with the top common words. Completed the web scraping function to scrape the word definition from the online dictionary. Compile evaluation dataset 0% completion Not started. Project Report and Demo 0% completion Not started. 2) Which tasks are pending? See above table for details. 3) Are you facing any challenges? For the datasets, we managed to find a Facebook children story datasets as a starting point and reviewed the data structure. We have compiled the datasets and completed some preliminary data cleansing, including removing short sentences, removing numbers and removing duplicate sentences. For the ranking function, we are still researching to determine the best approach. For now, we have decided to use the top 5000 common words and sentence structure difficulty to determine the difficulty of the resulting sentences and rank the results.
https://github.com/zhuxiaoyu909/CourseProject	CS410 Project Proposal.pdf	Team JXC Project Proposal - Free Topic 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Xiaoyu Zhu, xiaoyu23 (Captain) * Cheng Wei, chengw5 * Jiaxi Wu, jiaxiwu4 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? * Topic Description: we want to build a proof of concept for a children-friendly vocabulary learning app to help children memorize words easily by returning relevant example sentences ranking by difficulty after a user keys in a word. We think it's very beneficial for parents to utilize this app to help educate their children based on feedback from our team member who is a parent. * How the app works: a user will enter a word, the app will return the definition, synonyms, and relevant example sentences that are age appropriate and children friendly. The result synonyms and example sentences will be ranked based on difficulty as we would like to return the easiest to understand for better learning experiences. We will exclude stop words the purpose of this app given it doesn't provide much educational value. If we have time in the future to build the product, we will consider including user feedback to rank the results. * Preliminary data sources: * Word definition: (https://data.world/idrismunir/english-word-meaning-and-usage-examples) * Word ranking: (https://www.kaggle.com/rtatman/english-word-frequency) * Children friendly sentence: [Facebook Children Story Project](https://research.fb.com/downloads/babi/) * Planned approach and task: * Research for implementation details * Clean and compile datasets * Index by sentence # We plan to use the Metapy library. * Ranking function to rank the difficulty of the sentence # We plan to use GloVe to rank words for synonyms. * Compile evaluation dataset * Project Report and Demo * Expected outcome: * After the user enters the word, the following will return as results: # The word that the user enters # The definition and most relevant synonyms # Relevant sentences * Evaluation: * We will leverage the Cranfield Evaluation Methodology and manually create our own evaluation dataset. We will randomly select a few words and manually rank all the sentences that have the matching word returned by the app and compare it against the ranked results. 3. Which programming language do you plan to use? * We plan to use Python and Javascript. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Hours Research for implementation details 10 Clean and compile datasets 5 Index by sentence 20 Ranking function 30 Compile evaluation dataset 5 Project Report and Demo 10 Total Estimated hours: 80 hours
https://github.com/zhuxiaoyu909/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/periclesrocha/CourseProject	credits.md	Credits This project is used for educational uses only. It was built as a class project for the CS 410 Text Information Systems fall 2021 class taught at the University of Illinois at Urbana-Champaign. It uses the following resources: - Lyrics databse: obtained from the Open Lyrics Database project: https://github.com/Lyrics/lyrics
https://github.com/periclesrocha/CourseProject	CS 410 - Course Project Proposal.pdf	"CS 410 - Course Project Proposal - Fall 2021 Pericles Rocha (procha2@illinois.edu) - team leader/coordinator Gunther Correa Bacellar (gunther6@illinois.edu) ""My Kind of Music"" Motivation Music is an important part of human culture. While it is often thought of as simple entertainment, it can also impact how an individual feels and affect their mood. In fact, music can even be used in therapy to help relieve pressure or certain feelings. Researchers have pondered the possible therapeutic and mood boosting benefits of music for centuries. Specifically, researchers at Durham University and the University of Jyvaskyla, Finland1 discovered that even sad music can lift your mood, while other studies suggest music can boost happiness and reduce anxiety. From the drumbeats of our ancient ancestors to today's unlimited streaming services, music is an integral part of the human experience. In some cases, users may seek to boost their current feeling, regardless of what that feeling is, and they seek for the right song to enhance what they are feeling. Our proposal is to build a recommendation system that will suggest songs to a user based on their desired mood and a few keywords. We will ask the user to provide what their desired mood is from a 1-5 scale (where 1 is ""very sad"" and 5 is ""very happy""), a few key words, and suggest songs that match that sentiment and those keywords. This project falls into the ""Free Topic"" theme. Method We have obtained a database of song lyrics from a diverse set of music genres. Our application will determine the mood of each song using sentiment analysis on each of the song's lyrics, and build a sentiment index. This sentiment will be a 1-5 scale, where 1 represents negative sentiment and 5 represents positive. Based on the sentiment index, we will use text retrieval techniques to find songs that match the terms provided by the user, within the songs that match the desired mood. Finally, we will provide suggested song names to the user, ordered by relevance, based on the content of the song lyrics. Songs will be indexed for sentiment and key words based on their lyrics only. We will not analyze melodies or any audio conte nt. This application will be limited to the English language. We will use Python as our programming language. Milestones The following are the steps and key milestones for this project: Task Time needed Completion date Build the songs database with lyrics 2 hours Oct 30 Build the sentiment analysis algorithm and document indexes 12 hours Nov 8 Progress report 2 hours Nov 15 Build search engine to pick relevant song lyrics 12 hours Nov 22 Build user experience and app interface 8 hours Nov 29 Software code submission with documentation 8 hours Dec 9 Evaluation We will use roughly 3000 song lyrics for this project. Each song has its own characteristic in terms of what mood they convey in their lyrics. This is highly abstract and requires manual labor for testing and validation, so we will manually perform and document 300 tests (10% of the sample) to ensure the sentiment predicted by the algorithm is close enough to what the song lyrics are saying. Having tested and adjusted sentiment analysis, we will verify the relevance of the search engine using user queries and determine if the results are relevant or not. We will fine tune our algorithm until the index is optimal and provides relevant documents (in this case, song lyrics). 1 Research reveals pain and pleasure of sad music - Durham University (https://www.dur.ac.uk/news/newsitem/?itemno=28329)"
https://github.com/periclesrocha/CourseProject	CS 410 - Progress Report.pdf	"CS 410 - Course Project Progress Report - Fall 2021 Pericles Rocha (procha2@illinois.edu) - team leader/coordinator Gunther Correa Bacellar (gunther6@illinois.edu) ""My Kind of Music"" This document provides a status update on project ""My Kind of Music"". Progress When we first started this project, we knew we would need a database of song lyrics. We eventually found a database of almost 3000 songs hosted by an open source project called The Open Lyrics Database Project (https://github.com/Lyrics/lyrics). Every other solution in the industry requires some sort of paid membership, so for the learning purposes of this project, this open-sourced database will satisfy our requirements. Therefore, we've chosen to work with this database. When working with this database, however, we discovered that approximately 20% of the lyrics in this database were on languages different than English. We also discovered that some lyrics files were empty, containing only some song metadata, without containing any actual lyrics. As a result, we decided to implement some data cleaning routines to ensure we'd work only with quality data, and specifically, with song lyrics written in the English language. To achieve our data quality goals, we focused on three main pillars: - Metadata removal: the Open Lyrics Database Project standardizes how lyrics are submitted to them by requiring the insertion of metadata in the bottom of the lyrics files. This metadata includes the artist's name, the name of the album where this song was released, and additional, optional information such as the release year and a genre. This metadata starts with a sequence of underscores (""___"") in the bottom of the file, with subsequent lines that add the metadata in question. Our routine iterates through the lyrics files and discards anything starting from the line with the sequence of underscores. This allows us to use only the actual song content for sentiment analysis. - Language detection: as mentioned, roughly 20% of the lyrics in the database are written on languages different from English. To mitigate this, we implemented a routine that uses the TextBlob library (https://textblob.readthedocs.io/en/dev/) to detect the language a song uses. We then discard any songs whose lyrics are not written in English. - Detailed logging: our song classification script logs almost all operations done by the program. Amongst the information that it logs, we included the actual number of songs found on the source database, the number of songs that were found to be in English and were categorized, the number of non-English songs and their names (so that we could manually verify if this detection was accurate), how many errors occurred, and how many songs were classified per each mood (this process is described below). Logs are written in the ./logs folder. As a reminder, our program recommends songs to users based on their desired ""mood"" and some key words. So, if an user would like to hear songs in a bad mood and that contain certain keywords in its lyrics (e.g.: ""mad"", ""love""), the system will recommend songs whose lyrics are classified in a ""bad mood"" and that contains words relevant to the ones desired by the user. Although the song search mechanism is a ""live"" one that resolves queries while users submit them, song categorization is a batch process that happens in advance. We scan all songs in the source database and categorize them in one of five desired moods (1- Very Bad, 2-Bad, 3-Neutral, 4-Good, 5-Very Good) using sentiment analysis. The data cleaning process that we implemented allows us to start the categorization process with confidence, knowing that it will only contain the actual lyrics (without the metadata on the files), and knowing that we're only using songs in English. For the actual song categorization, this demands clarification, given some of the comments received from our project submission. We're using the Valence Aware Dictionary for sEntiment Reasoning (VADER) open-source package within the Natural Language Toolkit (NLTK). The VADER package is widely used in the industry for sentiment analysis, so training a model of our own with only 3000 songs would make a time-consuming exercise with little to zero benefits. Therefore, we decided to utilize VADER to score our songs, as opposed to implementing our own sentiment analysis algorithm and train or own models. To get the sentiment for songs, our program simply scans through the whole cleaned dataset and gets the polarity scores for each song. This returns a vector that contains the amount of positive, neutral, and negative sentiments in the lyrics, as well as a ""compound"" attribute which provides an average score between -1 and 1, where -1 represents very bad, or negative sentiment and 1 is very good, or positive. Songs are evenly categorized using the compound score as follows: - ""1-Very Bad"" when compound < -0.6 - ""2-Bad"" when compound >= -0.6 and < -0.2 - ""3-Neutral"" when compound >= -0.2 and <= 0.2 - ""4-Good"" when compound > 0.2 and <= 0.6 - ""5-Very Good"" when compound > 0.6 When songs are categorized, the actual lyrics file is copied to a destination folder that aggregates all songs within a given mood. The search engine that looks for keywords later on uses all documents in a given ""mood"" folder to look for relevant documents. Remaining Tasks We've dedicated most of our effort thus far in data quality and sentiment analysis. During the month of November, we will focus in the following tasks: - Enhance the sentiment analysis algorithm. Today, the algorithm passes the whole song to the analyzer. We understand that songs can have nuances, so we'll experiment with other approaches, such as taking the sentiment of each sentence in a song and then computing the average. Also, currently we use tokenization and remove stop-words from songs before we run sentiment analysis, so we'll continue to experiment to understand if this is optimal (estimated 8h) - Implement the text retrieval engine to fetch relevant documents ins response to the user query (estimated 6h) - UX and documentation (estimated 12h) Challenges The main challenge we've perceived thus far with the sentiment analysis algorithm was that our dataset didn't turn out to be well balanced across the different ""moods"". When looking at 2,444 lyrics (all the lyrics in English), they were categorized the following way: - 1-Very bad: 1183 songs (49%) - 2-Bad: 142 songs (6%) - 3-Neutral: 105 songs (4%) - 4-Good: 129 songs (5%) - 5-Very good: 885 songs (36%) This means nearly half of songs are contained in one category: the ""Very Bad"" mood. We've pondered this issue and eventually decided to proceed with the dataset as is. Note that 36% of the songs are present in the ""Very Good"" category, with the least amount of them being ""Neutral"". This makes sense when you think about the nature of music and how songs bring emotions to the edges. Music rarely brings neutral emotions to the listener. It can be inspiring, or depressing. Songs that are neutral in emotion don't see the light of day. So, this seems like a realistic representation of songs, and we've decided to proceed with the categorization as is."
https://github.com/periclesrocha/CourseProject	progress_report.md	"CS 410 - Course Project Proposal - Fall 2021 Pericles Rocha (procha2@illinois.edu) - team leader/coordinator Gunther Correa Bacellar (gunther6@illinois.edu) ""My Kind of Music"" This document provides a status update on project ""My Kind of Music"". Progress When we first started this project, we knew we would need a database of song lyrics. We eventually found a database of almost 3000 songs hosted by an open source project called The Open Lyrics Database Project (https://github.com/Lyrics/lyrics). Every other solution in the industry requires some sort of paid membership, so for the learning purposes of this project, this open-sourced database will satisfy our requirements. Therefore, we've chosen to work with this database. When working with this database, however, we discovered that approximately 20% of the lyrics in this database were on languages different than English. We also discovered that some lyrics files were empty, containing only some song metadata, without containing any actual lyrics. As a result, we decided to implement some data cleaning routines to ensure we'd work only with quality data, and specifically, with song lyrics written in the English language. To achieve our data quality goals, we focused on three main pillars: - Metadata removal: the Open Lyrics Database Project standardizes how lyrics are submitted to them by requiring the insertion of metadata in the bottom of the lyrics files. This metadata includes the artist's name, the name of the album where this song was released, and additional, optional information such as the release year and a genre. This metadata starts with a sequence of underscores (""___"") in the bottom of the file, with subsequent lines that add the metadata in question. Our routine iterates through the lyrics files and discards anything starting from the line with the sequence of underscores. This allows us to use only the actual song content for sentiment analysis. - Language detection: as mentioned, roughly 20% of the lyrics in the database are written on languages different from English. To mitigate this, we implemented a routine that uses the TextBlob library (https://textblob.readthedocs.io/en/dev/) to detect the language a song uses. We then discard any songs whose lyrics are not written in English. - Detailed logging: our song classification script logs almost all operations done by the program. Amongst the information that it logs, we included the actual number of songs found on the source database, the number of songs that were found to be in English and were categorized, the number of non-English songs and their names (so that we could manually verify if this detection was accurate), how many errors occurred, and how many songs were classified per each mood (this process is described below). Logs are written in the ./logs folder. As a reminder, our program recommends songs to users based on their desired ""mood"" and some key words. So, if an user would like to hear songs in a bad mood and that contain certain keywords in its lyrics (e.g.: ""mad"", ""love""), the system will recommend songs whose lyrics are classified in a ""bad mood"" and that contains words relevant to the ones desired by the user. Although the song search mechanism is a ""live"" one that resolves queries while users submit them, song categorization is a batch process that happens in advance. We scan all songs in the source database and categorize them in one of five desired moods (1-Very Bad, 2-Bad, 3-Neutral, 4-Good, 5-Very Good) using sentiment analysis. The data cleaning process that we implemented allows us to start the categorization process with confidence, knowing that it will only contain the actual lyrics (without the metadata on the files), and knowing that we're only using songs in English. For the actual song categorization, this demands clarification, given some of the comments received from our project submission. We're using the Valence Aware Dictionary for sEntiment Reasoning (VADER) open-source package within the Natural Language Toolkit (NLTK). The VADER package is widely used in the industry for sentiment analysis, so training a model of our own with only 3000 songs would make a time-consuming exercise with little to zero benefits. Therefore, we decided to utilize VADER to score our songs, as opposed to implementing our own sentiment analysis algorithm and train or own models. To get the sentiment for songs, our program simply scans through the whole cleaned dataset and gets the polarity scores for each song. This returns a vector that contains the amount of positive, neutral, and negative sentiments in the lyrics, as well as a ""compound"" attribute which provides an average score between -1 and 1, where -1 represents very bad, or negative sentiment and 1 is very good, or positive. Songs are evenly categorized using the compound score as follows: - ""1-Very Bad"" when compound < -0.6 - ""2-Bad"" when compound >= -0.6 and < -0.2 - ""3-Neutral"" when compound >= -0.2 and <= 0.2 - ""4-Good"" when compound > 0.2 and <= 0.6 - ""5-Very Good"" when compound > 0.6 When songs are categorized, the actual lyrics file is copied to a destination folder that aggregates all songs within a given mood. The search engine that looks for keywords later on uses all documents in a given ""mood"" folder to look for relevant documents. Remaining Tasks We've dedicated most of our effort thus far in data quality and sentiment analysis. During the month of November, we will focus in the following tasks: - Enhance the sentiment analysis algorithm. Today, the algorithm passes the whole song to the analyzer. We understand that songs can have nuances, so we'll experiment with other approaches, such as taking the sentiment of each sentence in a song and then computing the average. Also, currently we use tokenization and remove stop-words from songs before we run sentiment analysis, so we'll continue to experiment to understand if this is optimal (estimated 8h) - Implement the text retrieval engine to fetch relevant documents ins response to the user query (estimated 6h) - UX and documentation (estimated 12h) Challenges The main challenge we've perceived thus far with the sentiment analysis algorithm was that our dataset didn't turn out to be well balanced across the different ""moods"". When looking at 2,444 lyrics (all the lyrics in English), they were categorized the following way: - 1-Very bad: 1183 songs (49%) - 2-Bad: 142 songs (6%) - 3-Neutral: 105 songs (4%) - 4-Good: 129 songs (5%) - 5-Very good: 885 songs (36%) This means nearly half of songs are contained in one category: the ""Very Bad"" mood. We've pondered this issue and eventually decided to proceed with the dataset as is. Note that 36% of the songs are present in the ""Very Good"" category, with the least amount of them being ""Neutral"". This makes sense when you think about the nature of music and how songs bring emotions to the edges. Music rarely brings neutral emotions to the listener. It can be inspiring, or depressing. Songs that are neutral in emotion don't see the light of day. So, this seems like a realistic representation of songs, and we've decided to proceed with the categorization as is."
https://github.com/periclesrocha/CourseProject	README.md	"CS 410 - Course Project Proposal - Fall 2021 Pericles Rocha (procha2@illinois.edu) - team leader/coordinator Gunther Correa Bacellar (gunther6@illinois.edu) ""My Kind of Music"" Motivation Music is an important part of human culture. While it is often thought of as simple entertainment, it can also impact how an individual feels and affect their mood. In fact, music can even be used in therapy to help relieve pressure or certain feelings. Researchers have pondered the possible therapeutic and mood boosting benefits of music for centuries. Specifically, researchers at Durham University and the University of Jyvaskyla, Finland [^1] discovered that even sad music can lift your mood, while other studies suggest music can boost happiness and reduce anxiety. From the drumbeats of our ancient ancestors to today's unlimited streaming services, music is an integral part of the human experience. In some cases, users may seek to boost their current feeling, regardless of what that feeling is, and they seek for the right song to enhance what they are feeling. Our proposal is to build a recommendation system that will suggest songs to a user based on their desired mood and a few keywords. We will ask the user to provide what their desired mood is from a 1-5 scale (where 1 is ""very sad"" and 5 is ""very happy""), a few key words, and suggest songs that match that sentiment and those keywords. This project falls into the ""Free Topic"" theme. Method We have obtained a database of song lyrics from a diverse set of music genres. Our application will determine the mood of each song using sentiment analysis on each of the song's lyrics, and build a sentiment index. This sentiment will be a 1-5 scale, where 1 represents negative sentiment and 5 represents positive. Based on the sentiment index, we will use text retrieval techniques to find songs that match the terms provided by the user, within the songs that match the desired mood. Finally, we will provide suggested song names to the user, ordered by relevance, based on the content of the song lyrics. Songs will be indexed for sentiment and key words based on their lyrics only. We will not analyze melodies or any audio content. This application will be limited to the English language. We will use Python as our programming language. Milestones The following are the steps and key milestones for this project: Task | Time needed | Completion data ---- | ----------- |---------------- Build the songs database with lyrics | 2 hours | Oct 30 Build the sentiment analysis algorithm and document indexes | 12 hours | Nov 8 Progress report | 2 hours | Nov 15 Build search engine to pick relevant song lyrics | 12 hours | Nov 22 Build user experience and app interface | 8 hours | Nov 29 Software code submission with documentation | 8 hours | Dec 9 Evaluation We will use roughly 3000 song lyrics for this project. Each song has its own characteristic in terms of what mood they convey in their lyrics. This is highly abstract and requires manual labor for testing and validation, so we will manually perform and document 300 tests (10% of the sample) to ensure the sentiment predicted by the algorithm is close enough to what the song lyrics are saying. Having tested and adjusted sentiment analysis, we will verify the relevance of the search engine using user queries and determine if the results are relevant or not. We will fine tune our algorithm until the index is optimal and provides relevant documents (in this case, song lyrics). [^1]:Research reveals pain and pleasure of sad music - Durham University (https://www.dur.ac.uk/news/newsitem/?itemno=28329)"
https://github.com/bayarra/spacy.lang.mn	Progress report.pdf	Progress report Build Mongolian language package compatible with the Spacy NLP toolkit as alpha support model 1. Research about Spacy 2. Build stop-words dataset for Mongolian language 3. Build some basic lexical attributes for Mongolian language 4. Build some punctuations 5. Build some basic word tokenization exceptions for Mongolian language Completed * Forked the spaCy repo from github * Added the folder and specific required files for Mongolian language package into my repo * Created some example sentences for Mongolian language to test spaCy and its language model * Created the lex_attr.py file which can detect numerical words for Mongolian language * Created punctuation.py * Added initial version of Mongolian stop-words list using an existing github source * Created tokenizer_exceptions.py which created some initial versions of abbreviations for Mongolian language, for example weekdays abbr. etc. Pending * Compile the spaCy repo * Create test cases for Mongolian language model * Improve the stop words * Improve the abbreviations and punctuations * Test changes * Create the running version for Mongolian language model compatible with spaCy * Commit to the spaCy community * spaCy website changes if the change published Challenges * Compiling the whole spaCy repo * Mongolian stop words * Abbreviations and exceptions words in Mongolian languages * Test changes * Push to spaCy * Running version of my model
https://github.com/bayarra/spacy.lang.mn	Project Proposal.pdf	Theme 3.4 System Extension. Build Mongolian language package compatible with the Spacy NLP toolkit as alpha support model Goal: Build Mongolian language package compatible with Spacy NLP toolkit as alpha support. Create it as open-source project. Try to contribute and integrate it with the Spacy toolkit extensions list. Research: Spacy toolkit extensions. Look and build the stop-words dataset for Mongolian language. Build some basic lexical attributes, punctuations using regex, build some basic tokenization exceptions for Mongolian language. Requirements 1. Team: Single person because it's related to a specific foreign language - Mongolian language, not sure any other student knows Mongolian language 1. Name: Bayar Demberel 2. NetID: bd12@illinois.edu 3. Captain: Bayar Demberel 2. Chosen system: Spacy NLP toolkit. 1. Subtopic: Adding models for new languages 3. Build the stop words dataset for Mongolian language, build some basic lexical attributes, punctuations using regex, build some basic tokenization exceptions. 4. Once built the Mongolian language extension, people can use it with the Spacy toolkit to process texts on Mongolian language. Initial model will be able to tokenize given text with Spacy toolkit. The Spacy toolkit does not have included the Mongolian language yet, so I excited to decide to work on this. 5. Once if it successfully contributed to Spacy extensions, any developers could use it with Spacy toolkit for NLP for Mongolian language. If it could not yet able to contribute it to Spacy community, it will be still showing as publicly and developers may can download and use this package. 6. Language to use: Python 7. Total estimated hours for this extension would be at least 40 hours. I am really wanted to do this, so I will spend as much time as I can. 1. Research about Spacy 2. Build stop-words dataset for Mongolian language - May need to build some web crawlers to collect large amount of text and create the stop-words list. 3. Build some basic lexical attributes for Mongolian language 4. Build some punctuations 5. Build some basic word tokenization exceptions for Mongolian language The whole project will be publicly contributed as open-source community, so any developers can improve the system, it is open to anybody wanted to contribute.
https://github.com/bayarra/spacy.lang.mn	README.md	Build Mongolian language package compatible with the Spacy NLP toolkit as alpha support model Goal: Build Mongolian language package compatible with Spacy NLP toolkit as alpha support. Create it as open-source project. Try to contribute and integrate it with the Spacy toolkit extensions list. Research: Spacy toolkit extensions. Look and build the stop-words dataset for Mongolian language. Build some basic lexical attributes, punctuations using regex, build some basic tokenization exceptions for Mongolian language. Research about Spacy Build stop-words dataset for Mongolian language - May need to build some web crawlers to collect large amount of text and create the stop-words list. Build some basic lexical attributes for Mongolian language Build some punctuations Build some basic word tokenization exceptions for Mongolian language The whole project will be publicly contributed as open-source community, so any developers can improve the system, it is open to anybody wanted to contribute.
https://github.com/naviCh/CourseProject	CS410 Final Project Progress Report.pdf	CS410 Final Project Progress Report: Reddit text data curation and sentiment analysis Topic: Leaderboard Competition: Data Set Creation Team Members: Ivan Cheung (icheung2@illinois.edu) - Leader, Jeff Zhan (zhan35@illinois.edu), Austin Wang(austinw7@illinois.edu) What tasks have been completed? Written to grab submissions and corresponding comments in batch, the web crawler efficiently fetches the metadata necessary for annotations. Based on exploratory analysis, several functions were written to remove irrelevant submissions. For instance submissions with only one upvote were removed as they had no user interaction and generally had no comments. The top n comments and submissions were utilized to find not only conversational ones but also highly ones that were highly interacted with. After the data was crawled, a basic parser program had to be written to grab the data into a workable format. The parser fetched about 1200 posts with a minimum upvote count of 10, along with the top 10 comments, link, upvote/downvote ratio, and headline. This data will be part of the final submission to show that a data set has been created as per the project topic. We will also feed this data set into our sentiment analysis program that is part of the pending tasks. What tasks are still pending? There are about 1200 rows of data in our data set, and we are in the progress of manually annotating them. This will serve as our optimal results that we can compare our sentiment analysis program results with. Each of us will have to annotate all the rows to minimize one's personal bias on whether the news is positive or not. We estimate it will take almost a week for us to go through the entire data set. In addition, we still have to create the NLTK sentiment analysis tool that will read the data and provide baseline results. This will also include parameter tuning to achieve higher accuracy. We estimate the code, with fine tuning, will take a week of work. Finally, as mentioned in our proposal, a stretch goal will be to also create a TextBlob algorithm and compare the results of it to the NLTK one. Challenges: Currently, there are no major challenges or roadblocks with the project. With that said, however, there is still room for improvement. Firstly, we would like to expand our data set to include additional subreddits such as r/usnews and r/finance. This would greatly enhance the potential use cases and give a more holistic view. If time permits, we would additionally like to improve the baseline model, utilizing feature selection and various Natural Language Processing techniques.
https://github.com/naviCh/CourseProject	CS410 Final Project Proposal.pdf	CS410 Final Project Proposal: Reddit text data curation and sentiment analysis Topic: Leaderboard Competition: Data Set Creation Team Members: Ivan Cheung (icheung2@illinois.edu) - Leader, Jeff Zhan (zhan35@illinois.edu), Austin Wang(austinw7@illinois.edu) Background and motivation: Within the most recent years, there has been much research on sentiment analysis on various social media sites including Facebook and Twitter. Public sentiment is an integral portion for many different domains such as Marketing, Politics, Education, etc. Amongst current Reddit datasets, most fall under the domain of finance (e.g. a dataset on r/wallstreetbets). We propose to create a Reddit dataset with manual labels specifically geared towards news (data from r/worldnews and r/usnews) and create a baseline sentiment analysis algorithm. By hosting both as a competition task, we can leverage LiveDataLab to run various experiments ultimately pushing the accuracy of our predictions. The dataset can be used to help anticipate public sentiment on ensuing novel news topics while the baseline model serves as a benchmark for future work. Steps: In order to proceed with this project, there are several things we need to do. First, we need to choose at least one subreddit, and possibly multiple with a similar theme that can make a coherent data set. Then, we need to compile and scrape the data. In this step, there are several parameters for us to decide on, such as how much data we should scrape, what time period to use, as well as which comments in the articles we choose to keep or omit. Once we have compiled our data set, we need to manually judge and label the data. Finally, we will build a baseline sentiment analysis that other submissions will be compared against. Technical Implementation: There are a variety of techniques and libraries that will be needed to be used for this project. The main language is Python 3, as it contains a lot of support for web crawling and also for sentiment analysis. A basic web crawler will be written to extract the top N threads of a popular subreddit. The scraped data will have the necessary text data for sentiment analysis, and we will also add a section for manual judgement. The team will split the articles into three parts and manually analyze each for a positive, negative or neutral sentiment. A parser will be written to analyze each article's headline and contents. The parser will also be used to extract the thread's comments, as well as establish upvote/downvote thresholds which determine if the comment will be put in the dataset for text analysis. In addition to providing the base data set for the leaderboard competition, the team will also establish two baselines for submission acceptance. Python contains a variety of sentiment analysis and NLP libraries. The most popular one is NLTK, but a strong alternative is the TextBlob library. The team will use NLTK due to its tendency to work better with labelled data, which our team will provide via manual analysis. A stretch goal will be to use TextBlob and do a comparison on the accuracy of the two models. Schedule: In terms of a timeline, we aim to have a working scraper by October 31, which we can run on the subreddit(s) we choose. Then, we will scrape the data and start manually labeling the data. We aim to have this step done by November 15, when we need to submit the progress report. After that, we will build our baseline sentiment analysis, and depending on how much time we have left over, we can fine-tune the baseline or scrape and label additional data.
https://github.com/naviCh/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Installation From project root directory, run: bash pip install -r requirements.txt Setup Secrets Go to https://www.reddit.com/prefs/apps/ to get a client_id,client_secret, and user_agent. The file secrets should contain key-value pairs of the format key=value and should be placed in the project root directory. The secrets needed are: client_id - Client ID for personal script use client_secret - Secret user_agent - Reddit Username
https://github.com/dabadabadoue/CourseProject	CS 410 Project Progress Report.pdf	The tasks that has been completed: * Given a url, it can determine whether the webpage is a faculty webpage * This is done by checking various attributes on the webpage, and then if the attributes match up, then the page is a faculty page * The urls for the bios of the faculty on the webpage are saved to a txt file The tasks that are still pending: * Making this work on a large variety of webpages, as it is currently only tested on 1 webpage * General functionality exists, so just adapting to other types of pages and checking for other kinds of attributes will allow this to be done on other pages * Ensuring that all bio urls on a page are found, and that none of the urls are links to pages that are not a faculty bio * Write tests to verify the correctness of the results A challenge we are facing is the broad range of webpages that exist and all of the different ways we have to account for them.
https://github.com/dabadabadoue/CourseProject	CS 410 Project Proposal.pdf	1. Our names are Daniel Abdoue and Karthik Talluri, and our NetIDs are dabdoue2 and talluri4, respectively. The captain of the team is Daniel. 2. The system we will be improving is ExpertSearch, and the subtopic is automatically crawling faculty webpages. 3. We plan on accessing all the faculty webpages available by using a strategy similar to that in MP2, except on a much larger scale. We also plan on using various APIs to create the automation necessary for the project idea to be successful. In addition, we will introduce various ranking functions to ensure the project will come out to its best possible form. 4. We will demonstrate that our functionality works as expected by providing test cases of what would be found manually as well as what our code is capable of doing, and comparing the results. Given a page that we have not yet tested, we will get all the necessary URLs using our code, then do the same using a manual method, showing that not only are the results the same, but the time required to get the URLs will be significantly less. 5. The code will take in an input, say, the URL to the desired university to be scraped, and output a file with the faculty directory page, as well as all the faculty webpages that were found. This could also in theory be used to auto-populate the google doc that was used to hold the URLs we had to manually find. 6. We will be using python as our coding language as it provides an easy way to both analyze text, as well as crawl webpages, and automate these processes. 7. The main reason this project will take at least 40 hours is because of the wide variety of webpages our code has to work for. Being able to automatically crawl webpages is a time consuming task on its own, but having to account for all the various ways a website could be created and set up will require a lot of testing as well as different implementations. There are 2 main steps that need to be done it both sections of our task. The first is to try and find URLs or links on a webpage that lead to the page we are looking for, and the next step is to identity whether where we ended up is indeed what we wanted. These two steps have to be done for both faculty directory pages and for faculty webpages. The total time to complete the first step, including all testing to ensure what we are doing works on a variety of webpages, will be around 20 hours, including directory webpages and faculty webpages. Another 20 hours will be for step 2, as this step is quite similar to step 1, however, having solved step 1 does not necessarily make step 2 easier.
https://github.com/dabadabadoue/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/RBmint/CourseProject	progress_report.docx	1) Which tasks have been completed? As stated in the proposal, I've found the initial dataset used for the training. The CSV file is rather large and was downloaded from - https://www.kaggle.com/kazanova/sentiment140/download Also, I finished data pre-processing and cleaning, in the train_model.ipynb file. 2) Which tasks are pending? Connecting to the twitter API, train models, and analyze the data. 3) Are you facing any challenges? I'm currently working on training the models. I've faced some issues trying to connect to the twitter API but it is not a major problem. Other than that, everything is advancing as planned.
https://github.com/RBmint/CourseProject	project_proposal.docx	In your proposal, please answer the following questions: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Which programming language do you plan to use? Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 1. Name: Mingchong Zhai; NetID: mzhai4; Captain Only one student is in this team. 2. The topic is about sentiment analysis on tweets from Twitter. The task is to pass tweets to the model and predict the sentiment in the tweets. It is interesting because we can identify whether people are being positive or negative in their posts. The approach is to train models on a certain dataset containing tweets. Tools used will include jupyter notebook, pickle (to save trained models as a binary file), NTLK (for data pre-processing and cleaning), Tweepy (the twitter API) and some other python library. The expected outcome will be consisting of tweets and the predicted sentiment with a probability. We can manually check some tweets as well as the predicted sentiment and use different parameter in training to improve the accuracy. 3. Python. 4. Find the initial dataset used for training. (2-3h) Do data pre-processing and cleaning. (4-5h) Fetch new data from the twitter API. (4-6h) Build model(s) for analyzing the data. (6-10h) Analyze and print the data with some visualization. (2-3h)
https://github.com/RBmint/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Ben-Sutter/IntelligentBrowsingPaulBen	progrep.pdf	1. Which tasks have been completed? The frontend has been completed. We created our chrome extension with a very basic ui for users to input their query to search the page by. 2. Which tasks are pending? Connecting frontend and backend. This includes utilizing BM25 with our extension as well as finding a web scraping library to parse the page's text. 3. Are you facing any challenges? We are struggling to connect the extension with the webpage we are on. We are very new to web development so the main struggles tend to be with working with the extension and how the project will look in general.
https://github.com/Ben-Sutter/IntelligentBrowsingPaulBen	Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. pkim62, bsutter2, bsutter2 is the captain! 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? We chose the Intelligent Browsing topic and we have chosen to create a browser extension to allow you to search based on concept rather than exact words. The current system requires exact spelling and words for searching using ""ctrl-f"" but we would like to allow synonyms to be recognized together as one concept for searching. This uses text mining and analysis to provide easier searching. 3. Briefly describe any datasets, algorithms or techniques you plan to use We plan on using a common retrieval function like BM25 to index a document to improve the searches over a particular page. We may need a dataset to provide certain information for lexical analysis information. 4. How will you demonstrate that your approach will work as expected? We will show that our approach works as expected by illustrating a better understanding of core concepts in the course and providing a functional product that is an improvement of current extensions in chrome. The approach should increase precision of the ctrl-f function in chrome and reduce the number of useless words. 5. Which programming language do you plan to use? Python 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. With only two members of our group, this project will likely take well over the given 40 hours we should spend. We must learn how to create chrome extensions, create a program that uses text mining and analytical algorithms along with pooling algorithms to group common terms."
https://github.com/Ben-Sutter/IntelligentBrowsingPaulBen	README.md	CourseProject Proposal.pdf is our proposal
https://github.com/aruther11/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jaskirat-singh-pahwa/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/awang288/CourseProject	Project Progress Report.pdf	Amanda Wang, Jennifer Jasperse, Arely Alcantara (NetIDs: afw3, jbj3, arely2) Team: Push Mode People Progress Report Since writing our proposal, we have been busy at work building out our book recommender system using the GoodReads datasets. We have also been trying to document our code as we go using Jupyter notebooks. Based on the feedback that we received for our proposal, we realize that building a hybrid model is too much given the scope and time for this project, so we will not be implementing it. Below we will list our progress, remaining tasks, and challenges faced. Tasks in progress: * We started out by cleaning out the books and reviews dataset - focusing on English entries only. This took a bit longer than the anticipated 2 hours because each of us designed our own data-cleaning methods and due to the volume of processing required. We have completed preliminary cleaning on subsets of the data that are easier to work with and the full datasets - 15M reviews and 2.36M books. * We have researched and implemented a very simple content-based recommender where we take a book and generate the top 7 related books to recommend - we're hoping to flesh this out and incorporate some user data. * We were able to research, build, and evaluate a preliminary user-based collaborative filtering recommender system that uses a KNN model. This already took approximately 20 hours. Tasks Remaining: * Research and implement a web search query (Task 1) * Evaluate Task 1 * Test, debug, and revise code for Task 1 * Evaluate Task 2 * Test, debug, and revise code for Task 2 * Demo/final presentation Challenges: * A major challenge faced while designing the collaborative filtering model was the limitation on the size of data we could work with. Because we are using Jupyter notebooks, there is a size limit for the user-similarity matrix. As a workaround, we are currently just using datasets of ratings for specific genres, specifically poetry, which has fewer ratings and users [154,555 reviews and 36,636 users]. We will try to capture the user similarity information for the larger datasets using sparse matrices instead. * Someone also mentioned in the peer reviews whether we are using the entire dataset or just a subset so we are still debating on the best approach - we were also wondering if the dataset must be in the repo to facilitate setup and testing for the reviewers later on as we have had technical difficulties in pushing the full datasets to our repository. The full datasets are also much more difficult to train efficiently. Question: * Is the technical documentation supposed to be separate from the code or is it ok if we use Jupyter notebooks for the code + documentation blended together?
https://github.com/awang288/CourseProject	Proposal.pdf	"Names: Amanda Wang, Jennifer Jasperse, Arely Alcantara Team Name: Push Mode People Team Captain: Amanda Wang Emails: afw3@illinois.edu , jbj3@illinois.edu , arely2@illinois.edu NetIDs: afw3, jbj3, arely2 Project Proposal What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? For our final project, we have selected the 'free topic' theme and we're planning on doing a book recommendation system! Netflix, Spotify, and Tiktok exist for movie/music/social media lovers but what about a book recommender for book lovers? Can you imagine getting a personalized list of books you may like? And even having the ability to search a topic and get the perfect book to satisfy your needs? Well, we've got you covered! We plan on using a Goodreads book dataset to help recommend books to different users on the site. The data itself will be coming from this website, so we won't be scraping the website for information: https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home We found this topic interesting because there is a lot of information about books contained in reviews. It would be great to be able to harness that knowledge and use it to build a recommender system for users. We plan to do two main tasks in this project. First, we would like to be able to recommend books based on user input. For example, a user might input that they're interested in reading ""books with complex magic systems,"" and then we would output a list of books that they might be interested in. We plan on using the content we learned relating to ""web search queries"" for this part of the project. Our second main task is to build two recommender systems: one using content-based filtering, and one using collaborative filtering. Ideally, given that we have extra time, we would love to implement a hybrid model - one that combines both techniques. The books dataset currently has a property called 'similar_books', but we are unsure of where/how these recommendations were determined so we will ignore this portion of the data and build our own content-based approach as well as collaborative filtering approach. We could use book reviews to determine a list of topics each book covers (using the Probabilistic Topic Models), and then use this information along with book metadata and other users' ratings to recommend books to users. We are not yet sure how we will combine the two recommendation techniques, but we will build both systems separately first, and then read up on different ways to combine them (if time allows). The expected outcome would be one ""book search engine"" that can recommend books based on a simple query, and a recommender system that will recommend books to different users. We will evaluate the first task by manually labeling the output of our data, similar to MP 2.3. For evaluating our recommender systems, we will temporally segment each user's data into training and testing sets. We will evaluate our recommender systems based on how many of the books in the testing set we were able to accurately recommend based on the user's reading history in the training set. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. - Cleaning data, specifically removing reviews that aren't in English (2 hrs) - Research how to implement a web search query in Python (5 hrs) - Implement Task 1 (recommending books based on user inputted phrase) (15 hrs) - Test, debug, and revise code for Task 1 (5 hrs) - Evaluate Task 1 (5 hrs) - Research how to build content-based and collaborative filtering recommender systems (10 hrs) - Develop solution to cold-start problem for collaborative filtering (5 hrs) - Implement Task 2 - build 2 recommender systems: - One content-based (10 hrs) - One collaborative filtering (10 hrs) - Optional: one hybrid model (10 hrs) - Test and evaluate code for Task 2 (recommender systems) (10 hrs) - Develop demo and final documentation (15 hrs)"
https://github.com/awang288/CourseProject	README.md	"Book Recommender using GoodReads data By: Amanda Wang, Jennifer Jasperse, and Arely Alcantara Overview For our final project, we have decided to use this Goodreads dataset to build a book recommender! We realized that recommenders exist for movies, music, food, etc. and wanted to explore more on text information systems and books offer an incredible chance for us to experiment! We implemented 2 tasks: - Task 1: recommending books based on user inputted phrase - Task 2: building a content-based and collaborative-filtering recommender Proposal Our initial proposal for the project is found here Progress Report Our progress report is available here References: Mengting Wan, Julian McAuley, ""Item Recommendation on Monotonic Behavior Chains"", in RecSys'18. Mengting Wan, Rishabh Misra, Ndapa Nakashole, Julian McAuley, ""Fine-Grained Spoiler Detection from Large-Scale Review Corpora"", in ACL'19."
https://github.com/qiangni-gif/CourseProject	Progress Report.pdf	Project Progress Report CS-410 / Fall 2021 JunyangWang Which tasks have been completed? 1. Set up Environment, made requirements.txt file and installed compatible libraries. 2. Data Pre-Processing, removed stop words, removed Latin words. Tokenized words and removed punctuations. Performed POS tagging and extracted adjectives and nouns. Which tasks are pending? 1. To execute LDA and extract topics. 2. To perform sentiment analysis on each topic at a dataset level. 3. Code cleanup and Documentation. Are you facing any challenges? 1. Library incompatibilities with Python Version. 2. Had some trouble doing data cleaning implementation. Extracting meaningful words and Mapping lemmatization functions to every row in data frames 3. Facing some encoding error when work with datasets.
https://github.com/qiangni-gif/CourseProject	Project Proposal.pdf	Project Proposal CS-410 / Fall 2021 TripAdvisor JunyangWang 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Member: Junyang Wang (Jw111) as individual 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Topic Mining and Sentiment Analysis for Airline reviews. Why is it important: Customer satisfaction is always top of mind for airlines. Unhappy or disengaged customers naturally mean fewer passengers and less revenue. It's important that customers have an excellent experience every time they travel. Task: Using Topic Mining and Sentiment Analysis to determent customer satisfaction level for each airline company. Datasets: https://www.kaggle.com/efehandanisman/skytrax-airline-reviews https://github.com/quankiquanki/skytrax-reviews- dataset/blob/master/data/airline.csv Tools, Approach:  Complete the datasets pre-processing with Nltk library. Includes text tokenization, lowercasing, stop words removal and text stemming.  Topic Modeling with Gensim. Find the common words in the customer airline reviews.  Complete topic-based sentiment analysis.  Finally, we should able to monitor the airline brand reputation based on the varies topics that is positively or negatively credited by the reviewers. Outcome: My expected outcome is to show topic and sentiment level comparisons for each airline companies. Evaluate: I will compare the sentiment associated with the review to the given rating. 3. Which programming language do you plan to use? I will use python programming language for this project. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Dataset analyzing and preprocessing. - 4 hours Extract topics from preprocessed review dataset. -6 hours Design of sentiment analysis code. -6 hours Post processing and code cleaning. -5 hours
https://github.com/qiangni-gif/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/styloooo/CourseProject	CS410 Course Project Progress Report.pdf	CS410 Course Project Progress Report Due: 11/16/2021 Team: Panther Moderns * Tyler Davis (tadavis2@illinois.edu) (team leader) * Kee Dong (yuqingd2@illinois.edu) * Mukund Madhusudan (mukundm2@illinois.edu) Progress Made * Tasks decomposed and distributed * Architecture designed * Database schema defined * Django project initialized / configured Remaining Tasks * Implementation of scraping module * Implementation of index module * Implementation of retrieval module * Front-end design * Loose ends (connecting these modules, tests) Challenges/Issues Faced * Not sure what to do when encountering non-words in indexer (digits, symbols, etc.) * Initial solution: discard strings with non-alphabetic contents * Unclear how to implement dynamic scraper * Initial solution: Build based on CS410 MP scraper and expand functionality
https://github.com/styloooo/CourseProject	CS410 Project Proposal.pdf	CS410 Project Proposal Team Members Tyler Davis (tadavis2@illinois.edu) Kee Dong (yuqingd2@illinois.edu) Mukund Madhusudan (mukundm2@illinois.edu) What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? We propose a web application that enables a user to bookmark a document by entering a URL or a set of URLs (and a tag) to be indexed by the system into an inverted index for later full-text search and retrieval. Users can then enter a tag and a query and review the returned list of relevant documents and click through to the original URL. This tool would enable us to explore the implementation of an inverted index as well as test the functionality of various retrieval functions in a real-world context. Additionally, this tool would be useful to a user with research needs for compiling web documents for later review without having to document their contents manually. We plan to develop this tool as a web application that processes a documents' contents into an inverted index stored in a database. We will use the Django web server framework to implement this project and a MySQL database to store the inverted index. Asynchronous methods such as indexing will be implemented using the Celery library. We will evaluate our work by testing our system with a suite of tests against expected retrieval/indexing results. Which programming language do you plan to use? We will use the Python programming language and MySQL to implement the backend of this project. The frontend will include HTML/CSS and some JavaScript components. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 1. Architecture planning & data modeling (4 hours) 2. Dynamic web scraping module (10-15 hours) a. Ensuring the scraper can dynamically handle most URLs it is given will require some extra work compared to designing a scraper to handle a single page or a static set of pages. 3. Writing tests (4 hours) a. Each person should be responsible for writing tests for the module(s) they implement. 4. Front-end (10 hours) 5. Index module (creating the inverted index) (15 hours) a. None of us have any practical experience in constructing an inverted index, so this may take a more substantial amount of time compared to other web app components that we may previously have been exposed to in CS410 machine problems. 6. Retrieval module (15 hours) a. Similarly we don't have practical experience in text retrieval from an inverted index, which may add some additional complexity to the text retrieval module that needs to be accounted for in our estimate. 7. QA (10 hours) Total: 68-73 hours
https://github.com/styloooo/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Requirements Project built using Python 3.9.2 Django 3.2.9 Project Set Up Install PostgreSQL if you do not already have it. Enter the Postgres shell and create a new database called saveit: ```sql CREATE DATABASE saveit; ``` Clone this repo to where you will work on it: ```sh git clone http://this.repo/url ``` All of these commands are issued from the project's top directory that has manage.py in it. Create a virtual environment for the project (I highly recommend virtualenvwrapper). Configure your environmental variable to point to your local Postgres installation. Using virtualenvwrapper you need to open: $VIRTUAL_ENV/bin/postactivate and add the following line. Change psql_user and psql_password to your local PSQL install's username and password: sh export SAVEIT_DB_URL='postgres://psql_user:psql_password@localhost/saveit' Generate a Django secret key: ```sh python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())' ``` Save the secret key generated here to your environmental variables: sh export DJANGO_SECRET_KEY='secret-key-that-you-just-generated' Save the file and check that the environmental variable has been exported - if not, reopen your virtual environment and check again. Install the project dependencies and remember to do so every time you pull the repo. From the top project directory: ```sh pip install -r requirements.txt ``` If you run into permissions issues with any of the following commands, you need to change the permissions on each of these shell files to make them executable as such: ```sh chmod +x shell_file_name.sh ``` Finally, push the database migrations to Postgres: ```sh ./bin/makemigrations.sh ./bin/migrate.sh ``` Download the required NLTK corpora (you only need to do this once per configuration): sh ./bin/setup_nltk.sh Run the app to see if it launches without error: ```sh ./bin/runserver.sh ```
https://github.com/amyth18/CourseProject	CS410 Project Progress Report.pdf	CS410: Final Project Progress Report {ameetd2, purohit4}@illinois.edu The goal of our project is to build an enhanced gmail client that helps users manage the deluge of emails in their inbox in a much more efficient way by employing text mining techniques learnt in this course. Specifically, our goal is to mine dominant topics in the emails in a user's inbox and automatically categorize (assign labels) them mined topics. We have completed a detailed design of the solution, broken down the tasks among members of the team and started implementation of core modules of our solution. The following diagram captures the design of our solution. The data processing pipeline module will fetch emails using the Gmail API, extract text from the body and perform all the data cleaning operations like stemming, removing stop words etc and store the keywords in a .csv file (or a database). The topic modeling and analytics module will read this file and construct a vector for each email (treated as a document) and perform topic modelling using the gensim library. It will store the discovered topics for each email in a .csv file (or database). The email client WeApp will display an enhanced view of the user's inbox by grouping emails by topics and show a summarized view that will help users read mails that are of most interest to them. Implementation Status The detailed task breakdown and their current status is captured in the following table. Task Owner Status Data Pipeline Implementation OAuth 2.0 authorization workflow for accessing GMail ameetd2 Completed Retrieve mail from GMail Service ameetd2 Completed Text extraction from mail ameetd2 In Progress Data cleaning and curation ameetd2 Not Started Storing data ameetd2 Not Started Text Analytics Engine Feature engineering purohit4 In Progress Topic model Implementation purohit4 Not started Storing results purohit4 Not started Front End (Visualization) Visualization of topic Insights in Tableau purohit4 Not Started WebApp for enhanced inbox view ameetd2 Not Started Challenges 1. We faced some challenges in getting the OAuth 2.0 authorization flow working to access a user's Gmail inbox due mostly to the multiple ways of accomplishing this (e.g. front end vs. backend based implementation) and some of the methods being deprecated by Google Identity service. After a lot of wading through the Google Identity service we were finally able to get the access token to access a user's inbox. 2. We are also facing some challenges in extracting text from emails (due to various mime types). We may restrict our solution to emails of certain mime types only if we are not able to resolve this in time for your submission.
https://github.com/amyth18/CourseProject	CS410 Project Proposal.pdf	"CS410: Final Project Proposal Fall 2021 Topic [Theme5: Free Topics] Gmail Assistant: An intelligent bot to help users navigate a deluge of emails in their inbox. Team Name NetID Ameet Deulgaonkar (captain) ameetd2 Praveen Purohit purohit4 Problem For an individual, emails continue to be an important mode of ""text"" based communication in spite of the arrival of several modern communication platforms like social media (twitter, facebook), slack or WhatsApp especially when it comes to point to point communication (B2C or C2C). The size of an individual's inbox is growing more than ever. It is likely that we are receiving 10 times more emails than we were 5 years ago, but most email services do not provide any features for users to cope with this deluge of email. It is really hard for users to separate useful information (needle) from noise (haystack) using the basic key-word based search capabilities that most email services provide. Solution Our proposal is to use probabilistic topic modelling (either LDA or PLSA) and mine the topics in an user's inbox (e.g. finance, travel, promotions, newsletters, invitations etc.) and topic distributions in the emails and organize the emails using this meta-data. Using this information we intend to solve the following key use cases. 1. Automatically organize emails into categories such that the user can quickly locate emails of interest instead of scrolling the emails monotonically in chronological order or using naive keyword based search. 2. [STRETCH GOAL] A more intelligent search that uses a ""combination"" of state of the art ranking functions like BM25 and the mined topic information. We intend to first apply BM25 to obtain ranked emails and then improve the results by matching the topic(s) of the query (when possible) to the topic(s) of the email. Implementation We intend to implement the bot as a service (possibly hosted in the cloud) which downloads the user's emails and performs the text mining as mentioned in the solution section. The bot then stores the mined medata (i.e. topics and topic coverage) in a database. The bot will offer a simple WebUI for the users to register their gmail account credentials and get a ""more"" ""organized"" view of their inbox. An ideal way to build this front end would have been to have it either as a gmail plugin or a chrome extension, but we chose this approach due to paucity of time. Here is the high level architecture of our bot : Components 1. Backend will be built in python using metapy (search and index) and gensim (for topic modelling). 2. Frontend will be either a WebApp (e.g. react.js app) or any visualization tool like PowerBI or Tableau to view the topic modelling results. Work Estimate Task Estimate Backend: data pre-processing and storing 16 hours Backend: topic modelling 16 hours Backend: search and index [STRETCH GOAL] 8 hours Backend: APIs 8 hours Frontend: Organized email view 16 hours Frontend: Basic email list view 8 hours"
https://github.com/amyth18/CourseProject	README.md	Project Proposal Detailed project proposal can be found in Project Proposal.pdf Progress Report Progress report can be found at CS410 Project Progress Report.pdf
https://github.com/kaf4/CourseProject	Progress Report.pdf	"Team KAF4 Members: Captain Kristen Fisher [kaf4] Topic: Intelligent Learning Platform: ConceptView Goal: Extract concepts taught from lectures in to build a ""concept summary"" for each lecture. If concepts from lecture materials can be extracted intelligently, users can efficiently retrieve lectures containing relevant information based on these generated concept summaries. Progress Report At this point, I have extracted the text data needed for this project. This includes both transcripts as well as the text on the slides from all the lectures available on Coursera. I have cleaned the data to remove delimiters and other strange characters to create usable datasets. With this data I have been testing the extraction of n-grams and building a robust list of stop words. For evaluation I am using the provided ConceptView data to ensure the terms I have been extracting seem to include appropriate concepts covered in the lecture. Initially, I was only planning to use the transcripts, but I found including text from the slides seems to improve results in my testing. This week I plan to spend time researching additional topic modeling tools and techniques I can incorporate beyond the scope of the basic stuff I have been doing with MeTA during dataset testing. After that, the next stages will be testing out some of the tools I am researching, figuring out a way to build the concepts after filtering and/or ranking the terms, and then lastly generating the final summary from those results. I have not faced any significant challenges yet, but I think rebuilding a coherent summary from basically a bag of words might be difficult with limited human effort. However, this is something I plan to spend some time researching as well."
https://github.com/kaf4/CourseProject	Project Proposal.pdf	Team KAF4 Members: Captain Kristen Fisher [kaf4] Topic: Intelligent Learning Platform: ConceptView Goal: Extract concepts taught from lectures in to build a concept summary for each lecture. If concepts from lecture materials can be extracted intelligently, users can efficiently retrieve lectures containing relevant information based on these generated concept summaries. For this project, text data will come from CS410 lecture materials. Concept summaries built from the text data will be generated using techniques from this course including, but not limited to, text mining analysis with MeTA and topic modeling (PSLA/LDA). I anticipate all of the code for this project will be written in Python, and the ConceptView data provided in the CS410 Project Topics document will be used in the evaluation of generated concept summaries. Working alone, I plan to spend at least 20 hours on this project: * 3 HR: extracting and organizing text data from lectures into usable datasets * 2 HR: researching additional techniques, tools, algorithms, etc. * 10+ HR: extracting terms, topic modeling, compiling results to generate summaries * 5+ HR: troubleshooting, debugging, tuning, testing, documentation, etc.
https://github.com/kaf4/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/amelia2801/CourseProject	CS410 Project Progress Report.pdf	CS410 Fall21 Project Progress Report Topic: IMDb Actors Search Based on Movie Topics Team: Chris Nolan's Fans Club Progress Below is the progress made in this project: - [Done] Download and understand IMDb data sets We use the following datasets for this project: https://datasets.imdbws.com/name.basics.tsv.gz The dataset contains information that we will use throughout the project such as: nconst, primaryName, birthYear, primaryProfession, and knownForTitles. - [Done] Data scraping Currently we filter out the datasets with certain parameters (e.g birthYear > 1960, primaryProfession is actor/actress, and knownForTitles count > 3). This results in 771,701 actors. For the first step, we only used 2,500 actors' data (exported to file actors.csv). Based on this actors data, we scrape each actor's movie plot summaries from the IMDb website. It took around 2 hours to scrape 2500 actors' movie plot summaries. - [Done] Create document list Each scraped plot summary is written to a file that becomes the document. The document list consists of files of plot summaries. These documents can be found in the movieFile/ folder. - [Done] Associate each actor to a list of documents (movie plots) Create a single document for each actor that contains a list of the actor's movies' plot summaries. This document will be used to show a list of movies of the corresponding actor in the search results. These documents can be found in the actorFile/ folder. - [In progress] Implement/tweak BM25 search algorithm We currently use BM25 search algorithm from https://github.com/dorianbrown/rank_bm25 1/2 CS410 Fall21 Project Progress Report - [In progress] Web interfaces We have displayed the working solution in a simple web interface. Remaining Tasks - Improve the search algorithm. The current search functionality does not include stop-words removal and stemming. This will be added to improve its performance. - Improve the website performance. Pre-prepare display fields. Explore options to use cache. - Scrape more actors data (probably total of 10,000 actors) to have a bigger corpus. - Store the query frequency to a file. - Testing and evaluation by relevance judgement. - Project demo and documentation. Challenges/Issues - Takes time to scrape the data. - Plot summary missing for some of the movies - Web page performance issues. 2/2
https://github.com/amelia2801/CourseProject	CS410 Project Proposal.pdf	"CS410 Fa21 Project Proposal Team Members Team name: Chris Nolan's Fans Club Below is the list of the team members in this project. Name NetID Anasthasia Amelia Sugiharso* aas13 Aravind Pillai pillai5 Mike Zhou mikez2 * team captain Topic Our team chooses the Free Topics theme for the CS410 Course Project. The topic is scraping the IMDb website to capture the types of films individual actors act in and allow searching for actors by arbitrary topics and showing a list of movies matching those topics. For example, searching for the phrase ""time travel"" should return a list of actors, directors and producers who have been in movies related to the topic of ""time travel"". It is an interesting task because the existing IMDb website does not support the capability of searching for actors, directors, or producers based on the types of movies they have been involved in. The planned approach is to use the IMDb basic names dataset that we can get from https://www.imdb.com/interfaces/. Each entry is an actor, director or producer name and a list of the titles they have been involved in. We will use the titles to scrape the IMDb plot summary website for that title and build an inverted index for each title. Then the person's inverted index will be the sum of the indexes for the titles they have contributed to. We will use a search algorithm to power a search for actors based on user queries. The service will be hosted on a website and the inverted indexes stored in a database. The expected outcome is to create a website where users can search for actors, directors, or producers based on the content of the movies they have been involved in. The programming languages we are planning to use are including but not limited to Python and JavaScript. The work will be evaluated by relevance judgement by the group members. 1/2 Work Estimation Below are the breakdown of tasks that are needed to be completed in this project. Tasks Time (hours) Set up environment 3 Download and understand IMDB data sets 5 Scrape plot summary for each movie from IMDB web page and create document list. 15 Associate each actor to a list of documents (movie plots) 5 Create inverted index for the list of documents 10 Use BM25 search algorithm and rank the documents based on user search 15 Build the website user interface to display the results 10 Test the website 10 Report and Documentation 10 Total workload estimate 83 2/2"
https://github.com/amelia2801/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jpjun8/CourseProject	Instructions.docx	"Before completing these steps, please make sure that you have signed up for Microsoft CMT according to the directions listed in the Google doc. The following steps need to be completed ONCE by the Team Leader ONLY Fork the Course Project github repo by clicking on the ""Fork"" button on Github Your fork is the main place to submit all the course project submission, i.e. proposals, progress reports, code, final software documentation, demo presentations, etc. Naming: The project will be manually graded, so there are no specific names required as long as they are easily recognizable (or better yet, described in the Readme file). Format: The proposal, progress report, and final documentation can be a word/pdf document. Grading: The proposal, progress report, and final documentation will be peer-graded by all of you (with guidance from the instructors). We will later provide more details on the peer-grading process and reviewer functionality on CMT. 2. Log into Microsoft CMT and click on ""All Conferences"". Search for the conference named ""CS410Expo2021"" and click on the conference. 3. Click ""+ Create New Submission"" and select the track that you chose for your project. This should match your listed track in the Google sheets page. 4. Enter your Project Topic in the ""Title"" field. In the ""Abstract"" field, enter the URL of your fork created in step 1. 5. Add the emails of all the team members under authors. 6. Select the subject area that you feel best describes your project. 7. Finally, click on ""Submit"". All these steps need to be done ONLY ONCE for the course project. Just keep adding the relevant files to your Github fork for all the required submissions. Hope you enjoy the course project!!!"
https://github.com/jpjun8/CourseProject	Progress Report.docx	Progress Report Team MJ3 Team member: Joonpyo Jun (NetID: joonpyo3, only team member) Completed Tasks: Initialized the React application and named it course-project, which might be changed later to meet the concept of this project. This initial React application is compatible in both Web and Mobile environments. The development will be placed mostly under course-project/src directory and any necessary Node.js packages will be installed via command line with npm that updates package.json file automatically. Researched several University rankings dataset and popular fields of study in higher education institutions. cwurData.csv and timesData.csv will be used as main data files to list universities as they provide decent fields to rank institutions with, such as publications, citations, and quality of education, all in numerical values. Researched popular fields of study in higher education institutions dataset (all-ages.csv). Because this CSV file shows how many students are enrolled and employed after the graduation, it can be used to figure out which majors are popular in universities overall. Challenges: It was difficult to find a dataset that I wanted to use for this project: the one where universities are ranked for each major, e.g., Harvard University in 1st place for Law major but 5th place in Pharmaceutics major. Using Python in this project might turn the development more complex as it is not as compatible as JavaScript with React framework. So, as I planned in the proposal, the JavaScript will be mainly used in the development. Remaining Tasks: Front-end and Back-end development: Basic UI - A page where the user can select a major then the university rankings are displayed for the selected major. User Authentication - Using the Firebase Auth may be the best option to provide an easy yet strong authentication option. This is optional. Data retrieval and functionality implementations - Need to retrieve specific data from each dataset as there are multiple datasets, then it is necessary to combine them to facilitate the functionality implementations. Keep looking for the ways to provide more appealing and user-friendly interface. I believe that the user should be comfortable with the application rather than being overwhelmed by it.
https://github.com/jpjun8/CourseProject	Progress Report.pdf	Progress Report Team MJ3 Team member: Joonpyo Jun (NetID: joonpyo3, only team member) Completed Tasks:  Initialized the React application and named it course-project, which might be changed later to meet the concept of this project. This initial React application is compatible in both Web and Mobile environments. The development will be placed mostly under course-project/src directory and any necessary Node.js packages will be installed via command line with npm that updates package.json file automatically.  Researched several University rankings dataset and popular fields of study in higher education institutions. cwurData.csv and timesData.csv will be used as main data files to list universities as they provide decent fields to rank institutions with, such as publications, citations, and quality of education, all in numerical values.  Researched popular fields of study in higher education institutions dataset (all- ages.csv). Because this CSV file shows how many students are enrolled and employed after the graduation, it can be used to figure out which majors are popular in universities overall. Challenges:  It was difficult to find a dataset that I wanted to use for this project: the one where universities are ranked for each major, e.g., Harvard University in 1st place for Law major but 5th place in Pharmaceutics major.  Using Python in this project might turn the development more complex as it is not as compatible as JavaScript with React framework. So, as I planned in the proposal, the JavaScript will be mainly used in the development. Remaining Tasks:  Front-end and Back-end development:  Basic UI - A page where the user can select a major then the university rankings are displayed for the selected major.  User Authentication - Using the Firebase Auth may be the best option to provide an easy yet strong authentication option. This is optional.  Data retrieval and functionality implementations - Need to retrieve specific data from each dataset as there are multiple datasets, then it is necessary to combine them to facilitate the functionality implementations.  Keep looking for the ways to provide more appealing and user-friendly interface. I believe that the user should be comfortable with the application rather than being overwhelmed by it.
https://github.com/jpjun8/CourseProject	Project Proposal.docx	Project Proposal - Group MJ3 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Members: Joonpyo Jun (joonpyo3) Captain: Joonpyo Jun What is your free topic? Please give a detailed description. What is the task? The Free Topic that I chose is Education Recommender System for specific majors. In this project, I will create an application to recommend college/university or any educational institutions for specific major that each user selects, based on the ratings and reviews of institutions. Why is it important or interesting? Because the education nowadays is semi-mandatory for all students who want to make their dreams come true, selecting the best places to learn what they want to learn for their dreams is very important. What is your planned approach? I would start from researching popular majors in U.S. colleges then I will start to scrape the ratings and reviews (stars, out of 5, etc.) of each institution where researched majors are available. What tools, systems or datasets are involved? What is the expected outcome? I am planning to use JavaScript and React/Angular Framework along with Node.js. I may use some external database (Firebase) and some of Python to facilitate the development. Datasets, as stated above, will be scraped through the webpages that provide overall ratings and rankings of educational institutions. By the end of this project, I expect users to be able to see a list of institutions that provide the best education for the majors that they selected. How are you going to evaluate your work? Since it will be a development, there will be a lot of pre-builds and builds before the completion that I will review and self-assess each build to evaluate the application. Which programming language do you plan to use? As stated in 2., I will be using JavaScript, React/Angular Framework, and Node.js modules mainly, but external database (Firebase) and Python might also be used. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Setup (initialize React/Angular application and Firebase if needed): 3 hours Research Majors & Scrape Institutions' datasets: 5 hours Front-end: 2 hours Back-end: 5 hours Build, Review, and Optimize: 10 hours
https://github.com/jpjun8/CourseProject	Project Proposal.pdf	Project Proposal - Group MJ3 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Members: Joonpyo Jun (joonpyo3) Captain: Joonpyo Jun 2. What is your free topic? Please give a detailed description. What is the task? The Free Topic that I chose is Education Recommender System for specific majors. In this project, I will create an application to recommend college/university or any educational institutions for specific major that each user selects, based on the ratings and reviews of institutions. Why is it important or interesting? Because the education nowadays is semi-mandatory for all students who want to make their dreams come true, selecting the best places to learn what they want to learn for their dreams is very important. What is your planned approach? I would start from researching popular majors in U.S. colleges then I will start to scrape the ratings and reviews (stars, out of 5, etc.) of each institution where researched majors are available. What tools, systems or datasets are involved? What is the expected outcome? I am planning to use JavaScript and React/Angular Framework along with Node.js. I may use some external database (Firebase) and some of Python to facilitate the development. Datasets, as stated above, will be scraped through the webpages that provide overall ratings and rankings of educational institutions. By the end of this project, I expect users to be able to see a list of institutions that provide the best education for the majors that they selected. How are you going to evaluate your work? Since it will be a development, there will be a lot of pre-builds and builds before the completion that I will review and self-assess each build to evaluate the application. 3. Which programming language do you plan to use? As stated in 2., I will be using JavaScript, React/Angular Framework, and Node.js modules mainly, but external database (Firebase) and Python might also be used. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Setup (initialize React/Angular application and Firebase if needed): 3 hours Research Majors & Scrape Institutions' datasets: 5 hours Front-end: 2 hours Back-end: 5 hours Build, Review, and Optimize: 10 hours
https://github.com/jpjun8/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/gmotzespina/CourseProject	progress_report.md	Which tasks have been completed? [x] Chrome extension setup with typescript and react. [x] Chrome extension integration with PDF viewer. PDF.js Express [x] Perform search on PDF. Perform web scraping on the site. Perform word stemming and removing stopwords. Which tasks are pending? Perform web scraping on the site. Considering on using Cheerio or Puppeteer. Perform word stemming and removing stopwords. I'm considering on creating my own implementation for stopwords removal using NLTK's list of english stopwords Library considered for stemming: stemmer. Are you facing any challenges? Tried one Web Scrapping Library but it was not fully compatible with typescript and it was not maintained. Haven't found which Library to use for stemming and removing stopwords I need to find a way to dynamically choose the PDF file to render. Currently I have a hardcoded path to the PDF I'm using for teting.
https://github.com/gmotzespina/CourseProject	project_proposal.md	CS 410 - Final Project Proposal FALL SEMESTER 2021 gm31@illinois.edu 22 October 2022 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Guillermo Martinez Espina - Captain What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Intelligent Learning Platform It will allow the students to automatically see the first related page of the textbook depending on the lecture they are watching on coursera Briefly describe any datasets, algorithms or techniques you plan to use Web crawling and scraping to search for the title of the lecture Word stemming and stopwords removal How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? Whenever you switch a lecture in Coursera, the chrome extension will perform a search on the PDF textbook using keywords from the lecture's title and show the PDF results accordingly. Language: Typescript Framework: React Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Chrome extension setup with typescript and react 5h Chrome extension integration with PDF viewer 8h Perform web scraping on the site 5h Perform word stemming and removing stopwords 5h Perform search on PDF 3h At the final stage of your project, you need to deliver the following: Your documented source code. A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why.
https://github.com/gmotzespina/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/maatjes2/CourseProject	CS410 - Project progress report.pdf	"Course project progress report - team: maatjes2 Discriminatory tag generation on a collection of documents covering a specific subject 1. Which tasks have been completed? A set of research papers covering text retrieval/mining has been downloaded and pre-processed to be able to test the project functionality Reading-in of all documents in TXT format from a specific directory is implemented Filtering/tokenization of document contents has been implemented Constructing collection LM and document specific LMs implemented Normalizing LMs mostly implemented Printing program results to STDOUT seems to work 2. Which tasks are pending? Implementing user-specified directory upon launching the program Generating program result report, including top 10 discriminative tags per document, and writing it to disk Evaluating program results by comparing them with article abstracts Documenting and cleaning up the code base Preparing a user guide and demonstration for the program 3. Are you facing any challenges? Reading in PDF-files with python libraries proved tricky and unreliable (due to words being concatenated from some PDFs) The team therefore fell back to processing TXT-files only for now. Upon filtering and tokenizing document contents into clean ""bag of words"" representations, the ""Porter2Filter"" function of MetaPy, which should help with word stemming, does not seem to perform very well. It seems to ""pollute"" program results by rendering many words unrecognizable. The team considers not making use of stemming at the cost of having some duplicate program results. Underflow seems problematic when dividing exceedingly small floating-point numbers together. A solution might be to make use of logarithms before division. This will forfeit information regarding the exact probability distribution of words within documents but will preserve their ""rank""."
https://github.com/maatjes2/CourseProject	CS410 - Project proposal maatjes2.pdf	"Course project proposal - team: maatjes2 Discriminatory tag generation on a collection of documents covering a specific subject 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Tristan Maatjes (NetID: maatjes2) is the sole member and therefore captain of this team. o Despite the captain's search for additional crew members via the Campuswire forum, he found none that were in his time zone (CEST). o Due to the captain's busy schedule outside the CS410 course, involving a full-time job and a family with two young children, finding crew members living in the same time zone, and therefore having a high probability of sharing approximately the same rhythm, was of the utmost importance to him. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? * The free topic chosen for this project is ""Discriminatory tag generation on a collection of documents covering a specific subject"". o The aim of this project is to create a tool that helps researchers and students to easily spot the discriminatory subjects / specializations between seemingly similar documents on a particular subject matter. o One might think for example of students in the primary, exploratory phase of writing a thesis on a particular subject. During this phase, students generally need to review what research has already been done on their chosen subject. The result of this process is often included as part of their thesis/research as a literature review. o To assist students and researchers during such literature reviews, this project aims to develop a tool that will automatically generate discriminatory tags for each document or research paper under review, such that their added value or specialization vis a vis the rest of the document collection under scrutiny becomes immediately clear, without even having to manually compare the respective document abstracts. o The results of this tool will help its users in refining their document selection, setting reading priorities and priming their expectations before even starting to actually read any of the documents in the collection. * To realize such a tool, the team considers the following approach: o The program will take a directory path as input. In this directory, the user will have saved his selection of documents in an acceptable format (the first aim will be to read in .txt files, but this might be extended to more convenient formats as .doc and .pdf if there is enough time left). o All files in path are treated as text documents and their respective strings are treated by the program as separate documents and saved accordingly in internal data structures. o The program generates one general language model (LM) by using all these documents. o The program then generates one document-specific LM per document and normalizes these respective LM's using the collection LM. o Alternatively, each document-specific LM might be normalized by using a general English language LM (therefore covering a much broader scope than the collection under scrutiny) and subsequently normalized again by using a LM based on the document collection minus the specific document currently being compared. (This would increase computing time by N compared to the first method, N being the number of documents in the collection. Any advice from peers or course staff in choosing between these two methods would be highly appreciated. o Using the comparison results, the documents in the collection will be ranked in order of ""novelty/originality"" and will each be represented by a top ten word list (ranked in order of importance or ""impact"" on the document subject originality in comparison to the other documents). o The results will be output in a ""collection report""-file in the same directory as the input documents were in (the name of this document will include a specific string that will be used in a filter when reading in files in the first phase of the program, such that any previous reports will not be included in the document collection in case of repeated program execution on the same directory) o During program development, some bonus features might be included * The effectiveness of the previously described tool will be evaluated using multiple document collections, each covering different research subjects. The results will be compared to the document abstracts (and the rest of the document contents, if necessary) to validate the outcome. 3. Which programming language do you plan to use? * Assuming most libraries needed to effectively implement the algorithm described above are available, the project will be realized using the Python programming language. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * (2h - writing a course project proposal and correctly setting up CMT + GitHub) * 2h - assembling multiple document collections for testing purposes * 10h - writing the main code base * 3h - debugging * 2h - writing a progress report * 3h - adding bonus features (such as accepting more input formats) * 2h - testing and optimization * 3h - preparing a demonstration, showing the effectiveness of the product * ==== * 25+h"
https://github.com/maatjes2/CourseProject	README.md	CourseProject: Discriminatory tag generation on a collection of documents covering a specific subject - team: maatjes2 Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. At the final stage of your project, you need to deliver the following: Your documented source code and main results. Self-evaluation. Have you completed what you have planned? Have you got the expected outcome? If not, discuss why. A demo that shows your code can actually run and generate the desired results. If there is a training process involved, you don't need to show that process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand.
https://github.com/maaparna/CourseProject	CS 410 Project Progress Report.pdf	"Project Progress Report 1. Which tasks have been completed?  The GCP cloud set up. Creation of project and enabling the Video Intelligence API and setting up a service account for the application to access the API and the video from the cloud storage.  Installing Google Cloud SDK  Virtual environment set up.  The sample application provided by Google tutorials was tested.  A simple python script was created to run the ""Label Detection"" feature on the video stored on the cloud storage. A first segment of the video and its label were displayed to see if the annotation was close enough or not. 2. Which tasks are pending?  The video length was a short one for 3 minutes and worked. I have to set the timeout to a higher value and see if the simple code works for long videos or not.  I need to test the Text_Detection feature to see if the annotation works for videos containing Text in them.  Finally will have to annotate four videos, two from lectures, two from personal archives.  Display the results in an informative way.  Final report and presentation. 3. Are you facing any challenges?  Setting the Application default credentials was giving a lot of trouble. The Compute account is given ""Editor"" Access by default, and after a lot of trial and error, I found that the access privileges have to be slightly more elevated for it.  I am also having trouble getting the segment times correct. It looks like the version of Google Cloud SDK I have installed might be a bit old as some of the sample code had to be changed for it to work. This means getting the segment down microsecond is not working for me as it is not supported, and I have to find an alternative way.  Finally, displaying the results. I am getting each video segment and the different labels associated with it, but the depth of detection I expect is not there. I will have to explore custom labelling instead of using the default."
https://github.com/maaparna/CourseProject	CS 410 Project Proposal.pdf	CS 410 : Project Proposal Prepared for: CS 410 Prepared by: Aparna Anand 24 October 2021 INTELLIGENT LEARNING PLATFORM Team Members This will be a single member team. Aparna Anand (aparnaa2@illinois.edu) will be responsible for building and delivering the project and the milestones on time. Topic The topic chosen is from the Intelligent Learning Platform. The goal is to build a search engine that allows for video segment search. Project Outline All have faced a situation where we would like to have the ability to search through a video to just a particular segment of interest or search through our video archive to find a few specific moments. As mentioned in the Topic section, this project intends to create a search engine that will allow video segment search. For this to be possible, the videos need to be enriched with metadata. Metadata is not just the date, location and rich description but also a transcript of the video. If we could annotate any object, Text, or signs that appear in the videos, we could search for specific moments when they occur. In the project, the dataset used will be 1-2 lecture videos of class CS 410, 1-2 random videos from my video archive. I want to use the Google video intelligence API to annotate the videos from my archive using Label detection. For the lecture videos, I plan to use the API to detect Text, and I also want to store the transcription to help with search of segments from the video. The project proposed is relevant to what we are learning is because of questions such as * Which video segment is most relevant to the search text? * Have all the relevant video segments been retrieved? * Are they ranked correctly, or is there a ranking required? We have gone in-depth about the questions pointed out above for text-based search engines. The same applies by annotating the videos and allowing users to use Text to search relevant annotated video segments. The language I will use is Python. For using the API and getting the annotation results, I will need a google cloud account. As for displaying the results, I am still not very sure what to use. One of the links I have provided in the reference talks about using streamline.io, which is one of my considerations or I may build a simple webpage to display the results. Time Most of the content discussed above in the project outline section is new to me, and I will have to explore how the labelling works, storing the labels and building a simple search to pick the relevant video segments. Finally, once all this is in place, a proper visualisation to display the results. Being a single person team, this will take me 20+ hours. INTELLIGENT LEARNING PLATFORM An assumed split would be Exploring the API and how video annotation works: 1. Initial system setup ~2 hours 2. Label detection ~2-3 hours 3. Text detection ~2-3 hours 4. Transcript usage + labels from Text detection combined to enrich search ~ 2-3 hours 5. Retrieving and storing the labels and time segments ~2 hours 6. Simple search to use text and retrieve results(includes ranking and bringing relevant information) ~5-8 hours 7. visualisation of the results ~5 hours 8. Final project demo video and report ~2- 3 hours References https://mlbhanuyerra.github.io/2019-12-09-Video-Search-Engine-Salsa/ https://towardsdatascience.com/prototyping-my-video-search-engine-d6fb03c9bcd1 https://www.erikaagostinelli.com/post/getting-started-with-google-video-intelligence-api-using-python https://cloud.google.com/video-intelligence/docs/analyze-labels INTELLIGENT LEARNING PLATFORM
https://github.com/maaparna/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jhinukb/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yuchen9902/CourseProject	cs410 final project proposal.pdf	Team member: , yuchenz8 Yuchen Zeng Captain: , yuchenz8 Yuchen Zeng Topic: Movie review classifier Description: Following the algorithm described in Learning Word Vectors for Sentiment Analysis(https://aclanthology.org/P11-1015/), I plan to build a movie review classifier web application. This movie review web application will let the user to input a paragraph of review and the application will try to judge whether this paragraph is a positive review or a negative review. This task is interesting and can be used in several applications. It can be used by movie review websites like Rotten Tomatoes to automatically recognize and classify users' reviews. Methods: I plan to train my model using the n-gram model. I will compare between unigram, bigram, and 3-gram model to find the best performance model. I will also try doing some performance to increase my accuracy to predict the tags, like using the smoothing method and log. I will be using the dataset provided by the paper, it is a training dataset with reviews and its property. I will expect the overall accuracy of classification to be higher than 85%. In order to evaluate this outcome, I will calculate my accuracy based on a testing dataset. I may create the testing dataset myself. Programming language: Python Workload: Read the paper and configure the dataset --------2hr Build unigram, bigram, and 3-gram model on the dataset ---6hr Build testing dataset to test my model, search methods to improve performance---3hr Build smoothing method ---2hr Build the backend for the web application -----3hr Build the frontend for the web application ----4hr
https://github.com/yuchen9902/CourseProject	cs410 Project progress report.pdf	Project progress report 1) Which tasks have been completed? Firstly, after reading through the paper Learning Word Vectors for Sentiment Analysis, I found the database used by the paper (https://ai.stanford.edu/~amaas/data/sentiment/). I downloaded the database and separate the data into testing data and training data. Then I start to construct the skeleton of my coding part. I create three python files. One is classifier.py which will mainly be in charge of n-gram and other classification methods implemented. One data_loader.py file used to read data to my classifier. The last one is the main.py file which will parse the argument and run the program. I also finished implementing a unigram and bigram classifier for the project. I also add Laplace smoothing method to the model to avoid overfitting and overflow. 2) Which tasks are pending? The classifier still needs improvement to more gram and smoothing. I will need to work on data_loader in the future to ensure the data passed into the classifier is valid. The whole implementation of the web application is still pending. I will start working on it after finish implementing the classifier. 3) Are you facing any challenges? It seems like the data I got is bigger than I expected. Maybe I will need to modify the size of it if the size is affecting the performance.
https://github.com/yuchen9902/CourseProject	README.md	Movie review classifier Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/saketv2/CourseProject	progress-report.pdf	CS 410 Course Project Progress Report: Team Mango Project Coordinator: Saket Vissapragada Team Members: Ameya Gharpure (ameyapg2@illinois.edu) Amrith Balachander (amrithb2@illinois.edu) Sruthi Kilari (skilari2@illinois.edu) Saket Vissapragada (saketv2@illinois.edu), Shreya Sharma (ssharm90@illinois.edu) 1.) Progress made thus far: * Project Planning and Task Assignment i.) 10 hours total = 2 hours/person * 5 group members * Web Scraper for obtaining text from Google reviews i.) 25 hours total = 5 hours/person * 5 group members After developing our implementation plan for our project, we created organized schedules for our team and created a scrum board to organize the tasks that we want to complete (since we have a bigger team, this will help us stay on track). 2.) Remaining tasks: * Apply algorithms covered in class to generate sentiment analysis scores for each review based on commonly used words i.) 30 hours total = 6 hours/person * 5 group members * Rank these scores against each other to determine standardized star ratings and compute an average standardized score for restaurants. i.) 25 hours total = 5 hours/person * 5 group members * User testing to determine accuracy of standardized scores i.) 10 hours total = 2 hours/person * 5 group members As a team, we have gone through a high level approach to completing these tasks and read through online resources to get a better understanding of how we will develop our project. We hope to leave more time aside at the end for the purposes of bug fixes and code refactoring. 3.) Any challenges/issues being faced: * With the webscaper, we were having issues with accurately gathering the text from the Google reviews. We attempted to use a Yelp API to remedy this issue, but then we reverted back to a web scraping library called BeautifulSoup. * Now that we have gathered the raw text from the web pages, we are facing challenges with organizing the data. Once we work through this challenge, we will be able to apply our algorithms as well as standardize star ratings
https://github.com/saketv2/CourseProject	project-proposal.pdf	Course Project Proposal Team: Mango Project Coordinator: Saket Vissapragada Team Members: Ameya Gharpure (ameyapg2@illinois.edu) Sruthi Kilari (skilari2@illinois.edu) Saket Vissapragada (saketv2@illinois.edu), Shreya Sharma (ssharm90@illinois.edu) Amrith Balachander (amrithb2@illinois.edu) Chosen Topic: Intelligent Browsing - Standardizing Google Restaurant Reviews Issue: A common problem in restaurant reviewing is that there is no standardization behind the stars that are given in a restaurant review. We want to alleviate this issue by using sentiment analysis to give an unbiased and standardized ranking to all the text-based reviews. Programming language: Python/Javascript Theme and course relation : Our project relates to the theme of intelligent browsing by integrating a chrome extension that provides added functionality to Google review. This project relates to the course by using web scraping by parsing HTML files and creating a model. Estimated workload: - 20 hours * 5 members = 100 total hours for this project * Project Planning and Task Assignment * 10 hours total = 2 hours/person * 5 group members * Web Scraper for obtaining text from Google reviews * 25 hours total = 5 hours/person * 5 group members * Apply algorithms covered in class to generate sentiment analysis scores for each review based on commonly used words * 30 hours total = 6 hours/person * 5 group members * Rank these scores against each other to determine standardized star ratings and compute an average standardized score for restaurants. * 25 hours total = 5 hours/person * 5 group members * User testing to determine accuracy of standardized scores * 10 hours total = 2 hours/person * 5 group members Demonstrating approach accuracy: We will have volunteers assess what they believe the scores of reviews should be. We will present the reviews without the scores and use them to measure the accuracy of the scores generated from our algorithm.
https://github.com/saketv2/CourseProject	README.md	CourseProject Chrome extension to standardize Google restaurant reviews using sentiment analysis
https://github.com/aishanipal/GithubCommentsSentimentAnalysis	Progress Report.pdf	Captain: Aishani Pal - aishani2 Alyxandra Merritt - merritt9 November 15, 2021 Progress Report Progress Made We have completed the data collection portion of our project. Using the data from GHTorrent, the SQL database is saved locally. We created a script to query the data and save the information we need in organized .csv files. The relevant data we have saved are the project IDs, user IDs, and comment text. If we need any additional pieces of data later on, updating the script should not take long. We have also begun creating our own sentiment analysis tool. We are using the nltk package. So far, we have done tokenization and POS tagging on our comments. Remaining Tasks First, we need to finish creating our own sentiment analysis tool. Then, we will use the tool to replicate the paper's findings. Once we have reasonable results, we will continue with our own research goal: to analyze the emotions of GitHub commit comments associated with a person over time for a single project. Lastly, we will chart all of our findings. Challenges The dataset was quite large and initially created problems when reading the data we needed. However, after studying the SQL schema, we were able to extract the specific information we needed for our project. At first, we wanted to use SentiStrength, the tool used by the paper we are following, to accurately compare our findings with those of the paper. However, we found out that we have to pay 1000 euros in order to use the software. Thus, we have decided to not use SentiStrength to compare. Instead, we will use our own sentiment analysis tool to attempt to replicate the findings of the paper before proceeding with our own research question. Since we shifted the project slightly to focusing on our own sentiment analysis tool, we both planned to work on this task and agreed to update each other on progress that was made. Aishani was able to complete tokenization and POS tagging on the comments on her machine successfully. When Alyxandra attempted to build upon this work, she experienced challenges running this script on her own machine. Despite spending several hours attempting to resolve the error, running the code on multiple different environments and machines, and seeking help from Aishani, it is still unclear why Alyx is unable to run the existing code. This will be one of the first things that we plan to resolve as it is necessary that both of us are able to run and contribute code from our own machines.
https://github.com/aishanipal/GithubCommentsSentimentAnalysis	Proposal.pdf	Captain: Aishani Pal - aishani2 Alyxandra Merritt - merritt9 October 24, 2021 CS 410 Course Project Proposal Our main goal is to analyze the emotions of GitHub commit comments associated with a person over time for a single project. We will do so by analyzing multiple people across multiple projects. Our idea is inspired by the paper Sentiment analysis of commit comments in GitHub: An empirical study written by Emitza Guzman, David Azocar, and Yang Li. The study looks at sentiment for commit comments for the overall projects and analyzes several metrics including time and project approval. One Github commit comment feature the paper does not analyze, but we believe could be interesting, is to track comment sentiment for a singular user. Looking at this trend can tell us how a developer's mood changes over time as a deadline approaches. For example, we can answer questions like: does a developer become more negative closer to a deadline due to frustration? Or is the developer likely to become more positive as a result of better progress over time? We plan to take a very systematic approach to complete this project. First, we will gather our data using the GHTorrent dataset mentioned in the paper. Then, we will begin developing our Sentiment Analysis tool. Finally, we will create graphs and metrics displaying our sentiment of users change over time. This will allow us to draw conclusions in a manner similar to the paper. We will use the same GHTorrent dataset as mentioned in the paper (https://ghtorrent.org/msr14.html) and import it into MySql. Sentiment analysis and opinion mining will be discussed in the course in Week 11. We plan to leverage this lecture material with the approaches and tools mentioned to create our own sentiment analysis. Some packages we have previous knowledge of that might be helpful for this project are pandas to help work with the data, nltk for sentiment analysis, matplotlib for plotting data, and scipy for analysis. With the completion of the project, we expect to see either a positive, negative, or neutral trend in emotions across time. We will conduct both an observational and mathematical analysis to draw conclusions based on the sentiment analysis data we derive. We will evaluate our work by ensuring that our sentiment analysis tool produces similar results as the paper. That is, we will match our results of emotions with the programming language, time of day, and day of the week with those of the paper. Once we can reproduce the paper's results, we will continue with our research question regarding emotions over time for a single person. We plan to use Python to complete this project. Also, we will use MySql Database queries on the dataset. For our project team of two members the workload needs to be at least 40 hours and we have broken it down in the following table: Pal & Merritt 2 Task Estimated Time Data collection 2 hours Build sentiment analysis tool 15 hours Test our sentiment analysis with similar metrics as the paper 8 hours Run our sentiment analysis on our metric 12 hours Evaluate trend (observational and statistical analyses) 3 hours
https://github.com/aishanipal/GithubCommentsSentimentAnalysis	README.md	Github Commit Comments Sentiment Analysis Our main goal is to analyze the emotions of GitHub commit comments associated with a person over time for a single project.
https://github.com/winston86zhu/UserSentimentAnalysis	CS 410 Final Progress Report.pdf	CS 410 Final Project Proposal Group: LastMile Team Member: * Winston Zhu (hezhiz2) 1) Which tasks have been completed? I have completed the framework of the full-stack. I decided to use flask to build the App. The entry points to put and get have been integrated with the backend code. I have also made a very basic HTML static web with the plain text box, which will be used further for the user to input text for sentimental analysis. 2) Which tasks are pending? * Backend model using Naive Bayes and LSTM * The evaluation and comparison of the above 2 models * All the documents writing * Demo videos 3) Are you facing any challenges? No.
https://github.com/winston86zhu/UserSentimentAnalysis	CS410 Proposal.pdf	CS 410 Final Project Proposal Group: LastMile Requirements In your proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will be working on the project by myself. - Winston Zhu (hezhiz2@illinois.edu) 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Free topic: Sentiment or Hate Speech classification on Twitter data Description: The main objective is to classify the twitter text dataset regarding a realistic label. The label is multi-level and the input is pure text input, similar to what we worked on in MP2.2 or MP2.3. For example, given the Twitter text, we will identify the sentiment associated as being positive or negative. Positive and negative can be determined either via cross-comparing the positive or negative words by searching from an existing word dictionary (bad of words) or training and developing a neural network. The majority of the knowledge will come from the lectures in Week 12. I am planning to apply 1 or 2 models and compare the results for sentiment classification and perform a basic data visualization to the result. Task: I define the tasks into 3 parts, full stack framework, model construction and visualization. * Because I want to build a full stack, local product for users to interact with. I will start working on a full stack framework. Ideally this is done by flask or django, written in python. * The most important part comes to the model. I am planning to use one we learnt in class, such as Naive Bayes or LSTM(not learnt but mentioned in reading material) and another one using neural networks, such as CNN (convoluted neural network). * The last part I am planning to demonstrate the result using some of the evaluation methodologies we learnt in class, such as F1 score, average precision. Then I will create visualizations to compare different models. Why is it important or interesting: Nowadays people usually publish what they want on Twitter, given the freedom of speech. This is an advantage of information society, however, sometimes people offend other people or groups of people. This is where the fight begins and everyone has seen it while surfing the internet. I want my model to classify the sentiment or hatred of a text so that people at least get some alarm before publishing. Specifically as an Asian, I have seen the anti-Asian twits on the Internet, especially last year. What is your planned approach: First collect public labeled data, then do data cleaning and curation. The formalized data will then be fed into the model and we will use the trained model to test on some new data. We do this in an iterative manner and we document the results for different models. What tools, systems or datasets are involved: * Flask (web framework) * NLTK and MetaPy (Stemming and Tokenization) * Python autocorrect (Data Cleaning) * Panda (data structure) * Tensorflow (optional) * Dataset: https://www.kaggle.com/vkrahul/twitter-hate-speech * Dataset2: https://www.kaggle.com/kazanova/sentiment140 3. Which programming language do you plan to use? * Python * HTML in frontend * Some bash code for pre-defined script 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Report writing ( 4 hours in total) * Data collection (2-3 hours) * Data cleaning and curation (5 hours) * Model implementation, include training, testing and model comparison (5 hours) * Data visualization and presentation (5 hours)
https://github.com/winston86zhu/UserSentimentAnalysis	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Launch the App Go to http://127.0.0.1:8088/ once you run main.py
https://github.com/muneshb/CourseProject	Progress Report.pdf	FINAL PROJECT PROGRESS REPORT SCRAPING AND RANKING ROTTENTOMATOES UIUC: FALL'21 CS 410 - TEXT INFORMATION SYSTEMS Topic: Scraping and Ranking RottenTomatoes Theme: Intelligent Browsing Team Members: Jeremy Wisuthseriwong (jrw7), Munesh Bandaru (muneshb2), Supriya Puri (puri6) Team Captain: Munesh Bandaru (muneshb2) Overall Tasks Status Tasks Completion Percentage Data Scraping 100% Data refining 100% Modelling & Evaluation 60% Web Application for interaction 70% End to end testing 0% Project Report and Presentation 25% PROGRESS MADE 1. Data Scraping: * Built Python script to scrape the urls for the main top 100 movies page * Built Python script to scrape the content and reviews from each movie page 2. Data refining: * Refined the content dataset to include information such as movie title, synopsis, rating, genre, cast, and critic reviews 3. Modelling & Evaluation * Performed initial modelling using BM25 * Calculated ranking results for each sample query for the top10 movies * Performed initial evaluation by calculating average precision for each sample query and mean average precision for all the sample queries 4. Web Application to display results * Built a prototype of an interface for user query interaction * Include the title and url to be displayed in the webapp for the top 10 movies ranked according to the query REMAINING TASKS 1. Modelling & Evaluation * Continue modelling using BM25 parameters and other ranker algorithms * Continue relevance testing and evaluation using other algorithms such as NDCG@10 2. Web Application * Create a fully functional web app displaying the results for the input query 3. End to end testing * To verify the results are in sync with the query judgements that have been created by manually checking each movie for matching the input query. 4. Drafting Presentation and Project report CHALLENGES/ISSUES * Debugging reasons for low precision results * Domain research to improve the query matching by identifying appropriate stopwords. * Challenges in manually ranking query judgements for evaluating our model * Deciding on the most appropriate ranker algorithm
https://github.com/muneshb/CourseProject	Project Proposal.pdf	"FINAL PROJECT PROPOSAL SCRAPING AND RANKING ROTTENTOMATOES UIUC: FALL 2021 : CS 410 - TEXT INFORMATION SYSTEMS Topic: Scraping and Ranking RottenTomatoes Theme: Intelligent Browsing Team Members: Jeremy Wisuthseriwong (jrw7), Munesh Bandaru (muneshb2), Supriya Puri (puri6) Team Captain: Munesh Bandaru (muneshb2) TABLE OF CONTENTS Abstract Goal Problem statement Topic justification Dataset Algorithm Programming language Evaluation Workload justification ABSTRACT Vertical search engines have become increasingly popular, now-a-days, as they sift through limited databases for information. A general web search cannot accommodate all of the users' searches when it comes to specific topics without implicit assumptions. In particular, using a vertical search, a user can extensively use query based searches to get the desired results with high user ratings and reviews. One major example for searching a specific topic is ""Rotten Tomatoes"" - a review aggregation website for movies and television series. Its content is specialised for users browsing information on top rated movies and television entertainment - genre, cast, network or the critic and user ratings. Results from services like Rotten Tomatoes allow a user to rank results by User Reviews, Critic Reviews, Genres, Audience Score. Being able to track user experience for various movies and tv series could lead to a larger audience and greater profits . GOAL Our proposal is to scrape through the ""Top 100 movies of 2021"" on the Rotten Tomatoes website - https://www.rottentomatoes.com/top/bestofrt/?year=2021 and then rank and sort those 100 movies according to the user given query. PROBLEM STATEMENT On the Rotten Tomatoes web page, users can find various pre-defines ratings and rankings for movies and TV series like Best Movies of 2021, Popular Shows on Netflix but there isn't a way to effectively search and rank for a list of movies within that particular ranking matching the user query/interest. Example: Lets say a user is browsing the ""Top 100 movies of 2021"" on Rotten Tomatoes. But he is more interested in looking for thriller movies available on Hulu from this ranking list of 100 movies. When looking further, he could not find a way to filter and sort this list and needed to go through each movie description to find out whether a movie meets his interests or not. Sample query: ""thriller movies on Hulu"" TOPIC JUSTIFICATION As mentioned in the problem statement there isn't a way to effectively search and rank a list of the top 100 movies for 2021 based on a given user query. Finding the right things on the internet is not easy - returning things that are of relevance to the user along with filtering the content based on the popularity is a challenge. Any general search engine would parse all the pages related to the query and search in a breadth-first manner to collect results. A query-specific search more efficiently searches a small subset of content by focusing on a particular requirement. Through this project, we are trying to improve the user experience of browsing the content based on the user's interests. Here we are making our system to provide an intelligent way of browsing the content within the top 100 movies filtered and sorted based on the user query. DATASET https://www.rottentomatoes.com/top/bestofrt/?year=2021 ""Movies with 40 or more critic reviews vie for their place in history at Rotten Tomatoes. Eligible movies are ranked based on their Adjusted Scores."" Movies' data is stored on several popular websites, but when it comes to critic reviews there is no better place than Rotten Tomatoes. In the movies dataset each record represents a movie available on Rotten Tomatoes, with the URL used for the scraping, movie title, etc. We will be scraping our data from the url above and the dataset (CSV file) will include the below mentioned variables: 1. URL used for the scraping 2. movie title 3. Description 4. Genres 5. Duration 6. Cast 7. Director 8. Users' ratings 9. Critics' ratings 10. Network ALGORITHM We are considering the BM25 algorithm for this use case but we would like to experiment with other alternatives like jelinek-mercer , dirichlet-prior. PROGRAMMING LANGUAGE We are considering Python as our primary programming language to design our algorithm and using relevant libraries to assist with scraping web pages. We may assess other web-based languages in the event we need to implement additional functionality. We chose Python it has rich libraries for Scraping, Text processing and Modeling tasks EVALUATION During our evaluation phase, we are considering using relevance feedback of a binary label (0=not relevant, 1=relevant) in order to reliably evaluate the ranked output. Moreover, we're planning to assess the effectiveness of the ranking algorithm by using mean average precision at 10 documents, since MAP is the standard measure for comparing ranking algorithms. We're aiming to use 10 documents as an evaluation threshold with the assumption that users will not likely scan through all 100 documents. Lastly, we'll consider statistical significance testing to evaluate the average precision results, as we experiment with our ranking algorithm to ensure differences aren't simply due to particular queries that are chosen. WORKLOAD JUSTIFICATION Main Tasks Estimated Time Cost (Hrs.) Scraping the Main top 100 movies page 20 Scraping the description/content of each movie page 10 Refining the dataset 10 Modeling 10 Evaluation 10 Building an interface for user query interaction 20 End to end testing 10 Drafting Presentation and Project report 10 Miscellaneous Learning 10"
https://github.com/muneshb/CourseProject	README.md	"Scraping and Ranking RottenTomatoes UIUC: Fall'21 CS 410 - Text Information Systems Theme: Intelligent Browsing Team Members: Jeremy Wisuthseriwong (jrw7), Munesh Bandaru (muneshb2), Supriya Puri (puri6) Team Captain: Munesh Bandaru (muneshb2) This project is for the University of Illinois Urbana-Champaign CS-410 Text Informations Systems. Web application url: Not-so-rotten-tomatoes Project Proposal Final Project Proposal Project Progress Report Final Project Progress Report Abstract Vertical search engines have become increasingly popular, now-a-days, as they sift through limited databases for information. A general web search cannot accommodate all of the users' searches when it comes to specific topics without implicit assumptions. In particular, using a vertical search, a user can extensively use query based searches to get the desired results with high user ratings and reviews. One major example for searching a specific topic is ""Rotten Tomatoes"" - a review aggregation website for movies and television series. Its content is specialised for users browsing information on top rated movies and television entertainment - genre, cast, network or the critic and user ratings. Results from services like Rotten Tomatoes allow a user to rank results by User Reviews, Critic Reviews, Genres, Audience Score. Being able to track user experience for various movies and tv series could lead to a larger audience and greater profits . Introduction Finding the right things on the internet is not easy - there isn't a way to effectively search and rank a list of the top 100 movies for 2021 based on a given user query. Returning things that are of relevance to the user along with filtering the content based on the popularity is a challenge. Any general search engine would parse all the pages related to the query and search in a breadth-first manner to collect results. A query-specific search more efficiently searches a small subset of content by focusing on a particular requirement. Through this project, we are trying to improve the user experience of browsing the content based on the user's interests. On the Rotten Tomatoes web page, users can find various pre-defines ratings and rankings for movies and TV series like Best Movies of 2021, Popular Shows on Netflix but there isn't a way to effectively search and rank for a list of movies within that particular ranking matching the user query/interest.. Here we are making our system to provide an intelligent way of browsing the content within the top 100 movies filtered and sorted based on the user query. Example: Lets say a user is browsing the "" Top 100 movies of 2021, Rotten Tomatoes "" on Rotten Tomatoes. But he is more interested in looking for thriller movies available on Hulu from this ranking list of 100 movies. When looking further, he could not find a way to filter and sort this list and needed to go through each movie description to find out whether a movie meets his interests or not. Sample query: ""thriller movies on Hulu"" By using the web application, he can find the thriller movies available on Hulu (matching to the query) with movie information and tomatometer rating. Thus making it easier for him to explore his options and decide on the movie according to his liking. Overview of the Tasks Data Scraping and Refining: Scraping the Main top 100 movies page Scraping the description/content of each movie page: Title synopsis genres where to watch tomatometer rating reviews Modelling & Evaluation Create a ranker function to score each document per the sample query using BM25 parameters and other ranker algorithms Calculate ranking results for each sample query for the top10 movies Perform initial evaluation by calculating average precision for each sample query and mean average precision for all the sample queries Perform relevance testing and evaluation using other algorithms such as NDCG@10 Web Application for interaction Create a web-application for the user to input a query related to the movie and get a list of top10 rank based movies matching the search criteria End to end testing Create a query judgements document by manually checking each movie for matching the input query and verify that the results from the search function are in sync with the user provided initial relevance. References Top 100 movies of 2021, Rotten Tomatoes Metapy Search and IR evaluation Tutorial Implementation of the ranker functions How to scrape websites with Python and BeautifulSoup The Flask Mega Tutorial by Miguel Grinberg Deploy app on Heroku using CLI Deploy Flask app on Heroku using GitHub"
https://github.com/paulzuradzki/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/dyang5200/CourseProject	project-report.md	"Progress Report (Week 13) Progress made thus far We have written the code to scrape posts and comments from Reddit In our data/reddit/data.csv file, we have parsed through the comments for 1 market day (Feb 22, 2021), 2 stocks (GME, SPY), and 1 subreddit (r/wallstreetbets). We counted the number of comments and put them into 15 minute interval buckets to give an example of the distribution of number of comments throughout a day. Note that we chose a day during the height of the GME craze when wallstreetbets was very active. Remaining tasks We need to figure out a formula to calculate social media sentiment for a stock It's also important to make sure the formula is sensible so that the user can extract meaning from the sentiment and stock price plots. We need to fetch stock price and store that data so that we can graph it We need to plot our sentiment data vs. stock price stillmethod to analyze sentiment for the scraped data in a manner that is suitable for our needs We currently plan to graph social media sentiment alongside change in stock price (first derivative stock movement). However, we may wish to change this into something that tells a different narative. For instance, maybe plotting sentiment alongside stock price (instead of change in price) will make for a better visualization. (If we have time) Gather twitter data and sentiment Any challenges/issues being faced (In regards to the first bullet above) We plan to use NLTK Vader for classifying comments as positive or negative, but we still need a formula for ""sentiment"" for a stock at any given point in time. For starters, we can try a basic formula such as sentiment = # pos comments / # total comments for a given time frame (e.g. comments in the last 30 minutes, but this would be prone to very high variance, especially for smaller stocks and time frames. We noticed that some stocks and subreddits have periods where not many comments are made. This will make it very hard to create a visually meaningful graph of sentiment and stock price. Solution: We are thinking of limiting our sentiment analysis to popular stock tickers (e.g. SPY, TSLA, GME). Gathering sentiment/stock data in real time can be slow due to API limitations Solution: We will have to get the data and perform analysis ahead of time and store it locally or on a server to be accessed at runtime."
https://github.com/dyang5200/CourseProject	README.md	Spicy Chicken Stock Please open project-report.md to see our Week 13 Project Report.
https://github.com/dyang5200/CourseProject	Team Spicy Chicken Stock - CS 410 Project.pdf	Team Spicy Chicken Stock 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. [Captain] Danielle Yang (dy6) (Captain) b. Eric McCarthy (ericm4) c. Sabelle Huang (sabelle2) 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class a. We want to create a Chrome Extension that analyzes social media sentiment over various stock tickers. This will allow users to see how stock price movement is correlated with social media sentiment from various platforms, such as Twitter, Reddit (r/wallstreetbets, r/stocks, etc.), etc. This project was inspired from the craze during early 2021 where the 100x increase of stock price in Gamestop's stock (GME) was largely attributed to the subreddit known as r/wallstreetsbets. b. This relates to the class because we need to scrape various social media websites for data and then process and analyze the data for the user's needs. 3. Briefly describe any datasets, algorithms or techniques you plan to use: We plan to scrape data from Reddit, specifically stock-related subreddits such as * r/wallstreetbets * r/investing * r/robinhood * r/stocks * r/dogecoin * r/news Data Scale: On r/wallstreetbets, the most popular of the above stock subreddits, there typically 10,000 new comments per day (Source: https://subredditstats.com/r/wallstreetbets). If time permits, we may also analyze sentiment from other social media platforms such as Twitter. 4. How will you demonstrate that your approach will work as expected? a. We can start by analyzing stock sentiment for one stock and one subreddit over a short period of time. If this works well, we will increase our scope until we cover multiple stocks over longer periods of time. 5. Which programming language do you plan to use? a. Python, HTML/CSS, JavaScript Libraries: b. Tableau or D3 (for graphs) c. Selenium (for scraping) d. NLTK Vader (Sentiment analysis) 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Code for scraping data (~15 hr) b. Scraping data from social media posts (~5 hr) c. Implement Sentiment Analysis Algorithm (~5 hr) d. Consolidating and displaying data in a meaningful way (~15 hr) i. Come up with heuristics to plot the data (~10 hr) e. Creating chrome extension (~10 hr) At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why.
https://github.com/andrewson3107/CourseProject	Progress Report.pdf	Team 68++ Progress Report: - Andrew Son, andrew28 - Sujay Nanjannavar, sujaypn2 Completed Tasks: - Creating minimum working Chrome extension - Creation of example output from extension o Testing data to be used for evaluating extension o Drawn from sites such as level.fyi Pending Tasks: - Extend functionality of Chrome extension o Implement web scraping on specific sites - Possibly implementing a classifier such as BM25 to rank output from Chrome extension for additionally functionality Challenges: - Learning and developing in JavaScript, especially because some members do not have prior experience with this language - Determining how to write the classifier in JavaScript - Developing frontend on extension
https://github.com/andrewson3107/CourseProject	Project_Proposal_V1.docx	CS410 Text Information Systems (Fall 2021) Project Proposal: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Andrew Son / andrew28 (Captain) Sujay Nanjannavar / sujaypn2 What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? The topic we have chosen is to create a Chrome browser extension that enables Briefly describe any datasets, algorithms or techniques you plan to use How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? Applied programming languages will be Python and JavaScript. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task.
https://github.com/andrewson3107/CourseProject	PROPOSAL.pdf	CS 410 Team 68++ - Andrew Son (captain), andrew28@illinois.edu - Sujay Nanjannavar, sujaypn2@illinois.edu Abstract Our team will produce a Google Chrome extension that links job posting pages to potential salary and interview information that can be found online about that job. This allows those looking at a job listing to very quickly determine whether they are interested in the job or if they would be able to successfully complete the interview, without even needing to do a separate search. This will be done by scraping websites where previous applicants & employees report information about their interview process and job offer. For the scope of this project, the focus of the extension would be on CS-related positions (Software Engineer Intern, IT Intern, Data Scientist, etc.) and would use information from websites such as Glassdoor and levels.fyi. Datasets * Glassdoor * levels.fyi * Reddit (if time permits) Goals Run the Chrome Extension on a large sample Programming Language Because Chrome Extensions are written in JavaScript, we will use JavaScript and Python as our back-end for the webscraping to be performed. Work Estimation Task Time (hours) Become acclimated with JavaScript 5 Create browser extension UI 10 Write scraping code for salary 10 Write scraping code for interview information 20
https://github.com/andrewson3107/CourseProject	README.md	CS410 Final Project: Chrome Extension Team Members: Andrew Son: Andrew28 (Captain) Sujay Nanjannavar: Sujaypn2
https://github.com/shaukat2/CourseProject	Project Progress Report for ExpertSearch System Enhancement.pdf	Project Progress Report: Classification of Faculty directory pages and homepages Author: Sameen Shaukat Net ID: shaukat2 Overview: ExpertSearch System is a search engine that is developed by students of University of Illinois to facilitate faculty search based on their research interests. The goal for this project is to enhance or aid the ExpertSearch System by writing a classification utility to identify good faculty directory and faculty webpage URLs. This utility/functionality can be added to the existing search system later on. Goals: Goals for this project are * Identifying faculty directory URLs * Identifying faculty webpage URLs Proposed Solution: Following methodology will be used as a starting point to achieve above stated goals. 1. Use data provided in Assignment MP2.1 and MP2.3 labeled as good examples and collect data for bad URLs and classify these as bad examples. 2. Based on this data, scrape these URLs to pre-process data. 3. Automatic or manual feature extraction based on data collected in step 2. This will be decided on the basis of results achieved. 4. Develop a utility in Python to identify good URLs from bad ones. This will involve thorough analysis in order to choose the best classification technique. 5. Integrate this utility in ExpertSearch System. (This is optional and time-dependent) Progress Status: Tasks Done or In Progress: Data Collection: This part is completed to cater for both goals of this project. Bad URL examples were collected and added to the data set already available in MP2.1 and MP2.3. Labels were added to this dataset to help in classification. Data Pre-Processing: This part is done almost 70 percent. I am still working on improving data extraction. The steps that I have followed so far are. 1. Scrape each URL to get data. 2. Remove tags to get text. 3. Remove Whitespaces & unwanted characters. 4. Remove Stop Words, Apply Stemming and Lemmatizing using NLTK 5. Replace email addresses and Web addresses with text tags Feature Extraction: This part is still in progress. I have tried manual feature extraction and still want to explore automatic feature extraction as it is a particularly new area for me. Following features were extracted initially. * No. of times professor, assistant, lecturer appears in text * No of times faculty, staff and people appear in text * No of times university, institute, school, department appear in text * No of times email addresses appear in text. * No of times Journal, Research, Publications appear in text. This step is still in development. There is a chance I find more usable features to strengthen classification. Tasks to be Done: I intend to work on automatic feature extraction using simple bag of words model and enhancing it using TF-IDF. This is subject to change as per progress in project development and implementation as this is particularly a new area for me. Classification model needs to be chosen. Currently, I am planning on using Naive-Bayes, Linear Support Vector Machines, Logistic Regression, CNN (Time-Dependent) etc. Average Accuracy Scores, F1-Scores and Execution Time on my current machine will be catered to provide performance comparison across all classification algorithms being used. An integration utility needs to be written for integration with ExpertSearch System. (This is time- dependent) Challenges: Currently, I am having challenges with feature extraction as it is a comparatively new area for me as previously, I have only worked with full data set that had features already. Although, it is a challenge; it's also very exciting as I have learned a lot of techniques and tips during my research on this topic. Overall, I am content with the progress made so far.
https://github.com/shaukat2/CourseProject	Project Proposal_ExpertSearch System Enhancement_shaukat2.pdf	Project Proposal: Classification of Faculty directory pages and homepages Author: Sameen Shaukat Net ID: shaukat2 Overview: ExpertSearch System is a search engine that is developed by students of University of Illinois to facilitate faculty search based on their research interests. The goal for this project is to enhance or aid the ExpertSearch System by writing a classification utility to identify good faculty directory and faculty webpage URLs. This utility/functionality can be added to the existing search system later on. Goals: Goals for this project are  Identifying faculty directory URLs  Identifying faculty webpage URLs Proposed Solution: Following methodology will be used as a starting point to achieve above stated goals. 1. Use data provided in Assignment MP2.1 and MP2.3 labeled as good examples and collect data for bad URLs and classify these as bad examples. 2. Based on this data, scrape these URLs to pre-process data. 3. Automatic or manual feature extraction based on data collected in step 2. This will be decided on the basis of results achieved. 4. Develop a utility in Python to identify good URLs from bad ones. This will involve thorough analysis in order to choose the best classification technique. 5. Integrate this utility in ExpertSearch System. (This is optional and time-dependent) These steps will be used as a reference point and there is a possibility of addition of extra steps along the way of project completion. Initial plan is to build a separate utility that can perform the above tasks and then to integrate it into the ExpertSearch System later on depending on time availability. It's expected that item no 2, 3 and 4 will be most time consuming and take up to 70% of the project time. The results will be demonstrated by executing performance testing of the classification model for each task in project report. A utility which will return the results of classification model when URL is entered will also be provided to demonstrate the working of this system.
https://github.com/shaukat2/CourseProject	README.md	CourseProject [Classification of Faculty directory pages and homepages] Project Proposal - Project Proposal_ExpertSearch System Enhancement_shaukat2.pdf Project Progress Report for ExpertSearch System Enhancement.pdf
https://github.com/nasa-petal/search-engine	progress-report.docx	"Text Retrieval for biomimicry function identification in a corpus of biology papers CS-410 Fall 2021 Progress Report Tasks completed: Evaluation measures have been identified. For this project we will compare text classification approaches based on MAP, gMAP, and average precision by topic. These measures were selected because our task is to present a user with a ranked list of documents based on their relevancy to a particular topic. Class names have been identified. For this project we will only focus on the 10 level1 labels. physically_assemble_or_disassemble protect_from_harm sense_send_or_process_information chemically_modify_or_change_energy_state maintain_structural_integrity attach move process_resources sustain_ecological_community change_size_or_color Dataset has been compiled and loaded into github A Jupyter notebook that provides the template for our experiments has been created that performs the following: Loads the data from github. Preprocesses the data to remove papers with no titles or level1 labels Runs each paper through a pretrained multi-label text classification model (bart-large-mnli) that outputs scores for each label for each paper MAP, gMAP, average precision and PR curves for each topic are computed by comparing the model's predictions against the ground truth. Remaining Tasks: Try different hypothesis templates for bart-large-mnli Try different class names Save evaluation measures to disk so they can be compared later. Train a supervised multi-label text classification model such as SciBERT for comparison. Try out other text classification approaches such as LOTClass for comparison Challenges: Colab ""are you still there"" timeouts"
https://github.com/nasa-petal/search-engine	proposal.docx	"Text Retrieval for biomimicry function identification in a corpus of biology papers Brandon Ruffridge - br24 (Captain) Christian Ortiz - caortiz3 Jay Kim - jk73 Brendan Lynch - bplynch2 Identify biology papers that describe various biomimicry functions from a known list of 100 functions and rank them in order of relevance to a particular biomimicry function. An example would be a paper describing how geckos feet enables them to cling to surfaces, should be given a high relevancy rank for the biomimicry function, ""attach temporarily"". This open-source information retrieval and ranking system is intended to be used by designers and engineers who seek nature's solutions to their design and engineering problems, as well as by biologists who seek to extend the application of their scientific discoveries. The goal is to integrate it into the open-source PeTaL project. While the details of the project tasks will become clearer following the tech review of various approaches, at a high level the project will involve the following. Python will be the primary programming language used for this project. Gather Data (5 hours): We currently have about 1200 biology papers that have been labelled by biomimicry experts with the biomimicry functions described by the paper. We also have access through Microsoft Academic Graph (MAG) API to ~13 million biology papers. Paper metadata that may be useful for our task include title, abstract, authors, references, venue, and MAG topics. Preprocess (20 hours): Data preprocessing may include removing stopwords, stemming, lemmatization, and removing papers with short or empty abstracts. Train (40 hours): We will need to train model parameters to assign relevancy scores to papers for each topic in our set of 100 biomimicry topics. Models we may explore include retrieval models, topic models, and text classification models. Rank (20 hours): We will run the trained model on new biology papers to produce relevancy scores for each topic. Evaluate (20 hours): Due to the lack of relevancy judgements we will have to rely primarily on human evaluation of the ranked lists for each topic. We may be able to gain some insight into how well our ranking model performs by generating nDCG scores comparing the model's ranked list to the ideal ranked list produced by the 1200 papers that have known labels. Feedback (20 hours): If there is enough time, we will also implement an implicit feedback mechanism based on clickthroughs to improve the ranking algorithm."
https://github.com/nasa-petal/search-engine	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Evaluation Measures | Run ID | Model | MAP | gMAP | Description | | ------- | ------------------- | ----- | ------| ------------------------------------- | | 1 | bart-large-mnli | .29 | .28 | 50 papers from each label. 500 total. | | 2a | bart-large-mnli | .34 | .32 | same 100 papers as scibert test set. | | 2b | scibert | .47 | .42 | 400 train, 100 test. | Average Precision by Topic | Run ID | physically_assemble_or_disassemble | protect_from_harm | sense_send_or_process_information | chemically_modify_or_change_energy_state | maintain_structural_integrity | attach | move | process_resources | sustain_ecological_community | change_size_or_color | | ------- | ---------------------------------- | ----------------- | --------------------------------- | ---------------------------------------- | ----------------------------- | ------ | ---- | ----------------- | ---------------------------- | -------------------- | | 1 | .20 | .39 | .22 | .24 | .29 | .46 | .34 | .33 | .29 | .19 | | 2a | .19 | .47 | .21 | .49 | .22 | .39 | .50 | .33 | .45 | .16 | | 2b | .11 | .22 | .65 | .41 | .54 | .46 | .61 | .43 | .67 | .63 |
https://github.com/bids3/CourseProject	ProjectProgressReport.docx	Bidisha Paul Progress Report - CS410 Which tasks have been completed? I have completed the extraction of data from Amazon reviews. I have used the following link to scrape the data https://www.scrapehero.com/how-to-scrape-amazon-product-reviews/ However, due to amazon's anti-scraping measures, I was unable to do so. Thus, I have instead used the data in the link mentioned below https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products Which tasks are pending? I have the following pending task: Split the data into train-test and fit Naive Baiyes Check bias trade off and model performance on the test data Develop additional models and check performance Check performance using precision, recall Are you facing any challenges? I tried to scrape from Amazon website but it was preventing me from scraping them. Captcha issues and IP blocks were a major roadblock.
https://github.com/bids3/CourseProject	ProjectProposal_bids3.docx	Bidisha Paul Project Proposal In your proposal, please answer the following questions: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I, Bidisha Paul, will be working alone in the project. My Net id is bidisha3. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free-topic is sentiment analyses. My project will focus on comprehending the sentiment of customer review for different products in Amazon. Potential customers always want to know the opinion of existing users before they purchase a product. It would be interesting to know the positive and negative sentiment/opinion associated with a product to get a better understanding regarding the product. This helps amazon to understand which products are superior or inferior which further assists in making business decisions. I will scrape the customer reviews from Amazon website. The extracted reviews and ratings of the product will be saved in a csv file. This will be followed by exploratory data analysis, correlation analysis and feature engineering. Then I will split the data into train- test and try to fit Naive Bayes using Python's Scikit learn package. I will try to check bias-variance trade-off and model performance on the test data. I will try to develop additional models with different architecture like Regression, Random Forest on the data and compare the performance to check the best performing models. The expected outcome is to have a robust model with good performance on both training and testing dataset so that it can be used for prediction. To evaluate my model, I will check different performance metrics like precision, recall and F1 score. Which programming language do you plan to use? I plan to use python for my project. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I will be executing the entire project myself. The following are steps to be taken with relative hours required: Quick look at the data and exploratory data analysis - 3 hours Data Visualization and Train/Test Split -2 hours Correlation analysis and setting target variables - 1.5 hour Feature extraction and fitting a Naive Bayes algorithm - 2 hour Testing the current model output - 1.5 hour Develop and compare the current model output with other approaches (Random Forest, Logistic Regression etcetera) - 4 hours Selection of the best performing model and hyper parameter optimization of the selected model to enhance its performance further - 3 hours Writing detailed steps taken and the project's outcome and conclusion - 3 hours 2
https://github.com/bids3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/KefanChen0719/CourseProject	ProgressReport.pdf	Progress Report Which tasks have been completed? * Reviewed lecture notes which are closely related to our project goal and milestones, including PSLA, CPSLA, PSLA with prior, EM algorithm, etc. * Read relevant papers to get inspiration for the project: * http://sifaka.cs.uiuc.edu/czhai/pub/kdd06-mix.pdf * Investigated the MeTA toolkit package in detail and existing use cases. * Narrowed down the areas our project plans to contribute towards a general analysis engine: * Support interpretation of any topic in any given context of a collection of articles. * Support PLSA with prior. * Laid out specific milestones for the tasks we plan to accomplish. * Divided workload among team members to enable parallel work. Which tasks are pending? * Implement algorithms * Brainstorm and find appropriate datasets for algorithm validation and performance evaluation * Prepare software documentation and usage presentation Are you facing any challenges? * We find it challenging to find meaningful datasets to validate our implementation and evaluate algorithm performance. Especially for context based PSLA, we will need to use some datasets with meaningful context information.
https://github.com/KefanChen0719/CourseProject	ProjectProposal.pdf	Project Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Kefan Chen, kchen61@illinois.edu, (captain) Lijuan Geng, lijuang2@illinois.edu What system have you chosen? Which subtopic(s) under the system? System Extension -> MeTA Toolkit -> Extend the MeTA Toolkit to support text analysis Briefly describe any datasets, algorithms or techniques you plan to use EM algorithm to interpret pre-determined topics. PLSA algorithm and its extensions, for example, adopting different priors. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We are planning to add new functions/extensions to the existing Toolkit. We will use the following two ways to determine it works as expected. * By empirical judgement, since lots of the NLP tasks require human input, so we will manually judge if the output makes sense * By comparing the result to other Toolkit or framework that we can find, which implements a similar algorithm, or tries to achieve the same goal How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Our code will try to integrate closely and utilize as much as the current MeTA Toolkit can provide as possible. We do not plan to build our own systems, since everything provided by MeTA Toolkit would be super useful for our project and our goal is to make MeTA Toolkit support more use cases through our extensions. Which programming language do you plan to use? Python for the application layer. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Spend more time to understand the current MeTA package setup, from a developer perspective instead of a pure user perspective. (4 hours) * Determine the actual goals that we will want to achieve and how they should be integrated with the existing Toolkit. (4 hours) * Implement the extensions based on earlier determined ideas. (22 hours) * Test and validate our results against other available tools or procedures and further improvements. (6 hours) * Extra documentation, refactor, examples and code clean up. (4 hours) This track could contain multiple small milestones, goals and features that we can implement and achieve. We will determine the exact and appropriate scope as the investigation and project goes on depending on our progress. Since we are a team of 2 people and we would like each of us to get involved as much as possible, so not everything is parallelizable. For example, each of us will need to understand the current package, investigate the integration points and perform needed tests and validations.
https://github.com/KefanChen0719/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rps2ff23/CourseProject	Progress Report.pdf	"Progress Report Nov 14, 2021 Payal Ahuja, , Riya Simon Trupthi Hosahalli Chandrappa Task Estimated Hours Progress Scrape/collect training data 5 Completed Data cleaning and preprocessing 5 Completed * Optimization pending Design sentiment analysis technique 10 In progress Design tagging model 10 In progress Design clustering algorithm 10 Research Implement model 12 Research Test and verify 12 Build interface for user interaction 8 In progress Documentation and review 5 77 1. Which tasks have been completed? 1. Web Scraping: The code for fetching the comments and rating of professors with more than 50 ratings on ""Rate my Professor"" has been completed. The code for organizing the data and writing to a csv file has been completed. 2. UI: Research for tools to build a UI for the application has been completed. 3. Data cleaning and preparation for the Coursera review by course - static dataset 4. Building the tokenized and lemmatized static reviews for model input 5. Research on Multi-label classification and model creation: Reviewed various methods for classification using text to feature conversions with TF-IDF features using BagofWords, GLoVe and ELMO. 6. Sentiment Analysis: Researched various methods, implemented Vader, SVM to get probability distribution of sentiment scores 2. Which tasks are pending? 1. UI: We are currently building more components of the UI and working on deploying the application so it can be available to the public for testing. Below is a screenshot of the web application, built using Streamlit. We are continuing to work on the integration of the web application with the sentiment tagging model functions. After the sentiment model functions are complete, we need to implement a callback method that calls the function with a given input. 2. Define parameters to categorize sentiment as [strongly negative, negative, neutral, positive, strongly positive]. 3. Finessing the cleaning of data for better keyword extraction 4. Combine sentiment analysis and keyword extraction functionality. 5. Building the actual model for tagging utilizing the understanding of multi-label classification. 6. Integration of the UI input to the model and providing tag results for the final result display 3. Are you facing any challenges? a. Data cleaning with static Coursera dataset b. Integration of the sentiment analysis results and the multi-label classification approach for tagging the reviews"
https://github.com/rps2ff23/CourseProject	Proposal.pdf	"Course Review Sentiment Tagging NLP Trio Payal Ahuja (payalda2), (trupthi2), (riya5) Trupthi Hosahalli Chandrappa Riya Simon Topic The topic that our group chose to explore is Course Review Sentiment Tagging using NLP Techniques. We were interested in exploring how we can gain more insight into students' feedback and understand the underlying sentiment from the colloquial language they use. Additionally, we also want to understand what factors students consider when reviewing a course or a professor (ex. nature of the professor, course usefulness, etc.) At the same time, we want to help the reviewed entity (professors, course designers) make sense of the large set of feedback and extract meaningful data to report and use for further decision-making or performance evaluation. By using NLP techniques to extract key phrases, and building a machine learning model to generate sentiment tags based on those phrases, we hope to gain a better understanding of the underlying sentiment in the feedback process. Dataset We plan to scrape data from ratemyprofessors.com, a platform where students can leave reviews on their course instructors. We will use the review text and accompanying tags on the most rated professors at UIUC to train our model. Another dataset we are planning to use in conjunction is the Kaggle dataset on Coursera reviews, which includes reviews on over 600 different courses. We are planning to perform the necessary preprocessing to combine these datasets so that we have the following fields: Professor Name/ID Course Name/ID Review Text Numerical Rating Tags/Additional Info Implementation We are planning on creating an interactive interface where the student can type in reviews and receive recommendations on tags that match their review sentiment. Our interface will also include a platform on the professor-side, where course instructors can view reports and visualizations of the feedback they have received. The tools we are planning to use are: * Environment: JupyterLab/Google Colab, AWS * Languages: Python, Javascript * Libraries: plotly, matplotlib, scikit-learn, tensorflow, keras, nltk * Methods: POS-tagging, TF-IDF, K-means clustering, Markov chains MVP: Further Goal: Verification Our goal is to ask for explicit human feedback to verify that the tag sentiment that our model generates matches the sentiment of the student review. This will be a binary match, i.e. if the train and test are both of the same sentiment, we classify the match as a ""1"". Otherwise, we classify it as ""0."" In addition to this, we can use the numerical ratings for each course to compare with the sentiment ratings we generate. For example, if the course rating is around 2.0, yet we are generating many positive reviews, we will need to revisit our algorithm. Workload: Task Estimated Hours Scrape/collect training data 5 Data cleaning and preprocessing 5 Design sentiment analysis technique 10 Design tagging model 10 Design clustering algorithm 10 Implement model 12 Test and verify 12 Build interface for user interaction 8 Documentation and review 5 77"
https://github.com/rps2ff23/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/HaritaReddy/CourseProject	CS 410 Project Progress Report.pdf	"CS 410 Project Progress Report Course Topic Explorer (Free Topic) Team: HREC Members: Harita Reddy (haritar2) and Eric Crawford (ecraw3) 1) Which tasks have been completed? The completed tasks are as follows: * The backend setup is up and running. * University websites' scraping for course information has been completed and the data is stored in pickle files to avoid frequent scraping. * Implementation of search functions is completed and is returning the expected results for sample queries. * The backend has been integrated with the search implementation. * The work on implementing the recommendation functions (for recommending programs based on search history) has started and is in the initial phase where we are testing out different ways of vectorizing the text. Currently, we have pulled the programs from a single university. 2) Which tasks are pending? The pending tasks are as follows: * Add detail screen to the frontend that shows more information on the users selected course * Optimize the speed of the backend api. * Return the search results in a structured JSON format for the frontend. * Complete a working recommender implementation. 3) Are you facing any challenges? The challenges that we faced are: * Meta does not work with web servers like Django and Flask. There is a known issue with deadlocks in the pybind11 library that meta depends on. Remedied this issue by using the Falcon framework instead after investigating similar issues reported by other users (Issue Link). If time permits, we may also look into using AWS lambda and API gateway. * Text recommender function based on TF-IDF vectors is not returning the expected results for recommendation to the user. We need to further investigate how to improve the results returned by the recommender. * Scraping different websites with common code was challenging because different websites have different HTML layouts. But as originally planned, we were able to scrape courses from 10 different universities which have an ideal layout. 4) Answers to reviews Question: I was wondering what kinds of universities / programs you planned on including for your project (like schools with a wide variety of programs or smaller engineering or liberal arts schools). We are including courses from universities with a wide variety of programs, like UIUC, Georgia Tech etc. Suggestion: Include the % match of the program to the user's recent searches or just a bit of extra information like ""because you've recently searched ___"", <insert program> is a great fit for you because of their excellent computer science department! That is a great suggestion and we will try to implement this if the time permits. However, we are not planning to include this in the milestones for the project since we already have a significant amount of work to be completed as part of the milestones. We will take this up as an optional task, and if we are not able to do this by the end of the project timeline, we will add the feature later. Suggestion: Finish a minimum viable deliverable and then enhance it to your aspired functionality with the time you have left. Get something working quickly. Suggestion: Time estimates seem to be a bit aggressive to achieve all the milestones. We are implementing a minimum viable deliverable first, as suggested and then we will work on improving, such as improving the search or recommendation functionality further. We believe the minimum viable deliverable will be completed as per the timelines based on the progress we have made till now."
https://github.com/HaritaReddy/CourseProject	CS 410 Project Proposal.pdf	Course Topic Explorer CS 410 Project Proposal (Free Topic) OVERVIEW The aim of this project is to build a comprehensive search system for students looking for the different courses that US universities are offering. We intend to implement a search engine with some recommendation features built based on the users' recent search history. The major component of this project is the search feature, which the students can use to get the courses relevant to the keywords they enter. An advanced feature that we intend to build is a simple recommendation system that will recommend suitable programs that the student can apply to get knowledge on the topics they have been searching for. GOALS The goals of the project are: * Crawl multiple university websites' course catalogs. * Scrape the text data from those websites and store it in a structured format. * Enable smart search for students looking to take courses from top universities. * Implement some advanced search and recommendation features, including recommending users programs (for example, Master's in Information Sciences at XYZ university) based on their recent topic search history. TEAM The team name is HREC. This team consists of Harita Reddy (haritar2) and Eric Crawford (ecraw3). The team captain will be Harita Reddy. DESCRIPTION Motivation Currently, there is no easy way to search for courses offered by universities that cover a topic a student may be interested in. Most universities allow you to do small text searches about their courses, but those searches are not aware of offerings at other universities. Searching across multiple universities will give students a little more information to help them plan what school to attend and what courses to take at those universities. We wish to make it easier for students by getting all the information in one place, without spending hours browsing to get what they need. Approach The first step in our project is to scrape the data from university course catalog websites. This is a time-taking step because the course catalog websites of different universities are organized and formatted differently. We will need to study each site to understand how they tag the relevant information we are looking for in HTML. This information includes things such as course number, class location, teacher, and description of course. We need to arrive at a common format and store the required data from the scraped websites into this format. After having the required data, we plan to implement a smart search for users using the application. This work will have two components (1) Search and Ranking implementation in the backend, and (2) User Interface in the frontend. After implementing the basic search features, we intend to implement some advanced search and recommendation features. When a student searches for a course, recommend an online or an offline program offered by a university that caters to what the student wants to learn based on the user's recent search history. Some other advanced features include linking the MOOC if the course is being offered online. Milestone 1 Crawl and scrape the course catalog websites of selected universities. We are limiting our crawler to 10 universities due to time constraints, universities having multiple colleges, and because the course catalogs and program information websites of different universities are organized in different ways and can be quite unstructured. This is expected to take 10-15 hours. We will also start setting up the backend server to store the crawled and scraped data. The server can be used to also index and rank each course, then exposed to the frontend via restful APIs. Milestone 2 Work on the search features and build the user interface for the search either through a web or a desktop application. Implement basic search features on the data, including searching the most relevant courses based on the keywords. This is expected to take 20 hours because we will also be evaluating different similarity and ranking measures to obtain the best-suited algorithms for the job. Milestone 3 We intend to implement some advanced search and recommendation features in this phase. Some of the features that we will try to implement include: * When a student searches for a course, recommend an online or an offline program offered by a university that caters to what the student wants to learn. An interesting problem to explore would be if we can leverage item-based similarity to find programs that might be interesting to the student based on their recent search history. * If the course is actually being offered online as a MOOC via a platform such as Coursera, give a link to the MOOC. * Optional feature: Display the department (school within a university) that is offering the course and rank the departments based on the number of courses that the department is offering related to the topic the student is looking for. The user will also be able to drill down into the provided results to view the list of courses under that department that offers the course. This phase is expected to take 20 hours. Work Distribution This project is expected to take between 50-60 hours to complete and tasked out as follows: * Frontend Web/Desktop code -Eric * Backend Server infrastructure - Harita/Eric * Web scraping - Harita * Rank and Search implementation - Harita/Eric Tools, Systems, and Datasets We plan to use the following tools for our project: * User Interface: * Desktop/Web App using Kotlin Compose Desktop * Backend Server * Django * Web Scraping: * BeautifulSoup * Selenium * Search and Ranking: * MeTAPy * Apache Lucene (Optional) Dataset will be generated from the scraped data. Programming Languages The programming languages that we intend to use include Python and Kotlin. Kotlin will be mainly used for the App, whereas Python will be used for all the backend work such as scrawling, scaping, search, ranking, and recommendation. Expected Outcome and Evaluation The expected outcome of the project is a working UI that allows users to enter keywords to search for courses offered by different universities. The UI should display the relevant courses in the order of relevance. If the user has searched multiple times with keywords, the system should recommend a program offered by the US university based on the user's search history. We plan to evaluate the system by generating test cases for search. We will manually generate test cases containing the search keywords and the expected list of results. Using metrics such as Precision@10 documents, Average Precision, and Normalized Discounted Cumulative Gain, we will evaluate the search results.
https://github.com/HaritaReddy/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Running Projects Backend The backend is contained in a docker container and can be ran on your local desktop computer or in the cloud on any cloud provider that provides docker (pretty much all of them). The instructions below is just for running the server locally on your computer. Go to the backend folder and run docker-compose up -d. In about a minute, you should be able to go to http://localhost:8000 and see the site
https://github.com/assalupitudor/CourseProject	Project_Progress_Report.docx	CS410 Project Progress Report Team Name: ThePredictive Team Members: Vedant Vipul Jhaveri (vedantj2) - Captain Sagar Dalwadi (sagardd2) Sindhu Kopparam (sgk6) Which tasks have been completed? Our project proposal was to integrate a keyword extraction function in metapy similar to the one existing in SpaCy and integrate chart parser using NLTK. So far we have completed below tasks for respective topics: Integrate Chart parser using NLTK: Completed a research with respect to implementing Parse tree using NLTK library. Installation of NLTK and getting familiarized with the libraries NLTK offers to implement a parse tree Implemented a sample python program to integrate the NLTK library that displays the parse tree for a given statement. (Here is a screenshot of an output result of the sample program that was implemented) Integrate Key word extraction in Metapy Explored keyword extraction function in SpaCy Completed a research around how to implement a domain specific keyword extraction Partially implemented domain specific extraction Which tasks are pending? We will now focus on the below remaining tasks: Integrate Chart parser using NLTK: Research on how a program that uses an NLTK library to display a parse tree can be integrated with MeTA Implementation of an integration between python program containing NLTK library and MeTA toolkit Preparation of data set that can be used for testing Testing & validation of final outcome Documentation & presentation Integrate Key word extraction in Metapy Debug partially implemented code to further refine Preparation of data set that can be used for testing Testing & validation of final outcome Documentation & presentation Are you facing any challenges? The major challenge with respect to integrating chart parser using NLTK with MeTA toolkit is the lack of documentation around enhancing MeTA toolkit and integrating with other natural language processing tools. This requires more research time than expected at the beginning. The challenge with the key word extraction is to gather & create data to process and test for the code And another challenge with respect to keyword extraction is the performance evaluation of the function in metapy when compared to spacy
https://github.com/assalupitudor/CourseProject	Project_Progress_Report.pdf	CS410 Project Progress Report Team Name: ThePredictive Team Members: Vedant Vipul Jhaveri (vedantj2) - Captain Sagar Dalwadi (sagardd2) Sindhu Kopparam (sgk6) 1. Which tasks have been completed? Our project proposal was to integrate a keyword extraction function in metapy similar to the one existing in SpaCy and integrate chart parser using NLTK. So far we have completed below tasks for respective topics: Integrate Chart parser using NLTK: * Completed a research with respect to implementing Parse tree using NLTK library. * Installation of NLTK and getting familiarized with the libraries NLTK offers to implement a parse tree * Implemented a sample python program to integrate the NLTK library that displays the parse tree for a given statement. (Here is a screenshot of an output result of the sample program that was implemented) Integrate Key word extraction in Metapy * Explored keyword extraction function in SpaCy * Completed a research around how to implement a domain specific keyword extraction * Partially implemented domain specific extraction 2. Which tasks are pending? We will now focus on the below remaining tasks: Integrate Chart parser using NLTK: * Research on how a program that uses an NLTK library to display a parse tree can be integrated with MeTA * Implementation of an integration between python program containing NLTK library and MeTA toolkit * Preparation of data set that can be used for testing * Testing & validation of final outcome * Documentation & presentation Integrate Key word extraction in Metapy * Debug partially implemented code to further refine * Preparation of data set that can be used for testing * Testing & validation of final outcome * Documentation & presentation 3. Are you facing any challenges? * The major challenge with respect to integrating chart parser using NLTK with MeTA toolkit is the lack of documentation around enhancing MeTA toolkit and integrating with other natural language processing tools. This requires more research time than expected at the beginning. * The challenge with the key word extraction is to gather & create data to process and test for the code * And another challenge with respect to keyword extraction is the performance evaluation of the function in metapy when compared to spacy
https://github.com/assalupitudor/CourseProject	Project_Proposal.docx	Project Proposal Team ThePredictive 1.What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Vedant Vipul Jhaveri (vedantj2) - Captain Sagar Dalwadi (sagardd2) Sindhu Kopparam (sgk6) 2.What system have you chosen? Which subtopic(s) under the system? Theme: System Extension Topic: Enhance MeTA and Metapy usability Subtopics: Integrate a keyword extraction function in metapy similar to the one existing in SpaCy and integrate chart parser using NLTK. 3. Briefly describe any datasets, algorithms, or techniques you plan to use Keyword extraction: We will implement or integrate a keyword extraction function in metapy like the one existing in spaCy using the pre-existing en_core_web_sm model for the NLP. Parse tree: We will use NLTK and integrate the chart parser to show a parse tree for a sentence. Web interface for parameter setting in MP 2.4 (if advised by instructors): we will use HTML, JavaScript and/or other web programming languages that can interact with python programs in the backend. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation works better? Keyword extraction: We will apply the same set of documents/dataset using existing function in spaCy as well as newly integrated function in MeTA and compare the results between the two of them. Parse Tree: We will parse the same sentence using existing function in NLTK as well as newly integrated function in MeTA and compare the results between the two of them. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. We will use the forked metapy files and edit them to either call functions we implement or call external libraries such as NLTK and spaCy to add the functionality to metapy. 6. Which programming language do you plan to use? Programming language: Python 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Duration Keyword extraction - Research 6 hours Keyword extraction - Test data preparation 4 hours Keyword extraction - Implementation 12 hours Keyword extraction - Testing & Validation 4 hours Parse Tree - Research 6 hours Parse Tree - Test data preparation 4 hours Parse Tree - Implementation 11 hours Parse Tree - Testing and Validation 4 hours Overall documentation & presentation 9 hours
https://github.com/assalupitudor/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/RahulSinghalChicago/CourseProject	Progress Report.pdf	"Names and NetIDs of our team: David Newman (davidn4) Rahul Singhal (rahuls11) (Captain) Charlie Truong (ctruong4) Intelligent Browsing: Hacker News Jobs Progress Report 1) Which tasks have been completed? Our initial goal has been to construct a minimum viable product for our project. This consists of a Hacker News API, javascript chrome extension, and search/retrieval algorithms. We have successfully scaffolded a chrome extension and attached it to the Hacker News API. The ""whoishiring"" robot on Hacker News adds a new posting at the beginning of every month to aggregate technology job solicitations. On button click, the application successfully queries, pulls in, and displays these most recent jobs postings in the browser console. We have also implemented a simple search field. 2) Which tasks are pending? * We need to implement our scoring function to rank the relevant job postings. * We need to design and build the DOM manipulations that will activate after the user inputs their search. * We also need to implement a feedback mechanism to identify if our scoring function is performing well. 3) Are you facing any challenges? * Working with Javascript and Typescript can be challenging for those without experience doing frontend development. * If we want to deploy to the Chrome Web Store, we need to have enough progress done ahead of time to submit the app for approval. So, this may not be possible given the time constraints. We can bundle our app and commit it to Github if necessary for a reviewer to test it though. * Sorting the job posting results may be challenging because of nested comments. The HTML for the comments is a flattened structure, and it uses CSS to display the nested comments. So, if we simply sort results, we will lose child comments."
https://github.com/RahulSinghalChicago/CourseProject	Project Proposal.pdf	"Project Proposal Names and NetIDs of our team: David Newman (davidn4) Rahul Singhal (rahuls11) (Captain) Charlie Truong (ctruong4) What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? Often, popular websites like news.ycombinator.com (Hacker News) are simply presented and lack even rudimentary ways to search and filter the information presented. Hacker News sorts posts with an algorithm that takes into account the age of the post and the number of upvotes it has received. The problem with this regime is that it fails to take into account the preferences of the user. We propose to solve this problem by soliciting information from the user and incorporating it into a search and/or filtering scheme. We plan to focus on jobs' postings: ""Ask HN: Who is hiring?"" and ""Ask HN: Who wants to be hired?"" posts. We anticipate implementing a web extension that ranks job postings to take into account the users' profession and technology stack preferences. Briefly describe any datasets, algorithms or techniques you plan to use Hacker News posts are available through regular SQL queries at: https://cloud.google.com/bigquery/public-data There is a realtime API available as well: https://github.com/HackerNews/API We plan to explore the use of topic modeling algorithms and associated techniques like BM25, stemming, plsa, etc. to accomplish our goal of surfacing relevant information to the user. How will you demonstrate that your approach will work as expected? As compared to the default Hacker News ranking system, our extension will surface more relevant information to the user. This will be demonstrated by comparing the default ranking system to our own. Which programming language do you plan to use? Javascript and python. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Pre-proposal planning meetings: 4 hours x 3 people * Develop ranking function: 8 hours x 3 people * Create chrome extension: 8 x 3 people * Create deploy pipeline of extension to Chrome store * Input form from user * Update HackerNews page to reflect ranking result"
https://github.com/RahulSinghalChicago/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Setup Dependencies To install the app for development, first ensure you have NodeJS available. Follow these instructions to install Node Version Manager (NVM). Then run the following command to install NodeJS. nvm install 14.18.1 Then ensure you have the package manager yarn installed by doing npm install -g yarn. Next, navigate to the hacker-news-jobs directory and run yarn install. That will install all the depenencies to build the extension. To build the extension, run yarn build. That will output build files to hacker-news-jobs/build. Open Chrome and go to the url chrome://extensions. Select ""Load unpacked"" to install this local extension from the directory hacker-news-jobs/build. The extension should be available in your Chrome browser now. To test the install, go to Hacker News, then open the extension and click ""Send Message."" The links on Hacker News should turn red. After updating files, you need to run yarn build and reload the specific extension again in chrome://extensions. Key File Structure The main files are hacker-news-jobs/src/App.tsx and hacker-news-jobs/src/chrome/content.ts. App.tsx is a pop-up defined using the framework React. We can define our user form there. It has a simple button that communicates with the content.ts script, which gets loaded into the Hacker News page. The file content.ts listens for a message from the extension and executes a function. There's other options for also running a background script. The hacker-news-jobs/manifest.json file provides the extension information for Chrome. It identifies what file should get treated as the extension pop-up vs injected into specific pages. The .ts file extension means a file is TypeScript. The buid process transpiles TypeScript to JavaScript that the browser can understand. It will also convert React components to elements the browser can handle. HN API guide HN API framework code"
https://github.com/Kartikp2/CourseProject	Progress_Report.md	"1) Which tasks have been completed? Successfully linked transcript to video segments for Coursera course CS 410 Successfully completed indexing and ranking. We still need to do some finetuning. 2) Which tasks are pending? Linking the ranked queries back to the text segments UI (50 % complete). UI is built on top of (React + Bootstrap ) framework and allows the following: * Ability to search the videos based on a query term * Display the appropriate video sub-segments as ""links"" under the ""Search results"" section. Backend (started) : A REST endpoint is created to perform the search. Further work needs to be done to integrate the ranker component and then tie back the results to the video subsegments. 3) Are you facing any challenges? Facing challenges with linking queries and text segments Issues with extracting the proper Coursera links along with text Missing some data, most likely due to different srt format"
https://github.com/Kartikp2/CourseProject	Project_Proposal.md	"Abstract: Build a search engine that can support video segment search for Courseera lecture videos with transcripts. This would allow an user to type in a query and see a ranked list of short video segments so that the user can precisely locate which segment to watch in a lecture in order to know more about a concept. We intend to build the search engine based on the scrapping/indexing/ranking concepts learnt from this course. The end solution will provide an User Interface for the user to type in the query and to view the results as links to the video segments. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Matthew Kryczka - kryczka3 Selvaganapathy Thirugnanam - st26 Diana Arita - dianama2 Kartik Patel - kartikp2 Captain - Kartik Patel What topic have you chosen? Intelligent Learning Platform- Build a search engine that can support video segment search. This would allow you to type in a query and see a ranked list of short video segments so that you can precisely locate which segment to watch in a lecture in order to know more about a concept. Why this is a problem : Apart from searching videos by titles, There is no easy way to search by video content. This makes harder for users who are interested in finding precise locations of the videos where a concept or a term is mentioned. How does it relate to the theme and class ? Assuming, Each Video segment as a document, we will try to address the ""Pull"" mode of the text retrival problem by ranking the these documents based on the transcripts available for each video segment and return the segments that best matches the query. Briefly describe any datasets, algorithms or techniques you plan to use ? We plan to use course lectures videos (i.e videos of CS 410: Text Information Systems) as the dataset. At a very high level, we tend to perform the following : 1. Scrape the videos along with the transcripts 2. Extract documents out of scrapped videos and transcripts and build an association between them 3. Use Tokenizer(Stemming and other normalization techniques) to extract lexical units (words) 4. Use an Indexer (inverted Index) for faster response from the Search Engine. 5. Perform Ranking/Scoring based on Probabilistic retrieval functions (Eg: BM25) So UI will be built for the user to submit the query. Once submitted, the query is further normalized and sent to ranking function to score against the data available in the index. The top ranked results will then be displayed in the UI. The results would provide links to the video segments. How will you demonstrate that your approach will work as expected? Demo video for the demonstration. We will try to search for concept as keywords used in our lectures and the results should contain video segments related to the concepts. Which programming language do you plan to use? scrapping, indexing, ranking ---> Python UI ---> Javascript frameworks (either Angular or React) Backend ---> Python/Flask Time Allocation Scraping the video text ---> 10 hours Indexing ---> 20 hours Ranking ---> 20 hours UI(Backend & Frontend) ---> 10 hours Mapping the video ---> 20 hours"
https://github.com/Kartikp2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jahaas/CourseProject	Project Progress Report - Team Coco.pdf	Project Progress Report UIUC: Text Information Systems Group Name: Team Coco Theme: Free Topics Specific Topic: AllenNLP Question-and-Answer Chatbot Captain/Project Leader (and sole team member): Joel Haas, joelah2@illinois.edu Project Task Configure a question-and-answer chatbot using the Allen Institute for Artificial Intelligence open- source library AllenNLP. Will now use native python instead of AllenNLP due to inability to successfully install AllenNLP python library. Project Update I was not able to successfully install the AllenNLP python library. I spent 3 hours researching the AllenNLP capability. I then tried to install the AllenNLP library so I could start learning the library. However, after 2 hours troubleshooting and researching the installation challenges that others have also had trying to install the library, I decided I needed to move on. So now I will configure a question-and- answer chatbot using native python libraries rather than the AllenNLP python library. Programming Language: Python Tools: Scikit-Learn, NLTK, Pandas Systems: Personal laptop Datasets: Deepmind NarrativeQA Reading Comprehension https://github.com/deepmind/narrativeqa Expected Outcome: NLP pipeline for a Chatbot Evaluation Methodology: Compare the algorithm's predicted answers against the idea answers given in the annotated question and answer dataset Completed Tasks by Workload (13 hours): 3 hours - AllenNLP research 2 hours - Troubleshooting installation of allennlp python library 2 hours - Research labeled question / answer datasets 6 hours - Cleaning and preparing datasets Pending Tasks by Workload (15 hours): 7 hours - Configure and test Q&A NLP pipeline using TF-IDF and cosine similarity 3 hours - Evaluate performance and iterate 5 hours - Documentation and reporting requirements Challenges Preparing (wrangling) the datasets proved more challenging than expected. I have made quite a bit of progress so far, but still have more formatting remaining to get the data (the questions and the summary documents that are used for searching for the answers to the questions) into the required format for the TF-IDF and cosine similarity algorithms in my NLP pipeline.
https://github.com/jahaas/CourseProject	Project Proposal - Team Coco.docx	Project Proposal UIUC: Text Information Systems Group Name: Team Coco Theme: Free Topics Specific Topic: AllenNLP Chatbot Captain/Project Leader (and sole team member): Joel Haas, joelah2@illinois.edu Proposal Task Configure a chatbot using the Allen Institute for Artificial Intelligence open-source library AllenNLP Description AllenNLP is an open-source library developed by the Allen Institute for Artificial Intelligence. The library supports developers wanting to build high quality deep learning NLP models. The Allen Institute's goal was to simplify the deep learning NLP model building process. They do this by providing an abstraction layer for common components and models. This abstraction layer enables data scientists to easily configure, run, and manage NLP experiments. The abstraction layer is also intended to remove the developer from the underlying deep learning complexities, reduce the amount of code that needs to be written, and at the same time, increase re-use and flexibility of configured NLP pipelines across different use cases. The libraries are built on top of PyTorch, and AllenNLP provides APIs for common NLP tasks, so the developer doesn't need to write code for them. Examples include tokenizing the text, converting each text ID into a vector, and then creating a single vector for each input text, where the output of each is a tensor. These pre-processing steps are encapsulated in a Model constructor and AllenNLP handles the various data mappings, persistence, and retrieval. Users can adjust these Model constructor parameters as needed. In this project, I want to explore the AllenNLP library and how to configure a deep learning NLP pipeline specifically focused returning answers to user questions. Additional Information Interest: The Allen Institute for AI developed this library to ease the pain of configuring deep learning NLP pipelines so developers can focus on tackling hard AI problems. I want to explore the library and how the abstraction layer works. If it really is as they describe, it could be ground-breaking for NLP developers. Programming Language: Python Tools: AllenNLP library Systems: Personal laptop Datasets: Will need to find one to use Expected Outcome: Deep learning NLP pipeline for a Chatbot Evaluation Methodology: Manual inspection + comparing against existing AllenNLP chatbots Expected Workload by Task: 5 hours - AllenNLP research 5 hours - Configure and test simple AllenNLP pipeline 2 hours - Find datasets for training/testing 15 hours - Configure and debug deep learning NLP pipeline for a chatbot 5 hours - Evaluate performance and iterate 5 hours - Documentation and reporting requirements
https://github.com/jahaas/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Group Name: Team Coco Theme: Free Topics Specific Topic: AllenNLP Chatbot Captain/Project Leader (and sole team member): Joel Haas, joelah2@illinois.edu Proposal Task: Configure a chatbot using the Allen Institute for Artificial Intelligence open-source library AllenNLP
https://github.com/adp12/CourseProject	CS 410 Progress Report.docx	CS 410 Progress Report November 15, 2021 Team AHR Anthony Petrotte (adp12@illinois.edu) Hrishikesh Deshmukh (hcd3@illinois.edu) Rahul Jonnalagadda (rjonna2@illinois.edu) Report To reiterate, the goal of our project is firstly, to analyze the financial news cycle in different time intervals to create a time interval sentiment metric on particular global securities. Secondly, we will compile the intermediate sentiment results during the metric calculation into a time series dataset that can be compared to price movement in the underlying security. Throughout the process of completing the project, our group has collaborated to set up regular meeting times, which has led to timely organization. We have also identified and distributed key tasks during those meetings, which has tremendously increased our efficiency as a group. Our team has made consequential headway on our final project and the deliverables we had outlined in the project proposal. We began by setting up our group repository and creating the skeleton structure of our project files. The first portion of the project we have completed has been the utilization of the News API Client and the Google API to retrieve textual information. We have successfully compiled multiple news sources and retrieved information about stock tickers, stock prices, company name, and more using user-generated queries. The next component of our project that we have made progress towards has been determining the relevance of the textual information we retrieve. We wanted to create relevancy scores for our retrieved documents using a Python library with an implementation of BM25. However, we faced compatibility issues due to conflicts between libraries and the specific Python version installed. We resolved this issue by successfully implementing a modified version of BM25 within our own codebase, which we use by looping through the HTML sections and then using BM25 to establish relevance to the target. As a result, our group has also been able to classify retrieved documents and give them a relevancy score. The upcoming tasks we have to complete is to subsect relevant articles into their relevant and irrelevant parts, which will lead to smaller, reviewable sub-document sets. We have completed a rough implementation of this task, which includes a model, tokenizer, and classifier. Next, our team will develop a weight metric centered around the subject count, where the assigned weight is lowered with additional subjects, while a higher weight is assigned if the user target is the only subject of the document. Finally, we will use huggingface pre-trained models to analyze the sentiment of the relevant subset. The challenge with using these pre-trained models will be the need to review the output of the prebuilt models to see if they capture sentiment specific to the financial terminology our project is focused on. Overall, we are on track to complete the final project and its key deliverables on time.
https://github.com/adp12/CourseProject	CS 410 Progress Report.pdf	CS 410 Progress Report November 15, 2021 Team AHR * Anthony Petrotte (adp12@illinois.edu) * Hrishikesh Deshmukh (hcd3@illinois.edu) * Rahul Jonnalagadda (rjonna2@illinois.edu) Report To reiterate, the goal of our project is firstly, to analyze the financial news cycle in different time intervals to create a time interval sentiment metric on particular global securities. Secondly, we will compile the intermediate sentiment results during the metric calculation into a time series dataset that can be compared to price movement in the underlying security. Throughout the process of completing the project, our group has collaborated to set up regular meeting times, which has led to timely organization. We have also identified and distributed key tasks during those meetings, which has tremendously increased our efficiency as a group. Our team has made consequential headway on our final project and the deliverables we had outlined in the project proposal. We began by setting up our group repository and creating the skeleton structure of our project files. The first portion of the project we have completed has been the utilization of the News API Client and the Google API to retrieve textual information. We have successfully compiled multiple news sources and retrieved information about stock tickers, stock prices, company name, and more using user-generated queries. The next component of our project that we have made progress towards has been determining the relevance of the textual information we retrieve. We wanted to create relevancy scores for our retrieved documents using a Python library with an implementation of BM25. However, we faced compatibility issues due to conflicts between libraries and the specific Python version installed. We resolved this issue by successfully implementing a modified version of BM25 within our own codebase, which we use by looping through the HTML sections and then using BM25 to establish relevance to the target. As a result, our group has also been able to classify retrieved documents and give them a relevancy score. The upcoming tasks we have to complete is to subsect relevant articles into their relevant and irrelevant parts, which will lead to smaller, reviewable sub-document sets. We have completed a rough implementation of this task, which includes a model, tokenizer, and classifier. Next, our team will develop a weight metric centered around the subject count, where the assigned weight is lowered with additional subjects, while a higher weight is assigned if the user target is the only subject of the document. Finally, we will use huggingface pre-trained models to analyze the sentiment of the relevant subset. The challenge with using these pre-trained models will be the need to review the output of the prebuilt models to see if they capture sentiment specific to the financial terminology our project is focused on. Overall, we are on track to complete the final project and its key deliverables on time.
https://github.com/adp12/CourseProject	CS410 Project Proposal.docx	CS 410 Project Proposal October 24, 2021 Team AHR Members Anthony Petrotte (adp12@illinois.edu) Hrishikesh Deshmukh (hcd3@illinois.edu) Rahul Jonnalagadda (rjonna2@illinois.edu) Detailed Description The goal of our project is firstly, to analyze the financial news cycle in different time intervals to create a time interval sentiment metric on particular global securities. This would be a useful tool or addition to the task of stock screening, and could be implemented as an addition to a computational trading strategy. Secondly, we would compile the intermediate sentiment results during the metric calculation into a time series dataset that can be compared to price movement in the underlying security. This dataset would potentially have many uses, including the possibility to aid in identifying securities that are more prone to volatility from individual (and potentially more naive) investors. Our project would be considered a free topic relevant to the course that has a novel and useful purpose. Expected Outcomes A usable metric that could be used to gauge the financial news sentiment within a specified time interval. Dataset generation for statistical analysis between news sentiment and asset prices within the specified time interval. Toolsets Languages Python This is the most reasonable language to use for our project. It includes various libraries that simplify each major task. Additionally, as this class has been taught in Python, it would facilitate our work to reuse needed components from the coursework. Potential Python Libraries Numpy Beautifulsoup requests Metapy Pandas Tensorflow tkinter Huggingface For prebuilt NLP models Potential News APIs Google News Polygon.io Evaluation Due to the ambiguity of textual information, much of the evaluation will have to be done by hand. This will be made possible by creating datasets containing source urls and records of corresponding 'sub-document' sections and the weights and sentiment scores used to calculate the total metric. The ability to go through the sources will allow us to debug, optimize and empirically judge the accuracy of the approach. Planned Approach Set Target Compile News Sources Centered on Target (est. 12 hours) Utilize news APIs to retrieve textual information Due to simplicity of stock ticker symbols, need to have function to adjust query to include company name and potentially helpful key phrases Analyze Sources (approx. 45 hours) Determining Relevance (est. 30 hours) Problem: Most financial news articles are about more than one thing and many are irrelevant, can't gauge the relevance or sentiment on the whole article 2 Rounds to gauge relevance before analyzing sentiment Document Relevance Established by query from API Based on document relevance, find sections containing relevant information (having to do with target) Compile relevant sections into smaller reviewable sub-document sets Scanning Source Information Could use predefined list of 'Subjects' to determine change of focus Identify subject, until new subject detected, all information assumed to be relevant towards initial subject Could use an implementation of PLSA Could use an implementation of BM25 loop through html sections, use BM25 to establish relevance to target Weight metric Subject Count Weight lowered with additional subjects, highest weight if Target is only subject of document If Subject Count is >1 Target Count in document (or document section) Target Placement in document Higher weight placed towards top Analyze Sentiment of relevant set (est. 15 hours) Utilize huggingface pre-trained models Need to review the output of prebuilt models to see if they capture sentiment specific to financial lingo May potentially re-train existing model to properly gauge sentiment Output of the sentiment analysis should be directly applicable to the metric 1 to -1 100% positive to 100% negative sentiment Combine Weighted Relevance and Sentiment to create Metric (est. 3 hours) Workload This project will require each step in the Planned Approach section to be relatively reliable, and thus has potential to quickly reach the 20N (60) hours required per N members of the group. Due to the inconsistent nature of both textual data and the html layout of various news websites, creating a reliable pipeline may require extensive debugging to ensure the data used to calculate weights and sentiments is relatively uniform and reviewable. The estimated amount of hours are included at the major sections of the Planned Approach, based on our team of 3 members. Our Project Repo https://github.com/adp12/CourseProject.git Cited Sources https://github.com/huggingface/transformers https://github.com/mkhorasani/Trading_Sentiment_Analyzer/blob/master/TSA%20v1.0.py https://newsapi.org/s/google-news-api https://docs.google.com/document/d/1ubzdWekH2WLzft-IaSnkYflKrR7DqW-X7WsWiG30loI/edit#
https://github.com/adp12/CourseProject	CS410 Project Proposal.pdf	CS 410 Project Proposal October 24, 2021 Team AHR Members * Anthony Petrotte (adp12@illinois.edu) * Hrishikesh Deshmukh (hcd3@illinois.edu) * Rahul Jonnalagadda (rjonna2@illinois.edu) Detailed Description The goal of our project is firstly, to analyze the financial news cycle in different time intervals to create a time interval sentiment metric on particular global securities. This would be a useful tool or addition to the task of stock screening, and could be implemented as an addition to a computational trading strategy. Secondly, we would compile the intermediate sentiment results during the metric calculation into a time series dataset that can be compared to price movement in the underlying security. This dataset would potentially have many uses, including the possibility to aid in identifying securities that are more prone to volatility from individual (and potentially more naive) investors. Our project would be considered a free topic relevant to the course that has a novel and useful purpose. Expected Outcomes * A usable metric that could be used to gauge the financial news sentiment within a specified time interval. * Dataset generation for statistical analysis between news sentiment and asset prices within the specified time interval. Toolsets * Languages * Python # This is the most reasonable language to use for our project. It includes various libraries that simplify each major task. Additionally, as this class has been taught in Python, it would facilitate our work to reuse needed components from the coursework. * Potential Python Libraries * Numpy * Beautifulsoup * requests * Metapy * Pandas * Tensorflow * tkinter * Huggingface # For prebuilt NLP models * Potential News APIs * Google News * Polygon.io Evaluation Due to the ambiguity of textual information, much of the evaluation will have to be done by hand. This will be made possible by creating datasets containing source urls and records of corresponding 'sub-document' sections and the weights and sentiment scores used to calculate the total metric. The ability to go through the sources will allow us to debug, optimize and empirically judge the accuracy of the approach. Planned Approach * Set Target * Compile News Sources Centered on Target (est. 12 hours) * Utilize news APIs to retrieve textual information * Due to simplicity of stock ticker symbols, need to have function to adjust query to include company name and potentially helpful key phrases * Analyze Sources (approx. 45 hours) * Determining Relevance (est. 30 hours) # Problem: Most financial news articles are about more than one thing and many are irrelevant, can't gauge the relevance or sentiment on the whole article # 2 Rounds to gauge relevance before analyzing sentiment # Document Relevance # Established by query from API # Based on document relevance, find sections containing relevant information (having to do with target) # Compile relevant sections into smaller reviewable sub-document sets # Scanning Source Information # Could use predefined list of 'Subjects' to determine change of focus # Identify subject, until new subject detected, all information assumed to be relevant towards initial subject # Could use an implementation of PLSA # Could use an implementation of BM25 # loop through html sections, use BM25 to establish relevance to target # Weight metric # Subject Count # Weight lowered with additional subjects, highest weight if Target is only subject of document # If Subject Count is >1 # Target Count in document (or document section) # Target Placement in document # Higher weight placed towards top * Analyze Sentiment of relevant set (est. 15 hours) # Utilize huggingface pre-trained models # Need to review the output of prebuilt models to see if they capture sentiment specific to financial lingo # May potentially re-train existing model to properly gauge sentiment # Output of the sentiment analysis should be directly applicable to the metric # 1 to -1 # 100% positive to 100% negative sentiment * Combine Weighted Relevance and Sentiment to create Metric (est. 3 hours) Workload This project will require each step in the Planned Approach section to be relatively reliable, and thus has potential to quickly reach the 20N (60) hours required per N members of the group. Due to the inconsistent nature of both textual data and the html layout of various news websites, creating a reliable pipeline may require extensive debugging to ensure the data used to calculate weights and sentiments is relatively uniform and reviewable. The estimated amount of hours are included at the major sections of the Planned Approach, based on our team of 3 members. Our Project Repo https://github.com/adp12/CourseProject.git Cited Sources https://github.com/huggingface/transformers https://github.com/mkhorasani/Trading_Sentiment_Analyzer/blob/master/TSA%20v1.0.py https://newsapi.org/s/google-news-api https://docs.google.com/document/d/1ubzdWekH2WLzft-IaSnkYflKrR7DqW- X7WsWiG30loI/edit#
https://github.com/adp12/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team AHR Members * Anthony Petrotte (adp12@illinois.edu) * Hrishikesh Deshmukh (hcd3@illinois.edu) * Rahul Jonnalagadda (rjonna2@illinois.edu)
https://github.com/IanG89998/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/daisy91530/CourseProject	README.md	Applying BERT on Facebook Children's Book Test Abstract Our task is to apply BERT on Facebook Children's Book Test Dataset. In the dataset, each question is constructed by taking 20 consecutive sentences from the book text and leaving the 21st as the query statement. A word from the query is selected and masked, and the model is tasked with selecting which word from the text (of the chosen type) should be used to fill this placeholder in the query. Our goal is to measure how well language models can exploit wider linguistic context and ultimately create a model that is capable of answering a query based on a short story. Project Proposal The project proposal of this project can be found here. Progress Report The progress report of this project can be found here.
https://github.com/dkrovi2/CourseProject	progress-report.pdf	Progress Report: Build Experience Profiles from Resumes High-level Tasks Tasks Completed Tasks In Progress Tasks Pending Current Challenges High-level Tasks Please refer to the high-level tasks identified for this project in the proposal document. Tasks Completed Gather representative data set for training and evaluation This task has been completed. The dataset gathered contains resumes from multiple departments. We are confident this corpus is large enough to demonstrate the core idea of the project. Tasks In Progress Parsing engine to parse resumes and job descriptions Two members are currently working on implementing the parsing engine module that takes resumes and extracts information, in a batch and online mode job-description and extracts information in online mode for matching/searching at runtime Analysis engine to analyze resumes Two members are currently working on implementing the analysis engine, using the interface defined between the parsing and analysis engine, w.r.t the output format of the parsing engine. Tasks Pending The following tasks are pending start: Scoring engine Implement the Apache Lucene's TFIDFSimilarity to score the documents. Basic UI User specifies a job-description to get the ranked resumes as response Documentation Current Challenges A thorough analysis on how to give more weight to a skill that occurs less number of times in a resume, but the candidate worked on those skills for multiple years.
https://github.com/dkrovi2/CourseProject	README.md	Build Experience Profile from Resumes What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. alokk3@illinois.edu dkrovi2@illinois.edu jsaxena3@illinois.edu rathi9@illinois.edu What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? In this project, we use text extraction and retrieval for the following functions: * Parse resumes in doc and pdf format * Parse job descriptions in doc and pdf format * Build an analysis engine to extract experience details of a candidate on various tools and technologies * Rank the available set of resumes based on the skill set specified in the job description The current keyword based search used by many online websites might not be entirely accurate, as the correlation between the skills and the experience is often missing. For example, for a skill set of 'Spark', instead of just searching for the keyword 'Spark' in the resume, we want to know (for scoring purpose) - if the employee worked in Spark for X number of years, - did he have experience on Spark, in multiple organizations. We then create a score for each profile/resume based on the skill set mentioned in the query and rank them in order of score (highest to lowest). Which programming language do you plan to use? We will use the standard text retrieval tools and programming APIs (MeTA, python, numpy etc) with a customized algorithm to score each resume. Please justify that the workload of your topic is at least 20 * N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The following are the steps and key milestones for this project: | Task | Time needed | ETA | |:--------------------------------------------------------------|-------------:|----------------:| | Gather representative data set for training and evaluation | 8 hours | Nov 8 | | Parsing engine to parse resumes and job descriptions | 20 hours | Nov 15 | | Progress report | 2 hours | Nov 15 | | Analysis engine to analyze resumes | 30 hours | Nov 22 | | Scoring engine to match resumes to provided job description | 30 hours | Nov 29 | | Basic UI to search for resumes matching a job description | 24 hours | Dec 5 | | Software documentation | 8 hours | Dec 9 | | Total |122 hours | | Contributors alokk3@illinois.edu dkrovi2@illinois.edu jsaxena3@illinois.edu rathi9@illinois.edu
https://github.com/xingyuq2/CourseProject	progress-report.pdf	Progress Report 1. Which tasks are completed 1. Scraper of Netflix with video name, image url, netflix url, synopsis, casts, creators, mood tag 2. Storage of information into csv file and reading from csv file 3. Web api using flask with get request including get one video info by id, get all videos info, get recommendation by video id 4. Basic recommender system for content based recommendation of a video 2. which tasks are pending 1. Web page content rendering using react 2. Improvement of the recommender system 3. Any challenges 1. Scraping dynamically rendered data using button click with driver 2. Storing information of python dictionary into csv file 3. Reading information from csv file into pandas 4. Process data using pandas 5. Building of a basic recommender system
https://github.com/xingyuq2/CourseProject	project-proposal.pdf	"Project proposal Team member: Name: Xingyu Qiu NetID: xingyuq2 Captain: Xingyu Qiu Theme: Free topic Topic: Web application with recommender system Description: Web applications that shows list of TV shows and movies from Netflix. User can click the item to see the detailed information including video name, description, genres, release year, cast, creators. When user select a item, recommended videos will also show in the page using content based filtering. Motivation: People always don't know what kind of movies or TV shows they may like to watch due to the large number of choices. I want to build a web application for users to first browse what they like to watch, and then recommend them with similar videos using content based filtering. Task: Scraper for Netflix to retrieve all the information and store them in csv file. Build web API using flask for GET operations. Build web page using HTML, JavaScript, CSS. Build recommender system based on the information of videos. Expected outcome: Web page showing the list of videos with clickable image and name to navigate to a new detail page of that video with recommended other videos. Evaluation: Compare the result of recommended videos with Netflix videos in ""More Like This"" section of a specific video to evaluate the recommender system. Tools: Flask, BeautifulSoup, pandas, react Programming languages: Python, HTML, JavaScript, CSS Main tasks and estimated time: Scraper 4hrs API 4hrs Web page structure 4hrs Content render 4hrs Recommender system 4hrs"
https://github.com/xingyuq2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/wujameszj/CourseProject	progress report.pdf	UIUC CS410 Project Progress Report Zhou Wu zjwu2 Tasks completed: - Experimented and became familiar with the top2vec algorithm/library, including its main dependencies UMAP and HDBSCAN (dimension reduction and clustering algorithms) - Explored the genism python package for LDA - Developed and deployed Streamlit web app; incorporated top2vec into web app Tasks pending: - Develop scripts to scape select websites for text data - Learn advanced usage of Streamlit and incorporate additional features/algorithms into web app - Record presentation and write final report Challenges: Once I actually incorporated topic modeling into the web app, I realized just how long it takes to run. Depending on the dataset, it could take over 20 minutes, which is probably too long for users/reviewers. One potential solution could be to precompute the models offline and then simply load them for the web app, however, that would not allow the users to select custom date ranges to scrape websites, which was one of my hopes for this web app. Another solution is to limit the size of the dataset, thereby reducing the most time consuming step of the top2vec topic modeling algorithm, which appears to be the embedding step. The web app (incomplete) is hosted here: https://share.streamlit.io/wujameszj/courseproject/main/main.py
https://github.com/wujameszj/CourseProject	project proposal.pdf	What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? I, Zhou Wu (zjwu2), am working alone on this project on topic modelling, using Python and potentially R. Topic modelling is an important task in the field of information retrieval. When users such as data scientists and business analysts wish to browse or search through a collection of text, they do not always know exactly what to search for or what the best approach is to find what they need, especially if the collection is not curated. This is where topic modelling comes in: not only does it find latent topics - which serve as a form of summary of the text collection - but in the process, it also creates a model capable of finding similar documents, thereby enabling efficient search through the collection afterwards. In other words, topic modelling not only provides users with useful semantic insights, but also optimizes the subsequent task of retrieving information of value to users. The outcome/deliverable of this project is a program/web app that takes an existing text collection, or scrapes from a list of websites (reddit, sites hosting free ebooks, academic papers, movie reviews, etc.), performs topic modelling, creates summary visualizations, and allow users to perform queries to find documents under specific topics. I will be examining a variety of topic modelling techniques - including top2vec, LDA, PLSA, and lda2vec - and integrate the best one - or multiple algorithms and allow users to choose - into the program. If time permits, additionally, I would like to implement a document search functionality using the BM25 algorithm, as well as a recommender system based on user input. I will be using libraries such as nltk and spacy for NLP processing, sklearn for document vectorization, and genism and top2vec for topic modelling. I will evaluate the program by testing on various types of text collections and examining the coherence of the returned topics. Moreover, after each instance of topic modelling, I will also run different queries on the text collections to see if useful information is returned the users. Hrs Tasks 1. 3 Develop a script to scrap specific relevant websites 3 Explore, clean, and preprocess data from various sources 2. 5 Experiment various topic modelling approaches; test different algorithm parameters 2 Integrate best algorithm and parameters into code base 3. 5 Develop web app for visualization and topic/document query 4. (5) (If time permits) Allow user to select topics or documents/items of interests, and build a recommender system based on user input 5. 2 Write progress report 3 Write final report (compile results, create infographics) 1 Clean and package source code / jupyter notebooks 2 Produce presentation / demo video
https://github.com/wujameszj/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rfonod/CS410-CourseProject	Progress_Report.pdf	Project Progress Report CS 410 - Text Information Systems Fall 2021 Robert Fonod and Rhaam Rozenberg Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, 61820 USA I. Background T oday, more than ever, the Internet is inundated with contrasting opinions. Varying opinions might negatively infuence certain users' moods and lead to their exhaustion or frustration. Our proposed browser extension has the goal to predict the sentiment of the currently viewed web page. The predicted sentiment (positive, negative, or neutral) shall help the user to decide whether to engage or avoid reading a (news) article or blog post. We aim to communicate the predicted page sentiment by utilizing emoticons, displaying the predicted sentiment probability in a numerical form, and/or by adapting the browser's theme color. On user's request, the extension might also show examples of negative and/or positive sentences present on the currently viewed web page. II. Current State of Progress The numbered list below aims at answering the three questions required for the Project Progress Report submission. 1) Which tasks have been completed? In general, we have spent time researching diferent topics that will be needed to complete the project. The areas we researched were related to topics such as: how to scrape web pages and extract meaningful text from a HTML document; how to perform sentiment analysis using JavaScript using either an exiting Application Programming Interface (API) or an existing library; how to create and customize a browser extension. We also discussed who will be responsible for which part of the tool development. In particular, the following tasks have been accomplished to date1: * A suitable external JavaScript library for text sentiment analysis has been identifed and studied. The library source code is publicly available2 and can be reused or modifed under Apache License 2.0. The library implements two neural network based models to predict the input text sentiment. One is based on a 1D Convolutional Neural Network (CNN) and the second on Long Short Term Memory (LSTM) network architecture. The predicted value ranges from 0 (negative) to 1 (positive). The model has been trained by Google on 25,000 movie reviews scraped from Internet Movie Database (IMDb). The advantage of this 1The progress report submission date, December 15, is considered as the reference date. 2https://github.com/tensorfow/tfjs-examples library is that it can be used to perform sentiment analysis on text using the Layers API of TensorFlow.js. * A working sentiment analysis demo has been implemented in JavaScript using the above-mentioned library's API call. The demo is currently embedded into a standalone HTML website. Instead of scraping the content of a web page, the demo takes an input text from a HTML text area/feld. The input text is frst parsed into a list of standalone sentences. Then, for each sentence in the list, a sentiment score (value between 0 and 1) is predicted/computed. The fnal input text sentiment score is computed by a simple arithmetic average of the sentence sentiment scores. The individual sentence scores are sorted and stored for future reuse (e.g., to display, upon user's request, the top 3 positive/negative sentences). * A few initial web-pages to test the sentiment analyzer have been identifed. Specifcally, two pages with a negative sentiment and two pages with a positive sentiment. The two web pages with the negative sentiment can be found at: - https://www.ctinsider.com/opinion/article/Opinion-Why-I-hate-Christmas-16614264.php - https://eu.usatoday.com/story/entertainment/music/2021/11/06/travis-scott-astroworld- what-we-know-mass-casualty-event/6317049001/ while the two pages with the positive sentiment can be found at: - https://www.salonprivemag.com/the-most-beautiful-places-to-visit-in-russia/ - https://www.cinemablend.com/streaming-news/the-best-movies-based-on-true-stories-to- watch-streaming * Appropriate resources have been identifed to aid the implementation of the browser extension. 2) Which tasks are pending? Our holistic estimate suggests that 40-60% of tasks are still to be implemented. Among others, we plan to complete the following pending tasks: * Adapt the current text sentiment analysis methodology for a browser extension environment. * Implement an algorithm that will scrape the HTML content of the currently opened web page. * Implement an algorithm that will extract meaningful content (text) from the scraped web page. * Implement an adequate User Interface (UI) for the browser extension that facilitates easy understanding of the sentiment by the user, without compromising the utility of the page. This might include various visual elements, such as emoticons, browser theme color changes, etc. * Defne and implement an adequate interface between the back-end (sentiment analysis) and the front-end (UI) solutions. * Demonstrate the intended functionality of the extension (see the project proposal document for more details). * Validate the sentiments of selected testing web-pages with the help of human annotators. 2 * Final project writing and presentation preparation and recording. 3) Are you facing any challenges? * A potential challenge might be the accuracy of sentiment predictions. This is because the considered neural model was trained on movie reviews only. Therefore, the sentiment predictions based on, for instance, generic news articles or topic-specifc blog posts might be highly unreliable. This will be evaluated during the tool demonstration phase. If needed, a hybrid approach based on the neural model and a custom bag-of-words representation might be implemented. * The sentiment prediction might take a considerable amount of time when the text content of the page is too large. Performance improving approaches will be considered in that case. For instance, a random subset of sentences from the page text corpus might be selected. The cardinality of the needed subset to reliable estimate the sentiment will need to be found via heuristics. 3
https://github.com/rfonod/CS410-CourseProject	Project_Proposal.pdf	"Group Project Proposal CS 410 - Text Information Systems Fall 2021 Robert Fonod and Rhaam Rozenberg Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, 61820 USA The numbered list below aims at answering all the questions required for the Project Proposal submission. 1) What are the names and NetIDs of all your team members? Who is the captain? This group project consists of two team members: * Member 1: Robert Fonod (NetID: rfonod2) * Member 2: Rhaam Rozenberg (NetID: rhaamr2) Member 1 (rfonod2) is designated as captain. The name of this team is: EU2. 2) What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? * Theme: Intelligent Browsing * Topic: Sentiment Analyzing Browser Extension * Problem Description: Today, more than ever, the Internet is inundated with contrasting opinions. Varying opinions might negatively infuence certain users' moods and lead to their exhaustion or frustration. Our proposed browser extension has the goal to predict the sentiment of the currently viewed web page. The predicted sentiment (positive, negative, or neutral) shall help the user to decide whether to engage in reading an article or blog post and potentially avoid an unpleasant experience. We aim to communicate the predicted page sentiment by changing the browser or page background color and/or displaying the predicted sentiment probability in a numerical form (e.g., by automatically changing the extension icon). * Relation to the Theme: The proposed project is directly related to Internet browsers and the targeted functionality (sentiment analysis) is deemed to add an extra layer of ""intelligence"" to existing browsers1. * Relation to the Class: The proposed project will leverage the acquired knowledge from the CS-410 class in multiple ways. To name a few examples from the course Syllabus, we will rely on concepts such as web scraping, data mining of meaningful content of the viewed page, content-based fltering, and sentiment categorization. 3) Briefy describe any datasets, algorithms or techniques you plan to use. We will mainly rely on the browser's API to modify the browser's behavior and to access web content. This will allow us to build an efective extension with a user-friendly interface. We will also rely on an existing 1For the sake of this project, we will only focus on developing a Google Chrome extension. implementation of a sentiment analysis algorithm. This will be either achieved by calling a designated (free) API or by adopting an existing library for this task2. The rest of the extension's back-end will mostly consist of custom-built functions to extract and mine the useful parts of the scraped html content. 4) How will you demonstrate that your approach will work as expected? To demonstrate that the browser extension works as expected, we will select a few web pages and use human judgment to identify the true sentiment expressed by these pages. Then, the browser extension will be tested on the same pages and the predicted sentiment will be compared with the human judgments. To reduce human bias, each member will perform his/her own judgment on the same set of test pages and the fnal judgments will be cross-validated (e.g., by a weighted sum). The correct functionality of the front-end part of the extension will be tested on various browser versions and/or operating systems. 5) Which programming language do you plan to use? The use of the following programming languages is anticipated: JavaScript, HTML, CSS. 6) Please justify that the workload of your topic is at least 20 x N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Table 1 summarizes the anticipated tasks and their estimated workload for this project. Table 1 Estimated workload of various tasks. # Task Description Time 1 User interface development 10 h 2 Web scraping and data mining 20 h 3 Selection and adaptation of sentiment analyzing algorithm 5 h 4 Visual communication of predicted sentiment to user 5 h 5 Test pages selection, judgments, and functionality demonstration 3 h 6 Report writing and presentation preparation 7 h S 50 h 2Custom-built sentiment analyzer is out of scope of this project. 2"
https://github.com/rfonod/CS410-CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/devinsburke/CourseProject	Progress Report.pdf	PROJECT PROPOSAL WHICH TASKS ARE COMPLETED? WHICH ARE PENDING? Below is the original list of tasks, with a status next to each. Task Assigned To Est. Hours Status Create dot-matrix, cosine similarity, bag-of-words representation, mean average precision, and stop-word functionality in JavaScript Devin Burke 15 Completed Create pre-defined Cranfield methodology data sets, measures, and relevance judgments Joshua Ray 5 In Progress Implement Cranfield data sets in JavaScript using mean average precision as a measure Joshua Ray 5 In Progress Create algorithm to score and rank sentences based on implemented techniques, and adjust according to success based on Cranfield evaluation Devin Burke 10 In Progress Determine methodology for filtering page document to only applicable content (i.e., filter out ads, navigation, etc.) Joshua Ray 5 Completed Create underlying Chrome browser extension and configurability of what percent of the document should be returned Joshua Ray 15 In Progress Research and implement algorithm for calculating reading time-savings based on summary versus entire document Devin Burke 5 Completed 60 ARE YOU FACING ANY CHALLENGES? We have resolved and/or are in the process of resolving the following challenges:  We had to create a method to identify and omit non-sentence-based content from consideration, such as captions of images, tables of data, navigation, advertisements, basic headings, etc.  We had to create a method to identify when a period is and isn't the end of a sentence, such as: 1.23, J. Edgar Hoover, Figure 1., St., is...bad, Ph.D., etc.  We are in the process of throttling the number of sentences returned for very long pages, so we aren't returning, e.g., 70 sentences for a 700-sentence page.
https://github.com/devinsburke/CourseProject	Project Proposal.pdf	PROJECT PROPOSAL WHAT ARE THE NAMES AND NETIDS OF ALL YOUR TEAM MEMBERS? WHO IS THE CAPTAIN? Team Name: Washington Football Team Name NetID Role Devin Burke devinb3@illinois.edu Captain Joshua Ray ray18@illinois.edu Team Member WHAT TOPIC HAVE YOU CHOSEN? WHY IS IT A PROBLEM? HOW DOES IT RELATE TO THE THEME AND TO THE CLASS? The topic our project will cover is intelligent browsing. Specifically, we will create a Google Chrome browser extension that summarizes the webpage the user is currently on. It will pull the most relevant/descriptive sentences from the page and display those as a bulleted summary to the user. The number of sentences shown will vary depending on the length of the document. The problem this browser extension will solve is efficient comprehension of the document with minimal time investment. This project relates to the topic and class because it performs text retrieval using algorithms and techniques learned in the course to augment a user's experience and make browsing more intelligent. DESCRIBE ANY DATASETS, ALGORITHMS OR TECHNIQUES YOU PLAN TO USE. We will implement the vector-space model using bag-of-words representation of each sentence of the document. We will then create a similarity matrix using dot-matrix and measuring the cosine similarity between each sentence, based on the weight of overlapping words. In order to implement the benefit of IDF, we will use a stop-word collection to minimize the impact of common words. We will display the estimated time saved by reading our summary versus reading the entire document, which will be based on a statistical average words- per-minute read provided by reputable sources. HOW WILL YOU DEMONSTRATE YOUR APPROACH WILL WORK AS EXPECTED? We will implement the Cranfield methodology of pre-defining a list of documents, relevance-judgments about the summarizing sentences, and measures as to success thresholds. Using these, we will implement tests that utilize mean average precision (MAP) to measure our success. WHICH PROGRAMMING LANGUAGE DO YOU PLAN TO USE? As this is a Google Chrome extension, we will use JavaScript. We will implement the algorithms and calculations within JavaScript without the aid of an existing NLP library, and these calculations will be performed client-side. The results will be shown in HTML5/CSS3. Thus, this browser extension should be very fast because it does not need to communicate with an external server, which is part of what makes it unique. PLEASE JUSTIFY THAT THE WORKLOAD OF YOUR TOPIC IS AT LEAST 40 HOURS. Task Assigned To Est. Hours Create dot-matrix, cosine similarity, bag-of-words representation, mean average precision, and stop-word functionality in JavaScript Devin Burke 15 Create pre-defined Cranfield methodology data sets, measures, and relevance judgments Joshua Ray 5 Implement Cranfield data sets in JavaScript using mean average precision as a measure Joshua Ray 5 Create algorithm to score and rank sentences based on implemented techniques, and adjust according to success based on MAP evaluation Devin Burke 10 Determine methodology for filtering page document to only applicable content (i.e., filter out ads, navigation, etc.) Joshua Ray 5 Create underlying Chrome browser extension and configurability of what percent of the document should be returned Joshua Ray 15 Research and implement algorithm for calculating reading time- savings based on summary versus entire document Devin Burke 5 60
https://github.com/devinsburke/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/masamip2/CourseProject	ProjectProgress.pdf	"CS 410 Text Information Systems 2021 Fall - Project Progress Causal Topic Modeling Team Information  Team Name: MP  Team Members: Masami Peak (NetID: masamip2, Email: masamip2@illinois.edu) - Captain Topic Information Description: This project is building a causal topic model for identifying hidden key topics in the MLB (Major League Baseball) articles correlated to the annual MVP (Most Valuable Player) winner. Completed Tasks: 1. Obtaining Datasets: Prepared a python script to perform web crawling on explicitly defined URLs, such as Reuters, MLB, Wall Street Journals, NY Times and ESPN, to fetch MLB articles published in the month between April and October of the year between 2011 and 2021. 2. Data Cleaning and Preprocessing: Cleaned the dataset to fix any data issues and transforming it to well-formed dataset for topic modeling. 3. Dictionary and Corpus Creation: Created a dictionary and a corpus (TermID, Frequency) for a topic model. 4. Topic Model Building and Evaluation: Built a topic model LDA (Latent Dirichlet Allocation) and evaluated the quality of the model. Pending Tasks: 1. Topic Model Visualization and Analysis: Visualizing and analyzing the model for the project report. 2. Software Usage Tutorial Presentation: Documenting and presenting software usage/implementation. Challenges: 1. Web Crawling: Some sites (e.g. mlb.com) had 2 popups that interrupt web crawling. The time those popups appear did not seem consistent. I needed to set the time by which the popups should have shown up (e.g. 30 seconds) to handle the interruption. Also, some advertisements seemed interfering web crawling, so I needed to run the script multiple times to retrieve the articles on the site. 2. Data Cleaning and Preprocessing: Some manual inspections were needed to build a better vocabulary for a topic model. For example, a part of a sentence 'Buffalo, N.Y., the' would become 'buffalo n y the' after lowercasing and replacing any special character with a whitespace were applied. To achieve the ideal result: 'buffalo ny the', more preprocessing was required to apply. 3. Stop-words Removal, Lemmatization and Stemming: The order of applying stop-words removal, lemmatization (grouping together the inflected forms of a word) and stemming (reducing the derived forms of a word) gave slightly different effects on the vocabulary creation. For example. if a word 'player' is one of the stop-words, then stop-words removal is applied first and lemmatization is applied second, the word 'player' can be still found in text because lemmatization has transformed 'players' (plural) to 'player' (singular). I needed to try different orders of applying the document processes and different types of lemmatization to see the best result. 4. Model's Hyperparameter Tuning: Configuring the topic model's hyperparameters was done by building the model several times while the performance of the outcome was observed. The main topic ""Baseball"" is already known and analyzing the outcome of the optimized LDA model for finding the meaning of the hidden topics is somewhat challenging."
https://github.com/masamip2/CourseProject	ProjectProposal.pdf	CS 410 Text Information Systems 2021 Fall - Project Proposal Causal Topic Modeling Team Information  Team Name: MP  Team Members: Masami Peak (NetID: masamip2, Email: masamip2@illinois.edu) - Captain Topic Information Description: This project is building a causal topic model for identifying hidden key topics in the MLB (Major League Baseball) articles correlated to the annual MVP (Most Valuable Player) winner. Tasks: 1. Obtaining Datasets: Preparing a python script to perform web crawling on explicitly defined URLs, such as Reuters, MLB, Wall Street Journals, NY Times and ESPN, to fetch MLB articles published in the month between April and October of the year between 2011 and 2021. 2. Data Cleaning and Preprocessing: Cleaning the dataset to fix any data issues and transforming it to well-formed dataset for topic modeling. 3. Dictionary and Corpus Creation: Creating a dictionary and a corpus (TermID, Frequency) for a topic model. 4. Topic Model Building and Evaluation: Building a topic model LDA (Latent Dirichlet Allocation) and evaluate the quality of the model. 5. Topic Model Visualization and Analysis: Visualizing and analyzing the model for the project report. Interest/Motivation: At the end of the MLB season, each MVP in the 2 leagues is determined by the voters in the Baseball Writers' Association of America and the MVP need not come from a division winner or other playoff qualifier. Due of this, topic modeling on the MLB articles published before the MVP announcement can be used to discover the trend topics correlated to the MVP for the year. Additionally, this technique can be applied for analyzing any other sports awards and the Academy Awards (the Oscar). Approach: MLB articles as documents can be retrieved from MLB related webpages. The most popular topic model LDA extracts the topics discussed in the documents. Visualizing top 30 relevant terms to each topic distributed over documents interprets the topic. Tools/Systems/Datasets: On Python environment in a local machine, a Python script performs web crawling for obtaining MLB article dataset with id, headline, summary, created and source columns. Then, the rest of the listed tasks are completed in Jupyter Notebook while the project report is prepared. Expected Outcome: This project is an experiment, but it is interesting to find out if there are hidden trend topics from one year to the next. Evaluation: In addition to human judgement, topic model can be measured by computing coherence score which indicates the interpretability between the topics and the relevant words inferred by the model. Development Environment Information Programming Language: Python using Jupyter Notebook Main Modules: bs4 for BeautifulSoup, selenium for webdriver, numpy, pandas, nltk, gensim, pyLDAvis, matplotlib for pyplot Workload Information (Total: 30 hours) 1. Obtaining Datasets: 4 hours 2. Data Cleaning and Preprocessing: 4 hours 3. Dictionary and Corpus Creation: 3 hours 4. Topic Model Building and Evaluation: 4 hours 5. Topic Model Visualization and Analysis: 15 hours
https://github.com/masamip2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tomasanthony/CourseProject	Course Project Proposal.pdf	Team Name: Muskrat Attack Team Member Name: Tomas Anthony NetID: tomasaa2 Team Captain: Tomas Anthony Chosen System: System Extension Sub Topic: ExpertSearch - Extracting relevant information from faculty bios Datasets: I plan on using mined text from faculty bios, previously performed in MP 2. Algorithms and Techniques: I plan on using BERT based NLP techniques to fine tune a NER classifier model to better structure and retrieve information from the faculty bios. I will demonstrate using F1-Score and other metrics for the classifier models performance that the NER exceeds the performance of the Stanford boilerplate NER. My code will communicate with the system by being integrated into the code base in the same way the current NER system is. If necessary, due to Github size limitations, I will upload the ML NLP model to a hosting service to be interacted with from there. Programming Language: Python Workload Justification: I am a single person and will need at least 20 hours to data mine and fine tune a classifier model. Data mining could take between 5-10 hours. Coding the classification model could take another 10 hours. Integrating this into the existing ExpertSearch code base could take another 5. Logistic work and miscellaneous coding could be 5-10 hours.
https://github.com/tomasanthony/CourseProject	Progress Report.pdf	Progress Report 1. Which tasks have been completed? a. Mining and curating training data for the proposed BERT model 2. Which tasks are pending? a. Training and fine tuning the BERT model b. Integrating the BERT model into existing code structures 3. Are you facing any challenges? a. No
https://github.com/tomasanthony/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/kellycosgrove/CourseProject	CS_410_Progress_Report.pdf	Kelly Cosgrove 11/15/2021 Pop Punk Song Search Project Progress Report 1) Which tasks have been completed? I have researched the most popular Pop-Punk artists over the last 30 years and have put together a corpus with which I am going to train my vector space model, using the lyricsgenius Python library. I have also researched the Gensim library which I am planning to build the VSM with, as this was the topic for my Tech Review. 2) Which tasks are pending? I still need to create the vector space model, create the Dash-based GUI through which users will be able to interact with the VSM, and create/use test cases on the VSM. 3) Are you facing any challenges? One challenge I've faced so far is the library by which I used to pull the song lyric corpus. I was originally planning to use a Python library called lyrics-corpora. However, upon testing this library out I quickly found that it was buggy and not well maintained, and I was unable to leverage it for the task at hand. Luckily, I was able to find another library called lyricsgenius which utilized the Genius website API. This library worked well to create the corpus.
https://github.com/kellycosgrove/CourseProject	CS_410_Project_Proposal.pdf	Kelly Cosgrove 10/24/2021 Pop Punk Song Search Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Kelly Cosgrove - kellyc4 - Captain 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? a. Pop Punk is a genre that many people listened to in their angsty teen years. Often times it's hard for us to recollect a song; rather, we remember a lyric, the theme, or a few key words from it. The goal of this project is to facilitate this rediscovery by building a vector space model-based search engine that allows a user to type these lyrics/themes/key words and receive a list of possible song matches. b. This project will be interesting because rediscovering the songs one listened to the most in High School can be a particularly nostalgic experience. c. I plan to build a corpus consisting of song lyrics from the most popular Pop-Punk bands from the last 30 years, use this corpus to code a vector space model, and then build a GUI for users to interact with the model, containing a search bar and model output. d. I will be using the Gensim Python library to build the vector space model and the lyrics-corpora Python package to build the corpus. I will be using the Dash library to build the GUI. e. I expect the user will be able to type key words into a search bar which will then be run through the model and the closest song matches will be returned on a GUI. f. I will evaluate my work by creating a set of queries with relevance judgements and computing the F measure. 3. Which programming language do you plan to use? a. Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. 1 hour - Research most popular Pop-Punk artists of last 20 years b. 4 hours - Research lyrics-corpora library and build corpus of Pop-Punk songs c. 8 hours - Research Gensim library and create vector space model d. 8 hours - Research Dash and create Dash application e. 2 hours - Evaluate using set of pre-judged queries
https://github.com/kellycosgrove/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/benpcorn/CourseProject	Progress_Report.md	"1 Tasks Completed This project consists of the following tasks, as outlined in the proposal (see readme.md): [x] Text Mining: Done (Python, BeautifulSoup, Proxy service) [x] Text Processing: Initial implementation done. Moving to Fine tuning. (NLTK) [x] LDA: In Progress: Initial implementation done. Moving to Fine tuning. (Gensim) [ ] Sentiment Anlysis: Not Started [ ] Chome Extension: Not Started 2 Pending Tasks At this time, I have a functional end-to-end prototype of topic analysis for any given Amazon product, running locally on my machine. The remaining work involves: Creating AWS Lambdas for my text mining and text processing (cleaning + topic modeling). Fine tuning of the text processor to further clean the review data Fine tuning of the LDA model. Namely, implementing an algorithm to determine the ideal number of topics based on Topic Coherence and Perplexity, Sentiment Analysis by Topic. Chrome Extension: create the extension that enables new UI on Amazon.com for an end-user to initiate the analysis with the Lambdas, and the work to show the topics on the website. 3 Challenges Blocked: I encountered a problem early on where I was getting blocked by Amazon due to the request volume when scraping reviews. To get around this, I had to utilize a 3P proxy service to avoid blocking. Using the WebScrapingAPI: https://www.webscrapingapi.com/ for the proxy feature (provide URL with params and it returns raw HTML through proxies). Speed: Amazon only shows 10 reviews per page, so products with a lot of reviews are taking a lot of time to scrape. I tried to mitigate this through multi-threading, but it was still pretty slow (~5 minutes for 600 reviews) - turned out 1) I didn't implement it correctly and 2) I/O operations can't be multi-threaded, rather they need to be run on multiple processors. Locally, I can implement multi-processor support to speed this up and will take this up as a new piece of work, if time allows. Topic Quality: The initial set of topics I'm seeing from products aren't great. It's difficult to deduce what the ""common topic"" is from the topic keywoards returned by Gensim. There is an approach outlined here (https://towardsdatascience.com/%EF%B8%8F-topic-modelling-going-beyond-token-outputs-5b48df212e06) that combines LDA with RAKE. I'm going to try this."
https://github.com/benpcorn/CourseProject	README.md	"Amazon Product Review Analysis (Chrome Extension) | Intelligent Browsing Group Name Vinnie Team Members Benjamin Corn (bcorn2@illinois.edu) | Team Leader Problem According to research performed by The Medill Spiegel Research Center (SRC) at Northwestern University, about 95% of customers read reviews before making a purchase[1] and 82% of customers actively seek out negative reviews when making a purchase decision[2]. With reviews playing such a critical role for customers, the method of writing and reading reviews has largely remained the same: reviews consist of a star rating (typically 1-5), review text, and optional media (photos/videos); sorted in a descending order by review date. While the overall star rating (e.g. 4.7 out of 5) and list of reviews is helpful, there is an opportunity to surface additional insights about the product derived from customer reviews to 1) enable customers to make better purchasing decisions in less time and with greater confidence, and 2) enable companies (product managers, designers, engineers, etc) understand what customers really think about their products, and what the top issues and requests are for the product. Solution This project will focus on delivering a solution in two areas: 1. Automatic Topic Grouping: e.g. Screen Resolution, Battery Life, and Scratch Resistance for a mobile phone 2. Sentiment Breakdown by Topic: e.g. 70% positive sentiment for Screen Resolution The insights above will be automatically delivered directly on the Amazon Product Detail Page (PDP) in the form of an in-line widget powered by a Google Chrome Extension. Implementation Approach Review Text Mining Amazon doesn't have an API to retrieve reviews, so a scraping approach will be taken. Amazon is known to block IPs if they suspect them of scraping, so a client-facing solution executed in the user's browser by the extension is preferred. Puppeteer + Node.js or ScrapingBee will be considered. Scraped review data will be cleaned up and sent to an AWS Lambda to perform the subsequent analysis steps. Review Text Processing The raw reviews will be tokenized, stopwords removed, lemmatized, and stemmed using the NLTK and gensim libraries through an AWS Lambda function. Finally, we will create a bag of words from the reviews and our corpus. Topic Grouping (Modeling) with LDA To provide a good user-experience, there is a need to provide our analysis with low latency on the Product Detail Page on Amazon. Depending on the performance of LDA libraries, a tradeoff of accuracy for speed may need to be made. Two of the more popular libraries for LDA using Python are Sklearn and Gensim. Gensim provides more flexibility and the benefit of using GPUs, but GPUs would require a server architecture such as vast.ai or Amazon EC2 vs. using a serverless solution like AWS Lambdas. A technical evaluation between libraries will be completed during the project. The output of this step will be a set of topics derived from the reviews. Sentiment Analysis For each topic, the general sentiment of the reviews will be measured using the NLTK or TextBlob Python library. This will be executed on an AWS Lambda. Chrome Extension The topics and topic sentiment will be displayed in-line on the Product Detail Page (PDP) on Amazon.com through a Google Chrome Extension. The UI will consist of a summary view and ingress point below the star rating and a larger widget above the ""Top reviews"" section using the same HTML components and CSS styling that the Amazon site uses for a native experience. Result Caching (Stretch Goal) Due to the compute resources needed for this solution and the desire to provide a near real-time solution, caching of previous results should be implemented for each product that has been processed through the pipeline. The cache should have a short TTL (e.g., 30-60 days) to ensure the analysis remains accurate and up-to-date. AWS S3 could be used to store the results as JSON. Languages Extension: HTML, CSS, JavaScript Text Mining: Javascript Text Processing: Python LDA: Python Sentiment Analysis: Python Measuring Success Topic Coherence: I will measure the quality of the topics based on a Topic Coherence measure when determing the # of topics to return based on the review (document) collection. User Input: As a stretch goal, I will look to add a user feedback mechanism to the Chrome extension that shows a UI affordance on the product page asking ""which of these topics was most useful to you"" or ""did you find these topics helpful?"" with a thumbs up / thumbs down. This data can be stored in a NoSQL DB along with the product ID for future improvements. Task Breakdown (LOE Estimates) Text Mining: 8 hours Text Processing: 2 hours LDA: 8 hours (analysis of libraries, implementation, fine tuning parameters, AWS infrastructure) Sentiment Anlysis: 4 hours Chome Extension: 8 hours (wiring up the various lambdas, integrating the text mining solution, and building the UI) References [1] https://spiegel.medill.northwestern.edu/online-reviews/ [2] https://www.powerreviews.com/wp-content/uploads/2016/04/PowerofReviews_2016.pdf"
https://github.com/tlawx/CourseProject	Project Progress Report.docx	Project Progress Report Tony Law (Captain) | tonylaw2@illinois.edu Morgan Whitman | whitman5@illinois.edu Aareana Reza | areza5@illinois.edu Team Name: TAM 1) Which tasks have been completed? Found datasets that provide list of hospitals, prices, concepts/treatments Implemented parsers Established data structures and util functions to map and clean the data Confirmed how User will interact with data/search and created command line response that inputs Users' query 2) Which tasks are pending? Implementation of search functionality (20 hrs) Develop output with list of relevant treatments and associated hospitals Determine format, visualization / table Test and make any revisions (10 hrs) Bonus features: Additional functionality to implement recommendation system (10 hrs) Add front end for web search 3) Are you facing any challenges? Finding the best way to implement the search functionality using metapy including further developing this search to return relevant treatments
https://github.com/tlawx/CourseProject	Project Proposal.docx	"Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Tony Law | tonylaw2@illinois.edu Morgan Whitman | whitman5@illinois.edu Aareana Reza | areza5@illinois.edu Team Name: TAM 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Hospital Transparency and Treatment Search System. Healthcare costs make up one of individuals and employers largest expenses and prices continue to rise. New legislation in recent years has required hospitals to provide medical services price transparency by 2023. This data is accessible on hospital websites but not easily accessible to individuals searching by service. Planned Approach: Find datasets/APIs that provide a list of hospital price transparency Scrapper functionality to extract useful data from hospital websites Search functionality to allow users to sort through and filter data based on a query Provide a recommendation functionality to match results based on similar content Datasets: ""Cost of care"" and ""price transparency"" found on some hospital websites. Hospitals post prices on websites. Many times, in an Excel file format. (e.g., https://www.rush.edu/patients-visitors/billing/cost-care, https://www.uchicagomedicine.org/patients-visitors/patient-information/billing/price-transparency) CPT codes / Procedure code mapping/interchanging Kaggle Dataset hospital-price-transparency link Expected Outcome: A system that responds to a user query/search of medical service with a list of hospitals that are ranked according to reported prices and their ratings Work Evaluation To evaluate the project outcome, we plan to cross check our results with a specific known subset of hospitals/services to verify results 3. Which programming language do you plan to use? Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The project effort will be split into these main components: Find appropriate datasets and websites to extract/scrape data from 10 hrs. Data cleaning, mapping and transformation to make it ready for the search logic 10 hrs. Implementation of data parsers 10 hrs. Implementation of search functionality 20 hrs. Testing 10 hrs. Bonus feature: Additional functionality to implement recommendation system 10 hrs."
https://github.com/tlawx/CourseProject	README.md	CourseProject Search System For Hospital Transparency Data Healthcare costs make up one of individuals and employers largest expenses and prices continue to rise. New legislation in recent years has required hospitals to provide medical services price transparency by 2023. This data is accessible on hospital websites but not easily accessible to individuals searching by service. This project allows users to query/search of medical service and provide a list of hospitals that are ranked according to reported prices and their ratings Dataset dolthub Hospital Price Transparency link Kaggle Dataset hospital-price-transparency link
https://github.com/pmagunia/CourseProject	progress.pdf	CS 410 Progress Report Parag Magunia North Penn Networks Limited magunia2@illinois.edu November 13, 2021 This report answers the required progress report questions stated in Coursera. I. What is the progress made so far? The coding portion of the project is done. The team has tested the recommender system in a number of scenarios and it works as expected. Instead of using the Pearson Correlation Coefcient, the Cosine Similarity function was used as was discussed earlier in the course. The Chrome extension has been also built with an appropriate manifest fle. The backend LAMP server is also up and operational. II. What are the remaining tasks? The remaining tasks are to complete documentation and add the source code to the Github repo. There are some debug- ging tools I have designed which might need explanation in the documentation. I was also going to create a YouTube video which demonstrates that my recommender system works. III. What are the challenges/issues being faced? Initally, there was difciculty with creating the Chrome extension, but this has been resolved. Apparently, simply reload- ing the extension is not enough - I had to completely uninstall/reinstall the extension for the updated manifest fle to be recognized correctly. That was the main difculty. I spent several hours trying to fgure that out. The other tasks like coding the AJAX, JavaScript and PHP was not difcult. Originally, I thought jQuery could not be used, but it is supported with Chrome Extension Manifest v3 as I saw from the ofcial Chrome Extension example repositories. 1
https://github.com/pmagunia/CourseProject	proposal.pdf	CS 410 Project Proposal Parag Magunia North Penn Networks Limited magunia2@illinois.edu October 20, 2021 1 Introduction This proposal answers the required topic questions stated in CS 410 Project Topics. I. What are the names and NetIDs of all your team members? Who is the captain? The team consists of a single team member who is the captain: Parag Magunia with Illinois NetID magunia2 and email address magunia2@illinois.edu. The name of the team is North Penn Networks Limited. II. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? About ten years ago, the now defunct recommender system Stumble Upon pushed webpages to users based on user profiles and ratings history. However, the service shutdown in 2018. To this end, I am designing an intelligent webpage recommender system to fill the gap left by Stumble Upon's departure. The project directly relates to recommender systems discussed in week 6 of CS 410. III. Briefly describe any datasets, algorithms or techniques you plan to use Professor Zhai discussed the Pearson Correlation Coefficient, r, as a suitable algorithm for parts of a recommender sys- tem. This statistic will be used to push webpages most likely to be suitable for the user based on shared fields in a simple personality profile. IV. How will you demonstrate that your approach will work as expected? To demonstrate the proof-of-concept, two test users can be created with similar profiles. If user, ui, ranks a webpage with high marks, then that page should be pushed to user, uj, since they share similar interests. V. Which programming language do you plan to use? PHP will be used to design the backend of the system. JavaScript will be used for AJAX queries between the frontend and backend. A Chrome extension which will be provided to graders for evaluation. R will be used for Pearson correlation coefficient computations. VI. Please justify the workload of your topic. Task Estimated Effort(hours) Frontend development 15 Backend development 15 Documentation 2 1
https://github.com/pmagunia/CourseProject	README.md	CS 410 Course Project Parag Magunia / North Penn Networks Limited Deliverables proposal.pdf (Attached. Pleased see list of files above.) progress.pdf (Progress Report. Attached - Please see list of files above.)
https://github.com/VoidGoat/CourseProject-CS-410	ProgressReport.pdf	CS 410 Project Progress Report Kajetan Haas 1) Which tasks have been completed? - A basic Firefox browser extension has been set up using JavaScript, HTML, and CSS. It includes a search bar to search the page. 2) Which tasks are pending? - The approximate string matching algorithm needs to be implemented. - A dataset like WordNet needs to be used to match similar words. - The results need to be ranked and displayed in the pop up window. 3) Are you facing any challenges? - Developing a browser extension is more difficult than I anticipated as all the functionality needs to be set up in very specific ways, presumably for security reasons. One of the main difficulties was getting logging to work correctly for debugging, but now that this is resolved everything is much easier.
https://github.com/VoidGoat/CourseProject-CS-410	ProjectProposal.pdf	Team Leader: Kajetan Haas (kahaas@illinois.edu) CS 410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members: Kajetan Haas - kahaas (Captain/Leader) 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? The topic I have chosen is to create a Firefox browser extension that allows for more flexible searching in webpages. It is often difficult to find what you are looking for on a webpage if you are not sure of the exact text you need to find. Using fuzzy/flexible searching you can search for words and phrases that are close to your search phrase, but not exactly the same. They can then be ranked with BM25 and the best results can be displayed to the user. This is related to the Intelligent Browsing theme because it allows for users to be able to search more effectively and uses algorithms to provide more general results for a user's query. This topic is related to the overall class because it is about text retrieval and using search algorithms on text. 3. Briefly describe any datasets, algorithms, or techniques you plan to use: Fuzzy Search / Approximate String Matching https://en.wikipedia.org/wiki/Approximate_string_matching will be used to search for text in the web page that approximately matches the query. For example it may have a few letters changed, so typos are not an issue. BM25 Ranking will be used to rank the results of the closest matching parts of the web page. The WordNet dataset https://wordnet.princeton.edu/ or a similar database will be used to find words that are similar to your query word. For example, matching 'bread' with 'loaf'. 4. How will you demonstrate that your approach will work as expected? I will use the extension on webpages with non-trivial content and show that it is able to retrieve relevant content in the page from an inexact query. For example searching for 'algorithms' would still yield 'algorithm' or searching for 'bread' might yield 'bun' or 'loaf' since these are related words. 5. Which programming language do you plan to use? Since it is a browser extension it will be primarily programmed in JavaScript. It will also likely utilize HTML and CSS so that it can have an appealing interface. 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. There is one student on the team so there should be 20 hours of work: * Setting up basic browser extension - 1 hour * Indexing all the words on the current webpage - 1 hour * Use stemming and tokenization to split the webpage and queries into separate and searchable parts - 2 hours * Implementing an efficient fuzzy search algorithm - 4 hours * Creating an appealing interface to enter your query and see the results - 4 hours * Setting up the WordNet database to work in the browser extension - 6 hours * Rank the results using a heuristic formula and BM25 - 2 hours Total: 20 hours
https://github.com/VoidGoat/CourseProject-CS-410	README.md	Course Project (kahaas) This final project is to create a Firefox browser extension that allows for a more flexible way to search for content in webpages. The proposal is in ProjectProposal.pdf The progress report is in ProgressReport.pdf
https://github.com/andreev-io/Documentation-Search-with-Elastic-Search-and-GENRE	Progress Report.pdf	Progress Report Which tasks have been completed? 1. We created a StackOverflow crawler and successfully fetched over 1,000 training examples of StackOverflow question-answer pairs that link to Python documentation. 2. We indexed ElasticSearch on all of the subpages of https://docs.python.org/3/index.html and wrote the evaluator logic that computes the reciprocal rank of the ground truth answer for a StackOverflow question as retrieved by ElasticSearch. 3. We ran GENRE end-to-end on a single training example. Which tasks are pending? 1. Run our StackOverflow crawler for an extended period of time to fetch all training data. We estimate that we will have around 30,000 training examples. 2. Split fetched question-answer pairs into training and testing data sets. 3. Train GENRE on the training data set. Create evaluation logic for GENRE. 4. Run comparisons of GENRE and ElasticSearch performance. Are you facing any challenges? No, but we are cognizant of the risks of not having enough training data and/or not being able to train GENRE properly. These are the two areas we are prioritizing and are aiming to de-risk as soon as possible.
https://github.com/andreev-io/Documentation-Search-with-Elastic-Search-and-GENRE	Project Proposal.pdf	"Documentation Search with Elasticsearch and GENRE Team Softmax - Ilia Andreev (captain), iandre3@illinois.edu - Raj Krishnan, rajk3@illinois.edu - Sengill Kim, sk100@illinois.edu Abstract In this project, we will compare Generative Entity Retrieval and standard full text search methods for looking up knowledge base entities. We use Python documentation as the knowledge base, documentation sections (e.g. ""Instance Objects"" or ""Expression Lists"") as documents, and StackOverflow-like questions as queries. We answer these queries in two ways: by retrieving relevant documentation with Elasticsearch and by generating relevant documentation section names. Proposal A paper on Autoregressive Entity Retrieval came out in March 2021, along with the accompanying implementation called GENRE. This paper proposes a new method of searching knowledge bases. It uses a sequence-to-sequence model to handle end-to-end entity linking, entity disambiguation and document retrieval. In practice, this means that GENRE generates names of the knowledge base entries related to the query based on the context in which those entries have been referenced in the training data. We seek to compare this approach with an industry standard search solution - Elasticsearch, which is built on Apache Lucene - and see how the two approaches differ for the task of retrieving entities from documentation when presented with StackOverflow questions as queries. We find this work important since it compares some of the classical text retrieval approaches with more novel but less common methods. For this project, we are going to build our own training and test datasets. These datasets will consist of publicly available StackOverflow questions with popular answers linking to specific language documentation sections. We are going to use Python 3 Standard Library and Language Reference as our knowledge base. All systems implemented for this project will be written in Python. Our implementation steps will roughly involve the following: 1. Build a corpus of language documentation. 2. Index the language documentation into Elasticsearch with minimal tuning. 3. Build a corpus of queries from StackOverflow, retrieving user questions with at least one popular accepted answer. Filter the results to only consider the answers linking to Python documentation, and split the question-answer pairs into training and test datasets. 4. Use the training dataset from StackOverflow to train the GENRE sequence-to-sequence model. 5. Run the test queries against the model based on GENRE and Elasticsearch, and see how their Mean Reciprocal Rank metrics compare. We expect the workload to be distributed as follows: 1. Build a language documentation corpus: 5 hours 2. Index documentation into Elasticsearch: 5 hours 3. Build StackOverflow question-answer dataset: 20 hours 4. Train the sequence-to-sequence model using the StackOverflow dataset: 20 hours 5. Build a test harness to evaluate approaches: 15 hours 6. Compare approaches: 2 hours 7. Total: 67 hours"
https://github.com/genggeng88/CourseProject	CS 410 Project Progress Report.pdf	CS 410 Project Progress Report PS: We added two more tasks for our project: Task 0: Setting up local development environment Task 7: Setting up a docker environment and providing a public url to the users. (One of our teammates is especially interested in this part of work and will give a try.) 1) Which tasks have been completed? Task 0: Set up a local development environment. Status: All the teammates have successfully set up a local development environment for the ExpertSearch project (https://github.com/CS410Assignments/ExpertSearch) and could implement expertise searches on the local website: http://localhost:8095/. We can now get a list of faculty information, including name, university, location, and a simple bio, using a specific expertise search. We can also visit the faculty's email and web page using the hyperlinks in the list. Every teammate is familiar with the ExpertSearch project and clear with our following work. In the future, we can always make some simple tests before we commit the changes. 2) Which tasks are pending? Task 1~3: After showing the searching demo and discussing the specific methods on task 1~3, team members who are responsible for these tasks are now clear with the following work. Task 1 focuses on grabbing all possible links from a given link. Task 2 will choose faculty links from all the grabbed links using the Regex method. Task 3 focuses on identifying faculty links by binary classifier. Task 4: What's pending of task 6 are transforming current codes into somewhat adaptable to what the team has, which being able to allow task 6 seamlessly blended in. And tuning major functions like EM, log-likelihood and PLSA as result it is versatile. Besides continually re-learning given lecture videos, finding topic modeling examples from open source would be helpful. Task 5: Writing a topic mining function in extracting common search areas. Status: Just discussed with the team to finalize the requirements. We would like to add more information into the web page like: Research Interests .... Plan: Will test the current framework to know how to return new fields into json -- Nov 20, 2021 Implement new functions to obtain the new fields per faculty's home page -- Nov 28, 2021 3) Are you facing any challenges? Task 4: Topic modeling requires me to rewatch lecture videos more than twice. For anyone without a solid computer science background like myself. It's tough one even with my teammates' guidance. Challenges include the necessity of understanding a wide range of knowledge in order to work with teammates seamlessly, like LDA, Expectation-Maximum Algorithm, PLSA. Understanding these areas at every granularity helps substantially when bridging code with my teammates. After gaining solid understanding of what topic modeling requires, being able to transform conceptual knowledge into codes is one of the most crucial parts of this task. But I'm in a great team with talented students. We continually grow and share, in hope to make our project standout and meaningful. Task 7: To set up docker environment and provide a public url to the users (like TA) Status: Have spent 16 hours learning docker, gunicorn, nginx and web service.
https://github.com/genggeng88/CourseProject	CS410 Course Project Proposal_SearchExperts.pdf	CS410 Course Project Proposal Team Name: SearchExperts (Fall 2021) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Qin Geng (qingeng2@illinois.edu) (Capitain) b. Aditya Vyas (vyas7@illinois.edu) c. Junwei Chen (junweic4@illinois.edu) d. Peter Zhang (aijun2@illinois.edu) 2. What system have you chosen? Which subtopic(s) under the system? a. Theme: System Extension b. Specific Topic: ExpertSearch System 3. Briefly describe any datasets, algorithms or techniques you plan to use a. datasets: we have a huge resource of directory page URLs from MP2.1 and faculty webpage URLs from MP2.3. We will also crawl some other pages to get URLs (e.g. other URLs on the university websites, product websites, news sites, etc.) b. algorithms and techniques: we're going to build a regex-based model and a binary classifier model to help us identify specific webpages. As to extracting other information, we will use Natural Language Processing techniques to recognize names/profiles in web links and extract those. Another idea is using topic mining from MP3 to help extract top keywords in each topic which should be the common search areas. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? a. As to automatically crawling faculty webpages: we will add the function of automatically crawling random webpages, along with the function (a regex-based function and a binary classifier) of identifying faculty directory links and faculty webpage links. Our expectation is that it can work automatically, and give a satisfying accuracy on identifying, let's set 70%. b. As to extracting other information: we will add an extracting function based on Natural Language Processing techniques to recognize and extract names/profiles in web links. Also a topic mining function in extracting common search areas. As long as these two functions can work and extract extra information, it's an improvement to the original MP2. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. Our whole project will be based on MP2.1 and MP2.3, but we're going to develop new functions to improve MP2. Our external functions will eventually be integrated into MP2. 6. Which programming language do you plan to use? Python and its packages 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. There are mainly five tasks right now, and with the estimated workload (4 team members in total) as shown below: Task No. Task Description Estimated Workload hr Task 1 Writing a function of automatically crawling random web pages, and collecting them for the next step use. 10hr Task 2 Building a regex-based model to identify faculty directory webpages. Improving this model to achieve an accuracy of 70%. 15hr Task 3 Building a binary classifier model to identify faculty directory webpages. Improving this model to achieve an accuracy of 70%. 15hr Task 4 Writing an extracting function based on Natural Language Processing techniques to recognize and extract names/profiles in web links. 15hr Task 5 Writing a topic mining function in extracting common search areas. 15hr Task 6 Integrate all functions and models to MP2 and test the whole project. 20hr Total 90hr We may find more tasks that can be done later as we dive into this project more in the future. We may modify some of the tasks if we find out that they are not working later.
https://github.com/genggeng88/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chask8ng/CourseProject	Project Progress jkchang2.pdf	"Project Progress jkchang2 1. Completed Tasks a. Decided on user experience workflow for showing video ""key words"" i. Ideally users do not have to manually click through each video to view the keywords used. So all ""keywords"" will be pre-generated on the weekly outline page, as a caption section under each lecture video. This could be improved further in the future, but for now helps users quickly navigate past lectures for content explanation. b. Choose a method of identifying key words per video lecture. i. Given the time to implement a working solution for the final project and having a one-member team, the algorithm will be TF-IDF to pick out uncommon words as the ""key words"" to surface to students. 2. Pending tasks a. Writing source code of feature showing video ""key words"" i. Need to spend some more time to complete writing the code required, debug, and test. This will take a good amount of time left. b. Providing demo of the feature implementation i. Blocked by the completion of the source code. To include mock ups of the feature using the Coursera platform UI. 3. Challenges a. Nothing to call out here, project is moving along fine."
https://github.com/chask8ng/CourseProject	Project Proposal jkchang2.pdf	Project Proposal jkchang2 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Jonathan Chang, jkchang2, captain 2. What topic have you chosen? Why is it a problem? How does it relate to the theme and to the class? a. Intelligent learning platform. It is becoming increasingly common to learn online as opposed to a traditional classroom experience. This can be for a variety of reasons, including flexibility, cost, and individual preference. Specifically, I'm interested in text within the video lecture captions. There is a lot of information and keywords captured by lecturers, which are not sufficient to understand from the slides alone. To improve the student's learning experience, it would be great to highlight uncommon words used as keywords, tag them to videos, and allow for easier reviewing of material. Otherwise, most times students have to click through videos and search for the content they are looking for manually. This will turn the raw data into actionable insights for learners to optimize time spent learning instead of wondering where the information is located. 3. Briefly describe any datasets, algorithms or techniques you plan to use a. The raw data is captured by the captioning system Coursera provides. It would be possible to scan that text, apply something as simple as TF-IDF, and highlight keywords, as well as store them to be viewed alongside each video (TBD on optimal UI experience). 4. How will you demonstrate that your approach will work as expected? Which programming language do you plan to use? a. We can do a demo by taking a sample coursera lecture page, scraping the text, and showing the overlay via HTML/CSS/Javascript. Ideally the keywords can be organized and shown in a summary format. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Documenting the ideal user experience (is it highlighting keywords, can we call them out somehow in the navigation bar alongside videos?). Provide walkthrough of this and some visuals. i. 3 hours b. Testing some different methods to implement given short time to execute (TF-IDF, or otherwise) i. 5 hours c. Complete the actual source code with testing i. 7 hours d. Prepare the demo to be shown (ideally directly from the actual page itself, or as a standalone work for full functionality). Need to test out limitations and feasibility i. 5 hours
https://github.com/chask8ng/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/hungn2/CourseProject	CS410 Progress Report.pdf	"CS410: Project Progress Report Free Topics: Topic querying on UIUC MCS Slack and Campuswire Hung Nguyen Blake Jones Progress Made: * Implement CLI interface: * We began stubbing out the Click commands for our CLI interface. Since we haven't set up our mainline aggregation/query functions, these are largely just empty stubs with parameters/options. * We have set up our Python application, with a setup.py to enable packaging of our project. We have also included a requirements.txt to enable easy installation of application requirements. This allows end users to easily install and begin working with our application! * Implement Campuswire library * We have fully implemented our Campuswire API and interface. * This interface supports dataclasses to package relevant data to be used by our querying engine (CampusWireThread and CampusWireMessage). * Our CampusWire library supports pagination and caching of results, so we do not repetitively request the API when unnecessary. * Implement Slack interface * We have fully implemented our Slack interface using the slack_sdk python package. * Our Slack interface, like Campuswire, supports custom dataclasses to manage relevant extracted data, and features pagination and caching of slack results. Remaining Tasks: * Implement document ranking on merged Slack/Campuswire topics * Implemented clustering deduplication on results. * Creating evaluation test collection. * Parameter tuning * Clustering algorithm/parameters * Ranking function/parameters Challenges/Issues: * We had initial difficulties getting the Slack API to work. The documentation wasn't the best, so we had trouble finding what the Slack ""channel ID"" was at first. Fortunately, a quick google search gave as an easy answer from StackOverflow (https://stackoverflow.com/questions/40940327/what-is-the-simplest-way-to-find-a-slack-t eam-id-and-a-channel-id) * We had difficulty reverse-engineering the CampusWire API to understand authentication and API workflows. We used Google Chrome's developer tools to discern network calls to the CampusWire API and the appropriate API parameters/JSON content. * We had difficulty handling Campuswire authentication. Unlike the Slack API, the Campuswire Authentication routes didn't have a static API key which we could utilize. Rather, we used our individual sessions API token."
https://github.com/hungn2/CourseProject	CS410 Project Proposal.pdf	CS 410: Project Proposal Free Topics: Topic querying on UIUC Slack and Campuswire 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Hung Nguyen - hungn2 [Captain] Blake Jones - blakeaj2 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Free Topics: Topic querying on UIUC Slack and Campuswire. The UIUC MCS Slack has a variety of course specific channels. These channels are used year over year, resulting in a buildup of relevant information for a given course. This information can be vital to students in future semesters. Campuswire is another great resource for information on the content of a course's current semester, however students must be invited to a course, and only have access to that semester's content. As a result, Campuswire does not have the rich historical history as Slack does. This disconnect between these two data sources inhibits current students from leveraging previous students' experiences in learning new content. In addition, students are inundated with a variety of posts, many of which are duplicated (e.g. a slack question that was also posted to Campuswire). Our project will aim to merge historical Slack and semesterly Campuswire whilst removing duplicated answers, providing an efficient and concise querying service for UIUC course questions. Approach Our CLI tool will provide three modes of querying data: * Given an ad-hoc query, both Slack and Campuswire will be searched for relevant conversations. * Given a Campuswire question, Slack will be searched for relevant conversations. * Given a Slack message, Campuswire will be searched for relevant conversations. Users will execute our CLI tool inputting an ad-hoc query, or a slack/campuswire conversation. The CLI tool will then search the appropriate data sources and return the most relevant documents to the users query, ranked by relevance. Datasets: * CS410 2021FA Campuswire * #cs-410-text-info-syst Slack Channel Tools/Systems: * Python Click (CLI tooling) * MetaPy * Slack API * Beautiful Soup (for scraping Campuswire) * Scikit-learn (deduplication via clustering) Expected Outcome After inputting a query or slack/campuswire question, users can expect a ranked set of relevant documents from Slack/Campuswire. This document set will be deduplicated to minimize effort on behalf of the user. How are you going to evaluate your work? We will be using the Cranfield Evaluation Methodology to measure how well our system's result matches the ideal ranked list. We will be building our own test collection by running through several queries and then evaluating each document/conversation as relevant or non-relevant. 4. Which programming language do you plan to use? Python, as we will be building a CLI based Slack/Campuswire querier. 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * [3hr] Implement CLI interface * [9hr] Implement Campuswire library * Campuswire has no API, so we will create our own scraping library * [6hr] Implement document ranking on merged Slack/Campuswire topics * [6hr] Implemented clustering deduplication on results. * [12hr] Creating evaluation test collection. * [6hr] Parameter tuning * Clustering algorithm/parameters * Ranking function/parameters
https://github.com/hungn2/CourseProject	README.md	CourseProject By Blake Jones & Hung Nguyen Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/MadaoIsMyBrother/CourseProject	Project Progress Report.pdf	"1. Which tasks have been completed? We first loaded the datasets and read the CSV files. The three datasets include tweets about ""all brands of vaccines dataset,"" ""total vaccination for COVID-19 in the world dataset,"" and ""the extraction of tweets sentiment dataset."" Then, we preprocessed the datasets. For our language model, the only input we need is the 'tweet text.' Fastai can do the text preprocessing for us, and at the same time, we removed things from the 'text column,' such as URLs, emojis, and rows that are missing sentiment. Since there are two datasets, ""all brands of vaccines"" and ""the extraction of tweets sentiment,"" we cleaned the text and combined the two datasets with only two columns left: 'text column' and 'sentiment column.' The 'sentiment column' tells us about whether the corresponding text is positive, negative, or neutral. In addition, we transformed negative, positive, and neutral sentiments to 0, 1, and 2 correspondingly for further modeling. Next, we tokenized the texts and performed lowercase, punctuation removal, small token removal, stop words removal, lemmatization, and stemming to them. After all these steps, we got a dataset with four columns: original text, sentiment, final text, and text tokens. Afterward, in order to get a general sense of the sentiment prediction task, we firstly applied Naive Bayes to the dataset and obtained an accuracy of 0.6260. Then, we applied another baseline model to the dataset, XGBoost, which gave us a little higher accuracy of 0.6964 after rounds of parameters tuning. Furthermore, we added visualization to the vaccination progress data where we illustrated the correlations among multiple factors such as vaccination rate, total vaccinations, and daily vaccinations among many different countries. In the above sample figure, we made an interactive plot to show the vaccine doses per hundred people among different countries. Here we made a date as a slider bar to dynamically display the movement of the statistics. We also made interactive plots to show the movement of vaccination rate, daily vaccinations, and fully vaccinated people in given time duration. With the help of these plots, we can better illustrate the vaccination progress data and better analyze the correlation between vaccination progress and Twitter vaccine sentiments. 2. Which tasks are pending? Firstly, as mentioned before, the accuracies of sentiment prediction from baseline models were not high enough. The highest accuracy was only 0.6964. Therefore, we plan to self-learn some deep-learning-based models and then find the best one for our application to achieve higher accuracy for our prediction task. Secondly, we are still analyzing the timelines added into the datasets for each type of COVID-19 vaccine and then trying to visualize the relationship between vaccination and countries. For example, what type of vaccination is used in different countries, which vaccine is the most widely used, and the relationship between the number of vaccinations daily/per people/per country and try to visualize the progress of vaccinations evolved from each country. We will further improve the visualizations of vaccination progress data, implement visualizations of the predicted Twitter sentiment data, analyze these results, and find some correlations between Twitter sentiment and real vaccination progress. 3. Are you facing any challenges? Right now, the challenge we are facing includes interpreting sarcasm on some occasions. Expressing negative sentiment using backhanded compliments makes it hard for sentiment analysis tools to detect the true context of what the response is actually implying. Besides, social media contents, just like the tweets on Twitter, are text-based, which means that they are inundated with emojis. While NLP tasks can extract text from even images, emojis are a language in itself. Most of the time, we treat emojis as special characters that are removed from the data during data mining, but this may cause the loss of data insights and accuracy."
https://github.com/MadaoIsMyBrother/CourseProject	Project Proposal.pdf	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. We are group GUGUGU, and we have four team members: Jingze He (jingzeh2) Qiao Jiang (qiaoj2) Xinyi Fang (xinyif5) Zejie Zhou (zejie2) Zejie Zhou will be the captain. What is your free topic? Vaccine Sentimental Analysis with Tweets Please give a detailed description. We would perform sentiment analysis with a collected COVID-19 vaccines related tweets dataset to analyze the sentiment of the overall public discussion. A comparative analysis with the vaccine progress and the result of sentiment analysis of public discussion on Twitter will also be conducted to provide a better understanding of how the public discussion is correlated with vaccine progress. What is the task? Our task is to perform sentiment analysis on tweets about COVID-19 vaccines by using the dataset of tweets sentiments from all over the world. The dataset will include tweets about people's perspectives towards different kinds of vaccines, the number of different vaccinations per day, week, month and country over the number of people being vaccinated. By performing sentiment analysis, we can get an overall prediction of the relationship between vaccination rates and people's discussions on Twitter. Why is it important or interesting? It can help us predict the vaccination rate from twitter posts then take measures according to the predicted vaccination rates. The release of official statistics of vaccination rate need multiple steps and may cause delay from days to weeks. Predicting the vaccination rate from twitter posts can help the government prepare for future policies with shorter delay. To some extent, this project can be a small step towards the goal of herd immunity! What is your planned approach? We will firstly clean the dataset and then apply models to it to find a best model that enables analyzing sentiment of the target tweets. Next, we will add time information about world vaccination progress to get an overall sense of the relationship between public sentiment towards COVID-19 vaccine and the vaccination rate. What tools, systems, or datasets are involved? We will use two datasets found from Kaggle. The first dataset contains tweets discussing vaccines from December 2020 to March 2021. The second dataset contains vacacciation progress for countries all over the world. What is the expected outcome? 1. Collect data then process and extract texts from these data(twitter posts, vaccine related articles) 2. Get sentiment score/label by our sentiment prediction model in a specific time range 3. Find some relations between twitter posts sentiment and vaccination progress How are you going to evaluate your work? We will evaluate our work based on: 1. Properly process the text data(twitter posts, vaccination rate related articles) 2. The accuracy of our sentiment prediction model 3. Get meaningful conclusions from analyzing the relation between twitter posts sentiment and vaccination rate Which programming language do you plan to use? python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. We have four teammates: N = 4, 20*N = 80 Data collection: 10h Coding and Testing: 40h Analysis: 15h Writing: 15h
https://github.com/MadaoIsMyBrother/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jstephens/CourseProject	ProgressReport.md	Progress Report Completed Tasks I've started creating the pipeline for taking in books and analyzing the content. I'm also evaluating options for honing the dataset used and how to pick up missed English words that could uncover sentiment in original texts. Pending Tasks Creating the front-end for the application. Challenges I'm finding that a lot of words are viewed as neutral and as a result a lot of the texts I've inputted using the Hedonometer dataset have a sentiment reading of about 5 (neutral). I'm currently working with various methods to gain more meaning from the ratings I have for words, like creating a logarithmic scale or only including words with a very high or low sentiment rating.
https://github.com/jstephens/CourseProject	Proposal.pdf	Team: Jamie Stephens (jamieas2) Free Topic: In this assignment I'll develop a web application that can take a novel's full text as input and output a visualization displaying major characters' names and the sentimentality of interactions they were involved in with every other character, where applicable. I'll be utilizing a dataset comprised of about 10,000 English words that have been crowdsourced to evaluate their 'happiness score' as graded by native English speakers. This will be a new effort to quantify novels and could be used as part of a more complex recommender system - for example, if a user prefers largely positive or negative novels. I'll evaluate this work by entering the texts of books I've read already - I'll verify that all main characters are properly accounted for, as well as the approximate positivity or negativity of their interactions both in and of themselves and while associated with other characters. Tools, Systems, Datasets:  This will be partially user-driven, as the finished product should be able to handle any book of suitable length. A selection of public domain texts will be made available for testing convenience.  Hedonometer word list, supplied by the Vermont Complex Systems Center  Python's Plotly library, for sentimentality visualizations  Python's Dash library, for output into a HTML webpage Programming Language I expect to program this app in Python. Workload This project can be broken up into the following components, which I expect to take at least 20 hours:  Coding functions that can identify character names through text patterns  Establishing parameters for words that contain sentimentality information (for example, the number of words to search around a certain word to find associated characters)  Coding functions that can identify new chapters or changes in scenery  Building and debugging a web application  Deploying the web application
https://github.com/jstephens/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/samsong2/CourseProject	CS 410 Final Project Proposal.pdf	CS410 Project Proposal Team: Mission Lucy Captain: Sam Song(samsong2) Azim Keshwani(azimmk2) Kavithaa Suresh Kumar(ks64) Uttam Roy(uroy) For our course project we have chosen to build a search engine that can support video segment search. Going with the theme of Intelligent Learning Platforms, we aim to be able to allow the user to query a classes' video lectures and see a ranked list of clips from video lectures. The problem that we aim to solve is that lecture videos may have multi-topics in it and the same topics might be in multiple lectures. Through video segment search, we are allowing users to more easily access shorter clips of relevant information that may be scattered across multiple lectures Our project relates largely to the theme of Intelligent Learning Platform by adding more searchability to a learning platform. This project also relates to what we have learned in the first half of the class in regards to search engines, and the text retrieval problem. Although the information returned to the user is in the form of video clips, the same algorithms can largely still be applied. To create our video segment search engine we plan to have a web crawler scrape Coursera's website for the video clips and video transcripts. These videos and transcripts will be our dataset from which we will provide the video clips. Utilizing the video transcripts we can then rank video clips from most relevant to least relevant, using BM25 as the ranking algorithm. To implement this algorithm we are planning to use Python. We are going to create a web application to demonstrate the video segmentation search where the user can enter the query and the output would be a ranked list of video segments that is relevant to the query. The query results will be evaluated by having team members manually determine relevance. That way we can evaluate the search engine using metrics like precision, recall, and F1. Tasks: Hours required Web crawling to retrieve related information 10 Topic segmentation (identifying transition points) 40 Query matching 10 Evaluation 10
https://github.com/samsong2/CourseProject	CS 410 Progress Report.pdf	CS410 Progress Report Team: Mission Lucy Captain: Sam Song(samsong2) Azim Keshwani(azimmk2) Kavithaa Suresh Kumar(ks64) Uttam Roy(uroy) 1) Which tasks have been completed? * A web scraper to scrape for classes from Coursera has been implemented. * The web scraper can download the time stamped transcript and video link from each lecture given the course name. * The web scraper uses Selenium to get and load the page. Beautiful Soup is then used to scrape the pages. 2) Which tasks are pending? * Building the web interface to perform the search * Data preprocessing- removal of stop words, removal of extra characters, lowercase conversion,stemming * Finding similarity between the query and the segments to retrieve the results. * Ranking the list of relevant results 3) Are you facing any challenges? * Due to the fact that Coursera is a dynamically loaded webpage, there is a need to wait for the javascript to fully load the page before parsing. This results in around a 3-5 second delay when fetching for each page. As a result, scraping for an entire course takes a long time. * Scraping for a course from Coursera requires an user that is already enrolled in the course to provide authentication information. * Still deciding on approach to build out the web interface (packages needed, design, etc) * Decision on which python library (whoosh or scikit )to use for searching remains a challenge.
https://github.com/samsong2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/vquiros-atieh/CourseProject	Progress_Report_Potterheads.pdf	Accio Songs, Music Information Search Engine PotterHeads - Project Progress Report Kushagra Soni - Virginia Atieh soni14 vquiros2 Progress Task Estimated Time (hours) Estimated Completion Design UI 5 90% Resource Identification 5 95% Assembling Collection 5 100% Design Search and Mining Engine 10 100% Develop Search and Mining Engine 10 10% Develop UI 10 5% Integration of UI with Engine 5 5% Testing 10 5% Total 60 ~45% * Resource Identification - Finalized the Technology Stack we will be using to develop the project. o Programming Language: # Python, JavaScript, HTML o Frameworks: # Flask # CSS o Tools: # Elastic Search # Elastic App Search o APIs: # Artist and Track * Genius API # Track and Audio URLs: * Last.fm API * Spotify API # Lyrics: * MusixMatch API * Lyrics.ovh API # Video URLs: * YouTube API * Collection o Due to our use of Elastic App Search to create our search engine, our corpus will be assembled continuously as the number of queries executed by the user increases. * Design & Development of Search Engine o A preliminary search engine has been generated through Elastic App Search. Figure 1 - Search engine created through elastic app search Figure 2 - Basic HTML page for searching Figure 3 - Results displayed after search * Design for UI Figure 4: Artist Search, Result Selection Made Figure 5: Track Search, Results Displayed Figure 6: Lyrics Search, Results Displayed Figure 7: Track Selection Made Figure 8 - Potential Layout for Track Results Remaining Tasks * Implement user interface design and integrate design with search tool. o Adjust layout and styling as needed throughout implementation process. * Developing the various API connectors to fetch Artist, Track and Lyrics data. * Continue testing and fine-tuning search parameters to optimize the user's experience. Challenges * Resource Identification o No suitable API found for guitar chords; web scraping could be an option. * Develop UI o Track and video streaming may affect application performance; Simply the URL may be provided instead.
https://github.com/vquiros-atieh/CourseProject	Project_Proposal_Potterheads.pdf	CS410 - PotterHeads - Project Proposal Team Kushagra Soni - soni14 Virginia Atieh - vquiros2 (captain) Topic & Task Accio Songs - Music Information Retrieval Description: The world wide web is full of information. So much that sometimes, we can get lost trying to find we are looking for. For specific needs such as music related searches, it can involve visiting many webpages to find music videos, lyrics, sheet music, and artist biographies, all related to the same song. Approach: We propose to create a search engine tool specific to music information retrieval. We will take advantage of the already expansive resources available online and condense it into a user-friendly tool designed for musically inclined individuals. Task Estimated Time (hours) Design UI 5 Resource Identification 5 Creation of Collection/Corpus 5 Design Search and Mining Engine 10 Develop Search and Mining Engine 10 Develop UI 10 Integration of UI with Engine 5 Testing 10 Total 60 Programming Language & Tools We will consider the use of below mentioned programming languages and tools. We will also explore the possible usage of some libraries through our technical reviews. * Python and Flask * JavaScript * Angular * Apache Lucene Or Solr * Gensim Datasets: * http://millionsongdataset.com/musixmatch/ * https://www.hooktheory.com/theorytab/common-chord-progressions * https://www.ultimate-guitar.com/ * https://www.last.fm/ * https://spotipy.readthedocs.io/en/latest/ APIs * Shazam API * TheAudioDB API * Deezer API * Mourits Lyrics API * iTunes API * Spotify API * LastFM API * 30,000 radio stations and music charts API Outcome & Evaluation We anticipate producing a useful web search that will take the input of a song title, and return relevant information such as lyrics, chords, sheet music, videos, audios, artist biographies, streaming options, and other applicable resources. The functionality of our tool will be evaluated using sets of randomly selected test inputs, including variability across song genres. Explicit feedback will be used through the testing phase.
https://github.com/vquiros-atieh/CourseProject	README.md	Potterheads Course Project Music Information Retrieval Project Proposal: Proposal Not yet uploaded: Project Presentation Document: Final Presentation Document Project Documentation and run help: Documentation Video Presentation: Presentation Project Source Code: SourceCode Team Members: Virginia Atieh (vquiros2@illinois.edu) Kushagra Soni (soni14@illinois.edu)
https://github.com/samhq/CourseProject	CS410_PBSE_Progress_Report.pdf	Personalized Bookmark Search Engine CS 410 Text Information Systems (Fall 2021) Prof. ChengXiang Zhai Gazi Muhammad Samiul Hoque* Yuheng Xie+ Grace, Mu-Hui Yu++ Ying-Chen LeeSS Team PBSE: Progress Report 1 Overview During this initial phase, our team spent a large amount of time on architecture design and researching suitable frameworks and libraries to be used in our project. After fnalizing the details and requirements of our project, we allocated specifc tasks to each team member based on our strengths and experiences. We are on track to completing our project before the submission deadline. 2 Task Statuses Below is the list of tasks and their respective completion statuses. Tasks that are currently under devel- opment are labelled as WIP (Work in Progress). Task Completion Status Frontend (Chrome Extension) 1 Manifest File Completed 2 OAuth Authentication Component WIP (50% Completed) 3 Search Component Pending 4 Results Component Pending 5 Styling Pending Backend Server 1 Page Crawler Completed 2 Managing User Bookmark & Content Indexes WIP (10% Done) 3 Searching WIP (60% Done) 4 User Management APIs Pending 5 Application API Completed 6 Integration & Deployment Pending 7 Accuracy Test Pending Documentation 1 Usage & Architecture Doc Pending 2 Tutorial Presentation Pending *ghoque2@illinois.edu +yuhengx2@illinois.edu ++muhuiyu2@illinois.edu SSyclee6@illinois.edu 1 3 Challenges The only challenge that our team is facing so far is in developing the chrome extension, as none of us has worked on it before. However, this is not a major blocker for us since the chrome extension API is well documented we would be able to fgure our way out eventually. 4 Discussions on the Reviewers Feedbacks Below are the feedback that we have received, and our responses to those feedback. 1. Feedback: The proposal didn't have detail on how the page content data are being stored as well as how the data is being retrieved after the user bookmark a page. Response: The page content data are indexed as plain text fles, and the backend creates mappings of these content fles to users internally. When the search function is called by a specifc user, the backend will fetch the indexed fles mapped to that user to perform the search. 2. Feedback: I feel that the report should have more details on how their approach looks like for evaluating that their system is working as expected. Response: Manual testing will be done to ensure that all the functions of the chrome extension are working as intended. If there is sufcient time, we will also write unit tests for our backend func- tions and frontend components. The search algorithm will be evaluated using the cranfeld evaluation methodology, where the relevant judgements for the test set will be collected from our family members and friends. 2
https://github.com/samhq/CourseProject	CS410_PBSE_Project_Proposal.pdf	"Personalized Bookmark Search Engine CS 410 Text Information Systems (Fall 2021) Prof. ChengXiang Zhai Gazi Muhammad Samiul Hoque* Yuheng Xie+ Grace, Mu-Hui Yu++ Ying-Chen LeeSS 1 Introduction Most, if not all modern internet browsers such as Google Chrome ofer a bookmark feature that allows users to retain website URLs for future reference. This makes it very convenient for users to access their favourite websites. However, as a bookmark list grows larger over time, users will have a hard time searching through it to fnd the relevant contents they want. Moreover, there isn't a way to search into the contents of those bookmarks as a bookmark only contains the URL and title of a particular web page. Thus, we are proposing a personalized bookmark search engine to address these shortcomings. 2 Our Project For easy installation, we will develop the personalized bookmark search engine as a Google Chrome browser extension. When the user bookmarks a website using our extension, it will send the current web page ad- dress to our back-end server. There it will parse and index the page for the user. A search box will also be implemented in the widget that allows the user to enter a query term and it will fetch relevant contents from the index through our back-end API. The theme of our project is Intelligent Browsing, where we implement the ""bookmark-and-search"" func- tion on top of Google Chrome to allow users to browse intelligently through the web. We'll apply the web crawling, indexing, and searching techniques we have learnt in this class for this project. 3 System Algorithms BM25 will be used as the search algorithm. Programming Language On the front-end, Vue.js, Javascript, HTML and CSS will be used to create the Google Chrome extension and the web application to display search results. Python and Flask will be used to create the back-end application which serves the API for search, indexing and functions to retrieve user information. Primary Tasks and Workload Below is the task breakdown for this project: Page Crawler & Indexing Implementing a page crawler to parse the current web page.  10 hours Manage Bookmark Implementing functions for indexing of bookmarks and their contents in the server.  10 hours *ghoque2@illinois.edu +yuhengx2@illinois.edu ++muhuiyu2@illinois.edu SSyclee6@illinois.edu 1 Searching Implementing the BM25 search function and other related utility functions.  10 hours User Management Implementing functions to associate bookmarks and their contents with the cur- rent user.  15 hours Back-end Developing a back-end web application which serves the above functionalities on separate routes for the front-end application to call.  25 hours Front-end Developing a ""pop-up"" front-end web application for the extension that displays the search results and other relevant contents.  30 hours Integration & Deployment Integrating all the front-end components together as an install-able Google Chrome extension, and integrate the server app that can be easily deploy-able to any server.  20 hours Testing Testing and fxing of any potential bugs  10 hours Total time required:  130 hours (tentative). The workload will be evenly spread among all team members. 4 Demonstration We'll frst demonstrate that our approach works by showcasing that the bookmarking and search functions are working as intended from a user's perspective. To demonstrate that our search algorithm is reasonably accurate, we'll compare its accuracy with other systems on some predefned queries. Finally, we can also perform user testing by asking our friends and relatives to test out the extension and gathering their feedback for evaluation afterwards. 5 Team Information Our group name is PBSE, and we are a four-member team. Our details are as follows: * Gazi Muhammad Samiul Hoque (NetID: ghoque2) - Captain * Yuheng Xie (NetID: yuhengx2) * Grace, Mu-Hui Yu (NetID: muhuiyu2) * Ying-Chen Lee (NetID: yclee6) 6 Future of this Application A possible future extension to this project is to create a user account management service that allows users to sync and persist their bookmarks across multiple devices and browsers. After logging into the service, a user can search over all his bookmarks and retrieve his desired contents anytime, anywhere. 7 Conclusion Our project supports intelligent browsing by allowing users to perform search on their bookmarked contents, which increases their productivity when browsing the web. 2"
https://github.com/samhq/CourseProject	README.md	CourseProject - Fall 2021 Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. PBSE: Personalized Bookmark Search Engine Most, if not all modern internet browsers such as Google Chrome offer a bookmark feature that allows users to retain website URLs for future reference. This makes it very convenient for users to access their favourite websites. However, as a bookmark list grows larger over time, users will have a hard time searching through it to find the relevant contents they want. Moreover, there isn't a way to search into the contents of those bookmarks as a bookmark only contains the URL and title of a particular web page. Thus, we are proposing a personalized bookmark search engine to address these shortcomings. Theme Intelligent Browsing Members Gazi Muhammad Samiul Hoque (NetID: ghoque2) - Captain Yuheng Xie (NetID: yuhengx2) Grace, Mu-Hui Yu (NetID: muhuiyu2) Ying-Chen Lee (NetID: yclee6)
https://github.com/samhq/CourseProject	tasks.md	Task Breakdown Frontend Tasks Frontend development will be on the Chrome extension itself. Below are the VueJS components to be developed. Login Page User Content Page with Bookmarking and Search functionality Background script with event listeners Backend Tasks The backend tasks are broken into several tasks. User Management TODO: @samiul Crawling and Indexing Check backend/contents.py file. Searching Check backend/search.py file. Prepare Accuracy Test TODO: @samiul to define the tasks @grace and @yingchen to implement Backend api /register For registering a new user /auth For authenticating an user /add For adding a new bookmark for an user /search For searching Dockerization and Deployment TODO: @samiul
https://github.com/kreonjr/CourseProject	Project Progress Report.pdf	University of Illinois, Urbana-Champaign CS410 1 Fall 2021 Project Progress Report Topic: Topic Mining Title: Team Projects Topic Mining and Browsing Team: Topic Thunder Members: * Creon Creonopoulos (team captain): creonc2@illinois.edu * Suhas Ashok Bhat: suhasb2@illinois.edu * Kurt Tuohy: ktuohy@illinois.edu 1) Progress made thus far Our initial focus was surrounded on building an infrastructure and gather all the data we need to work on before we provide it to our algorithms for mining. More specifically: * Using GitHub as our data source, we have been able to write the necessary scripts that gather all the documents containing the text we need to work on Topic Mining and Categorization. * We have implemented a basic dashboard website, hosted it on firebase to make it publicly available, connected it to a backend (also firebase), that gets populated with the final output json that our algorithms will produce. * We have decided the final structure of our data, in order to present the users the course project filtering. This will help with the frontend display and allow users to filter projects per topic in the dashboard * We have completed some data cleaning, removing non-text bases content and we are in the process of deciding how much more cleaning we will do to our data (punctuation, lemmatization, stemming etc.) 2) Remaining tasks We have completed the infrastructure of our project and we are now mostly focused on the algorithmic parts, trying to get a final output for our frontend to consume. In detail: * Implementation of the algorithms to mine topics from the text and perform some clustering to allow for further filtering on the frontend University of Illinois, Urbana-Champaign CS410 2 Fall 2021 * Complete the frontend design by allowing users to filter on topics, allow for course project links to be interactable and potentially implement a very lightweight search method to search for the categories in our data * Evaluate topic modeling and categorizing of our algorithms against a sample of empirically generated topics from a sample of projects 3) Any challenges/issues being faced Although our focus initially was going to only be topic mining from the project submissions, we have decided to also consider some topic clustering and categorization. This will help with our final output data and its display on the frontend dashboard. Furthermore: * At first, many of the python scraping libraries were not able to pull all the text data on GitHub correctly so there were a couple of different libraries that had to be tried out but we overcame that issue. * We have also gone back and forth with our data cleaning implementation, especially surrounding what text symbols we should keep or not (like email symbols or symbols in URLs). We are still in discussion about this item.
https://github.com/kreonjr/CourseProject	Project Proposal for CS410.pdf	"University of Illinois, Urbana-Champaign CS410 1 Fall 2021 Project Proposal for CS410 Topic: Topic Mining Title: Team Projects Topic Mining and Browsing Team: Topic Thunder 1. What are the names and NetIDs of all your team members? Who is the captain? * Creon Creonopoulos creonc2@illinois.edu * Kurt Tuohy ktuohy@illinois.edu * Suhas Ashok Bhat suhasb2@illinois.edu Team Captain will be Creon Creonopoulos 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? Project Topic: Past CS410 Team Projects Topic Mining Description: The objective of our project will be to run one or more topic mining algorithms (Probabilistic Latent Semantic Analysis, Latent Dirichlet Allocation etc.) through all the previous Team Project submissions and split them into thematic categories. We will then create and host a UI Dashboard where students will be able to filter projects by topic instead of having to go through each project separately to figure out what its focus is. This will facilitate discovery of past projects on particular themes. Dataset: We will use a cumulative collection of all previously submitted projects found on GitHub (https://github.com/CS410Assignments/CourseProject/network/members) and we will run our algorithms through each project's Final Reports, Progress Reports and Project Proposals. Tools: * Python libraries to scrape through project reports: GitPython (https://github.com/gitpython-developers/GitPython) and PyPDF2 (https://www.analyticsvidhya.com/blog/2021/09/pypdf2-library-for- working-with-pdf-files-in-python/) * Python libraries to prepare and mine text: NLTK (https://www.nltk.org/) and GenSim University of Illinois, Urbana-Champaign CS410 2 Fall 2021 (https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to- topic-modeling-in-python/) * ReactJS to build the frontend user facing dashboard (https://reactjs.org/) * A database system (Firebase or SQLite) to store the results and source the dashboard. * GitHub Projects for task tracking (https://github.com/kreonjr/CourseProject/projects/1) 3. What is the expected outcome? How are you going to evaluate your work? 1. We expect a user-friendly webpage that will contain a list of links to all the projects on GitHub, which will also contain a filter selector that will list all the mined topics and will be used to only show project links to the selected topic. 2. Our work can be evaluated by making sure that NO project link remains without a topic assigned to it, even for those that do not have the ability to be mined (video reports). 3. Observation-based, e.g., observing the top 'N' words in a topic. 4. Interpretation-based, e.g., 'word intrusion' and 'topic intrusion' to identify the words or topics that ""don't belong"" in a topic or document. 4. Which programming language do you plan to use? * Python for Topic Mining * JavaScript for UI construction 5. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main tasks: * Scrape through GitHub links to find all project reports 8-10 hours * Clean and analyze the data and create a corpus 20-25 hours * Code the algorithms to mine topics and assign each doc to a topic 28- 30 hours * Evaluate Topic creation and document assignment 10-15 hours * Code the database storing mechanism for the categorized data 8-10 hours * Code the frontend to fetch and display the data either as a whole or as filtered 8-10 hours"
https://github.com/kreonjr/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/machilusZ/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/peterzukerman/Twitter-Sarcasm-Detection	README.md	"Our solution and winning model for the Coursera Text Classification Competition TABLE OF CONTENTS Table of Contents About The Project Project Repo Structure Our Approach Understanding Data Getting Started Built With Prerequisites Installation Usage License Contact References Text Classification Competition: Twitter Sarcasm Detection About the Project The goal of this competition/project is to classify a given sequence of tweets (responses) as sarcastic or non-sarcastic. The tweets with its corresponding immediate context and full context is provided as continous responses to each tweets.The tweets are provided with conversation context which is an ordered list of dialogue. The objective of this competition is to predict the ""label"" of the response (tweets) using the given context (either immediate or full context) We present our best model based on BERT (Bi-directional Enconding Representations from Transfomers) using pre-trained stock weights of BERT-Base model and demonstrate the winning solution able to classify sarcasm with F1-score of 76.09%. Project Repository Structure Please follow the links to navigate to respective folders - - Data - Source Code - Results - Documentation - Proposal - Progress Report - Video Presentation - Long Form Video Link - Short Form Video Link - Detailed Project Presentation Our Approach Understanding Data: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | Getting Started All our models are in notebook format (.ipynb) and can be easily replicated using Jupyter / Google Colab or any other notebook environment. We recommend anaconda distribution to create virtual environments for Python and recommend Google Colab for TensorFlow (TF)/Keras Implementations. In order to replicate, reproduce or rerun our BERT model, we recommend downloading pre-trained stock weights as given below Built With Python Google Colab Keras TensorFlow Prerequisites Git Python 3.6 or above. Jupyter or Anaconda distribution Google Colab TensorFlow GPUs Keras BERT Pre-trained Stock Weights. You can download and use pre-trained stock weights of BERT-Base Model from here Installations Following packages/libraries are required for fully functioning of our BERT Model - ```bash install data pre-processing libraries $ pip install genism $ pip install ekphrasis ``` Install/Import the necessary libraries and frameworks ```bash Install key libraries and frameworks $ pip install tensorflow-gpu $ pip install --upgrade grpcio $ pip install tqdm $ pip install bert-fo-tf2 ``` Install/Import the necessary libraries and frameworks ```bash Import the following packages/libraries/frameworks import pandas as pd import numpy as np import tensorflow as ft from tensorflow import keras import bert from bert import BertModelLayer from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights from bert.tokenization.bert_tokenization import FullTokenizer ``` Install/Import the necessary evaluation metrics ```bash from sklearn.metrics import confusion_matrix, classification_report from sklearn import preprocessing ``` Download pre-trained weights from BERT-Base model ```bash !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip !unzip uncased_L-12_H-768_A-12.zip ``` Usage To clone and run our model, you'll need Git or git GUI clients like Git Kraken for Windows or Tower for Mac and Python From your command line or terminal application or git client: ```bash Clone this repository $ git clone https://github.com/dheerajpatta/CourseProject.git Go into the repository $ cd models Install above prerequisties and dependencies pip install * Run the Jupyter Notebook https://github.com/dheerajpatta/CourseProject/blob/main/models/sarcasm_classification_bert_large.ipynb ``` Additional References - - Google Colab - All about setting up Google Colab like a Pro from here - If you want to use BERT with Colab, you can get started with the notebook BERT FineTuning with Cloud TPUs Contact Artsiom Strok (astrok2@illinois.edu) Peter Zukerman (peterz2@illinois.edu) Dheeraj Patta (npatta2@illinois.edu) License Our solution for Text Classificaiton competition - Twitter Sarcasm Detection is licensed under the terms of the GPL Open Source license and is available for free."
https://github.com/AnirudhaPatil/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/DiptamGit/CourseProject	Project progress report-converted.pdf	Offensive Language Detection - Project progress report For our project we have decided to divide our entire project work into four basic modules, - View - Extractor - Analyzer - Repository View For view module we have decided on using react / dash combination, we may switch to static html webpage based on our needs, view will be used for two main reason, one from ui we will input what hashtags we want to search, that will be fed into our extractor system. Also, our ui will have separate dashboard, where will have visual elements of different datasets, like tweet counts, their overall sentiment, etc. Extractor This module will be used to get the tweets for a list of hashtags, we have decided to use Java for this module, we have signed up for Twitter developer account, we are using twitter official hbc api for getting our tweets, twitter/hbc: A Java HTTP client for consuming Twitter's realtime Streaming API (github.com) We have been able to complete the coding of this module, and we were able to get tweets from api successfully for particular hashtags, please find code snippets below, PAGE 1 And we are getting outputs like this, Now we are working on cleaning the tweets, so that we can use them directly to our analyzer module without any deformed text Analyzer We have decided on using Python for this module, there will be python script running in background, where we will feed our cleaned tweets, and we will scan for offensive words in that text and mark that tweet accordingly, also we are planning for doing sentiment analysis of the tweets, we are still deciding on that topic. Repository We plan on storing all our analyzed tweets on mongoDB on cloud, so that we use data from our UI component.
https://github.com/DiptamGit/CourseProject	proposal.md	Sentiment Analyis and Offensive Language Detection By: Riya Gupta, Chitra Uppalapati, and Diptam Sarkar What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - riyag3 (captain), chitrau2, diptams2 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic is conducting sentiment analysis as well as polite and impolite language detection on a set of aroundtop tweets from the daily trending hashtags, dynamically. We are trying to categorize the most popular tweets as positive, negative, depending on the topic. This is an interesting project because it allows us to sense what the overall attitude and feeling is towards certain ideas when examining the most relevant tweets per hashtag. This can allow us to find certain patterns and sentiments in the trending hashtags, which can help us identify how people feel about popular discussions and products as well as how polarized specific topics on Twitter are. This project will be most helpful for identifying sentiment towards political discussions as well as new products. We plan to use Twitter HBC, the Java HTTP client for accessing the Twitter API, to fetch our 1,000 tweets. Then, we will build a system that classifies the tweets. When analyzing our data, we will experiment with different classifiers and evaluate our system using the standard classification evaluations metrics (Precision, Recall, and F-score). The expected outcome is to display our results on a web app that we will create using React. Which programming language do you plan to use? - Python, Java, Javascript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: - Dynamically get tweets from daily trending hashtags (20 hours) - Input hashtags from our web app - Clean our data - store in local - Perform sentiment analysis/language detection (positive, negative,) (20 hours) - Feed tweets into modules for language analysis - Update tweets with language analysis information - Output results on a web app (20 hours) - Create a good UI - Add information on how to use the tool/its purpose
https://github.com/DiptamGit/CourseProject	README.md	About the project There is a growing trend amongst enterprises now a days to do market study using sentiment analysis and profanity checking, when any company releases a new product for announce a merger, they normally introduce a hashtags or campaign slogan, and leveraging that they try to collect user data from social media to get an overview. Our goal pf this project was to provide a unified solution, which anyone can use for researching a keyword or hashtags and do sentiment analysis on that dataset with a view. We tried to make our entire app dynamic and, so that it will readily deployable and open to any further enhancement. Installation Instructions To run the code in your local system you will need four things, assuming you already have git installed Java 8 Maven 3+ Node js 12 Python 3 Installation instructions which I found helpful: - For Java you can use this link, we used grallvm for dev use, but any jdk would work as long as its 8, please refrain from using 11, it may not run spring modules. - Node Js use this link - Python 3, we used Anaconda distribution but technically any 3 distribution should work - Maven 3.6 is being used in this project but you can use any 3 above version as well Once you install and everything is setup, go your command prompt and check if they are in your path and setup was correct. sh $ java -version $ node -v $ python -version $ maven -v If all of them responded correctly, please proceed to the next section How to run the application Clone the application and you should see three main folder - data analyzer - python web and sentiment analysis module - file-read-api - node api for monitoring - twitter-api - spring web module for two twitter api and web views Open three command prompt, and cd into three separte folder run below from twitter api sh mvn -v spring-boot:run run below from data analyzer sh pip install flask pip install nltk pip install preprocessor pip install profanity-check python api.py run below from file-read-api sh node app.js if eveything is running and you can see no error in terminal, open any web browser(except older ie) and type (http://localhost:8080/search.html) ### Tools/Languages - Intellij IDEA - VS Code - Java - Spring - HTML/JS/CSS - Jquery - Bootstrap - NodeJs - Python Team Diptam Sarkar Riya Gupta Chitra Uppalapati ### Project Demo Presentation Credits We referred ideas and codes for inspiration from Spring official documentation, NLTK sentiment analyis examples, Bootstrap 5 docs, freecodecamp sentiment analysis youtube videos
https://github.com/yunfeim2/CourseProject	README.md	CourseProject This project is about reproducing the results for latent aspect analysis. The source code is from the paper provided with the data. Software installation first we should install the nltk tool using the command: import nltk nltk.download() The project will be base on a mix of java and python run the project first we should generator the key work for our latent aspect analysis by using the code python key_generator.py the stopwords.txt is for the stop work that will like occur in all kinds of documents. Therefore we ignore the exsitence of the those words to produce a better keyword collection. After that, download the NLP from https://opennlp.apache.org/ to install and put the enviromental variable to path to the folder you installed Next, we should using java platform to run the analyse.java under src and the final result of the hotel data will be listed under vectors folder in vector_CHI_4000.dat presentation vedio : https://mediaspace.illinois.edu/media/1_64gzbvp4 Sources http://sifaka.cs.uiuc.edu/~wang296/Data/index.html http://sifaka.cs.uiuc.edu/~wang296/ https://www.cs.virginia.edu/~hw5x/paper/p618.pdf
https://github.com/williamdlupo/CourseProject	README.md	PolitiTweet Application Suite PolitiTweet is an application suite that mines US politician tweets, builds a Lucene Index and provides a web based application for querying and displaying results. The deployed, working appliation s located at: https://polititweetui.azurewebsites.net TweetMiner - A C# Azure function that runs at 12pm EST to extract and store the previous 24 hours of tweets of US politicians. Leverages the Twitter API and stores tweets in an Azure database. Repo Url: https://github.com/williamdlupo/TweetMiner SearchApi - A Java Azure function application that builds and queries a Lucene Index built upon tweets stored in an Azure database. Repo Url: https://github.com/williamdlupo/SearchApi PolitiTweet-UI - A .Net Core 3 MVC application that accepts user queries, sends queries to deployed SearchApi and displays matching tweets.
https://github.com/BhuvaneswariPeriasamy/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/rfraser3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/wdchild/CourseProject	documentation.pdf	"Thursday, December 3, 2020 Course Project: Final Report W. Daniel Child Notes on Setup To run this code on your own computer you will need to install Stanza, chromedriver, search engine parser, and BeautifulSoup. Installing chromedriver. You must also have the correct version of chromedriver according to your OS. I can only speak for my own system, but if using this code on a Mac with Catalina (MacOS 10.15.x), then it is critical that you have the appropriate version of chromedriver installed in the folder where the Python code is running. Also, because Catalina has strict developer recognition requirements, you may have to give the driver permission to run. Under System Preferences / Security, you must explicitly bless chromedriver. For details https:// stackoverflow.com/questions/60362018/macos-catalinav-10-15-3-error-chromedriver-cannot- be-opened-because-the-de for details. The version of chromedriver I am uploading to github may not be the one you need for your own computer. Installing Stanza. Theoretically, installation is simple enough using pip. Simply run >>> pip install stanza Although Stanza can be installed using Anaconda for Python 3.7 or earlier, conda installs will not work if using Python 3.8. (I inadvertently tried running stanza from Python 3.7 within conda after a pip install and promptly received missing resource errors. So if you use Anaconda regularly and want to use Stanza, you should install it as follows: >>> conda install -c stanfordnlp stanza Taking this step resolves error messages involving missing resources. Search Engine Parser. It appears that Google has discontinued free options for using their search API. Another option has been developed for Python, however, and it seems to work perfectly well. So I have used that as my starting point. You can install the search engine parser as follows: Warren Daniel Child Progress Report of 1 9 Thursday, December 3, 2020 >>> pip install search-engine-parser This search-engine-parser supports different search engines, including Google, Yahoo, and (allegedly) Bing. However, during tests, Bing failed miserably, flagging searches as possibly illegal, so I chose not to use it. I did compare Yahoo and Google for the query `Zi ' (literally ""black characters"" but also the name of a flower, in Japanese) and found that they had very different results. Accordingly, I wrote code that made it possible to use either or both of those engines. Project Objective As delineated in the project proposal, the purpose of this project is to develop an application that can help Japanese translators search for contextual information on rare Japanese patent terms and expressions, and to show those terms and expressions in the context where they are used. When one searches for rare patent terms, search engines routinely return irrelevant pages, and one has to hunt for relevant information out of a large number of retrieved urls. From a translator's perspective, it would be extremely helpful to have concise term reports that contain extracts showing exactly where the expression is used. As a bonus, this project also includes automatically generated syntactic analyses of the extracts that are retrieved. Why Is Finding Terms Problematic In Japanese patents, one frequently encounters obscure terms that are not readily found in dictionaries. In addition, because Japanese does not partition words by spaces, parsers routinely make mistakes in determining word boundaries. In the case of rare terms, the tendency is to break a longer term into something shorter. As a result, search engines like Google frequently return text retrieval results that do not in fact contain the full term in question. For example, imagine you want to find an expression that contained the characters ABCDE. As it turns out, if BC is considered to be a word, and if DE is considered to be a word, then a typical browser search is liable to return urls that match BC, DE, or even BCDE, but not the full expression ABCDE. Such returned urls are often unhelpful. Because so many pages end up being off topic, the translator is frequently forced to browse a large number of irrelevant pages before finding one that contains the term of interest. The purpose of this project is to make it easy to locate pages that actually contain the full term in Warren Daniel Child Progress Report of 2 9 Thursday, December 3, 2020 question (ABCDE) and to then use Stanza's NLP parser to analyze passages that actually contain the data. Application Structure There are three main pieces of code: analyzer.py scraper.py search.py Now, search.py is basically a main function where one can adjust parameters before running a search and conducting an analysis. The other files  namely, scraper.py and analyzer.py,  implement classes that respectively perform the scraping and analysis methods. Running the Application The analyzer and scraper files serve to implement Analyzer and Scraper classes that I created. Creating classes in this way made it possible for me to encapsulate nearly all of the functionality that is needed for this project. This makes running the code is extremely simple. To run the application, you simply need to set three parameters or variables within search.py: (1) the expression to search (e.g. target = Zi ) (2) whether you want to use a Google or Yahoo engine for the search (3) the maximum depth of the search The expression here should be Japanese. I have provided a number of expressions for testing purposes directly into the code. To run one program, you simply need to copy the correct variable names into the class constructor: scraper = Scraper(target = <target variable name here>, max_depth = <set depth value here>, engine = <""google"", ""yahoo"", or ""both"" (default)>) The search engine parser returns a page of results that one would see on a normal browser. Because I am using a headless browser, one page of results corresponds to approximately 8 - Warren Daniel Child Progress Report of 3 9 Thursday, December 3, 2020 10 urls. The max_depth variable indicates how many pages of urls you want to explore (a depth of 5 would correspond to approximately 50 urls). To avoid long runtimes during project exploration, you may want to keep this to 1. Operation Once these parameters have been set, the line scraper.run_search() will run code to search each page for the target expression. There is a lot that goes on behind the scenes. Scraper operations can be summarized as follows: until max depth is reached or while no matches are found... process a set of pages by obtaining the html (""soup"") for each page searching for the expression on each page collecting text passages where the expression is found The biggest issue here was cleaning up the ""soup"" produced by BeautifulSoup. Most sources recommend ""decompose"" but it turned out to not be that helpful. A more productive approach was to regular expressions to find if the expression was found, and to indicate which element contained the target expression, and then return that element's text. The results will indicate whether a given url has matches, as follows: Url 0 has matches for Chu Lai Xing  a Chu Lai Xing  Url 1 has no matches for Chu Lai Xing  Url 2 has matches for Chu Lai Xing  h3 2014.12.02 Chu Lai Xing toChu Lai Gao noWei i p Tu Mu Jian Zhu Yong Yu ni, [Chu Lai Xing ] to[Chu Lai Gao ] gaarimasuga, kono2tsuhatotemoyokuSi taYan Xie desuga, Yi Wei haYi na rimasu.  Warren Daniel Child Progress Report of 4 9 Thursday, December 3, 2020 Syntactic Analysis After the max depth has been reached and all urls have been explored, then operations are conducted by the analyzer, whose main function is to utilize Stanza functions. The analyzer obtains the data that was stored by the scraper under its ""results"" instance variable and applies Stanza parsing and analysis to that data. This analysis shows that the word in question is similar to another but has a different meaning. 2014.12.02 CD NUM Chu Lai  VV VERB Xing  NN NOUN to PS ADP Chu Lai Gao  NN NOUN no PN ADP Wei i NN NOUN Tu Mu  NN NOUN  SYM SYM Jian Zhu  NN NOUN Yong Yu  NN NOUN ni PS ADP ,  SYM PUNCT [ SYM PUNCT Chu Lai Xing  NN NOUN ]  SYM PUNCT to PQ ADP [ SYM PUNCT Chu Lai Gao  NN NOUN ]  SYM PUNCT ga PS ADP ari VV VERB masu AV AUX ga PC SCONJ ,  SYM PUNCT kono JR DET 2 CD NUM tsu XSC NOUN ha PK ADP totemo RB ADV yoku RB ADV Si  VV VERB Warren Daniel Child Progress Report of 5 9 Thursday, December 3, 2020 ta AV AUX Yan Xie  NN NOUN desu AV AUX ga PC SCONJ ,  SYM PUNCT Yi Wei  NN NOUN ha PK ADP Yi nari VV VERB masu AV AUX .  SYM PUNCT Parts of speech are indicated in the analysis. Because the default printout is rather long, I have used a more tabular approach then the default approach used by Stanza At the end, the extracted text and Stanza analysis are written to file, and if all goes well, the program terminates. Project Evaluation Being able to get clean text containing the target expression is a huge win. The analysis from Stanza (shown above on this page) is helpful as well. The project has been successful in that it does what it is supposed to do. Avoiding Occasional Pitfalls * Chromedriver Errors Twenty hours is not enough time to develop a truly robust parsing function. Things sometimes go wrong when scraping. The page may no longer exist, or it may have an anti-scraping function embedded in it. Such issues seem to throw an error within chromedriver itself. Here is an example of what can happen: Traceback (most recent call last): File ""search.py"", line 29, in <module> scraper.run_search() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 106, in run_search self.process_page_set(self.current_page_set) Warren Daniel Child Progress Report of 6 9 Thursday, December 3, 2020 File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 99, in process_page_set self.get_soup(retrieved_urls[url_num]) File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/scraper.py"", line 64, in get_soup self.driver.get(url) (ENTERING GOOGLE CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/ remote/webdriver.py"", line 333, in get self.execute(Command.GET, {'url': url}) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/ remote/webdriver.py"", line 321, in execute self.error_handler.check_response(response) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/ remote/errorhandler.py"", line 242, in check_response raise exception_class(message, screen, stacktrace) selenium.common.exceptions.WebDriverException: Message: unknown error: net::ERR_CONNECTION_CLOSED (Session info: headless chrome=87.0.4280.67) There is not much I can do about errors internal to chromedriver, so to handle the issue more gracefully, I wrapped the driver's get(url) function in a try-except block. If html retrieval fails, then the program will move on to the next url. This seems to have solved the problem, though many more hours of testing would be needed to make sure every conceivable problem could be anticipated. * Stanza Errors It turns out that Stanford's Stanza wrapper for CoreNLP is also buggy. Consider this error: Traceback (most recent call last): File ""search.py"", line 44, in <module> analyzer.analyze_data() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/analyzer.py"", line 24, in analyze_data doc = self.nlp(d) (ENTERING STANZA CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__ doc = self.process(doc) Warren Daniel Child Progress Report of 7 9 Thursday, December 3, 2020 File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process doc = self.processors[processor_name].process(doc) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/ pos_processor.py"", line 30, in process sort_during_eval=True) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/ data.py"", line 48, in __init__ self.data = self.chunk_batches(data) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/ data.py"", line 150, in chunk_batches (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data]) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/ common/data.py"", line 39, in sort_all return sorted_all[2:], sorted_all[1] IndexError: list index out of range This seems to have occurred because Stanza got confused when parsing a very long text passage. To circumvent such errors, I similarly employed a try-except block. * File Write Errors Because the data that is written to file for future analysis is based on Stanza, analogous errors can appear when writing to file. *** WRITING DATA TO FILE: Chu Lai Gao  analysis.txt *** Traceback (most recent call last): File ""search.py"", line 45, in <module> analyzer.write_analysis() File ""/Users/danielchild/Desktop/TIS/PROJECT/CourseProject/ProjectCode/analyzer.py"", line 41, in write_analysis doc = self.nlp(d) (ENTERING STANZA CODE BELOW) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 166, in __call__ doc = self.process(doc) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/core.py"", line 160, in process doc = self.processors[processor_name].process(doc) Warren Daniel Child Progress Report of 8 9 Thursday, December 3, 2020 File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/pipeline/ pos_processor.py"", line 30, in process sort_during_eval=True) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/ data.py"", line 48, in __init__ self.data = self.chunk_batches(data) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/pos/ data.py"", line 150, in chunk_batches (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data]) File ""/Users/danielchild/opt/anaconda3/lib/python3.7/site-packages/stanza/models/ common/data.py"", line 39, in sort_all return sorted_all[2:], sorted_all[1] IndexError: list index out of range Once again, to make sure that the program exits gracefully, I used a try-except block. Assessment of the Analysis Being able to quickly isolate instances where a particular expression is used, while bypassing web pages that contain portions of the expression but not the entire expression, is highly useful. The Stanza analysis is somewhat erratic: given two different text passages, a portion of the expression may sometimes be interpreted as a verb, and elsewhere as a noun, even though the usage is exactly the same in both contexts. Clearly more work needs to be done on Stanza's end. Future Avenues of Development I have already gone well over 30 hours on this project, but if I had more time I would add functions to check for definitions specifically, as well for English translations of the terms. I would also like to explore other Japanese morphological analyzers to see if they perform better. Still, in its current form, this project does what it is supposed to, and should provide a solid foundation for people who want to develop a more robust and function-filled application. Warren Daniel Child Progress Report of 9 9"
https://github.com/wdchild/CourseProject	project proposal.pdf	Captain: Warren D. Child (only member) UIUC ID: wchild2 Email: wchild2@illinois.edu PLEASE NOTE: My original CMT link had an incorrect email address and is invalid. The email above is correct. Topic: Japanese Patent Term Retrieval: Identifying the Immediate Context in Which Japanese Patent Terms Are Used for Linguistic Comparison Language: Python Other tools: Google Programmable Search Engine Kuromoji (Japanese Morphological Analyzer) Stanza (Python Wrapper for Stanford Core NLP) Abstract: Japanese patents use a large number of fairly esoteric terms and expressions that are not found in standard dictionaries, and that can therefore be tricky to translate. Such terms are a headache for patent translators, and typically one needs to look at a number of documents where the word is employed to understand its significance. This project seeks to build on top of the standard text retrieval capacities of a major search engine like Google and add functionality that will make it easy to quickly compare sentences containing such a term. Providing laser-focused contextual usage should prove beneficial for technical translators. Discussion: Unlike normal text retrieval, where one is looking for topic relevance in documents, here one is looking for the correct usage of terms in context. For example, in normal text retrieval, the presence of a term within the title might suggest a higher level of relevance. But from the standpoint of understanding how a word is used in context, it is actual sentences that one hopes to find. The topic and the term are not necessarily that closely related. Difficulties: Obviously, one diffi culty will be with accurately parsing words in Japanese, which does not employ spaces to parse separate words. Japanese kanji (hiragana and katakana) can sometimes help to indicate word boundaries, though as often as not, hiragana is used in place of kanji. Equally troublesome is the tendency to sometimes use hiragana (or even katakana) in place of characters, or to use variant characters for the same character. I will be researching ways to address this issue. It is unclear how successful Kuromoji will be with more esoteric patent terms. If it does not work or does not allow one to supplement their dictionaries, I may have to use another tool (Stanford's Stanza being a possible alternative). Added Functionality: Taking the search results from the initial search, I will be parsing the retrieved documents to identify context sentences where the critical term occurs. Those sentences will be retrieved in a single document for comparison purposes. Search results that do not have the requisite vocabulary, or only have it in a non-sentence, will be eliminated from the results. Success Benchmarks: If I am able to routinely prepare clean sets of sample sentences for problematic Japanese patent-specific terms, I will consider this successful. Often, search results return a potentially useful document, but because of document length, it is hard to see where the relevant portion or portions are. Having cleanly parsed relevant sample sentences would be a great time-saver for translators and other linguists. Hours Expended: Between having to learn the Kuromoji API, possibly Stanza, features of the Google Programmable Search Engine, and writing scripts to parse the relevant parts of the returned documents should take more than 20 hours. Note: I discussed this topic proposal with the professor, and the idea of adding functionality to Google was actually his suggestion. I am open to more suggestions on how to make this a useful tool for persons like myself who work as translators.
https://github.com/wdchild/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/wdchild/CourseProject	wdc_progress_report.pdf	"Saturday, November 28, 2020 Course Project Progress Report W. Daniel Child Project Objective As delineated in the project proposal, my goal is to develop a suite of search functions that make it easier to search for rare Japanese patent terms and to show those terms in context. With rare terms like this, search engines often return irrelevant pages, and one has to hunt for the immediate context in question. From a translator's perspective, it would be extremely helpful to have concise reports on term contexts to understand how the terms are used. The following is a report on what I have accomplished so far. Setting Up Search Engine Capabilities It appears that Google has discontinued free options for using their search API. Another option has been developed for Python, however, and it seems to work perfectly well. So I am using that as my starting point. So I installed the search engine parser as follows: pip install search-engine-parser It turns out that this search-engine-parser supports different search engines, including Google, Yahoo, and (allegedly) Bing. However, Bing failed miserably, flagging searches as possibly illegal, and so I didn't use it. I did compare Yahoo and Google for the query `Zi ' (""black characters"" in Japanese) and found that they had very different results. Google Top 3 'https://www.google.com/url?q=http://verdure.tyanoyu.net/ kuromoji.html&sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAZ6BAgHEAE&usg=A OvVaw1Bg6RJJuLCKaDfYsL1sHWf' 'https://www.google.com/url?q=http://www.jugemusha.com/jumoku-zz- kuromoji.htm&sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAd6BAgIEAE&usg=AO vVaw17dFX2w6L1A6Nug5SQGJT6' Warren Daniel Child Progress Report of 1 3 Saturday, November 28, 2020 'https://www.google.com/url?q=https://www.nakagawa-masashichi.jp/shop/g/ g4547639507716/ &sa=U&ved=2ahUKEwjInaflt6btAhWDFVkFHeg5C2E4ChAWMAh6BAgJEAE&usg=AOvVaw2JVwx- Z7pwG8sEWU7Vb9Su' Yahoo Top 3 'https://ja.wikipedia.org/wiki/ %25E3%2582%25AF%25E3%2583%25AD%25E3%2583%25A2%25E3%2582%25B8' 'https://www.weblio.jp/content/ %25E9%25BB%2592%25E6%2596%2587%25E5%25AD%2597' 'https://search.rakuten.co.jp/search/mall/ %25E9%25BB%2592%25E6%2596%2587%25E5%25AD%2597/' Spot-checking the different websites, I actually found some of the Yahoo websites more helpful. I have therefore decided to incorporate both search engines into the patent search system. Parser Once you have a list of web pages, the next step is, of course, to be able to parse these individual pages. I am currently developing the parser (based on BeautifulSoup) to look for the target terms. Since the point is to understand rare terms from context, I need to see sentences and not just headings or titles, which will not be of much help from a context standpoint. I am also going to develop a capability that is sensitive to the possibility of the term being defined, and I am also entertaining the possibility of looking for cases where the term is actually translated into English. Text Analysis and Issues Being Faced I have already tested Stanza (Stanford's Python wrapper for Stanford CoreNLP) and it is looks to be capable of handling Japanese so long as you install it correctly (you need to pay attention to whether you are using vanilla Python or Python via an Anaconda environment) and utilize the right module. Given its inability to properly parse some fairly straightforward terms, however, it is equally clear that the terms I will be testing against will not tend to be in the Stanza dictionaries, meaning that they will not be parsed correctly. Warren Daniel Child Progress Report of 2 3 Saturday, November 28, 2020 That may not matter. I will be able to recognize such terms as appearing in consecutive lemmas. Stanza has a function that enables you to parse sentences, so once I have identified the surrounding context, I should be to pull the context that I am looking for. I have not yet decided whether to it will be necessary to incorporate alternative Japanese tokenizers such as Juman or Kuromoji. What Remains to Be Done Obviously, I need to continue working on the parser, and to make it successfully pulls out the information I need from each of the web pages returned by the search engine parser. Stanza sentence identification needs to be smoothly integrated so that it goes to work on the data extracted by the web parser. Once this has been done, I want to demonstrate how many websites could be bypassed by using this system, and how much easier it is for a translator like myself to find contextual information about rare terms. The final step will be to generate clean reports based on the data that I have extracted. Warren Daniel Child Progress Report of 3 3"
https://github.com/chiragcshetty/CourseProject	README.md	CourseProject Student: Chirag C. Shetty (cshetty2) Paper: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 [link] Introduction The paper explores a further improvement like PLSA in mining topics. In PLSA, k topics are mined from the entire collection. However, collection may have subset and we may be interested in knowing the topics within a collection while also comparing across different collections. The paper adds one more level of generative variable (lambda_c) and tries to achieve this. Data The original paper from 2004 had used a set on news articles and reviews fro epionions.com. The site is no longer active and the dataset wasn't archived anywhere. So I decided to write a scraper, starting with the codes used in the MP's. I chose CNN, which has a search feature on its webpage. So I scrap the webpage resulting from searching a topic of interest and extract the news articles. This mostly involved handcrafting the extraction process. Procedure for scraping The main python file is called scrap.py Edit the 'name' variable to indicate the topic. Files extracted will be stored with this name no_pages: Number of pages to search. Each page has 10 articles Run scrap.py (tested for python3.5), by setting dir_url to a topic search page on cnn webpage Example: For example this webpage shows for the search 'election': https://www.cnn.com/search?q=election run python (3.5 used) scrap.py. The extracted docs will be stored in the folder 'cnn' You can run it for as many topics as you wish Baseline model For baseline, the paper uses the standard PLSA model. Starting with PLSA code from MP3, background model was added. Thus complete PLSA was implemented at plsa_proj.py. Cross-Collection Mixture Model The model is implemented at ccmix.py. Following at the EM update equations from the paper Procedure: Run scrap.py, by setting dir_url to a topic search page on cnn webpage. Set appropritae variables as described in scrap.py Set N - number of docs of each kind in the collection name_set=list of names of each collection eg: ['elon','bezos'] Set number_of_topics Run the code The output displays top_n words in each distribution Important notes 1) In calculating c(w,d) that count of word w in doc d across all words and docs, smoothing must be applied. No c(w,d) should be exactly 0. Esle it'll cause divison by zero problem. In the code, term_doc_matrix stores c(w,d) 2) In the EM update steps given in the paper, observe the update for P(w/theta j,i) i.e the collection specific word distributions. Since both numerator and denominator are summed over the entire collection, P(w/theta j,i) will not capture features specific to the sub-collections. They will all behave similarly. Hence in implementation, the summations are only taken over the docs in collection concerned
https://github.com/chiragcshetty/CourseProject	Report_Project_Cross-Collection_Mixture_Model.pdf	CS 440 Project: Cross-Collection Mixture Model 1 CS 440 Project: Cross-Collection Mixture Model Student: Chirag C. Shetty (cshetty2 Paper: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining KDD 2004. ACM, New York, NY, USA, 743748. DOI10.1145/1014052.1014150 [link] Github link: https://github.com/chiragcshetty/CourseProject Introduction The paper explores a further improvement like PLSA in mining topics. In PLSA, k topics are mined from the entire collection. However, collection may have subset and we may be interested in knowing the topics within a collection while also comparing across different collections. The paper adds one more level of generative variable (lambda_c) and tries to achieve this. Data The original paper from 2004 had used a set on news articles and reviews fro epionions.com. The site is no longer active and the dataset wasn't archived anywhere. So I decided to write a scraper, starting with the codes used in the MP's. I chose CNN, which has a search feature on its webpage. So I scrap the webpage resulting from searching a topic of interest and extract the news articles. This mostly involved handcrafting the extraction process. Procedure for scraping The main python file is called scrap.py  Edit the 'name' variable to indicate the topic. Files extracted will be stored with this name  no_pages: Number of pages to search. Each page has 10 articles  Run scrap.py (tested for python3.5, by setting dir_url to a topic search page on cnn webpage Example: For example this webpage shows for the search 'election': https://www.cnn.com/search?q=election CS 440 Project: Cross-Collection Mixture Model 2  run python 3.5 used) scrap.py. The extracted docs will be stored in the folder 'cnn'  You can run it for as many topics as you wish Baseline model For baseline, the paper uses the standard PLSA model. Starting with PLSA code from MP3, background model was added. Thus complete PLSA was implemented at plsa_proj.py. Cross-Collection Mixture Model The model is implemented at ccmix.py. Following at the EM update equations from the paper Procedure:  Run scrap.py, by setting dir_url to a topic search page on cnn webpage. Set appropritae variables as described in scrap.py  Set N - number of docs of each kind in the collection  name_set=list of names of each collection eg: ['elon','bezos']  Set number_of_topics  Run the code  The output displays top_n words in each distribution Important notes CS 440 Project: Cross-Collection Mixture Model 3 1 In calculating c(w,d) that count of word w in doc d across all words and docs, smoothing must be applied. No c(w,d) should be exactly 0. Esle it'll cause divison by zero problem. In the code, term_doc_matrix stores c(w,d) 2 In the EM update steps given in the paper, observe the update for P(w/theta j,i) i.e the collection specific word distributions. Since both numerator and denominator are summed over the entire collection, P(w/theta j,i) will not capture features specific to the sub-collections. They will all behave similarly. Hence in implementation, the summations are only taken over the docs in collection concerned Experiments and Results To experiment we need related document sub-collections, each of which have a common theme. One good example of such a collection is about famous people in related fields. I chose 'Elon Musk' and 'Bill Gates'. Both are billionaire businessmen, hence there will be similarities in news articles about them. However of-late they are in news for very different reasons. So each sub-collection has its own features. Articles are scrapped from cnn, in order of recency and stored in folder cnn. There are 29 files for each category. PLSA Baseline lamba_b is kept at 0.9. Increasing it too much seemed to include informative but words into background model. The word 'tesla' for instance for the chosen dataset. Decreasing lamba_b too much lets stopwords leak into topic distributions. Around 0.9 seemed the right compromise. Number of topics is taken to be 2 PLSA gives the following result: Clearly the topic 0 refers to 'Elon Musk' with words like 'tesla', 'rocket', 'texas' Musk moving to Texas has been in news a lot recently). Topic 1 is not as clearly associated with Gates. However given Gates' charity work, especially in healthcare, the topic 1 makes sense CS 440 Project: Cross-Collection Mixture Model 4 CCM Model lamba_b was retained at 0.9 and lambda_c was taken as 0.7. With same dataset as above this is the result of CCM With lamba_c = 0.6 lamba_c= 0.4 CS 440 Project: Cross-Collection Mixture Model 5 The difference between Collection 1 and collection 2 specific distribution is stark and informative. They clearly are clustering around Musk's articles and Gates' article. However, the topics themselves do not show much distinction. There is some flavour of topic 0 being about technology while topic 1 is about society and governance. But one can not decisively say so. It might also be possible that the articles donot have enough variety to force the EM to cluster well. Better data with more latent topics may reveal further benefits of the CCM model. Also, knobs of lambda_c and lambda_b can be optimized further. ____________________________________________________________________________________________
https://github.com/jiezheng5/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/pdwivedi08/CourseProject	CS 410 - Project Proposal.pdf	CS 410 - Text Information System: Text Classification Competition Team Members: Harsangeet Kaur (kaur13@illinois.edu): Team Member Pradeep Dwivedi (pkd3@illinois.edu): Team Lead Competition: Text Classification Competition Programming Language: We are planning to use Python as the programming language Proposal Details: We are new to machine learning and have no prior experience implementing language processing models. However, we are very interested to learn SOTA language processing models. We plan to use LSTM, GPT3 and other transformers to achieve best classification results.
https://github.com/pdwivedi08/CourseProject	Project_Progress_Report_Classification_Competition.pdf	Progress Report for Tweet Classification Competition About the project: Our team is working on classifying the tweets in sarcasm and non-sarcasm categories. We intend to use deep learning algorithms in natural language processing to classify the tweets and get highest level of accuracy, thus, not only beating the baseline but will also aim to secure the top rank in the competition. Progress made so far: We've analyzed the problem carefully. We looked into multiple deep learning algorithms which can be used to solve our problem. We also looked into multiple transformers based deep learning algorithms too, to solve our problem. We did lot of data cleaning in the input file, so as to feed the right input to the classification algorithm. We used Python's Pandas, Reg Ex on Anaconda's Jypyter notebook for data cleaning. Based on the algorithms, which we tried so far, we've been able to solve the tweet classification problem and beat the baseline. We are currently ranked #16 in the leaderboard. We'll however try to improve on our rank. We've used Google's T5 based classification transformer to receive best result so far on the classification problem. On the Leaderboard in Livelab, our submission can be found with the id: pdwivedi08 Remaining Tasks: We'll continue to try to improve our ranking in the competition. Additionally, we've following tasks left: 1. Detailed documentation of the project 2. Presentation on the project execution 3. Code cleaning and comments update Challenges: We don't have any major challenge at this time in the project completion. We did face some challenge earlier to decide the right algorithm to use and our research and self-study did help in that.
https://github.com/pdwivedi08/CourseProject	README.md	Tweet Classification Competition About the Project We've worked on the text classification competition project for the tweets. The train and test datasets are provided as part of competition and we intend to use current state-of-the-art machine learning NLP algorithms to beat the baseline on this text classiciation competition project. Presentation Link on Youtube Please refer below youtube link for the voice-over presentation for our project: https://youtu.be/H1xQwJkV5cA Team members: Harsangeet Kaur (kaur13@illinois.edu): Team Member Pradeep Dwivedi (pkd3@illinois.edu): Team Lead Our submission can be found in the spreadsheet available in CMT, with the ID: pkd3@illinois.edu Our submission on the Leaderboard in Livelab can be found for the ID: pdwivedi08 Overview This software can be used to classify tweets in Sarcasm and Not-Sarcasm categories. This can't be however, used for any other text classification or sentiment analysis with same level of accuracy or F1-score. This software achieves the high level of precision, recall and F1 score as against the generic transformers, since it has been especially trained on the tweet classification. Implementation Documentation We've made use of Google's T5 based fine-tuned transformer for twitter sarcasm detection. This model has been trained to identify sarcasm on tweets. We've used the Google Colab notebook to train the model and it took nearly 12 hours to train the model with the given data, on single GPU. We used the Trainer API from Huggingface to write the training code as it's easier to use. Also, we used the autotokenizer from the transformers library in Huggingfacce. We cleaned the test data for training and testing in such a way that the tweets are taken in correct sequence - first the orginal tweet and then it's responses in the chronolocial order. Also, we've removed all the filler words using regex and regular python functions from the tweets before using them for training and testing. We defined a function eval_conversation, to evaluate the curated tweets one-by-one and provide the output in Sarcasm and Not-Sarcams categories. We tried support vector machines (SVM) and T5 based transformer for this project. We got following values of precision, recall and F1 score with both these algorithm: precision recall f1 SVM 0.48314606741573035 0.14333333333333334 0.22107969151670953 T5 based 0.7030114226375909 0.7522222222222222 0.726784755770263 The second approach i.e. the use of T5 based transfomer helped us to beat the baseline. Our final execution matrics can be viewed on the Leaderboard in Livelab, for the id: pdwivedi08 Usage Documentation All the code of the software is written in the jupyter notebooks, which can be opened from Anaconda IDE. 'classifyTweets.ipynb' is the main notebook which has the code to execute the test dataset. The 'test.jsonl' is stored inside the Data directory and the directory is included in the github. All other libraries needed to execute this code, are part of the 'classifyTweets.ipynb' notebook and would be imported when the notebook is executed. Therefore, no additional installation of any module is needed. The project github has the video demostration of the code execution as well and that can be used to install and run this software. Since the model is running T5 based transformer and the code has few displays, it will take around 5-7 minutes for the execution of the whole notebook on a macbook pro of 8 GB memory. The execution speed in-general will vary based on the hardware of the machine, used for running the notebook. For any further question related to the installation or the working of the software, please contact our team, at the below email IDs: Harsangeet Kaur (kaur13@illinois.edu) Pradeep Dwivedi (pkd3@illinois.edu) Detail of the Contributions of Team-members Our team didn't has any prior background in natural languange processing(NLP) or machine learning (ML). Therefore, we started with understanding ML in the context of NLP and reading about it, online and on forums. Huggingface.co greatly helped us understanding the deep learning aspect of ML on NLP. Harsangeet Kaur tried support vector machine algorithm for text classification whereas Pradeep Dwivedi tried transformers for solving the problem. Together, we worked on the data cleaning, model training, software documentation and preparing the final presentation. References https://huggingface.co/mrm8488/t5-base-finetuned-sarcasm-twitter stackoverflow.com https://huggingface.co/transformers/main_classes/trainer.html https://www.w3schools.com/python/python_regex.asp https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34
https://github.com/srashee2/CourseProject	CS410 Progress Report.pdf	CS410 Progress Report Team Starks Saad Rasheed - srashee2 - Javier Huamani - huamani2 Sai Allu - allu2 Which tasks have been completed? Our team has currently taken the The New York Times Annotated Corpus dataset and extracted the data given to us into more useful excel and CSV files. Our team has taken these csv files and have used different libraries (gensim, pandas, sklearn, and nltk) to create an example of topic modelling with time series feedback. Which tasks are pending? To successfully recreate the paper our team still needs to find the stock time series data and run that against our code, while refining the code to find the optimal execution. Lastly, we have to implement measures of quality such as a Granger test. 3) Are you facing any challenges? The stock time series data seems to be not referenced in the paper. We also have an issue with deterministic / non-deterministic results when running our code.
https://github.com/srashee2/CourseProject	Project Proposal Submission.pdf	Project Proposal Submission 1. Saad Rasheed - srashee2 - Team Leader Javier Huamani - huamani2 Sai Allu - allu2 2. Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback a. https://www.biz.uiowa.edu/faculty/trietz/papers/ITMTF.pdf 3. Python 4. Yes a. https://catalog.ldc.upenn.edu/LDC2008T19
https://github.com/srashee2/CourseProject	README.md	CS410 Final Project: Iterative Topic Modeling with Time Series Feedback Team Starks Saad Rasheed - srashee2 Javier Huamani - huamani2 Sai Allu - allu2 Demonstration https://youtu.be/BC6qcxoF8XQ Purpose Team starks set out to recreate Mining Causal Topics in Text Data Title Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Authors Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier Citation Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Introduction In our final project we create a text mining application to find causal topics from corpus data. Our application takes a probabilistic topic model and using time-series data we explore topics that are causally correlated with said time-series data. We improve on the topics at each iteration by using prior distributions. To determine causality we are using Granger causality tests which is a popular testing mechanism used with lead and lag relationships across time series. Libraries pandas - Used for data manipulation and analysis scikit-learn - Used for classification, regression and clustering algorithms nltk - Used for symbolic and statistical natural language processesing pyLDAvis - Used to help interptret topics from a LDA topic model statsmodels - Used for statistical computations Files Corpus Data * NYT2000_1.csv * NYT2000_2.csv IEM Stock Data * IEM2000.xlsx Main Application * LDA.py Code Walkthrough We begin by reading in the corpus data that we segmented into two files to be able to store alongside the code. We then clean the data by removing NaN values and other filtering. We do more filtering and remove unneccesary characters and then lemmatize the data. We then read in the IEM data and normalize it. We then take the corpus data and generate the counts and vocabulary to create a document term matrix Now we're able to fit the LDA model, we use 15 topics and have found that number to be optimal. For each date we create a topic stream and aggregate topic coverages and plot them. To evaluate causality we then run Granger tests against each topic and output the p values for the f Tests against each lag. To determine the optimal lag value we aggregate p values. We sort the p values in ascending order. Of the top 25 words for each topic we run granger causality tests and pearson coefficient tests. We only continue if we get a p value of less than .05. To actually create the priors we evaluate a topic based on its negative or positive bias. If a topic has a dominated negative or positive bias we create a prior for each word and assign it to a single topic. Conversely, if there is no negative or positive bias we split the word into two topics and assign it to a single topic. Our code then iterates using the generated prior (On the first iteration the priors are empty) and fits the LDA model again according to the max iteration. How to run The easiest way to run our code is to download Anaconda and run it through jupyter notebook $ git clone https://github.com/srashee2/CourseProject.git $ cd CourseProject $ jupyter notebook You can now click on LDA.ipynb and click run all cells. It will take some time to run through the code, approximately 1 hour. Contributions Team Starks came together over the course of a few months with weekly meetings to understand, learn and recreate Iterative Topic Modeling with Time Series Feedback. More specific contributions for the team members can be found below. All team members did the following: library research, paper breakdown and documentation. Saad Rasheed - Logistical work, Corpus Text Extraction, Presentation, and LDA modeling iteration Javier Huamani - Text Filtering and Manipulation, LDA Modeling, Granger Causality, and Pearson Coefficient Tests Sai Allu - Text Filtering and Manipulation, LDA Modeling, Granger Causality, and Presentation
https://github.com/zen030/CourseProject	Project Progress Report.docx	Project Progress Report Project Topic: BERT Sentiment Analysis to Detect Twitter Sarcasm Project Team Member Name: Zainal Hakim NetID: zainalh2 Result (answer.txt) Progress status: 100% (Completed) The project scores (beating the baseline): F1-Score Recall Precision Project Result 0.757905138339921 0.8522222222222222 0.6823843416370107 Baseline 0.723 0.723 0.723 Software Code Progress status: 100% (Completed) Source code files: Training and Evaluation (link) Evaluation (for DEMO) (link) Documentation Progress status: 100% (Completed) The document files: Proposal (link) Project Documentation (link) Demo Video Progress status: 100% (Completed) Demo video URL: https://www.youtube.com/watch?v=PsYn2lUWpQg Challenges in The Project To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project.
https://github.com/zen030/CourseProject	Project Progress Report.pdf	Project Progress Report Project Topic: BERT Sentiment Analysis to Detect Twitter Sarcasm Project Team Member * Name: Zainal Hakim * NetID: zainalh2 Result (answer.txt) * Progress status: 100% (Completed) * The project scores (beating the baseline): F1-Score Recall Precision Project Result 0.757905138339921 0.8522222222222222 0.6823843416370107 Baseline 0.723 0.723 0.723 Software Code * Progress status: 100% (Completed) * Source code files: o Training and Evaluation (link) o Evaluation (for DEMO) (link) Documentation * Progress status: 100% (Completed) * The document files: o Proposal (link) o Project Documentation (link) Demo Video * Progress status: 100% (Completed) * Demo video URL: https://www.youtube.com/watch?v=PsYn2lUWpQg Challenges in The Project * To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. * It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. * Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project.
https://github.com/zen030/CourseProject	Project_Documentation.docx	"BERT Sentiment Analysis to Detect Twitter Sarcasm (Naive Approach) Zainal Hakim zainalh2@illinois.edu Table of content A. Introduction 3 B. Bidirectional Encoder Representations from Transformers (BERT) 3 C. Dataset Description 4 D. The Naive Approach 4 E. The Model, Training, and Evaluation 5 F. The Software Code 6 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review 6 1. Colab Configuration 6 2. Mounting Google Drive to Colab session (To save result files) 9 3. The Main Python Class 10 4. Training and Evaluation experiments 13 5. Save the result files to Google Drive 14 F.2. DEMO_Model_Evaluation.ipynb Code Review 15 1. The first 3-steps are already explained in detail in the previous section 15 2. Preparing the encoded testing dataset and data loader 15 3. Run the evaluation batch iteration 16 4. Generate the 'anwer.txt' file 17 5. Post 'answer.txt' to LiveDataLab for scoring 17 G. Result and Conclusion 18 Reference 18 Appendix 19 1. answer.txt 19 Introduction Sarcasm is a form of figurative language that implies a negative sentiment while displaying a positive sentiment on the surface (Joshi et al., 2017). I present a Naive approach to detect Twitter tweet sarcasm sentiment using a transformers-based pre-trained model that considers only the response tweet. This approach completely ignores the context of the response tweet to train the model. The model uses a transformer encoder to generate the embedding representation for the response. The model is trained and evaluated on the given training and testing datasets. My best performance model gives an F1-score of 75.79%, beating the Classification Competition baseline score after four epoch iterations (epoch # 4). ""C. Dataset Description"" section of this document explains further the response and context relationship. Important files in the project: Documented software code Training and evaluation (link) Evaluation of a trained-model for demo purpose (link) Best performance testing set predictions (answer.txt) (link) Best performance trained-model (link) Training dataset (link) Testing dataset (link) Bidirectional Encoder Representations from Transformers (BERT) This project uses BERT, a transformer-based technique for Natural Language Processing pre-training developed by a team in Google. The original English language BERT model comes with two pre-trained model types: Model Type Layer Hidden Head Parameter Corpus Word Base 12 768 12 110 M 800 M Large 24 1024 16 340 M 2.500 M Table 1: BERT original model types BERT Large model essentially has better computing leverage than the base model. Google team trained the large model using a larger corpus word size than the base model. The large model is expected to perform better than the base model in most of the NLP tasks such as sentiment analysis. Original BERT paper is available here (link). Dataset Description There are two Twitter tweet datasets available for this project: Training dataset: a labeled dataset to train the model Testing dataset: tweet with a unique ID to evaluate the trained-model For the training dataset, each line contains a JSON object with the following columns: label: SARCASM or NOT_SARCASM response: the classified tweet context: the conversation context of the response For the testing/evaluation dataset, each line contains a JSON object with the following columns: id: unique identifier for the sample response: the tweet to be classified context: the conversation context of the response Training Dataset Testing Dataset 5000 lines 1800 lines Table 2: Dataset size statistics A more detailed dataset description is available in the project competition Github repository (link). The Naive Approach I hypothesize the context does not always support the sentiment of a response. Context can have an opposing effect on the sentiment of a response. I hypothesize there are 2 types of context: A Positive context is a context that supports the sentiment of a response. A Negative context is a context that does not support the sentiment of a response. Sentiment Negative Context Sentiment Negative Context Fig.1: Illustration of context reduces sentiment quality Positive Context S e n t i m e n t Positive Context S e n t i m e n t Fig.2: Illustration of context increase sentiment quality It is critical to utilize the context to support the response's sentiment. For this project, I consider only the sentiment-labeled response to training the model, and I completely ignore the context. I call this a Naive approach. In the future project, I can use advanced machine learning techniques to utilize response and context to train the model by selectively reconstruct the context to support the sentiment of a response. The Model, Training, and Evaluation In this project, I use the datasets to train and evaluate BERT Large uncased and base uncased models. I use the original BERT paper as a reference (A.3 Fine-tuning Procedure) to choose hyperparameters for my experiments. The hyperparameters in my experiments are: Learning rate: 2e-5 Batch size: 5 (considering memory size) Epochs: 4 iterations Epsilon: 1e-8 Random seed value: 17 BERT model can handle a text with a maximum of 512 characters. If the input text is more than 512 characters, the model truncates the text to 512 characters. Response text in training and testing datasets is less than 512 characters, in this case, we are guaranteed to consider all words in the response text to train and evaluate the model. The Python source code below prints the response maximum characters for training and testing datasets. Fig.3: Source code to check maximum training and evaluation response characters length Response Max. Chars Training Dataset 315 Testing Dataset 310 Table 3: Maximum response characters length The Software Code For the software code of this project, I implemented two Google Colab Notebooks: NAIVE_BERT_sentiment_analysis.ipynb: Training and evaluation notebook (link) DEMO_Model_Evaluation.ipynb: Evaluation of selected trained-model notebook (for DEMO purpose) (link) I use the Google Colab PRO environment to implement and test the software code. Introduction about Google Colab is available here link. Software code uses the following main Python libraries: Numpy 1.18.5 Pytorch 1.7.0+cu101 Huggingface Transformers 3.5.0 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review This notebook trains and evaluates the BERT Large uncased and base uncased models using the provided datasets. In the end, the trained-model and evaluation results are copied and stored in the project Google Drive folder. Colab Configuration Install Python modules required for the notebook. Copy train.jsonl and test.jsonl files from Google Drive to Colab session I have already copied train.jsonl and test.jsonl files to a Google Drive account created for this project. The files are shared with the public. The following code will copy the files from Google Drive to the Colab session. The source code above will prompt a URL. Click the URL, it will prompt the Google account login page. Select the Google account to run the notebook: Click the ""Allow"" button to allow Google Cloud SDK to access the Google account. That finally prompts the verification code. Copy the code and paste it in the ""Enter verification code"" text box. Press ""Enter"" That will copy the training and testing datasets to the Colab session! We are ready to train and evaluate the model using the datasets. Mounting Google Drive to Colab session (To save result files) To save the result files (the trained-model and answer.txt) to Google Drive, we need to mount Google Drive to the Colab session. In this project, I mount the Google Drive directory to the './content/uiuc' folder. At the end of this notebook code execution, the Google Drive mounted folder in the Colab session will look like the following: Mounted Google Drive in Colab Session where to keep result files permanently The Main Python Class This project implements a Python class, BERT_Model, that handles the following tasks: Read the dataset from JSONL files into a list of JSON Convert list of JSON to Pandas DataFrame Create the BERT Model Run the training and save the model for each epoch Evaluate the model and store the result into a file Below are the class signatures: For details please check the source code here link. The source code comment describes what each step does. The main algorithm of the training: Create a BERT tokenizer. We use the tokenizer to encode the text and prepare the dataset. Create a BERT model using the chosen hyperparameter. Create AdamW optimizer (link), it is used to calculate the convergence in the model. It is considered the fastest convergence algorithm. Create a data loader. The data loader main task is passing dataset batch to model. It will make sure all the text in the dataset is processed by the model. Set the model to train mode. Iterate the data loader to pass the dataset batch to the model until the training is completed. After the training is completed, save the trained model to a file. The main algorithm of testing: Load the model file to the memory and set the model to evaluation mode. Create the tokenizer, encode the input data for testing, and prepare the dataset. Create the data loader to handle the evaluation batch. Run the evaluation. One of the most important outputs from the model is logits. In this project, logits is an array of two elements. The model is configured to have the first element of logits represents SARCASM, and the second element represents NOT_SARCASM. We use the NumPy argmax function to return the index of the maximum value in logits. If the maximum value is the first element, the function returns the index array of 0 (SARCASM) If the maximum value is the second element, the function returns the index array of 1 (NOT_SARCASM) The final step is to write the output to a file (answer.txt) Training and Evaluation experiments To test my Naive hypothesis, I run experiments with the same hyperparameters on two BERT models. Experiment-1: BERT base uncased Experiment-2: BERT LARGE uncased Both experiments code above will generate result files in the Colab session folder below: Save the result files to Google Drive Colab deletes result files when the session ends. We need to store the files permanently in other locations, in this project I use Google Drive. The code below will copy the result files to the project Google Drive folder (the Google Drive folder has been mounted in the earlier step). In this project, I save the epoch # 4 model file only. We use the files stored in the project Google Drive folder to run a project demo. The next section will illustrate how to use the trained-model file to evaluate the testing dataset. F.2. DEMO_Model_Evaluation.ipynb Code Review In the demo notebook, I demonstrate how to generate 'answer.txt' from the BERT Large uncased trained model stored in the project Google Drive folder. The trained-model is available to the public here link. With this model, we will reproduce the evaluation result which is available to the public here link. The demo video is available here link. The main algorithm in the notebook demo: The first 3-steps are already explained in detail in the previous section Colab configuration (Python modules import and installation) Copy the trained-model and testing dataset files from Google Drive to the Colab session. We need to authorize Colab to access the Google account described in the previous section. Prepare Panda DataFrame for the testing dataset. Preparing the encoded testing dataset and data loader The main steps: Create the BERT tokenizer to encode the testing dataset. Create the data loader to run the evaluation in batches. Preparing the Python objects for the evaluation Run the evaluation batch iteration The main steps: Set the device to GPU, if applicable. Load the trained model from file to memory. Set the model to evaluation mode. Data loader iteration to pass text to evaluate in batches to the model. The model output logits (how to process logits is described in the previous section) Generate the 'anwer.txt' file Generate the sentiment output to the screen and 'answer.txt' file. The previous section describes the source code snippet. Post 'answer.txt' to LiveDataLab for scoring For F1, precision, and recall scores evaluation, I post the 'answer.txt' to LiveDataLab. Leaderboard snapshot on 03-Nov-2020 Result and Conclusion In summary, the testing (evaluation) results from BERT Large and base models: Model F1-Score Recall Precision BERT Large uncased 0.757905138339921 0.8522222222222222 0.6823843416370107 BERT Base uncased 0.7458777885548012 0.8544444444444445 0.6617900172117039 Surprisingly, the base model performs almost as good as the large model. In this project, I did try to use different trained model such as RoBERTa and XLNet (and different hyperparameters), but I could not produce a result higher than BERT Large uncased score. The project scope as proposed in the project proposal is to explore BERT hence, I am reporting the result for BERT models only. In the future, I would like to explore more on the following topics: To use advanced machine learning techniques to explore other hyperparameters for BERT models. To utilize both the context and response to training the models. I hypothesize that the context can be used as an additional dataset to train the model. To explore another model, such as RoBERTa and XLNet. Reference Aditya Joshi, Pushpak Bhattacharyya, and Mark J. Car- man. 2017. Automatic Sarcasm Detection: A Survey. ACM Computing Surveys, 50(5):1-22. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Appendix answer.txt twitter_1,NOT_SARCASM twitter_2,SARCASM twitter_3,SARCASM twitter_4,NOT_SARCASM twitter_5,SARCASM twitter_6,SARCASM twitter_7,NOT_SARCASM twitter_8,SARCASM twitter_9,NOT_SARCASM twitter_10,SARCASM twitter_11,NOT_SARCASM twitter_12,SARCASM twitter_13,SARCASM twitter_14,NOT_SARCASM twitter_15,SARCASM twitter_16,SARCASM twitter_17,SARCASM twitter_18,SARCASM twitter_19,SARCASM twitter_20,NOT_SARCASM twitter_21,NOT_SARCASM twitter_22,SARCASM twitter_23,NOT_SARCASM twitter_24,SARCASM twitter_25,SARCASM twitter_26,SARCASM twitter_27,NOT_SARCASM twitter_28,NOT_SARCASM twitter_29,SARCASM twitter_30,NOT_SARCASM twitter_31,SARCASM twitter_32,NOT_SARCASM twitter_33,NOT_SARCASM twitter_34,SARCASM twitter_35,NOT_SARCASM twitter_36,SARCASM twitter_37,SARCASM twitter_38,SARCASM twitter_39,SARCASM twitter_40,SARCASM twitter_41,SARCASM twitter_42,NOT_SARCASM twitter_43,SARCASM twitter_44,NOT_SARCASM twitter_45,SARCASM twitter_46,NOT_SARCASM twitter_47,SARCASM twitter_48,SARCASM twitter_49,NOT_SARCASM twitter_50,SARCASM twitter_51,NOT_SARCASM twitter_52,NOT_SARCASM twitter_53,SARCASM twitter_54,SARCASM twitter_55,SARCASM twitter_56,SARCASM twitter_57,NOT_SARCASM twitter_58,NOT_SARCASM twitter_59,SARCASM twitter_60,SARCASM twitter_61,NOT_SARCASM twitter_62,SARCASM twitter_63,SARCASM twitter_64,SARCASM twitter_65,SARCASM twitter_66,NOT_SARCASM twitter_67,SARCASM twitter_68,NOT_SARCASM twitter_69,SARCASM twitter_70,SARCASM twitter_71,NOT_SARCASM twitter_72,SARCASM twitter_73,SARCASM twitter_74,SARCASM twitter_75,NOT_SARCASM twitter_76,NOT_SARCASM twitter_77,SARCASM twitter_78,SARCASM twitter_79,NOT_SARCASM twitter_80,SARCASM twitter_81,NOT_SARCASM twitter_82,NOT_SARCASM twitter_83,SARCASM twitter_84,NOT_SARCASM twitter_85,SARCASM twitter_86,SARCASM twitter_87,NOT_SARCASM twitter_88,SARCASM twitter_89,SARCASM twitter_90,NOT_SARCASM twitter_91,NOT_SARCASM twitter_92,SARCASM twitter_93,SARCASM twitter_94,SARCASM twitter_95,SARCASM twitter_96,SARCASM twitter_97,SARCASM twitter_98,NOT_SARCASM twitter_99,NOT_SARCASM twitter_100,NOT_SARCASM twitter_101,SARCASM twitter_102,SARCASM twitter_103,NOT_SARCASM twitter_104,NOT_SARCASM twitter_105,SARCASM twitter_106,SARCASM twitter_107,NOT_SARCASM twitter_108,NOT_SARCASM twitter_109,SARCASM twitter_110,SARCASM twitter_111,SARCASM twitter_112,SARCASM twitter_113,SARCASM twitter_114,SARCASM twitter_115,SARCASM twitter_116,NOT_SARCASM twitter_117,NOT_SARCASM twitter_118,SARCASM twitter_119,NOT_SARCASM twitter_120,NOT_SARCASM twitter_121,SARCASM twitter_122,SARCASM twitter_123,NOT_SARCASM twitter_124,SARCASM twitter_125,NOT_SARCASM twitter_126,NOT_SARCASM twitter_127,SARCASM twitter_128,NOT_SARCASM twitter_129,NOT_SARCASM twitter_130,SARCASM twitter_131,NOT_SARCASM twitter_132,SARCASM twitter_133,NOT_SARCASM twitter_134,NOT_SARCASM twitter_135,SARCASM twitter_136,NOT_SARCASM twitter_137,SARCASM twitter_138,NOT_SARCASM twitter_139,NOT_SARCASM twitter_140,SARCASM twitter_141,SARCASM twitter_142,SARCASM twitter_143,SARCASM twitter_144,SARCASM twitter_145,SARCASM twitter_146,SARCASM twitter_147,SARCASM twitter_148,SARCASM twitter_149,SARCASM twitter_150,SARCASM twitter_151,SARCASM twitter_152,NOT_SARCASM twitter_153,NOT_SARCASM twitter_154,SARCASM twitter_155,SARCASM twitter_156,NOT_SARCASM twitter_157,SARCASM twitter_158,SARCASM twitter_159,SARCASM twitter_160,SARCASM twitter_161,NOT_SARCASM twitter_162,SARCASM twitter_163,SARCASM twitter_164,SARCASM twitter_165,SARCASM twitter_166,SARCASM twitter_167,SARCASM twitter_168,SARCASM twitter_169,NOT_SARCASM twitter_170,NOT_SARCASM twitter_171,NOT_SARCASM twitter_172,NOT_SARCASM twitter_173,SARCASM twitter_174,SARCASM twitter_175,SARCASM twitter_176,SARCASM twitter_177,SARCASM twitter_178,NOT_SARCASM twitter_179,SARCASM twitter_180,SARCASM twitter_181,SARCASM twitter_182,NOT_SARCASM twitter_183,SARCASM twitter_184,SARCASM twitter_185,NOT_SARCASM twitter_186,SARCASM twitter_187,SARCASM twitter_188,SARCASM twitter_189,NOT_SARCASM twitter_190,NOT_SARCASM twitter_191,NOT_SARCASM twitter_192,NOT_SARCASM twitter_193,SARCASM twitter_194,NOT_SARCASM twitter_195,NOT_SARCASM twitter_196,SARCASM twitter_197,NOT_SARCASM twitter_198,SARCASM twitter_199,SARCASM twitter_200,SARCASM twitter_201,SARCASM twitter_202,SARCASM twitter_203,NOT_SARCASM twitter_204,NOT_SARCASM twitter_205,SARCASM twitter_206,NOT_SARCASM twitter_207,SARCASM twitter_208,SARCASM twitter_209,SARCASM twitter_210,SARCASM twitter_211,SARCASM twitter_212,NOT_SARCASM twitter_213,NOT_SARCASM twitter_214,NOT_SARCASM twitter_215,NOT_SARCASM twitter_216,SARCASM twitter_217,SARCASM twitter_218,SARCASM twitter_219,SARCASM twitter_220,SARCASM twitter_221,SARCASM twitter_222,NOT_SARCASM twitter_223,NOT_SARCASM twitter_224,SARCASM twitter_225,NOT_SARCASM twitter_226,SARCASM twitter_227,NOT_SARCASM twitter_228,NOT_SARCASM twitter_229,SARCASM twitter_230,NOT_SARCASM twitter_231,SARCASM twitter_232,SARCASM twitter_233,SARCASM twitter_234,NOT_SARCASM twitter_235,SARCASM twitter_236,NOT_SARCASM twitter_237,SARCASM twitter_238,NOT_SARCASM twitter_239,SARCASM twitter_240,SARCASM twitter_241,SARCASM twitter_242,SARCASM twitter_243,NOT_SARCASM twitter_244,NOT_SARCASM twitter_245,NOT_SARCASM twitter_246,NOT_SARCASM twitter_247,SARCASM twitter_248,SARCASM twitter_249,SARCASM twitter_250,SARCASM twitter_251,SARCASM twitter_252,SARCASM twitter_253,NOT_SARCASM twitter_254,NOT_SARCASM twitter_255,SARCASM twitter_256,NOT_SARCASM twitter_257,NOT_SARCASM twitter_258,SARCASM twitter_259,SARCASM twitter_260,SARCASM twitter_261,NOT_SARCASM twitter_262,SARCASM twitter_263,NOT_SARCASM twitter_264,SARCASM twitter_265,SARCASM twitter_266,SARCASM twitter_267,SARCASM twitter_268,NOT_SARCASM twitter_269,SARCASM twitter_270,NOT_SARCASM twitter_271,NOT_SARCASM twitter_272,NOT_SARCASM twitter_273,NOT_SARCASM twitter_274,SARCASM twitter_275,SARCASM twitter_276,NOT_SARCASM twitter_277,NOT_SARCASM twitter_278,SARCASM twitter_279,SARCASM twitter_280,NOT_SARCASM twitter_281,SARCASM twitter_282,NOT_SARCASM twitter_283,SARCASM twitter_284,SARCASM twitter_285,NOT_SARCASM twitter_286,NOT_SARCASM twitter_287,NOT_SARCASM twitter_288,NOT_SARCASM twitter_289,NOT_SARCASM twitter_290,SARCASM twitter_291,SARCASM twitter_292,SARCASM twitter_293,SARCASM twitter_294,SARCASM twitter_295,NOT_SARCASM twitter_296,SARCASM twitter_297,SARCASM twitter_298,SARCASM twitter_299,SARCASM twitter_300,NOT_SARCASM twitter_301,SARCASM twitter_302,NOT_SARCASM twitter_303,SARCASM twitter_304,SARCASM twitter_305,NOT_SARCASM twitter_306,SARCASM twitter_307,NOT_SARCASM twitter_308,SARCASM twitter_309,SARCASM twitter_310,NOT_SARCASM twitter_311,SARCASM twitter_312,NOT_SARCASM twitter_313,SARCASM twitter_314,NOT_SARCASM twitter_315,SARCASM twitter_316,SARCASM twitter_317,NOT_SARCASM twitter_318,NOT_SARCASM twitter_319,SARCASM twitter_320,SARCASM twitter_321,NOT_SARCASM twitter_322,NOT_SARCASM twitter_323,NOT_SARCASM twitter_324,SARCASM twitter_325,NOT_SARCASM twitter_326,NOT_SARCASM twitter_327,SARCASM twitter_328,NOT_SARCASM twitter_329,SARCASM twitter_330,SARCASM twitter_331,SARCASM twitter_332,SARCASM twitter_333,SARCASM twitter_334,NOT_SARCASM twitter_335,NOT_SARCASM twitter_336,SARCASM twitter_337,NOT_SARCASM twitter_338,SARCASM twitter_339,NOT_SARCASM twitter_340,SARCASM twitter_341,NOT_SARCASM twitter_342,SARCASM twitter_343,SARCASM twitter_344,NOT_SARCASM twitter_345,SARCASM twitter_346,NOT_SARCASM twitter_347,SARCASM twitter_348,SARCASM twitter_349,SARCASM twitter_350,SARCASM twitter_351,SARCASM twitter_352,SARCASM twitter_353,NOT_SARCASM twitter_354,SARCASM twitter_355,SARCASM twitter_356,SARCASM twitter_357,SARCASM twitter_358,NOT_SARCASM twitter_359,SARCASM twitter_360,NOT_SARCASM twitter_361,SARCASM twitter_362,SARCASM twitter_363,SARCASM twitter_364,SARCASM twitter_365,NOT_SARCASM twitter_366,SARCASM twitter_367,SARCASM twitter_368,SARCASM twitter_369,NOT_SARCASM twitter_370,NOT_SARCASM twitter_371,SARCASM twitter_372,SARCASM twitter_373,SARCASM twitter_374,NOT_SARCASM twitter_375,SARCASM twitter_376,NOT_SARCASM twitter_377,SARCASM twitter_378,NOT_SARCASM twitter_379,NOT_SARCASM twitter_380,SARCASM twitter_381,SARCASM twitter_382,NOT_SARCASM twitter_383,SARCASM twitter_384,NOT_SARCASM twitter_385,NOT_SARCASM twitter_386,SARCASM twitter_387,SARCASM twitter_388,SARCASM twitter_389,NOT_SARCASM twitter_390,NOT_SARCASM twitter_391,SARCASM twitter_392,SARCASM twitter_393,NOT_SARCASM twitter_394,SARCASM twitter_395,SARCASM twitter_396,SARCASM twitter_397,NOT_SARCASM twitter_398,NOT_SARCASM twitter_399,SARCASM twitter_400,SARCASM twitter_401,SARCASM twitter_402,NOT_SARCASM twitter_403,SARCASM twitter_404,NOT_SARCASM twitter_405,NOT_SARCASM twitter_406,SARCASM twitter_407,SARCASM twitter_408,SARCASM twitter_409,SARCASM twitter_410,SARCASM twitter_411,SARCASM twitter_412,NOT_SARCASM twitter_413,SARCASM twitter_414,SARCASM twitter_415,SARCASM twitter_416,SARCASM twitter_417,SARCASM twitter_418,SARCASM twitter_419,NOT_SARCASM twitter_420,NOT_SARCASM twitter_421,SARCASM twitter_422,NOT_SARCASM twitter_423,SARCASM twitter_424,SARCASM twitter_425,SARCASM twitter_426,NOT_SARCASM twitter_427,SARCASM twitter_428,SARCASM twitter_429,NOT_SARCASM twitter_430,SARCASM twitter_431,SARCASM twitter_432,SARCASM twitter_433,SARCASM twitter_434,SARCASM twitter_435,NOT_SARCASM twitter_436,SARCASM twitter_437,SARCASM twitter_438,NOT_SARCASM twitter_439,NOT_SARCASM twitter_440,SARCASM twitter_441,SARCASM twitter_442,SARCASM twitter_443,SARCASM twitter_444,SARCASM twitter_445,SARCASM twitter_446,SARCASM twitter_447,SARCASM twitter_448,SARCASM twitter_449,SARCASM twitter_450,NOT_SARCASM twitter_451,NOT_SARCASM twitter_452,NOT_SARCASM twitter_453,SARCASM twitter_454,SARCASM twitter_455,NOT_SARCASM twitter_456,NOT_SARCASM twitter_457,SARCASM twitter_458,SARCASM twitter_459,SARCASM twitter_460,SARCASM twitter_461,SARCASM twitter_462,SARCASM twitter_463,SARCASM twitter_464,SARCASM twitter_465,NOT_SARCASM twitter_466,SARCASM twitter_467,NOT_SARCASM twitter_468,SARCASM twitter_469,SARCASM twitter_470,SARCASM twitter_471,SARCASM twitter_472,SARCASM twitter_473,NOT_SARCASM twitter_474,SARCASM twitter_475,NOT_SARCASM twitter_476,SARCASM twitter_477,NOT_SARCASM twitter_478,SARCASM twitter_479,SARCASM twitter_480,SARCASM twitter_481,NOT_SARCASM twitter_482,NOT_SARCASM twitter_483,NOT_SARCASM twitter_484,SARCASM twitter_485,NOT_SARCASM twitter_486,SARCASM twitter_487,NOT_SARCASM twitter_488,SARCASM twitter_489,SARCASM twitter_490,NOT_SARCASM twitter_491,NOT_SARCASM twitter_492,NOT_SARCASM twitter_493,SARCASM twitter_494,SARCASM twitter_495,SARCASM twitter_496,SARCASM twitter_497,NOT_SARCASM twitter_498,SARCASM twitter_499,SARCASM twitter_500,SARCASM twitter_501,SARCASM twitter_502,SARCASM twitter_503,SARCASM twitter_504,SARCASM twitter_505,SARCASM twitter_506,SARCASM twitter_507,SARCASM twitter_508,NOT_SARCASM twitter_509,NOT_SARCASM twitter_510,SARCASM twitter_511,NOT_SARCASM twitter_512,NOT_SARCASM twitter_513,SARCASM twitter_514,SARCASM twitter_515,NOT_SARCASM twitter_516,NOT_SARCASM twitter_517,SARCASM twitter_518,SARCASM twitter_519,NOT_SARCASM twitter_520,SARCASM twitter_521,SARCASM twitter_522,SARCASM twitter_523,NOT_SARCASM twitter_524,SARCASM twitter_525,SARCASM twitter_526,SARCASM twitter_527,SARCASM twitter_528,SARCASM twitter_529,NOT_SARCASM twitter_530,NOT_SARCASM twitter_531,NOT_SARCASM twitter_532,SARCASM twitter_533,NOT_SARCASM twitter_534,SARCASM twitter_535,SARCASM twitter_536,SARCASM twitter_537,NOT_SARCASM twitter_538,NOT_SARCASM twitter_539,SARCASM twitter_540,SARCASM twitter_541,SARCASM twitter_542,SARCASM twitter_543,NOT_SARCASM twitter_544,NOT_SARCASM twitter_545,SARCASM twitter_546,SARCASM twitter_547,SARCASM twitter_548,NOT_SARCASM twitter_549,NOT_SARCASM twitter_550,SARCASM twitter_551,SARCASM twitter_552,NOT_SARCASM twitter_553,SARCASM twitter_554,SARCASM twitter_555,NOT_SARCASM twitter_556,SARCASM twitter_557,SARCASM twitter_558,SARCASM twitter_559,SARCASM twitter_560,SARCASM twitter_561,NOT_SARCASM twitter_562,SARCASM twitter_563,SARCASM twitter_564,SARCASM twitter_565,SARCASM twitter_566,SARCASM twitter_567,SARCASM twitter_568,SARCASM twitter_569,NOT_SARCASM twitter_570,SARCASM twitter_571,SARCASM twitter_572,SARCASM twitter_573,SARCASM twitter_574,SARCASM twitter_575,NOT_SARCASM twitter_576,SARCASM twitter_577,SARCASM twitter_578,NOT_SARCASM twitter_579,NOT_SARCASM twitter_580,NOT_SARCASM twitter_581,NOT_SARCASM twitter_582,NOT_SARCASM twitter_583,SARCASM twitter_584,SARCASM twitter_585,SARCASM twitter_586,SARCASM twitter_587,SARCASM twitter_588,SARCASM twitter_589,NOT_SARCASM twitter_590,NOT_SARCASM twitter_591,SARCASM twitter_592,SARCASM twitter_593,NOT_SARCASM twitter_594,SARCASM twitter_595,SARCASM twitter_596,NOT_SARCASM twitter_597,NOT_SARCASM twitter_598,SARCASM twitter_599,NOT_SARCASM twitter_600,SARCASM twitter_601,NOT_SARCASM twitter_602,SARCASM twitter_603,NOT_SARCASM twitter_604,SARCASM twitter_605,NOT_SARCASM twitter_606,NOT_SARCASM twitter_607,SARCASM twitter_608,SARCASM twitter_609,NOT_SARCASM twitter_610,NOT_SARCASM twitter_611,NOT_SARCASM twitter_612,SARCASM twitter_613,SARCASM twitter_614,NOT_SARCASM twitter_615,NOT_SARCASM twitter_616,SARCASM twitter_617,SARCASM twitter_618,NOT_SARCASM twitter_619,SARCASM twitter_620,NOT_SARCASM twitter_621,SARCASM twitter_622,NOT_SARCASM twitter_623,SARCASM twitter_624,NOT_SARCASM twitter_625,SARCASM twitter_626,SARCASM twitter_627,NOT_SARCASM twitter_628,NOT_SARCASM twitter_629,SARCASM twitter_630,SARCASM twitter_631,NOT_SARCASM twitter_632,SARCASM twitter_633,NOT_SARCASM twitter_634,SARCASM twitter_635,SARCASM twitter_636,NOT_SARCASM twitter_637,SARCASM twitter_638,NOT_SARCASM twitter_639,SARCASM twitter_640,NOT_SARCASM twitter_641,NOT_SARCASM twitter_642,SARCASM twitter_643,SARCASM twitter_644,SARCASM twitter_645,NOT_SARCASM twitter_646,NOT_SARCASM twitter_647,NOT_SARCASM twitter_648,NOT_SARCASM twitter_649,SARCASM twitter_650,NOT_SARCASM twitter_651,SARCASM twitter_652,NOT_SARCASM twitter_653,NOT_SARCASM twitter_654,SARCASM twitter_655,NOT_SARCASM twitter_656,NOT_SARCASM twitter_657,SARCASM twitter_658,NOT_SARCASM twitter_659,SARCASM twitter_660,SARCASM twitter_661,SARCASM twitter_662,SARCASM twitter_663,NOT_SARCASM twitter_664,SARCASM twitter_665,NOT_SARCASM twitter_666,NOT_SARCASM twitter_667,SARCASM twitter_668,SARCASM twitter_669,SARCASM twitter_670,NOT_SARCASM twitter_671,SARCASM twitter_672,SARCASM twitter_673,SARCASM twitter_674,SARCASM twitter_675,SARCASM twitter_676,NOT_SARCASM twitter_677,NOT_SARCASM twitter_678,SARCASM twitter_679,SARCASM twitter_680,NOT_SARCASM twitter_681,NOT_SARCASM twitter_682,SARCASM twitter_683,NOT_SARCASM twitter_684,NOT_SARCASM twitter_685,SARCASM twitter_686,SARCASM twitter_687,NOT_SARCASM twitter_688,SARCASM twitter_689,SARCASM twitter_690,SARCASM twitter_691,NOT_SARCASM twitter_692,SARCASM twitter_693,SARCASM twitter_694,NOT_SARCASM twitter_695,SARCASM twitter_696,NOT_SARCASM twitter_697,SARCASM twitter_698,NOT_SARCASM twitter_699,NOT_SARCASM twitter_700,NOT_SARCASM twitter_701,SARCASM twitter_702,NOT_SARCASM twitter_703,SARCASM twitter_704,SARCASM twitter_705,NOT_SARCASM twitter_706,SARCASM twitter_707,NOT_SARCASM twitter_708,SARCASM twitter_709,NOT_SARCASM twitter_710,SARCASM twitter_711,SARCASM twitter_712,SARCASM twitter_713,SARCASM twitter_714,SARCASM twitter_715,SARCASM twitter_716,SARCASM twitter_717,SARCASM twitter_718,SARCASM twitter_719,NOT_SARCASM twitter_720,NOT_SARCASM twitter_721,SARCASM twitter_722,NOT_SARCASM twitter_723,SARCASM twitter_724,SARCASM twitter_725,SARCASM twitter_726,NOT_SARCASM twitter_727,NOT_SARCASM twitter_728,SARCASM twitter_729,SARCASM twitter_730,SARCASM twitter_731,NOT_SARCASM twitter_732,SARCASM twitter_733,NOT_SARCASM twitter_734,NOT_SARCASM twitter_735,SARCASM twitter_736,SARCASM twitter_737,SARCASM twitter_738,SARCASM twitter_739,SARCASM twitter_740,SARCASM twitter_741,SARCASM twitter_742,SARCASM twitter_743,SARCASM twitter_744,NOT_SARCASM twitter_745,NOT_SARCASM twitter_746,SARCASM twitter_747,SARCASM twitter_748,NOT_SARCASM twitter_749,NOT_SARCASM twitter_750,NOT_SARCASM twitter_751,SARCASM twitter_752,NOT_SARCASM twitter_753,SARCASM twitter_754,SARCASM twitter_755,NOT_SARCASM twitter_756,SARCASM twitter_757,NOT_SARCASM twitter_758,NOT_SARCASM twitter_759,SARCASM twitter_760,SARCASM twitter_761,NOT_SARCASM twitter_762,SARCASM twitter_763,NOT_SARCASM twitter_764,SARCASM twitter_765,SARCASM twitter_766,SARCASM twitter_767,SARCASM twitter_768,NOT_SARCASM twitter_769,NOT_SARCASM twitter_770,NOT_SARCASM twitter_771,SARCASM twitter_772,NOT_SARCASM twitter_773,SARCASM twitter_774,NOT_SARCASM twitter_775,SARCASM twitter_776,SARCASM twitter_777,NOT_SARCASM twitter_778,SARCASM twitter_779,NOT_SARCASM twitter_780,SARCASM twitter_781,NOT_SARCASM twitter_782,NOT_SARCASM twitter_783,SARCASM twitter_784,SARCASM twitter_785,NOT_SARCASM twitter_786,SARCASM twitter_787,SARCASM twitter_788,NOT_SARCASM twitter_789,SARCASM twitter_790,SARCASM twitter_791,NOT_SARCASM twitter_792,NOT_SARCASM twitter_793,SARCASM twitter_794,NOT_SARCASM twitter_795,SARCASM twitter_796,SARCASM twitter_797,SARCASM twitter_798,SARCASM twitter_799,SARCASM twitter_800,NOT_SARCASM twitter_801,NOT_SARCASM twitter_802,NOT_SARCASM twitter_803,SARCASM twitter_804,NOT_SARCASM twitter_805,SARCASM twitter_806,SARCASM twitter_807,NOT_SARCASM twitter_808,SARCASM twitter_809,SARCASM twitter_810,NOT_SARCASM twitter_811,SARCASM twitter_812,SARCASM twitter_813,SARCASM twitter_814,SARCASM twitter_815,SARCASM twitter_816,SARCASM twitter_817,SARCASM twitter_818,NOT_SARCASM twitter_819,SARCASM twitter_820,NOT_SARCASM twitter_821,NOT_SARCASM twitter_822,NOT_SARCASM twitter_823,SARCASM twitter_824,NOT_SARCASM twitter_825,NOT_SARCASM twitter_826,SARCASM twitter_827,SARCASM twitter_828,NOT_SARCASM twitter_829,SARCASM twitter_830,SARCASM twitter_831,NOT_SARCASM twitter_832,NOT_SARCASM twitter_833,NOT_SARCASM twitter_834,NOT_SARCASM twitter_835,SARCASM twitter_836,SARCASM twitter_837,SARCASM twitter_838,SARCASM twitter_839,SARCASM twitter_840,SARCASM twitter_841,SARCASM twitter_842,SARCASM twitter_843,SARCASM twitter_844,SARCASM twitter_845,NOT_SARCASM twitter_846,NOT_SARCASM twitter_847,SARCASM twitter_848,NOT_SARCASM twitter_849,NOT_SARCASM twitter_850,SARCASM twitter_851,SARCASM twitter_852,NOT_SARCASM twitter_853,NOT_SARCASM twitter_854,NOT_SARCASM twitter_855,NOT_SARCASM twitter_856,SARCASM twitter_857,SARCASM twitter_858,SARCASM twitter_859,NOT_SARCASM twitter_860,NOT_SARCASM twitter_861,NOT_SARCASM twitter_862,NOT_SARCASM twitter_863,SARCASM twitter_864,NOT_SARCASM twitter_865,SARCASM twitter_866,SARCASM twitter_867,SARCASM twitter_868,SARCASM twitter_869,SARCASM twitter_870,SARCASM twitter_871,SARCASM twitter_872,NOT_SARCASM twitter_873,SARCASM twitter_874,NOT_SARCASM twitter_875,NOT_SARCASM twitter_876,NOT_SARCASM twitter_877,NOT_SARCASM twitter_878,SARCASM twitter_879,NOT_SARCASM twitter_880,SARCASM twitter_881,NOT_SARCASM twitter_882,NOT_SARCASM twitter_883,SARCASM twitter_884,SARCASM twitter_885,SARCASM twitter_886,SARCASM twitter_887,NOT_SARCASM twitter_888,SARCASM twitter_889,NOT_SARCASM twitter_890,SARCASM twitter_891,SARCASM twitter_892,NOT_SARCASM twitter_893,SARCASM twitter_894,NOT_SARCASM twitter_895,SARCASM twitter_896,NOT_SARCASM twitter_897,NOT_SARCASM twitter_898,SARCASM twitter_899,SARCASM twitter_900,NOT_SARCASM twitter_901,SARCASM twitter_902,SARCASM twitter_903,SARCASM twitter_904,SARCASM twitter_905,SARCASM twitter_906,SARCASM twitter_907,SARCASM twitter_908,SARCASM twitter_909,SARCASM twitter_910,NOT_SARCASM twitter_911,NOT_SARCASM twitter_912,NOT_SARCASM twitter_913,SARCASM twitter_914,SARCASM twitter_915,SARCASM twitter_916,SARCASM twitter_917,SARCASM twitter_918,NOT_SARCASM twitter_919,SARCASM twitter_920,NOT_SARCASM twitter_921,SARCASM twitter_922,SARCASM twitter_923,NOT_SARCASM twitter_924,SARCASM twitter_925,NOT_SARCASM twitter_926,NOT_SARCASM twitter_927,NOT_SARCASM twitter_928,NOT_SARCASM twitter_929,NOT_SARCASM twitter_930,NOT_SARCASM twitter_931,SARCASM twitter_932,SARCASM twitter_933,NOT_SARCASM twitter_934,NOT_SARCASM twitter_935,SARCASM twitter_936,SARCASM twitter_937,SARCASM twitter_938,SARCASM twitter_939,SARCASM twitter_940,SARCASM twitter_941,SARCASM twitter_942,NOT_SARCASM twitter_943,NOT_SARCASM twitter_944,SARCASM twitter_945,SARCASM twitter_946,SARCASM twitter_947,NOT_SARCASM twitter_948,SARCASM twitter_949,SARCASM twitter_950,SARCASM twitter_951,NOT_SARCASM twitter_952,SARCASM twitter_953,NOT_SARCASM twitter_954,SARCASM twitter_955,SARCASM twitter_956,SARCASM twitter_957,SARCASM twitter_958,SARCASM twitter_959,NOT_SARCASM twitter_960,SARCASM twitter_961,SARCASM twitter_962,SARCASM twitter_963,NOT_SARCASM twitter_964,NOT_SARCASM twitter_965,SARCASM twitter_966,SARCASM twitter_967,SARCASM twitter_968,SARCASM twitter_969,SARCASM twitter_970,SARCASM twitter_971,SARCASM twitter_972,NOT_SARCASM twitter_973,SARCASM twitter_974,NOT_SARCASM twitter_975,SARCASM twitter_976,NOT_SARCASM twitter_977,SARCASM twitter_978,SARCASM twitter_979,SARCASM twitter_980,SARCASM twitter_981,SARCASM twitter_982,NOT_SARCASM twitter_983,SARCASM twitter_984,SARCASM twitter_985,SARCASM twitter_986,NOT_SARCASM twitter_987,NOT_SARCASM twitter_988,SARCASM twitter_989,NOT_SARCASM twitter_990,NOT_SARCASM twitter_991,SARCASM twitter_992,SARCASM twitter_993,SARCASM twitter_994,SARCASM twitter_995,SARCASM twitter_996,SARCASM twitter_997,SARCASM twitter_998,SARCASM twitter_999,NOT_SARCASM twitter_1000,NOT_SARCASM twitter_1001,NOT_SARCASM twitter_1002,SARCASM twitter_1003,SARCASM twitter_1004,SARCASM twitter_1005,SARCASM twitter_1006,SARCASM twitter_1007,SARCASM twitter_1008,NOT_SARCASM twitter_1009,NOT_SARCASM twitter_1010,SARCASM twitter_1011,NOT_SARCASM twitter_1012,SARCASM twitter_1013,SARCASM twitter_1014,SARCASM twitter_1015,NOT_SARCASM twitter_1016,NOT_SARCASM twitter_1017,NOT_SARCASM twitter_1018,SARCASM twitter_1019,NOT_SARCASM twitter_1020,SARCASM twitter_1021,NOT_SARCASM twitter_1022,NOT_SARCASM twitter_1023,SARCASM twitter_1024,SARCASM twitter_1025,NOT_SARCASM twitter_1026,NOT_SARCASM twitter_1027,SARCASM twitter_1028,SARCASM twitter_1029,NOT_SARCASM twitter_1030,SARCASM twitter_1031,NOT_SARCASM twitter_1032,NOT_SARCASM twitter_1033,NOT_SARCASM twitter_1034,SARCASM twitter_1035,NOT_SARCASM twitter_1036,SARCASM twitter_1037,NOT_SARCASM twitter_1038,SARCASM twitter_1039,SARCASM twitter_1040,SARCASM twitter_1041,NOT_SARCASM twitter_1042,SARCASM twitter_1043,SARCASM twitter_1044,NOT_SARCASM twitter_1045,SARCASM twitter_1046,SARCASM twitter_1047,NOT_SARCASM twitter_1048,SARCASM twitter_1049,NOT_SARCASM twitter_1050,SARCASM twitter_1051,NOT_SARCASM twitter_1052,NOT_SARCASM twitter_1053,SARCASM twitter_1054,SARCASM twitter_1055,NOT_SARCASM twitter_1056,SARCASM twitter_1057,NOT_SARCASM twitter_1058,SARCASM twitter_1059,SARCASM twitter_1060,NOT_SARCASM twitter_1061,SARCASM twitter_1062,SARCASM twitter_1063,NOT_SARCASM twitter_1064,NOT_SARCASM twitter_1065,SARCASM twitter_1066,NOT_SARCASM twitter_1067,SARCASM twitter_1068,NOT_SARCASM twitter_1069,SARCASM twitter_1070,SARCASM twitter_1071,SARCASM twitter_1072,NOT_SARCASM twitter_1073,NOT_SARCASM twitter_1074,SARCASM twitter_1075,NOT_SARCASM twitter_1076,SARCASM twitter_1077,NOT_SARCASM twitter_1078,SARCASM twitter_1079,SARCASM twitter_1080,SARCASM twitter_1081,SARCASM twitter_1082,NOT_SARCASM twitter_1083,SARCASM twitter_1084,SARCASM twitter_1085,SARCASM twitter_1086,SARCASM twitter_1087,NOT_SARCASM twitter_1088,NOT_SARCASM twitter_1089,NOT_SARCASM twitter_1090,NOT_SARCASM twitter_1091,SARCASM twitter_1092,SARCASM twitter_1093,NOT_SARCASM twitter_1094,NOT_SARCASM twitter_1095,SARCASM twitter_1096,NOT_SARCASM twitter_1097,NOT_SARCASM twitter_1098,NOT_SARCASM twitter_1099,NOT_SARCASM twitter_1100,SARCASM twitter_1101,SARCASM twitter_1102,SARCASM twitter_1103,SARCASM twitter_1104,SARCASM twitter_1105,SARCASM twitter_1106,NOT_SARCASM twitter_1107,SARCASM twitter_1108,SARCASM twitter_1109,SARCASM twitter_1110,NOT_SARCASM twitter_1111,SARCASM twitter_1112,SARCASM twitter_1113,SARCASM twitter_1114,NOT_SARCASM twitter_1115,SARCASM twitter_1116,SARCASM twitter_1117,NOT_SARCASM twitter_1118,NOT_SARCASM twitter_1119,NOT_SARCASM twitter_1120,SARCASM twitter_1121,SARCASM twitter_1122,NOT_SARCASM twitter_1123,SARCASM twitter_1124,SARCASM twitter_1125,SARCASM twitter_1126,NOT_SARCASM twitter_1127,NOT_SARCASM twitter_1128,NOT_SARCASM twitter_1129,NOT_SARCASM twitter_1130,SARCASM twitter_1131,SARCASM twitter_1132,NOT_SARCASM twitter_1133,SARCASM twitter_1134,NOT_SARCASM twitter_1135,NOT_SARCASM twitter_1136,SARCASM twitter_1137,SARCASM twitter_1138,NOT_SARCASM twitter_1139,SARCASM twitter_1140,NOT_SARCASM twitter_1141,SARCASM twitter_1142,SARCASM twitter_1143,SARCASM twitter_1144,SARCASM twitter_1145,SARCASM twitter_1146,NOT_SARCASM twitter_1147,SARCASM twitter_1148,NOT_SARCASM twitter_1149,SARCASM twitter_1150,NOT_SARCASM twitter_1151,NOT_SARCASM twitter_1152,SARCASM twitter_1153,NOT_SARCASM twitter_1154,NOT_SARCASM twitter_1155,SARCASM twitter_1156,SARCASM twitter_1157,SARCASM twitter_1158,SARCASM twitter_1159,SARCASM twitter_1160,NOT_SARCASM twitter_1161,NOT_SARCASM twitter_1162,SARCASM twitter_1163,NOT_SARCASM twitter_1164,SARCASM twitter_1165,NOT_SARCASM twitter_1166,SARCASM twitter_1167,SARCASM twitter_1168,NOT_SARCASM twitter_1169,SARCASM twitter_1170,SARCASM twitter_1171,SARCASM twitter_1172,SARCASM twitter_1173,SARCASM twitter_1174,SARCASM twitter_1175,SARCASM twitter_1176,NOT_SARCASM twitter_1177,NOT_SARCASM twitter_1178,SARCASM twitter_1179,NOT_SARCASM twitter_1180,SARCASM twitter_1181,NOT_SARCASM twitter_1182,NOT_SARCASM twitter_1183,SARCASM twitter_1184,SARCASM twitter_1185,NOT_SARCASM twitter_1186,SARCASM twitter_1187,SARCASM twitter_1188,SARCASM twitter_1189,SARCASM twitter_1190,SARCASM twitter_1191,SARCASM twitter_1192,SARCASM twitter_1193,NOT_SARCASM twitter_1194,NOT_SARCASM twitter_1195,NOT_SARCASM twitter_1196,NOT_SARCASM twitter_1197,NOT_SARCASM twitter_1198,SARCASM twitter_1199,SARCASM twitter_1200,SARCASM twitter_1201,NOT_SARCASM twitter_1202,SARCASM twitter_1203,NOT_SARCASM twitter_1204,NOT_SARCASM twitter_1205,NOT_SARCASM twitter_1206,NOT_SARCASM twitter_1207,SARCASM twitter_1208,SARCASM twitter_1209,NOT_SARCASM twitter_1210,SARCASM twitter_1211,SARCASM twitter_1212,SARCASM twitter_1213,NOT_SARCASM twitter_1214,SARCASM twitter_1215,SARCASM twitter_1216,NOT_SARCASM twitter_1217,NOT_SARCASM twitter_1218,NOT_SARCASM twitter_1219,SARCASM twitter_1220,NOT_SARCASM twitter_1221,SARCASM twitter_1222,SARCASM twitter_1223,NOT_SARCASM twitter_1224,NOT_SARCASM twitter_1225,SARCASM twitter_1226,SARCASM twitter_1227,SARCASM twitter_1228,SARCASM twitter_1229,SARCASM twitter_1230,SARCASM twitter_1231,SARCASM twitter_1232,SARCASM twitter_1233,SARCASM twitter_1234,SARCASM twitter_1235,SARCASM twitter_1236,SARCASM twitter_1237,NOT_SARCASM twitter_1238,SARCASM twitter_1239,NOT_SARCASM twitter_1240,SARCASM twitter_1241,NOT_SARCASM twitter_1242,SARCASM twitter_1243,SARCASM twitter_1244,NOT_SARCASM twitter_1245,NOT_SARCASM twitter_1246,NOT_SARCASM twitter_1247,SARCASM twitter_1248,SARCASM twitter_1249,NOT_SARCASM twitter_1250,NOT_SARCASM twitter_1251,SARCASM twitter_1252,SARCASM twitter_1253,NOT_SARCASM twitter_1254,SARCASM twitter_1255,SARCASM twitter_1256,NOT_SARCASM twitter_1257,SARCASM twitter_1258,NOT_SARCASM twitter_1259,NOT_SARCASM twitter_1260,SARCASM twitter_1261,SARCASM twitter_1262,SARCASM twitter_1263,SARCASM twitter_1264,SARCASM twitter_1265,NOT_SARCASM twitter_1266,NOT_SARCASM twitter_1267,SARCASM twitter_1268,SARCASM twitter_1269,NOT_SARCASM twitter_1270,SARCASM twitter_1271,SARCASM twitter_1272,SARCASM twitter_1273,NOT_SARCASM twitter_1274,SARCASM twitter_1275,SARCASM twitter_1276,NOT_SARCASM twitter_1277,NOT_SARCASM twitter_1278,NOT_SARCASM twitter_1279,NOT_SARCASM twitter_1280,SARCASM twitter_1281,NOT_SARCASM twitter_1282,NOT_SARCASM twitter_1283,SARCASM twitter_1284,NOT_SARCASM twitter_1285,SARCASM twitter_1286,NOT_SARCASM twitter_1287,NOT_SARCASM twitter_1288,SARCASM twitter_1289,SARCASM twitter_1290,SARCASM twitter_1291,SARCASM twitter_1292,SARCASM twitter_1293,SARCASM twitter_1294,NOT_SARCASM twitter_1295,SARCASM twitter_1296,SARCASM twitter_1297,SARCASM twitter_1298,SARCASM twitter_1299,SARCASM twitter_1300,SARCASM twitter_1301,SARCASM twitter_1302,SARCASM twitter_1303,NOT_SARCASM twitter_1304,NOT_SARCASM twitter_1305,SARCASM twitter_1306,NOT_SARCASM twitter_1307,SARCASM twitter_1308,SARCASM twitter_1309,SARCASM twitter_1310,SARCASM twitter_1311,SARCASM twitter_1312,SARCASM twitter_1313,NOT_SARCASM twitter_1314,SARCASM twitter_1315,SARCASM twitter_1316,SARCASM twitter_1317,SARCASM twitter_1318,NOT_SARCASM twitter_1319,NOT_SARCASM twitter_1320,SARCASM twitter_1321,SARCASM twitter_1322,SARCASM twitter_1323,SARCASM twitter_1324,SARCASM twitter_1325,SARCASM twitter_1326,NOT_SARCASM twitter_1327,NOT_SARCASM twitter_1328,NOT_SARCASM twitter_1329,SARCASM twitter_1330,SARCASM twitter_1331,SARCASM twitter_1332,SARCASM twitter_1333,NOT_SARCASM twitter_1334,SARCASM twitter_1335,NOT_SARCASM twitter_1336,NOT_SARCASM twitter_1337,SARCASM twitter_1338,NOT_SARCASM twitter_1339,SARCASM twitter_1340,SARCASM twitter_1341,SARCASM twitter_1342,SARCASM twitter_1343,SARCASM twitter_1344,SARCASM twitter_1345,NOT_SARCASM twitter_1346,SARCASM twitter_1347,SARCASM twitter_1348,SARCASM twitter_1349,SARCASM twitter_1350,SARCASM twitter_1351,SARCASM twitter_1352,NOT_SARCASM twitter_1353,SARCASM twitter_1354,NOT_SARCASM twitter_1355,SARCASM twitter_1356,SARCASM twitter_1357,SARCASM twitter_1358,SARCASM twitter_1359,SARCASM twitter_1360,SARCASM twitter_1361,SARCASM twitter_1362,NOT_SARCASM twitter_1363,SARCASM twitter_1364,NOT_SARCASM twitter_1365,SARCASM twitter_1366,SARCASM twitter_1367,SARCASM twitter_1368,SARCASM twitter_1369,NOT_SARCASM twitter_1370,NOT_SARCASM twitter_1371,NOT_SARCASM twitter_1372,NOT_SARCASM twitter_1373,NOT_SARCASM twitter_1374,SARCASM twitter_1375,NOT_SARCASM twitter_1376,SARCASM twitter_1377,SARCASM twitter_1378,NOT_SARCASM twitter_1379,SARCASM twitter_1380,SARCASM twitter_1381,SARCASM twitter_1382,SARCASM twitter_1383,SARCASM twitter_1384,NOT_SARCASM twitter_1385,SARCASM twitter_1386,NOT_SARCASM twitter_1387,SARCASM twitter_1388,SARCASM twitter_1389,SARCASM twitter_1390,SARCASM twitter_1391,SARCASM twitter_1392,SARCASM twitter_1393,SARCASM twitter_1394,NOT_SARCASM twitter_1395,SARCASM twitter_1396,NOT_SARCASM twitter_1397,SARCASM twitter_1398,SARCASM twitter_1399,SARCASM twitter_1400,NOT_SARCASM twitter_1401,SARCASM twitter_1402,SARCASM twitter_1403,SARCASM twitter_1404,SARCASM twitter_1405,NOT_SARCASM twitter_1406,NOT_SARCASM twitter_1407,SARCASM twitter_1408,SARCASM twitter_1409,SARCASM twitter_1410,SARCASM twitter_1411,SARCASM twitter_1412,NOT_SARCASM twitter_1413,NOT_SARCASM twitter_1414,SARCASM twitter_1415,SARCASM twitter_1416,SARCASM twitter_1417,SARCASM twitter_1418,NOT_SARCASM twitter_1419,NOT_SARCASM twitter_1420,NOT_SARCASM twitter_1421,NOT_SARCASM twitter_1422,NOT_SARCASM twitter_1423,SARCASM twitter_1424,SARCASM twitter_1425,SARCASM twitter_1426,NOT_SARCASM twitter_1427,SARCASM twitter_1428,SARCASM twitter_1429,SARCASM twitter_1430,NOT_SARCASM twitter_1431,NOT_SARCASM twitter_1432,SARCASM twitter_1433,NOT_SARCASM twitter_1434,NOT_SARCASM twitter_1435,SARCASM twitter_1436,SARCASM twitter_1437,SARCASM twitter_1438,SARCASM twitter_1439,SARCASM twitter_1440,NOT_SARCASM twitter_1441,SARCASM twitter_1442,NOT_SARCASM twitter_1443,NOT_SARCASM twitter_1444,NOT_SARCASM twitter_1445,NOT_SARCASM twitter_1446,NOT_SARCASM twitter_1447,SARCASM twitter_1448,SARCASM twitter_1449,SARCASM twitter_1450,SARCASM twitter_1451,SARCASM twitter_1452,SARCASM twitter_1453,SARCASM twitter_1454,SARCASM twitter_1455,SARCASM twitter_1456,SARCASM twitter_1457,NOT_SARCASM twitter_1458,NOT_SARCASM twitter_1459,NOT_SARCASM twitter_1460,SARCASM twitter_1461,SARCASM twitter_1462,SARCASM twitter_1463,SARCASM twitter_1464,SARCASM twitter_1465,SARCASM twitter_1466,SARCASM twitter_1467,NOT_SARCASM twitter_1468,SARCASM twitter_1469,SARCASM twitter_1470,SARCASM twitter_1471,SARCASM twitter_1472,SARCASM twitter_1473,NOT_SARCASM twitter_1474,NOT_SARCASM twitter_1475,SARCASM twitter_1476,SARCASM twitter_1477,SARCASM twitter_1478,NOT_SARCASM twitter_1479,SARCASM twitter_1480,NOT_SARCASM twitter_1481,SARCASM twitter_1482,NOT_SARCASM twitter_1483,SARCASM twitter_1484,SARCASM twitter_1485,NOT_SARCASM twitter_1486,SARCASM twitter_1487,SARCASM twitter_1488,SARCASM twitter_1489,SARCASM twitter_1490,SARCASM twitter_1491,NOT_SARCASM twitter_1492,SARCASM twitter_1493,SARCASM twitter_1494,SARCASM twitter_1495,NOT_SARCASM twitter_1496,SARCASM twitter_1497,NOT_SARCASM twitter_1498,SARCASM twitter_1499,NOT_SARCASM twitter_1500,NOT_SARCASM twitter_1501,NOT_SARCASM twitter_1502,NOT_SARCASM twitter_1503,NOT_SARCASM twitter_1504,SARCASM twitter_1505,NOT_SARCASM twitter_1506,SARCASM twitter_1507,SARCASM twitter_1508,SARCASM twitter_1509,SARCASM twitter_1510,SARCASM twitter_1511,SARCASM twitter_1512,SARCASM twitter_1513,SARCASM twitter_1514,SARCASM twitter_1515,NOT_SARCASM twitter_1516,SARCASM twitter_1517,NOT_SARCASM twitter_1518,SARCASM twitter_1519,NOT_SARCASM twitter_1520,SARCASM twitter_1521,SARCASM twitter_1522,SARCASM twitter_1523,SARCASM twitter_1524,NOT_SARCASM twitter_1525,SARCASM twitter_1526,NOT_SARCASM twitter_1527,SARCASM twitter_1528,SARCASM twitter_1529,SARCASM twitter_1530,SARCASM twitter_1531,NOT_SARCASM twitter_1532,SARCASM twitter_1533,SARCASM twitter_1534,SARCASM twitter_1535,SARCASM twitter_1536,NOT_SARCASM twitter_1537,SARCASM twitter_1538,SARCASM twitter_1539,SARCASM twitter_1540,SARCASM twitter_1541,SARCASM twitter_1542,SARCASM twitter_1543,NOT_SARCASM twitter_1544,SARCASM twitter_1545,NOT_SARCASM twitter_1546,SARCASM twitter_1547,SARCASM twitter_1548,NOT_SARCASM twitter_1549,SARCASM twitter_1550,SARCASM twitter_1551,SARCASM twitter_1552,SARCASM twitter_1553,SARCASM twitter_1554,SARCASM twitter_1555,NOT_SARCASM twitter_1556,SARCASM twitter_1557,SARCASM twitter_1558,SARCASM twitter_1559,SARCASM twitter_1560,SARCASM twitter_1561,SARCASM twitter_1562,SARCASM twitter_1563,NOT_SARCASM twitter_1564,SARCASM twitter_1565,SARCASM twitter_1566,NOT_SARCASM twitter_1567,SARCASM twitter_1568,NOT_SARCASM twitter_1569,NOT_SARCASM twitter_1570,SARCASM twitter_1571,SARCASM twitter_1572,SARCASM twitter_1573,NOT_SARCASM twitter_1574,NOT_SARCASM twitter_1575,NOT_SARCASM twitter_1576,SARCASM twitter_1577,SARCASM twitter_1578,SARCASM twitter_1579,SARCASM twitter_1580,NOT_SARCASM twitter_1581,SARCASM twitter_1582,NOT_SARCASM twitter_1583,NOT_SARCASM twitter_1584,NOT_SARCASM twitter_1585,SARCASM twitter_1586,SARCASM twitter_1587,NOT_SARCASM twitter_1588,SARCASM twitter_1589,NOT_SARCASM twitter_1590,NOT_SARCASM twitter_1591,SARCASM twitter_1592,NOT_SARCASM twitter_1593,NOT_SARCASM twitter_1594,SARCASM twitter_1595,NOT_SARCASM twitter_1596,SARCASM twitter_1597,SARCASM twitter_1598,NOT_SARCASM twitter_1599,SARCASM twitter_1600,SARCASM twitter_1601,SARCASM twitter_1602,NOT_SARCASM twitter_1603,NOT_SARCASM twitter_1604,SARCASM twitter_1605,SARCASM twitter_1606,SARCASM twitter_1607,SARCASM twitter_1608,NOT_SARCASM twitter_1609,SARCASM twitter_1610,SARCASM twitter_1611,SARCASM twitter_1612,NOT_SARCASM twitter_1613,SARCASM twitter_1614,NOT_SARCASM twitter_1615,SARCASM twitter_1616,SARCASM twitter_1617,SARCASM twitter_1618,NOT_SARCASM twitter_1619,NOT_SARCASM twitter_1620,SARCASM twitter_1621,SARCASM twitter_1622,NOT_SARCASM twitter_1623,NOT_SARCASM twitter_1624,NOT_SARCASM twitter_1625,SARCASM twitter_1626,SARCASM twitter_1627,SARCASM twitter_1628,NOT_SARCASM twitter_1629,SARCASM twitter_1630,NOT_SARCASM twitter_1631,NOT_SARCASM twitter_1632,SARCASM twitter_1633,SARCASM twitter_1634,SARCASM twitter_1635,SARCASM twitter_1636,SARCASM twitter_1637,NOT_SARCASM twitter_1638,SARCASM twitter_1639,SARCASM twitter_1640,SARCASM twitter_1641,NOT_SARCASM twitter_1642,SARCASM twitter_1643,SARCASM twitter_1644,SARCASM twitter_1645,SARCASM twitter_1646,SARCASM twitter_1647,NOT_SARCASM twitter_1648,NOT_SARCASM twitter_1649,SARCASM twitter_1650,NOT_SARCASM twitter_1651,NOT_SARCASM twitter_1652,SARCASM twitter_1653,SARCASM twitter_1654,SARCASM twitter_1655,SARCASM twitter_1656,SARCASM twitter_1657,SARCASM twitter_1658,NOT_SARCASM twitter_1659,SARCASM twitter_1660,SARCASM twitter_1661,NOT_SARCASM twitter_1662,SARCASM twitter_1663,NOT_SARCASM twitter_1664,SARCASM twitter_1665,SARCASM twitter_1666,SARCASM twitter_1667,NOT_SARCASM twitter_1668,SARCASM twitter_1669,NOT_SARCASM twitter_1670,SARCASM twitter_1671,SARCASM twitter_1672,SARCASM twitter_1673,NOT_SARCASM twitter_1674,SARCASM twitter_1675,SARCASM twitter_1676,SARCASM twitter_1677,SARCASM twitter_1678,NOT_SARCASM twitter_1679,SARCASM twitter_1680,SARCASM twitter_1681,NOT_SARCASM twitter_1682,SARCASM twitter_1683,SARCASM twitter_1684,SARCASM twitter_1685,SARCASM twitter_1686,SARCASM twitter_1687,SARCASM twitter_1688,NOT_SARCASM twitter_1689,NOT_SARCASM twitter_1690,NOT_SARCASM twitter_1691,SARCASM twitter_1692,SARCASM twitter_1693,SARCASM twitter_1694,NOT_SARCASM twitter_1695,NOT_SARCASM twitter_1696,SARCASM twitter_1697,SARCASM twitter_1698,NOT_SARCASM twitter_1699,SARCASM twitter_1700,NOT_SARCASM twitter_1701,SARCASM twitter_1702,SARCASM twitter_1703,SARCASM twitter_1704,NOT_SARCASM twitter_1705,SARCASM twitter_1706,SARCASM twitter_1707,NOT_SARCASM twitter_1708,SARCASM twitter_1709,NOT_SARCASM twitter_1710,SARCASM twitter_1711,SARCASM twitter_1712,NOT_SARCASM twitter_1713,SARCASM twitter_1714,NOT_SARCASM twitter_1715,NOT_SARCASM twitter_1716,SARCASM twitter_1717,NOT_SARCASM twitter_1718,NOT_SARCASM twitter_1719,NOT_SARCASM twitter_1720,SARCASM twitter_1721,SARCASM twitter_1722,NOT_SARCASM twitter_1723,SARCASM twitter_1724,SARCASM twitter_1725,NOT_SARCASM twitter_1726,SARCASM twitter_1727,NOT_SARCASM twitter_1728,SARCASM twitter_1729,NOT_SARCASM twitter_1730,SARCASM twitter_1731,SARCASM twitter_1732,SARCASM twitter_1733,SARCASM twitter_1734,NOT_SARCASM twitter_1735,SARCASM twitter_1736,SARCASM twitter_1737,SARCASM twitter_1738,SARCASM twitter_1739,NOT_SARCASM twitter_1740,SARCASM twitter_1741,SARCASM twitter_1742,NOT_SARCASM twitter_1743,NOT_SARCASM twitter_1744,SARCASM twitter_1745,SARCASM twitter_1746,SARCASM twitter_1747,SARCASM twitter_1748,SARCASM twitter_1749,SARCASM twitter_1750,NOT_SARCASM twitter_1751,SARCASM twitter_1752,SARCASM twitter_1753,NOT_SARCASM twitter_1754,SARCASM twitter_1755,NOT_SARCASM twitter_1756,SARCASM twitter_1757,NOT_SARCASM twitter_1758,SARCASM twitter_1759,SARCASM twitter_1760,SARCASM twitter_1761,SARCASM twitter_1762,SARCASM twitter_1763,NOT_SARCASM twitter_1764,NOT_SARCASM twitter_1765,SARCASM twitter_1766,SARCASM twitter_1767,NOT_SARCASM twitter_1768,SARCASM twitter_1769,SARCASM twitter_1770,SARCASM twitter_1771,SARCASM twitter_1772,SARCASM twitter_1773,NOT_SARCASM twitter_1774,SARCASM twitter_1775,NOT_SARCASM twitter_1776,SARCASM twitter_1777,NOT_SARCASM twitter_1778,SARCASM twitter_1779,NOT_SARCASM twitter_1780,SARCASM twitter_1781,NOT_SARCASM twitter_1782,SARCASM twitter_1783,SARCASM twitter_1784,SARCASM twitter_1785,SARCASM twitter_1786,SARCASM twitter_1787,NOT_SARCASM twitter_1788,SARCASM twitter_1789,NOT_SARCASM twitter_1790,SARCASM twitter_1791,NOT_SARCASM twitter_1792,SARCASM twitter_1793,NOT_SARCASM twitter_1794,SARCASM twitter_1795,SARCASM twitter_1796,NOT_SARCASM twitter_1797,SARCASM twitter_1798,NOT_SARCASM twitter_1799,NOT_SARCASM twitter_1800,NOT_SARCASM"
https://github.com/zen030/CourseProject	Project_Documentation.pdf	"BERT Sentiment Analysis to Detect Twitter Sarcasm (Naive Approach) Zainal Hakim zainalh2@illinois.edu Table of content A. Introduction 3 B. Bidirectional Encoder Representations from Transformers (BERT) 3 C. Dataset Description 4 D. The Naive Approach 4 E. The Model, Training, and Evaluation 5 F. The Software Code 6 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review 6 1. Colab Configuration 6 2. Mounting Google Drive to Colab session (To save result files) 9 3. The Main Python Class 10 4. Training and Evaluation experiments 13 5. Save the result files to Google Drive 14 F.2. DEMO_Model_Evaluation.ipynb Code Review 15 1. The first 3-steps are already explained in detail in the previous section 15 2. Preparing the encoded testing dataset and data loader 15 3. Run the evaluation batch iteration 16 4. Generate the 'anwer.txt' file 17 5. Post 'answer.txt' to LiveDataLab for scoring 18 G. Result and Conclusion 18 Reference 19 Appendix 19 1. answer.txt 19 A. Introduction Sarcasm is a form of figurative language that implies a negative sentiment while displaying a positive sentiment on the surface (Joshi et al., 2017). I present a Naive approach to detect Twitter tweet sarcasm sentiment using a transformers-based pre-trained model that considers only the response tweet. This approach completely ignores the context of the response tweet to train the model. The model uses a transformer encoder to generate the embedding representation for the response. The model is trained and evaluated on the given training and testing datasets. My best performance model gives an F1-score of 75.79%, beating the Classification Competition baseline score after four epoch iterations (epoch # 4). ""C. Dataset Description"" section of this document explains further the response and context relationship. Important files in the project: 1. Documented software code * Training and evaluation (link) * Evaluation of a trained-model for demo purpose (link) 2. Best performance testing set predictions (answer.txt) (link) 3. Best performance trained-model (link) 4. Training dataset (link) 5. Testing dataset (link) B. Bidirectional Encoder Representations from Transformers (BERT) This project uses BERT, a transformer-based technique for Natural Language Processing pre- training developed by a team in Google. The original English language BERT model comes with two pre-trained model types: Model Type Layer Hidden Head Parameter Corpus Word Base 12 768 12 110 M 800 M Large 24 1024 16 340 M 2.500 M Table 1: BERT original model types BERT Large model essentially has better computing leverage than the base model. Google team trained the large model using a larger corpus word size than the base model. The large model is expected to perform better than the base model in most of the NLP tasks such as sentiment analysis. Original BERT paper is available here (link). C. Dataset Description There are two Twitter tweet datasets available for this project: 1. Training dataset: a labeled dataset to train the model 2. Testing dataset: tweet with a unique ID to evaluate the trained-model For the training dataset, each line contains a JSON object with the following columns: * label: SARCASM or NOT_SARCASM * response: the classified tweet * context: the conversation context of the response For the testing/evaluation dataset, each line contains a JSON object with the following columns: * id: unique identifier for the sample * response: the tweet to be classified * context: the conversation context of the response Training Dataset Testing Dataset 5000 lines 1800 lines Table 2: Dataset size statistics A more detailed dataset description is available in the project competition Github repository (link). D. The Naive Approach I hypothesize the context does not always support the sentiment of a response. Context can have an opposing effect on the sentiment of a response. I hypothesize there are 2 types of context: 1. A Positive context is a context that supports the sentiment of a response. 2. A Negative context is a context that does not support the sentiment of a response. Fig.1: Illustration of context reduces sentiment quality Sentiment Negative Context Fig.2: Illustration of context increase sentiment quality It is critical to utilize the context to support the response's sentiment. For this project, I consider only the sentiment-labeled response to training the model, and I completely ignore the context. I call this a Naive approach. In the future project, I can use advanced machine learning techniques to utilize response and context to train the model by selectively reconstruct the context to support the sentiment of a response. E. The Model, Training, and Evaluation In this project, I use the datasets to train and evaluate BERT Large uncased and base uncased models. I use the original BERT paper as a reference (A.3 Fine-tuning Procedure) to choose hyperparameters for my experiments. The hyperparameters in my experiments are: - Learning rate: 2e-5 - Batch size: 5 (considering memory size) - Epochs: 4 iterations - Epsilon: 1e-8 - Random seed value: 17 BERT model can handle a text with a maximum of 512 characters. If the input text is more than 512 characters, the model truncates the text to 512 characters. Response text in training and testing datasets is less than 512 characters, in this case, we are guaranteed to consider all words in the response text to train and evaluate the model. The Python source code below prints the response maximum characters for training and testing datasets. Positive Context S e n t i m e n t Fig.3: Source code to check maximum training and evaluation response characters length Response Max. Chars Training Dataset 315 Testing Dataset 310 Table 3: Maximum response characters length F. The Software Code For the software code of this project, I implemented two Google Colab Notebooks: 1. NAIVE_BERT_sentiment_analysis.ipynb: Training and evaluation notebook (link) 2. DEMO_Model_Evaluation.ipynb: Evaluation of selected trained-model notebook (for DEMO purpose) (link) I use the Google Colab PRO environment to implement and test the software code. Introduction about Google Colab is available here link. Software code uses the following main Python libraries: - Numpy 1.18.5 - Pytorch 1.7.0+cu101 - Huggingface Transformers 3.5.0 F.1. NAIVE_BERT_sentiment_analysis.ipynb Code Review This notebook trains and evaluates the BERT Large uncased and base uncased models using the provided datasets. In the end, the trained-model and evaluation results are copied and stored in the project Google Drive folder. 1. Colab Configuration a. Install Python modules required for the notebook. b. Copy train.jsonl and test.jsonl files from Google Drive to Colab session I have already copied train.jsonl and test.jsonl files to a Google Drive account created for this project. The files are shared with the public. The following code will copy the files from Google Drive to the Colab session. The source code above will prompt a URL. Click the URL, it will prompt the Google account login page. Select the Google account to run the notebook: Click the ""Allow"" button to allow Google Cloud SDK to access the Google account. That finally prompts the verification code. Copy the code and paste it in the ""Enter verification code"" text box. Press ""Enter"" That will copy the training and testing datasets to the Colab session! We are ready to train and evaluate the model using the datasets. 2. Mounting Google Drive to Colab session (To save result files) To save the result files (the trained-model and answer.txt) to Google Drive, we need to mount Google Drive to the Colab session. In this project, I mount the Google Drive directory to the './content/uiuc' folder. At the end of this notebook code execution, the Google Drive mounted folder in the Colab session will look like the following: Mounted Google Drive in Colab Session where to keep result files permanently 3. The Main Python Class This project implements a Python class, BERT_Model, that handles the following tasks: o Read the dataset from JSONL files into a list of JSON o Convert list of JSON to Pandas DataFrame o Create the BERT Model o Run the training and save the model for each epoch o Evaluate the model and store the result into a file Below are the class signatures: For details please check the source code here link. The source code comment describes what each step does. The main algorithm of the training: 1. Create a BERT tokenizer. We use the tokenizer to encode the text and prepare the dataset. 2. Create a BERT model using the chosen hyperparameter. 3. Create AdamW optimizer (link), it is used to calculate the convergence in the model. It is considered the fastest convergence algorithm. 4. Create a data loader. The data loader main task is passing dataset batch to model. It will make sure all the text in the dataset is processed by the model. 5. Set the model to train mode. Iterate the data loader to pass the dataset batch to the model until the training is completed. 6. After the training is completed, save the trained model to a file. The main algorithm of testing: 1. Load the model file to the memory and set the model to evaluation mode. 2. Create the tokenizer, encode the input data for testing, and prepare the dataset. 3. Create the data loader to handle the evaluation batch. 4. Run the evaluation. One of the most important outputs from the model is logits. In this project, logits is an array of two elements. The model is configured to have the first element of logits represents SARCASM, and the second element represents NOT_SARCASM. 5. We use the NumPy argmax function to return the index of the maximum value in logits. o If the maximum value is the first element, the function returns the index array of 0 (SARCASM) o If the maximum value is the second element, the function returns the index array of 1 (NOT_SARCASM) 6. The final step is to write the output to a file (answer.txt) 4. Training and Evaluation experiments To test my Naive hypothesis, I run experiments with the same hyperparameters on two BERT models. 1. Experiment-1: BERT base uncased 2. Experiment-2: BERT LARGE uncased Both experiments code above will generate result files in the Colab session folder below: 5. Save the result files to Google Drive Colab deletes result files when the session ends. We need to store the files permanently in other locations, in this project I use Google Drive. The code below will copy the result files to the project Google Drive folder (the Google Drive folder has been mounted in the earlier step). In this project, I save the epoch # 4 model file only. We use the files stored in the project Google Drive folder to run a project demo. The next section will illustrate how to use the trained-model file to evaluate the testing dataset. F.2. DEMO_Model_Evaluation.ipynb Code Review In the demo notebook, I demonstrate how to generate 'answer.txt' from the BERT Large uncased trained model stored in the project Google Drive folder. The trained-model is available to the public here link. With this model, we will reproduce the evaluation result which is available to the public here link. The demo video is available here link. The main algorithm in the notebook demo: 1. The first 3-steps are already explained in detail in the previous section * Colab configuration (Python modules import and installation) * Copy the trained-model and testing dataset files from Google Drive to the Colab session. We need to authorize Colab to access the Google account described in the previous section. * Prepare Panda DataFrame for the testing dataset. 2. Preparing the encoded testing dataset and data loader The main steps: * Create the BERT tokenizer to encode the testing dataset. * Create the data loader to run the evaluation in batches. Preparing the Python objects for the evaluation 3. Run the evaluation batch iteration The main steps: * Set the device to GPU, if applicable. * Load the trained model from file to memory. * Set the model to evaluation mode. * Data loader iteration to pass text to evaluate in batches to the model. * The model output logits (how to process logits is described in the previous section) 4. Generate the 'anwer.txt' file Generate the sentiment output to the screen and 'answer.txt' file. The previous section describes the source code snippet. 5. Post 'answer.txt' to LiveDataLab for scoring For F1, precision, and recall scores evaluation, I post the 'answer.txt' to LiveDataLab. Leaderboard snapshot on 03-Nov-2020 G. Result and Conclusion In summary, the testing (evaluation) results from BERT Large and base models: Model F1-Score Recall Precision BERT Large uncased 0.757905138339921 0.8522222222222222 0.6823843416370107 BERT Base uncased 0.7458777885548012 0.8544444444444445 0.6617900172117039 Surprisingly, the base model performs almost as good as the large model. In this project, I did try to use different trained model such as RoBERTa and XLNet (and different hyperparameters), but I could not produce a result higher than BERT Large uncased score. The project scope as proposed in the project proposal is to explore BERT hence, I am reporting the result for BERT models only. In the future, I would like to explore more on the following topics: * To use advanced machine learning techniques to explore other hyperparameters for BERT models. * To utilize both the context and response to training the models. I hypothesize that the context can be used as an additional dataset to train the model. * To explore another model, such as RoBERTa and XLNet. Reference 1. Aditya Joshi, Pushpak Bhattacharyya, and Mark J. Car- man. 2017. Automatic Sarcasm Detection: A Survey. ACM Computing Surveys, 50(5):1-22. 2. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Appendix 1. answer.txt twitter_1,NOT_SARCASM twitter_2,SARCASM twitter_3,SARCASM twitter_4,NOT_SARCASM twitter_5,SARCASM twitter_6,SARCASM twitter_7,NOT_SARCASM twitter_8,SARCASM twitter_9,NOT_SARCASM twitter_10,SARCASM twitter_11,NOT_SARCASM twitter_12,SARCASM twitter_13,SARCASM twitter_14,NOT_SARCASM twitter_15,SARCASM twitter_16,SARCASM twitter_17,SARCASM twitter_18,SARCASM twitter_19,SARCASM twitter_20,NOT_SARCASM twitter_21,NOT_SARCASM twitter_22,SARCASM twitter_23,NOT_SARCASM twitter_24,SARCASM twitter_25,SARCASM twitter_26,SARCASM twitter_27,NOT_SARCASM twitter_28,NOT_SARCASM twitter_29,SARCASM twitter_30,NOT_SARCASM twitter_31,SARCASM twitter_32,NOT_SARCASM twitter_33,NOT_SARCASM twitter_34,SARCASM twitter_35,NOT_SARCASM twitter_36,SARCASM twitter_37,SARCASM twitter_38,SARCASM twitter_39,SARCASM twitter_40,SARCASM twitter_41,SARCASM twitter_42,NOT_SARCASM twitter_43,SARCASM twitter_44,NOT_SARCASM twitter_45,SARCASM twitter_46,NOT_SARCASM twitter_47,SARCASM twitter_48,SARCASM twitter_49,NOT_SARCASM twitter_50,SARCASM twitter_51,NOT_SARCASM twitter_52,NOT_SARCASM twitter_53,SARCASM twitter_54,SARCASM twitter_55,SARCASM twitter_56,SARCASM twitter_57,NOT_SARCASM twitter_58,NOT_SARCASM twitter_59,SARCASM twitter_60,SARCASM twitter_61,NOT_SARCASM twitter_62,SARCASM twitter_63,SARCASM twitter_64,SARCASM twitter_65,SARCASM twitter_66,NOT_SARCASM twitter_67,SARCASM twitter_68,NOT_SARCASM twitter_69,SARCASM twitter_70,SARCASM twitter_71,NOT_SARCASM twitter_72,SARCASM twitter_73,SARCASM twitter_74,SARCASM twitter_75,NOT_SARCASM twitter_76,NOT_SARCASM twitter_77,SARCASM twitter_78,SARCASM twitter_79,NOT_SARCASM twitter_80,SARCASM twitter_81,NOT_SARCASM twitter_82,NOT_SARCASM twitter_83,SARCASM twitter_84,NOT_SARCASM twitter_85,SARCASM twitter_86,SARCASM twitter_87,NOT_SARCASM twitter_88,SARCASM twitter_89,SARCASM twitter_90,NOT_SARCASM twitter_91,NOT_SARCASM twitter_92,SARCASM twitter_93,SARCASM twitter_94,SARCASM twitter_95,SARCASM twitter_96,SARCASM twitter_97,SARCASM twitter_98,NOT_SARCASM twitter_99,NOT_SARCASM twitter_100,NOT_SARCASM twitter_101,SARCASM twitter_102,SARCASM twitter_103,NOT_SARCASM twitter_104,NOT_SARCASM twitter_105,SARCASM twitter_106,SARCASM twitter_107,NOT_SARCASM twitter_108,NOT_SARCASM twitter_109,SARCASM twitter_110,SARCASM twitter_111,SARCASM twitter_112,SARCASM twitter_113,SARCASM twitter_114,SARCASM twitter_115,SARCASM twitter_116,NOT_SARCASM twitter_117,NOT_SARCASM twitter_118,SARCASM twitter_119,NOT_SARCASM twitter_120,NOT_SARCASM twitter_121,SARCASM twitter_122,SARCASM twitter_123,NOT_SARCASM twitter_124,SARCASM twitter_125,NOT_SARCASM twitter_126,NOT_SARCASM twitter_127,SARCASM twitter_128,NOT_SARCASM twitter_129,NOT_SARCASM twitter_130,SARCASM twitter_131,NOT_SARCASM twitter_132,SARCASM twitter_133,NOT_SARCASM twitter_134,NOT_SARCASM twitter_135,SARCASM twitter_136,NOT_SARCASM twitter_137,SARCASM twitter_138,NOT_SARCASM twitter_139,NOT_SARCASM twitter_140,SARCASM twitter_141,SARCASM twitter_142,SARCASM twitter_143,SARCASM twitter_144,SARCASM twitter_145,SARCASM twitter_146,SARCASM twitter_147,SARCASM twitter_148,SARCASM twitter_149,SARCASM twitter_150,SARCASM twitter_151,SARCASM twitter_152,NOT_SARCASM twitter_153,NOT_SARCASM twitter_154,SARCASM twitter_155,SARCASM twitter_156,NOT_SARCASM twitter_157,SARCASM twitter_158,SARCASM twitter_159,SARCASM twitter_160,SARCASM twitter_161,NOT_SARCASM twitter_162,SARCASM twitter_163,SARCASM twitter_164,SARCASM twitter_165,SARCASM twitter_166,SARCASM twitter_167,SARCASM twitter_168,SARCASM twitter_169,NOT_SARCASM twitter_170,NOT_SARCASM twitter_171,NOT_SARCASM twitter_172,NOT_SARCASM twitter_173,SARCASM twitter_174,SARCASM twitter_175,SARCASM twitter_176,SARCASM twitter_177,SARCASM twitter_178,NOT_SARCASM twitter_179,SARCASM twitter_180,SARCASM twitter_181,SARCASM twitter_182,NOT_SARCASM twitter_183,SARCASM twitter_184,SARCASM twitter_185,NOT_SARCASM twitter_186,SARCASM twitter_187,SARCASM twitter_188,SARCASM twitter_189,NOT_SARCASM twitter_190,NOT_SARCASM twitter_191,NOT_SARCASM twitter_192,NOT_SARCASM twitter_193,SARCASM twitter_194,NOT_SARCASM twitter_195,NOT_SARCASM twitter_196,SARCASM twitter_197,NOT_SARCASM twitter_198,SARCASM twitter_199,SARCASM twitter_200,SARCASM twitter_201,SARCASM twitter_202,SARCASM twitter_203,NOT_SARCASM twitter_204,NOT_SARCASM twitter_205,SARCASM twitter_206,NOT_SARCASM twitter_207,SARCASM twitter_208,SARCASM twitter_209,SARCASM twitter_210,SARCASM twitter_211,SARCASM twitter_212,NOT_SARCASM twitter_213,NOT_SARCASM twitter_214,NOT_SARCASM twitter_215,NOT_SARCASM twitter_216,SARCASM twitter_217,SARCASM twitter_218,SARCASM twitter_219,SARCASM twitter_220,SARCASM twitter_221,SARCASM twitter_222,NOT_SARCASM twitter_223,NOT_SARCASM twitter_224,SARCASM twitter_225,NOT_SARCASM twitter_226,SARCASM twitter_227,NOT_SARCASM twitter_228,NOT_SARCASM twitter_229,SARCASM twitter_230,NOT_SARCASM twitter_231,SARCASM twitter_232,SARCASM twitter_233,SARCASM twitter_234,NOT_SARCASM twitter_235,SARCASM twitter_236,NOT_SARCASM twitter_237,SARCASM twitter_238,NOT_SARCASM twitter_239,SARCASM twitter_240,SARCASM twitter_241,SARCASM twitter_242,SARCASM twitter_243,NOT_SARCASM twitter_244,NOT_SARCASM twitter_245,NOT_SARCASM twitter_246,NOT_SARCASM twitter_247,SARCASM twitter_248,SARCASM twitter_249,SARCASM twitter_250,SARCASM twitter_251,SARCASM twitter_252,SARCASM twitter_253,NOT_SARCASM twitter_254,NOT_SARCASM twitter_255,SARCASM twitter_256,NOT_SARCASM twitter_257,NOT_SARCASM twitter_258,SARCASM twitter_259,SARCASM twitter_260,SARCASM twitter_261,NOT_SARCASM twitter_262,SARCASM twitter_263,NOT_SARCASM twitter_264,SARCASM twitter_265,SARCASM twitter_266,SARCASM twitter_267,SARCASM twitter_268,NOT_SARCASM twitter_269,SARCASM twitter_270,NOT_SARCASM twitter_271,NOT_SARCASM twitter_272,NOT_SARCASM twitter_273,NOT_SARCASM twitter_274,SARCASM twitter_275,SARCASM twitter_276,NOT_SARCASM twitter_277,NOT_SARCASM twitter_278,SARCASM twitter_279,SARCASM twitter_280,NOT_SARCASM twitter_281,SARCASM twitter_282,NOT_SARCASM twitter_283,SARCASM twitter_284,SARCASM twitter_285,NOT_SARCASM twitter_286,NOT_SARCASM twitter_287,NOT_SARCASM twitter_288,NOT_SARCASM twitter_289,NOT_SARCASM twitter_290,SARCASM twitter_291,SARCASM twitter_292,SARCASM twitter_293,SARCASM twitter_294,SARCASM twitter_295,NOT_SARCASM twitter_296,SARCASM twitter_297,SARCASM twitter_298,SARCASM twitter_299,SARCASM twitter_300,NOT_SARCASM twitter_301,SARCASM twitter_302,NOT_SARCASM twitter_303,SARCASM twitter_304,SARCASM twitter_305,NOT_SARCASM twitter_306,SARCASM twitter_307,NOT_SARCASM twitter_308,SARCASM twitter_309,SARCASM twitter_310,NOT_SARCASM twitter_311,SARCASM twitter_312,NOT_SARCASM twitter_313,SARCASM twitter_314,NOT_SARCASM twitter_315,SARCASM twitter_316,SARCASM twitter_317,NOT_SARCASM twitter_318,NOT_SARCASM twitter_319,SARCASM twitter_320,SARCASM twitter_321,NOT_SARCASM twitter_322,NOT_SARCASM twitter_323,NOT_SARCASM twitter_324,SARCASM twitter_325,NOT_SARCASM twitter_326,NOT_SARCASM twitter_327,SARCASM twitter_328,NOT_SARCASM twitter_329,SARCASM twitter_330,SARCASM twitter_331,SARCASM twitter_332,SARCASM twitter_333,SARCASM twitter_334,NOT_SARCASM twitter_335,NOT_SARCASM twitter_336,SARCASM twitter_337,NOT_SARCASM twitter_338,SARCASM twitter_339,NOT_SARCASM twitter_340,SARCASM twitter_341,NOT_SARCASM twitter_342,SARCASM twitter_343,SARCASM twitter_344,NOT_SARCASM twitter_345,SARCASM twitter_346,NOT_SARCASM twitter_347,SARCASM twitter_348,SARCASM twitter_349,SARCASM twitter_350,SARCASM twitter_351,SARCASM twitter_352,SARCASM twitter_353,NOT_SARCASM twitter_354,SARCASM twitter_355,SARCASM twitter_356,SARCASM twitter_357,SARCASM twitter_358,NOT_SARCASM twitter_359,SARCASM twitter_360,NOT_SARCASM twitter_361,SARCASM twitter_362,SARCASM twitter_363,SARCASM twitter_364,SARCASM twitter_365,NOT_SARCASM twitter_366,SARCASM twitter_367,SARCASM twitter_368,SARCASM twitter_369,NOT_SARCASM twitter_370,NOT_SARCASM twitter_371,SARCASM twitter_372,SARCASM twitter_373,SARCASM twitter_374,NOT_SARCASM twitter_375,SARCASM twitter_376,NOT_SARCASM twitter_377,SARCASM twitter_378,NOT_SARCASM twitter_379,NOT_SARCASM twitter_380,SARCASM twitter_381,SARCASM twitter_382,NOT_SARCASM twitter_383,SARCASM twitter_384,NOT_SARCASM twitter_385,NOT_SARCASM twitter_386,SARCASM twitter_387,SARCASM twitter_388,SARCASM twitter_389,NOT_SARCASM twitter_390,NOT_SARCASM twitter_391,SARCASM twitter_392,SARCASM twitter_393,NOT_SARCASM twitter_394,SARCASM twitter_395,SARCASM twitter_396,SARCASM twitter_397,NOT_SARCASM twitter_398,NOT_SARCASM twitter_399,SARCASM twitter_400,SARCASM twitter_401,SARCASM twitter_402,NOT_SARCASM twitter_403,SARCASM twitter_404,NOT_SARCASM twitter_405,NOT_SARCASM twitter_406,SARCASM twitter_407,SARCASM twitter_408,SARCASM twitter_409,SARCASM twitter_410,SARCASM twitter_411,SARCASM twitter_412,NOT_SARCASM twitter_413,SARCASM twitter_414,SARCASM twitter_415,SARCASM twitter_416,SARCASM twitter_417,SARCASM twitter_418,SARCASM twitter_419,NOT_SARCASM twitter_420,NOT_SARCASM twitter_421,SARCASM twitter_422,NOT_SARCASM twitter_423,SARCASM twitter_424,SARCASM twitter_425,SARCASM twitter_426,NOT_SARCASM twitter_427,SARCASM twitter_428,SARCASM twitter_429,NOT_SARCASM twitter_430,SARCASM twitter_431,SARCASM twitter_432,SARCASM twitter_433,SARCASM twitter_434,SARCASM twitter_435,NOT_SARCASM twitter_436,SARCASM twitter_437,SARCASM twitter_438,NOT_SARCASM twitter_439,NOT_SARCASM twitter_440,SARCASM twitter_441,SARCASM twitter_442,SARCASM twitter_443,SARCASM twitter_444,SARCASM twitter_445,SARCASM twitter_446,SARCASM twitter_447,SARCASM twitter_448,SARCASM twitter_449,SARCASM twitter_450,NOT_SARCASM twitter_451,NOT_SARCASM twitter_452,NOT_SARCASM twitter_453,SARCASM twitter_454,SARCASM twitter_455,NOT_SARCASM twitter_456,NOT_SARCASM twitter_457,SARCASM twitter_458,SARCASM twitter_459,SARCASM twitter_460,SARCASM twitter_461,SARCASM twitter_462,SARCASM twitter_463,SARCASM twitter_464,SARCASM twitter_465,NOT_SARCASM twitter_466,SARCASM twitter_467,NOT_SARCASM twitter_468,SARCASM twitter_469,SARCASM twitter_470,SARCASM twitter_471,SARCASM twitter_472,SARCASM twitter_473,NOT_SARCASM twitter_474,SARCASM twitter_475,NOT_SARCASM twitter_476,SARCASM twitter_477,NOT_SARCASM twitter_478,SARCASM twitter_479,SARCASM twitter_480,SARCASM twitter_481,NOT_SARCASM twitter_482,NOT_SARCASM twitter_483,NOT_SARCASM twitter_484,SARCASM twitter_485,NOT_SARCASM twitter_486,SARCASM twitter_487,NOT_SARCASM twitter_488,SARCASM twitter_489,SARCASM twitter_490,NOT_SARCASM twitter_491,NOT_SARCASM twitter_492,NOT_SARCASM twitter_493,SARCASM twitter_494,SARCASM twitter_495,SARCASM twitter_496,SARCASM twitter_497,NOT_SARCASM twitter_498,SARCASM twitter_499,SARCASM twitter_500,SARCASM twitter_501,SARCASM twitter_502,SARCASM twitter_503,SARCASM twitter_504,SARCASM twitter_505,SARCASM twitter_506,SARCASM twitter_507,SARCASM twitter_508,NOT_SARCASM twitter_509,NOT_SARCASM twitter_510,SARCASM twitter_511,NOT_SARCASM twitter_512,NOT_SARCASM twitter_513,SARCASM twitter_514,SARCASM twitter_515,NOT_SARCASM twitter_516,NOT_SARCASM twitter_517,SARCASM twitter_518,SARCASM twitter_519,NOT_SARCASM twitter_520,SARCASM twitter_521,SARCASM twitter_522,SARCASM twitter_523,NOT_SARCASM twitter_524,SARCASM twitter_525,SARCASM twitter_526,SARCASM twitter_527,SARCASM twitter_528,SARCASM twitter_529,NOT_SARCASM twitter_530,NOT_SARCASM twitter_531,NOT_SARCASM twitter_532,SARCASM twitter_533,NOT_SARCASM twitter_534,SARCASM twitter_535,SARCASM twitter_536,SARCASM twitter_537,NOT_SARCASM twitter_538,NOT_SARCASM twitter_539,SARCASM twitter_540,SARCASM twitter_541,SARCASM twitter_542,SARCASM twitter_543,NOT_SARCASM twitter_544,NOT_SARCASM twitter_545,SARCASM twitter_546,SARCASM twitter_547,SARCASM twitter_548,NOT_SARCASM twitter_549,NOT_SARCASM twitter_550,SARCASM twitter_551,SARCASM twitter_552,NOT_SARCASM twitter_553,SARCASM twitter_554,SARCASM twitter_555,NOT_SARCASM twitter_556,SARCASM twitter_557,SARCASM twitter_558,SARCASM twitter_559,SARCASM twitter_560,SARCASM twitter_561,NOT_SARCASM twitter_562,SARCASM twitter_563,SARCASM twitter_564,SARCASM twitter_565,SARCASM twitter_566,SARCASM twitter_567,SARCASM twitter_568,SARCASM twitter_569,NOT_SARCASM twitter_570,SARCASM twitter_571,SARCASM twitter_572,SARCASM twitter_573,SARCASM twitter_574,SARCASM twitter_575,NOT_SARCASM twitter_576,SARCASM twitter_577,SARCASM twitter_578,NOT_SARCASM twitter_579,NOT_SARCASM twitter_580,NOT_SARCASM twitter_581,NOT_SARCASM twitter_582,NOT_SARCASM twitter_583,SARCASM twitter_584,SARCASM twitter_585,SARCASM twitter_586,SARCASM twitter_587,SARCASM twitter_588,SARCASM twitter_589,NOT_SARCASM twitter_590,NOT_SARCASM twitter_591,SARCASM twitter_592,SARCASM twitter_593,NOT_SARCASM twitter_594,SARCASM twitter_595,SARCASM twitter_596,NOT_SARCASM twitter_597,NOT_SARCASM twitter_598,SARCASM twitter_599,NOT_SARCASM twitter_600,SARCASM twitter_601,NOT_SARCASM twitter_602,SARCASM twitter_603,NOT_SARCASM twitter_604,SARCASM twitter_605,NOT_SARCASM twitter_606,NOT_SARCASM twitter_607,SARCASM twitter_608,SARCASM twitter_609,NOT_SARCASM twitter_610,NOT_SARCASM twitter_611,NOT_SARCASM twitter_612,SARCASM twitter_613,SARCASM twitter_614,NOT_SARCASM twitter_615,NOT_SARCASM twitter_616,SARCASM twitter_617,SARCASM twitter_618,NOT_SARCASM twitter_619,SARCASM twitter_620,NOT_SARCASM twitter_621,SARCASM twitter_622,NOT_SARCASM twitter_623,SARCASM twitter_624,NOT_SARCASM twitter_625,SARCASM twitter_626,SARCASM twitter_627,NOT_SARCASM twitter_628,NOT_SARCASM twitter_629,SARCASM twitter_630,SARCASM twitter_631,NOT_SARCASM twitter_632,SARCASM twitter_633,NOT_SARCASM twitter_634,SARCASM twitter_635,SARCASM twitter_636,NOT_SARCASM twitter_637,SARCASM twitter_638,NOT_SARCASM twitter_639,SARCASM twitter_640,NOT_SARCASM twitter_641,NOT_SARCASM twitter_642,SARCASM twitter_643,SARCASM twitter_644,SARCASM twitter_645,NOT_SARCASM twitter_646,NOT_SARCASM twitter_647,NOT_SARCASM twitter_648,NOT_SARCASM twitter_649,SARCASM twitter_650,NOT_SARCASM twitter_651,SARCASM twitter_652,NOT_SARCASM twitter_653,NOT_SARCASM twitter_654,SARCASM twitter_655,NOT_SARCASM twitter_656,NOT_SARCASM twitter_657,SARCASM twitter_658,NOT_SARCASM twitter_659,SARCASM twitter_660,SARCASM twitter_661,SARCASM twitter_662,SARCASM twitter_663,NOT_SARCASM twitter_664,SARCASM twitter_665,NOT_SARCASM twitter_666,NOT_SARCASM twitter_667,SARCASM twitter_668,SARCASM twitter_669,SARCASM twitter_670,NOT_SARCASM twitter_671,SARCASM twitter_672,SARCASM twitter_673,SARCASM twitter_674,SARCASM twitter_675,SARCASM twitter_676,NOT_SARCASM twitter_677,NOT_SARCASM twitter_678,SARCASM twitter_679,SARCASM twitter_680,NOT_SARCASM twitter_681,NOT_SARCASM twitter_682,SARCASM twitter_683,NOT_SARCASM twitter_684,NOT_SARCASM twitter_685,SARCASM twitter_686,SARCASM twitter_687,NOT_SARCASM twitter_688,SARCASM twitter_689,SARCASM twitter_690,SARCASM twitter_691,NOT_SARCASM twitter_692,SARCASM twitter_693,SARCASM twitter_694,NOT_SARCASM twitter_695,SARCASM twitter_696,NOT_SARCASM twitter_697,SARCASM twitter_698,NOT_SARCASM twitter_699,NOT_SARCASM twitter_700,NOT_SARCASM twitter_701,SARCASM twitter_702,NOT_SARCASM twitter_703,SARCASM twitter_704,SARCASM twitter_705,NOT_SARCASM twitter_706,SARCASM twitter_707,NOT_SARCASM twitter_708,SARCASM twitter_709,NOT_SARCASM twitter_710,SARCASM twitter_711,SARCASM twitter_712,SARCASM twitter_713,SARCASM twitter_714,SARCASM twitter_715,SARCASM twitter_716,SARCASM twitter_717,SARCASM twitter_718,SARCASM twitter_719,NOT_SARCASM twitter_720,NOT_SARCASM twitter_721,SARCASM twitter_722,NOT_SARCASM twitter_723,SARCASM twitter_724,SARCASM twitter_725,SARCASM twitter_726,NOT_SARCASM twitter_727,NOT_SARCASM twitter_728,SARCASM twitter_729,SARCASM twitter_730,SARCASM twitter_731,NOT_SARCASM twitter_732,SARCASM twitter_733,NOT_SARCASM twitter_734,NOT_SARCASM twitter_735,SARCASM twitter_736,SARCASM twitter_737,SARCASM twitter_738,SARCASM twitter_739,SARCASM twitter_740,SARCASM twitter_741,SARCASM twitter_742,SARCASM twitter_743,SARCASM twitter_744,NOT_SARCASM twitter_745,NOT_SARCASM twitter_746,SARCASM twitter_747,SARCASM twitter_748,NOT_SARCASM twitter_749,NOT_SARCASM twitter_750,NOT_SARCASM twitter_751,SARCASM twitter_752,NOT_SARCASM twitter_753,SARCASM twitter_754,SARCASM twitter_755,NOT_SARCASM twitter_756,SARCASM twitter_757,NOT_SARCASM twitter_758,NOT_SARCASM twitter_759,SARCASM twitter_760,SARCASM twitter_761,NOT_SARCASM twitter_762,SARCASM twitter_763,NOT_SARCASM twitter_764,SARCASM twitter_765,SARCASM twitter_766,SARCASM twitter_767,SARCASM twitter_768,NOT_SARCASM twitter_769,NOT_SARCASM twitter_770,NOT_SARCASM twitter_771,SARCASM twitter_772,NOT_SARCASM twitter_773,SARCASM twitter_774,NOT_SARCASM twitter_775,SARCASM twitter_776,SARCASM twitter_777,NOT_SARCASM twitter_778,SARCASM twitter_779,NOT_SARCASM twitter_780,SARCASM twitter_781,NOT_SARCASM twitter_782,NOT_SARCASM twitter_783,SARCASM twitter_784,SARCASM twitter_785,NOT_SARCASM twitter_786,SARCASM twitter_787,SARCASM twitter_788,NOT_SARCASM twitter_789,SARCASM twitter_790,SARCASM twitter_791,NOT_SARCASM twitter_792,NOT_SARCASM twitter_793,SARCASM twitter_794,NOT_SARCASM twitter_795,SARCASM twitter_796,SARCASM twitter_797,SARCASM twitter_798,SARCASM twitter_799,SARCASM twitter_800,NOT_SARCASM twitter_801,NOT_SARCASM twitter_802,NOT_SARCASM twitter_803,SARCASM twitter_804,NOT_SARCASM twitter_805,SARCASM twitter_806,SARCASM twitter_807,NOT_SARCASM twitter_808,SARCASM twitter_809,SARCASM twitter_810,NOT_SARCASM twitter_811,SARCASM twitter_812,SARCASM twitter_813,SARCASM twitter_814,SARCASM twitter_815,SARCASM twitter_816,SARCASM twitter_817,SARCASM twitter_818,NOT_SARCASM twitter_819,SARCASM twitter_820,NOT_SARCASM twitter_821,NOT_SARCASM twitter_822,NOT_SARCASM twitter_823,SARCASM twitter_824,NOT_SARCASM twitter_825,NOT_SARCASM twitter_826,SARCASM twitter_827,SARCASM twitter_828,NOT_SARCASM twitter_829,SARCASM twitter_830,SARCASM twitter_831,NOT_SARCASM twitter_832,NOT_SARCASM twitter_833,NOT_SARCASM twitter_834,NOT_SARCASM twitter_835,SARCASM twitter_836,SARCASM twitter_837,SARCASM twitter_838,SARCASM twitter_839,SARCASM twitter_840,SARCASM twitter_841,SARCASM twitter_842,SARCASM twitter_843,SARCASM twitter_844,SARCASM twitter_845,NOT_SARCASM twitter_846,NOT_SARCASM twitter_847,SARCASM twitter_848,NOT_SARCASM twitter_849,NOT_SARCASM twitter_850,SARCASM twitter_851,SARCASM twitter_852,NOT_SARCASM twitter_853,NOT_SARCASM twitter_854,NOT_SARCASM twitter_855,NOT_SARCASM twitter_856,SARCASM twitter_857,SARCASM twitter_858,SARCASM twitter_859,NOT_SARCASM twitter_860,NOT_SARCASM twitter_861,NOT_SARCASM twitter_862,NOT_SARCASM twitter_863,SARCASM twitter_864,NOT_SARCASM twitter_865,SARCASM twitter_866,SARCASM twitter_867,SARCASM twitter_868,SARCASM twitter_869,SARCASM twitter_870,SARCASM twitter_871,SARCASM twitter_872,NOT_SARCASM twitter_873,SARCASM twitter_874,NOT_SARCASM twitter_875,NOT_SARCASM twitter_876,NOT_SARCASM twitter_877,NOT_SARCASM twitter_878,SARCASM twitter_879,NOT_SARCASM twitter_880,SARCASM twitter_881,NOT_SARCASM twitter_882,NOT_SARCASM twitter_883,SARCASM twitter_884,SARCASM twitter_885,SARCASM twitter_886,SARCASM twitter_887,NOT_SARCASM twitter_888,SARCASM twitter_889,NOT_SARCASM twitter_890,SARCASM twitter_891,SARCASM twitter_892,NOT_SARCASM twitter_893,SARCASM twitter_894,NOT_SARCASM twitter_895,SARCASM twitter_896,NOT_SARCASM twitter_897,NOT_SARCASM twitter_898,SARCASM twitter_899,SARCASM twitter_900,NOT_SARCASM twitter_901,SARCASM twitter_902,SARCASM twitter_903,SARCASM twitter_904,SARCASM twitter_905,SARCASM twitter_906,SARCASM twitter_907,SARCASM twitter_908,SARCASM twitter_909,SARCASM twitter_910,NOT_SARCASM twitter_911,NOT_SARCASM twitter_912,NOT_SARCASM twitter_913,SARCASM twitter_914,SARCASM twitter_915,SARCASM twitter_916,SARCASM twitter_917,SARCASM twitter_918,NOT_SARCASM twitter_919,SARCASM twitter_920,NOT_SARCASM twitter_921,SARCASM twitter_922,SARCASM twitter_923,NOT_SARCASM twitter_924,SARCASM twitter_925,NOT_SARCASM twitter_926,NOT_SARCASM twitter_927,NOT_SARCASM twitter_928,NOT_SARCASM twitter_929,NOT_SARCASM twitter_930,NOT_SARCASM twitter_931,SARCASM twitter_932,SARCASM twitter_933,NOT_SARCASM twitter_934,NOT_SARCASM twitter_935,SARCASM twitter_936,SARCASM twitter_937,SARCASM twitter_938,SARCASM twitter_939,SARCASM twitter_940,SARCASM twitter_941,SARCASM twitter_942,NOT_SARCASM twitter_943,NOT_SARCASM twitter_944,SARCASM twitter_945,SARCASM twitter_946,SARCASM twitter_947,NOT_SARCASM twitter_948,SARCASM twitter_949,SARCASM twitter_950,SARCASM twitter_951,NOT_SARCASM twitter_952,SARCASM twitter_953,NOT_SARCASM twitter_954,SARCASM twitter_955,SARCASM twitter_956,SARCASM twitter_957,SARCASM twitter_958,SARCASM twitter_959,NOT_SARCASM twitter_960,SARCASM twitter_961,SARCASM twitter_962,SARCASM twitter_963,NOT_SARCASM twitter_964,NOT_SARCASM twitter_965,SARCASM twitter_966,SARCASM twitter_967,SARCASM twitter_968,SARCASM twitter_969,SARCASM twitter_970,SARCASM twitter_971,SARCASM twitter_972,NOT_SARCASM twitter_973,SARCASM twitter_974,NOT_SARCASM twitter_975,SARCASM twitter_976,NOT_SARCASM twitter_977,SARCASM twitter_978,SARCASM twitter_979,SARCASM twitter_980,SARCASM twitter_981,SARCASM twitter_982,NOT_SARCASM twitter_983,SARCASM twitter_984,SARCASM twitter_985,SARCASM twitter_986,NOT_SARCASM twitter_987,NOT_SARCASM twitter_988,SARCASM twitter_989,NOT_SARCASM twitter_990,NOT_SARCASM twitter_991,SARCASM twitter_992,SARCASM twitter_993,SARCASM twitter_994,SARCASM twitter_995,SARCASM twitter_996,SARCASM twitter_997,SARCASM twitter_998,SARCASM twitter_999,NOT_SARCASM twitter_1000,NOT_SARCASM twitter_1001,NOT_SARCASM twitter_1002,SARCASM twitter_1003,SARCASM twitter_1004,SARCASM twitter_1005,SARCASM twitter_1006,SARCASM twitter_1007,SARCASM twitter_1008,NOT_SARCASM twitter_1009,NOT_SARCASM twitter_1010,SARCASM twitter_1011,NOT_SARCASM twitter_1012,SARCASM twitter_1013,SARCASM twitter_1014,SARCASM twitter_1015,NOT_SARCASM twitter_1016,NOT_SARCASM twitter_1017,NOT_SARCASM twitter_1018,SARCASM twitter_1019,NOT_SARCASM twitter_1020,SARCASM twitter_1021,NOT_SARCASM twitter_1022,NOT_SARCASM twitter_1023,SARCASM twitter_1024,SARCASM twitter_1025,NOT_SARCASM twitter_1026,NOT_SARCASM twitter_1027,SARCASM twitter_1028,SARCASM twitter_1029,NOT_SARCASM twitter_1030,SARCASM twitter_1031,NOT_SARCASM twitter_1032,NOT_SARCASM twitter_1033,NOT_SARCASM twitter_1034,SARCASM twitter_1035,NOT_SARCASM twitter_1036,SARCASM twitter_1037,NOT_SARCASM twitter_1038,SARCASM twitter_1039,SARCASM twitter_1040,SARCASM twitter_1041,NOT_SARCASM twitter_1042,SARCASM twitter_1043,SARCASM twitter_1044,NOT_SARCASM twitter_1045,SARCASM twitter_1046,SARCASM twitter_1047,NOT_SARCASM twitter_1048,SARCASM twitter_1049,NOT_SARCASM twitter_1050,SARCASM twitter_1051,NOT_SARCASM twitter_1052,NOT_SARCASM twitter_1053,SARCASM twitter_1054,SARCASM twitter_1055,NOT_SARCASM twitter_1056,SARCASM twitter_1057,NOT_SARCASM twitter_1058,SARCASM twitter_1059,SARCASM twitter_1060,NOT_SARCASM twitter_1061,SARCASM twitter_1062,SARCASM twitter_1063,NOT_SARCASM twitter_1064,NOT_SARCASM twitter_1065,SARCASM twitter_1066,NOT_SARCASM twitter_1067,SARCASM twitter_1068,NOT_SARCASM twitter_1069,SARCASM twitter_1070,SARCASM twitter_1071,SARCASM twitter_1072,NOT_SARCASM twitter_1073,NOT_SARCASM twitter_1074,SARCASM twitter_1075,NOT_SARCASM twitter_1076,SARCASM twitter_1077,NOT_SARCASM twitter_1078,SARCASM twitter_1079,SARCASM twitter_1080,SARCASM twitter_1081,SARCASM twitter_1082,NOT_SARCASM twitter_1083,SARCASM twitter_1084,SARCASM twitter_1085,SARCASM twitter_1086,SARCASM twitter_1087,NOT_SARCASM twitter_1088,NOT_SARCASM twitter_1089,NOT_SARCASM twitter_1090,NOT_SARCASM twitter_1091,SARCASM twitter_1092,SARCASM twitter_1093,NOT_SARCASM twitter_1094,NOT_SARCASM twitter_1095,SARCASM twitter_1096,NOT_SARCASM twitter_1097,NOT_SARCASM twitter_1098,NOT_SARCASM twitter_1099,NOT_SARCASM twitter_1100,SARCASM twitter_1101,SARCASM twitter_1102,SARCASM twitter_1103,SARCASM twitter_1104,SARCASM twitter_1105,SARCASM twitter_1106,NOT_SARCASM twitter_1107,SARCASM twitter_1108,SARCASM twitter_1109,SARCASM twitter_1110,NOT_SARCASM twitter_1111,SARCASM twitter_1112,SARCASM twitter_1113,SARCASM twitter_1114,NOT_SARCASM twitter_1115,SARCASM twitter_1116,SARCASM twitter_1117,NOT_SARCASM twitter_1118,NOT_SARCASM twitter_1119,NOT_SARCASM twitter_1120,SARCASM twitter_1121,SARCASM twitter_1122,NOT_SARCASM twitter_1123,SARCASM twitter_1124,SARCASM twitter_1125,SARCASM twitter_1126,NOT_SARCASM twitter_1127,NOT_SARCASM twitter_1128,NOT_SARCASM twitter_1129,NOT_SARCASM twitter_1130,SARCASM twitter_1131,SARCASM twitter_1132,NOT_SARCASM twitter_1133,SARCASM twitter_1134,NOT_SARCASM twitter_1135,NOT_SARCASM twitter_1136,SARCASM twitter_1137,SARCASM twitter_1138,NOT_SARCASM twitter_1139,SARCASM twitter_1140,NOT_SARCASM twitter_1141,SARCASM twitter_1142,SARCASM twitter_1143,SARCASM twitter_1144,SARCASM twitter_1145,SARCASM twitter_1146,NOT_SARCASM twitter_1147,SARCASM twitter_1148,NOT_SARCASM twitter_1149,SARCASM twitter_1150,NOT_SARCASM twitter_1151,NOT_SARCASM twitter_1152,SARCASM twitter_1153,NOT_SARCASM twitter_1154,NOT_SARCASM twitter_1155,SARCASM twitter_1156,SARCASM twitter_1157,SARCASM twitter_1158,SARCASM twitter_1159,SARCASM twitter_1160,NOT_SARCASM twitter_1161,NOT_SARCASM twitter_1162,SARCASM twitter_1163,NOT_SARCASM twitter_1164,SARCASM twitter_1165,NOT_SARCASM twitter_1166,SARCASM twitter_1167,SARCASM twitter_1168,NOT_SARCASM twitter_1169,SARCASM twitter_1170,SARCASM twitter_1171,SARCASM twitter_1172,SARCASM twitter_1173,SARCASM twitter_1174,SARCASM twitter_1175,SARCASM twitter_1176,NOT_SARCASM twitter_1177,NOT_SARCASM twitter_1178,SARCASM twitter_1179,NOT_SARCASM twitter_1180,SARCASM twitter_1181,NOT_SARCASM twitter_1182,NOT_SARCASM twitter_1183,SARCASM twitter_1184,SARCASM twitter_1185,NOT_SARCASM twitter_1186,SARCASM twitter_1187,SARCASM twitter_1188,SARCASM twitter_1189,SARCASM twitter_1190,SARCASM twitter_1191,SARCASM twitter_1192,SARCASM twitter_1193,NOT_SARCASM twitter_1194,NOT_SARCASM twitter_1195,NOT_SARCASM twitter_1196,NOT_SARCASM twitter_1197,NOT_SARCASM twitter_1198,SARCASM twitter_1199,SARCASM twitter_1200,SARCASM twitter_1201,NOT_SARCASM twitter_1202,SARCASM twitter_1203,NOT_SARCASM twitter_1204,NOT_SARCASM twitter_1205,NOT_SARCASM twitter_1206,NOT_SARCASM twitter_1207,SARCASM twitter_1208,SARCASM twitter_1209,NOT_SARCASM twitter_1210,SARCASM twitter_1211,SARCASM twitter_1212,SARCASM twitter_1213,NOT_SARCASM twitter_1214,SARCASM twitter_1215,SARCASM twitter_1216,NOT_SARCASM twitter_1217,NOT_SARCASM twitter_1218,NOT_SARCASM twitter_1219,SARCASM twitter_1220,NOT_SARCASM twitter_1221,SARCASM twitter_1222,SARCASM twitter_1223,NOT_SARCASM twitter_1224,NOT_SARCASM twitter_1225,SARCASM twitter_1226,SARCASM twitter_1227,SARCASM twitter_1228,SARCASM twitter_1229,SARCASM twitter_1230,SARCASM twitter_1231,SARCASM twitter_1232,SARCASM twitter_1233,SARCASM twitter_1234,SARCASM twitter_1235,SARCASM twitter_1236,SARCASM twitter_1237,NOT_SARCASM twitter_1238,SARCASM twitter_1239,NOT_SARCASM twitter_1240,SARCASM twitter_1241,NOT_SARCASM twitter_1242,SARCASM twitter_1243,SARCASM twitter_1244,NOT_SARCASM twitter_1245,NOT_SARCASM twitter_1246,NOT_SARCASM twitter_1247,SARCASM twitter_1248,SARCASM twitter_1249,NOT_SARCASM twitter_1250,NOT_SARCASM twitter_1251,SARCASM twitter_1252,SARCASM twitter_1253,NOT_SARCASM twitter_1254,SARCASM twitter_1255,SARCASM twitter_1256,NOT_SARCASM twitter_1257,SARCASM twitter_1258,NOT_SARCASM twitter_1259,NOT_SARCASM twitter_1260,SARCASM twitter_1261,SARCASM twitter_1262,SARCASM twitter_1263,SARCASM twitter_1264,SARCASM twitter_1265,NOT_SARCASM twitter_1266,NOT_SARCASM twitter_1267,SARCASM twitter_1268,SARCASM twitter_1269,NOT_SARCASM twitter_1270,SARCASM twitter_1271,SARCASM twitter_1272,SARCASM twitter_1273,NOT_SARCASM twitter_1274,SARCASM twitter_1275,SARCASM twitter_1276,NOT_SARCASM twitter_1277,NOT_SARCASM twitter_1278,NOT_SARCASM twitter_1279,NOT_SARCASM twitter_1280,SARCASM twitter_1281,NOT_SARCASM twitter_1282,NOT_SARCASM twitter_1283,SARCASM twitter_1284,NOT_SARCASM twitter_1285,SARCASM twitter_1286,NOT_SARCASM twitter_1287,NOT_SARCASM twitter_1288,SARCASM twitter_1289,SARCASM twitter_1290,SARCASM twitter_1291,SARCASM twitter_1292,SARCASM twitter_1293,SARCASM twitter_1294,NOT_SARCASM twitter_1295,SARCASM twitter_1296,SARCASM twitter_1297,SARCASM twitter_1298,SARCASM twitter_1299,SARCASM twitter_1300,SARCASM twitter_1301,SARCASM twitter_1302,SARCASM twitter_1303,NOT_SARCASM twitter_1304,NOT_SARCASM twitter_1305,SARCASM twitter_1306,NOT_SARCASM twitter_1307,SARCASM twitter_1308,SARCASM twitter_1309,SARCASM twitter_1310,SARCASM twitter_1311,SARCASM twitter_1312,SARCASM twitter_1313,NOT_SARCASM twitter_1314,SARCASM twitter_1315,SARCASM twitter_1316,SARCASM twitter_1317,SARCASM twitter_1318,NOT_SARCASM twitter_1319,NOT_SARCASM twitter_1320,SARCASM twitter_1321,SARCASM twitter_1322,SARCASM twitter_1323,SARCASM twitter_1324,SARCASM twitter_1325,SARCASM twitter_1326,NOT_SARCASM twitter_1327,NOT_SARCASM twitter_1328,NOT_SARCASM twitter_1329,SARCASM twitter_1330,SARCASM twitter_1331,SARCASM twitter_1332,SARCASM twitter_1333,NOT_SARCASM twitter_1334,SARCASM twitter_1335,NOT_SARCASM twitter_1336,NOT_SARCASM twitter_1337,SARCASM twitter_1338,NOT_SARCASM twitter_1339,SARCASM twitter_1340,SARCASM twitter_1341,SARCASM twitter_1342,SARCASM twitter_1343,SARCASM twitter_1344,SARCASM twitter_1345,NOT_SARCASM twitter_1346,SARCASM twitter_1347,SARCASM twitter_1348,SARCASM twitter_1349,SARCASM twitter_1350,SARCASM twitter_1351,SARCASM twitter_1352,NOT_SARCASM twitter_1353,SARCASM twitter_1354,NOT_SARCASM twitter_1355,SARCASM twitter_1356,SARCASM twitter_1357,SARCASM twitter_1358,SARCASM twitter_1359,SARCASM twitter_1360,SARCASM twitter_1361,SARCASM twitter_1362,NOT_SARCASM twitter_1363,SARCASM twitter_1364,NOT_SARCASM twitter_1365,SARCASM twitter_1366,SARCASM twitter_1367,SARCASM twitter_1368,SARCASM twitter_1369,NOT_SARCASM twitter_1370,NOT_SARCASM twitter_1371,NOT_SARCASM twitter_1372,NOT_SARCASM twitter_1373,NOT_SARCASM twitter_1374,SARCASM twitter_1375,NOT_SARCASM twitter_1376,SARCASM twitter_1377,SARCASM twitter_1378,NOT_SARCASM twitter_1379,SARCASM twitter_1380,SARCASM twitter_1381,SARCASM twitter_1382,SARCASM twitter_1383,SARCASM twitter_1384,NOT_SARCASM twitter_1385,SARCASM twitter_1386,NOT_SARCASM twitter_1387,SARCASM twitter_1388,SARCASM twitter_1389,SARCASM twitter_1390,SARCASM twitter_1391,SARCASM twitter_1392,SARCASM twitter_1393,SARCASM twitter_1394,NOT_SARCASM twitter_1395,SARCASM twitter_1396,NOT_SARCASM twitter_1397,SARCASM twitter_1398,SARCASM twitter_1399,SARCASM twitter_1400,NOT_SARCASM twitter_1401,SARCASM twitter_1402,SARCASM twitter_1403,SARCASM twitter_1404,SARCASM twitter_1405,NOT_SARCASM twitter_1406,NOT_SARCASM twitter_1407,SARCASM twitter_1408,SARCASM twitter_1409,SARCASM twitter_1410,SARCASM twitter_1411,SARCASM twitter_1412,NOT_SARCASM twitter_1413,NOT_SARCASM twitter_1414,SARCASM twitter_1415,SARCASM twitter_1416,SARCASM twitter_1417,SARCASM twitter_1418,NOT_SARCASM twitter_1419,NOT_SARCASM twitter_1420,NOT_SARCASM twitter_1421,NOT_SARCASM twitter_1422,NOT_SARCASM twitter_1423,SARCASM twitter_1424,SARCASM twitter_1425,SARCASM twitter_1426,NOT_SARCASM twitter_1427,SARCASM twitter_1428,SARCASM twitter_1429,SARCASM twitter_1430,NOT_SARCASM twitter_1431,NOT_SARCASM twitter_1432,SARCASM twitter_1433,NOT_SARCASM twitter_1434,NOT_SARCASM twitter_1435,SARCASM twitter_1436,SARCASM twitter_1437,SARCASM twitter_1438,SARCASM twitter_1439,SARCASM twitter_1440,NOT_SARCASM twitter_1441,SARCASM twitter_1442,NOT_SARCASM twitter_1443,NOT_SARCASM twitter_1444,NOT_SARCASM twitter_1445,NOT_SARCASM twitter_1446,NOT_SARCASM twitter_1447,SARCASM twitter_1448,SARCASM twitter_1449,SARCASM twitter_1450,SARCASM twitter_1451,SARCASM twitter_1452,SARCASM twitter_1453,SARCASM twitter_1454,SARCASM twitter_1455,SARCASM twitter_1456,SARCASM twitter_1457,NOT_SARCASM twitter_1458,NOT_SARCASM twitter_1459,NOT_SARCASM twitter_1460,SARCASM twitter_1461,SARCASM twitter_1462,SARCASM twitter_1463,SARCASM twitter_1464,SARCASM twitter_1465,SARCASM twitter_1466,SARCASM twitter_1467,NOT_SARCASM twitter_1468,SARCASM twitter_1469,SARCASM twitter_1470,SARCASM twitter_1471,SARCASM twitter_1472,SARCASM twitter_1473,NOT_SARCASM twitter_1474,NOT_SARCASM twitter_1475,SARCASM twitter_1476,SARCASM twitter_1477,SARCASM twitter_1478,NOT_SARCASM twitter_1479,SARCASM twitter_1480,NOT_SARCASM twitter_1481,SARCASM twitter_1482,NOT_SARCASM twitter_1483,SARCASM twitter_1484,SARCASM twitter_1485,NOT_SARCASM twitter_1486,SARCASM twitter_1487,SARCASM twitter_1488,SARCASM twitter_1489,SARCASM twitter_1490,SARCASM twitter_1491,NOT_SARCASM twitter_1492,SARCASM twitter_1493,SARCASM twitter_1494,SARCASM twitter_1495,NOT_SARCASM twitter_1496,SARCASM twitter_1497,NOT_SARCASM twitter_1498,SARCASM twitter_1499,NOT_SARCASM twitter_1500,NOT_SARCASM twitter_1501,NOT_SARCASM twitter_1502,NOT_SARCASM twitter_1503,NOT_SARCASM twitter_1504,SARCASM twitter_1505,NOT_SARCASM twitter_1506,SARCASM twitter_1507,SARCASM twitter_1508,SARCASM twitter_1509,SARCASM twitter_1510,SARCASM twitter_1511,SARCASM twitter_1512,SARCASM twitter_1513,SARCASM twitter_1514,SARCASM twitter_1515,NOT_SARCASM twitter_1516,SARCASM twitter_1517,NOT_SARCASM twitter_1518,SARCASM twitter_1519,NOT_SARCASM twitter_1520,SARCASM twitter_1521,SARCASM twitter_1522,SARCASM twitter_1523,SARCASM twitter_1524,NOT_SARCASM twitter_1525,SARCASM twitter_1526,NOT_SARCASM twitter_1527,SARCASM twitter_1528,SARCASM twitter_1529,SARCASM twitter_1530,SARCASM twitter_1531,NOT_SARCASM twitter_1532,SARCASM twitter_1533,SARCASM twitter_1534,SARCASM twitter_1535,SARCASM twitter_1536,NOT_SARCASM twitter_1537,SARCASM twitter_1538,SARCASM twitter_1539,SARCASM twitter_1540,SARCASM twitter_1541,SARCASM twitter_1542,SARCASM twitter_1543,NOT_SARCASM twitter_1544,SARCASM twitter_1545,NOT_SARCASM twitter_1546,SARCASM twitter_1547,SARCASM twitter_1548,NOT_SARCASM twitter_1549,SARCASM twitter_1550,SARCASM twitter_1551,SARCASM twitter_1552,SARCASM twitter_1553,SARCASM twitter_1554,SARCASM twitter_1555,NOT_SARCASM twitter_1556,SARCASM twitter_1557,SARCASM twitter_1558,SARCASM twitter_1559,SARCASM twitter_1560,SARCASM twitter_1561,SARCASM twitter_1562,SARCASM twitter_1563,NOT_SARCASM twitter_1564,SARCASM twitter_1565,SARCASM twitter_1566,NOT_SARCASM twitter_1567,SARCASM twitter_1568,NOT_SARCASM twitter_1569,NOT_SARCASM twitter_1570,SARCASM twitter_1571,SARCASM twitter_1572,SARCASM twitter_1573,NOT_SARCASM twitter_1574,NOT_SARCASM twitter_1575,NOT_SARCASM twitter_1576,SARCASM twitter_1577,SARCASM twitter_1578,SARCASM twitter_1579,SARCASM twitter_1580,NOT_SARCASM twitter_1581,SARCASM twitter_1582,NOT_SARCASM twitter_1583,NOT_SARCASM twitter_1584,NOT_SARCASM twitter_1585,SARCASM twitter_1586,SARCASM twitter_1587,NOT_SARCASM twitter_1588,SARCASM twitter_1589,NOT_SARCASM twitter_1590,NOT_SARCASM twitter_1591,SARCASM twitter_1592,NOT_SARCASM twitter_1593,NOT_SARCASM twitter_1594,SARCASM twitter_1595,NOT_SARCASM twitter_1596,SARCASM twitter_1597,SARCASM twitter_1598,NOT_SARCASM twitter_1599,SARCASM twitter_1600,SARCASM twitter_1601,SARCASM twitter_1602,NOT_SARCASM twitter_1603,NOT_SARCASM twitter_1604,SARCASM twitter_1605,SARCASM twitter_1606,SARCASM twitter_1607,SARCASM twitter_1608,NOT_SARCASM twitter_1609,SARCASM twitter_1610,SARCASM twitter_1611,SARCASM twitter_1612,NOT_SARCASM twitter_1613,SARCASM twitter_1614,NOT_SARCASM twitter_1615,SARCASM twitter_1616,SARCASM twitter_1617,SARCASM twitter_1618,NOT_SARCASM twitter_1619,NOT_SARCASM twitter_1620,SARCASM twitter_1621,SARCASM twitter_1622,NOT_SARCASM twitter_1623,NOT_SARCASM twitter_1624,NOT_SARCASM twitter_1625,SARCASM twitter_1626,SARCASM twitter_1627,SARCASM twitter_1628,NOT_SARCASM twitter_1629,SARCASM twitter_1630,NOT_SARCASM twitter_1631,NOT_SARCASM twitter_1632,SARCASM twitter_1633,SARCASM twitter_1634,SARCASM twitter_1635,SARCASM twitter_1636,SARCASM twitter_1637,NOT_SARCASM twitter_1638,SARCASM twitter_1639,SARCASM twitter_1640,SARCASM twitter_1641,NOT_SARCASM twitter_1642,SARCASM twitter_1643,SARCASM twitter_1644,SARCASM twitter_1645,SARCASM twitter_1646,SARCASM twitter_1647,NOT_SARCASM twitter_1648,NOT_SARCASM twitter_1649,SARCASM twitter_1650,NOT_SARCASM twitter_1651,NOT_SARCASM twitter_1652,SARCASM twitter_1653,SARCASM twitter_1654,SARCASM twitter_1655,SARCASM twitter_1656,SARCASM twitter_1657,SARCASM twitter_1658,NOT_SARCASM twitter_1659,SARCASM twitter_1660,SARCASM twitter_1661,NOT_SARCASM twitter_1662,SARCASM twitter_1663,NOT_SARCASM twitter_1664,SARCASM twitter_1665,SARCASM twitter_1666,SARCASM twitter_1667,NOT_SARCASM twitter_1668,SARCASM twitter_1669,NOT_SARCASM twitter_1670,SARCASM twitter_1671,SARCASM twitter_1672,SARCASM twitter_1673,NOT_SARCASM twitter_1674,SARCASM twitter_1675,SARCASM twitter_1676,SARCASM twitter_1677,SARCASM twitter_1678,NOT_SARCASM twitter_1679,SARCASM twitter_1680,SARCASM twitter_1681,NOT_SARCASM twitter_1682,SARCASM twitter_1683,SARCASM twitter_1684,SARCASM twitter_1685,SARCASM twitter_1686,SARCASM twitter_1687,SARCASM twitter_1688,NOT_SARCASM twitter_1689,NOT_SARCASM twitter_1690,NOT_SARCASM twitter_1691,SARCASM twitter_1692,SARCASM twitter_1693,SARCASM twitter_1694,NOT_SARCASM twitter_1695,NOT_SARCASM twitter_1696,SARCASM twitter_1697,SARCASM twitter_1698,NOT_SARCASM twitter_1699,SARCASM twitter_1700,NOT_SARCASM twitter_1701,SARCASM twitter_1702,SARCASM twitter_1703,SARCASM twitter_1704,NOT_SARCASM twitter_1705,SARCASM twitter_1706,SARCASM twitter_1707,NOT_SARCASM twitter_1708,SARCASM twitter_1709,NOT_SARCASM twitter_1710,SARCASM twitter_1711,SARCASM twitter_1712,NOT_SARCASM twitter_1713,SARCASM twitter_1714,NOT_SARCASM twitter_1715,NOT_SARCASM twitter_1716,SARCASM twitter_1717,NOT_SARCASM twitter_1718,NOT_SARCASM twitter_1719,NOT_SARCASM twitter_1720,SARCASM twitter_1721,SARCASM twitter_1722,NOT_SARCASM twitter_1723,SARCASM twitter_1724,SARCASM twitter_1725,NOT_SARCASM twitter_1726,SARCASM twitter_1727,NOT_SARCASM twitter_1728,SARCASM twitter_1729,NOT_SARCASM twitter_1730,SARCASM twitter_1731,SARCASM twitter_1732,SARCASM twitter_1733,SARCASM twitter_1734,NOT_SARCASM twitter_1735,SARCASM twitter_1736,SARCASM twitter_1737,SARCASM twitter_1738,SARCASM twitter_1739,NOT_SARCASM twitter_1740,SARCASM twitter_1741,SARCASM twitter_1742,NOT_SARCASM twitter_1743,NOT_SARCASM twitter_1744,SARCASM twitter_1745,SARCASM twitter_1746,SARCASM twitter_1747,SARCASM twitter_1748,SARCASM twitter_1749,SARCASM twitter_1750,NOT_SARCASM twitter_1751,SARCASM twitter_1752,SARCASM twitter_1753,NOT_SARCASM twitter_1754,SARCASM twitter_1755,NOT_SARCASM twitter_1756,SARCASM twitter_1757,NOT_SARCASM twitter_1758,SARCASM twitter_1759,SARCASM twitter_1760,SARCASM twitter_1761,SARCASM twitter_1762,SARCASM twitter_1763,NOT_SARCASM twitter_1764,NOT_SARCASM twitter_1765,SARCASM twitter_1766,SARCASM twitter_1767,NOT_SARCASM twitter_1768,SARCASM twitter_1769,SARCASM twitter_1770,SARCASM twitter_1771,SARCASM twitter_1772,SARCASM twitter_1773,NOT_SARCASM twitter_1774,SARCASM twitter_1775,NOT_SARCASM twitter_1776,SARCASM twitter_1777,NOT_SARCASM twitter_1778,SARCASM twitter_1779,NOT_SARCASM twitter_1780,SARCASM twitter_1781,NOT_SARCASM twitter_1782,SARCASM twitter_1783,SARCASM twitter_1784,SARCASM twitter_1785,SARCASM twitter_1786,SARCASM twitter_1787,NOT_SARCASM twitter_1788,SARCASM twitter_1789,NOT_SARCASM twitter_1790,SARCASM twitter_1791,NOT_SARCASM twitter_1792,SARCASM twitter_1793,NOT_SARCASM twitter_1794,SARCASM twitter_1795,SARCASM twitter_1796,NOT_SARCASM twitter_1797,SARCASM twitter_1798,NOT_SARCASM twitter_1799,NOT_SARCASM twitter_1800,NOT_SARCASM"
https://github.com/zen030/CourseProject	Proposal.pdf	BERT Sentiment Analysis to Detect Twitter Sarcasm 1. Project Team Member * Name: Zainal Hakim * NetID: zainalh2 2. Project Topic The project topic falls under the Text Classification Competition option. The main goals of this project: 1. To explore sentiment analysis using a state-of-the-art method 2. To beat the baseline score using the given training and sample datasets 3. Sentiment Classifier using BERT Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art pre- training Natural Language Processing (NLP) model developed by Google. In this project, sentiment analysis uses BERT to detect sarcasm in Twitter tweets. 4. Programming Language The project uses Python 3.8 programming language to implement software code. 5. Previous Experience with BERT I have no previous experience with BERT nor with Deep Learning. I have a little experience with the Python programming language and the Pandas library.
https://github.com/zen030/CourseProject	README.md	Hint: Download the PDF files to view it 1. Project Result Summary The project score outperforms the baseline score on the project competition Leaderboard. - The project F1 score: 0.757905138339921 - The baseline F1 score: 0.723 2. Project Team Member Name: Zainal Hakim NetID: zainalh2 3. Project Topic The project topic falls under the Text Classification Competition option. The main goals of this project: 1. To explore sentiment analysis using a state-of-the-art method 2. To beat the baseline score using the given training and sample datasets 4. Sentiment Classifier using BERT Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art pre-training Natural Language Processing (NLP) model developed by Google. In this project, sentiment analysis uses BERT to detect sarcasm in Twitter tweets. 5. Programming Language & Library The project uses: - Python 3.8 programming language to implement software code. - Huggingface library: https://huggingface.co 6. Previous Experience with BERT I have no previous experience with BERT nor with Deep Learning. I have a little experience with the Python programming language and the Pandas library. 7. Important File(s) The FINAL trained models: 1. BERT LARGE uncased model: https://drive.google.com/file/d/1EMcBXsFPqOVg4w_-Nob4ebWA0qTr9SLQ/view?usp=sharing 2. BERT Base uncased model: https://drive.google.com/file/d/1--_k6QVpRIV3HtP-PzWjm9066ebtmA8S/view?usp=sharing The hyperparameters in my experiments are: - Learning rate: 2e-5 - Batch size: 5 (considering memory size) - Epochs: 4 iterations - Epsilon: 1e-8 - Random seed value: 17 8. Demo Demo video is available: - Here https://drive.google.com/file/d/1PAmInsMvXlgkB3jZFt9qRu-SbtsoIQBJ/view?usp=sharing - or here https://www.youtube.com/watch?v=PsYn2lUWpQg 9. Challenges To train and evaluate the BERT model requires computing power: a fast CPU and a large RAM size. It needs a dedicated environment such as Google Colab. To train the large models in my experiments, it requires a Google Colab PRO, which is the paid version. It is not easy to predict the results of the experiments since BERT is one of the Deep Learning algorithms that involves many hidden parameters. We can easily overfit the model with the given parameters and text inputs. There is no easy way to explain why one parameter performs better than the other parameter. Selecting a feature from the tweet to identify the sentiment is one of the most challenging parts of the project.
https://github.com/subhasishb-coder/CourseProject	CS410_Project_Documentation.pdf	"Project: Text Classification Competition Team: Subhasish Bose (sbose4) and Soumya Kanti Dutta (skdutta2) This documentation is created during CS 410: Text Information System Final Project and it contains the details of the project. Table of Contents Introduction ............................................................................................................................... 1 The Training dataset Content ........................................................................................................................ 1 The Test dataset content ............................................................................................................................... 2 Dataset size statistics .................................................................................................................................... 2 Project Objective ........................................................................................................................................... 2 Approach and Workflow ............................................................................................................ 2 Data Preprocessing and Feature Engineering ................................................................................................ 2 Training Models............................................................................................................................................. 3 Validation of Training Data ............................................................................................................................ 3 Running Code on Test Data and Leaderboard Score ...................................................................................... 4 Contribution .................................................................................................................................................. 4 Setup and Usage Instructions..................................................................................................... 4 Software Dependencies ................................................................................................................................. 4 Setup and Usage Instructions ........................................................................................................................ 5 References .................................................................................................................................. 5 Introduction As final project for CS 410 Text Information System, we participated in Text Classification Competition to detect Twitter Sarcasm. We were given both Training and Test datasets. The Training dataset Content label: SARCASM or NOT_SARCASM response: The Tweet to be classified context: The conversation context of the response example: {""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""]} The Test dataset content id: String identifier for sample. This id is required for project submission and grading. response: The Tweet to be classified context: The conversation context of the response example: {""id"": ""twitter_1"", ""response"": ""@USER @USER @USER My 3 year old , that just finished reading Nietzsche and then asked me : \"" ayo papa why these people always trying to cancel someone on Twitter , trying to pretend like that makes them better themselves ? \"" . To which I replied \"" idk \"" , and he just \"" cuz hoes mad \"" . Im so proud . <URL>"", ""context"": [""Well now that \u2019 s problematic AF <URL>"", ""@USER @USER My 5 year old ... asked me why they are making fun of Native Americans .."", ""@USER @USER @USER I will take shit that didn't happen for $ 100"", ""@USER @USER @USER No .. he actually in the gifted program and reads on second grade level . ... and he knows Kansas City is in Missouri""]} Dataset size statistics Train Test 5000 1800 Project Objective Our project objective is to learn from the Training dataset and predict the labels of Test dataset (SARCASM or NOT_SARCASM). Approach and Workflow Data Preprocessing and Feature Engineering - First, we read the Training and Test Data of jsonl format to Pandas data frame. Function: read_jsonl_to_dataFrame - Then we applied the following data cleaning and feature engineering steps on the Training and Test Data. Function: simple_feature_engieering_and_data_cleansing 1) Combined Response and Context Tweets in the data frame both in training and test data. 2) Converted the dataset to lower case. 3) Got rid of '@USER', '<URL>', Web URL Links, Hashtags. 4) Next, we got rid of stop words. We used nltk.corpus.stopwords for this purpose. 5) We removed the emojis as well. 6) We removed all punctuations and special characters. 7) Lastly, we stripped each word to get rid of additional 'space'. - We also used sklearn.feature_extraction.text.TfidfVectorizer to incorporate additional feature engineering with the following parameters: * max_features =20000 * min_df=1 * max_df=0.5 * binary=1 * use_idf=1 * smooth_idf=1 * sublinear_tf=1 * ngram_range=(1,3) Training Models We have tried the following algorithms on training data. a) Linear SVC b) Naive Bayes c) Logistic Regression d) Random Forest e) Neural network - BERT These experimentation code can be found in code/other_model_experimentation folder. https://github.com/subhasishb-coder/CourseProject/tree/main/code/other_model_experiments To run the BERT code the following file needs to be downloaded separately - glove.twitter.27B.100d.txt needs to be downloaded for Neural Network Among these Logistic Regression provided us the best performance. So, we designed our final code with Logistic Regression. We used sklearn.linear_model.LogisticRegression, with the following parameters. * class_weight='balanced' * solver='newton-cg' * C=1 Validation of Training Data We got the following performance matrix, doing a train test split of 80/20: Classification Result for Logistic Regression precision recall f1-score support NOT_SARCASM 0.78 0.72 0.75 519 SARCASM 0.72 0.78 0.75 481 accuracy 0.75 1000 macro avg 0.75 0.75 0.75 1000 weighted avg 0.75 0.75 0.75 1000 Overall accuracy for Logistic Regression 0.75 Running Code on Test Data and Leaderboard Score Once we validated the performance of Logistic Regression on training data, we applied it on Test Dataset. Functions: write_prediction_results_in_list and final_prediction_calculation. We created answer.txt file with test dataset labels which we uploaded for grading. We were able to beat the baseline. We tried with multiple times adjusting the feature vector. Leaderboard snapshot: Contribution Data Preprocessing - Subhasish Feature Engineering - Soumya Model Training - Soumya Validation and Adjustment of feature vector - Subhasish Setup and Usage Instructions Software Dependencies * Python==3.8.3 * nltk==3.5 * pandas==1.0.5 * scikit_learn==0.23.2 Setup and Usage Instructions 1. conda create -n ""project_demo"" python=3.8.3 2. conda activate project_demo 3. git clone https://github.com/subhasishb-coder/CourseProject.git 4. cd CourseProject 5. pip install nltk==3.5 6. pip install pandas==1.0.5 7. pip install scikit_learn==0.23.2 8. cd code 9. python TestClassficationCompetion_Sarcasm_Detection.py References https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db"
https://github.com/subhasishb-coder/CourseProject	CS410_Project_Progress_Report.pdf	Project: Text Classification Competition Team: Subhasish Bose (sbose4) and Soumya Kanti Dutta (skdutta2) This report contains information on project progress as of 11/25/2020. Progress: We have completed the following steps: 1) Get train and test datasets from GitHub Repo. 2) Inspect the datasets to understand the format and relation. 3) Performed data cleaning steps on the dataset. 4) Performed train-test split on training dataset to test accuracy. 5) Tested the dataset with the following algorithms: a) Linear SVC b) Naive Bayes c) Logistic Regression d) Random Forest e) Neural network - BERT Remaining Tasks: The following tasks are pending: 1) Run the above-mentioned algorithms on Test Dataset. 2) Our software will go with the majority decision found from these 5 algorithms mentioned above. For example, if any 3/4/5 of these algorithms find the tweet as 'SARCASM' - our code with tag it as 'SARCASM'. 3) Create output in desired format. 4) Create project Report. 5) Create Software Usage tutorial presentation. 6) Code, Project Report and Presentation submission. Issues/ Challenges: As the test dataset is not labeled, it is difficult for us to judge accuracy of our code on it.
https://github.com/subhasishb-coder/CourseProject	Final Project Proposal.pdf	Final Project Proposal Document Topic: Competitions 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Subhasish Bose (Captain) - sbose4@illinois.edu Soumya Kanti Dutta - skdutta2@illinois.edu 2. Which competition do you plan to join? Text Classification competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods We are prepared to learn state-of-the-art neural network classifiers. We have heard of classifiers like CNN (Convolution Neural Network), RNN (Recurrent Neural Network) and frameworks/ libraries like Keras, Spacy, NLTK, Scikit-learn. We don't have any prior experience with these. 4. Which programming language do you plan to use? Python
https://github.com/subhasishb-coder/CourseProject	README.md	"CourseProject Introduction As final project for CS 410 Text Information System, we participated in Text Classification Competition to detect Twitter Sarcasm. We were given both Training and Test datasets. The Training dataset Content label: SARCASM or NOT_SARCASM response: The Tweet to be classified context: The conversation context of the response example: {""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""]} The Test dataset content id: String identifier for sample. This id is required for project submission and grading. response: The Tweet to be classified context: The conversation context of the response example: {""id"": ""twitter_1"", ""response"": ""@USER @USER @USER My 3 year old , that just finished reading Nietzsche and then asked me : \"" ayo papa why these people always trying to cancel someone on Twitter , trying to pretend like that makes them better themselves ? \"" . To which I replied \"" idk \"" , and he just \"" cuz hoes mad \"" . Im so proud . "", ""context"": [""Well now that \u2019 s problematic AF "", ""@USER @USER My 5 year old ... asked me why they are making fun of Native Americans .."", ""@USER @USER @USER I will take shit that didn't happen for $ 100"", ""@USER @USER @USER No .. he actually in the gifted program and reads on second grade level . ... and he knows Kansas City is in Missouri""]} Dataset size statistics Train Test 5000 1800 Project Objective Our project objective is to learn from the Training dataset and predict the labels of Test dataset (SARCASM or NOT_SARCASM). Setup and Usage Instructions Software Dependencies: 1) Python==3.8.3 2) nltk==3.5 3) pandas==1.0.5 4) scikit_learn==0.23.2 Setup and Usage Instructions: 1) conda create -n ""project_demo"" python=3.8.3 2) conda activate project_demo 3) git clone https://github.com/subhasishb-coder/CourseProject.git 4) cd CourseProject 5) pip install nltk==3.5 6) pip install pandas==1.0.5 7) pip install scikit_learn==0.23.2 8) cd code 9) python TestClassficationCompetion_Sarcasm_Detection.py Software Usage Tutorial Link: https://mediaspace.illinois.edu/media/t/1_xwb0wmzt"
https://github.com/soumya834-msit/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jasonzhang929/CourseProject	progress_report.pdf	Progress report Siyuan Zhang (siyuan3) 1. Task completed: For this project, I have already downloaded the dataset, wrote a couple of manually engineered features and trained basic models( linear regression, logistic regression, SVM) using these features as inputs on the training dataset. 2. Pending tasks: The performance not yet reaching the baseline but close. I'm planning on adding more features and using more advanced models such as deep neural networks for classification. Im confident that I will be able to outperform the baseline by a fair amount of margins. 3. Challenges: Not really at this point, everything is ok and progress being made matches my plan.
https://github.com/jasonzhang929/CourseProject	project proposal.pdf	Project Proposal 1 Names: Siyuan Zhang siyuan3 2 Competition: Text classification 3 Detail I'm planning on trying a mixture of models for this text. This includes classical models such as regression models and SVMs to train a set of engineered features. I'm also planning on using more advanced models such as deep neural networks to achieve better performance. 4. Language Python3
https://github.com/jasonzhang929/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/jasonzhang929/CourseProject	Siyuan_project_documentation.pdf	Text Classification Competition - Twitter Sarcasm Detection CS410 Fall 2020 Siyuan Zhang(siyuan3@illinois.edu) Introduction The goal of our project is to design implement and test models that can classify the existence of sarcasm in twitter messages. Since this is a competition that focuses on the performance of the model we built, there is a baseline requirement of a f1 score greater than 0.723. Proposed Methods Our methods mainly focusing on using machine learning techniques since the problem we try to tackle here is mainly a text classification problem. A large part of applied machine learning is about feature engineering so a large part of our project is about discovering and experimenting different feature extraction methodologies. After the features are generated from the dataset, we then deployed different machine learning algorithms to train and classify the data set provided. We compare the performance of all the classifiers we experimented. Preprocessing We implemented couple standard text preprocessing functions to clean our data. Methods we used includes special character processing, upper/lower case process, punctuation signs, stemming and lemmatization, as well as stop words. We apply these data cleaning process before the data is fed into the classifiers for training. However, later we determined that such data cleaning actually degraded the test time performance by couple percent so we took this part out in the final version of our testing code. The reason for this is that we suspect in twitter message there are lots of special words and characters that actually play an important role in expressing the message and data cleaning takes a lot of these important information out which makes it harder to classify the sarcasm in the messages. Feature Engineering We considered different features to use in this project. Since the context data is provided in the dataset in addition to the responses, we leverage both to extract useful features. We considered word count vectors, TF-IDF vectors, word embeddings and NLP features for this task. Among those features, we found the TF-IDF vectors to be great features that can achieve good classification performance on the test set so it was selected as out feature. We extract TF-IDF weights from both the response string as well as the concatenated context string across the data. We adopted these weights for both the unigram model and the bigram model. Machine learning classifiers The classifiers we considered here includes support vector machines, random forests, Boosting from the classical machine learning techniques as well as multilayer perception model from the deep learning models. For all models we believe since they were used extensively in many applications and were able to achieve good performance, they should deliver promising results on the task as well. For each of the models, we performed random searches and grid searches on all the model hyper-parameters to select the best parameters for the actual training and testing process. Implementation Details Thanks to the extensive amount of research and development done in this area in the past decades, we were able to make good use of many standard packages and libraries to achieve a lot of functionalities that we require in this project. We mainly used pandas data frame to load the dataset for easier processing. The TF-IDF vectors are extracted using TfidfVectorizer from the scikit learn library. We set it to use both the unigram and bigram from the data and extracts features from both the response and the context column. After both vectors are calculated, we concatenate them for each data point and use as the features for the machine learning classifiers. We also leveraged different classifiers mainly from the eras, sklearn and xgboost library. There are very well written versions of these algorithms in these libraries and we use them to train and test these classsifiers. After picking the best model, we then producing the test time output using the Test dataset as input and saved the result into the Answer text file for benchmarking. Experiments Detail Since the test set is reserved for grading, we split the train set into a 4 1 split and leave 20% of our training data as validation set during out model selection and tuning process. We first perform random search on hyper parameters of each of out models to narrow down the search range of the parameters. Then we performed grid search over the parameters to determine the best performing hyper parameters for all the models. Afterwards, all models are trained on the train set and their accuracy were compared to determine the best model. Although some model tested very well during the training phase, their test time performance is not as good due to potential overfitting so we had to do models selection manually by uploading to GitHub and compare the test set performance on the livedatalab server. Below is the detailed performance of all out models on different metrics. From the experiment data above, we can see that the random forest classifier not only have a very high training time performance, but also a very good test time f1 score and Training Accuracy Test Precision Test Recall Test F1 SVM 0.8272 0.658 0.688 0.6725 Random forest 0.9998 0.615 0.894 0.7288 Boosting 0.9216 0.621 0.718 0.6656 Multilayer Percep 0.9182 0.520 0.923 0.6612 is above the 0.723 baseline f1 performance. Other 3 models especially the boosting and the multilayer perception model perform well during training time put didn't achieve similar performance on the test and the reason can be potential overfitting. The final parameter we used on the random forest is 1000 estimator with a decision threshold of 0.465. Conclusion In this project we experimented many techniques on text classification including feature engineering and machine learning classifiers. The final performance of our best model using TF-IDF feature vectors combined with random forest classifier together is able to achieve a good F1 score of 0.7288 which is beyond the baseline. We are happy with the result and this project has been an amazing learning experience for me. Software Used Installation guide The following software packages needs to be installed in order to run our code: python pandas nltk tensorflow keras numpy scipy scikit_learn xgboost These dependencies can be downloaded and installed automatically by first cloning our repo to local and then run the following line in the terminal: Install -r requirements.txt Our code is located in the sarcasm_detection.py, simply run the code and generated prediction result of the test set will be saved in Answer.txt All of out code is fully documented, please refer to our code for more implementation details. Reference https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to- understand-and-implement-text-classification-in-python/ https://www.mfz.es/machine-learning/an-end-to-end-machine-learning-project- part-i-text-classification-in-python/
https://github.com/jasonzhang929/CourseProject	Siyuan_project_presentation.pptx	Siyuan Zhang(siyuan3@illinois.edu) CS410 Project Text Classification Competition Team Siyuan Text Classification Twitter Sarcasm Detection Our approach Text classification Preprocessing Feature Engineering Machine Learning Techniques Experiment Results How to Install requirements In command line: git clone https://github.com/jasonzhang929/CourseProject.git Install -r requirements.txt How to run our code All of our code are located at sarcasm_detection.py # define svm model svm = svm.SVC(random_state=8, kernel='linear', C=0.1, probability=True) # define random forest model rf_model = RandomForestClassifier(n_estimators=1000, random_state=17, warm_start=True, verbose=1) # specify which model to use model = rf_model # run svm model # run_svm() # run mlp model # run_MLP() # run xgboost boosting model # run_boost() # run random forest or svm model here run_model(model) Code Demo Thank you!
https://github.com/varunhari2020/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/davidmg4/CourseProject	CS410_FA20_Final Project Proposal_David_Gutierrez.docx	David Gutierrez CS410: Text Information Systems Final Project Proposal Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Individual team Name: David Gutierrez NetID: davidmg4 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Sentiment analysis based on geography in the United States using Twitter. This tweet-based text analysis will break down each geographic region (50 states) into a sentiment analysis based on sub-topics as government (e.g. corruption), weather (e.g. natural disasters), quality of life (e.g. cost of living), pollution (e.g. air quality), lifestyle (e.g. traffic) to help people make informed decisions about where to live without the potential biases and pitfalls present in survey data. I expect that this will line up with publicly available information, but may present some interesting challenges inherent to social media such as the use of irony and sarcasm. I plan to use Twitter's built-in API and library along with the NLTK library to train the classifier and implement the analysis. Once this data is collected, I plan to use numpy's libraries to perform a statistical analysis to measure the significance of each sentiment and create a threshold value above which the positive or negative value will be considered significant on a per-state basis. Then they will be compared against other states to generate an interactive grid with which user will be able to sort by state and sub-topic as well as apply filters to narrow down to only specific states that they care about Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Download Twitter tweet training database/information retrieval via web-scraping: 5 hours Develop Codebase to parse both geography and sub-topic information: 10 hours Analysis of data : 5 hours Presentation in web-based intuitive and interactive data visualization: 5 hours
https://github.com/davidmg4/CourseProject	CS410_FA20_Final Project Proposal_David_Gutierrez.pdf	David Gutierrez CS410: Text Information Systems Final Project Proposal Fall 2020 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Individual team i. Name: David Gutierrez ii. NetID: davidmg4 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? a. Topic: Sentiment analysis based on geography in the United States using Twitter. This tweet-based text analysis will break down each geographic region (50 states) into a sentiment analysis based on sub-topics as government (e.g. corruption), weather (e.g. natural disasters), quality of life (e.g. cost of living), pollution (e.g. air quality), lifestyle (e.g. traffic) to help people make informed decisions about where to live without the potential biases and pitfalls present in survey data. I expect that this will line up with publicly available information, but may present some interesting challenges inherent to social media such as the use of irony and sarcasm. I plan to use Twitter's built-in API and library along with the NLTK library to train the classifier and implement the analysis. Once this data is collected, I plan to use numpy's libraries to perform a statistical analysis to measure the significance of each sentiment and create a threshold value above which the positive or negative value will be considered significant on a per-state basis. Then they will be compared against other states to generate an interactive grid with which user will be able to sort by state and sub-topic as well as apply filters to narrow down to only specific states that they care about 3. Which programming language do you plan to use? a. Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. a. Download Twitter tweet training database/information retrieval via web-scraping: 5 hours b. Develop Codebase to parse both geography and sub-topic information: 10 hours c. Analysis of data : 5 hours d. Presentation in web-based intuitive and interactive data visualization: 5 hours
https://github.com/davidmg4/CourseProject	David Gutierrez - Progress Report.pdf	David Gutierrez CS410: Text Information Systems Fall 2020 November 29, 2020 Course Project Progress Report Individual team (NetID: davidmg4) 1) Progress made thus far: I have successfully applied and been approved for a Twitter developer account. This has enabled me to pull all of the most recent tweets in a search for the previous ten days (instead of the prior year as I had hoped). I have been able to implement code to pull the text data into a CSV file. The resultant data file is then read into a second script that loads it into Pandas data frames and parses the text data using stop-words from NLTK and pulling a sentiment analysis for each tweet using TextBlob. Zero values are dropped and then the remaining sentiment scores are averaged together by state and topic to produce the final values, visualized as percentages. This data is then dropped into an HTML file that features a map of US states as well as a sortable table for each state in addition to a composite score to gauge overall sentiment across the five topics. 2) Remaining tasks: a. Re-run the Twitter scraper to get the full text of tweets (currently truncated at 140 characters) b. Fully implement dataViewer.html to pull data automatically from the CSV file c. Rework the CSS on the same to enable a visual histogram for each topic when hovering over the map for a particular state and color enhancements for numerical values d. Code cleanup and integration into a single workflow 3) Any challenges/issues being faced: a. Time series data limited to prior 7-10 days b. API limited to 15-minute intervals (takes all day to collect five topic-words for all 50 states) c. Spam/bot usage is rampant on Twitter d. Overlap in states/topics (including a gross over-weighting of political data due to residual election data) e. Location data also severely limited and often inaccurate f. Limited data for less populated states g. Some topic words are insufficiently measurable (quality of life) or inherently biased (e.g. pollution)
https://github.com/davidmg4/CourseProject	DocumentationPresentation.pptx	"Sentiment Analysis of American States by Topic Natural Language Processing of Tweets from the Last 7 Days Using Python David Gutierrez University of Illinois at Urbana-Champaign CS410: Text Information Systems Overview Individual team (NetID: davidmg4) Goal: Conduct a working sentiment analysis based on ""Quality of Life"" topics for all 50 states using Twitter data to get a nearly-real time view of each state's qualities from real users. Topics chosen: Government Weather Economy Nature Lifestyle GitHub repo: https://github.com/davidmg4/CourseProject Workflow Apply for Twitter developer account Install and implement Tweepy API for scraping Tweet data in Python Write each to a CSV file with state and topic tags Clean tweets to prepare for Natural Language Processing (remove non-text data, stop words, etc.) Conduct analysis using Pandas Dataframe data structures and TextBlob's sentiment analysis algorithm Include mean for each category and average them an 'overall score' for view in summary CSV file as well as an interactive HTML file Resources Tweepy (Twitter API) Pandas (Dataframes) NLTK (Stopwords) Textblob (Sentiment Analysis) JSON/JQuery/Ajax (Web Features) Bootstrap (Web Formatting) Step 1: Setup Step 2: Tweet Scraping Step 3: NLP and Analysis Step 3: NLP and Analysis (cont'd) Results Results Good template for folks who are indecisive about where they might want to move or travel Allows for ranking and filtering of resultant data Novel use of NLP for practical data and a ""finger on the pulse"" of social media users More context information would be useful, but still robust enough for most users Limitations Most limitations of this project are specific to Twitter, subject to further study: Free/educational developer account limited to tweet data from the last 7 to 10 days Tweepy API rate limits cause lengthy delays in scraping time Tweets by and large not geotagged for a specific location so including the state name was next-best-option Sentiments only make sense on a relative scale - subject to fluctuation by type of users in the system as well as data availability limitations (e.g. not many tweets in/about North Dakota) Ideal case is to have a real time stream of Twitter data analysis uploaded to the web with histogram data Conclusion Demonstrates a functional workflow for taking topic and context data to generate non-text data Provides useable data interaction to compare and rank states based on topic sentiment as well as overall score Easily adapted for other topics and/or geographies Potential for even more data visualization and interactivity in the future Lessons Learned Many readily available libraries and packages for NLP in Python Robust tools for reading and writing to CSV files Ability to craft and aggregate HTML code/documents from simple Python scripts Analysis only limited by quality of data (and processing time) Had a lot of fun!"
https://github.com/davidmg4/CourseProject	README.md	CS410 Course Project Documentation File Sentiment Analysis of American States by Topic by David Gutierrez Individual team (NetID: davidmg4) Natural Language Processing of Tweets from the Last 7 Days Using Python CS410: Text Information Systems University of Illinois at Urbana-Champaign Video Link to Walkthrough: https://youtu.be/3uJ9P5MayGI Note: private API keys for Tweepy Twitter API withheld. If you would like to run this code on your own machine, please email me at @illinois.edu
https://github.com/samvalenp/CourseProject	project_documentation.pdf	Project Documentation | Classification Competition This is an individual project. Samuel Valenzuela (samuelv4@illinois.edu) I followed two different approaches for the classification competition. One was using BERT as an embedding layer and the other was to train BERT and use it as a classifier. The second attempt was the one that performed better than the baseline and the one I document more in detail in the notebook with a clear structure. I also provide the notebook for the first approach but is not very well documented or easy to follow as I tried many different things and the code is a bit of a mess. To run the models I recommend to use Google Colab. I always used Google Colab for this project as it runs much faster than on my laptop. It also avoids having to install libraries or dealing with python environments. First approach: sam_disbert.ipynb Second approach: sarcasm_transformers.ipynb BERT as embedding layer (sam_disbert.ipynb) My first idea to solve the classification problem was to use a pre-trained model as an embedding layer that turns the text into a vector of numbers that can be used as features for a Machine Learning model. I use BERT to get high quality features out of the data. Because BERT has been already pre-trained with massive general datasets, it is useful for using it as an embedding layer that takes the tweets and outputs a vector of numbers that can be used as features. Here I list the different variations I tried: * BERT base and DistilBERT. I tried both models for the embedding layer. DistilBERT worked better. * Logistic Regression as ML model. It worked very well obtaining a 0.69 f1. * Linear SVC as ML model. It obtained around 0.66 f1. * Random Forest as ML model. It worked the best with a 0.712 f1. Very close to the baseline. * Convolutional Neural Networks. I tried many different networks with different number of layers and neurons. It worked also well but didn't get a higher f1 than Random Forest. * Added a subset of data from Ghosh dataset for training. This dataset has over 50,000 tweets. This didn't help at all and the f1 was lower. Maybe the type of sarcasm is different in some way in each dataset. DistilBERT ML model Input text feature vector Sarcasm/ not sarcasm Fine tune transformers (sarcasm_transformers.ipynb) My second attempt was about actually training pre-trained models and using it as a classifier instead of an embedding layer. Here I only use the transformer and not an additional ML model. I tried different transformers provided from the SimpleTransformers library including BERT, DistilBERT, XLM, XLNet and RoBERTa. Most of them performed better than the baseline. The one that showed better and more consistent results is RoBERTa with 0.74 f1 score.
https://github.com/samvalenp/CourseProject	project_report.pdf	Project Progress Report | Classification Competition Which tasks have been completed? * Research about the problem: I have completed a research about NLP and classification problems. I am not new to Machine Learning but I am new to NLP, so I had to do a deep research on the current technologies used for classification problems and for problems similar to tweets classification and sarcasm detection. * Find possible solutions: I research articles, githubs and papers on models that solve similar problems to the classification competition. I deeply went through three or four that I could base my models from. * Implement models: I implemented some models based on the ones I found during my research. I applied them to the classification problem and found a decent result on the train data partitioned in train and test. Which tasks are pending? * Surpass the baseline in ranking: my F1 score is 0.678 and the baseline is 0.723. Are you facing any challenges? * Overwhelming start: I already went through the challenge of starting and ramping up. At the begining I had very little experience on using classification models for NLP. To overcome this challenge I did an extensive research on the current state of the art and I learnt in detail some of the most popular algorithms. * Surpass the baseline in ranking: my F1 score is 0.678 and the baseline is 0.723.
https://github.com/samvalenp/CourseProject	README.md	CourseProject Project proposal Members: samuelv4 I would like to join the text classification competition. I am prepared to learn state-of-the-art neural network classifiers. I have worked in a NN competition before and I would love to do it again. I haven't worked a lot on text classification but I have worked with LSTM networks which might help me to ramp up and learn the state-of-the-art for the competition. I am planning on using python and known libraries like TensorFlow and Keras.
https://github.com/mikhaidn/CourseProject	Proposal.md	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I am the only contributor for this project, my netID isdmikha2 What system have you chosen? What function are you adding? How will the new function benefit the users? I have chosen to expand the EducationalWeb system. I've chosen to add the ability to compare university courses (or entire degree programs between schools) by extracting key terms from a universities graduation requirements and course syllabi.This allows students to understand whether a concept they're trying to learn is in fact a core/fringe part of their class (as compared to other courses) For example, unlike UIUC, Rose-Hulman's Multivariate Calculus course doesn't cover certain vector calculus concepts (Green's theorem, etc). They're dropped because of the shorter term (quarter system vs semester), in general this allows for more flexible curricula, majors that require vector calculus knowledge will still get it elsewhere. Regardless, there are a few questions that could be answeredL: what is the minimal set of key terms that should be covered in order for a course to be called ""multivariate calculus""? What other interesting patterns could come up when comparing course syllabi? What are the key terms that a B.S. Computer Scientist from UIUC should know? How will you demonstrate that the new function works as expected? I will demonstrate that the new functionality works by producing histograms to vizualize the word vectors of each syllabus. How will your code communicate with or utilize the system? Unsure at this time, this may just be a standalone package. Which programming language do you plan to use? python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 5 hours - get used to the old system 5 hours - generate sample data 5 hours - generate keywords for each syllabus and present them 5 hours - presentation and fine tuning"
https://github.com/mikhaidn/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/everbrightw/CourseProject	cs410_progress_report.pdf	CS410 Progress report Netids: td2 (Tianli Ding) yimengh2 (Yimeng Han) yusenw2 (Yusen Wang) Progress made: We were setting up the environment to make EducationalWeb work on our end and reading the source code and project structures. We have also proposed a file to parse a whole pdf file into single files. We are planning to write a scraping script to automatically download slides from several course websites and auto integrate those slides with the current system. Remaining task: We are planning to use beautifulsoup and simple URL requests to auto download slides from several UIUC courses' websites to scale up the current system Challenges faced: This is a huge system with lots of folders and files. Therefore we still need more time to understand and make more improvements.
https://github.com/everbrightw/CourseProject	final_project_documentation.pdf	CS411 report By Yusen Wang, Yimeng Han, Tianli Ding Video Introduction: https://youtu.be/rsyHiEcATLI 1) An overview of the function of the code (i.e., what it does and what it can be used for). First, our implementation has a general crawler that can automatically scrape lecture slides in different courses from the UIUC CS courses platform (https://courses.grainger.illinois.edu). We were scraping all links from course websites and filtering them by finding potential links that could lead us to a lecture slide. Then, we split the slides crawled from those websites and split them into 1 page pdfs stored in folders corresponding to course names. After that, we implement a Jaccard similarity function to select related slides of each page, record the result in a csv file. Then based on the relationships of the slides, we render the final result on the website. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. In crawling/scraper.py & crawling/utils.py: For the crawler, we choose the https://courses.grainger.illinois.edu as our start page to scrape our courses' url that are listed on this website. For this final project, we are only scraping courses that have 'CS' as a prefix. After we have all the course urls, we will iterate all of them and step into every course website to find where could the lecture slides/notes be located using keywords(['slides', 'slide', 'lecture','lectures','note','notes','resources','resource']). Then we start to scrape all the tags <a href=true and determine whether the links are a potential lecture slide by simply checking whether the url ends with .pdf and .pptx (since we have already filtered those links with the keyword, this constraint could very likely be the lecture slides that we want to scrape). And we parse those links to a formal format and download them into the corresponding folder. All the tasks are automated and we can expect to see a system with a large scale of lecture slides using this crawler. In pdf.js/static/getRelatedFiles.py: We first extract text in pdf pages, tokenize the content and select key words after eliminating punctuations, stopwords. Then compare Jaccard similarities of the keywords from one pdf with all other pdfs, when the similarity is greater than 0.3, we mark these two pdfs as related. Furthermore, we limit the size of the related slides as 12, so that when rendering them on the website, the list won't be too long. Then, we record them in ranking.csv after changing the format according to the original format in the platform EducationalWeb; and record all slide names in slide_names.txt. In pdf.js/parsePDF.py: We put the results of the scraped folders under raw_slides. We filtered out those corrupted slides.Then we first followed the naming convention to change the course and slides name, then split each of the slides each long pdf into a folder containing single slides for rendering. In model.py: We modified the model.py file so that other than cs410, other courses can also be rendered in the webpage. Moreover, we changed the path related_slides_path, slides_path,so that the related files are derived from our own algorithm and include all slides from different courses. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. The following instructions have been tested with Python2.7 on Linux and MacOS 1. You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html 2. Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ 3. Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static =========== NEWLY ADDED FEATURES ========= 4. Run python scraper.py from CourseProject/crawling/ to scrape lecture slides from the website 5. Then run python parsePDF under EducationalWeb/pdf.js to normalize the slides name and save one PDF into a folder with single slides. 6. Run python getRelatedFiles.py in EducationalWeb/pdf.js/static to get every single slide's related slides with ranking scores 7. From EducationalWeb/pdf.js/build/generic/web, run the following command: gulp server 8. In another terminal window, run python app.py from EducationalWeb/ 9. The site should be available at http://localhost:8096/ 4) Brief description of contribution of each team member in case of a multi-person team. We reviewed the original code on the EducationalWeb, analyzed the structures and functions in their repository, then ran code based on the instructions together. Then we had meetings discussing the structures and functions that we aim to implement. Yusen is mainly responsible for implementing the crawling part, Tianli normalizes the slides name and saves one PDF into a folder with single slides, Yimeng takes charge of implementing the 'related slides' algorithm. After one finished his/her part, other team members did code review and added comments for improvements. We wrote documentations and recorded demo videos together.
https://github.com/everbrightw/CourseProject	README.md	EducationalWeb video introduction https://youtu.be/rsyHiEcATLI how to run The following instructions have been tested with Python2.7 on Linux and MacOS You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ Run python scraper.py from CourseProject/crawling/ to scrape lecture slides from the website Then run python parsePDF under EducationalWeb/pdf.js/ to normalize the slides name and save one PDF into a folder with single slides. Run python getRelatedFiles.py in EducationalWeb/pdf.js/static to get every single slide's related slides with ranking scores From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/
https://github.com/Ludy11xc/CS-410-CourseProject	CS 410 Progress Report.pdf	"CS 410 Progress Report 11/29/20 ludy2 Topic: Reproducing a paper- Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Progress Made: All progress thus far has been related to planning and researching. Collection of data from NY Times articles for evaluating the proposed algorithm has been completed. I have also done some research and thinking on how I will go about implementing the proposed algorithms (PLSA implemented based on the Lemur information retrieval toolkit) to run on the data. Remaining Tasks: Implement the proposed algorithms in python, run code on dataset with and without guidance from the time series, as well as with tweaked parameters (number of topics and strength of the prior), evaluate and analyze results and compare with results from the published paper. To do the data analysis, I will evaluate performance using Grainger tests. I will then be able to determine if I have successfully reproduced the results found in the paper, that ""ITMTF finds topics that are both more pure and more highly correlated with the external time series than typical topic modeling, especially with a strong feedback loop."""
https://github.com/Ludy11xc/CS-410-CourseProject	CS 410 Project Proposal.pdf	Adam Ludy Ludy2 CS 410 Project Proposal Reproducing a paper: Causal topic modeling 1. I am working as an individual. My name is Adam Ludy, my netid is ludy2. 2. I chose the paper about casual topic modeling. Mining causal topics in text data. 3. I plan on using python. 4. I believe I will be able to obtain the dataset used in the paper. I can access the NY Times articles from 2000 from these links: https://spiderbites.nytimes.com/2000/, https://spiderbites.nytimes.com/2001/ 5. N/A 6. N/A
https://github.com/Ludy11xc/CS-410-CourseProject	README.md	"CS-410-CourseProject Presentation https://mediaspace.illinois.edu/media/1_8aiq79tk Objective Reproducing results from a paper: ""Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback"" Problems Data: The first problem encountered was obtaining the data in order used to produce the results. Unfortunately, I missed the piazza post to request access to the NY times dataset. So, I wrote a webscraper (data_scraper.py) to scrape articles from NY Times, which I was able to save in the data folder. I only saved results which contained ""bush"" or ""gore"" so it was a manageable size to be uploaded. For the Market data, I manually copied the data into a csv. Scope: The second problem encountered was the scope of the problem. After reading through the paper and doing a couple of hours of research, I discovered this would be a very complex result to produce, especially working as an individual, and factoring in my own gaps of knowledge involving the algorithm to be implemented. I was unable to completely recreate the algorithm using time stamp series feedback. Results I used an LDA model to find the top 3 words from topics discovered from the relevant documents. These are the results. | Top 3 Words in Significant Topics | | ---------- | | party nader vote | | tax plan social | | oil price juniper | | street music sunday | | company court death | | debate right candidate | | city game old | | clinton cheney know | | school test student | | clinton lazio mr_clinton | Looking at these words, we can definitely see some topics that were very relevant to the presidential election. Many of these words were also mined from the algorithm used in the paper. Setup/Run Code * If desired, set up virtual env. Then, to set up the code: * ``` Clone repo, could take a couple seconds-minutes depending on download speed because of data (20-30 MB) git clone https://github.com/Ludy11xc/CS-410-CourseProject.git Navigate into repo cd CS-410-CourseProject Install dependencies. If EnvironmentError is encountered, rerun with --user pip install -r requirements.txt ``` If you would like to run the websraper, you can with python data_scrapper.py However, know that it will most likely take multiple hours to complete, and the data has already been scraped and is present in the data folder. Then to run the code, ``` Run this to get results from current model python lda.py OR, run this to train a new model and get new results python lda.py train ```"
https://github.com/hernang2/CourseProject	Documentation.pdf	"Documentation Overview This crawler is used to mine faculty university faculties and generate a ""json"" file with an entry per professor. This entry will have the following fields: faculty, url, location, email, name, top_terms and bio. The purpose of this was to feed the Expert Search app with structured text that had additional information but couldn't fit the data to work with the MeTA corpus types. It can be used to gather info on professors, including the top five terms related to their bios. Implementation The code borrows from MP2.1 to crawl through faculties' professor profiles. We give an array of faculty home pages and from there the script tries to infer what links to professor profiles we have in it. Then using nltk's word_tokenize we determine the words with the highest frequencies in the bios text, by counting frequency of words and normalizing by max frequency. After that we use a ""heuristic"" with the aid of BeautifulSoup and regex to try to gather the email, facultie and name of the professor. I couldn't implement location as I didn't setup the maps api that was used in the Expert Search's get_location.py script. At the end, it dumps a json with the faculties data in a file called ""bios_json.txt"" under the sample folder, alongside the main.py Usage We assume a machine running python 3.5 using the pip installer The script that needs to run is main To run this script you need to install the following packages: * bs4: pip install beautifulsoup4 * selenium: pip install -U selenium * nltk: pip install nltk For selenium we assume you're testing with the firefox dirver. If not, get the driver for your browser and use it in the constructor on main.py line 181. Some browsers might get tricky. To specify which university faculties you want to crawl, modify dir_url in main.py line 185"
https://github.com/hernang2/CourseProject	ProjectProgressReport.pdf	ExpertSearch - Extracting relevant information As stated by the project topics document, the goal of this project is to enhance the ExpertSearch app (https://github.com/CS410Fall2020/ExpertSearch/) to convert the unstructured text in faculty webpages into more structured text, so that we can more accurately extract email and faculty names from the faculty member bios crawled data. Progress -Setup a vm running a Linux distro and set it up with Python 2.7 -Get code from git repo -Run up with gunicorn and necessary packages on localhost -Started modifying the code around the ranker for the structured text update Remaining tasks -Determine best text structure -Update results structure in server.py's search method to use this new structure -Push code to this repo -Host website on the cloud Challenges -This App is made to run using Python 2.7 on Linux. Had some issues setting up the vm I'm using to work on and getting sure python, pip, gunicorn and other packages were installed and running using the correct version -I can see potential issues related to python 2.7 when deploying to the cloud, but Python 2.7 still has support until next year, enough to deliver the project.
https://github.com/hernang2/CourseProject	README.md	ExpertSearch - Extracting relevant information This is the final project for the UIUC Text Information Systems course. For a visual walkthrough you can use the following links: You can find the video about how to instal and run: https://mediaspace.illinois.edu/media/1_sktdzqnu You can find the video about how the code works: https://mediaspace.illinois.edu/media/1_ht2q1bdw To view presentation go to: https://1drv.ms/u/s!AsAuk2iSocrzkKROEBv6Q6XLC0_Rbw?e=8fbitD # Overview The puropose of this project is to be able to crawl through faculty pages and gather info on professors that gets stored in a structured data format. This structured that would have the fields that the Expert Search system currently has, like email, name and faculty, but additionally I'm trying to gather top terms that can be used to give a better idea of the bio's expertise. # Implementation To gather the top terms I used the nltk library to tokenize the professor's bio. Then I calculate the maximum frequency and normalize the counts of all the terms using this figure. After giltering for stopwords and single characters, I order by frequency and take the first five elements. bio = visible_text.strip() stopwords = nltk.corpus.stopwords.words('english') word_frequencies = {} for word in nltk.word_tokenize(bio): if word not in stopwords: if word not in word_frequencies.keys(): word_frequencies[word] = 1 else: word_frequencies[word] += 1 max_frequency = max(word_frequencies.values()) for word in word_frequencies.keys(): word_frequencies[word] = (word_frequencies[word] / max_frequency) word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True) top_terms = list(k[0] for k in word_frequencies if len(k[0]) > 1)[:5] To generate the json, I crawled through faculty pages from MP2.1's sheet looking for hyperlinks. If the hyperlink has content that resembles a name and a valid href it starts the mining. It asumes the name is nearby. It browses to the url, gathers all non-html text and does a little format as the bio. It looks for some types of tags and looks top see if they match an email regex, or departments after tokenizing and filtering for small sentences that include at least one word related to the description of a department. After that it collects everything in a json (for this code look at Crawler/sample/main.py line 113) # Limitations The professor gathering algorithm is a hit and miss, because it has many false positives, like Student Info, Academic Resources, etc. I didn't compare it against a database of common names (and I guess someone could be named Academic Resources). Maybe main failure was connecting it with the Expert Search App, as it depends of some certain valid formats that the corpus that gets fed to MeTApy. I couldn't make it work of Json, or strip MeTApy entirely an replce the ranking code with a different library, like nltk.
https://github.com/reckoner-david/CourseProject	Project Progress Report.pdf	Project Progress Report Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu Progress: I have made two submissions for my challenge (username: reckoner) and have achieved a best rank of 8 (with only 3 epochs and small sequence lengths). A majority of the time till now was spent on getting the framework and evaluation pipelines ready. The remaining time will be spent on training larger models with better parameter searching. Task Status Comments/Challenge Dataset Preparation Done We have to create dataset in a format so that we can try a variety of problem formulations Framework Done Model Supported: Bert, Distilbert and RoBerta Models. Task Supported: Binary Classification Sentence Pair Classification Method 1: (distilbert-base-uncased, Only Response, no pre-processing) Done Got Rank 20 and F1_Score:0.73 Method 2: (distilbert-base-uncased,Response+Context, no pre-processing) Done Got Rank 8 and F1_Score:0.756 Hyper Parameter Searching In Progress Due to GPU resource constraints we have not run this step. Next Step: * Add pre-processing. * Try on larger models (Roberta-Large, bert-large) * Do hyper parameter searching Challenges The challenge with deep learning models is the resource. Currently we have only used lighter models and smaller parameters of sequence length and epochs to save compute resources. For the most competitive baselines we will have to do a hyperparameter grid search which is costly and the free google codelab resources might not be enough to run them. In that case we will have to spend some time in streamlining our pipeline and implementing early stopping metrics. Currently it takes ~1:50 min to train 1 epoch of the model.
https://github.com/reckoner-david/CourseProject	Project Report.pdf	Project Report Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu Work Done: I have made 11 submissions for my challenge (username: reckoner) and have achieved a best rank of 8. After sometime I started investigating models with lower parameters which still allow me to beat the baseline. The current leaderboard results are with a distilbert model, Epoch-8, 'max_seq_length': 256, lr: 3e-5. That model is also loaded into `demo.ipnyb`  and can be loaded and run. Task Status Comments/Challenge Dataset Preparation Done We have to create dataset in a format so that we can try a variety of problem formulations Framework Done Model Supported: Bert, Distilbert and RoBerta Models. Task Supported: Binary Classification Sentence Pair Classification Method 1:  (distilbert-base-uncased, Only Response, no pre-processing) Done Got Rank 20  and F1_Score:0.73 Method 2: (distilbert-base-uncased,Response+Context, no pre-processing) Done Got Rank 8  and F1_Score:0.756 Hyper Parameter Searching Done Used wandb hyper-parameter tuning on lr. Sequence size manually tested. Model We used a transfer learning package called SimpleTransformers. This package supports creating and training huggingface transformer models and provides necessary abstractions to make the process faster. We make use of https://simpletransformers.ai/docs/sentence-pair-classification/ module of the library. This module trains a transformer model to predict over a pair of sentences. The idea is to use the response of the tweet and the context as the pair of sentences. i.e. text_a , text_b => Sarcasm/Not Sarcasm Here text_a is response text_b is concatenation of (context 2 and context 1) For testing parameters I used a 80:20 split. Final training was done on all the data points. Other Methods Tried: * I also tried a simple classification model(within simpletransformers library) with only the response. It gave me F1_Score:0.73 * I also tried Roberta Large model . This model did not give me a successful result. The reason for this was that as the model was very large, only small sequence lengths were fitting in the GPU (seq: 32). This proved to be too small to capture the sentence embeddings and this variation of the model failed. * I also tried Bert and Bert Large  model. They gave similar performance to distilbert models so I investigated only distilbert. HyperParameter Tuning: SimpleTransformers library provides hyperparameter tuning support with the wandb. I investigated optimal lr. For sequence length I investigated by hand as I observed that small changes in sequence length did not affect the scores by a lot. Optimal lr: 3.1134e-5 Sequence: 256 Epoch: 8 After 4000 steps (or 8 epochs) the model stopped automatically as we put the early stop parameter. With this when the model stops learning the training procedure stops itself. (Results reported on 1000 samples withheld from the training data) Due to GPU costs I only performed hyperparameter tuning on sentence pair classification distilbert model. Summary As I wanted to use large transformers models, I made use of a specialised library which abstracts many functions required for transfer learning. Due to this the task of using complex models such as distilbert, bert, Roberta become really easy and straightforward.
https://github.com/reckoner-david/CourseProject	README.md	CourseProject Demo Video: https://drive.google.com/file/d/1oJy1Io6Fu3mG6kxovVSx_pd4jJgacsP6/view?usp=sharing If data does not download with google drive mounting please download data from: https://drive.google.com/drive/folders/19V4W6yhWjPz_qPKqQUCs86JSRpQIhS9s?usp=sharing Project Report: https://github.com/reckoner-david/CourseProject/blob/main/Project%20Report.pdf Source Code File: https://github.com/reckoner-david/CourseProject/blob/main/demo.ipynb (Notebook links to Colab) Source Code Documentation and Setup Guide: Look at Demo Video (it has explanation too) Documentation File: https://github.com/reckoner-david/CourseProject/blob/main/Source%20Code%20Documentation.pdf
https://github.com/reckoner-david/CourseProject	Source Code Documentation.pdf	Source Code Documentation Twitter Sarcasm Classification Challenge Sahil Rishi - sahilr2@illinois.edu from simpletransformers.classification import ClassificationModel This is the library we are using for transfer learning. train_args={ 'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 8, 'fp16': False, 'sliding_window': False, 'learning_rate': 3e-05, 'max_seq_length': 256, 'do_lower_case': True, 'train_batch_size': 8, 'evaluate_during_training': False } model = ClassificationModel('distilbert', 'distilbert-base-uncased', args=train_args, use_cuda=False) Train_args: It specifies the parameters for the model. We define the learning rate, epoch and max_seq_length here. We also define the do_lower_case as true as we use the uncased model. For final training, we train on all the data so 'evaluate_during_training' is False. model = ClassificationModel('distilbert', '/content/drive/MyDrive/Colab Notebooks/model.zip (Unzipped Files)/checkpoint-5000-epoch-8', args=train_args, use_cuda=False) We can load a saved model via this API. Just put the path of the model in. The Classification Model API handles Model instantiation Saving per epoch Training loops Prediction Loops Converting/Preprocessing of data So we don't have to explicitly handle all these details.
https://github.com/reckoner-david/CourseProject	Text Classification Competition.pdf	Text Classification Competition Team: Team Name: reckoner Members: Sahil Rishi - sahilr2@illinois.edu Individual Team Member - Sahil was chosen unanimously as the captain of the team Which competition do you plan to join? Text Classification Competition https://github.com/CS410Fall2020/ClassificationCompetition Are you prepared to learn state-of-the-art neural network classifiers: Yes, our team is keen on learning and using transformer models for classification. They have shown to be very efficient for classification and transfer learning allows training on low numbers of samples. We plan to use the hugging face transformer library. Previous Experience: Previously we have used seq2seq transformer models, BART and Bert (Tensorflow). Which programming language do you plan to use? Python
https://github.com/john-james-sf/CourseProject	IR Competition Project Proposal.pdf	IR Competition Project Proposal John James jtjames2 This project aims to leverage the information retrieval competition to explore state-of-the-art learn to rank system configuration document retrieval models. Learn to rank document retrieval system configuration is an area of active research that seeks to improve document retrieval performance by predicting system configuration parameters for a query that maximizes the likelihood of relevant documents. Approach The proposed approach is adapted from Deveaud et. al [1] and is comprised of the following four phases:  Pre-processing: Given a collection, the first step is to determine all feasible system configurations C and conduct document indexing. Next, the documents are ranked and the effectiveness of each query/system configuration pair is stored to file. The query and system configuration features serve as the feature set and the effectiveness scores represent the labels of the training set.  Training: This step makes use of the training examples constructed from the query features, system configuration features, and relevance labels measured by the respective evaluation metric. The learning to rank algorithm trains a model to maximize the effectiveness metric (e.g. nDCG), Once the training is completed, a learned model is generated.  Document Ranking: Taking as input, an unseen test set of queries, the trained model produces a ranked list of system configurations for each query.  Evaluation: Finally, the top ranked system configuration for each query is applied and the overall system performance is computed. Features The learn to rank features include:  Retrieval Model Features such as: o Absolute Discount Smoothing o Dirichlet Prior Smoothing o Jelinek-Mercer Smoothing o Okapi BM25 o Pivoted Length Normalization  Query Features including: o Query Statistics e.g., Mean and standard deviation variants of IDF o Linquistic Features: Synonyms Hyponyms, Meronyms, etc... I look forward to exploring these techniques in the IR competition!
https://github.com/john-james-sf/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/henryg3/CourseProject	CS 410 Progress Report.pdf	Henry Guan (henryg3), Kevin Yu (yuey8) CS 410 Project Progress Report - Improving a System (EducationalWeb) 11/29/2020 1. So far, we have made some progress on our project itself; we believe we are close to finishing the bulk slide downloader. 2. Remaining tasks we have to complete are finishing the bulk downloader, scaling up the system, and improving the UI where necessary. 3. Challenges we are currently facing are that although the bulk downloader works, it only downloads the first 10 slides of each weekly course lecture. Additionally, there is a lot of code to navigate (220,000+ lines of code in one of the files), so it takes a while to figure out where we can implement what we proposed in our project proposal.
https://github.com/henryg3/CourseProject	FinalProjectDocumentation.pdf	In order to run and test our code: Please read the README for our codebase. Video link: https://www.youtube.com/watch?v=bV4k16nsyQY&feature=youtu.be Scaling up the system: This allows for more courses to be added to EducationalWeb, and this is done by modifying model.py within the codebase, specifically, modifying the functions that take in the list of courses that are passed into EducationalWeb, as well as being able to correctly parse and sort slides. Bulk-Download: This utility allows the user to download up to five pdf slides at a time compared to one pdf slide before. We implemented additional functions on top of the original download in pdf.js. We first connected our function and button correctly onto the event bus, and for each time the bulk-download button is clicked, the current lecture page index is parsed as a parameter, and we are able to download the next few slides using that index. Since the pdf data is loaded one at a time, the bulk-download feature uses downloadByUrl instead of regular download by data. Multiple-pages skip: This utility allows the user to skip five pages at a time(next or prev), and makes the process of bulk-downloading much easier. The frontend of this utility is implemented in slide.html, and includes simply two buttons and their on-click events. The backend is implemented in model.py and is built on top of the original next and prev functionalities.
https://github.com/henryg3/CourseProject	FinalProjectProposal.pdf	EducationalWeb System Project Proposal Questions 1. yuey8, henryg3; the captain is henryg3 2. The system we have chosen is the EducationalWeb System. The subtopic(s) we have chosen under the system is to allow downloading slides in bulk and scaling up the current system. 3. For bulk downloading, we plan to implement a web bulk downloader on the current website. And to scale up the system, we plan to obtain datas from platforms such as coursera or UIUC courses using web crawling techniques. 4. We will demonstrate that our function works as expected by showing how the base system will perform without this function, versus the functionality of the system after the functions are implemented. 5. Our code will utilize the system by adding more contents from different sources and enables bulk downloading. 6. We are planning to use Python and JavaScript. 7. The workload of our topic is at least 40 hours (2 people in our team). a. Understanding the EducationalWeb System: approximately 1~2 hours b. Researching about our techniques 3~4 hours c. Bulk downloading: approximately 12~15 hours d. Implementing web crawling techniques to scale up the system: 24~27 hours
https://github.com/henryg3/CourseProject	README.md	Video Link: https://www.youtube.com/watch?v=bV4k16nsyQY&feature=youtu.be How to run our code: Please git clone this repository to whichever directory you'd like. You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/
https://github.com/nykznykz/CourseProject	Documentation.pdf	Documentation Overview of Code 1. app.py This file contains functionality for the web app which serves recommendations to the user. See demo here. 2. model.py This file contains logic and algorithms powering the recommendations in the webapp. 3. src/basic_approach/collaborative_filtering.py This file contains an implementation of basic collaborative filtering as mention in class during week 6. 4. notebooks/neural_network.ipynb This notebook contains an approach using neural networks to predict the user's final rating. 5. src/**/scraper.py There are various scraper.py scripts used to scrape and assemble the dataset used. Implementation details This is our stack: 1. Data Wrangling: pandas, numpy 2. Web Scraping: beautifulsoup, requests, regex * Please refer to scraper.py for more details 2. Model: scikit-learn, scipy, tensorflow * See requirements.txt for more 3. Web Framework: flask * Run app.py on localhost:5000 ``` 4. Front End: html & css This is the data that we managed to scrape: Content Data * all_recipes.csv * 1100+ Recipes from * 460+ Cuisines & Categories Content Data * all_users.csv * 55K Users * 73K Ratings The website we scraped this data from has much more users and ratings available but this is what we managed to collect with limited amount of time and compute. We tried the following approaches for our recommender system: 1. Basic collaborative filtering a. Suggest recipes that similar users also liked. Similarity is based on what recipes the users have rated and calculated using cosine similarity. 2. Content based filtering a. Suggest recipes similar to the recipes that the user liked. Similarly is based on the categories of the recipe and other content based features and calculated using cosine similarity. 3. Matrix Factorization a. Use singular value decomposition to discover latent factors that describe users and items. The matrix can be reconstructed and used to predict unobserved user-recipe ratings. b. This approach performed the best in terms of test rmse and predicting the ratings the users give to recipes on a held out test set. It is a classic method that performs well on the dataset we scraped. 4. Neural Networks a. Use an embedding layer to learn embeddings for users and items. Multiple dense layers are built on top of the concatenation of the embeddings to learn a function that predicts the rating. In addition content based features can be concatenated to the embeddings and used for prediction. b. It was found that adding further text based features from the title of the recipe slightly improved the test rmse. Overall the test rmse beats the basic collaborative filtering approach but loses to the Matrix Factorization approach. c. The model was found to perform better when more data was scraped and added to the dataset. It is possible that it may beat the matrix factorization approach if even more data was scraped. Usage of Software Webapp See demo here. 1. Clone the repo (https://github.com/nykznykz/CourseProject) 2. Install requirements.txt a. pip install -r requirements.txt 3. Run app.py a. python app.py 4. Navigate to localhost:5000 in the browser. Basic collaborative filtering approach See this readme for detailed documentation. Neural Network approach Run the jupyter notebook and follow along. Contribution of Group Members Tanmoy: Creating the webapp (app.py), modeling with SVD and other methods (model.py) and web scraping Nikolas: Modeling using deep learning approach (notebooks/neural_network.ipynb), comparison of methods, reports & creation of video Dikra: Web scraping, modeling with basic collaborative filtering approach used in class (src/basic_approach/collaborative_filtering.py) Self evaluation Overall we have completed the tasks we set out to do: 1. Scraping real world recipe data 2. Benchmarking what we learned in class with our own methods 3. Creating a webapp to serve actual recommendations to users We think the following could have been done if more time were available: 1. Scraping more data and constructing a larger dataset 2. Engineering more user and item based features for use together with the neural network
https://github.com/nykznykz/CourseProject	Progress Report.pdf	Progress Report Link to proposal: https://github.com/nykznykz/CourseProject/blob/main/Proposal.pdf Completed Tasks We have completed the first task: 1. Scraping a. Scraping reviews & ratings b. Scraping recipes Challenges 1. Dataset size a. While we managed to perform scraping on the website smoothly, getting a large dataset of users ratings is challenging. We expect to overcome this eventually as we will continue scraping and growing the dataset along with the completion of subsequent tasks. 2. Sparsity a. One concern is that the dataset might be sparse as each recipe is only reviewed by a small subset of users. We plan to address this by framing the recommendations as a classification problem and in doing so, we will be able to only train on observed ratings. Pending Tasks 1. Construction of dataset a. Basic dataset of user_id, recipe_id, rating b. Construction of features i. Exploratory Data Analysis ii. Feature engineering 2. Setting up baseline approach 3. Model training a. Creating general model architecture b. Tuning of hyperparameters c. Others (e.g. dropout, l2 normalization, embedding layers etc.) 4. Model Evaluation a. Evaluation of baseline & actual model 5. Post analysis 6. Compiling results and writing of report
https://github.com/nykznykz/CourseProject	Proposal.pdf	Free Topic: Food Recipe Recommender System Team Members 1. Lee Kar Heng Nikolas Basil (Captain) * nblee2@illinois.edu 2. Mochammad Dikra Prasetya * mdp9@illinois.edu 3. Tanmoy Mishra * tanmoym2@illinois.edu Description For this project, we chose the task of creating a Recommender System for food recipes. From a project perspective, it would be interesting to go from scraping real world data (from a recipe platform with millions of users and recipes) to training a model to evaluating it against an approach detailed in class. In general, we think it is also interesting and appropriate for the current situation where people are spending more time staying home and likely looking to pursue/get better at home cooking. Being in this position ourselves, we realise that finding a variety of recipes can be daunting. While we may have a sudden inspiration to cook a specific dish (e.g. lasagna) and are able to find such a recipe through pull based text retrieval methods like the search engine, finding more recipes after that point might be difficult. To tackle this problem, we aim to rely on the strengths of a push based approach such as recommendations to alleviate the decision fatigue and steer users to similar recipes that they are likely to enjoy (e.g. mac & cheese / pasta). Overview of approach 1. Scrape recipes on this site that have some number of reviews: https://www.allrecipes.com/recipe/281306/lime-ginger-chicken-kabobs-with-peanut-sauc e/ a. For each recipe, scrape data including the users who gave a rating and the actual rating they gave. From there, we are able to find other reviews that this user also rated. We can then add this to our list of recipes to scrape. b. This will help build an interaction dataset where each review is rated by multiple users and each user has rated multiple recipes. c. Many users provide a detailed description of their review, explaining their rationale for the rating. If time permits, we can use this as user features when scoring future recipes for a particular user or we can mine this information to learn more about a given recipe and generate new features. 2. Scrape metadata for each recipe to be used as features: a. Things like preparation technique b. Ingredients used etc c. Maybe even images if we want to do some CV based recommendations 3. Prepare dataset from the above two steps 4. Setting up a baseline (e.g. basic collaborative filtering) a. We do something similar to what we've learnt in class: i. For a user and recipe pair, average the ratings of that recipe from similar users. ii. Similar users are defined by a similarity threshold, with previous interactions used to calculate similarity. 5. Feature engineering a. User Features: We can learn something about a particular user given the user's past reviews. b. Recipe Features: We can learn something about a particular recipe given what users have said about it as well as the contents of the recipe itself. 6. Model Training a. We aim to predict the rating a user will give to a recipe 7. Model Evaluation a. Some data will be held back for evaluation. b. We will compare the predicted vs actual ratings. Tools 1. Selenium (scraping) 2. Python & Pandas (data manipulation) 3. Tensorflow (Recommender System) 4. Numpy / Scikit Learn (evaluation) Datasets We will create our own dataset by scraping www.allrecipes.com. The raw dataset, final dataset as well as code used to scrape and process the data will be made available. This dataset is expected to be rich in features and large in quantity https://expandedramblings.com/index.php/allrecipes-facts-statistics/ Expected Outcome We will have a rich dataset to train a good recommender system as well as evaluate on. We aim to beat the baseline approach taught in class. Evaluation RMSE: Actual ratings vs Predicted ratings NDCG: Rank recipes by ratings and calculate NDCG based on actual ratings. Both evaluation approaches should be able to be applied on the baseline and actual approach to enable comparison. Programming language Python Expected Workload We anticipate that we will easily spend more than 20*3 = 60 hours on the project due to the richness of the dataset allowing us to implement many techniques on it as well as the lengthy process likely required to obtain such a dataset. A rough breakdown of the work is as follows: 1. Scraping (Total 15 hours) a. Scraping reviews & ratings (7.5 hours) b. Scraping recipes (7.5 hours) We estimate this based on the given time allocated in MP2.1 (4hrs) while providing some buffer given the more complicated nature of the website which likely has measures to deter scraping. 2. Construction of dataset (Total 20 hours) a. Basic dataset of user_id, recipe_id, rating (3 hours) b. Construction of features i. Exploratory Data Analysis (7 hours) ii. Feature engineering (10 hours) 3. Setting up baseline approach (5 hours) 4. Model training (10 hours) a. Creating general model architecture b. Tuning of hyperparameters c. Others (e.g. dropout, l2 normalization, embedding layers etc.) 5. Model Evaluation (5 hours) a. Evaluation of baseline & actual model 6. Post analysis (5 hours) 7. Compiling results and writing of report (5 hours) Any extra time from unexpected surpluses can be channelled into subtasks like sentiment analysis/topic modelling for analysis & insights or used directly as model features.
https://github.com/nykznykz/CourseProject	README.md	CourseProject A video presentation can be found here Documentation for the project can be found here Recipe Recommender System Driven by my curiousity of how Netflix, Youtube and Spotify serve personalized recommendations, I decided to learn how to create my own recommender system. Machine Learning Problem: Given a person's preferences in past recipes, could I predict other new recipes they might enjoy? I created Seasonings, a Recipe Recommender System. The motivation behind this web app is to help users discover personalized and new recipes, and prepare for grocery runs! I received a lot early positive feedback and plan future improvements to the UX and model. I had a lot of fun making this, and plan to use this whenever I need a jolt of inspiration in the kitchen! Data Data was scraped from allrecipes.com, as there was no public API. I narrowed the scope to focus on Chef John's recipes (from FoodWishes.com). Content Data all_recipes.csv 1100+ Recipes from 460+ Cuisines & Categories Content Data all_users.csv 55K Users 73K Ratings Tech Stack Data Wrangling: pandas, numpy Web Scraping: beautifulsoup, requests, regex Please refer to scraper.py for more details Model: scikit-learn, scipy See requirements.txt for more Web Framework: flask Run app.py on localhost:5000 ``` Front End: html & css Models Please refer to model.py Collaborative Filtering - Suggest recipes that other users similar to you also liked (Cosine Similarity) If I liked Spaghetti Al Tonno, and another user similar to me liked Perfect Prime Rib and I haven't tried it, the model would recommend that recipe. Content Based Filtering - Suggest recipes that are similar to recipes that you like (Cosine Similiarity) If I liked Spaghetti Al Tonno, the model would recommend Italian Meatballs, because Italian Meatballs are similar to Spaghetti, in terms of the categories both recipes share (Italian, World Cuisine). Matrix Factorization - Suggest recipes that you like, uncover latent factors, in a lower dimensional space (Singular Value Decomposition) If I liked Turkey, and I liked Cranberry Sauce, the model would recommend Pumpkin Pie because it picked up a latent factor that you liked Thanksgiving dishes, where the other models would not be able to. Model Evaluation My final model was a hybrid recommender that tackled the cold-start problem with a content recommender, augmented with user preferences, and factorization to rank recipes based on a voting classifier rule.
https://github.com/nykznykz/CourseProject	Recipe Recommender System.pdf	Recipe Recommender System 2 Motivation * Increased interest in home cooking during this period. * While we usually know what we want to cook initially, we may run out of ideas at some point. * Require the help of a push-based approach like recommender systems. * Use case: I have tried and liked recipes for italian cuisine. Recommend me new recipes based on this information Solution Solution * Seasonings is a webapp that can recommend new recipes under a wide variety of contexts * Example contexts: * People with similar tastes also like... * Because you liked <Recipe Name>... * Tastebreakers... * All the user has to do is provide 5 anchor recipes for Seasonings to generate recommendations. Demo
https://github.com/bojiang3/CourseProject	050B.docx	"INTEROFFICE MEMORANDUM To: Ms. Beth Springer, VP John Repogle, CEO Subject: Burt's Bees Date: 8/19/2019 After reviewing this case, I have concluded that Clorox will not become eco-friendly by only purchasing Burt's Bees. However, with Clorox's purchase of Burt's Bees and the Clorox's new initiative to ""think about the Greater Good,"" this acquisition can provide the Clorox with insight on how a successful eco-friendly business operates. Implementing these practices will give Clorox an advantage over their competitors who are trying to enter the eco-friendly market as well. The Burt's Bees brand will see some negative impact because many of their fans are environmentally conscious, and Clorox's reputation when it comes to being environmentally friendly is not well perceived. With this acquisition, Clorox will see more of a benefit than Burt's Bees. With Clorox's purchase of Burt's Bees, they now have an insight into the eco-friendly consumer market that not many of their competitors do not have. According to research done by Clorox, ""53% of consumers"" plan on purchasing more eco-friendly products within the next year. These consumers are also willing to pay more for those eco-friendly products. The information provided tells us there is now a need for more eco-friendly products in the consumer market. Clorox will be able to charge more for these ""luxury"" products, which can be up to 57% more than standard (not eco-friendly) products. Clorox will now be in the position to set the standard of what natural cleaning products should be and will be making a profit from their efforts. Another benefit for Clorox in this acquisition is Burt's Bees research lab; this lab is full of competitor's products that Burt's Bees have been testing. These products are being tested to see if they can be considered a natural product. This testing will give insight into what companies have tried and failed when qualifying for the Natural Products Association label. It will lead Clorox's brand of natural products to become the standard for what natural cleaning products should be. Clorox needs to implement its initiative for ""thinking about the Greater Good"" and inform the market that they are serious about producing more natural and eco-friendly products. Burt's Bees will not see such a positive impact from this acquisition. Although their product does not bare the Clorox name, they are now associated with the company, and many consumers think of bleach when seeing or hearing Clorox. Burt's Bees will need to focus on informing their consumers they still maintain their values of being eco-friendly. There is a need for eco-friendly products in the consumer market, and they will continue to make a profit if they retain those eco-friendly values. Clorox will achieve profitability more so than Burt's Bees in this acquisition. When Clorox implements Burt's Bees business practices to their company and especially to their line of eco-friendly products, it will have a positive impact. There is a need for more eco-friendly products in the consumer market, and consumers are willing to pay for those products. Implementing the eco-friendly products will ultimately lead to a profit even though the cost for producing these products will be more. Clorox should then advertise the fact that they are the standard when it comes to reliable, eco-friendly products. Although Burt's Bees will not be as profitable as Clorox, there is still a need for their eco-friendly products in the consumer market."
https://github.com/bojiang3/CourseProject	CS 410 Final Project Report.pdf	Group SN #196 CS 410 Final Project Report Group SN # 196: Bojiang Li, David Ye, Yunfei Ma Sentiment Analysis Tool (Option 5: Free topic) This project aims to help students improve paper-reading and -writing ability by providing sentimental and keyword analysis. Midway Topic Change: Since during our original planned project implementation - EducationalWeb, we encountered too many unforeseen obstacles, we midway decided to switch something new - this sentimental analysis project. We have completed the project for giving feedback on the sentiment and giving scores for the sentimentals. We may keep working on the frontend later to to improve the user experience. Technology Used Programming language: Python Microsoft Azure Azure.ai.textanalytics library Azure.core.credentials library Azure.cognitiveservices library Msrest.authentication library Sample Sample Input Text/Paper Group SN #196 Sample Output Result Outcomes and Results - In the rendered, according to every sentence, it will give a score to the phrase. - Sentimental analysis will be given to each one of the sentences. - The high scores on both positive and negative will be noticed because it does not follow the required neutral tone of the essay. 050B.docx  is the sample file we test for the sentimental analysis. For grammercheck.py , it is the code for checking the right wording for the whole paper. Main.py  stores the user side code for this project. Video Presentation https://mediaspace.illinois.edu/media/1_h6ncumvp
https://github.com/bojiang3/CourseProject	Project Proposal - Group SN.pdf	"Project Proposal - Group SN 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team members: Bojiang Li (bojiang3), Zhanyuan Ye(zye19), Yunfei Ma(yunfeim2). Captain: Bojiang Li 2. What system have you chosen? Which subtopic(s) under the system? We have chosen EducationalWeb system. The subtopic under the system is ""Improving the usability and reach of the existing system "". 3. Briefly describe the datasets, algorithms or techniques you plan to use We will use Elasticsearch to implement searching functionality. We plan to use Python and JavaScript to build the frontend and backend. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We will have a user interface to test if our function can have the expected performance like observing if the searching results are related to the search keys. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will just implement some additional functions based on the finished system. 6. Which programming language do you plan to use? We plan to use Python and JavaScript. 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Project Setup - 15 hours * Implement Function 1 - 15 hours * Implement Function 2 - 15 hours * Implement Function 3 - 15 hours * Test and optimize - 10 hours"
https://github.com/bojiang3/CourseProject	README.md	"CS 410 Final Project For the specific information, sample text input and output results, please see ""CS 410 Final Project Report"" in the main menu. Sentiment Analysis Tool (Option 5: Free topic) Group SN (#196) Bojiang Li, Yunfei Ma, David Ye This project aims to help students improve paper-reading and -writing ability by providing sentimental and keyword analysis. The source code is provided in the main folder. Software installation First install the azure coginitive services package service. pip3 install azure3 pip3 install cognitiveservices pip3 install msrest pip3 install docx2txt Then direct run the file with Python. - Python3 works python text_analytics_bing_search_key_phase.py The result will be rendered in textanalyticresult.docx Discussion & Midway Topic Change Since during our original planned project implementation - EducationalWeb, we encountered too many unforeseen obstacles, we decided to switch something new - this sentimental analysis project. We have completed the project for giving feedbacks on the sentiment and giving scores for the sentimentals. We may keep working on the frontend later to to improve the user experience. Result and Outcome In the rendered, according to every sentence, it will give a score to the phrase. Sentimental analysis will be given to each one of sentences. The high scores on both positive and negetive will be noticed because it does not follow the require neutral tone of the essay. 050B.docx is the sample file we test for the sentimental analysis. For grammercheck.py, it is the code for checking the right wording for the whole paper. Main.py will be the user side code for this project Video presentation https://mediaspace.illinois.edu/media/1_h6ncumvp References/Sources https://docs.microsoft.com/en-us/office/troubleshoot/word/spelling-grammar-checker-underline-color https://realpython.com/sentiment-analysis-python/ https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"
https://github.com/bojiang3/CourseProject	[Progress report] .pdf	Progress Report Paper ID 196 Progress Made: Our team had 3 zoom meetings to discuss and understand the EducationWeb System and we made discussions about the potential bullet points and the potential improvements for the system. The small improvements we are interested in are scaling up the current system, Allowing downloading slides in bulk, adding more context to explanation, and integrating the tool with piazza. We may finally abandon some of the our choices based on the difficulty and time. Remaining tasks:We will push the remaining code to git, and we have finished 1 one the bullet point we mentioned above, and we are still working on scale up the system and allowing downloading slides in bulk this week, we may finish our code production on Dec,5 and have a zoom meeting to test and make improvements of the code. In addition, we may make a Youtube video to present our results to the peer reviewer. Challenges/Issues: If encountered we will attend office hour.
https://github.com/woshinisenbaba/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/CapnDanger/CourseProject	Progress Report.pdf	Bryan Holcomb CS 410: Text Information Systems Final Project Progress Report Currently, I have the majority of the sentiment analysis code written and I am in the process of testing on small, manually-copied articles. My next step is to complete testing, which I plan to do this week. The next steps after testing is complete are to build a webscraper to build up a corpus of news articles from various sources. I foresee one challenge being to find an easily-accessible API that allows scraping of web pages without ads or irrelevant content.
https://github.com/CapnDanger/CourseProject	Proposal.pdf	Bryan Holcomb CS 410: Text Information Systems Project Proposal Team Member: Bryan Holcomb (bryanph2) as individual Topic: Sentiment Analysis of News Articles Around the 2020 US Presidential Election For my course project, I plan to do a sentiment analysis of news articles around the 2020 US Presidential Election. There are a number of applications for which we can use this sentiment analysis, including an evaluation/comparison of biases between different news sources, as well as measure the general opinion of the candidates in different audiences and demographics. Additionally, we can evaluate how sentiment towards each candidate has changed over time. For this analysis, I plan to build a web crawler to obtain a large sample of articles from various websites, including large national news outlets (CNN, NBC, ABC, Fox News), at least one international source (i.e. BBC), and various newspapers from major cities around the country. For each article, I will track the date and source, and then parse each article and perform a sentiment analysis. I plan to use a number of packages to do the work, including MeTa, NLTK, Beautiful Soup, and IBM Watson. The vast majority, if not 100% of the project, will be done in Python. Hours Breakdown: Web Crawler design and setup: ~5 hours - This could take longer depending on how much customization is needed for each source I use, and depending on my ability to re-use the same procedures and my ability to run my scrapers behind paywalls. Parsing and database setup: ~5 hours - Once I have the articles saved as documents, I believe parsing should be very straightforward, as done in lecture and the MPs. Design of Sentiment Analysis code: ~7 hours - This will take the bulk of my effort. I believe the most challenging part will be to divide articles that discuss both candidates and group each sentiment with the correct candidate. Statistical analysis of results: ~3 hours
https://github.com/CapnDanger/CourseProject	README.md	"Documentation Python Files cnn_scraper.py: Used to scrape CNN articles. Accesses the publicly available CNN API. fox_scraper.py: Used to scrape Fox News articles. Accesses the back-end API to scrape article information, then uses those links to access articles. reuters_scraper.py: Used to scrape Reuters articles. Scrapes list of articles and then uses those links to access articles. ibm_sentiment.py: Uses the IBM Watson Natural Language Understanding API to analyze the sentiment of each corpus of articles. Targets are set to analyze sentiment for keywords ""Trump"" and ""Biden"". See inline comments for detailed descriptions of code functions. Note that the scrapers take a long time to run (sometimes upwards of 1-2 hours), while the sentiment analysis takes ~20-30 minutes to run. Output Files XXX_urls.csv: contains data on each corpus. All files contain the publish date, headline, url, and trump/biden sentiment scores. Depending on the source, may also include authors or category. XXX_scores.csv: output of ibm_sentiment for each file. This can be manually copy/pasted to the respective XXX_urls file to populate those columns. XXX_body.txt: output of XXX_scraper scripts. Each file contains one article per line, in the same order as the respective XXX_urls file. Other File chromedriver: necessary for selenium package to operate the scraper API Documentation The IBM Watson Natural Language Understanding API documentation can be found at https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#sentiment Other attributions Select portions of the scraper codes have been adapted from MP2.1. Video Demonstration https://mediaspace.illinois.edu/media/t/1_ksj5ytyq"
https://github.com/yunhezhang/CourseProject	README.md	CourseProject Final report - https://github.com/yunhezhang/CourseProject/blob/main/doc/final_report.pdf. Demo is at https://github.com/yunhezhang/CourseProject/blob/main/demo.mp4. Directory structure: doc - documentations src - source code for the SVM model and web crawler trainingData - training data for the SVM model
https://github.com/mihiryerande/CS-410-Fall-2020-Anime-Text-Analytics	README.md	Anime Text Analytics This is a Course Project for UIUC CS 410: Text Information Systems (Fall 2020). To view the output of this project, please visit https://animetextanalytics.azurewebsites.net. A video presentation of the work done can be viewed here: https://tinyurl.com/animetextanalytics Team Produced by Team Nani (He  !?): * Karan Bokil (karanb2) @bokilenator * Mihir Yerande (yerande2) @mihiryerande Explanation Anime shows are categorized into various genres, such as Shonen or Mecha, for example. This project attempts to use the Latent Dirichlet Allocation (LDA) algorithm to determine such genres from text data. LDA works by training a model on input text data to obtain topics and topic coverages. We use text data, scraped and cleaned, from myanimelist.com, where there are short synopses of anime shows. The topics produced by our LDA model are referred to as LDA genres. Each genre is a probabilistic distribution over words, which would ideally reflect a genre understandable to humans. In addition, each anime show can be assigned a topic coverage (i.e. genre breakdown). For example, we might determine that a show is 71% Shonen and 29% Mecha. Website The output of the project has been published to a website, which can be found here: https://animetextanalytics.azurewebsites.net LDA Implementation This section steps through the implementation of the project from start to finish. The LDA code and output is all stored in the source_code directory. Scraper The raw text data is scraped from myanimelist.net, specifically from the list beginning here. The output has already been written to scraped.jl. The scraper is implemented in Python using the scrapy framework. See animespider.py. To run the scraper, navigate to the containing directory, and run the following command: scrapy runspider animespider.py -o scraped.jl The spider's log will automatically write to spider_log.txt, in the same directory. LDA Input In order to run LDA, the raw text must be tokenized and cleaned. The output of this step has already been written to lda_input.jl. See write_lda_input.py for the implementation. To run the text cleaning, navigate to the containing directory, and run the following command: python write_lda_input.py Print-out should appear in the console as each show's raw text is cleaned. LDA Model After cleaning the raw text, we can now train our LDA model. The trained LDA model has already been saved to lda_model. See write_lda_model.ipynb for further explanation. LDA Output After training the LDA model, we can obtain the desired output about genres. The output of this step has already been written to lda_output. See write_lda_output.ipynb for further explanation. LDA Distances After obtaining the genre-breakdowns, we can determine similarity between anime shows based on their respective breakdowns. We use the Hellinger distance utilities provided in gensim, as described here. The output of this step has already been written to lda_distance. See write_lda_distance.ipynb for further explanation. Database Due to the size of our dataset, we did not feel processing Just in Time from a website performance perspective would be good. Thus, we preprocessed most of the data from the aforementioned steps and converted into a relational database for easy access by the web framework. After attempting to utilize Azure CosmosDB as well as Azure SQL, we ended up choosing to go with a SQLite database because it is light, easy to iterate testing on, and can easily be included as part of the repo, being only 5MB and self contained. Our attempts at using CosmosDB and Azure SQL were hindered by slow upload times, as we had to parse from json and upload around 50,000 records, which would have taken several hours. The Azure Stack would have provided us some ecosystem advantages such as use of their BM25 ranking solution, Azure Cognitive Search, but nevertheless, we were able to find a different avenue for full text search. The database can be initialized simply by running python init.py from the root of the repo. This file will delete the former tables, create the new tables, parse the JSON and populate the Database alongside the relationships between the various tables. Text Retrieval Model After researching different libraries for text retrieval models, we settled on utilizing the MSearch library, as it has the most integrated support with our web framework Flask. MSearch serves as a wrapper around Whoosh, a pure Python search engine library that capitalizes on Okapi BM25 ranking function. Within app.py, we have marked fields in the various tables with __searchable__ and created a custom route and view to collect and see the results of a query. The search thus exceeds the utility of normal database queries, facilitating results across different tables with ranking. The inverted index is created during the data population phase in init.py. Web Development and Hosting The website is made using Flask, a lightweight web framework in Python. It utilizes a standard MVC architecture and communicates cleanly with the Database via SQLAlchemy. The site is hosted and deployed on Microsoft Azure using Azure Web Apps. The frontend Javascript and CSS components is all developed utilizing the Materialize framework. Routing and models are all present in app.py, see comments for details. The application can be run locally via Python 3.8 simply by running pip install -r requirements.txt and python app.py. Further documentation can be found at https://animetextanalytics.azurewebsites.net/documentation.
https://github.com/antonioalfonso/CourseProject	Project-Progress-Report-AG.pdf	CS 410 - Text Information Systems Antonio Gomez Lopez Progress Report: Project: Text Classification Competition (Sarcasm on Twitter) 1) Which tasks have been completed? So far, as part of the project I have completed the following tasks: 1. Take training data into pandas (Python) 2. Processing of the data which includes: a. Removal of @USER references b. Removal of HMTL references c. Removal of hashtag symbols d. Removal of links e. Removal of punctuation f. Removal of stopwords g. Removal / Substitution of emojis by text (testing these two scenarios). h. Removal of other special symbols. 3. I have created a classifier using Neural Networks, specifically using LSTM with an Embeddings Layer (activation function being sigmoid, which I find appropriate for a classifier). 4. I have tested three approaches / packages: a. Word2Vec b. Glove c. FastText So far, I have gotten my best results (69.5% F1 Score) with Fast Text under specific conditions (see below). I was expecting much better results with Glove but that turned out not to be the case. What am I currently working on? 5. With the NN as described above, I am currently testing different embedding dimensions (25, 50, 100, 200) for the embeddings layer. I have no conclusion yet, but it seems like I am getting better results with Embedding dimension = 100. 6. Currently testing two optimizers: a. Started with Adam at learning rate 0.01, results were somewhat overwhelming (F1 score ~55%) b. I found better results by using Adamax (which is recommended for NN using embeddings), and at learning rate 0.001. Because of the smaller learning rate, I need to run more epochs (hence is slower to learn, which is totally expected), but I managed to improve to my best result using that. c. I am using cross-binary entropy as loss function, which again I find pertinent for the problem I am trying to optimize for (classification issue). d. It seems, however, that I am facing a barrier as I cannot move past 70% with my current efforts (see below on what's pending and issues facing). CS 410 - Text Information Systems Antonio Gomez Lopez 2) Which tasks are pending? I have a functioning NN model that is able to assess the tweets as required by the assignment. So in that sense I have something to show . Having said that, and as mentioned before, I am not achieving the desired result (beating the baseline, currently at above 72.3% F1 Score. Things I still have pending to do: * Further research for models that can take into advantage context information (which I am currently not using), such as replies to tweets, which could shed more light on the intention of the original tweet that is subject to sentiment analysis. * Check other potential approaches to the problem that do not include training a Neural Network (although I am not sure they could be better). 3) Are you facing any challenges? Yes. I have a working model, but I am not hitting high numbers to beat the baseline, and I would like to do so. Going further I would like to have a high F1 score (likely by having a high precision score, but open to less accuracy for a higher recall, if that gets me above the F1 score). With my current approach I am unable go above 69.5% F1 Score (with various numbers in precision ranging from 55% to 63%). My other challenges are: * I am currently not using the context information for tweets given in the training and tests set, and I think I am missing out on important information to improve my algorithms / models. I am not using this information mostly because I do not know how to embed or incorporate that information into the NN to improve the predictions (or how to use it at all even if it were not a NN). * I am currently converting emojis to text, but I am not sure if this is making any difference (versus, say leaving the emojis untouched and let the NN process them as they are represented). * I am not sure I am using the best optimizer and loss function (I think I am, but I don't know what else is out there). So, if you have any guidance, to a paper, specific model in Kaggle, whatever that can be of use for me, please feel free to share that information with me as part of the feedback. I am working in this project alone.
https://github.com/antonioalfonso/CourseProject	Project-Submission-AG.pdf	CS410: Project Proposal Antonio Gomez Lopez 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I am doing this project individually. So, I am the captain . Full name: Antonio Gomez Lopez, NETID: aag8 2. Which competition do you plan to join? Text Classification (Sarcasm detection on Twitter). 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods: I am prepared to learn state-of-the-art neural network classifiers. I am interested in this challenge because I can see the difficulty for a non-human to discern from the literal to intended meaning of the words. Prior Experience: I have taken two Machine Learning courses (one in UT Austin, the other one currently in UIUC), in which I have implemented versions of the following (as part of projects): a. Perceptron Algorithm b. Logistic Regression (which is not quite useful for the classification issue but still). c. Support Vector Machine d. Naive Bayes I also studied the theory for K-Nearest neighbor, but never implemented it. In additionto this, I took a Reinforcement Learning class at UT Austin, and in it, as part of our practical experience, we had the choice to work with either TensorFlow or PyTorch in order to implement policy evaluation using Neural Networks, as well as the REINFORCE algorithm. I implemented both successfully using TensorFlow (which is the one framework with which I have experience). 4. Which programming language do you plan to use? Python
https://github.com/antonioalfonso/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection Submission by Antonio Gomez Lopez This model was built using Neural Networks, as a consequence the training time can vary significantly depending on whether you train in a local environment (e.g. your computer), or GPUs (e.g. Google Colab) IMPORTANT: You can find the tutorial here (I show it running Colab and beat the baseline live :-)) https://www.youtube.com/watch?v=p1T-ekliduA If you want to run it using Colab, there is a read-only version of the notebook available on the link below: https://colab.research.google.com/drive/1kEjXNHX-tHkjG38BJtw9ol2QH67lGXaG?usp=sharing Otherwise, use the ClassificationCompetition_BERT_AntonioGomez.ipynb and run it locally. I recommend using Jupyter, but I assume any good Python editor would do the work. Check 2) to make sure you have both train.jsonl and test.jsonl files in the right directory. 1) What's included in this submission? 1.1) This file includes details on what needs to be setup. 1.2) The reference to Google Colab (above) as well as the .ipynb file contain the commented source code. 1.3) answer.txt contains the test set predictions (as required in https://docs.google.com/document/d/13ANy7FHYovh_2JL3gVrVvzXScDh5ol5l5XS2Nlp4DN4/edit) 2) What is required to run this model? In both cases you need to have the files train.jsonl, and test.jsonl ready to be loaded. 2.1) If you want to use the Google Colab instance, you should: 2.1.1) mount your google drive (assumes you have a gmail or otherwise google-compatible account) 2.1.2) upload the train.jsonl and test.jsonl at the root of your drive As you run the code, you might get the following message: ""Warning: This notebook was not authored by Google. This notebook was authored by aalfonzo@gmail.com. It may request access to your data stored with Google, or read data and credentials from other sessions. Please review the source code before executing this notebook. Please contact the creator of this notebook at aalfonzo@gmail.com with any additional questions."" 2.2)If you want to use the local run instance, you should: 2.2.1) From the folder you are running the .ipynb file, you should have a /data folder, in which you will place both the train.jsonl and the test.jsonl files. 3) About the workbooks: 3.1) Both workbooks (ipynb) have cell-by-cell runs with my comments on the code explaning either what the code does or why did I choose specific parameter values. -Remember that the local run instance will train much more slowly (it can take you many hours to get to the trained model). For instance, in my laptop it took roughly 35 minutes per epoch, so 35x3=105 minutes, a little bit less than two hours to have the model trained. 4) Please note that: 4.1) The final design, fine-tuning and training of the network, including number of nodes, size of input, batch-size, dropout rates, which optimizer and learning rate to use were all of my authorship after several tests being performed. 4.2) Having said that, when it comes to the Neural Network solution I am presenting, I got inspired by two other projects I found in Kaggle, and that I am referencing here: [A] Sarcasm detection using BERT (92% accuracy), by Raghav Khemka. https://www.kaggle.com/raghavkhemka/sarcasm-detection-using-bert-92-accuracy, [B] Sarcasm detection with Bert (val accuracy 98.5%), by Carmen SanDiego (clearly a pseudonym). https://www.kaggle.com/carmensandiego/sarcasm-detection-with-bert-val-accuracy-98-5 [C] https://arxiv.org/pdf/1810.04805.pdf To get information about best optimizers and learning rates to use for BERT 4.3) How did I arrive to the succesful model? Background: Important to note, in the course of the project I have worked with different pre-defined packages (and data transformations), that I am not including here for the sake of focus on what's really the working solution. Having said that, however, the exploring of different alternatives was what had made my views evolve (looking for higher F1 scores). Below is a summary of what I worked with and the average F1 scores I got in each case. Technology F1 Score - MetaPy (no features, Naive Bayes) ~57% - Embeddings and GloVe ~66% - Embeddings and Doc2Vec ~68% - Embeddings and FastText (by Facebook) ~70% - Embeddings and BERT ~73.1% Note that the setup of each model was dependent on the pre-defined package I was using. Explanation of the model: It is based on a Neural Network with 5 layers (including the input and output layers, and including one layer with embeddings provided by BERT). I included a chance to Dropout on the basis of potential overfitting (having a too big network for the task at hand). In addition, Optimizer, learning rate, batch size, and loss functions were ALL parameters up for fine-tuning. There is some pre-processing of the data: Mostly removed punctuation, special symbols and stopwords. Having said that, I obtained best results when I did not removed @USER, references or emoticons. Some encoding / padding was included to keep consistent the size of the input for tweets either in training or testing. How did the training happen? For all the methods used (except Metapy - Naive Bayes), all of my training and initial fine-tuning happened first with the optimizer, learning rate, and ""size"" of embeddings (this last one was not the case for BERT). I also experimented with pre-processing of data, but as I mentioned earlier, I seem to obtained better results when I did not remove as many references that I originally thought would be mostly noise (e.g. @USER, ). In the specific case of BERT I performed the following fine-tuning (not necessarily in this order): Optimizer: Started with Adam, then moved to AdamWeightDecay, and finally got best results with Adamax. As per the documentation in Tensorflow, Adamax is recommended when the model includes an embeddings layer. Learning rate and number of epochs: I used 2e-5 as direct recommendation of the BERT paper: https://arxiv.org/pdf/1810.04805.pdf number of epochs is also recommended in this paper. I experimented however with large numbers (large as 10), but it was very clear that I was overfitting the model with the training data. I found that a number of epochs between 2-4 would work best, but needed additional fine-tuning. Dropout rate: From the referenced model, they use a dropout rate of 20%. I was still overfitting in many instances after the third epoch, so ended up increasing the droupout to 40%, and with that I could mitigate the problem of overfitting the model with the training model. Pre-processing data: I coded different functions to remove different parts of the tweets that I thought would constitute noise, but ended commenting most of them and leaving the functions that would remove just the stopwords and special punctuation symbols. Batch-size: I experimented with different batch sizes making sure that I was maximizing the maximum input from BERT (512 bytes). I ended up with an input of 64 and batch size of 8, but I equally beat the baseline with input size of 128 and batch size of 4. 5) Other important details about this implementation: Throughout the workbook you will find comments that seek to justify my choices many of the parameters and fine-tuning completed. The most important things to know are: 5.1) The model worked best when running only for 3 epochs, given my selection of optimizer and learning rate. 5.2) I had to dropout roughly 40% of the nodes (according to literature I consulted it is considered normal to do dropouts between 20% and 50%. This was the case because I was getting overfitting on the training data already in the third epoch (which indicates that maybe my network was too big to begin with...). 5.3) I tried multiple times to provide a saved model so that you could run the network the same way I did, however for unknown reasons, even though I was able to succesfully save my models, I was not able to load them back again. 5.4) As a consequence of 5.3), it is likely that a run of the model as it is might not render results that beat the baseline, (because is dependent on the training), so take that into consideration as you do the peer-review or assessment. 4.5) Having said that, I included my best run as part of the project submission (also as it was required as per homework guidelines (see https://docs.google.com/document/d/13ANy7FHYovh_2JL3gVrVvzXScDh5ol5l5XS2Nlp4DN4/edit)"
https://github.com/kplhung/CourseProject	documentation.pdf	"Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai December 13th, 2020 Documentation 1. Code functionality overview This code parses a large data set1 and extracts a set of documents consisting of paragraphs related to the 2000 United States presidential election, contested between Republican George W. Bush and Democrat Al Gore. It then goes on to apply latent Dirichlet allocation (LDA), a generative statistical model, to this document set, thus identifying candidate topics in the set. Normalized prices based on the Democratic candidate, Al Gore, are then extracted from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market2, and Granger testing is performed to determine possible causality of the candidate topics, indicating which topical key words are most likely to impact the candidate's IEM price positively or negatively--that is, their likelihood of being elected. This can be taken as a prior for future iterations of causal topic mining. In terms of future applications, the code can be modified to identify target paragraphs of any nature from The New York Times corpus. Various external time series data can also be used in conjunction--physical data comes to mind as a useful application. Or, public opinion polling on issues like climate change or social problems could be coupled with data from The New York Times corpus in order to find any sort of causal relationship between media coverage and public opinion surrounding those issues. 1 Namely, The New York Times corpus, from May to October 2000 2 A prediction market allowing traders to buy and sell contracts based on that year's presidential election. Hung 2 2. Software implementation Note: mining_causal_topics.ipynb combines code and results from identify_candidate_topics.py, iowa_electronic_markets.py, and granger_testing.py into one coherent file; this notebook is the easiest place to see code and results past the election paragraph parsing stage, though documentation has been organized based on the subfiles for ease. find_election_paragraphs.py: filters May through October 2000 New York Times articles for election-related paragraphs - find_election_paragraphs(directory): given a directory containing XML article data, walks through all subdirectories and parses all files. Writes all election-related paragraphs (those containing ""Gore"" or ""Bush"") to a file called ""election_paragraphs.csv"" o Helper functions: # parse_xml(xml, election_paragraphs): checks XML article for person tag, then parses relevant articles' full text for relevant paragraphs # contains_election_words(text): returns true if and only if ""Bush"" or ""Gore"" is a substring of the input text # format_date(file_path): returns formatted date from directory path identify_candidate_topics.py: applies LDA to the document set to identify candidate topics - Data wrangling o Wrangle and clean paragraph data by removing punctuation and lowercasing text o Remove stop words from the paragraph data and vectorize the text using scikit- learn, then learn the vocabulary dictionary, deriving a document-term matrix over the paragraph set - Topic modeling with LDA o Using LDA, learn the documents as bags of words and identify candidate topics in the paragraph set o Transform the document-term matrix according to the fitted LDA model, and index on date (dates range from May 1st, 2000 to October 31st, 2000) Hung 3 iowa_electronic_markets.py: wrangles, cleans, and normalizes IEM 2000 Presidential Winner- Takes-All Market data - Data wrangling o format_date(date): given IEM-style date (mm/dd/yy), returns NYT-style date (yyyy-mm-dd) o Read from ""iem_2000.txt""--candidate price data from May to October 2000-- and drop irrelevant columns (namely: units, volume, low price, high price, and average price), keeping only date, contract, and last price data; index on date o Derive lists of Democratic and Republican candidate prices based on the ""contract"" field (""Dem"" indicates Democrat; ""Rep"" indicates Republican) o Compute normalized prices based on the Democratic candidate # For each date index, normalized price = Dem price / (Dem price + Rep price) - Election coverage vs. election forecast o Concatenate the normalized price data to the topic coverage data from identify_candidate_topics.py; this brings together topic words and price data, indexed by date granger_testing.py: perform Granger tests to determine significant causal words and their impact - Granger testing o Perform Granger tests3 to test causality with max lags up to 3 - Significant causal words o Compute average p-values across time lags (from 1 up to 3) for each candidate topic term, based on parameter F tests 3 Granger tests compute lagged correlation measures to find potential causal relationships between time series-- here, they look for potential causal relationships between New York Times coverage and candidate electability Hung 4 o Sort candidate topic terms by causality probability, where a smaller p-value corresponds to a larger probability that there exists some type of causal relationship between the two time-indexed data - Prior influence o Determine positive impact terms and negative impact terms, where positive impact terms are the ones with p-values in the lower half, and negative impact terms are the ones with p-values in the upper half 3. Usage - Install Jupyter using the documentation provided here - Open terminal and run the following command: o git clone https://github.com/kplhung/CourseProject.git - Launch Jupyter Notebook and navigate to the CourseProject directory - Open and run find_election_paragraphs.ipynb o [Kernel] - [Restart & Run All] - Open and run mining_causal_topics.ipynb o [Kernel] - [Restart and Run All] 4. Team member contributions This was an individual project."
https://github.com/kplhung/CourseProject	progress_report.pdf	"Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai November 29th, 2020 Progress Report 1. Completed tasks - Filter The New York Times corpus for articles from May 2000 to October 2000 that mention ""Bush"" or ""Gore."" - Apply latent Dirichlet allocation (LDA) to the cleaned New York Times corpus to identify possible topics. - Use Granger testing to test causality and to derive the set of candidate causal topics with time lags. 2. Pending tasks - Apply Granger testing to find the most significant causal words among top words for each candidate topic, and track these words' impact values. - Define a prior on the parameters of the topic model using these significant terms and their corresponding impact values, and apply LDA to the New York Times corpus using this prior. - Iterate, and replicate one set of experimental results by using prices from the Iowa Electronic Markets 2000 Presidential Winner-Takes-All Market. 3. Challenges - Fully understanding how to set the strength of the prior in each iteration by using m."
https://github.com/kplhung/CourseProject	project_proposal.pdf	"Hung 1 Pei Lun Hung CS 410 Professor ChengXiang Zhai October 25th, 2020 Project Proposal Project Topic: Reproducing a Paper on Causal Topic Modeling 1. Team Name: Pei Lun Hung (project coordinator/leader) NetID: prhung2 Email address: prhung2@illinois.edu 2. Paper I plan to reproduce the results from Hyun Duk Kim et al.'s paper, ""Mining causal topics in text data: Iterative topic modeling with time series feedback.""1 3. Implementation Language I plan to implement the project in Python. 4. Data Set The paper uses The New York Times Annotated Corpus. As such, I have registered for an account with the Linguistic Data Consortium and am awaiting acceptance from admin. 5 Backup Data Set There is a digital archive2 of The New York Times, with articles from the 1850s to the present day. As backup, I can scrape articles from January 1st, 1987 to June 19th, 2007 from this digital archive in order to obtain a similar data set to the aforementioned corpus, sans annotations. 1 Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 2 https://spiderbites.nytimes.com/"
https://github.com/kplhung/CourseProject	README.md	"CourseProject Topic: Reproducing a Paper on Causal Topic Modeling Reference: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Video: https://www.screencast.com/t/yiDQ5OrjQwj Documentation Documentation is reproduced below. However, for better formatting and reading, please see the PDF version. 1. Code functionality overview This code parses a large data set and extracts a set of documents consisting of paragraphs related to the 2000 United States presidential election, contested between Republican George W. Bush and Democrat Al Gore. It then goes on to apply latent Dirichlet allocation (LDA), a generative statistical model, to this document set, thus identifying candidate topics in the set. Normalized prices based on the Democratic candidate, Al Gore, are then extracted from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market , and Granger testing is performed to determine possible causality of the candidate topics, indicating which topical key words are most likely to impact the candidate's IEM price positively or negatively--that is, their likelihood of being elected. This can be taken as a prior for future iterations of causal topic mining. In terms of future applications, the code can be modified to identify target paragraphs of any nature from The New York Times corpus. Various external time series data can also be used in conjunction--physical data comes to mind as a useful application. Or, public opinion polling on issues like climate change or social problems could be coupled with data from The New York Times corpus in order to find any sort of causal relationship between media coverage and public opinion surrounding those issues. 2. Software implementation Note: mining_causal_topics.ipynb combines code and results from identify_candidate_topics.py, iowa_electronic_markets.py, and granger_testing.py into one coherent file; this notebook is the easiest place to see code and results past the election paragraph parsing stage, though documentation has been organized based on the subfiles for ease. find_election_paragraphs.py: filters May through October 2000 New York Times articles for election-related paragraphs * find_election_paragraphs(directory): given a directory containing XML article data, walks through all subdirectories and parses all files. Writes all election-related paragraphs (those containing ""Gore"" or ""Bush"") to a file called ""election_paragraphs.csv"" Helper functions: parse_xml(xml, election_paragraphs): checks XML article for person tag, then parses relevant articles' full text for relevant paragraphs contains_election_words(text): returns true if and only if ""Bush"" or ""Gore"" is a substring of the input text * format_date(file_path): returns formatted date from directory path identify_candidate_topics.py: applies LDA to the document set to identify candidate topics * Data wrangling Wrangle and clean paragraph data by removing punctuation and lowercasing text Remove stop words from the paragraph data and vectorize the text using scikit-learn, then learn the vocabulary dictionary, deriving a document-term matrix over the paragraph set * Topic modeling with LDA Using LDA, learn the documents as bags of words and identify candidate topics in the paragraph set Transform the document-term matrix according to the fitted LDA model, and index on date (dates range from May 1st, 2000 to October 31st, 2000) iowa_electronic_markets.py: wrangles, cleans, and normalizes IEM 2000 Presidential Winner-Takes-All Market data * Data wrangling format_date(date): given IEM-style date (mm/dd/yy), returns NYT-style date (yyyy-mm-dd) Read from ""iem_2000.txt""--candidate price data from May to October 2000-- and drop irrelevant columns (namely: units, volume, low price, high price, and average price), keeping only date, contract, and last price data; index on date Derive lists of Democratic and Republican candidate prices based on the ""contract"" field (""Dem"" indicates Democrat; ""Rep"" indicates Republican) Compute normalized prices based on the Democratic candidate * For each date index, normalized price = Dem price / (Dem price + Rep price) * Election coverage vs. election forecast Concatenate the normalized price data to the topic coverage data from identify_candidate_topics.py; this brings together topic words and price data, indexed by date granger_testing.py: perform Granger tests to determine significant causal words and their impact * Granger testing Perform Granger tests to test causality with max lags up to 3 * Significant causal words Compute average p-values across time lags (from 1 up to 3) for each candidate topic term, based on parameter F tests Sort candidate topic terms by causality probability, where a smaller p-value corresponds to a larger probability that there exists some type of causal relationship between the two time-indexed data * Prior influence Determine positive impact terms and negative impact terms, where positive impact terms are the ones with p-values in the lower half, and negative impact terms are the ones with p-values in the upper half 3. Usage Install Jupyter using the documentation provided here Open terminal and run the following command: git clone https://github.com/kplhung/CourseProject.git Launch Jupyter Notebook and navigate to the CourseProject directory Open and run find_election_paragraphs.ipynb: [Kernel] -> [Restart & Run All] Once find_election_paragraphs.ipynb is finished running, open and run mining_causal_topics.ipynb: [Kernel] -> [Restart and Run All] 4. Team member contributions This was an individual project."
https://github.com/pinkychauhan89/FakeNewsClassifier	Presentation.pptx	"FAKE NEWS CLASSIFIER Pinky Chauhan University of Illinois at Urbana-Champaign Overview/Objective Build a classifier system based on machine learning Able to identify fake news from real/reliable news given a news title/text as input Can be integrated with social media platforms to flag/filter out potentially fake articles CLASSIFIER NEWS (TITLE, TEXT) RELIABLE NEWS FAKE NEWS Why Fake News Classifier? Increasingly prevalent Widespread on social media and websites Hard to distinguish Similar tone/style as reliable news Hard for human eye to catch differences Machine learning can help Technology/Libraries Dataset https://www.kaggle.com/c/fake-news/data train.csv: A full training dataset with the following attributes: id: unique id for a news article title: the title of a news article author: author of the news article text: the text of the article; could be incomplete label: a label that marks the article as potentially unreliable 1: unreliable/fake 0: reliable test.csv: A testing training dataset with all the same attributes at train.csv without the label. submit.csv: A sample submission to be populated with predictions of classifier on test.csv Code Structure Execution Data analysis Preprocessing Feature Extraction Vectorization Model training and tuning Performance assessment Data Analysis Data distribution (Real vs fake labels) Attributes contribution towards category Author's distribution Polarity/Sentiment Analysis Preprocessing Feature Extraction Cleaned article (title + text) used as feature Vectorization Term frequency (TF) based vector over unigrams Term frequency (TF) based vector over unigrams and bigrams Term frequency/inverse document frequency (TF-IDF) based vector over unigrams TF-IDF based vector over unigrams and bigrams TF-IDF based vector over unigrams, bigrams and trigrams Model Training & Tuning Naive bayes (Multinomial) Logistic Regression SVM using Linear SVC SGDC classifier Decision Tree Naive Bayes (Multinomial) Probabilistic classifier inspired by the Bayes theorem under a simple assumption that attributes are conditionally independent Training with smoothing Without smoothing Unigrams/Bi-grams/Trigrams term frequency vectorizer Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Logistic Regression Uses a logistic function to model a binary dependent variable Used when dependent variable is binary in nature Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Support Vector Machines (Linear SVC) Fit to the data, returning a ""best fit"" hyperplane that divides, or categorizes data LinearSVC only supports a linear kernel, is faster and can scale a lot better Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Tuning regularization SGDC (Stochastic gradient descent Classifier) Linear classifier optimized by the Stochastic gradient descent (SGD) Faster convergence Looking for the minima of the loss using SGD Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Decision Tree Builds classification or regression models in the form of a tree structure Utilizes an if-then rule Unigrams/Bi-grams/Trigrams term frequency/inverse documents frequency vectorizer Performance Evaluation Accuracy Precision (macro/micro) Recall (macro/micro) F1 score Confusion matrix Precision-recall curve Final Model Linear SVC with unigrams TF-IDF vectorization Export trained model using pipeline and joblib Tester Tester.ipynb Loads trained model using joblib Add text and title as inputs in notebook Kaggle https://www.kaggle.com/pinkychauhan/fakenewsclassifierusingnltk-sklearn References https://www.kaggle.com/c/fake-news/data https://scikit-learn.org/stable/user_guide.html https://medium.com/datadriveninvestor/python-data-science-getting-started-tutorial-nltk-2d8842fedfdd https://matplotlib.org/tutorials/introductory/pyplot.html https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623"
https://github.com/pinkychauhan89/FakeNewsClassifier	ProgressReport.pdf	Progress Report By: Pinky Chauhan Topic: Fake news classification using machine learning 1) Which tasks have been completed? o As per the recommendation of project proposal reviewer in CMT, I changed the dataset to the one suggested by the reviewer https://www.kaggle.com/c/fake-news/data o I have been acquainting myself with different classification algorithm details and also nltk, sklearn, pandas libraries to work on this project. o Data Analysis is complete using matplotlib, nltk sentiment analyzer and manual run-through to understand the observations listed in the dataset and its contribution towards the classification. o Preprocessing of data is done using nltk to setup training and test datasets, handle missing values, performing tokenization, removing stop words, lemmatization, encode categorical variables as needed. o Feature Selection to keep only the most relevant variables that are used for training. o Vectorization using sklearn libraries to map words to a corresponding vector of real numbers to find word similarities, etc. o Model design and training using several classification algorithms using sklearn libraries (Naive-Bayes, Decision tree and Logistic Regression so far) o Data and preliminary notebook are available in Github repo. 2) Which tasks are pending? o Models hyperparameter tuning and validation to assess the accuracy and avoid overfitting. o Performance evaluation of the different model algorithms used: compute and analyze the metrics precision, recall, F1 score, etc. o Create API/script that will take news text as input and generate its classification as real or fake as the result. o If time permits, will also try to add a submission of this notebook on Kaggle and evaluate accuracy against other submissions. Task Status Understand classification algorithms in depth and familiarize with nltk (I am new to machine learning world and will need to research/obtain a deeper understanding) Complete Environment setup Complete Data analysis and preprocessing Complete Feature selection and vectorization Complete Model design, training and hyperparameters tuning Model design and training complete; Tuning in progress Testing and evaluation In progress Integration with final output script/API To be done Prepare presentation To be done 3) Are you facing any challenges? o Nothing major at this time. I am relatively new to machine learning, NLTK, sklearn libraries. But there is good information available online and that has been very helpful thus far.
https://github.com/pinkychauhan89/FakeNewsClassifier	ProjectProposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Pinky Chauhan (pinkyc2) I will be working on this project individually. All administrative work will be done by me. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Fake news classification using machine learning Details/Task: I plan to build a system that is able to tell apart fake news from real news given some title and/or news content as the input. This is essentially a classification problem where I will train several models using machine learning on the following dataset from Kaggle: https://www.kaggle.com/hiteshkumargupta/fake-news-classification. These trained models will then predict the category of news item from a test dataset of fake/real news articles. Models will be evaluated based on performance metrics to choose the final model that will be used for predictions in the classifier script/API for final submission. Why fake news classifier? Fake news is becoming increasingly prevalent nowadays especially with the wide- spread usage of social media platforms which can be easily misused to propagate factually incorrect information to the users. With an average person spending many hours in each day coming across multiple posts, tweets, news articles, etc. while on social media, it becomes important to be able to segregate actual facts from cooked up fake stories/news. Such a tool can then be integrated with social media platforms to flag such articles or filter those out. It is an interesting problem to solve since fake news articles can come very close to the tone or style of the real news to make it sound authentic and hence not very easy to identify. Planned approach: I plan to divide the project into the following steps: * Data Analysis to understand the observations listed in the dataset and their contribution towards the classification. * Preprocessing to setup training and test datasets, handle missing values, performing tokenization, remove stop words, stemming, encode categorical variables. * Feature Selection to keep only the most relevant variables * Vectorization to map words to a corresponding vector of real numbers to find word similarities, etc. * Model design to train, tune hyperparameters, validation, test using several classification algorithms (XGBoost, Naive-Bayes, Decision tree, Linear classification, SVM, etc.) * Performance evaluation: compute and analyze the metrics precision, recall, F1 score, etc. * Create API/script that will take news text as input and generate it's classification as real or fake as the result. Tools/systems/datasets: Tools/Systems: I plan to leverage nltk for preprocessing tasks, numpy and pandas, sklearn for machine learning, matplotlib, etc. for this project. Dataset: https://www.kaggle.com/hiteshkumargupta/fake-news-classification The dataset comprises of 2 subsets: * train.csv with about 40000 observations for training the models and * test.csv with about 4000 observations for testing. The csv files comprise of the following columns: Index: Counter for each observation Title: Summary of the news article Text: body of the article Subject: Topic category of news article: political news, government news, etc. Date: Date of the news article Class: Only available in train.csv with labels as fake/real marked by users/contributors. Expected outcome: An API/python script/Jupiter notebook that can accept news title and/or text as input and output the category for the news item as fake news or real news. Evaluation: I will be training several models using different algorithms and evaluate their performances on the test dataset using precision, recall, F1 score, etc. 3. Which programming language do you plan to use? Python/Jupiter notebook 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Time Understand classification algorithms in depth and familiarize with nltk (I am new to machine learning world and will need to research/obtain a deeper understanding) 8 hours Environment setup 1 hour Data analysis and preprocessing 3 hours Feature selection and vectorization 2 hours Model design, training and hyperparameters tuning 10 hours Testing and evaluation 4 hours Integration with final output script/API 1 hour Prepare presentation 2 hours Total 31 hours
https://github.com/pinkychauhan89/FakeNewsClassifier	ProjectReport.pdf	Fake News Classifier: Pinky Chauhan University of Illinois at Urbana Champaign Overview: This objective of this project is to build a classifier system based on machine learning that is able to identify fake news from real/reliable news given a news title and/or news text content as the input. Such a tool can be integrated with social media platforms to flag potentially fake articles or filter those out. This is essentially a data categorization problem where I have trained several classifier models on the following dataset from Kaggle: https://www.kaggle.com/c/fake- news/data The dataset comprises of csv format files for training and testing with each file containing id, news title, text and author fields. The train.csv also has label field to categorize data as Reliable (label value 0) and Fake (label value 1) After evaluation based on various performance metrics, one of the models (in this case, Linear SVC over unigram bag-of-words/TF-IDF representation) is integrated in the final tester notebook to test with news data. The classifier takes a news article (title and text) as input and provides a prediction for the news article as either of the 2 categories: o Fake News o Reliable News Software Implementation Details: * Data Analysis: - Comprises of checking several attributes to evaluate their contribution towards classification. For such a classifier, the text and title of the news make obvious choices as features. - I also analyzed authors' distribution using pandas and polarity/sentiment differences using NLTK vader sentiment intensity analyzer library on the dataset. * Preprocessing of data: - Handling missing values by removing any rows with no text and title, preprocess data to remove any punctuations, remove any words with length 3 or less, stop words removal, tokenization and lemmatization using NLTK libraries * Feature selection: - Concatenated news title and text into article field and preprocessed it. Article comprises the feature to train the model * Vectorization - Different vector forms listed below have been used using NLTK vectorization/transformation libraries: o Term frequency (TF) based vector over unigrams bag of words representation o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams o Term frequency (TF) based vector over unigrams and bigrams o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams and bigrams o Term frequency/inverse document frequency (TF-IDF) based vector over unigrams, bigrams and trigrams * Training/hyperparameter tuning/validation using classification models: - Models used (sklearn libraries): o Naive bayes (With/without smoothing, TF vs TF-IDF vectors, Unigram/N-gram) o Logistic Regression (TF-IDF vectors using Unigrams/N-grams) o SVM using Linear SVC (TF-IDF vectors using Unigrams/N-grams, Regularization) o SGDC classifier (TF-IDF vectors using Unigrams/N-grams) o Decision Tree (TF-IDF vectors using Unigrams/N-grams) * Performance evaluation: - Compute and analyze metrics using sklearn metrics libraries o Precision (macro/micro), recall (macro/micro), F1 (macro/micro) o Classification Accuracy o Confusion matrix to see distribution of true/false positives/negatives - Select the best performing model based on evaluation results (SVM using Linear SVC using TF-IDF vector over unigrams) - Results: * Save/export trained model: - Using pipeline to specify all steps (vectorizer/classifier), fit training data and exporting model using joblib library accuracy precision(macro) precision(micro) recall(macro) recall(micro) f1_score(macro) f1_score(micro) Decision Tree (TFIDF/Uni-bi-trigram) 95.92% 0.959 0.959 0.959 0.959 0.959 0.959 Decision Tree (TFIDF/Uni-bigram) 95.60% 0.956 0.956 0.956 0.956 0.956 0.956 Decision Tree (TFIDF/Unigram) 92.81% 0.928 0.928 0.928 0.928 0.928 0.928 Linear SVC (TFIDF/Uni-bi-trigram) 95.87% 0.959 0.959 0.959 0.959 0.959 0.959 Linear SVC (TFIDF/Uni-bigram) 96.04% 0.96 0.96 0.96 0.96 0.96 0.96 Linear SVC (TFIDF/Unigram) 96.04% 0.96 0.96 0.96 0.96 0.96 0.96 Linear SVC (TFIDF/Unigram/Regularization) 87.27% 0.873 0.873 0.873 0.873 0.873 0.873 Logistic Regression (TFIDF/Uni-bi-trigram) 93.73% 0.937 0.937 0.937 0.937 0.937 0.937 Logistic Regression (TFIDF/Uni-bigram) 93.62% 0.936 0.936 0.936 0.936 0.936 0.936 Logistic Regression (TFIDF/Unigram) 94.27% 0.943 0.943 0.943 0.943 0.943 0.943 Multinomial naive bayes (TF/Uni-bigram/Smoothing) 92.44% 0.931 0.924 0.923 0.924 0.924 0.924 Multinomial naive bayes (TF/Unigram/NoSmoothing) 92.06% 0.923 0.921 0.92 0.921 0.92 0.921 Multinomial naive bayes (TF/Unigram/Smoothing) 89.75% 0.907 0.897 0.897 0.897 0.897 0.897 Multinomial naive bayes (TFIDF/Uni-bi-trigram/Smoothing) 75.46% 0.831 0.755 0.761 0.755 0.742 0.755 Multinomial naive bayes (TFIDF/Uni-bigram/Smoothing) 80.94% 0.861 0.809 0.808 0.809 0.802 0.809 Multinomial naive bayes (TFIDF/Unigram/NoSmoothing) 91.83% 0.921 0.918 0.918 0.918 0.918 0.918 Multinomial naive bayes (TFIDF/Unigram/Smoothing) 81.98% 0.865 0.82 0.818 0.82 0.813 0.82 SGDC (TFIDF/Uni-bi-trigram) 95.46% 0.955 0.955 0.954 0.955 0.955 0.955 SGDC (TFIDF/Uni-bigram) 95.65% 0.957 0.957 0.957 0.957 0.957 0.957 SGDC (TFIDF/Unigram) 95.56% 0.956 0.956 0.956 0.956 0.956 0.956 * Kaggle submission: - Predicted results for data in test.csv and submitted notebook/results to Kaggle (https://www.kaggle.com/pinkychauhan/fakenewsclassifierusingnltk-sklearn) - Accuracy: 94% * Create script (Jupyter notebook) that will take news text as input and generate classification as reliable news or fake news. Installation/Execution Details: Code is written using Jupyter notebook and python 3 Code structure: data: This directory contains the dataset from Kaggle (https://www.kaggle.com/c/fake- news/data). There are 3 files: o train.csv: To use for analysis, training, validation o test.csv: Test dataset for submission of results to Kaggle competition o submit.csv: File containing results/predictions for data in test.csv notebooks: This directory contains 2 notebooks: o FakeNewsClassifierTraining.ipynb: Jupyter notebook containing code/results for data analysis, cleanup, features set up, vectorization, training using various classifier algorithms, tuning and performance evaluation/comparison, model pipeline creation/export, prediction of results for test.csv for Kaggle submission o Tester.ipynb: This notebook loads the pretrained/exported model and predicts the category for a given news article. Use this notebook to test the classifier. model: This directory contains the pretrained model exported by FakeNewsClassifierTraining.ipynb notebook and loaded by Tester.ipynb results: This directory contains the summarized performance metrics from different models used for training and a copy of the submit.csv file generated from predictions for data in data/test.csv Code Setup: Install python 3 and Jupyter notebook Install the following python/machine learning libraries: o re: For regular expression matching o itertools: To iterate over data o pandas: For Data analysis/representation as Dataframes o nltk: Natural language toolkit o sklearn: For model selection, training, evaluation, export using pipeline o matplotlib: For visualization o joblib: For model export and load Checkout the project from main branch in Github Launch Jupyter notebook and navigate to the directory where project is checked out Tester.ipynb located in notebooks folder can be used for testing the classifier by providing values for title and text FakeNewsClassificationTraining.ipynb can also be executed to see all stages entailed in bulding the classifier and training/evaluation of different models Note: In case you see an issue around missing packages stopwords, punkt, vader_lexicon or wordnet, download them one time using below commands: nltk.download('vader_lexicon') nltk.download('punkt') nltk.download('stopwords') nltk.download('wordnet') References: https://www.kaggle.com/c/fake-news/data https://scikit-learn.org/stable/user_guide.html https://medium.com/datadriveninvestor/python-data-science-getting-started-tutorial- nltk-2d8842fedfdd https://matplotlib.org/tutorials/introductory/pyplot.html https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623
https://github.com/pinkychauhan89/FakeNewsClassifier	README.md	Fake News Classifier: By: Pinky Chauhan (University of Illinois at Urbana Champaign) This objective of this project is to build a classifier system based on machine learning that is able to identify fake news from real/reliable news given a news title and/or news text content as the input. Such a tool can be integrated with social media platforms to flag potentially fake articles or filter those out. This is essentially a data categorization problem where I have trained several classifier models on the following dataset from Kaggle: https://www.kaggle.com/c/fake-news/data After evaluation based on various performance metrics, one of the models (in this case, Linear SVC over unigram bag-of-words/TF-IDF representation) is integrated in the final tester notebook to test with news data. The classifier takes a news article (title and text) as input and provides a prediction for the news article as either of the 2 categories: - Fake News - Reliable News Final Project report/documentation: https://github.com/pinkychauhan89/CourseProject/blob/main/ProjectReport.pdf Presentation:https://github.com/pinkychauhan89/CourseProject/blob/main/Presentation.pptx
https://github.com/jacobvp2/CourseProject	ctm.pdf	"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/221653786 A cross-collection mixture model for comparative text mining Conference Paper * January 2004 DOI: 10.1145/1014052.1014150 * Source: DBLP CITATIONS 247 READS 208 3 authors, including: Some of the authors of this publication are also working on these related projects: Health News Quality View project Atulya Velivelli 15 PUBLICATIONS 370 CITATIONS SEE PROFILE Bei Yu Syracuse University 40 PUBLICATIONS 1,103 CITATIONS SEE PROFILE All content following this page was uploaded by Bei Yu on 31 July 2016. The user has requested enhancement of the downloaded file. A Cross-Collection Mixture Model for Comparative Text Mining ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Atulya Velivelli Department of Electrical and Computer Engineering University of Illinois at Urbana Champaign Bei Yu Graduate School of Library and Information Science University of Illinois at Urbana Champaign ABSTRACT In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent com- mon themes across all collections as well as summarize the similarity and differences of these collections along each com- mon theme. This general problem subsumes many interest- ing applications, including business intelligence and opinion summarization. We propose a generative probabilistic mix- ture model for comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algo- rithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. Categories and Subject Descriptors: H.3.3 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Comparative text mining, mixture models, clus- tering 1. INTRODUCTION Text mining is concerned with extracting knowledge and patterns from text [5, 6]. While there has been much re- search in text mining, most existing research is focused on one single collection of text. The goals are often to extract basic semantic units such as named entities, to extract rela- tions between information units, or to extract topic themes. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'04, August 22-25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00. In this paper, we study a novel problem of text mining re- ferred to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differ- ences of these collections along each common theme. Specif- ically, the task involves: (1) discovering the different com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. The need for comparative text mining exists in many different applica- tions, including business intelligence, summarizing reviews of similar products, and comparing different opinions about a common topic in general. In this paper, we study the CTM problem and propose a generative probabilistic mixture model for CTM. The model simultaneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The mixture model is based on component multinomial distribution models, each characterizing a different theme. The common themes and collection-specific themes are explicitly modeled. The pro- posed model can be estimated efficiently using the Expectation- Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. The rest of the paper is organized as follows. In Section 2, we briefly introduce the problem of CTM. We then present a baseline simple mixture model and a new cross-collection mixture model in Section 3 and Section 4. We discuss the experiment results in Section 5. 2. COMPARATIVE TEXT MINING 2.1 A motivating example With the popularity of e-commerce, online customer eval- uations are becoming widely provided by online stores and third-party websites. Pioneers like amazon.com and epin- ions.com have accumulated large amounts of customer input including reviews, comments, recommendations and advice, etc. For example, the number of reviews in epinions.com is more than one million[4]. Given a product, there could be up to hundreds of reviews, which is impossible for the readers to go through. It is thus desirable to summarize a collection of reviews for a certain type of products in order to provide the readers the most salient feedbacks from the peers. For review summarization, the most important task is to identify different semantic aspects of a product that the reviewers mentioned and to group the opinions accord- ing to these aspects to show similarities and differences in the opinions. For example, suppose we have reviews of three different brands of laptops (Dell, IBM, and Apple), and we want to summarize the reviews. A useful summary would be a tab- ular representation of the opinions as shown in Table 1, in which each row represents one aspect (subtopic) and differ- ent columns correspond to different opinions. Table 1: A tabular summary Subtopics Dell IBM Apple Battery life long enough short short Memory good bad good Speed slow fast fast It is, of course, very difficult, if not impossible to pro- duce such a table completely automatically. However, we can achieve a less ambitious goal - identifying the semantic aspects and identifying the common and specific character- istics of each product in an unsupervised way. This is a concrete example of comparative text mining. 2.2 The general problem The example above is only one of the many possible appli- cations of comparative text mining. In general, the task of comparative text mining involves: (1) discovering the com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. It is very hard to precisely define what a theme is, but it corresponds roughly to a topic or subtopic. The granularity of themes is application-specific. CTM is a fundamental task in ex- ploratory text analysis. In addition to opinion comparison and summarization, it has many other applications, such as business intelligence (comparing different companies), cus- tomer relationship management (comparing different groups of customers), and semantic integration of text (comparing component text collections). CTM is challenging in several ways: (1) It is a completely unsupervised learning task; no training data is available. (It is for the same reason that CTM can be very useful for many different purposes - it makes minimum assumptions about the collections and in principle we can compare any arbitrary partition of text.) (2) We need to identify themes across different collections, which is more challenging than identifying topic themes in one single collection. (3) The task involves a discrimination component - for each discov- ered theme, we also want to identify the unique information specific to each collection. Such a discrimination task is dif- ficult given that we do not have training data. In a way, CTM goes beyond the regular one-collection text mining by requiring an ""alignment"" of multiple collections based on common themes. Since no training data is available, in general, we must rely on unsupervised learning methods, such as clustering, to perform CTM. In this paper, we study how to use prob- abilistic mixture models to perform CTM. Below we first describe a simple mixture model for clustering, which repre- sents a straightforward application of an existing text min- ing method, and then present a more sophisticated mixture model specifically designed for CTM. 3. CLUSTERING WITH A SIMPLE MIXTURE MODEL   th  th  th "" $ % th ' th Figure 1: The Simple Mixture Model A naive solution to CTM is to treat the multiple collec- tions as one single collection and perform clustering. Our hope is that some clusters would represent the common themes across the collections, while some others would rep- resent themes specific to one collection (see Figure 1). We now present a simple multinomial mixture model for clus- tering an arbitrary collection of documents, in which we assume there are k latent common themes in all collections, and each is characterized by a multinomial word distribu- tion (also called a unigram language model). A document is regarded as a sample of a mixture model with these theme models as components. We fit such a mixture model to the union of all the text collections we have, and the obtained component multinomial models can be used to analyze the common themes and differences among the collections. Formally, let C = {C1, C2, ..., Cm} be m comparable col- lections of documents. Let th1, ..., thk be k theme unigram language models and thB be the background model for all the collections. A document d is regarded as a sample of the following mixture model (based on word generation). pd(w) = lBp(w|thB) + (1 - lB) k j=1 [pd,jp(w|thj)] where w is a word, pd,j is a document-specific mixing weight for the j-th aspect theme, and k j=1 pd,j = 1. lB is the mix- ing weight of the background model thB. The log-likelihood of all the collections C is log p(C|L) = m i=1 dCi wV [c(w, d) x log(lBp(w|thB) + (1 - lB) k j=1 (pd,jp(w|thj)))] where V is the set of all the words (i.e., vocabulary), c(w, d) is the count of word w in document d, and L = ({thj, pd,j}k j=1 is the set of all the theme model parameters. The purpose of using a background model is to ""force"" clustering to be done based on more discriminative words, leading to more informative and more discriminative component models. We control this effect through thB. The model can be estimated using any estimator. For example, the Expectation-Maximization (EM) algorithm [3] can be used to compute a maximum likelihood estimate with the following updating formulas: p(zd,w = j) = p(n) d,j p(n)(w|thj) k j'=1 p(n) d,j'p(n)(w|thj') p(zd,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j p(n)(w|thj) p(n+1) d,j = wV c(w, d)p(zd,w = j) j' wV c(w, d)p(zd,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,w = B))p(zd,w = j) w'V m i=1 dCi c(w', d)(1 - p(zd,w' = B))p(zd,w' = j) This mixture model is closely related to the probabilis- tic latent semantic indexing model (PLSI) proposed in [7] and treats CTM as a single-collection text mining problem. However, such a simple model is inadequate for CTM for two reasons: (1) We have completely ignored the structure of collections. As a result, we may have clusters that repre- sent only some, not all of the collections. (2) There is no easy way to identify which theme cluster represents the common information across collections and which represents specific information to a particular collection. Below we present a more sophisticated coordinated mixture model, which is specifically designed for CTM and addresses these two defi- ciencies. 4. CLUSTERING WITH A CROSS- COLLECTION MIXTURE MODEL  th th    th     th    th    th    th    th    th Figure 2: The Cross-Collection Mixture Model 4.1 The model Our main idea for improving the simple mixture model for comparative text mining is to explicitly distinguish com- mon theme clusters that characterize common information across all collections from special theme clusters that char- acterize collection-specific information. Thus we now con- sider k latent common themes as well as a potentially dif- ferent set of k collection-specific themes for each collection (illustrated in Figure 2). These component models directly correspond to all the information we are interested in discov- ering. The sampling distribution of a word in document d (from collection Ci) is now collection-specific. Specifically, it involves the background model (thB), k common theme models (th1, ..., thk), and k collection-specific theme models (th1,i, ..., thk,i), which are to capture the unique information about the k themes in collection Ci. That is, pd(w|Ci) = (1 - lB) k j=1 [pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))] +lBp(w|thB) where lB is the weight on the background model thB and lC is the weight on the common theme model thj (as opposed to the collection-specific theme model thj,i). Intuitively, when we ""generate"" a word, we first decide whether to use the background model thB according to lB; the larger lB is, the more likely we will use thB. If we decide not to use thB, then we need to decide which theme to use; this is controlled by pd,j, the probability of using theme j when generating words in d. Finally, once we decide which theme to use, we still need to decide whether we should use the common theme model or the collection-specific theme model, and this is con- trolled by lC, the probability of using the common model. The weighting parameters lB and lC are intentionally to be set by the user, and their interpretation is as follows. lB reflects our knowledge about how noisy the collections are. If we believe the text is verbose, then lB should be set to a larger value. In our experiments, a value of 0.9 - 0.95 often works well. lC indicates our emphasis on the commonality, as opposed to the speciality in comparative text mining. A larger lC would allow us to learn a richer common theme model, whereas a smaller one would learn a weaker com- mon theme model, but stronger special models. The optimal value depends on the specific applications. According to this generative model, the log-likelihood of the whole set of collections is log p(C) = m i=1 dCi wV [c(w, d) log[lBp(w|thB) +(1 - lB) k j=1 pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))]] 4.2 Parameter estimation We estimate the background model thB using all the avail- able text in the m text collections. That is, ^p(w|thB) = m i=1 dCi c(w, d) m i=1 dCi w'V c(w', d) Since lB and lC are set manually, this leaves us with the following parameters to estimate: (1) the common theme models, th = {th1, ..., thk}; (2) the special theme models for each collection Ci, thCi = {th1,i, ..., thk,i}; and (3) the theme mixing weights for each document d: pd = {pd,1, ..., pd,k}. p(zd,Ci,w = j) = p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) k j'=1 p(n) d,j'(lCp(n)(w|thj') + (1 - lC)p(n)(w|thj',i)) p(zd,Ci,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) p(zd,Ci,j,w = C) = lCp(n)(w|thj) lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i) p(n+1) d,j = wV c(w, d)p(zd,Ci,w = j) j' wV c(w, d)p(zd,Ci,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)p(zd,Ci,j,w = C) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)p(zd,Ci,j,w' = C) p(n+1)(w|thj,i) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)(1 - p(zd,Ci,j,w = C)) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)(1 - p(zd,Ci,j,w' = C)) Figure 3: EM updating formulas for the cross-collection mixture model As in the simple mixture model, we can also use the EM algorithm to compute a maximum likelihood estimate. The updating formulas are shown in Figure 3. Each EM iteration involves scanning all the text once, so the algorithm is quite scalable. 4.3 Using the model Once the model is estimated, we will have k collection- specific models for each of the m collections and k common theme models across all collections. Each of these mod- els is a word distribution or unigram language model. The high probability words can characterize the theme/cluster extracted. Such words can often be used directly as a sum- mary or indirectly (e.g., through a hidden Markov model) to extract relevant sentences to form a summary of the cor- responding theme. The extracted word distributions can also be used in many other ways, e.g., to classify other text documents or to link the related passages in the text collec- tions so that a user can navigate the information space for comparative analysis. We can input our bias for CTM through setting lB and lC manually. Specifically, lB allows us to input our knowledge about the noise (stop words) in the data - if we know the text data is verbose, then we should set lB to a high value, whereas if the data is concise and mostly content-bearing keywords, then we need to set lB to a smaller value. Sim- ilarly, lC allows us to input a trade-off between extracting common theme models (setting lC to a higher value) vs. ex- tracting collection-specific models (setting lC to a smaller value). Such biases cannot be learned by the maximum like- lihood estimator. Indeed, maximizing the data likelihood is only a means to achieve our ultimate goal, which is why we want to regularize our model in a meaningful way so that we can impose certain preferences while maximizing the data likelihood. The flexibility and control provided by lB and lC make it possible for a user to control the focus of the results of comparative text mining. 5. EXPERIMENTS AND RESULT ANALYSIS We evaluated the Simple Mixture model (SimpMix) and the Cross-Collection Mixture model (CCMix) on two do- mains - war news and laptop reviews. 5.1 War news The War news data consists of news excerpts on two com- parable events: (1) Iraq war and (2) Afghanistan war, both of which occurred in the last two years. The Iraq war news excerpts were a combination of 30 articles from the CNN and BBC web sites over the last one year span. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov. 2001. Our goal is to compare these two wars and find out their common and specific characteristics. The results of using either the simple mixture model or the cross-collection mixture model are shown in Table 2, where the top words of each theme model are listed along with their probabilities. We set lB = 0.95 for SimpMix and set lb = 0.9, lC = 0.25 for CCMix; in both cases, the number of clusters is fixed to 5. Variations of these parameters are discussed later. We see that although there are some interesting themes in the results of SimpMix (e.g., cluster3 and cluster4 appear to be about American and British inquiry into the pres- ence of weapons in Iraq, respectively, while cluster2 suggests the presence of British soldier in Basra, a town in southern Iraq), they are all about Iraq war. We do not see any obvi- ous theme common to both Iraq war and Afghanistan war. This is expected given that SimpMix pools all documents together without exploiting the collection structure. In contrast, the results of CCMix explicitly suggest the common themes and the corresponding collection-specific themes. For example, cluster3 clearly suggests that in both wars, there has been loss of lives. Furthermore, the top words in the corresponding Iraq theme include names of some key defense people that are involved in the Iraq war (e.g., ""Hoon"" is the last name of the british defense secre- tary and ""Sanchez"" is the last name of the U.S General in Iraq). In comparison, the top words in the corresponding Afghanistan theme includes the name of the U.S Defense secretary who had an important role in the Afghan war. Cluster4 and cluster5 are also meaningful themes. The common theme captured in Cluster4 is the Monday briefings by an official spokesman of a political administration during both wars; the corresponding special themes indicate the dif- ference in the topics discussed in the briefings (e.g., weapon inquiry for Iraq war and Bin Laden for Afghanistan war). The common theme of Cluster5 is about the diplomatic role Table 2: War news results using SimpMix model (top) vs. CCMix model (bottom) Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common will 0.019 british 0.017 weapons 0.022 inquiry 0.052 countries 0.026 theme let 0.012 soldiers 0.015 kay 0.021 intelligence 0.036 contracts 0.023 words united 0.012 baghdad 0.015 rumsfeld 0.017 dossier 0.024 allawi 0.012 god 0.011 air 0.011 commission 0.014 hutton 0.021 hoon 0.012 inspectors 0.011 basra 0.011 group 0.014 claim 0.019 russian 0.010 your 0.010 mosque 0.010 senate 0.011 wmd 0.019 international 0.010 nation 0.010 southern 0.01 survey 0.010 mps 0.018 russia 0.009 n 0.010 fired 0.010 paper 0.010 committee 0.017 reconstruction 0.009 Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common us 0.042 mr 0.029 killed 0.036 monday 0.036 united 0.042 theme nation 0.030 marines 0.025 month 0.032 official 0.032 nations 0.04 words will 0.024 dead 0.023 deaths 0.023 i 0.029 with 0.03 action 0.022 general 0.022 one 0.023 would 0.028 is 0.025 re 0.022 defense 0.019 died 0.022 where 0.025 it 0.024 border 0.019 key 0.018 been 0.022 do 0.025 they 0.023 its 0.017 since 0.018 drive 0.018 spokesman 0.022 diplomatic 0.023 ve 0.016 first 0.016 according 0.015 political 0.021 blair 0.022 Iraq god 0.022 iraq 0.022 troops 0.016 intelligence 0.049 n 0.03 theme saddam 0.016 us 0.021 hoon 0.015 weapons 0.034 weapons 0.024 words baghdad 0.013 baghdad 0.017 sanchez 0.012 inquiry 0.028 inspectors 0.023 your 0.012 nato 0.015 billion 0.01 commission 0.017 council 0.016 live 0.01 iraqi 0.013 spokeswoman 0.008 independent 0.016 declaration 0.015 Afghan paper 0.021 story 0.028 taleban 0.026 bin 0.031 northern 0.040 theme afghan 0.019 full 0.026 rumsfeld 0.020 laden 0.031 alliance 0.040 words meeting 0.014 saturday 0.016 hotel 0.012 steinberg 0.027 kabul 0.030 euro 0.012 e 0.015 front 0.011 taliban 0.023 taleban 0.025 highway 0.012 rabbani 0.012 dropped 0.010 chat 0.019 aid 0.020 played by the United Nations (UN). The corresponding spe- cial themes again suggest the difference between the two wars. The Iraq theme indicates the role of UN in sending weapon inspectors to Iraq; the Afghanistan theme refers to Northern Alliance that received aid from the UN and came to power in Afghanistan after the defeat of Taliban. 5.2 Laptop customer reviews This data set was constructed to test our models for com- paring opinions of customers on different laptops. We man- ually downloaded the following 3 review sets from epin- ions.com [4], filtering out the misplaced ones: Apple iBook (M8598LL/A) Mac Notebook (34 reviews), Dell Inspiron 8200 (8TWORH) PC Notebook (22 reviews), IBM ThinkPad T20 2647 (264744U) PC Notebook (42 reviews). The results on this data set are generally similar to those on war news. Due to the limit of space, we only show the CCMix results in Table 3, which are obtained by setting lC=.7 and lB=.96 and fixing the number of clusters to 8. Here we again see many very interesting common themes; in- deed, the top two words in the common themes can provide a very good summary of the themes (e.g., ""sound and speak- ers"" for cluster1, ""battery hours"" for cluster5, and ""Mi- crosoft Office"" for cluster8). However, the special themes, although suggesting some differences among the three lap- tops, are much harder to interpret. This may be because there is a great deal of variation in product-specific opin- ions in the data, which makes the data extremely sparse for learning a coherent collection-specific theme for each of the eight themes. 5.3 Parameter tuning When we vary lB and lC in CCMix, the results are gen- erally different. Specifically, when lB is set to a small value, non-informative stop words tend to show up in common themes. A reasonable value for lB is generally higher than 0.9 - in that case, the model automatically eliminates the non-informative words from the theme clusters, allowing for more discriminative clustering. Indeed, in all our experi- ments, we have intentionally retained all the stop words, and the model is clearly able to filter out non-informative words, though in some cases, they still show up as top words in the common themes of the news data. They can be ""eliminated"" by using an even higher lB, but then we may end up having insufficient information to learn a common theme reliably. lC affects the vocabulary allocation between the common and collection-specific themes. In the news data experiments, when we change lC to a value above 0.4, the collection-specific terms would dominate the common theme models. In the laptop data experiments, when lC is less than 0.7, we lose many content keywords of the com- mon themes to the corresponding collection-specific themes. Both lB and lC are intentionally left for a user to tune so that we can incorporate application-specific bias into the model. 6. RELATED WORK The most related work to our work is the coupled clus- tering method presented in [8], which appears to be one of the very few studies considering the clustering problem in multiple collections. They extend the information bottle- neck approach to discover common clusters across different collections. Comparative text mining goes beyond this by analyzing both the similarities and collection-specific differ- ences. We also use a completely different approach based on probabilistic mixture models. Another related work is [10], where cross-training is used for learning classifiers from mul- tiple document sets. Our work differs from it in that we per- form unsupervised learning. The aspect models studied in [7, 2] are also related to our work but they are closer to our baseline model and are not designed for comparing multiple collections. There are many studies in document clustering [1]. Again, the difference lies in that they consider only one collection and thus are similar to the baseline model. Our work is also related to document summarization, es- pecially multiple document summarization (e.g.,[9, 12]). In- deed, we can the results of CTM as a special form of sum- mary of multiple text collections. However, an important difference is that while a summary intends to retain the ex- plicit information in text (to maintain fidelity), CTM aims at extracting non-obvious implicit patterns. 7. CONCLUSIONS AND FUTURE WORK In this paper, we define and study a novel text mining problem referred to as comparative text mining. It is con- Table 3: Laptop review results using CCMix model Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Cluster6 Cluster7 Cluster8 C sound 0.035 port 0.023 ram 0.105 m 0.027 battery 0.129 t 0.039 cd 0.095 office 0.037 O speakers 0.035 jack 0.021 mb 0.037 trackpad 0.018 hours 0.080 modem 0.017 drive 0.076 microsoft 0.021 M playback 0.034 ports 0.018 memory 0.034 chip 0.013 life 0.060 internet 0.017 rw 0.055 little 0.018 M feel 0.019 will 0.018 256mb 0.027 improved 0.012 5 0.038 later 0.014 dvd 0.049 basic 0.015 O pros 0.017 your 0.017 128mb 0.021 volume 0.012 end 0.016 configuration 0.014 combo 0.025 6 0.014 N cons 0.017 warm 0.013 tech 0.020 did 0.011 3 0.016 free 0.013 drives 0.023 under 0.013 market 0.017 keep 0.012 128 0.020 latch 0.011 high 0.015 vga 0.012 rom 0.020 mhz 0.012 size 0.014 down 0.012 support 0.018 make 0.010 processor 0.014 were 0.012 floppy 0.017 word 0.011 D rests 0.026 banias 0.019 options 0.039 inspiron 0.061 dells 0.032 fans 0.019 apoint 0.017 0 0.046 E palm 0.022 svga 0.014 sodimm 0.025 pentium 0.052 ran 0.017 shipping 0.017 blah 0.015 angle 0.018 L 9000 0.020 record 0.014 eraser 0.021 8200 0.03 prong 0.015 2nd 0.016 hook 0.011 portion 0.0154 L smart 0.018 supposedly 0.013 crucial 0.018 toshiba 0.027 requiring 0.014 tracking 0.015 tug 0.011 usb 0.0153 reader 0.018 rebate 0.013 sdram 0.018 440 0.026 second 0.011 spoke 0.015 2499 0.011 specials 0.014 A magazine 0.011 osx 0.040 macos 0.019 macos0.016 g4 0.016 iphoto 0.031 airport 0.075 appleworks 0.060 P ipod 0.010 quartz 0.015 personal 0.018 netscape 0.013 interlaced 0.016 itunes 0.027 burn 0.035 word 0.021 P strong 0.01 instance 0.014 shield 0.016 apache 0.009 mac 0.016 import 0.021 4x 0.018 result 0.016 L icon 0.009 underneath 0.012 airport 0.016 ie5 0.008 imac 0.014 book 0.018 reads 0.014 spreadsheet 0.013 E choppy 0.008 cooling 0.012 installation 0.015 ll 0.008 powermac 0.012 quicktime 0.016 schools 0.013 excel 0.012 I technology 0.023 rj 0.033 exchange 0.023 company 0.021 thinkpad 0.077 thinkpads 0.020 t20 0.04 list 0.015 B outdated 0.020 chik 0.018 hassle 0.016 570 0.017 ibm 0.047 connector 0.018 ultrabay 0.030 factor 0.013 M surprisingly 0.018 dsl 0.017 disc 0.015 turn 0.017 covers 0.029 connectors 0.018 tells 0.021 months 0.013 trackpoint 0.014 45 0.015 t23 0.012 buttons 0.015 lightest 0.028 bluetoot 0.018 device 0.021 cap 0.013 recommend 0.013 pacbell 0.012 cdrw 0.015 numlock 0.012 3000 0.027 sturdy 0.011 number 0.020 helpdesk 0.0128 cerned with discovering any latent common themes across a set of comparable collections of text as well as summariz- ing the similarities and differences of these collections along each theme. We propose a generative cross-collection mixture model for performing comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. We define the model and present the EM algorithm that can estimate the model ef- ficiently. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method based on a simple mixture model. Experiment results show that the cross-collection mixture model is quite effective in dis- covering the latent common themes across collections and performs significantly better than the baseline simple mix- ture model. The proposed model has many obvious applica- tions in opinion summarization and business intelligence. It also has many other less obvious applications in the general area of text mining and semantic integration of text. For example, our model can be used to compare the course web pages from the major computer science department web sites to discover core computer science topics. It can also be used to compare literature collections in different communities to support concept switching [11]. The work reported in this paper is just an initial step toward a promising new direction. There are many interest- ing future research directions. First, it may be interesting to explore how we can further improve the CCMix model and its estimation. One interesting direction is to explore the Maximum A Posterior (MAP) estimator, which would allow us to incorporate more prior knowledge in a princi- pled way. For example, a user may already have certain thematic aspects in mind. With MAP estimation, we can easily add that bias to the component models. Second, we can generalize our model to model semi-structured data to perform more general comparative data mining. One way to achieve this goal is to introduce additional random variables in each component model so that we can model any struc- tured data. Finally, it would be very interesting to explore how we could exploit the learned theme models to provide additional help to a user who wants to perform comparative analysis. For example, the learned common theme models can be used to construct a hidden Markov model (HMM) to identify the parts in the text collections about the common themes, and to connect them through automatically gener- ated hyperlinks. This would allow a user to easily navigate through the common themes. 8. REFERENCES [1] D. Baker and A. McCallum. Distributional clustering of words for text classification. In Proceedings of ACM SIGIR 1998, 1998. [2] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statist. Soc. B, 39:1-38, 1977. [4] epinions.com, 2003. http://www.epinions.com/. [5] R. Feldman and I. Dagan. Knowledge discovery in textual databases. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, 1995. [6] M. A. Hearst. Untangling text data mining. In Proceedings of ACL'99, 1999. [7] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR'99, pages 50-57, 1999. [8] Z. Marx, I. Dagan, J. Buhmann, and E. Shamir. Coupled clustering: a method for detecting structural correspondence. Journal of Machine Learning Research, 3:747-780, 2002. [9] K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. E. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI-99. [10] S. Sarawagi, S. Chakrabarti, and S. Godbole. Cross-training: Learning probabilistic mappings between topics. In Proceedings of ACM SIGKDD 2003. [11] B. R. Schatz. The interspace: Concept navigation across distributed communities. Computer, 35(1):54-62, 2002. [12] H. Zha. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of ACM SIGIR 2002. View publication stats View publication stats"
https://github.com/jacobvp2/CourseProject	Progress.pdf	Team GOAT! (Getting Our Act Together) Reproducing Paper: A cross-collection mixture model for comparative text mining 1: Progress As a group we have decided to choose a rather large dataset from the New York Times regarding the presidential election. Our paper that we are modeling also used the Expectation- Maximization (EM) algorithm to evaluate a model on two different text data sets one being a news article data set. We are currently doing more research on the EM algorithm as this was a rather recent lesson in our class. Obviously, we have a GitHub repository to collaborate on for our coding portions. Also using Google Docs for the paper creation. 2: Remaining Tasks We have to implement our algorithm and then use an experiment to verify our implementation. 3: Challenges/Issues Faced Unfortunately, one of our group members lost a family member over the break, so we had to reallocate some of the workload recently. Other than that, we are having issues figuring out helpful libraries and strategies so that our EM algorithm can work on large datasets.
https://github.com/jacobvp2/CourseProject	README.md	"CS410 Course Project Team GOAT Background ## Reproducing a Paper. For our final project, we attempted to reproduce results from the (contexual text mining) research paper listed below: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 The actual paper has been included in our repository for your own reference. Setup/Dependencies ## This repo assumes that the user has Python3 as well as Pip. If not, they can be found here. Our project has two package dependencies, newsapi and numpy. Using the command ""pip install -r requirements.txt"" the user should be able to install the correct versions of both packages. At this point, cd into directory /CourseProject/, run mixture.py, and follow the prompts in order to run our implementation of the Collective Text Mining comparison model. Data ## Our data is rather fluid in this case. We have an example of our demonstration video in a youtube link here. In an attempt to keep the data as similar as possible to the original paper, please use terms ""Iraq"" and ""Afghanistan"" when prompted."
https://github.com/jacobvp2/CourseProject	TEAM GOAT.pdf	TEAM GOAT: PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. *Captain Bolded 2. Which paper have you chosen? ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? Yes
https://github.com/tgw4uiuc/CourseProject	metapy_tutorial.md	"metapy: (experimental) Python bindings for MeTA diff - Author tgw4's note regarding attribution: I created the OS - specific sections of this, and the note regarding python 2.7 and - 3.4-3.7 being easier to use, the rest was created by the - previous tutorial author(s) who created https://github.com/meta-toolkit/metapy/blob/master/README.md. This project provides Python (2.7 and 3.x are supported) bindings for the MeTA toolkit. They are still very much under construction, but the goal is to make it seamless to use MeTA's components within any Python application (e.g., a Django or Flask web app). This project is made possible by the excellent pybind11 library. Outline Generic Instructions OS Specific Instructions Chromebook Ubuntu 20.04LTS CentOS 8.2.2004 Windows 10 Note that metapy will be much easier to install in python verisons 2.7 or 3.4 through 3.7, while later versions require compiling all the source code. Generic setup notes (the easy way) ```bash Ensure your pip is up to date pip install --upgrade pip install metapy! pip install metapy ``` This should work on Linux, OS X, and Windows with pretty much any recent Python version >= 2.7. On Linux, make sure to update your pip to version 8.1 (or newer) so you can install from a binary package---this will save you a lot of time. Compiling it yourself (the hard way) You will, of course, need Python installed. You will also need its headers to be installed as well, so look for a python-dev or similar package for your system. Beyond that, you'll of course need to satisfy the requirements for building MeTA itself. This repository should have everything you need to get started. You should ensure that you've fetched all of the submodules first, though: bash git submodule update --init --recursive Once that's done, you should be able to build the library like so: bash mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release make You can force building against a specific version of Python if you happen to have multiple versions installed by specifying -DMETAPY_PYTHON_VERSION=x.y when invoking cmake. The module should be written to metapy.so in the build directory. OS Specific Instructions Chromebook Setup Recently released Chromebooks (in the last couple of years as of December 2020) have a option to run a linux beta. Metapy can be installed on these Chromebooks. Check here for a list of compatible Chromebooks: https://www.chromium.org/chromium-os/chrome-os-systems-supporting-linux If your Chromebook is compatible, see this page to turn on the linux beta mode: Chomebook Linux Beta Mode Once you have Linux Beta installed and working, these are the steps to install metapy: Open a linux terminal then: ``` update package lists sudo apt-get update Install python3.7 sudo apt-get install python3.7 install pip sudo apt-get install python3-pip update pip pip3 install --upgrade pip install metapy pip3 install metapy ``` That's it, metapy is now installed. Ubuntu Setup Version 20.04 LTS Ubuntu 20.04 LTS comes wiht python 3.8 installed, which will not make it easy to install metapy. The easy way: (using a previous version of python) Install python version 3.4 through 3.7 from python.org. Make python3.7 your active version. See the instructions here for help switching versions: https://linuxconfig.org/ubuntu-20-04-python-version-switch-manager Now install pip: ``` install pip sudo apt-get install python3-pip update pip pip3 install --upgrade pip install metapy pip3 install metapy ``` That's it, metapy is now installed! The hard way: (sticking with python 3.8) First lets update the list of packages available to install: ``` update package lists sudo apt update install pip sudo apt install python3-pip update pip pip3 install --upgrade pip make an install file directory mkdir metapy cd metapy pip3 download metapy ls (to see the filename it downloaded .. should be metapy(something).tar.gz tar -xvf metapy-0.2.13.tar.gz (or whatever your filename is you downloaded above) ls (to see the name of the directory it created) cd (directory name from above, will be metapy-(something)) cd deps mkdir icu-61.1 now download the ""icu4c-61_1-src.tgz"" file from https://github.com/unicode-org/icu/releases/tag/release-61-1 and copy it to the icu-61.1 directory you created above. move back up to the directory above the meta directory cd .. cd .. cd .. when you 'ls' you should see ""metapy"" as one of the subdirectories install metapy pip3 install metapy (this will takeawhile as the metapy package is compiled). ``` Once its completed, that's it, metapy is now installed. CentOS Setup Version 8.2.2004 Centos is quite easy, as python 3.6 and pip are installed by default. All you need to do is: Make sure your user account is in the ""sudoers"" group, see this tutorial here: https://linuxize.com/post/how-to-add-user-to-sudoers-in-centos/ Then to install metapy: sudo pip3 install --upgrade pip sudo pip3 install metapy That's it, you now have metapy installed! Windows 10 Setup First, be sure to have python version 2.7 or 3.4-3.7 installed. To check if python is installed, open a command prompt window an type: `` check if proper version of python is installed python --version `` If it is not, download it from python.org (be sure to grab one of the versions mentioned above, not necessarily the latest version.) Once python is installed, check if pip is installed: ``` check for pip pip --version ``` If pip is not installed, get it from https://pypi.org/project/pip/. Once pip is installed, upgrade it to the latest version, and install metapy: ``` upgrade pip pip install --upgrade pip install metapy pip install metapy ``` That's it, you now have metapy installed!"
https://github.com/tgw4uiuc/CourseProject	MeTA_tutorial.md	"MeTA: ModErn Text Analysis Please visit our web page for information and tutorials about MeTA! Build Status (by branch) master: develop: diff - Author tgw4's note regarding attribution: I created the - following OS specific sections of this tutorial: - Chromebook - Ubuntu 20.04 LTS - CentOS 8.2.2004 - And also added the icu4c and xlocale.h info to the ""Generic Setup Notes"" as well as some general editing and reordering. - The rest was created by the previous tutorial author(s) who created https://github.com/meta-toolkit/metapy/blob/master/README.md. Outline Intro Documentation Tutorials Citing Project Setup Generic Setup Notes Mac OS X Chromebook Ubuntu Arch Linux Fedora CentOS EWS/EngrIT (this is UIUC-specific) Windows Intro MeTA is a modern C++ data sciences toolkit featuring text tokenization, including deep semantic features like parse trees inverted and forward indexes with compression and various caching strategies a collection of ranking functions for searching the indexes topic models classification algorithms graph algorithms language models CRF implementation (POS-tagging, shallow parsing) wrappers for liblinear and libsvm (including libsvm dataset parsers) UTF8 support for analysis on various languages multithreaded algorithms Documentation Doxygen documentation can be found here. Tutorials We have walkthroughs for a few different parts of MeTA on the MeTA homepage. Citing If you used MeTA in your research, we would greatly appreciate a citation for our ACL demo paper: latex @InProceedings{meta-toolkit, author = {Massung, Sean and Geigle, Chase and Zhai, Cheng{X}iang}, title = {{MeTA: A Unified Toolkit for Text Retrieval and Analysis}}, booktitle = {Proceedings of ACL-2016 System Demonstrations}, month = {August}, year = {2016}, address = {Berlin, Germany}, publisher = {Association for Computational Linguistics}, pages = {91--96}, url = {http://anthology.aclweb.org/P16-4016} } Project setup Generic Setup Notes There are rules for clean, tidy, and doc. After you run the cmake command once, you will be able to just run make as usual when you're developing---it'll detect when the CMakeLists.txt file has changed and rebuild Makefiles if it needs to. To compile in debug mode, just replace Release with Debug in the appropriate cmake command for your OS above and rebuild using make after. Note: as of December 2020, the icu4c repository moved from icu-porject.org to github.com/unicode-org/icu. MeTa requires icu4c to compile, so you will likely need to download it manually to allow MeTa to successfully compile. The Ubuntu 20.04 LTS, Centos 8.2.2004, and Chromebook sections were updated in December 2020, and address this specifically, but for other OS versions, as a general guide the process is (do this before you do the final ""make"" step): `Download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory where you've downloaded the MeTa source. Copy it to the meta/deps/icu-58.2/ directory (create those deps and icu-58.2 directories if they don't exist yet.) One other thing that may cause issues in newer OS distributions: The MeTa source uses xlocale.h, which is no longer included in many newer OS distributions. We can use locale.h instead, so this will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Don't hesitate to reach out on the forum if you encounter problems getting set up. We routinely build with a wide variety of compilers and operating systems through our continuous integration setups (travis-ci for Linux and OS X and Appveyor for Windows), so we can be fairly certain that things should build on nearly all major platforms. Mac OS X Build Guide Mac OS X 10.6 or higher is required. You may have success with 10.5, but this is not tested. You will need to have homebrew installed, as well as the Command Line Tools for Xcode (homebrew requires these as well, and it will prompt for them during install, or you can install them with xcode-select --install on recent versions of OS X). Once you have homebrew installed, run the following commands to get the dependencies for MeTA: bash brew update brew install cmake jemalloc lzlib icu4c To get started, run the following commands: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=clang++ cmake ../ -DCMAKE_BUILD_TYPE=Release -DICU_ROOT=/usr/local/opt/icu4c make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Chromebook Build Guide Recently released Chromebooks (in the last couple of years as of December 2020) have a option to run a linux beta. MeTa can be built on these Chromebooks. Check here for a list of compatible Chromebooks: https://www.chromium.org/chromium-os/chrome-os-systems-supporting-linux If your Chromebook is compatible, see this page to turn on the linux beta mode: Chomebook Linux Beta Mode Once you have Linux Beta installed and working, these are the steps to install Meta: ``` Need gcc/g++-7, gcc/g++-8 or later will not work sudo apt-get update sudo apt-get install gcc-7 g++-7 next update the system and get needed files sudo apt-get update sudo apt-get install software-properties-common install dependencies sudo apt-get install cmake libicu-dev git libjemalloc-dev zlib1g-dev ``` Now we need to make sure that we use gcc/g++-7 and not the newer version that is installed by default on the system. (You may want to remove these links after you are finished successfully building MeTa). ``` sudo ln -s /usr/bin/gcc-7 /usr/local/bin/gcc sudo ln -s /usr/bin/g++-7 /usr/local/bin/g++ ``` Now quit and restart the linux beta terminal window. Next lets download the MeTa files: ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project cd .. (make sure you are in the meta/build/ directory) cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu Build Guide The directions here depend greatly on your installed version of Ubuntu. To check what version you are on, run the following command: bash cat /etc/issue Based on what you see, you should proceed with one of the following guides: Ubuntu 20.04 LTS Build Guide Older Ubuntu Versions (no longer supported, instructions remain for reference purposes) - Ubuntu 15.10 Build Guide - Ubuntu 14.04 LTS Build Guide - Ubuntu 12.04 LTS Build Guide Ubuntu 20.04 LTS Build Guide Update the list of available packages, and then install gcc-7, g++-7 and other prerequisites. sudo apt update sudo apt install gcc-7 sudo apt install g++-7 sudo apt install git cmake make libjemalloc-dev zlib1g-dev Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project, make sure we're using gcc/g++ version 7 cmake ../ -DCMAKE_BUILD_TYPE=Release -DCMAKE_C_COMPILER=gcc-7 -DCMAKE_CXX_COMPILER=g++-7 make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 15.10 Build Guide Ubuntu's non-LTS desktop offering in 15.10 has enough modern software in its repositories to build MeTA without much trouble. To install the dependencies, run the following commands. bash apt update apt install g++ git cmake make libjemalloc-dev zlib1g-dev Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 14.04 LTS Build Guide Ubuntu 14.04 has a recent enough GCC for building MeTA, but we'll need to add a ppa for a more recent version of CMake. Start by running the following commands to install the dependencies for MeTA. ```bash this might take a while sudo apt-get update sudo apt-get install software-properties-common add the ppa for cmake sudo add-apt-repository ppa:george-edison55/cmake-3.x sudo apt-get update install dependencies sudo apt-get install g++ cmake libicu-dev git libjemalloc-dev zlib1g-dev ``` Once the dependencies are all installed, you should double check your versions by running the following commands. bash g++ --version should output g++ (Ubuntu 4.8.2-19ubuntu1) 4.8.2 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should output cmake version 3.2.2 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Ubuntu 12.04 LTS Build Guide Building on Ubuntu 12.04 LTS requires more work than its more up-to-date 14.04 sister, but it can be done relatively easily. You will, however, need to install a newer C++ compiler from a ppa, and switch to it in order to build meta. We will also need to install a newer CMake version than is natively available. Start by running the following commands to get the dependencies that we will need for building MeTA. ```bash this might take a while sudo apt-get update sudo apt-get install python-software-properties add the ppa that contains an updated g++ sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt-get update this will probably take a while sudo apt-get install g++ g++-4.8 git make wget libjemalloc-dev zlib1g-dev wget http://www.cmake.org/files/v3.2/cmake-3.2.0-Linux-x86_64.sh sudo sh cmake-3.2.0-Linux-x86_64.sh --prefix=/usr/local ``` During CMake installation, you should agree to the license and then say ""n"" to including the subdirectory. You should be able to run the following commands and see the following output: bash g++-4.8 --version should print g++-4.8 (Ubuntu 4.8.1-2ubuntu1~12.04) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash /usr/local/bin/cmake --version should print cmake version 3.2.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project (set C and CXX flags to use the gcc/g++ 7 compiler we installed earlier. CXX=g++-4.8 /usr/local/bin/cmake ../ -DCMAKE_BUILD_TYPE=Release -D CMAKE_C_COMPILER=gcc-7 -D CMAKE_CXX_COMPILER=g++-7 make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Arch Linux Build Guide Arch Linux consistently has the most up to date packages due to its rolling release setup, so it's often the easiest platform to get set up on. To install the dependencies, run the following commands. bash sudo pacman -Sy sudo pacman -S clang cmake git icu libc++ make jemalloc zlib Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=clang++ cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Fedora Build Guide This has been tested with Fedora 22+ (the oldest currently supported Fedora as of the time of writing). You may have success with earlier versions, but this is not tested. (If you're on an older version of Fedora, use yum instead of dnf for the commands given below.) To get started, install some dependencies: ```bash These may be already installed sudo dnf install make git wget gcc-c++ jemalloc-devel cmake zlib-devel ``` You should be able to run the following commands and see the following output: bash g++ --version should print g++ (GCC) 5.3.1 20151207 (Red Hat 5.3.1-2) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should print cmake version 3.3.2 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system with the following command: bash ./unit-test --reporter=spec CentOS Build Guide CentOS 8.2.2004 The first step is to install gcc/g++ 7.5.0 (or any version between 4.8.5 and 7.5.0, inclusive). MeTa won't compile properly with gcc 8.0 or higher. Once that is installed, this is how to setup MeTa: sudo yum install git cmake make libjemalloc-dev zlib1g-dev ``` clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . ``` Since the repository for the icu4c files has changed since the MeTa package was created, we will need to manually download the file and place it in the right directory for the build process to pick up. Now we need to download the ""icu4c-58_2-src.tgz"" file from here: Github icu4c 58-2 page (direct link to file) Now copy the file from your download location to the correct directory. Copy it to the meta/deps/icu-58.2/ directory (create those directories if they don't exist yet) Next, the MeTa source uses xlocale.h, which is no longer included in many newer linux distributions. We can use locale.h instead, so we will link it to there: ``` link xlocale.h to locale.h sudo ln -s /usr/include/locale.h /usr/local/include/xlocale.h ``` Now we'll configure move back to the build directory, configure the Makefile with cmake, and then make the project: ``` configure and build the project cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Older versions of CentOS: MeTA can be built in CentOS 7 and above. CentOS 7 comes with a recent enough compiler (GCC 4.8.5), but too old a version of CMake. We'll thus install the compiler and related libraries from the package manager and install our own more recent cmake ourselves. ```bash install build dependencies (this will probably take a while) sudo yum install gcc gcc-c++ git make wget zlib-devel epel-release sudo yum install jemalloc-devel wget http://www.cmake.org/files/v3.2/cmake-3.2.0-Linux-x86_64.sh sudo sh cmake-3.2.0-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir ``` You should be able to run the following commands and see the following output: bash g++ --version should print g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4) Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash /usr/local/bin/cmake --version should print cmake version 3.2.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). Once the dependencies are all installed, you should be ready to build. Run the following commands to get started: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project /usr/local/bin/cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. EWS/EngrIT Build Guide Note: Please don't do this if you are able to get MeTA working in any other possible way, as the EWS filesystem has a habit of being unbearably slow and increasing compile times by several orders of magnitude. For example, comparing the cmake, make, and unit-test steps on my desktop vs. EWS gives the following: | system | cmake time | make time | unit-test time | | -------------- | ----------- | ----------- | ---------------- | | my desktop | 0m7.523s | 2m30.715s | 0m36.631s | | EWS | 1m28s | 11m28.473s | 1m25.326s | If you are on a machine managed by Engineering IT at UIUC, you should follow this guide. These systems have software that is much too old for building MeTA, but EngrIT has been kind enough to package updated versions of research software as modules. The modules provided for GCC and CMake are recent enough to build MeTA, so it is actually mostly straightforward. To set up your dependencies (you will need to do this every time you log back in to the system), run the following commands: bash module load gcc module load cmake/3.5.0 Once you have done this, double check your versions by running the following commands. bash g++ --version should output g++ (GCC) 5.3.0 Copyright (C) 2015 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. and bash cmake --version should output cmake version 3.5.0 CMake suite maintained and supported by Kitware (kitware.com/cmake). If your versions are correct, you should be ready to build. To get started, run the following commands: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta/ set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project CXX=which g++ CC=which gcc cmake ../ -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system. Windows Build Guide MeTA can be built on Windows using the MinGW-w64 toolchain with gcc. We strongly recommend using MSYS2 as this makes fetching the compiler and related libraries significantly easier than it would be otherwise, and it tends to have very up-to-date packages relative to other similar MinGW distributions. Note: If you find yourself confused or lost by the instructions below, please refer to our visual setup guide for Windows which includes screenshots for every step, including updating MSYS2 and the MinGW-w64 toolchain. To start, download the installer for MSYS2 from the linked website and follow the instructions on that page. Once you've got it installed, you should use the MinGW shell to start a new terminal, in which you should run the following commands to download dependencies and related software needed for building: bash pacman -Syu git make patch mingw-w64-x86_64-{gcc,cmake,icu,jemalloc,zlib} --force (the --force is needed to work around a bug with the latest MSYS2 installer as of the time of writing.) Then, exit the shell and launch the ""MinGW-w64 Win64"" shell. You can obtain the toolkit and get started with: ```bash clone the project git clone https://github.com/meta-toolkit/meta.git cd meta set up submodules git submodule update --init --recursive set up a build directory mkdir build cd build cp ../config.toml . configure and build the project cmake .. -G ""MSYS Makefiles"" -DCMAKE_BUILD_TYPE=Release make ``` You can now test the system by running the following command: bash ./unit-test --reporter=spec If everything passes, congratulations! MeTA seems to be working on your system."
https://github.com/tgw4uiuc/CourseProject	ProgressReport.pdf	Thomas Wright tgw4@illinois.edu CS410 Progress Update The project chosen was: 2.1 Meta Toolkit - Enhance available tutorials for installing and using the tool on different platforms. 1) Which tasks have been completed? To ensure that nothing is left over from previous installations and make sure the tutorials will work starting from zero, multiple spare PCs were set up with SSD drives to allow quick, fresh installations of operating systems and the MeTa code. 3 PCs have thus been set up to allow working on them in parallel, so what while one is busy installing or compiling, work can be done on another. The latest versions of two popular Linux distribution (CentOS 8.2.2004 and Ubuntu 20.04 LTS) and the latest version of Windows 10 (October 2020 release) were downloaded, USB installation media created, and the OSes installed. MeTa was successfully installed on both of the above Linux versions after significant troubleshooting (see section #3 below). Metapy and pytoml were successfully installed on CentOS. Troubleshooting is still underway to try to get it working on Ubuntu. Detailed notes were taken on the installing and troubleshooting efforts (and the discovered workarounds) so far, which will be turned into HTML tutorials for the MeTa toolkit webpage. 2) Which tasks are pending? The next task which is pending is to further troubleshoot and find a way to get metapy to install on Ubuntu. There have been significant challenges to doing this, see part 3 below for details. Another task that is pending is to try to installing MeTa and metapy on Windows. With all of the problems discovered trying to get it working on the Linux versions, it was not possible to get to the Windows attempt yet, but hopefully in the next several days that will be possible. Next, as mentioned in the project proposal, investigation will be done into whether MeTa and/or metapy can be installed on a Chromebook. Given all of the issues uncovered in just getting it to work on newer versions of Linux where it had worked before, it is looking less likely to be able to get it working on a Chromebook, but a good attempt will be made to see if its workable. Then HTML versions of the installation notes will need to be created and placed on the project Github site, and following that the installation instructions will be followed one more time from scratch to ensure they work correctly. Finally, the final project writeup/documentation will need to be completed. 3) Are you facing any challenges? Yes, several challenges have risen causing significant challenges related to installing MeTa and metapy. MeTa/metapy don't seem to have been maintained recently. In the case of MeTa, I would estimate around 5 years, given that the existing tutorials refer to Ubuntu version 14.04 LTS and GCC version 4.8.5, which were released 6 and 5 years ago respectively. Metapy seems to be more recently maintained, within the past couple of years. Quite a few things have changed in those past several years that make it difficult to install MeTa and metapy. First, the source files for MeTa and metapy both download and compile versions of the unicode utility icu4c (version 58-2 for MeTa, 61-1 for metapy). Icu4c is no longer hosted at the site given in the sourcecode (icu-project.org), the source code is now hosted on Github instead. I needed to go through the error logs from the compiler to see where it was calling that, and then adjust the source files to point to the new Github location. This affected MeTa on both CentOS and Ubuntu, but only metapy on Ubuntu. Metapy for CentOS appears to use a pre-compiled version, but for Ubuntu it wants to compile the source code to install it. I am still working on changing the source code to fix this (it is not as easy to work with the source code for metapy, as the pip installer tries to do it all in one operation, I'm working on how to break this into a download step and then a compile/installation step so that the source code can be modified in-between those two steps. This code repository change appears to have been a recent change (in the last month or two), as I used metapy on Ubuntu 20.04 LTS to do MP2.2 for the class, and it installed with no issues then, but won't install now on a fresh installation of Ubuntu 20.04 LTS. Next, the MeTa source code includes xlocale.h. This was part of the glibc library, but was removed a few years ago, and is not included in current Linux distributions. In troubleshooting this, it was determined that for CentOS, it needed to be pointed to /usr/include/bits/types/__locale_t.h instead, while for Ubuntu, it should be redirected to /usr/include/locale.h. Finally, MeTa's source code is not compatible with the GCC 8 (CentOS) and GCC 9 (Ubuntu) versions that are part of the modern Linux distributions. The way that operator overloading is handled changed between gcc 7 and 8, and since the MeTa sourcecode uses this, it will not compile. GCC 7 had to be installed and used to compile MeTa instead of the version 8 or 9 that came with the OSes. This was easier done on Ubuntu than on CentOS. On Ubuntu, it was possible to point the package installer apt-get to use an archive site and install a GCC 7 package relatively easily. For CentOS, there did not seem to be any equivalent archive for the yum package installer used by that OS. The source code for GCC 7 had to be downloaded and compiled from scratch to install it. Given the enormous size and complexity of GCC, this took several hours to compile. Hours spent so far: Installing SSDs and RAM in PCs: 1 hour Downloading install ISOs and creating install USB sticks for several OSes: 1 hour Installing OSes: 1 hour Installing MeTa: general installation testing: 1 hour Installing and Troubleshooting MeTa installation: icu4u repository change: 3 hours Installing and Troubleshooting MeTa installation: xlocale.h fix: 1 hour Installing and Troubleshooting MeTa installation: GCC version issues fix: 6 hours Installing metapy/troubleshooting on Ubuntu: 2 hours so far (ongoing) Writing this summary report: 2 hours Estimated time spent so far: 18 hours
https://github.com/tgw4uiuc/CourseProject	Project Final Report and Documentation.pdf	"Thomas Wright tgw4@illinois.edu CS410 Project Documentaon and Final Report Part 1 - Documentaon 1. Installaon instrucons - This project was quite different from most of the others. It was not about coding, but rather to improve the MeTA and metapy tutorials. Therefore, there is no code to install. Rather, the product of this project was to produce installaon instrucons for MeTA and metapy for various operang systems. These instrucons have been created in the form of github .md files, so they can be transferred to the MeTa and metapy github sites to replace or augment the tutorial instrucons already there. The locaon of these .md tutorial files are in the directory here: hps://github.com/tgw4uiuc/CourseProject The files are MeTA_tutorial.md and metapy_tutorial.md. Click on either of these files to open them and display the tutorial instrucons. 2. Source Code - This project doesn't have formal source code, per se. That was not the goal of this project, creang tutorial instrucons was the goal. Thus, the deliverables are the .md files, which take the place of the 'code' deliverable. ""The documentaon is the code, and the code is the documentaon."" Please note that this project was to improve the exisng tutorials, so much of the older tutorial informaon is sll there. For the purposes of the ""plagarism"" queson on part 6 of the grading rubric, I am only claiming the CentOS 8.2.2004, Ubuntu 20.04 LTS, Chromebook and some notes in the general setup secons as my own for the MeTA instrucons, and the CentOS 8.2.2004, Ubuntu 20.04 LTS, Chromebook and Windows 10 instrucons in the metapy tutorial as my own. The rest of the documentaon is from the previous author(s) of the tutorials. 3. Soware usage tutorial - There is a link to the usage video in the video_link.md file in the hps://github.com/tgw4uiuc/CourseProject directory. Click that file to see the link to the video. Part 2 - Project Report Project Goal: The goal of this project was to improve the exisng tutorials for installing MeTA and metapy on various operang systems. The exisng tutorials were outdated and hadn't been updated in several years, and as a consequence, I found many obstacles that had to be overcome to install MeTA and metapy, as many changes had occurred since then that had prevented easy installaon. In most cases, those problems were overcome, and both MeTA and metapy were installed successfully. In addion, since recently produced Chromebooks can now have a beta feature that allows them to use a linux shell, I wanted to invesgate the possibility of geng MeTA and metapy working on a Chromebook. This was also achieved successfully. Project Results: MeTA was successfully installed on these OSes: (all newer versions of OSes already in the tutorial, or a whole new OS in the case of the Chromebooks): ChromeOS Ubuntu 20.04 LTS CentOS 8.2.2004 Metapy was successfully installed on the above OSes as well, and also confirmed that is it sll installable under Win10, with some caveats (python versions 2.7 or 3.4-3.7). The only one that failed was installing MeTA on Windows 10. I tried the msys2 method as described in the exisng tutorial, trying different versions of gcc/g++ and much debugging me, but could not get it to successfully compile. I also tried cygwin, as it is another linux-like environment similar to msys2, but I could not get it to install there either. I had already spent more than half a day trying to get it to work there, and had to give up on that part of the project, as there was no more me for further experimentaon there. Overall, I would say the project was quite successful, and has provided working tutorials for several updated and new OSes that will allow people to get MeTA successfully installed. I would have liked to do tutorials for even more OS versions, but with all of the unexpected problems that were found and the troubleshoong me that was required to find workarounds for them, there was no me to do any others. As it was, just doing these took far more me than the required 20 hours for the project. Problems Encountered: There were several problems encountered during the project related to the age of the MeTA and metapy packages and the evoluon of newer OS versions since the MeTA and metapy packages were last updated. Those problems are as follows: 1. gcc/g++ incompability - The way that gcc/g++ handle operator overflow was changed between gcc/g++ versions 7 and 8. This causes problems for the MeTa source code, and it will not successfully compile on versions 8 and above. Thus earlier versions of the compilers must be installed to successfully compile the code. 2. xlocale.h no longer supported. This .h file does not exist on many newer linux versions, and thus the MeTA code will error out when trying to compile as it looks for this file. The fix for this is to point xlocale.h to locale.h, which provides the funconality for MeTA to compile successfully. 3. The icu4c source repository has moved from icu-project.org to github.com/unicode-org. This causes both the MeTA and metapy code to fail because the make process can no longer download the required source file. The fix for this is to either download the file manually and copy it into the proper directory in the build files (which is what is done in the tutorials) or to edit the makefiles to point them to the new repository. 4. Metapy is harder to install on python version 3.8 and higher, as .whl ('wheel') preprocessed code packages are available for python version 2.7 and 3.4 through 3.7, but not 3.8 and up. These 'wheels' make installaon easy, as they do not need to be compiled from scratch, just downloaded and installed. When installing on python 3.8 and higher, they need to be compiled from the source code, which runs into the icu4c repository problem noted above. Suggesons for improvement of MeTA and metapy (and possible project ideas for future students): There are several things that would greatly improve the usability of MeTA and metapy, especially for new users. These ideas might make good project oportunies for future students to CS410, especially if they have experience with c++ and makefiles/cmake. These would be addressing the problems noted above. 1. Update for modern versions of gcc/g++. Update the code to fix the incompabilies with versions of the compiler ranging from version 8.0 and up. 2. Switch the code from using xlocale.h to locale.h. From what I found researching xlocale vs. locale.h online, there is no funconal difference between them for MeTA's needs, but this should of course be confirmed further by the project team. 3. Point the code/makefiles to the new icu4c source repository. Change the references from the old icu-project.org repository to the new github.com/unicode-org repository. 4. For metapy, create new 'wheel' .whl packages for python 3.8 and up. This will greatly simplify installaon for new users. Time Spent: Installing SSDs and RAM in test PCs: 1 hour Downloading install ISOs and creang install USB scks for several OSes: 1 hour Installing OSes: 1 hour Installing MeTA: general installaon tesng: 1 hour Installing and Troubleshoong MeTA installaon: icu4u repository change: 3 hours Installing and Troubleshoong MeTA installaon: xlocale.h fix: 1 hour Installing and Troubleshoong MeTa installaon: GCC version issues fix: 6 hours Installing metapy/troubleshoong on Ubuntu: 3 hours Installing metapy/troubleshoong on CentOS:  1/2  hour Invesgang which Chromebooks I had access to could support the beta linux mode, and seng that up: 1 hour Installing and Troubleshoong MeTA on the Chromebook: 4.5 hours Installing and Troubleshoong metapy on the Chromebook: 1 hour Clean re-install of OSes and verifying tutorials work: 5 hours Wring .md tutorial files: 4 hours Wring summary report: 2 hours Wring this final report: 3 hours Creang, processing and uploading soware usage video: 2 hours Total me spent: 40 hours (at least, some of the above are fairly conservave esmates)."
https://github.com/tgw4uiuc/CourseProject	ProjectProposal.pdf	"Thomas G Wright tgw4@illinois.edu CS410 Fall 2020 Project Proposal Option 2: Improving a System * 2.1 Meta Toolkit * Enhance available tutorials for installing and using the tool on different platforms This project will cover improving (or creating from scratch if not currently existant) the installation/setup tutorials for MeTA-metapy on several different platforms. OSes which will be considered in improving these tutorials include Windows and multiple Linux versions (Ubuntu, RedHat, Amazon AWS Linux, and perhaps others as time allows) and hopefully ChromeOS if I can get it working (see next to last paragraph below). The current MeTA Setup Guide tutorials on meta-toolkit.org (or the meta-toolkit section on github) seem to have been done quite some time ago, judging by the fact that the latest Ubuntu version listed is 14.04 and the current version is 20.04. The existing tutorials will be reviewed and tested to be sure that they still work, correcting and enhancing where needed. Instructions for newer versions of software, such as Ubuntu 20.04 will be added, as well as instructions for other Linux distributions. Also, there seems to be very little documentation for installing/setting up metapy. This project will add tutorials for that on several OSes. These will start from the assumption of a fresh OS install, unlike what instructions seem to exist online, which are very sparse and assume you have pip, git, etc. already installed on your computer. How to get to that stage seems to be a cause of frustration for several that have had problems. These tutorials would be set up to be a link off the tutorials section of the main page, in a section called something like ""metapy setup guide"". All of these tutorials will be written in HTML, and will aim to stay consistent with the style of the current page, which is nealy all text. Some enhancements with screenshots may be added to clarify things where needed, such is currently being done in the ""visual setup guide for Windows"" tutorial currently on the site. In addition to OS types which already exist in the tutorials, I will try to see if it is possible to put metapy and/or MeTa on ChromeOS. Online searches reveal that it is possible to put python and gcc on ChromeOS, I will experiment to see if that can be extended to include MeTa or metapy. This will be a whole new OS where it hasn't been done before, which I feel will be a nice addition to the MeTa/metapy universe, especially as the popularity of Chromebooks grows. Regarding whether this fulfills the 20 hours per team member, I believe this will easily do that. This is a 1 person team, so it only needs to cover 20 hours total. To ensure that the results from the tutorials are correct and repeatable, I will be sure to use fresh OS installs so that nothing that was already on an existing system from previous installs would cause erroneous results. In the case of writing a whole new tutorial, once it is complete, it will be repeated following the tutorial exactly on a new fresh install, this takes extra time, but I feel it is necessary to be sure the tutorial is accurate. I have a few old PCs that aren't being used for anything at the moment, so this way one can be re-imaged with a fresh OS install in parallel while working with another one, so it will take some time to do that , but it won't be too excessive. By also trying to make it work on ChromeOS, I feel this will easily go over 20 hours total of work, as I expect that experiment to take quite a bit of time. Time spent will be tracked, so that if the total time is under 20 hours, more OSes/Linux versions will be added to ensure well over 20 hours of time was put into the project."
https://github.com/tgw4uiuc/CourseProject	README.md	"Files for the CS410 Course Project: (see above file list) The software usage tutorial video is located here: (and a link is also in the video_link.md file) https://youtu.be/nbpwsQBnId0 The MeTA_tutorial.md and metapy_tutorial.md files are the main deliverables for this project, and can be viewed by clicking on them. The ""Project Final Report and Documentation.pdf"" file contains the final report as well as some suggested future fixes for MeTA and metapy, as well as some suggested MeTA and metapy improvements for following CS410 classes. Finally, there are also the ProjectProposal.pdf and ProgressReport.pdf files that were submitted previously."
https://github.com/tgw4uiuc/CourseProject	video_link.md	Video is located here: https://youtu.be/nbpwsQBnId0
https://github.com/EsportsNoEyes/CourseProject	CS 410 Project Proposal.pdf	"CS 410 Project Proposal Group: LiveDataLab Admins Team Members * Yanbo Chen, ychen380 * Linfei Jing, ljing2 * Huaminghui Ding, hding14 * Captain: Yanbo Chen Project Topic Text classification competition We choose the classification competition and are fully prepared to learn state-of-the-art neural network classifiers. We have learnt about some classical machine learning algorithms such as Naive Bayes, Decision Tree. We also have basic knowledge about pattern mining from text files using Apriori algorithm and FP-Growth. Our team member used to write those algorithms from scratch without using predefined libraries to classify animals in a zoo's data-set. Those will be helpful for text classification algorithm design in the project. Programming Language Python. Problem to solve Given a training set containing tweets from Twitter, each with a label ""SARCASM"" or ""NOT_SARCASM"", train a classifier to predict the label for each tweet in the test set. Methods to use 1) Naive Bayes Classifier. Before we dive into neural network classifiers, we decide to apply Naive Bayes Classifier first and see if it performs well on the problem. 2) Neural Network Classifiers If Naive Bayes Classifier does not work well on predicting the label, we are prepared to learn some neural network classifiers such as LSTM and BERT. 3)Deep Learning Frameworks We plan to use some popular frameworks such as Keras library in Tensorflow."
https://github.com/EsportsNoEyes/CourseProject	CS410 Project Progress Report.pdf	"CS 410 Project Progress Report Group: LiveDataLab Admins Team Members * Yanbo Chen, ychen380 * Linfei Jing, ljing2 * Huaminghui Ding, hding14 * Captain: Yanbo Chen Progress Made Based on the plan in our project proposal, we designed and implemented a Naive Bayes Classifier to classify the given data into two categories ""SARCASM"" and ""NOT_SARCASM"". The performance of the current classifier has already beat the baseline. Remaining Tasks Some further tuning to make the Naive Bayes algorithm even more accurate. Challenges/Issues Being Faced What method to use for tuning the algorithm and we are considering if some weighted method could be used over Naive Bayes to make it more precise specifically for this task."
https://github.com/EsportsNoEyes/CourseProject	README.md	"CS410 Classification Competition Usage First clone the source code to local, and use this command to install all the dependencies pip3 install -r requirements.txt, assuming Python3 is used. Use command python3 prediction.py to generate the prediction results which is stored in answer.txt If there is any errors with nltk package when running the code, please try to install suggested additional dependencies to solve the issue. We also welcome our reviewers to schedule a live demo. Description of Algorithm As we planned in the project proposal, the first method we tried is classifier that based on Naive Bayes. The equation to compute the probability is We used Laplace smoothing when we calculate the probability of every single key in the ""SARCASM"" and ""NOT_SARCASM"" dictionary. The equation for calculating the probability is and . UNK stands for the words that we have not seen in the training data. D stands for the dictionary we used when we calculate the probability of its words. It can be either ""SARCASM"" dictionary or ""NOT_SARCASM"" dictionary. Alpha stnads for the laplace smoothing parameter we set before training, default to be 1.0. Count(W) stands for the number of times a specific word W appeared in the training data. V stands for the size of the corresponding dictionary. Overview of Functions & Implementation Details reader.py, provide helpers to load the datasets into proper data structures to be used by the algorithm loadFile(name,stemming,sarcasm,training): The helper function to load training data and test data. The parameter ""name"" indicates the directory path to the file of data. The parameter ""sarcasm"", a boolean variable, indicates whether the training data is labelled as ""SARCASM"" or ""NOT_SARCASM"". The parameter ""training"", a boolean variable, indicates whether the input data file is training or test. ""stemming"" is provided as an optional parameter to enable stemming. It returns a list containing the tweets. load_dataset(train_dir,dev_dir,stemming): It loads data and form structures that can be used by naive bayes algorithm using loadFile(). It returns lists indicating the label of each data entry. prediction.py, the wrapper file that is called by the user to run the our Naive Bayes Classifier main(args): It is a wrapper function that extracts data, run naive bayes algorithm and output prediction results using the functions provided by reader.py and naive_bayes.py naive_bayes.py, implementation of our Naive Bayes Classifier naiveBayes(train_set, train_labels, dev_set, smoothing_parameter): The wrapper function of our implemented Naive Bayes algorithm. do_unigram(train_set, trai_labels, dev_set, smoothing_parameter): It applies unigram model and predict the labels of tweets. get_probability(tweet,p_dict): It calculates the sum of the probability of a word in a dictionary. get_probability_dict(some_dict,word_count,smoothing_parameter): It calculates the probability of every single key in the ""SARCASM"" and ""NOT_SARCASM"" dictionary, including the words that we have not seen in the training data. We used the equations mentioned in the above section to calculate the probability. get_dicts(train_set, train_labels): It creates ""SARCASM"" dictionary and ""NOT_SARCASM"" dictionary respectively and store the count of the occurrences of all the ""SARCARSM"" words and all the ""NOT_SARCASM"" words Based on the training data given, we first tried to predict the labels using the response tweets without context tweets as our dictionary. The accuracy was around 0.69. Then we included the context tweets into our dictionary. This time the accuracy beat the baseline. We tried to tune the laplace smoothing parameter and it turned out that the default one gave the highest. We also tried with stemming using the PorterStemmer of nltk package but it does not improve the accuracy considerably. Contribution of team members We went through design of algorithms, coding, and documentation together."
https://github.com/Xinpeij/CourseProject	Project Proposal .pdf	CS410 Project Proposal Team Members: * Xinpei Jiang, xinpeij2 (individual team) Topic: * Text Classification Competition l I plan to learn and use state-of-the-art neural network classifiers in the competition. l Such state-of-the-art methods might include deep neural networks like CNN, RNN, LSTM, or the NLP transformer models like Google's BERT, l I also plan to learn and use deep learning frameworks like TensorFlow in which I might apply libraries like Keras. l I plan to use Python for this project.
https://github.com/Xinpeij/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/mshaw0707/CourseProject	CS410 Progress Report.pdf	Progress Report 11/29/2020 1) Progress made thus far I am using a Jupyter notebook in Google Colab for the project with Pytorch. I have read the data into Pandas arrays and am working on training the models. Currently I have the code in place to split the data into training/validation/test sets and the scaffold code for training the models (currently investigating using LSTM and BERT type models). 2) Remaining tasks - Finish the training code to obtain the optimal hyperparameters for the best models - Format the output into answers.txt for submission on livedatalab - Add a readme/comments. The code should be straightforward for anyone to run via Google Colab (I am on the free tier) 3) Any challenges/issues Not currently blocked on anything; I am fairly confident that one of the models will be able to beat the baseline from the reading I have done online regarding using Pytorch for text classification.
https://github.com/mshaw0707/CourseProject	CS410 Project Proposal.pdf	CS410 Fall 2020 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Mathew Shaw, NetId: mcshaw2 I am doing the project solo. 2. Which competition do you plan to join? I will be joining the Text Classification competition. 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I plan to use PyTorch to create the classifier for the competition. I have previously used the framework for some experimental image classification tasks. If PyTorch does not work out for this task I will look into alternatives like Keras. 4. Which programming language do you plan to use? Python
https://github.com/mshaw0707/CourseProject	README.md	CourseProject (Text Classification) Overview The project I chose was the sarcasm detection competition. I successfully beat the baseline by training a PyTorch BERT model using the Huggingface transformers library. I first researched what types of models could be used for this manner of text classification and found that the Huggingface library (https://huggingface.co/) had multiple pretrained models to suit this goal. Some minor data cleanup and preprocessing on the training/test data sets, along with concatinating the immediate context with the response allowed me to beat the baseline. Libraries/Languages Python Pytorch Huggingface transformers Pandas SKLearn Implementation I developed the entire project within a Google Colab (https://colab.research.google.com/) notebook. I used the free tier for all development and the final model training. I recommend opening the ipynb in Colab to run the code without needing to configure any of the libraries locally. Usage Open CS410Project.ipynb in Google Colab (https://colab.research.google.com/) The data storage for a Google Colab workspace is ephemeral, so the train.jsonl and test.jsonl files must be uploaded to the workspace storage each time. (Optional) For faster model training, click Edit -> Notebook Settings, then set the Hardware Accelerator to 'GPU' inside the popup. This was available for me using the free tier of Colab during development. The code will default to CPU computation if this is not done and the model will take longer to train. Run all code blocks in order. The file answer.txt will be created in the workspace for download. I used 5 epochs for training and it generated predictions that beat the baseline. Presentation I did not have time to do a presentation.
https://github.com/alex6499cat/CourseProject	CS 410 - JAWs Project Progress Report.pdf	"CS 410 - JAWs Project Progress Report Sentiment Analysis of Customer Support Tweets 1) Which tasks have been completed? * Replacing emojis with descriptions of emojis * General data cleaning to remove urls, unnecessary line endings, etc * Running topic analysis on Amazon, Apple, and Uber Tweets * Creating a parallel database that links all the tweets in the same thread. * Sentiment analysis of the first and last user tweet of each thread to determine change in sentiment due to the interaction with the customer service team * Summarize the average initial sentiment of the customer, and the average improvement of sentiment by company. * Visualize how the sentiment changes over time for one company. 2) Which tasks are pending? * Sentiment analysis between successive customer tweets of a thread * Generating a language model representing ""successful"" customer service tweets * Visualizing/Summarizing data * Continue topic analysis on airline companies trying different topic counts * Incorporating topic analysis into sentiment analysis of companies * Visualize how the company compares to others in the same industry regarding the effectiveness of their team in improving customer sentiment. 3) Are you facing any challenges? * General runtime of scripts * Choosing what parts of our analyses are most interesting to summarize and present * Finding coherent topics of tweets programmatically"
https://github.com/alex6499cat/CourseProject	CS410 - JAWs Project Proposal.pdf	"Team name:  JAWs Team members: Joan Ball - joan2 Alex Ginglen - aging2 Walter Griebenow - wfg2 - Team Captain CS - 410 Project Proposal:  Sentiment Analysis of Customer Support Tweets 1. Project objective / Hypotheses to test What is the function of the tool? Our tool analyzes customer support tweets generated by both customers and call center personnel and performs several textual analyses: * Determines the sentiment of the initial tweet, response tweet and subsequent interactions. * Determines the topics/categories that exist in order to categorize tweets. * Relating to complaint, such as bad service, faulty product, etc * Relating to industry, such as software, food service, etc * Searches in the response tweets word unigrams and bigrams that partially characterize the content of the response. * Mines possible associations of those uni or bi-grams with the change in the sentiment between the initial customer tweet and the tweet after the interaction with the call center employee (to estimate ""success"" of the customer service) * Uses non textual variables to control for variations due to company / industry and topic (maybe timeliness of response too). For further exploration: * Find correlations between inbound and outbound topic which would be the initial step for predicting the answer from the question the customer asks Who will benefit from such a tool? The idea behind this project is to find associations between the language used and the effectiveness of the interaction with the customer, that may lead to a focused A/B testing which in turn would be used to establish best practices for the service personnel. Does this kind of tools already exist? If similar tools exist, how is your tool different from them? We can assume that technology savvy companies already have discovered the best language to approach angry or dissatisfied customers and have clear guidelines for different types of interactions. But probably smaller companies do not have the resources to perform this type of analysis and rely on common sense and the experience of their employees. Would people care about the difference? We do not know yet what we will find but this path could be much faster than discovering language patterns only with the experience. Also, this would help to flatten the learning curve of new employees by informing of empirical best practices. How are you going to evaluate your work? Initially, we will evaluate our work by observing that our tool produces accurate results when analyzing the sentiments of a small amount of data. A larger scale test could be running our tool on a large, similar dataset that has already had a sentiment analysis done to it and compare the results. We will also visualize our output and analyzed input data to better understand it. 2. Course topics covered by the project / Tools to be used in analysis and display of results What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We will apply sentiment analysis, topic mining, and general CS techniques like parsing text to remove useless words, database wrangling to put together related tweets and some type of regression analysis. We intend to do the development work in Python. 3. Data description What existing resources can you use? Our group has identified a suitable dataset in Kaggle that can be accessed in this site: https://www.kaggle.com/thoughtvector/customer-support-on-twitter This dataset is a large corpus of over 3 million tweets and replies from big brands like Apple, Amazon, Delta and T-mobile. This dataset is organized in one row per tweet in 7 fields per row that include the links to previous and next (if applicable) tweets. 4. Other How will you demonstrate the usefulness of your tool? We will run the analysis using a Jupyter Notebook that can be run by the TA. This notebook will include graphs and tables that show the uncovered relationships (hopefully some with statistical significance) A very rough timeline to show when you expect to finish what 1. Data exploration and data wrangling - 11-05-2020 - 3 hours per person 2. Sentiment analysis and topic mining by type (customer or employee) by 11-15-2020 - 5 hours per person 3. Word unigrams and bigrams in the response and their association with the change in sentiment - 11-25-2020 - 5 hours per person 4. Project Presentation Made - 11-30-2020 - 3 hours person 5. Optional: prediction of response topic from inbound topic by - TBD"
https://github.com/alex6499cat/CourseProject	README.md	"Sentiment Analysis of Customer Service Tweets Presentation: https://youtu.be/wcCyMLzrOu8 Our goals Note: it was not our intention to create any library or software, but instead to do a deep dive into a data set and present our findings. Because of this, our presentation is more about our findings instead of usage, and our ""testable"" code is more of a playground with limited functionality, running all code that we used to generate our findings could take days. As discussed in our proposal, we set out to analyze 1 million customer service tweets. This analysis includes: - Analysis of the overall sentiment change of a thread. That is, the difference in sentiment between the first and last tweet of one customer in a thread. This data was stratified by time and company to further compare and analyze how well the top companies stack up against each other. - Analysis of common topics by company. With tweets being short and complaints not being too broad within the scope of one company, we found that a smaller amount of topics (k) tends to find more distinct topics. While these topics aren't given any human readable title, you can infer what a topic might be about based on its unigram language model. - Analysis of successive sentiment. That is, the difference in sentiment of customer tweets that ""sandwich"" a customer service tweet. As opposed to the earlier mentioned sentiment analysis, this had the intention of finding successful language, instead of comparing companies against each other or themselves over time. But again with tweets being short and customer service responses tending to be formulaic, there wasn't much difference in language between successful and unsuccessful customer service responses. Testing/""Playground"" Along with python notebooks, we also have a well put together excel spreadsheet that includes data from our first goal and that also has several sheets you can interact with. That spreadsheet is found at output/Sentiment summary by company and month.xlsx We have provided several playground files, or files meant for our tester to ""test"" our code with since they are more lightweight. - Overall sentiment change - tester files: Jupyter Notebooks/FromAdjacencyToDataframe.ipynb Jupyter Notebooks/Sentiment improvement by company and month.ipynb Jupyter Notebooks/SentimentAnalysis.ipynb - dependencies: pandas, datetime, stanza CoreNLP (java & pytorch needed for stanza) - Topic analysis: - tester files: Jupyter Notebooks/DeltaTopicsDetermination.ipynb Jupyter Notebooks/Sentiment of topics.ipynb - dependencies: gensim, nltk, collections, pandas - Consecutive sentiment change (developed on python 3.7.3) - tester files: Jupyter Notebooks/con_sent_tester.ipynb - dependencies: pandas, gensim"
https://github.com/alex6499cat/CourseProject	Sentiment analysis of customer service tweets.pptx	"Sentiment Analysis of Customer service tweets JAWs Team - CS410 Joan Ball, Alex Ginglen, Walter Griebenow Objective Monitor effectiveness of the service center in improving customers satisfaction and compare to industry benchmarks Find the topics most asked for in the tweets and their typical sentiment Portray the characteristics of a successful interaction with the customer Process followed Text cleaning Convert emoticons to words Associated tweets into Threads Customer Sentiment: initial & final Compare by industry / time Topic analysis Relate to sentiment Sentiment change after interaction Language of effective interactions 1 2 3 4 1. Pre processing Cleaning Remove unneeded Twitter Handles Remove urls and &amp; Emoji to text Replace emoticons with corresponding english words (grin face, happy face) Weave tweets into Threads Gather related tweets in a sequence to build a conversation 1. Translate Emojis Script EmojiTranslate.py Process Remove Emojis for topic analysis Translate emojis to text description for sentiment analysis Smile Face 2. Effectiveness of customer service Initial sentiment of customer tweets is improved by interaction with the customer service team. How do we compare? (Tech industry) Spotify not only has better initial sentiment but also they are better in improving the final sentiment. Microsoft lags behind. 2. Monitoring the sentiment in time Timely detect changes in sentiment of customer tweets. Delta Airlines does better than the average of its industry, but there is always space for improvement. Did something different happen in June 2017? 3. Topic Analysis Process Remove Stopwords Incorporate Bigrams Component Incorporate TF-IDF Component Create model using Latent Dirichlet Allocation Execute model on list of tweets Aggregate list of Tweets with assigned topics 3.Topic Analysis Results Generated topics did not directly match single, clear topics identifiable by humans. Topics tended to match some specific areas Topic 2 tended to match tweets about gates. Topic 5 tended to match tweets requesting help or being on hold. In order to identify coherent and granular topics consistently, our algorithm would need human intervention such as in text categorization or by providing a pre-classified list of tweets 3.Topic to Sentiment Some topics tended to have higher average sentiments than others Topic 0 tended to match tweets praising customer service and had a higher average sentiment starting a tweet thread Topic 5 tended to match tweets requesting help and being on hold and had the lowest average sentiment starting a tweet thread The more positively associated topics tended to improve less in sentiment Airlines may need to pay special attention to topics that have a low starting sentiment and a low sentiment improvement 4. Successive Sentiment More granular than overall sentiment Instead of evaluating company success, evaluating what successful or helpful speech looks like Sentiment analysis isn't perfect, makes for some interesting results! 4. Successive Sentiment: ""Best"" Results Analysis not always perfect, but it's not hard to see where things were interpreted wrong Correct strong positive change, but the CS tweet did not assist in any way Might have interpreted ""wining"" as ""winning"". ""good luck"" also added to evaluating as positive sentiment Good feedback! 4. Successive Sentiment: ""Worst"" Results Lots of these were genuinely confusing Customer response not related to CS response Not sure how to interpret this as a human Good interpretation! 4. Successive Sentiment: Unigrams ""Best"" tweets have sentiment change +3 ""Good"" tweets have sentiment change greater than or equal to +2 4. Successive Sentiment: Unigrams ""Worst"" tweets have sentiment change -3 ""Bad"" tweets have sentiment change less than or equal to -2 Future improvements Enhance Topic detection by initializing topics with keywords: Airlines: flight delay, bag claim, phone long, cancel refund, etc. Technology: battery life, screen size, game start, etc. Model the Quality of employee response as a Function of Language used: Q=f(L) Naive Bayes logit regression: TF best CS tweets TF worst CS tweets Add Bigrams as language features, here we analyzed unigrams only Industry, Topic, Time as context"
https://github.com/Asciotti/CourseProject	CS410 Progress Report.pdf	"Progress Report Andrew Sciotti, sciotti2 Which tasks have been completed: The following tasks have been completed: - Setup environment (for python 2.7) - Debugged code so that it was working in original state - Identified the location where to hook into the EductionalWeb system to apply auto-text summarization to the returned results - Implemented two text summarization prototypes, extractive using shallow NLP & abstractive using deep NLP Which tasks are pending: - Implement summarizers into rest of the code base (currently sitting separate using an example text paragraph - Fully hook in the summarizer to display on the EductionalWeb browser (see ""challenges"") - Explore improvements to the shallow NLP extractive text summarization (stemming, stop word removal, word embedding, etc) Are you facing any challenges: 1) Errors during running I cannot get the full workflow to work. The scoring function inside the ""get_explanation"" function of model.py freezes during computation. See https://github.com/Asciotti/CourseProject/blob/master/model.py#L311 However, this only occurs when I run it via the app.py, if I were to manually run the scorer, though ipython/command line, it has no issues. Because of this, I cannot fully close the loop. 2) The data (para_idx_data) is not representative of what exists on the publicly hosted website The sourced ""explanations"" from the http://timan102.cs.illinois.edu/explanation/ website seem to be derived from the full text of the textbook, but the data provided in the git repo appears to only be snippets/headers of the text."
https://github.com/Asciotti/CourseProject	CS410-Course Project.pdf	"Team APS Members: Andrew Sciotti (sciotti2), sciotti2@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Member/Captain: Andrew Sciotti, sciotti2 2. What system have you chosen? Which subtopic(s) under the system? I have chosen to improve the EducationalWeb System, specifically by providing more context and improved reading comprehension to the explanations in the form of summarization. For instance, the explanation of ""PLSA"" is (roughly) 63 sentences long! 3. Briefly describe the datasets, algorithms or techniques you plan to use The goal is to implement extractive summarization on the retrieved relevant explanations provided by EducationalWeb System. The dataset will be provided via the textbook, ""Text data management and analysis: a practical introduction to information retrieval and text mining"". The fundamentals of the extractive summarization are based on PageRank, but for text, coined (not so creatively), TextRank (reference: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf). This is an unsupervised algorithm. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? The most straightforward method of evaluation is to manually obtain a few examples of explanations that are lengthy and evaluate the effective conciseness provided by the function. Quantitative measures are unlikely to be evaluated, so the results will be qualitatively evaluated. Ideally, if there were sufficient users, something like A/B testing would be implemented, but that is entirely out of scope. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly The plan is to identify where the explanation is retrieved and forwarded to the webpage, and intercept that function call to be routed through the extractive summarizer. 6. Which programming language do you plan to use? Python 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Project Plan: - Setup environment, 2 hours - Familiarization with EducationalWeb System, 5 hours - Progress Report, 1 hour - Deep dive into the retrieval/explanation system, 3 hours - Implement extractive summarization, 3 hours - Debug & evaluated extractive summarization, 3 hours - Document code & prepare tutorial, 3 hours"
https://github.com/Asciotti/CourseProject	README.md	"EducationalWeb TUTORIAL FOUND HERE: https://mediaspace.illinois.edu/media/1_5ohs6cp4 If you are not familiar with the EducationalWeb, it is a resource provided that contains a multitude of features related to the lectures slides including: search term throughout slides/audio Downloading slides Providing reference to slides that are related ""Explain"" highlighted words on the slides Setup The following instructions have been tested with Python3.7.9 on Windows. You should have ElasticSearch installed and running Ubuntu - https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Windows - https://www.elastic.co/guide/en/elasticsearch/reference/current/windows.html MacOs - https://www.elastic.co/guide/en/elasticsearch/reference/current/brew.html Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ Download lemur-stopwords.txt from here -- https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt lemur-stopwords.txt needs to be placed under EducationalWeb/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server In another terminal, setup your python environment (requires python 3.7.9 - I recommend making a clean venv/conda environment) by running pip install -r requirements.txt. If using conda, the following is a useful command to make the correct environment conda create --name uiuc3.7 python=3.7.9 Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/ Run through the examples (at the end of the video tutorial) to see the usage and benefits of the summarization. Motivation: The ""Explanation"" feature available in the EducationalWeb system can be used to provide an explanation of the highlighted words on a given slide. The issue is the returned explanations can be too lengthy and not all that specific to the query word. One could improve these explanations by providing a more targeted, concise summary of the raw explanation return. This can be done in a few manners, I chose to look at extractive methods (using shallow NLP) and abstractive methods (using deep pre-trained NN based language models). Extractive summarization can be thought of as restating the most useful or ""main"" points in a document. Abstract summarization can be thought of as paraphrasing the document itself in a (typically) more concise way [1]. Concepts of explanation ""finding"" The general process to discover explanations is as follows - this is essentially a document retrieval problem given a query. 1. Preprocess corpus for future ranking on query words 2. Extract the query word from the webpage 3. With query, corpus (processed), and ranking algo (BM25 in our case), determine most likely documents (In our case documents were provided in the repo already, they appear to be headers of chapters) 4. Take top-k (1 in our case) most likely documents 5. Given the best matched document, then run summarization on the content 6. Return the summarized ""explanation"" to the user Extractive Summary Process (ref [2]) Provided into the extractive summary process is a preprocessed document that contains a list of lists for each sentene then each word in said sentence. In the extractive summary process we utilize a vector space method ""encoding"" of each sentence. Each element of the vector is the count of a single unique ""token"". We use the nltk python language package for tokenization. The similarity measure between each sentence with every other sentence is computed via cosine similarity. This gives us an MxM matrix (M=# sentences), the similarity matrix. The pagerank algorithm [5] can be used to return probabilities of ""landing"" on each page (sentence in our case). The sentences with the highest probabilities are most likely to occur, and are thus the most ""useful"". We add some additional weighting to sentences which contain the query word itself - to try to improve the likelihood of seeing the word in the explanations given. You can run an example of abstractive summarization by running the following: python extractive_summarizer.py It will run a summary of the below input data (selected document) which resulted from the query annotations. In this section, we're going to continue our discussion of web search, particularly focusing on how to utilize links between pages to improve search. In the previous section, we talked about how to create a large index on using MapReduce on GFS. Now that we have our index, we want to see how we can improve ranking of pages on the web. Of course, standard IR models can be applied here; in fact, they are important building blocks for supporting web search, but they aren't sufficient for the following reasons. First, on the web we tend to have very different information needs. For example, people might search for a web page or entry page-this is different from the traditional library search where people are primarily interested in collecting literature information. These types of queries are often called navigational queries, where the purpose is to navigate into a particular targeted page. For such queries, we might benefit from using link information. For example, navigational queries could be facebook or yahoo finance. The user is simply trying to get to those pages without explicitly typing in the URL in the address bar of the browser. Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. These features provide an opportunity to use extra context information of the document to improve scoring. Finally, information quality greatly varies. All this means we have to consider many factors to improve the standard ranking algorithm, giving us a more robust way to rank the pages and making it more difficult for spammers to manipulate one signal to improve a single page's ranking. You should see: Secondly, web documents have much more information than pure text; there is hierarchical organization and annotations such as the page layout, title, or hyperlinks to other pages. First, on the web we tend to have very different information needs. In this section, we're going to continue our discussion of web search, particularly focusing on how to utilize links between pages to improve search. One might notice that without any postprocessing, the resultant summary can be devoid of any context or fluidity. Abstractive Summary Process (ref [3]) Abstractive summary utilizes a pre-trained deep NN from HuggingFace [4]. We utilize the T5 language network to perform summarization. The T5 network has an accompanying tokenizer that ingests the raw document (sentences). We have picked the T5-small model (and tokenizer) rather than the medium/large models due to computation time (we are running locally on CPUs). The larger models 1) are more accurate and 2) have the ability to process longer documents. The small is limited to 512 tokens, which roughly translates to 500 words or ~20-30 sentences. Without going into the details ([3] can provide more information) of the generational configuration, we choose to utilize beam search to improve the summary in addition to constraining the length of the summary returned to [50,150] tokens. You can run the following to see an example of an abstract summary (note the output might be slightly different from yours). Input text same as for extractive summarization example. python abstract_summarizer.py we're going to continue our discussion of web search. in the previous section, we talked about how to create a large index on using MapReduce on gfs. we want to see how we can improve ranking of pages on the web. the standard IR models can be applied here, but they aren't sufficient for the following reasons. Given the network we used, it was not possible to add any weighting/favoring towards generating sentences with the query word. Overview of selected pieces of code Below will highlight a fews places in the code that are specific to the augmentation of the Explanation functionality. This code base is large but most of it is related to the web based portion whereas a select few python files are used to actually perform much of the backend functionality. app.py : Contains the flask server code, handles various API calls (unmodified) model.py : Handles the model that performs document-query retreival. get_explanation is the function that returns the Explanation results. Inside this function is where the main ""injection"" occurrs. After we retrieve the normal explanation (aka the highest scoring document), we feed that result directly into the summarizer functions. ranker.py : Retrieves the highest ranking documents given the query word and corpus. Replaced metapy ranker w/ rank_bm25 3rd party package due to bugs. extractive_summarizer.py : Contains extractive summary code, if run alone via command line, will summarize an example text. abstract_summarizer.py : Contains abstract summary code, if run alone via command line, will summarize an example text. Demo The demo is provided via the video tutorial found here: https://mediaspace.illinois.edu/media/1_5ohs6cp4 The slide used to demo can be found here (given the app is running and you can connnect to it: http://localhost:8096/next_slide/cs-410/86/cs-410----13_week-12----02_week-12-lessons----05_12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis_TM-42-cplsa.txt----slide2.pdf The word example we will go through is for Coverage in the above slides. If you don't want to go through the video. Go through the installation above such that you can see the slides. Go to those specific set of slides (click on link). Highlight ""coverage"" in the bottom right corner and hit the box with the graduation cap on it in the top right. If you hover over the boxes it shoulds say ""Explain selected text"" You should see the unmodified ""Explanation"" Now go to your text editor and open app.py. Modify SUMMARIZER="""" to SUMMARIZER=""EXTRACT"" located at the top of the file. Hit save file. Wait a few seconds then reload the page. Highlight ""coverage"" again and hit the explain button. See the extractive summarized ""Explanation"". Now go to your text editor and open app.py. Modify SUMMARIZER=""EXTRACT"" to SUMMARIZER=""ABSTRACT"" located at the top of the file. Hit save file. Wait a few seconds then reload the page. Highlight ""coverage"" again and hit the explain button. See the abstract summarized ""Explanation"". References [1] https://www.quora.com/Natural-Language-Processing-What-is-the-difference-between-extractive-and-abstractive-summarization [2] https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70 [3] https://huggingface.co/blog/how-to-generate [4] https://huggingface.co/ [5] https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"
https://github.com/xiang-wang2020/CourseProject	progress_report.pdf	"1) Progress made thus far The website is deployed. Front-end is near finished. The database is built. It is based on university, location, name, and other information. 2) Remaining tasks The backend and the search function. 3) Any challenges/issues being faced. Based on the TA's instruction, I have narrowed my goal into ""replicate ExpertSearch,"" and I would like to make sure if this is sufficient. I find my peers say the course TA could provide the existing data. I wonder if this is true."
https://github.com/xiang-wang2020/CourseProject	project_proposal.pdf	Project Proposal I. Team Information This will be an individual team. Project Name: Individual Project Topic: 2.2 ExpertSearch System Team Member: Xiang Wang (xiangw3@illinois.edu) Project Leader/coordinator: Xiang Wang II. Details Brief idea: I have chosen to work on the ExpertSearch System. I would like to write a my own version of the ExpertSearch System, and the system will have functions includes: a website supporting search functions, a database filled with crawled data from testing university, and a new function to support automatically crawling data from a given university. Usefulness demonstration: If anyone ever need to quickly look up a professor, this website will save them time from search themselves, and the new function will make the whole website live, because it expands over time and will be improved every time a user use it. About tools: I would like to use python for backend, and mysql for database. I do not have an idea about search algorithm yet, but I will have it figured out soon. How to show my function work as expected: My plan includes adding a new function, and I crawl some data (university faculty info) as my test data and test them against my function. About time: I think building the website, from front end to back end will take at least 15 hours, build the new function will take 10 hours, and testing and other stuff will take 10 hours. Together they are about 35 hours. About timeline: By week 12: Have the website ready. By week 14: Have the new function ready.
https://github.com/xiang-wang2020/CourseProject	README.md	"Video Demo https://drive.google.com/file/d/1X6JIXAFP2zzAArOhJZVI2y-YqfIeZ-SR/view?usp=sharing How to set up: download the package to your local computer open a terminal and move into the folder run ""pip install -r packages.txt"" run python start.py go to http://127.0.0.1:5000/ in your browser What does it do: The goal is to replicate ExpertSearch System, a course project built by former CS410 students (Original Link: http://timan102.cs.illinois.edu/expertsearch//#). The essential function is to search and display ranked information of faculties from many universities. The algorithm behind ExpertSearch is relevance ranker. In this version, I used cosine similarity to rank the documents. The documents are turned into vectors, following the guidance of tf*idf algorithm, and ranked based on how similar they are from the query. Meanwhile, you are able to set up the must-include and must-exclude keywords and set up the number of results you want to see. Introduce my files: Most of the data cleaning and processing work is in process.py. start.py handles the backend, and templates/index.html handles the frontend. The search result will be displayed by templates/result. html. faculties.py includes 999 lines of crawled data from different universities, and every line is one section/person. testcase.txt includes some sample testcases for the grader."
https://github.com/dinghuaminghui/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yhui288/CourseProject	Documentation.pdf	"Documentation 1. Project Submission * Project Code: https://github.com/yhui288/ClassificationCompetition * Presentation Video: https://drive.google.com/file/d/1WlLuUBaauCe_yQBEOWXbl8Yxxz WSVEQF/view?usp=sharing 2. Overview of The Function of The Project Our team participated in the Text Classification Competition. This competition aims to predict if tweets with given context are sarcasm. We are given training data with labels. The goal is to correctly predict if tweets in test data are sarcasm. Team Member: Changcheng Fu (cf7), Jiaying Li (jl63), Yanghui Pang (yanghui2) Leaderboard Name: Yanghui Pang Rank: ~55 Project File Structure (Download our project here): * train.py/train.ipynb: the program we train the classifier and predict the results for test data * answer.txt: the file storing predicted results for test data * saved_weight.pt: the saved model (the file is too large, we didn't push it to github) 3. Code Implementation * The process of training (what train.py/train.ipynb does): 1) Install packages and import necessary packages 2) Preprocessing Data: Data Loading + Data cleaning 3) Import the pre-trained BERT model and tokenizer 4) Tokenize the data 5) Create train/validate dataloaders 6) Initialize a BERT model, optimizer and loss function 7) Define functions for training and evaluating 8) Start training the model (also save the best model) 9) Predict the test data with the saved model * The models we tried to beat the baseline: First, we try to use traditional models from sklearn to access this problem. We tried Naive Bayes, SVM, LogisticRegression, RandomForestClassifier, KNeighborsClassifier and SGDClassifier from sklearn. Among all these classifiers, the SGDClassifier performed the best which has the highest f1 among them, but the score still cannot beat the baseline. Then, we analyzed the data we have and decided to train a convolutional neural net with pooling, but still found the result is not ideal. We also tried to use fine-tune BERT model with linear layer and dropout layer, but we found that the loss rate is relatively high. Finally, we tried the pre-trained BERT basic model from huggingface. By referencing the research papers, we adjusted the some hyperparameters such as learning rate and the number of epochs, and also tried various loss function. This pre- trained BERT model with adjusted parameters give us prediction that could beat the baseline. 4. Usage of Software (how to run our code) * Run Locally (With python version 3.8) pip3 install -r requirements.txt python3 train.py Or * Run on Colab (Tutorial for running our code on Colab) 1) Upload ""train.ipynb"" from our project 2) Create ""data"" folder, and upload ""train.jsonl"" and ""test.jsonl"" to ""data"" folder 3) Run the code block step by step * The trained model will be saved in ""saved_weight.pt""; Predicted results for test data will be saved in ""answer.txt"" 5. Division of Labor * Changcheng Fu: Tried training a neutral network with CNN and pooling layer; Tried BERT with linear and dropout layer; Tried a pretrained BERT basic model, achieved the best performance and beat the baseline; Wrote the progress report. * Jiaying Li: Tried built-in modules from sklearn such as RandomForestClassifier, KNeighborsClassifier, LogisticRegression; Wrote documentation; Presentation. * Yanghui Pang: Wrote the proposal; Tried built-in modules from sklearn such as Naive Bayes, SVM, SGDClassifier; Revised documentation; Presentation."
https://github.com/yhui288/CourseProject	Progress report.pdf	CS 410 Progress Report 1. Progress made thus far We have already implemented the BERT-based pre-trained model using python and then using linear transformation with train and validation set to fine-tuned the model for text classification. We've already finished the majority of our code but got F1 score of 70.14, which is below the baseline. 2. Remaining tasks We still need to make improvements to our code: adding CNN or doing data cleaning for our training and test set. In addition, we still need to make full documentation for our code and optimize our code if possible after beating the baseline. Also, we need to make our own tutorial video at the end to show our functionality and code. 3. Any challenges/issues being faced. Currently, we can not simply improve our F1 score by adding epoch or simply changing hyperparameter like learning rate, l2 regulation rate, or batch size to improve our performance. As a result, we need more efficient and effective method to increase our model's precision and recall.
https://github.com/yhui288/CourseProject	Proposal.pdf	CS 410 Final Project Proposal - Team1 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Yanghui Pang (yanghui2) Member: Changcheng Fu (cf7), Jiaying Li (jl63) 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We choose the text classification competition, and we are prepared to learn state-of-the- art neural network classifiers. Some neural classifiers that we heard of are the MLP classfier and Keras Neural Network Classifier. We also heard of deep learning frameworks such as PyTorch, TensorFlow. However, none of us have any relevant prior experience with such methods except mp assignments written in CS440: Artificial Intelligence. 4. Which programming language do you plan to use? Python
https://github.com/yhui288/CourseProject	README.md	"CourseProject - Text Classification Competition Team Member: Changcheng Fu (cf7), Jiaying Li (jl63), Yanghui Pang (yanghui2) Code Submission Find it in ""Code"" directory, or Click here to download our project Project Code: containing the code for training model, saved model, predicted results - train.ipynb/train.py: model training (instruction to run it can be found in ""Documentation.pdf"") - answer.txt: predicted reuslts - saved_weights.pt: the saved model (the file is too large, we didn't push it to github) Presentation Video Click here to watch the presentation video Documentation Find it in ""Documentation.pdf"". Containing the overview of the project, description of our code, instruction to run code, what we did, the division fo labor File Structure Documentation.pdf Proposal.pdf Progress Report.pdf"
https://github.com/ZhengyuLi97/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/NK10/CourseProject	Final Report.pdf	"1) An overview of the function of the code (i.e., what it does and what it can be used for). As part of this project, I am predicting if a given text (tweet) be SARCASM and NOT_SARCASM based on the response text. This can be extend to other text based application like sentiment analysis where we can find if a text is sarcasam or not and based on that mark the text as positive or negative sen timent or retrain the model for sentiment analysis. If we have to repurpose the code, we do have to train it on the text data so that the model learn the underlying details for better prediction. * 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. The code is implemented in python and using various library like Pandas, Numpy, transformer, sklearn. Below is the snapshot o f the versions of different packages * About Project * About data: * Importing libraries and loading data * Encoding the data to prepend and append the sentence with CLS and SEP token which is needed by BERT # padding data : To keep the fixed length for each row of the text data in train and test. # creating attention mask : Creating a attention mask for train and test data which is needed for the model # Converting data to torch tensor # Creating dataloader # Pre Processing of text data :To remove the noise from the text data. * Loading BertForSequenceClassification model : Loading pretrain BERT model. * Creating Scheduler : Needed for BERT * Training and validation of the model : Where I am training the train data and doing the testing on validation test * Prediction on testing data : Predicting the label for the test data. * Generating output file * References * I have trained the model on the Google Colab using the GPU option. Without the GPU the training was taking 15 hours - 20 hours. With GPU I was able to trained the model in 5 min -10 mins. I have added the comments on the code file for better readability and explaining the steps. Below are the high level steps. * 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Go to 'https://colab.research.google.com/notebooks/intro.ipynb' * Click on file, upload notebook and sign in with google account ( if you don't have an account, if possible create one. If it's not possible, please run it on any machine (preferred with GPU if not then CPU) with has the packages mentioned above. * * Browse the file 'Text Classification Competition.ipynb' * In order to run the code, we need an environment with the above packages. I would recommend to run it on Google Colab with GP U by following the below steps. * Once the 'Text Classification Competition.ipynb' is imported, please upload the test and train files by click on the arrow highlighted red below. If the arrow is not visible click on the folder icon on the left and then select the arrow: * Final Report Saturday, December 12, 2020 12:31 AM CS 410 Text Analysis Page 1 https://margaretmz.medium.com/running-jupyter-notebook-with-colab-f4a29a9c7156 # Please follow below link on how to import the file and run it on Google Colab GPU. * Once GPU is selected on Google Colab Runtime. click on Runtime on the menu and select ""Run all"" option as shown below * * Same has been explained in the video attached on the github. 4) Things I tried I have tried various method as mentioned below: i. I have done the preprocessing of the data like, removing stopwords, any character which is not alphanumeric, remove punctation. However with BERT only preprocessing I did was remove the following tokens ('@user','..','<url>'). ii. I have tried various model, Navies Bayes (various variation like, MultinomialNB,ComplementNB,BernoulliNB), Logistic Regression, SVM, however with all of these I was not able to beat the baseline. I was revolving around .66 -.70. Out of all these BernoulliNB was able to provide better result. iii. I have tried Neural networks as well but still not able to beat the baseline. I was still revolving around .66 -.70 iv. I have tried CNN with various filters and kernel but still not able to beat the baseline. I was still revolving around .66 -.70 v. I have tried LSTM with various combinations like, Bidirectional, different units (64,128,256) and also regularization but still it revolve around .68 -.71. vi. In the end I have tried pretrained uncase base BERT model and was able to beat the baseline. vii. 1) Smoothing parameter for Navies Bayes 2) Tried TFIDF with various ngram_range 3) Regularization L1 and L2 on deep neural network as well as other machine learning model. 4) Used the word embedding 'glove.twitter.27B.100d.txt' as well as 'glove.twitter.27B.200d.txt' in neural network model. 5) Tried adding dropouts, multilayer network in LSTM/BiDirectional LSTM. I have tried various hyperparameter tuning for these models as well like: CS 410 Text Analysis Page 2"
https://github.com/NK10/CourseProject	Progress Report.pdf	As part of the final project, I have opted for the classification problem. Below is the status for my progress: I have done the preprocessing of the data like, removing stopwords, any character which is not alphanumeric, remove punctation. a. I have tried various model, Navies Bayes, Logistic Regression, SVM, however with all of these I was not able to beat the baseline. I am still revolving around .66 -.70 b. I have tried Neural networks as well but still not able to beat the baseline. I am still revolving around .66 -.70 c. I have tried CNN with various filters and kernel but still not able to beat the baseline. I am still revolving around .66 -.70 d. I have tried LSTM with various combinations like, Bidirectional, different units (64,128,256) and also regularization but still it revolve around .68 -.71 e. Progress made thus far: 1) a. I am still working on improving my vocabulary as well as will try BERT to see if I can beat the baseline. Remaining tasks: 2) a. I am still trying to find on how to improve the model F1 score to beat the baseline. I think improving the vocabulary will be helpful but not sure what other things should I try. Any challenges/issues being faced: 3) Progress Report Sunday, November 29, 2020 10:45 AM CS 410 Text Analysis Page 1
https://github.com/NK10/CourseProject	Proposal.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team: 1) Nitin Kumar (UIN : 656280346). I am working as individual. Option selected : classification competition, Yes, i am prepared to learn state-of-the-art neural network classifiers Some of the Neural classifiers and deep learning frameworks that you may have heard of. LSTM, RNN,GRU Describe any relevant prior experience with such methods I have used some of these techniques for a proof of concept in one of my MOOC courses. Which programming language do you plan to use? I will be using the python and use Keras library and other libraries like Spacy, Gensim or NLTK
https://github.com/NK10/CourseProject	README.md	CourseProject Following is the short description of the files: 1) Final Report.pdf : This contain the answers for the questions. 2) Project_Code_walk_Through.mp4 : This is the video that shows walk thorugh of the code 3) Text_Classifocation_Competition.ipynb : This is the code for the classifciation competition 4) answer.txt : This will contains the outcome of the prediction of 1800 records given as test data. 5) test.jsonl : This contains the 1800 test records 6) train.jsonl : This contains the 5000 train records. 7) Progress Report.pdf : This is old file which i submitted for progress report task. 8) Proposal.md : This is initial proposal document i submitted at the begining of the project.
https://github.com/blake-wright/CourseProject	Course_Project_Proposal.pdf	Course Project Proposal 1. I, Blake Wright, will be working on this project individually. NetID: blakekw2 2. I will join the text classification competition. 3. I am ready to learn state-of-the-art neural network classifiers. I am in constant search for classifiers or frameworks that I will be using. BERT seems to be a very popular framework. However, I did also find XLNet which can empirically outperform BERT on some instances and RoBERTa which also outperforms BERT on the GLUE benchmark. I am interested in some of the later listed models that are based off BERT as I find it interesting that they tried to address key problems they found lacking in BERT. Experience: I have no experience in any of these methods. I have used Tensorflow very mildly to try to build a prediction model for stock and cryptocurrency prices. However, no experience in using text classification. I hope this project will further my interest in using these frameworks. 4. I plan on using Python as my programming language.
https://github.com/blake-wright/CourseProject	Progress Report.pdf	Blake Wright Nov 29, 2020 Progress Report - Text Classification 1. I have done more research and determined to use BERT (from Google) in order to complete my project. I have also done research on how to use BERT and some tutorials to familiarize myself with the libraries. I have also got my environment, libraries, and data set as needed and ran a short test to ensure I had everything. 2. The main pending task is the implementation of the coding and further documentation on my overall project. 3. The primary challenge was time. I have been busier than expected with my job the past couple weeks and haven't been able to really get to focus on the project. However, the next couple weeks I will be able to focus in and complete the project. I do feel like the implementation and producing a reasonable score will be the most difficult part. I am sure I will run into roadblocks during the coding.
https://github.com/blake-wright/CourseProject	README.md	"Classification Competition - Analyzing Twitter Tweets Video link to presentation: https://drive.google.com/file/d/17dMSY3kKD93lZayn_cvysVckzSeQaHD7/view?usp=sharing In the video I go over most of this README besides the setup. I do have another video below that is my own video of the environment setup. Video link to environment setup (farther down you will see the videos from Jeff Heaton where I sourced this information): https://drive.google.com/file/d/1hQFQuth2hXUuuUFBRt_a4RNttmnmoQ8x/view?usp=sharing conda env create -v -f tensorflow.yml python -m ipykernel install --user --name tensorflow --display-name ""Python 3.7 (tensorflow)"" Setting up your environment You will need the following libraries to successfully run my project: Library Version Used Pip install cmd Tensorflow 2.3.1 pip install tensorflow Sklearn-learn 0.23 pip install sklearn Transformers 3.5.1 pip install transformers Pandas 1.1.3 pip install pandas Numpy 1.18.5 pip install numpy Torch 1.7.1 pip install torch The following videos can be used as a reference on how to setup a miniconda python environment if you don't have any of the libraries and want some setup automatically. However, for the tensorflow.yml file you will want to update the tensorflow=2.0 to tensorflow=2.3.1. Or just copy the below. ``` name: tensorflow dependencies: - python=3.7 - pip>=19.0 - jupyter - tensorflow=2.3.1 - scikit-learn - scipy - pandas - pandas-datareader - matplotlib - pillow - tqdm - requests - h5py - pyyaml - flask - boto3 - pip: - bayesian-optimization - gym - kaggle ``` For Windows: https://www.youtube.com/watch?v=RgO8BBNGB8w For MacOS: https://www.youtube.com/watch?v=MpUvdLD932c&t=372s Running the project Before trying to run the project note that this is a very resource demanding program. I have tested it on the following pieces of hardware. Desktop: CPU: 3.9 GHz Ryzen 7 3800X RAM: 32GB DISK: < 10GB available Laptop: CPU: 3.1 GHz i5 RAM: 16GB DISK: < 200GB available If your hardware isn't able to run please contact me and you can use my system. I am working on additional ways to test on lower RAM devices. To run this project: If you have jupyter notebook you can locate the file, select file, and choose the Cell tab and then select Run All. Structure of the project I first imported the data that was given by using the pandas library. ``` read in train & test data trainData = pd.read_json('train.jsonl', lines = True) testData = pd.read_json('test.jsonl', lines = True) ``` Next I converted the labels (""SARCASM"" and ""NOT_SARCASM"") to binary values. This was done because the model requires a binary label to respond to. for i in range(len(trainData)): if trainData['label'][i] == ""SARCASM"": trainData['label'][i] = 1 else: trainData['label'][i] = 0 From here I set my modelId, tokenizer, and model. I choose to include case and I felt that sometimes when people are sending out sarcastic tweets they may often use letter case to further voice their sarcasm. As you can see I used the AutoTokenizer from the transformers library as I would not have to switch it when using different models. I initiailly wrote this project using DistilBert, which is far faster than Bert and is almost as accurate. I also did try using BERT, I did get better results (~2% accuracy) but when I uploaded them to the leaderboard they were slightly worse. I also tried XLNet and RoBERTa but with my code they were performing up to 10% less accurate than BERT. I believe this was because I was not able to configure them as precisely. ``` setting modelId, tokenizer, and model modelId = ""distilbert-base-cased"" tokenizer = AutoTokenizer.from_pretrained(modelId) model = DistilBertModel.from_pretrained(modelId) ``` Next up was getting all of the data ready for the model. Tokenizing the data, padding the lengths, and masking so the padding was not used. ``` tokenized = trainData['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens = True))) tokenized_2 = testData['response'].apply((lambda x: tokenizer.encode(x, add_special_tokens = True))) trainPad = pad_sequences(tokenized, maxlen = 100, padding='post') testPad = pad_sequences(tokenized_2, maxlen = 100, padding='post') trainMask = np.where(trainPad != 0,1,0) testMask = np.where(testPad != 0,1,0) converting to int64 trainInput = torch.tensor(trainPad).to(torch.int64) testInput = torch.tensor(testPad).to(torch.int64) converting to tensor type trainMask = torch.tensor(trainMask) testMask = torch.tensor(testMask) ``` I concurrently modeled both the training set and the testing set. I have done these concurrently because upon prediction the input will have to match what it was trained against. with torch.no_grad(): output = model(trainInput, attention_mask = trainMask) with torch.no_grad(): outputTest = model(testInput, attention_mask = testMask) Here I prepared features and labels that will be used to train and test. trainFeats = output[0][:,0,:].numpy() testFeats = outputTest[0][:,0,:].numpy() labels = trainData['label'] trainFeats, valFeats, train_labels, test_labels = train_test_split(trainFeats, labels) I tried many, many different classifiers and RandomForestClassifier teneded to fair the best at ~77%. 77% accuracy was lower than expected and desired. I will expand on improvements in the future in the 'Improvement' section. classifier = RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=8, random_state=2) classifier.fit(trainFeats, train_labels) classifier.score(valFeats, test_labels) For the rest of the program I am just creating the answer file and writing to it. There is a small bug where it sometimes does not write the entire file. I am troubleshooting this problem. I included a print screen so you could see the results being written. ``` results = classifier.predict(testFeats) f = open(""answer.txt"", ""w"") print(results[1799]) for i in range(len(results)): id = str(testData['id'][i]) answer = str(results[i]) f.write(id) f.write(',') if answer == ""1"": f.write(""SARCASM"") else: f.write(""NOT_SARCASM"") f.write('\n') print(id, answer) ``` Improvements I spent a large (maybe too much) time trying to train the model instead of using a pretrained model for BERT. I was not very successful in this and wish I could have had more time to expand on it as I feel like this would have greatly increased results. I also considered manipulating the tweets. Some things I considered were taking out common words that would be used in a sarcastic or not sarcastic and throwing them out as my model may have been able to better train on relevant information. There was also consideration on furthering expanding weighting on the hashtags found in tweets as many of them seemed to correlate strongly from my point of view. However, I was not sure how to go about this. I did not use the context as part of the analysis as well. I feel like this could have been a big improvement without too much more additional code. However, time was a factor in this project and I was not able to complete this task. Credit/Documentation Huggingface's website was a great help. The transformers library is maintained by them which was used in this project. They also provide ample of documentation on how to use them. I also found their examples extremely useful in understanding the flow of the program. I have included links to both the home page and to the example. https://huggingface.co/ https://huggingface.co/transformers/model_doc/distilbert.html (Documentation on library) https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb Jay Alammar was also a great help to this project. His visual guide helped fill in knowledge gaps of how each part of the model worked and which types it needed. (http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)"
https://github.com/Clara9/LARA_Reproduce_410	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Clara9/LARA_Reproduce_410	The Proposal.docx	The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/Clara9/LARA_Reproduce_410	The_Proposal.pdf	The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/vickyli99/CourseProject	Progress Report.docx	Progress Report We have implemented the 5.1 mentioned in the paper, which includes importing data set downloaded from the website, converting word into lower cases, removing punctuations, stop words, and stemming each word to its root. Our remaining task is to implement the rest of the functions from the paper, including 5.2 - 5.4, and get the result from our program by the end of this project. The formulas mentioned in the paper are a bit confusing to us, and we will make sure we can understand the math behind it so that we can finish this project.
https://github.com/vickyli99/CourseProject	README.md	CourseProject Course Project for CS 410: Reproducing a paper, Latent Aspect Rating Analysis without Aspect Keyword Supervision. Paper link: https://www.cs.virginia.edu/~hw5x/paper/p618.pdf Stage 1 By Nov 29 Reproduce of Step 5.1 file: test.py First we remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects). Then we convert all the words into lower cases and remove punctuations and stop words. In vocab.txt we write vocabulary appearance based on reviews. If a word appears several times in the same review, it would only be counted as once. We then filtered out words that have less than ten occurrences. Step 5.2 In Progress Documentation 1.Overview This project consists of tasks of preprocessing data and implementing LARA functions. We get the data from http://timan.cs.uiuc.edu/ downloads.html and we focused on TripAdvisor data for this project. 2.Programming Language and Packages Python 3.X Packages: numpy, scipy, math, re, random, nltk 3.Implementation Clean.py This is the python program for preprocessing the data, we did the following for this part: 1) remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects); 2) convert all the words into lower cases; and 3) removing punctuations, stop words, and the terms occurring in less than 10 reviews in the collection. Lara.py This is the main program we implemented all the functions for building this LARA model. In this program, we implemented function such as update_mu, update_beta, E_step, M_step etc. Load.py This is the python program we have to load our data and build our vocabulary. 4.Project Members Ziyuan Wei (ziyuan3@illinois.edu) Xinyi He (xinyihe4@illinois.edu) Weijiang Li (wl13@illinois.edu) Dingsen Shi (dingsen2@illinois.edu) Qunyu Shen (qunyus2@illinois.edu) We decided to collaborate with another team, led by Xinyi He, half way through the project since we met some challenges when understanding the methods used in the paper. Then we splitted our tasks between two groups, our group (led by Weijiang Li, collaborating with Ziyuan Wei) focuses on the implementation of preprocessing and EM steps in the building up the model, and the other group contributed to the rest of the functions such as negative likelihood, and another main part of this project is the implementation of bootstrap. Team members from both team worked hard to try to get the code done based on the method description from the paper. When implementing EM steps, we separate the procedure into two functions, e-step() and m-step(), before adopting a new function runEM() to combine and output the previous data. 5. Video Link Here is the link to the demo video on mediaspace: https://mediaspace.illinois.edu/media/t/1_fo2gtfej
https://github.com/vickyli99/CourseProject	The Proposal.docx	The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/vickyli99/CourseProject	The_Proposal.pdf	The Proposal Names: Weijiang Li, Ziyuan Wei NetID: wl13, ziyuan3 Captain: Weijiang Li Paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Progaming language: Python Dataset: Yes both datasets used in the paper can be obtained from http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/yuranw3/CourseProject	README.md	CourseProject Team-RPDD Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Group Name: Team-RPDD Presentation Link https://mediaspace.illinois.edu/media/t/1_fylki6t8
https://github.com/yuranw3/CourseProject	Team-RPDD progress report.pdf	Team Name: Team-RPDD Team Member: Yuran Wang netid: yuranw3 [captain] Hongru Wang netid:hongru2 Zhengyu Li netid: zhengyu6 1) Progress made thus far We've read the papers.We have started writing code. We have not finished the coding session and still in progress. 2) Remaining tasks We will debug our program soon once we finish it. 3) Any challenges/issues being faced We have not encountered any specific challenges yet.
https://github.com/yuranw3/CourseProject	Team-RPDD project documentation.pdf	"Documentation Team: Team-RPDD Team member:Yuran Wang (netid:yuranw3) [captain] Zhengyu Li (netid:zhengyu6) Hongru Wang (netid:hongru2) Project: Reproducing A Paper(Latent aspect rating analysis) 1) An overview of the functions of the code: a) Create vocabulary from the dataset. b) Use stemmer on the dataset to create a vocabulary corpus of the reviews, which will be used for aspect mining and rating. c) Use BootStrapping to mine aspects(stored in\Data\Seeds\hotel_bootstrapping.dat ), use regression to calculate weight for each aspect. d) Calculate rating per minded aspects. e) The results are in Data/Vectors/vector_CHI_4000.dat"" 2) Implementation documentation: a) Acknowledgment: The authors of the paper released the codes for LARA on his personal website(http://sifaka.cs.uiuc.edu/~wang296/) . We modified the original version of the code for our projects. b) Review dataset sources: http://sifaka.cs.uiuc.edu/~wang296/Data/index.html c) Setup instructions: i) Add colt.jar, concurrent.jar, opennlp-maxent-3.0.1-incubating.jar, opennlp-tools-1.5.1-incubating.jar, and JRE System Library[Java-SE-1.7] to the libraries of Java Build Path. Then run the ""Analyzer"" file. 3) Usage documentations: a) In order to run the code, the user needs to import tools from opennlp. You can import the core toolkit directly from Maven. Then run mvn install. The detailed dolumentation can be found here https://github.com/apache/opennlp. b) It requires tools from NTLK c) In order to run the code, the user also needs Python3 and Java. 4) Work distribution: a) We worked together so everyone on the team participated. 5) Final report: The authors of the paper developed the LARAM to effectively solve the problem of LARA, including automatically identifying meaningful topical aspects, inferring interesting differences in aspect ratings within reviews, and modeling users' preferences with the inferred relative emphasis on different aspects. Such detailed analysis of opinions at the level of topical aspects enabled by LARAM can support multiple application tasks, including aspect opinion summarization, ranking of entities based on aspect ratings, and analysis of reviewers rating behavior. Our project borrowed the ideas from the author's work about LARA, and our work is mainly based on aspects-mining."
https://github.com/yuranw3/CourseProject	Team-RPDD project proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team member1:Yuran Wang (netid:yuranw3) [captain] Team member2:Zhengyu Li (netid:zhengyu6) Team member3:Hongru Wang (netid:hongru2) 2. Which paper have you chosen? We choose option 1: Reproducing a paper as our project. And we choose paper Latent aspect rating analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? Yes
https://github.com/Madokami/CourseProject	CS410 project proposal.pdf	CS410 Project Proposal In your project proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. xuzhizs2@illinois.edu (individual project) 2. Which competition do you plan to join? Text classification competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I'm planning to learn about using tensor flow to build neural network classifiers. I've had some experience before doing image classification with OpenCV. 4. Which programming language do you plan to use? Python
https://github.com/Madokami/CourseProject	Final report.pdf	Text Classification Competition Project demo youtube link In case the youtube video is unavailable, the video can also be found in the github repo, named project_demo.wmv Environment setup Requires python3.8 and tensorflow already setup. One easy way is to directly use the tensorflow docker container which already has python and tensorflow installed: tensorflow/tensorflow:latest Running the code python3 train.py // this will generate the ML model used to make classifications python3 evaluate.py // evaluates the test dataset and output results How the training works The model trained is a RNN (recurrent neural network). For each line of input, the response and the entire context, are read and tokenized as a text vector. Specifically, a vocabulary is created from all the words observed, so each word could be represented using an integer between 0 and the maximum vocabulary size. Each tokenized line along with its corresponding tag are then fed into the model for training.
https://github.com/Madokami/CourseProject	progress_report.pdf	Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? 1. I set up a docker container to train a tensorflow recurrent neural network for the text classification competition. So far the best tf1 I got is around 0.69, which is about 0.03 away from the baseline. 2. Need to work on improving the model to beat the baseline and create a video and documentation on running the model. 3. Improving the model is easy, and takes a lot of trial and error.
https://github.com/Madokami/CourseProject	README.md	CourseProject The topic of this course project is Text Classification Competition. Project demo video link: https://youtu.be/44ZIyAuVs78
https://github.com/shashivrat/CourseProject	CS410 - Twitter Text Classifier.pdf	Project Proposal - Text Classification Competition 1. Team Name: SSW Classifiers Members: * Saravana Somasundaram (captain) - ss129 * Shashivrat Pandey - spandey6 * Walter Tan - wstan2 2. Competition - Text Classification 3. Neural Networks: a. Convolutional Neural Network (CNN) b. Recurrent Neural Network (RNN) c. Hierarchical Attention Network (HAN) 4. References: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han- f0e887214d5f c. https://realpython.com/python-keras-text-classification/#convolutional-neural- networks-cnn 5. The programming language we will be using is Python. Our group is prepared to learn state-of-the art network classifiers. We are excited to learn techniques to improve our ML skillset and will apply what we learned to our current/future work projects. Some Deep Learning frameworks we've heard of include PyTorch, TensorFlow, Kearas, and Sonnet. TensorFlow and PyTorch seem to be the most popular and used by many users and institutions worldwide. Our group has never worked with these technologies, but are excited to learn these new technologies for this competition. We will be using Python for this project and we will try our best to come up with an optimized code to improve the performance of application.
https://github.com/shashivrat/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jkghub/CourseProject	Proposal.docx	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Jeffrey Kuhn (kuhn9) I plan to complete the project on my own. Which paper have you chosen? ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No, not the identical dataset. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, it's possible to get a very similar dataset. I can retrieve articles and reviews from BBC / CNN. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?"
https://github.com/jkghub/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/philipcori/CourseProject	ProgressReport.pdf	CS 410 Final Project Progress Report: Improving a System Captain: Philip Cori (pcori2), Team Member 1: Henry Moss (htmoss2), Team Member 2: Kyle Maxwell (kylem6) Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Progress Made So Far Our team has been able to recreate the data results from the paper above, using RecQ, a Python library for recommender systems which includes the SERec algorithm. We have collected preliminary results on a particular approach for measuring the closeness between friends. We experimented with using the number of mutual friends as the driving factor in measuring closeness. Specifically, we used the formula: Closeness = m / n Where m is the number of mutual friends between the two friends and n is the total number of friends between the two friends. This seemed like a reasonable approach given that based on intuition, it seems two friends would be closer if they have more mutual friends. Not only this, but two friends are also considered closer if they have less total friends, which gives more weight to their own friendship. All results use 5-fold cross validation. Baseline results of SERec algorithm recreated on local machine: - Precision: 0.0449469214437 - Recall: 0.455598102652 - F1: 0.0818217388808 - MAP: 0.146067212641 - NDCG: 0.334412555937 Results using mutual friends closeness measure: - Precision: 0.0455850128798 - Recall: 0.461858940973 - F1: 0.0829799791008 - MAP: 0.149447844847 - NDCG: 0.340728707592 As can be seen, we've managed to increase the NDCG measure by 1.89% and the MAP measure by 2.31%. The second improvement strategy we are exploring is the modification of the matrix factorization model in the Rating Component of the SERec Boost algorithm. Incorporating the Weighted Rating Matrix Factorization methods from Collaborative Filtering for Implicit Feedback Datasets (Yifan Hu et al, KDD 2009). This matrix factorization model uses the implicit feedback data as an indication of positive and negative preference associated with vastly varying confidence levels. Primarily, we are layering this model to initialize the latent factors theta and beta, the user preferences and item attributes respectively. The social exposure component is then incorporated afterwards, only requiring a small number of iterations for the expectation-maximization algorithm to compute. Results from modified matrix factorization: Top 100, 5-fold cross validation - Precision: 0.0493 - Recall: 0.4993 - F1: 0.0897 - MAP: 0.1656 - NDCG: 0.3702 This is a 10.7% increase in NDCG and 13.4% increase in MAP score from the baseline results. Remaining Tasks Regarding measuring closeness between friends, we will continue to experiment with other social network concepts in measuring closeness. The Lastfm dataset also contains data not currently used by the SERec algorithm, so we will also investigate whether this can be used as well. This data includes data about tags on different artists and which users placed which tags. We are also given time stamps to record when such tagging events occurred. Regarding the matrix factorization, there is plenty of potential to improve the integration of the Weighted Rating Matrix Factorization model. As mentioned previously, we layered the components, but it seems that the Social Exposure Component could be incorporated in all iterations of the WRMF training. Additionally, we will likely conduct tests of significance for our attempted improvements to tell whether they are actual improvements or due to randomness in the data. Challenges/Issues Faced So Far Understanding the architecture and class structure of the RecQ system has taken the majority of our work so far. It takes time to identify code areas that are adjustable that will still keep the system functioning. An additional challenge is the time needed to collect results. Running the 5 iterations of the EM algorithm proposed takes roughly 30 minutes to complete. Furthermore, it has been challenging to improve the model based on social contagion, as suggested in the original paper. Social contagion is such a broad concept that it has been more difficult than expected to understand how to quantify and incorporate this idea into our preexisting model. We will continue looking into this as well as researching any possible improvements to be found from social structural influence, which was another method mentioned in the original published paper.
https://github.com/philipcori/CourseProject	ProjectDocumentation.pdf	"CS 410 Course Project: Improving a System - Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Captain: Philip Cori (pcori2), Team Member 1: Henry Moss (htmoss2), Team Member 2: Kyle Maxwell (kylem6) Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation Overview Our project is improving the recommendation algorithm discussed in this paper . To improve this system, we built on an existing recommendation system framework called RecQ . This system provides a host of different recommendation algorithms and datasets to test them on, including the one proposed in this paper. The Paper Key Excerpt from the Abstract This paper is concerned with how to make efficient use of social information to improve recommendations. Most existing social recommender systems assume people share similar preferences with their social friends. Which, however, may not hold true due to various motivations of making online friends and dynamics of online social networks. Inspired by recent causal process based recommendations that first model user exposures towards items and then use these exposures to guide rating prediction, we utilize social information to capture user exposures rather than user preferences. We assume that people get information about products from their online friends and they do not have to share similar preferences, which is less restrictive and seems closer to reality. Relevant Summary Of the two methods presented in the paper, we focused on the method they describe as social boosting. We will refer to this algorithm as SERec and more specifically, SERec Boost. The primary assumption leveraged by social boosting is that a user's exposure to an item is boosted by their friends. People are likely to receive information about a product from friends' discussion and shared feelings. As such, a consumer is more likely to have exposure to an item if their friends have interacted with the item. This can help the model disambiguate between the situations in which a user did not interact with an item because they dislike the item, or they did not notice the item. SERec has two main components, the Rating Component and the Social Exposure Component. The Rating Component is a matrix factorization model for rating prediction. The Social Exposure component calculates the exposure priori for each user item pair. The modularity of the system lends itself to extensibility and improvement. As suggested in the conclusion of the paper, the system presents itself with multiple avenues for exploring alternative methods for matrix factorization, and for integrating social exposure information. Software Implementation For information about the implementation and architecture of the entire RecQ system, please refer to the original repository. Our implementations build on the architecture of this system by adding our custom algorithm extensions to the RecQ framework. Development Environment Setup 1. Install miniconda 2. Create a new miniconda environment with Python 2.7 3. Run conda install mkl-source 4. Clone this repository 5. Download the lastfm dataset and add it to a folder named ""dataset"" within the repository 6. Activate the environment and install dependencies with conda install --file requirements.txt 7. Run python main.py 8. Follow the prompt and entire the desired algorithm to run Usage The original SERec boost algorithm discussed in the paper can be run by inputting the corresponding value (""s10"") for the SERec algorithm. Similarly, each of our individual extensions can be run by inputting the corresponding value displayed in the prompt. Users can compare results with the baseline results of the original SERec boost algorithm: Evaluation statistics are generated for a recommendation list of length 100. Precision 0.0479560590416 Recall 0.485595079529 F1 0.0872914462838 MAP 0.156959948956 NDCG 0.357199124032 Team Member Contributions Philip Cori Algorithm description I implemented an algorithm for measuring friend closeness, which is a suggested extension mentioned in the original paper. It can be run by inputting ""s11"" upon running main.py. To measure friend closeness, I use the formula: closeness = n / m , where n is the number of mutual friends between the two friends, and m is the total number of friends between the two friends. The intuition behind this is that it would seem two friends are closer friends if they share many mutual friends. More mutual connections should imply more personal interaction, exposure, and therefore closeness. Furthermore, two friends are considered closer if they have less total friends, which gives more weight to their own friendship. Results Three different formulas were tested based on the above idea. 1. Closeness = n / m * 10 It can be seen that unfortunately the results did not improve from the above implementation. All measures are within 0.04% of the baseline results. From further analysis, I found that only 9.4% of friendships consist of friends with any mutual friends. This low percentage can partially explain why adding a closeness measure may not have much impact, since it depends largely on the number of mutual friends as the determining factor. On the right certain statistics are shown to give an idea of the distribution of the closeness between friends. It seems that there is a fairly wide distribution of closeness measures, which could lead to unstable results as well. 2. Closeness = n 2 / m * 10 Precision 0.0478126532299 Recall 0.48445040571 F1 0.0870353717949 MAP 0.157030933278 NDCG 0.35660261507 Mean 1.177611443146 Median 0.666666666666 Stdev 1.583484225324 Min 0.072463768115 Max 10.0 Next, I tried squaring the number of mutual friends. The logic behind this was to give this factor even more weight, such that the closeness benefits quadratically with the more mutual friends they have. However, it seemed to diminish our results slightly further. It likely over-weighed some closeness terms, causing other friend exposures to be dominated by relationships with even only a few mutual friends. 3. Closeness = log 2 (n/m + 1) * 10 Given that squaring mutual friends further decreased results, I tried using a log transform that instead introduces diminishing returns from a higher closeness score. It can be seen that the standard deviation of the closeness measure is much less and every connection is being treated more equally. I add 1 to n/m to prevent any closeness measures from becoming negative. The results now are actually a slight improvement over the original baseline results. Although precision deteriorated slightly, NDCG, MAP, F1, and Recall improved by 1.06%, 1.28%, 1.08%, and 1.11% respectively. Conclusions From these results, it can be concluded that using mutual friends and total number of friends can accurately model the closeness between two friends. However, as discovered by experimenting different transformations of this idea, it is important not to smooth this measure slightly and not overweight certain connections. A dataset that would yield even better results Precision 0.0473455166473 Recall 0.479583256631 F1 0.086182863339 MAP 0.15522424781 NDCG 0.353131761035 Mean 1.3374456997343211 Median 0.7142857142857142 Stdev 1.965461767041007 Min 0.07246376811594203 Max 39.67032967032967 Precision 0.0484710617097 Recall 0.491026838687 F1 0.0882323703985 MAP 0.158975281865 NDCG 0.360987047292 Mean 0.920228915514409 Median 0.7369655941662061 Stdev 0.6752581440071108 Min 0.10092890885078087 Max 3.4594316186372978 for the introduced formula would contain a more dense social network where more mutual friends are present, as well as a pattern that friendships with a higher n/m measure do in fact imply a stronger correlation between the way two friends ""rank"" items (ie. artists in the Lastfm dataset). Henry Moss For this project, I tried to evaluate how we could incorporate social contagion into the existing recommendation algorithm, as this was one of the last suggestions in the original paper for further improvement. Unfortunately, after a lot of research into social contagion, it seems to be a fairly arbitrary concept and difficult to measure. The recommendation algorithm is already trying to calculate how much one user is potentially influenced by the friends they are in contact with, which is a simple definition of what social contagion is. Additionally, I helped Philip experiment with his algorithm for measuring friend closeness. After trying to find a way to implement TF-IDF weighting into our algorithm, I hypothesized that it could be beneficial to have diminishing weight on increased number of friends, so that it values closer friends at an increased rate. This boosted our NDCG values from around 0.356 to 0.36. After that, I tried other rates of diminishing return with different log powers of l og 3 and l og 10 , along with Philip's idea to prevent any of the values from being negative, but found that neither were as successful as l og 2 . 1. Closeness = log 3 (n/m + 1) * 10 2. Closeness = log 10 (n/m + 1) * 10 Precision 0.0339437367304 Recall 0.344395578472 F1 0.0617967648459 MAP 0.0755357909461 NDCG 0.216285701749 Mean 0.9437401212835262 Median 0.5874549356790257 Stdev 1.07063674944364 Min 0.06572152931440885 Max 6.309297535714574 These were both unsuccessful ideas, as using log3 decreased the MAP and NDCG scores by 51.8% and 39.5%, while using log10 decreased the MAP and NDCG scores by 65.2% and 48.9%, respectively. I also looked into social structural influence and decided that one way to try to implement this into our project was by stretching the data to include friends of friends, in addition to just counting the closeness of direct friendships. I tried changing n, which was originally the number of mutual friends between user 1 and user 2, to also include any mutual friends of user 2's friends. This expands the network out a degree, and I was hoping that with more data to work with, the recommendation algorithm could be more successful. 3. Closeness = n / m * 100, where n is the number of mutual friends and friends of friends As you can see, this slightly lowered our MAP and NDCG score, which we have been using as our main measurements of improvement. I experimented with a few other factors into the algorithm, such as using log 2 again, but these were the best results I came up with overall. Kyle Maxwell Algorithm description Precision 0.0315127388535 Recall 0.319221388131 F1 0.0573627683574 MAP 0.0546393023159 NDCG 0.182298909362 Mean 0.4553789242585401 Median 0.2802872360024353 Stdev 0.5136685928528778 Min 0.03135713852858582 Max 3.0102999566398116 Precision 0.0446282847314 Recall 0.452188532552 F1 0.0812387859384 MAP 0.144929322365 NDCG 0.332269062625 Mean 11.607520985265921 Median 6.896551724137931 Stdev 15.32296991067004 Min 0.5681818181818182 Max 100.0 The potential improvement I explored was the modification of the matrix factorization model in the Rating Component of the SERec Boost algorithm. By incorporating the Weighted Rating Matrix Factorization methods from Collaborative Filtering for Implicit Feedback Datasets (Yifan Hu et al, KDD 2009) , I was able to achieve small but successful improvements. This matrix factorization latent factor model uses implicit feedback data as an indication of positive and negative preference associated with varying confidence levels. Meaning, it not only models the user's preferences of an item, but models the probability that they have consumed the item. I utilized this implicit feedback model to first converge on the user and item latent factor vectors without any use of the social connectivity information. Then, I used the SERecBoost Social Exposure Component to update the social exposure prior based on those latent factors. Finally, the original SERecBoost algorithm is initialized with the pre-trained latent factors, and only requires a small number of iterations for the expectation-maximization algorithm to adequately converge. Results Statistics at 100 Recommendations, 5-fold cross validation Conclusions As suggested in the original Collaborative Filtering with Social Exposure, there is still room for additional work and improvement. By utilizing other novel matrix factorization techniques, we were able to achieve 5.6% MAP and 3.7% NDCG score increase. Precision 0.0492 Recall 0.4981 F1 0.0895 MAP 0.1657 NDCG 0.3705"
https://github.com/philipcori/CourseProject	ProjectProposal.pdf	"Project Proposal: Improving a System System: https://arxiv.org/pdf/1711.11458.pdf 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Philip Cori, pcori2 Team Member 1: Henry Moss, htmoss2 Team Member 2: Kyle Maxwell, kylem6 2. What system have you chosen? Are you adding a function or improving a function? What function? We will improve on the recommender system proposed in Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation by Menghan Wang, Xialin Zheng, and Yang Yang. We will attempt to improve the system in 3 ways: * Write a function to measure closeness between friends, as suggested by the paper. * Write a function that performs matrix factorization for the the Rating Component * Improve model based on recent social network analysis techniques (examples listed are social contagion and/or social structural influence). One possibility is to weigh items by an IDF term based on the items' number of occurrences in exposures between friends. 3. If you are adding a function, why is the new function important or interesting? How will it benefit the users? If you are improving a function, what are the main limitations of the current function? How are you going to improve it? How will your improvements benefit the users? Implementing these new functions could improve the MAP, recall, and NCDG metrics. Firstly, a limitation of the current algorithm is that it assumes all friends are equally close. Therefore, if we can successfully measure ""closeness"" between friends, this can allow us to give more accurate recommendations. Secondly, the current algorithm does not use matrix factorization for the rating component. Implementing this can improve our metrics. Lastly, if we are able to use IDF weighting to filter out ""main-stream"" recommendations, our users will get more unique recommendations. This will make the recommendations that they receive from similar users more genuine and useful. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? The MAP, recall, and NDCG metrics will be used to determine if our experiments improved the recommender system. 5. How will your code communicate with or utilize the system? We will directly extend the libraries of the framework and recommender system. 6. Which programming language do you plan to use? We will use Python to implement our extensions. 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Implementing these functions will take considerable time for three reasons. Firstly, it will take time to fully understand the approach presented in the paper. Secondly, it will take time to understand the open source framework that implements the approach discussed. Lastly, it will take time to implement and experiment with the new functions. Each person will do: * 2h Team Meetings * 1h Read Paper * 1h Getting started with framework * 2h Research into possible feature improvements * 10-20h Implementation of feature improvement * 2h Evaluation, Reporting"
https://github.com/philipcori/CourseProject	README.md	"Course Project Project Documentation https://github.com/philipcori/CourseProject/blob/main/ProjectDocumentation.pdf Project Presentation https://www.youtube.com/watch?v=U2XECocV8fU&feature=youtu.be RecQ Introduction Founder: @Coder-Yu Main Contributors: @DouTong @Niki666 @HuXiLiFeng @BigPowerZ @flyxu RecQ is a Python library for recommender systems (Python 2.7.x) in which a number of the state-of-the-art recommendation models are implemented. To run RecQ easily (no need to setup packages used in RecQ one by one), the leading open data science platform Anaconda is strongly recommended. It integrates Python interpreter, common scientific computing libraries (such as Numpy, Pandas, and Matplotlib), and package manager. All of them make it a perfect tool for data science researcher. Besides, GPU based deep models are also available (TensorFlow is required). Latest News 22/09/2020 - DiffNet proposed in SIGIR'19 has been added. 19/09/2020 - DHCF proposed in KDD'20 has been added for comparison, althought it doesn't work very well. 29/07/2020 - ESRF proposed in my TKDE manuscript (under review) has been added. 23/07/2020 - LightGCN proposed in SIGIR'20 has been added. 17/09/2019 - NGCF proposed in SIGIR'19 has been added. 13/08/2019 - RSGAN proposed in ICDM'19 has been added. 09/08/2019 - Our paper is accepted as full research paper by ICDM'19. 02/20/2019 - IRGAN proposed in SIGIR'17 has been added (tuning...) 02/12/2019 - CFGAN proposed in CIKM'18 has been added. 02/04/2019 - NeuMF proposed in WWW'17 has been added. 10/09/2018 - An Adversarial training based Model: APR has been implemented. 10/02/2018 - Two deep models: DMF CDAE have been implemented. 07/12/2018 - Algorithms supported by TensorFlow: BasicMF, PMF, SVD, EE (Implementing...) Architecture of RecQ Features Cross-platform: as a Python software, RecQ can be easily deployed and executed in any platforms, including MS Windows, Linux and Mac OS. Fast execution: RecQ is based on the fast scientific computing libraries such as Numpy and some light common data structures, which make it run much faster than other libraries based on Python. Easy configuration: RecQ configs recommenders using a configuration file. Easy expansion: RecQ provides a set of well-designed recommendation interfaces by which new algorithms can be easily implemented. Data visualization: RecQ can help visualize the input dataset without running any algorithm. How to Run it 1.Configure the **xx.conf** file in the directory named config. (xx is the name of the algorithm you want to run) 2.Run the **main.py** in the project, and then input following the prompt. How to Configure it Essential Options Entry Example Description ratings D:/MovieLens/100K.txt Set the path to input dataset. Format: each row separated by empty, tab or comma symbol. social D:/MovieLens/trusts.txt Set the path to input social dataset. Format: each row separated by empty, tab or comma symbol. ratings.setup -columns 0 1 2 -columns: (user, item, rating) columns of rating data are used; -header: to skip the first head line when reading data social.setup -columns 0 1 2 -columns: (trustor, trustee, weight) columns of social data are used; -header: to skip the first head line when reading data recommender UserKNN/ItemKNN/SlopeOne/etc. Set the recommender to use. evaluation.setup -testSet ../dataset/testset.txt Main option: -testSet, -ap, -cv -testSet path/to/test/file (need to specify the test set manually) -ap ratio (ap means that the ratings are automatically partitioned into training set and test set, the number is the ratio of test set. e.g. -ap 0.2) -cv k (-cv means cross validation, k is the number of the fold. e.g. -cv 5) Secondary option:-b, -p, -cold -b val (binarizing the rating values. Ratings equal or greater than val will be changed into 1, and ratings lower than val will be changed into 0. e.g. -b 3.0) -p (if this option is added, the cross validation wll be executed parallelly, otherwise executed one by one) -tf (model training would be conducted on TensorFlow if TensorFlow has been installed) -cold threshold (evaluation on cold-start users, users in training set with ratings more than threshold will be removed from the test set) item.ranking off -topN -1 Main option: whether to do item ranking -topN N1,N2,N3...: the length of the recommendation list. *RecQ can generate multiple evaluation results for different N at the same time output.setup on -dir ./Results/ Main option: whether to output recommendation results -dir path: the directory path of output results. Memory-based Options similarity pcc/cos Set the similarity method to use. Options: PCC, COS; num.shrinkage 25 Set the shrinkage parameter to devalue similarity value. -1: to disable simialrity shrinkage. num.neighbors 30 Set the number of neighbors used for KNN-based algorithms such as UserKNN, ItemKNN. Model-based Options num.factors 5/10/20/number Set the number of latent factors num.max.iter 100/200/number Set the maximum number of iterations for iterative recommendation algorithms. learnRate -init 0.01 -max 1 -init initial learning rate for iterative recommendation algorithms; -max: maximum learning rate (default 1); reg.lambda -u 0.05 -i 0.05 -b 0.1 -s 0.1 -u: user regularizaiton; -i: item regularization; -b: bias regularizaiton; -s: social regularization How to extend it 1.Make your new algorithm generalize the proper base class. 2.Rewrite some of the following functions as needed. - readConfiguration() - printAlgorConfig() - initModel() - buildModel() - saveModel() - loadModel() - predict() Algorithms Implemented Note: We use SGD to obtain the local minimum. So, there have some differences between the original papers and the code in terms of fomula presentation. If you have problems in understanding the code, please open an issue to ask for help. We can guarantee that all the implementations are carefully reviewed and tested. Any suggestions and criticism are welcomed. We will make efforts to improve RecQ. Rating prediction Paper SlopeOne Lemire and Maclachlan, Slope One Predictors for Online Rating-Based Collaborative Filtering, SDM 2005. PMF Salakhutdinov and Mnih, Probabilistic Matrix Factorization, NIPS 2008. SoRec Ma et al., SoRec: Social Recommendation Using Probabilistic Matrix Factorization, SIGIR 2008. SVD++ Koren, Factorization meets the neighborhood: a multifaceted collaborative filtering model, SIGKDD 2008. RSTE Ma et al., Learning to Recommend with Social Trust Ensemble, SIGIR 2009. SVD Y. Koren, Collaborative Filtering with Temporal Dynamics, SIGKDD 2009. SocialMF Jamali and Ester, A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks, RecSys 2010. EE Khoshneshin et al., Collaborative Filtering via Euclidean Embedding, RecSys2010. SoReg Ma et al., Recommender systems with social regularization, WSDM 2011. LOCABAL Tang, Jiliang, et al. Exploiting local and global social context for recommendation, AAAI 2013. SREE Li et al., Social Recommendation Using Euclidean embedding, IJCNN 2017. CUNE-MF Zhang et al., Collaborative User Network Embedding for Social Recommender Systems, SDM 2017. SocialFD Yu et al., A Social Recommender Based on Factorization and Distance Metric Learning, IEEE Access 2017. Item Ranking Paper BPR Rendle et al., BPR: Bayesian Personalized Ranking from Implicit Feedback, UAI 2009. WRMF Yifan Hu et al.Collaborative Filtering for Implicit Feedback Datasets, KDD 2009. SBPR Zhao et al., Leveraing Social Connections to Improve Personalized Ranking for Collaborative Filtering, CIKM 2014 ExpoMF Liang et al., Modeling User Exposure in Recommendation, WWW 2016. CoFactor Liang et al., Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence, RecSys2016. TBPR Wang et al. Social Recommendation with Strong and Weak Ties, CIKM 2016. CDAE Wu et al., Collaborative Denoising Auto-Encoders for Top-N Recommender Systems, WSDM 2016. DMF Xue et al., Deep Matrix Factorization Models for Recommender Systems, IJCAI 2017. NeuMF He et al. Neural Collaborative Filtering, WWW 2017. CUNE-BPR Zhang et al., Collaborative User Network Embedding for Social Recommender Systems, SDM 2017. IRGAN Wang et al., IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models, SIGIR 2017. SERec Wang et al., Collaborative Filtering with Social Exposure: A Modular Approach to Social Recommendation, AAAI 2018. APR He et al., Adversarial Personalized Ranking for Recommendation, SIGIR 2018. IF-BPR Yu et al. Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation, CIKM 2018. CFGAN Chae et al. CFGAN: A Generic Collaborative Filtering Framework based on Generative Adversarial Networks, CIKM 2018. NGCF Wang et al. Neural Graph Collaborative Filtering, SIGIR 2019. DiffNet Wu et al. A Neural Influence Diffusion Model for Social Recommendation, SIGIR 2019. RSGAN Yu et al. Generating Reliable Friends via Adversarial Learning to Improve Social Recommendation, ICDM 2019. LightGCN He et al. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, SIGIR 2020. DHCF Ji et al. Dual Channel Hypergraph Collaborative Filtering, KDD 2020. Category Generic Recommenders UserKNN ItemKNN BasicMF SlopeOne SVD PMF SVD++ EE BPR WRMF ExpoMF Social Recommenders RSTE SoRec SoReg SocialMF SBPR SREE LOCABAL SocialFD TBPR SERec Network Embedding based Recommenders CoFactor CUNE-MF CUNE-BPR IF-BPR Deep Recommenders APR CDAE DMF NeuMF CFGAN IRGAN Baselines UserMean ItemMean MostPopular Rand Related Datasets Data Set Basic Meta User Context Users Items Ratings (Scale) Density Users Links (Type) Ciao [1] 7,375 105,114 284,086 [1, 5] 0.0365% 7,375 111,781 Trust Epinions [2] 40,163 139,738 664,824 [1, 5] 0.0118% 49,289 487,183 Trust Douban [3] 2,848 39,586 894,887 [1, 5] 0.794% 2,848 35,770 Trust LastFM [4] 1,892 17,632 92,834 implicit 0.27% 1,892 25,434 Trust Reference [1]. Tang, J., Gao, H., Liu, H.: mtrust:discerning multi-faceted trust in a connected world. In: International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, Wa, Usa, February. pp. 93-102 (2012) [2]. Massa, P., Avesani, P.: Trust-aware recommender systems. In: Proceedings of the 2007 ACM conference on Recommender systems. pp. 17-24. ACM (2007) [3]. G. Zhao, X. Qian, and X. Xie, ""User-service rating prediction by exploring social users' rating behaviors,"" IEEE Transactions on Multimedia, vol. 18, no. 3, pp. 496-506, 2016. [4] Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. 2nd Workshop on Information Heterogeneity and Fusion in Recom- mender Systems (HetRec 2011). In Proceedings of the 5th ACM conference on Recommender systems (RecSys 2011). ACM, New York, NY, USA Thanks If you our project is helpful to you, please cite one of these papers. @inproceedings{yu2018adaptive, title={Adaptive implicit friends identification over heterogeneous network for social recommendation}, author={Yu, Junliang and Gao, Min and Li, Jundong and Yin, Hongzhi and Liu, Huan}, booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management}, pages={357--366}, year={2018}, organization={ACM} } @article{yu2019generating, title={Generating Reliable Friends via Adversarial Training to Improve Social Recommendation}, author={Yu, Junliang and Gao, Min and Yin, Hongzhi and Li, Jundong and Gao, Chongming and Wang, Qinyong}, journal={arXiv preprint arXiv:1909.03529}, year={2019} }"
https://github.com/davidtt2/CourseProject	CS410 Project Final Report.pdf	"David Tran (davidtt2) CS410 Fall2020 Project Final Report and Documentation 1 CS410 Project Final Report: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject Overview of the Function of the Code This project uses a text retrieval method through Python's pandas & Selenium in order to gain information about the top companies in the technology industry. After retrieving that information, the Python file will generate a json file within the Angular project that will be read and displayed in the user interface. From the user interface, the user can search for technology companies by name. The expected results from this code is that whenever the Python file is ran, it will obtain the top technology companies for that year. For example, if this code was run in 2021, it will produce similar results with those top companies without any failure or bugs. Software Implementation (How to Run) 1) Clone project (git clone https://github.com/davidtt2/CourseProject.git) 2) Run ""CS410 Project Data.py"" (keep file structure unchanged) - Requires file to be in same directory as cs410-project - Requires chromedriver.exe in same directory as py file (also requires Chrome) - Different versions can be downloaded at https://chromedriver.chromium.org/downloads - Python file will run Selenium webdriver scripts - After, it will create a companies.json info file at root and in Angular project 3) cd to /cs410-project/src 4) Run the Angular script (ng serve -o) 5) UI will open in browser Documented Usages for this Project This project relies on Python and Angular. Modules that may need to be imported/installed to run: - npm install @angular/cli - ng add @angular/material - pip install pandas - pip install selenium - others Future Goals The current future goals are to add a dropdown for each company that shows their career websites and have useful information to prospective students looking for a place to join. Besides having this dropdown with extra information, I plan to implement a recommendation system based on the companies that the user has searched for. Team Member Contributions Because this was a single member team, I completed all the work on my own. Task Project Hours Research and UI Mockup 5 Hours Topic Mining to Retrieve Data in Python 15 Hours Parsing Useful Information from Data Retrieved 10 Hours Developing the UI in Angular 10 Hours Connecting the Angular UI with the Data 5 Hours Testing the UI and Python 5 Hours Total: 50 Hours David Tran (davidtt2) CS410 Fall2020 Project Final Report and Documentation 2 Current User Interface Video https://youtu.be/mfuLOdaO55Q Please contact me for any comments or questions."
https://github.com/davidtt2/CourseProject	CS410 Project Progress Report.pdf	CS410 Final Project Progress Report: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject 1) Which tasks have been completed? Task 1: Created a wireframe mockup of the interface and its interactions - Designed mockup for site - Decided which data is relevant Task 2: Basic layout of the user interface in Angular and TypeScript - Basic components set up - Mock data inputted until Python data scrape is finished Task 3: Basic method of data collection in Python - Collected data from relevant websites - Removed html tags 2) Which tasks are pending? Task 1: Clean up the data retrieved from Python data scrape Task 2: Add more information in a drop-down under each company Task 3: Add content-based filtering based on user click to generate recommendation. Task 4: Fix up the user interface 3) Are you facing any challenges? Challenge 1: Figuring out how to transfer data from Python scrape to TypeScript Challenge 2: Formatting the objects in the user interface Screenshot of Current UI: (work in progress)
https://github.com/davidtt2/CourseProject	CS410 Project Proposal.pdf	CS410 Final Project Proposal: Free Topic - Topic Mining David Tan Sang Tran (davidtt2@illinois.edu) https://github.com/davidtt2/CourseProject 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. One team member Name NetID Captain David Tan Sang Tran davidtt2 Yes 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Free Topic/Task: I plan to create a website that displays information about top software companies, including technologies used, headquarters information, links to applications, internship opportunities, and other relevant information. (This project would be similar to the ExpertSearch, but I will build this project from scratch). Interesting: This will be interesting because it would be good for Computer Science students to see a compiled list of all relevant companies that they can apply to after graduation. Technologies, Tools, Systems: I plan to use Python with a text mining algorithm to parse links and collect data about all of the top software companies available. Selenium and requests will be used to collect the information, and then Pandas will be used to transfer the data into csv sheets. After the data is collected, TypeScript and Angular will be used to display the data and UI. Datasets will be the internet and various links which would be where the information is collected. Expected Outcome: The expected outcome is a website that houses all the top technology company information for students to view and use for applications. Self-Evaluation: I will evaluate my work based on how well I meet my own expectations. I expect a fully functioning website that will have a nice UI and all the needed information about the various companies. 3. Which programming language do you plan to use? To collect the data, I will use Python, Selenium, Requests, and Pandas. For the website, I will use Angular, TypeScript, ng-charts, and Angular Material library. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Hours Research and UI Mockup 3 Hours Topic Mining to Retrieve Data in Python 10 Hours Parsing Useful Information from Data Retrieved 5 Hours Developing the UI in Angular 10 Hours Connecting the Angular UI with the Data 2 Hours Testing the UI and Python 5 Hours Total: 35 Hours
https://github.com/davidtt2/CourseProject	CS410 Project Video Link.pdf	https://www.youtube.com/watch?v=mfuLOdaO55Q
https://github.com/davidtt2/CourseProject	README.md	"Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. CourseProject CS410Fall2020 David Tan Sang Tran (davidtt2) { Individual Team } David Tran Project Proposal David Tran Progress Report David Tran Final Report and Documentation David Tran Project Video How to Run 1) Clone project (git clone https://github.com/davidtt2/CourseProject.git) 2) Run ""CS410 Project Data.py"" (keep file structure unchanged) - Requires file to be in same directory as cs410-project - Requires chromedriver.exe in same directory as py file (also requires Chrome) - Different versions can be downloaded at https://chromedriver.chromium.org/downloads - Python file will run Selenium webdriver scripts - After, it will create a companies.json info file at root and in Angular project 3) cd to /cs410-project/src 4) Run the Angular script (ng serve -o) 5) UI will open in browser Modules that may need to be imported/installed to run: - npm install @angular/cli - ng add @angular/material - pip install pandas - pip install selenium - others Description This project uses a text retrieval method through Python's pandas & Selenium in order to gain information about the top companies in the technology industry. After retrieving that information, the Python file will generate a json file within the Angular project that will be read and displayed in the user interface. From the user interface, the user can search for technology companies by name. Please contact me for any assistance or comments. (davidtt2@illinois.edu)"
https://github.com/joel515/CourseProject	Final_Project_Progress_Report.pdf	"Joel Kopp (joelk2) CS 410 - Fall 2020 Final Project Progress Report 1. COMPLETED: Abstracts have been scraped from the ACM Digital Library for two prominent data mining researchers: Jiawei Han (UIUC) and Philip S. Yu (UIC). To match the temporal time frame, abstracts were only obtained up until the end of 2005. It is likely that Han's abstracts were used in the actual paper, but I am not so sure about Yu. Label Author School Link Author A Jiawei Han UIUC ACM DL Author B Philip S. Yu UIC ACM DL An initial PLSA model was repurposed using the model created in MP3. A ""frequent pattern mining"" topic was found withing Han's abstracts, including a good match of coverage, but not for Yu's. I will attempt to contact the author to determine see if I can find the name of the second author. Otherwise, I will proceed with the two authors named above. (The scraper and initial data can be found here.) 2. PENDING: I need to implement the Fixed Coverage CPLSA model described in the paper. At this point I feel I understand the mechanics behind it, it is just a matter of coding it. Following that, I will compile the documentation and compose the video tutorial. 3. CHALLENGES: The main challenge is duplicating the data set used by the authors. While this is not entirely necessary, I feel that it would provide a good sanity check for my work if the data sets match."
https://github.com/joel515/CourseProject	Final_Project_Proposal.pdf	"Joel Kopp (joelk2) CS 410 - Fall 2020 Final Project Proposal 1) Team members Joel Kopp (joelk2) 2) Chosen paper Subtopic - Contextual text mining Paper - ""A mixture model for contextual text mining"" Experiment - Temporal-Author-Topic analysis 3) Programming language Python 4) Datasets The dataset will be comprised of abstracts from 2 authors with substantial works that span a large-enough time frame. The abstracts will be scraped from profiles in the ACM Digital Library, similar to the experiment in the paper itself. A sample abstract can be found here: https://dl.acm.org/doi/10.1145/3410992.3410995."
https://github.com/joel515/CourseProject	kdd06-mix.pdf	"A Mixture Model for Contextual Text Mining Qiaozhu Mei Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 qmei2@uiuc.edu ChengXiang Zhai Department of Computer Science University of Illinois at Urbana-Champaign Urbana,IL 61801 czhai@cs.uiuc.edu ABSTRACT Contextual text mining is concerned with extracting topical themes from a text collection with context information (e.g., time and location) and comparing/analyzing the variations of themes over different contexts. Since the topics covered in a document are usually related to the context of the doc- ument, analyzing topical themes within context can poten- tially reveal many interesting theme patterns. In this paper, we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases. Specifically, we extend the probabilistic latent seman- tic analysis (PLSA) model by introducing context variables to model the context of a document. The proposed mixture model, called contextual probabilistic latent semantic anal- ysis (CPLSA) model, can be applied to many interesting mining tasks, such as temporal text mining, spatiotempo- ral text mining, author-topic analysis, and cross-collection comparative analysis. Empirical experiments show that the proposed mixture model can discover themes and their con- textual variations effectively. Categories and Subject Descriptors: H.3.3 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Contextual text mining, context, mixture model, EM algorithm, theme pattern, clustering 1. INTRODUCTION A text document is often associated with various kinds of context information, such as the time and location at which the document was produced, the author(s) who wrote the document, and its publisher. The contents of text doc- uments with the same or similar context are often corre- lated in some way. For example, news articles written in the period of some major event all tend to be influenced by the event in some way, and papers written by the same researcher tend to share similar topics. In order to reveal in- teresting content patterns in such contextualized text data, it is necessary to consider context information when ana- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'06, August 20-23, 2006, Philadelphia, Pennsylvania, USA. Copyright 2006 ACM 1-59593-339-5/06/0008 ...$5.00. lyzing the topics covered in such data. Indeed, there have been several recent studies in this direction. For example, the time stamps of text documents have been considered in some recent work on temporal text mining [9, 16, 14, 4]. Also, author-topic analysis is studied in [17], and cross- collection comparative text mining is studied in [18]. All these studies consider some kinds of context information, i.e., time, authorship, and subcollection. Time, authorship, and subcollection are by no means the only possible context information of a document. In fact, any metadata entry of a document can indicate a context and all documents with the same value of this metadata en- try can be considered as in the same context. For example, the source of a news article, the author's age group, occu- pation, and location of a weblog article, and the citation frequency of a research paper, are all reasonable context information. Moreover, a document may belong to multi- ple contexts, and any combination of its metadata entries makes a ""complex"" context. By analyzing the variations of topics over these contexts, a lot of interesting text mining tasks can be addressed, such as spatiotemporal text mining, author-topic evolutionary analysis over time, and opinion comparison over different age groups and occupations. However, existing techniques are usually tuned for some specific tasks, and are not applicable to consider other kinds of contexts. For example, one cannot directly use the tem- poral text mining techniques to model the occupation of authors. This indicates a serious limitation of existing con- textual analysis of themes: every time when a new combina- tion of context information is to be considered, people have to seek for solutions in an ad hoc way. Therefore, it is highly desirable to introduce a general text mining problem, contextual text mining, which is abstracted from a family of text mining tasks with various types of contextual analysis. It is desirable to derive a model that is highly general to conduct the common tasks of these specific contextual text mining problems, and easy to be applied to each of them with appropriate regularization. In this work, we define the general problem of Contex- tual Text Mining (CtxTM) and its common tasks, which is abstracted from a family of specific text mining problems. We extend the probabilistic latent semantic analysis (PLSA) model to incorporate context information, and develop a contextual probabilistic latent semantic analysis (CPLSA) model to facilitate contextual text mining in a general way. By fitting the model to the text data to mine, we can (1) discover the global salient themes from the collection of doc- uments; (2) analyze the content variation of the themes in 649 Research Track Poster any given view of context; and (3) analyze the coverage of themes associated with any given context. These tasks are general and can be easily applied to differ- ent specific contextual text mining problems. In this paper, we show that many existing contextual theme analysis prob- lems can be defined as special cases of CtxTM, and can be solved with regularized versions of the mixture model we proposed, corresponding to the context information and the mining tasks it involves. Although it may not be the only possible model for contextual text mining, the model is quite flexible to adapt different assumptions. 2. CONTEXTUAL TEXT MINING Given a collection of documents with context information, we assume that there is a set of topics, or themes in the col- lection which vary over different contexts. Our goal is gen- erally to conduct context-sensitive analysis of these themes. As stressed in previous work [14], a theme in a contextual- ized text collection D is a probabilistic distribution of words that characterizes a semantically coherent topic or subtopic. Without loss of generality, we will assume that there are al- together k major themes in our collection, Th = {th1,..., thk}. To model the context of a document, we introduce a con- cept called context feature, which is defined as any meta-data of a document (e.g., the time stamp in temporal text mining or authorship in author-topic analysis). The context that a document belongs to can be indicated by the context fea- tures of this document, which is formally defined as follows: Definition 1 (Context) Let F = {f1, f2, ..., f|F |} be a set of context features. A Context c in a document collection is decided by any combination of context fea- tures in F, formally c  2|F|. The whole set of possi- ble contexts is denoted as C = {c1, ..., cn}. Suppose D = {(D1, C1), ..., (D|D|, C|D|)} is a collection of documents, each document Di is a sequence of words from a vocabulary set V = {w1, ..., w|V |}, and Ci  F is a set of context features which are associated with the document Di. A document Di belongs to a context c iff. Ci  c. This tells us that a document can belong to multiple contexts. In another word, the contexts are possible to overlap. In contextual text mining, our goal is to analyze the top- ics/subtopics in such a text collection in a context-sensitive way. Specifically, we would like to model the k major themes and how they vary according to different contexts, and would also like to model the coverage of different themes in a doc- ument or documents that share certain kinds of context. To accommodate context-sensitive theme analysis, we con- sider variations of these k themes over different contexts. For example, if the context we are interested in is time, we will assume that there is a potentially distinct ""version"" of the k themes in each different time period; different such ""ver- sions"" model the variations of themes across time stamps. We formally define such a variation as a View of themes. Definition 2 (View) A view of themes in a contextu- alized text collection D is a sequence of themes thi1, ..., thik, where thil is the variation of theme thl according to view vi. We will assume that there are n views in our collection, v1, ..., vn, each corresponds to a context ci. Therefore, a doc- ument is assumed to potentially have multiple views; pre- cisely which views are taken depends on the document and its context. Each view vi is assumed to be taken in any doc- uments in the context ci, which can also be overlapping. Definition 3(Context Support) The support of a con- text ci, s(ci) is the set of documents in context ci, i.e., s(ci) = {Dj|Cj  ci}. Since each context is associated with a view, we also call s(ci) as the support of the view vi. To analyze the strength of themes, we further model the variable coverage of different themes in a document. For ex- ample, some documents would favor some particular themes and thus would have a larger coverage of them. Definition 4 (Coverage) A coverage of themes in a document (kj) is a distribution over the themes p(l|ki). Clearly, k l=1 p(l|kj) = 1. We will assume that there are m distinct theme coverages in our collection, k1, ..., km. For example, if we assume that each document has a potentially distinct theme coverage, then m = |D|. In general, however, a document can cover themes according to multiple coverages. For example, if we are interested in modeling theme coverage associated with time stamps, we may assume that the actual theme coverage in a document would be a mixture of the document-specific theme coverage and another theme coverage associated with the time context of the document. We use c(kj) to denote the contexts where the coverage kj is applicable, and we also define the support of a coverage in the same way as we define that of a context. Definition 5 (Coverage Support) The support of a coverage kj, s(kj) is the set of documents in which the cov- erage kj is taken, i.e., s(kj) = {Di|c  c(kj) s.t. Ci  c}. The latent structure of themes, views and coverages in a contextualized document collection is illustrated in Figure 1. View0: th01, th02, .... , th0k View1: th11, th12, .... , th1k Viewn: thn1, thn2, .... , thnk .... Theme: 1, 2, .... , k Coverage0: Coverage1: Coveragem: .... .... .... .... c0 c1 cn c2 Collection Document Contexts Figure 1: The theme-view-coverage structure in a text collection With these definitions, the task of Contextual Text Mining (CtxTM) can be defined as to recover the n views, vi = (thi1, ..., thik), i = 1, ..., n, and the m theme coverages k1, ..., km from the collection D, and to analyze them in a context-sensitive way. There are many different ways to an- alyze the views and theme coverages. Below we discuss a few interesting cases. 1. Theme extraction: We may extract the global salient themes. Although each theme thl varies in different contexts, it is also beneficial to have an explicit model for thl in a global view. Basically, this will give us the common information that is shared by all the variations of thl in all dif- ferent contexts. In practice, we always include a global view v0, which corresponds to a global context c0 = F. Clearly, all documents Di  D belong to c0 since Ci  c0. 2. View comparison: We may compare the n views. The comparison of a theme thl from different views usually represents the content variation of thl corresponding to dif- 650 Research Track Poster ferent contexts. By comparing thil for each view vi which corresponds to context ci, we can analyze the influence of the context ci on the contents of thl. 3. Coverage comparison: We may compare the m cov- erages. The variations of p(l|kj) can tell us how likely thl is covered by the documents in the coverage support s(kj). By associating p(l|kj) within contexts c(kj) that kj is applica- ble, we can analyze how closely a theme is associated to a context, or how context-sensitive a theme is. 4. Others: With contextual text mining, we can also analyze other problems such as the influence of an individ- ual context feature on the theme coverage, e.g., the theme- location distribution in spatiotemporal theme analysis. Among these cases, 2 and 3 are the most important, which distinguish contextual text mining from the traditional theme extraction work, and the application of them facilitate other types of analysis. With the definition of the general problem of contextual text mining (CtxTM), we can show that some specific con- textual text mining problems are special cases of CtxTM. For example, in temporal text mining, each context feature is a time stamp. Therefore, a context is either a time stamp or a set of consecutive time stamps, or time period. A view of themes is taken in all the documents in the corresponding time. The goal of temporal text mining is mainly to compare the coverage variation over different contexts (e.g., theme life cycles in [14]), and sometimes also the content variation of themes over different views, (e.g., evolutionary theme pat- tern in [14]). In author-topic analysis, each context feature is an author, and each context is either an author or a set of authors. Each view is then taken in the document with the same author or authors. We are interested in comparing the content variations over different views (authors) [17]. 3. A CONTEXTUAL MIXTURE MODEL In this section, we propose an extension of the Probabilis- tic Latent Semantic Analysis (PLSA) model [7, 8], called Contextual Probabilistic Latent Semantic Analysis (CPLSA) model, for contextual text mining. Our main idea is to al- low a document to be generated using multiple views and multiple coverages. The views and coverages actually used in a document usually depend on its context, which could be the time or location where the document is written, the source from which the document comes, or any other meta- data. We first propose the general CPLSA model, and then introduce two simplified versions of this model that are es- pecially suitable for two representative tasks of contextual text mining. 3.1 The CPLSA Model In CPLSA, we assume that document D (with context C) is generated by generating each word in it as follows: (1) Choose a view vi according to the view distribution p(vi|D, C). (2) Choose a coverage kj according to the cov- erage distribution p(kj|D, C). (3) Generate a word using thil. Formally, the log-likelihood of the whole collection is log p(D) =  (D,C)D  wV c(w, D) log( n  i=1 p(vi|D, C) x m  j=1 p(kj|D, C) k  l=1 p(l|kj)p(w|thil)) The parameters are the view selection probability p(vi|D, C), the coverage distribution selection probability p(kj|D, C), the coverage distribution p(l|kj), and the theme distribu- tion p(w|thil). As a mixture model, we have a total of n x k multino- mial distribution component models. Each set of k multino- mial distributions, thi1, ..., thik, represents a potentially dis- tinct view of the topics that we are interested in. How- ever, while we can potentially use all the views to generate a document, often the generation of a particular document D in a particular context C only involves a subset of these views. This is because in any interesting context mining scenario, different views generally have different supporting documents, though it is also common for the views to over- lap in some supporting documents. More specifically, the view selection distribution p(vi|D, C) determines which views will actually be used when generat- ing words in document D. This distribution would assign zero probabilities to those views that are not selected. For example, if the views that we are to model correspond to the temporal context of a document and we have one global view spanning in the entire time period, then a document at time point ti would be generated using two different views - the view corresponding to time point ti and the global view, which is applied to all the documents. Orthogonal to the choice of views, we also assume that we have choices of theme coverage distributions. The differ- ent coverage distributions are to reflect the uneven coverage of topics in different context and to capture the common coverage patterns. For example, if we suspect that the cov- erage may vary depending on the location of the authors, we can associate a particular coverage distribution to each location, which will be shared by all the documents in the location. After we learn such coverage distributions, we can then compare them across different locations. Once again, exactly which coverage distributions to use would depend on the context of the document to be generated. The mixture model can be fit to a contextualized collec- tion D using a maximum likelihood estimator. The EM al- gorithm [5] can be used in a straightforward way to estimate the parameters; the updating formulas are as follows: p(zw,i,j,l = 1) = p(t)(vi|D,C)p(t)(kj |D,C)p(t)(l|kj)p(t)(w|thil)  n i'=1 p(t)(vi' |D,C)  m j'=1 p(t)(kj' |D,C)  k l'=1 p(t)(l'|kj' )p(t)(w|thi'l' ) p(t+1)(vi|D, C) =  wV c(w,D)  m j=1  k l=1 p(zw,i,j,l=1)  n i'=1  wV c(w,D)  m j=1  k l=1 p(zw,i',j,l=1) p(t+1)(kj|D, C) =  wV c(w,D)  n i=1  k l=1 p(zw,i,j,l=1)  m j'=1  wV c(w,D)  n i=1  k l=1 p(zw,i,j',l=1) p(t+1)(l|kj) =  (D,C)D  wV c(w,D)  n i=1 p(zw,i,j,l=1)  k l'=1  (D,C)D  wV c(w,D)  n i=1 p(zw,i,j,l' =1) p(t+1)(w|thil) =  (D,C)D c(w,D)  m j=1 p(zw,i,j,l=1)  w'V  (D,C)D c(w',D)  m j=1 p(zw',i,j,l=1) However, since the model has many parameters and has a high-degree of freedom, fitting it with a maximum like- lihood estimator, in general, would face a serious problem of multiple local maxima. Fortunately, in contextual text mining, we almost always associate them with appropriate partitions of context. As a result, the model is often highly constrained. For example, if all we are interested in is to compare non-overlapping views across different time, then p(kj|D, C) becomes a delta function, i.e., p(kj|D, C) = 1 if and only if kj is the coverage distribution for the time context of D, and p(kj|D, C) = 0 for all other kj's. Unfortunately, even with such constraints, the model may 651 Research Track Poster still have many free parameters to estimate. One possibil- ity is to add some parametric constraint such as assuming all coverage distributions are from the same Dirichlet distri- bution as done in LDA [2], which would clearly reduce the number of free parameters; indeed, we can easily generalize our model in the same way as LDA generalizes PLSA [8]. However, one concern with such a strategy is that the para- metric constraint is artificial and may restrict the capacity of the model to extract discriminative themes, which is our goal in contextual text mining. Another approach is to fur- ther regularize the estimation of the model by heuristically searching for a good initial point in EM; specific heuristics would depend on the particular contextual text mining task. This approach is adopted in our experiments and will be fur- ther discussed in Section 3.2. In order to model the noise (e.g., common English words) in the text, we could designate the first theme as modeling such noise. That is, all th1j's will be set to model the noise. We may further tie all of them so that we have just one common background unigram language model th1. This can also be regarded as applying an infinitely strong prior on the first theme in all views. 3.2 Special Versions of CPLSA Considering that the most important tasks of contextual text mining are view comparison and theme coverage com- parison across contexts, as discussed in Section 2, we in- troduce two special cases of CPLSA, which are particularly useful to do these two tasks. We first introduce the special version of CPLSA to facili- tate view comparison. In some cases, we are only interested to model the content variation of themes across contexts, e.g., when we are analyzing the theme evolutions over time [14], or comparing the common themes and corresponding specific themes across subcollections [18]. In these cases, we can fairly assume that the theme coverage over contexts is fixed, thus does not depend on the contexts that a docu- ment is in. Under this assumption, the m j=1 p(kj|D, C) in the model will be simplified as m j=1 p(kj|D). If we further assume that there is only one coverage k applicable to each document, the log-likelihood function can be written as  (D,C)D  wV c(w, D) log( n  i=1 p(vi|D, C) k  l=1 p(l|kD)p(w|thil)) where kD is the coverage associated with the document D. We call this simplified version of model as fixed-coverage contextual mixture model (FC-CPLSA). If we have three views, where one is the global view and the other two correspond to subcollections, it will allow us to compare the common themes and specific themes in the two views, as discussed in [18]. If each view corresponds to a time stamp, this model will allow us to analyze the content evolutions of themes over time, as discussed in [14]. In some other cases, we are only interested to model the variation of theme coverage over contexts, e.g., when we are analyzing the life cycles (i.e., strength variations over time) of themes. In these cases, we are not interested in the content variation of local themes, and thus make the assumption that different views of themes are stable. With this assumption, we can simplify the model likelihood as  (D,C)D  wV c(w, D) log( m  j=1 p(kj|D, C) k  l=1 p(l|kj)p(w|thl)) where p(w|thl) is the global word distribution of theme l, which does not vary across contexts. We call this simplified model as fixed-view contextual mixture model (FV- CPLSA). If the only context feature is time, we have two types of coverage distributions kD and kT , where kD is the coverage distribution corresponding to each document and kT is the theme coverage for each time period. This will allow us to model the theme life cycles, as introduced in [14]. If we have two context features, time and location, and each context is a combination of time stamp and location, we also have two groups of theme coverage distributions, kD and kT L. This will allow us to analyze the spatiotemporal theme distributions in a spatiotemporal text mining framework. With these two special simplified versions, the CPLSA model can be applied to solve a broad family of text mining problems with contextual analysis. 4. EXPERIMENTS We apply the general CPLSA model presented in Section 3 to three different datasets and text mining tasks. Empirical results show that this model can model the themes and their variations across different contexts effectively. 4.1 Temporal-Author-Topic analysis In this experiment, we evaluate the performance of the CPLSA models on author-topic comparative analysis. If two authors have similar research interest, we assume that there is a set of common themes which can be found in their publications. Since different author has different preferences and focuses, the content of these themes will also vary cor- responding to each author. Previous work on author-topic analysis only consider the authorship of documents as the context [17]. Intuitively, however, the topics that an au- thor favors also evolute over time. We add another type of context information, i.e., publication time, to test the effec- tiveness of our model on handling multiple types of contexts. We collect the abstracts of 282 papers published by two famous Data Mining researchers from ACM Digital library. We split the whole time line into three spans: before the year 1993, from 1993 to 1999, and after the year 1999. This will give us 12 possible views as in Table 1. Since we are not interested in analyzing the coverage variations across contexts (i.e. time and authors), we assume the coverage of themes only depends on documents but not on the contexts. #Context Views Features (A and B are two authors) 0 Global View 1 A; B; < 92; 93 ~ 99; 00 ~ 05 A, < 92; A, 93 ~ 99; A, 00 ~ 05 2 B, < 92; B, 93 ~ 99; B, 00 ~ 05 Table 1: Possible Views in Author-Topic Analysis Therefore, we use the FC-CPLSA model presented in Sec- tion 3.2 to model the themes and their views corresponding to different contexts. Our goal is thus to estimate all the parameters in the regularized model, and compare p(w|thjl) over different view vj. To avoid the EM algorithm being trapped in suboptimal local maximums, we need to make associations between each thjl to its corresponding global view thl. We achieve this by selecting a good starting point for the EM algorithm. Specif- ically, we begin with a prior of a large p(v0|D, C) to view 0, which is the global view. This ensures us to get the strong signal of global themes instead of local biased themes. In the following iterations, we gradually decay this prior and 652 Research Track Poster Vews: Global Author A Author B Author A: 2000~ 1993~1999 2000~ pattern 0.110689 project 0.0444375 research 0.0550772 close 0.0805878 rule 0.0616733 index 0.0430914 frequent 0.040613 itemset 0.0432976 next 0.0308254 pattern 0.072078 distribute 0.0567852 graph 0.0343051 frequent-pattern 0.0393 intertransaction 0.03072 transition 0.0308254 sequential 0.0462879 researcher 0.0324659 web 0.0306886 Author sequential 0.0359059 support 0.0264818 panel 0.0275384 min support 0.03526 algorithm 0.0217309 gspan 0.0273849 Topic method 0.0214187 associate 0.0258175 technical 0.0275384 length 0.0315721 over 0.0162951 substructure 0.02005 Analysis pattern-growth 0.02035 frequent 0.0181942 technology 0.0258949 threshold 0.0296533 fdm 0.0227141 gindex 0.016431 condense 0.0184008 closet 0.0176081 article 0.0154127 frequent 0.0196054 study 0.0116576 bide 0.016431 increment 0.0138457 apriori 0.0170468 revolution 0.0154127 top-k 0.0176324 scable 0011357. magnitude 0.0151909 constraint 0.0130636 prefixspan 0.0130272 tremendous 0.0154127 without 0.0175662 pass 0.011357 size 0.0114699 push 0.0103159 pseudo 0.0109016 innovate 0.0154127 fp-tree 0.0102471 disclose 0.011357 xml 0.010954 Table 2: Comparison of the content of theme ""Frequent Pattern Mining"" over different views terminate the EM algorithm early when the average view distribution for view 0 (i.e.,  DD p(v0|D, C)/|D|) drops under a threshold, say 0.1. This gives us a good starting point for the EM algorithm. Then, we do this procedure again for multiple trials and select the best start point (i.e., the one with the highest likelihood). Finally, we run the EM algorithm beginning with this selected start point until it converges. The results for this experiment are selectively presented in the following table. In Table 2, we see that the content of this selected theme varies over different views. From the global view, in which all documents are included, we can tell that this theme is talking about frequent pattern mining. From the view of Au- thor A, we see specific frequent pattern mining techniques such as database projection, apriori, prefixspan, and closet. From the view of Author B, we see that he is not as deep into techniques of mining frequent patterns, but rather more as- sociated with introductional and innovated work of frequent pattern mining. From the view of the years before 1993, the corresponding theme barely has any connection to fre- quent pattern mining. This is reasonable however, since the first and most influential paper of frequent pattern mining was published in 1993. From the view of year 1993 to 1999, we see that this theme evolutes to talk about association rules, which is perhaps the most important application of frequent pattern mining at that time. Specific techniques, such as fdm (Fast Distributed Mining of associate rules) ap- pears high in the word distribution. From the view of the years after 1999, it is interesting to see the appearance of more new applications of frequent pattern mining, such as graphs and web. The terms corresponding to specific tech- niques of mining graph patterns and sequential patterns, e.g., gspan and bide, are with high probabilities in the theme word distribution. In the view corresponding to a combined context (Author A and after 1999), the top terms include ""close"", ""top-k"", and ""fp-tree"", which well reveal the pref- erences of author A in frequent pattern mining. The view specific theme for the combined context ""Author B after 1999"" is not well associated with the global theme again, which is consistent to the fact that Author B is not activate in frequent pattern mining any more after 2000. This experiment shows that the CPLSA model can ex- tract and compare the theme variations over different views effectively. 4.2 Spatiotemporal theme analysis In this experiments, we show the effectiveness of CPLSA models on spatiotemporal analysis of themes. The context features we consider in this experiment is time stamps and location information of documents. The tasks of this spe- cific contextual text mining problem are: (1) extract global themes from the collection, which are shared by different time and locations; (2) for each time stamp, compute the distribution of theme and locations, from which we can draw the theme distribution snapshots over locations; and (3) compare the views of themes across contexts. It is interesting to see that the second task is not a com- mon task of CtxTM. Let a context C be denoted as (t, l) where t and l refer to time and location, the task is to esti- mate p(k|D, (t, l)) for each k, and P(th, l|t) for any t: p(th, l|t) =  k:(t,l)c(k) p(th|k)p(k|t, l)p(l|t) We collect 9377 MSN Space documents with a time-bounded query submitted to Google blogsearch, with the keywords ""Hurricane Katrina"". In this dataset, 7118 documents pro- vide explicit location information, and the locations of oth- ers are tagged as ""unknown"". We segment the time stamps into six weeks, extract and compare the common themes over different locations in United States. Each combination of the 50 States and six week consists a unique ""context"", which gives us 50*6 = 300 contexts. Since there are many contexts, it is difficult to estimate all the views precisely. Since we are only interested in the strength variations of global themes over all the contexts, it is reason- able to simplify the model by assuming that the content of the global themes does not vary over contexts. Therefore, we use the FV-CPLSA model presented in Section 3.2 to model the global themes and their coverage variations over time and locations. We further assume that p(kC|D, C) is a constant that controls the impact of the context on selecting the coverage of themes. By estimating the free parameters, our goal is to compute the theme-location coverage: p(th, l|t) = p(th, l, t)  th'  l' p(th', l', t) = p(th|kt,l)p(t, l)  th'  l' P(th'|kt,l')P(t, l') where p(kt,l|t, l) = 1, p(t, l) can be computed from the word count in time period t at location l divided by the total word count in the collection. With p(th, l|t) computed, we can visualize the theme-location coverage by fulfill p(th, l|t) in a snapshot map. In Figure 2, we show one of the 10 global themes we extracted from the blog dataset and its theme-location coverage at different time. From the top terms in this theme, we can infer that this theme is talking about aid and donations that were made to the hurricane affected areas. Figure 2 well demonstrates the evolution of theme-location coverage over different time periods. A detailed description of theme variation over time and location can be found in [13]. The next task is similar to the experiment in Section 4.1, which is to compare the views of themes across contexts. Specifically, we partition the states into four groups: Af- fected States; Peripheral States; Coast States; and Inland States. We partition the time line into spans with the length of two weeks. Then we use the FC-CPLSA model to com- pare the views of themes corresponding to different contexts. The results are selectively shown in Table 3. It is easy to see that from the view of ""Periphery States"", the content of the theme ""donation"" is quite similar to the common theme extracted in Figure 2. People tend to talk about donations and supplies with food. However, from the view of ""Affected Areas"", which corresponds to the hur- 653 Research Track Poster (a) Week1: 08/23-08/29 (b) Week Three:09/06-09/12 (c) Week Five: 09/20-09/26 Figure 2: Selected snapshots for theme ""Aid and Donation"" of Hurricane Katrina. Affected States Peripheral States Week1-2 Week5-6 medical 0.0192 donate 0.0238 donate 0.0351 their 0.0142 comfort 0.0141 relief 0.0204 help 0.0296 help 0.0120 health 0.0137 red 0.0132 relief 0.0181 family 0.0091 ship 0.0133 cross 0.0105 red 0.0151 rebuild 0.0088 volunteer 0.0129 link 0.0086 please 0.0143 school 0.0080 hospital 0.0090 food 0.0078 cross 0.0142 children 0.0068 team 0.0081 medical 0.0074 need 0.0134 need 0.0061 assist 0.0081 supply 0.0069 volunteer 0.0120 health 0.0059 care 0.0072 charity 0.0067 victim 0.0084 evacuee 0.0057 service 0.0053 volunteer 0.0060 blood 0.0057 parish 0.0051 Table 3: Comparison of the content of the theme ""Aid and Donation"" over different views ricane affected states such as Louisiana, people care more about medical aid and hospital cares. In the first two weeks, the view of this theme is still quite similar to the common theme. However in the last two weeks, we can notice that the ""helps"" become more about rebuilding and helping the returning evacuees. This group of experiments show that our general model is effective to analyze spatiotemporal theme patterns. 4.3 Event Impact Analysis In many scenarios, a collection of documents are usually associated with a series of events. For example, weblogs usually reflect people's opinions about the events happen- ing. The research topics covered by scientific literatures are also likely to be affected by the influential related events, such as the invention of WWW, and the proposing of a new research direction. The impact of such event can usually be analyzed by comparing the themes in the documents pub- lished before versus after the event. In this experiment, we apply CPLSA on the problem of event impact analysis. Since each event gives a possible segmentation of the time line, this analysis also provides an evaluation of CPLSA on modeling overlapping views that are not orthogonal to each other. Although the experiments in previous sections also covers some overlapping views (e.g., a view corresponding to a location and a view corresponding to a time stamp), these overlaps are caused by different types of, or orthog- onal context features (e.g., time and location). In reality however, the overlapping views with the same type of con- text feature is desirable. For example, a business analyzer may need to analyze and compare the customers' opinions in the first week, in the first month, in the first season, or in the first year after a new product is released. One strength of our model is that we allow the analysis views that overlap with each other. In this experiment, we evaluate our model on event impact analysis and overlapping view analysis. We collect the abstracts of 1472 papers published in 28 years' SIGIR conferences from ACM Digital Library. We select two influential events to the Information Retrieval community in the 90s. One is the beginning of Text RE- trieval Conferences (TREC) in 1992, which provide large- scale standard text datasets and judgements for many re- trieval problems. The other is the introduction of language model into Information Retrieval in 1998, which began a genre of research and led to a lot of publications. Our goal is to use the CPLSA model to reveal the impact of these two events in IR research, i.e., how the content of research topics change after the two events. To achieve this, we assign the abstracts in SIGIR proceed- ings into four contexts, each corresponds to a time span. The first context includes all the documents were published be- fore 1993, in which is the first SIGIR conference after the start of TREC. The second context contains documents pub- lished on or after that. The third context includes abstracts before the year 1998, in which the first paper of language model in information retrieval was published. The fourth context contains all abstracts published on or after 1998. It is clear that there are overlaps between these contexts. We also include a global view, which corresponds to all the abstracts in SIGIR proceedings. We use the same strategy as presented in Section 4.1 to avoid the EM algorithm to be trapped in unexpected local maximums. We extract 10 salient global themes from this collection and present the most interesting one. From the global view in Table 4, we see that this theme is talking about retrieval models, especially term weight- ing and relevance feedback. The content of this common theme varies from different views. From the Pre-Trec view, which corresponds to the time before 1993, we see that vec- tor space model dominates, and boolean queries are men- tioned frequently. In the Post-Trec view, however, we no- tice that XML retrieval model has been paid more attention to. Also, we see specific types of data (email) and other terms related to the nature of TREC (e.g., collect, judge- ment, rank). It is more interesting when comparing the view ""Pre-Language Model"" and ""Post-Language Model"". We see that before 1998, the retrieval models are dominated by probabilistic models. After 1998, however, it is very clear that language model dominates the theme. The top ranked terms have changed to indicate language models, parame- ter estimations, likelihood and probability distributions, and language model smoothing. This is consistent with our prior knowledge. The overlapping views, for example Pre-LM and Pre-Trec, do share some content but clearly with different focuses. Pre-Trec, which is more faraway, emphasizes vector space model while Pre-LM emphasizes probabilistic models. This experiment shows that our method is effective to ana- lyze event impact and model the overlapping views. 5. RELATED WORK The most relevant work is the Probabilistic Latent Se- mantic Analysis model (PLSA) proposed by Hofmann [7, 8], which models a document as a mixture of aspects, where each aspect is represented by a multinomial distribution over the whole vocabulary. Our CPLSA model is a natural ex- tension of PLSA to incorporate context. To avoid overfitting in PLSA, Blei and co-authors proposed a generative aspect model called Latent Dirichlet Allocation (LDA), which could 654 Research Track Poster Views: Global Pre-Trec Post-Trec Pre-Language Model Post-Language Model term 0.159983 vector 0.0514067 xml 0.0677684 probabilist 0.0777954 model 0.16867 relevance 0.0751814 concept 0.0297583 element 0.0212121 model 0.0431573 language 0.0752643 weight 0.0659849 extend 0.0297405 email 0.0197383 logic 0.0403557 estimate 0.0520434 feedback 0.0372254 model 0.0291697 collect 0.0191258 ir 0.0337741 parameter 0.0281169 SIGIR independence 0.031063 space 0.0236088 locate 0.0187425 boolean 0.028073 distribution 0.0268227 model 0.0309212 boolean 0.0151455 judgment 0.0140086 fuzzy 0.0201544 probable 0.0205655 frequent 0.0233021 function 0.0123171 rank 0.010205 algebra 0.0199632 smooth 0.0197662 probabilist 0.018762 u 0.00898533 overlap 0.00975133 probable 0.0124902 score 0.0166799 document 0.0173198 feedback 0.00860945 contextual 0.00936265 estimate 0.0119202 retrieval 0.0137085 assume 0.0172082 specify 0.0083182 solution 0.00913 weight 0.0111257 markov 0.0118979 dependency 0.0157547 correlate 0.00779721 subtopic 0.00791172 rank 0.0107045 likelihood 0.00585364 Table 4: Comparison of theme content over different views in SIGIR collection also extract a set of themes from a document collection [2]. LDA, however, does not model context either. Although we have not explored it, one can also make LDA contextualized in the same way as we have done to PLSA in this paper. Re- cently, some extensions of this work have considered some specific types of context. For example, temporal context is considered in [6, 16, 4, 14]. Multi-collection context is an- alyzed in [18]. Author-topic analysis is proposed in [17]. Li et al. proposed a probabilistic model to detect retrospec- tive news events by explaining the generation of ""four Ws1"" from each news article [11]. Our work is a generalization of these studies of specific context and provides a general prob- abilistic model which can be applied to all kinds of context. Temporal context is also addressed in Kleinberg's work on discovering bursty and hierarchical structures in streams [9] and some work on topic/event/trend detection and tracking (e.g., [1, 3, 12, 10, 15]). However, most of this work assumes one document only belongs to one topic and cannot be easily generalized to analyze other contexts. 6. CONCLUSIONS In this paper, we present a study of the general problem of contextual text mining. We formally defined the basic tasks of contextual theme analysis, and proposed a novel prob- abilistic mixture model to extract themes and model their content and coverage variations over different, possibly over- lapping contexts. The problem definition and the proposed model are quite general and cover a family of specific contex- tual theme analysis problems and methods as special cases. Empirical experiments on three different datasets show that the proposed model is effective for extracting the themes and comparing the views and coverages of themes across quite different contexts. Our work is an initial step toward a general model for con- textual text mining. An important future research direction is to further study how to better estimate the proposed mix- ture model as discussed in Section 3.1. Another important future research direction is to create evaluation criteria and judgements so that we can quantitatively evaluate different contextual text mining approaches. 7. ACKNOWLEDGMENTS This work was in part supported by the National Science Foundation under award numbers 0425852, 0347933, and 0428472. 8. REFERENCES [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. Topic detection and tracking pilot study: Final report. In Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, 1998. 1who, when, where and what (keywords) [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993-1022, 2003. [3] S. Boykin and A. Merlino. Machine learning of event segmentation for news on demand. Commun. ACM, 43(2):35-41, 2000. [4] C. C. Chen, M. C. Chen, and M.-S. Chen. Liped: Hmm-based life profiles for adaptive event detection. In Proceeding of KDD '05, pages 556-561, 2005. [5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statist. Soc. B, 39:1-38, 1977. [6] T. L. Griffiths and M. Steyvers. Fiding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl.1):5228-5235, 2004. [7] T. Hofmann. Probabilistic latent semantic analysis. In Proceedings of UAI'99. [8] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR'99. [9] J. Kleinberg. Bursty and hierarchical structure in streams. In Proceedings of KDD '02, pages 91-101. [10] A. Kontostathis, L. Galitsky, W. M. Pottenger, S. Roy, and D. J. Phelps. A survey of emerging trend detection in textual data mining. Survey of Text Mining, pages 185-224, 2003. [11] Z. Li, B. Wang, M. Li, and W.-Y. Ma. A probabilistic model for retrospective news event detection. In Proceedings of SIGIR'05, pages 106-113, 2005. [12] J. Ma and S. Perkins. Online novelty detection on temporal sequences. In Proceedings of KDD'03, pages 613-618, 2003. [13] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic approach to spatiotemporal theme pattern mining on weblogs. In Proceedings of WWW '06, pages 533-542, 2006. [14] Q. Mei and C. Zhai. Discovering evolutionary theme patterns from text: an exploration of temporal text mining. In Proceeding of KDD'05, pages 198-207, 2005. [15] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event threading within news topics. In Proceedings of CIKM'04, pages 446-453, 2004. [16] J. Perkio, W. Buntine, and S. Perttu. Exploring independent trends in a topic-based search engine. In Proceedings of WI '04, pages 664-668, 2004. [17] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. Griffiths. Probabilistic author-topic models for information discovery. In Proceedings of KDD'04, pages 306-315, 2004. [18] C. Zhai, A. Velivelli, and B. Yu. A cross-collection mixture model for comparative text mining. In Proceedings of KDD'04, pages 743-748, 2004. 655 Research Track Poster"
https://github.com/joel515/CourseProject	README.md	"CS410 Course Project - FC-CPLSA Background This project attempts to duplicate the Temporal-Author-Topic analysis in section 4.1 of the paper A Mixture Model for Contextual Text Mining (KDD 2006) by Qiaozhu Mei and ChengXiang Zhai. This model replicates the fixed-coverage contextual mixture model (FC-CPLSA) covered in section 3.2 of the research paper. The FC-CPLSA model is a specialized version of the CPLSA mixture model where the coverage over different contexts remains fixed. The experiment attempts to perform an author-topic comparitive analysis on reasearch paper abstracts between two different authors over three different time periods. The context features here are the author of the paper and the time which it was written. In this case, each context featre, and combinations thereof, are evaluated as different ""views"". The combination of all context features together form a global topic view. The mixture model is constructed such that each subview is then related to the global view, but specific to one of the two authors, one of the three timeframes, or one of the six combinations of author and timeframe. The following table from the reasearch paper illustrates the 12 applicable views more clearly: | # Context Features | Views | | --------------------- | ------------------------------------------ | | 0 | Global View | | 1 | Author A; Author B; < 1992; 1993 to 1999; 2000 to 2005 | | 2 | A and < 1992; A and 1993 to 1999; A and 2000 to 2005; B and < 1992; B and 1993 to 1999; B and 2000 to 2005 | The code works by essentially adding a ""view"" matrix to the vanilla PLSA algorithm. This matrix holds the probability of a document/context belonging to a specific view. Since a document and context has zero probability of belonging to a view that does not pertain to that context, the corresponding location of that document/view on the matrix is initially zeroed out. This is how the contextual features were incorporated as additional features to the mixture model. Data An attempt was made to replicate the data used in the Temporal-Author-Topic experiment of section 4.1, which originally consisted of the abstracts from ""two famous Data Mining researchers"" from the ACM Digital Library prior to the papers publication in 2006. Since the names of the two authors was not provided, an guess was made using the listing found here. Abstracts were then scraped for Jiawei Han (UIUC) and Philip S. Yu (UIC) published prior to 2006. The processed CSV file containing the associated metadata can be found here. Setup This setup assumes that the user has git and Python >= 3.7 installed. Clone this repository on a local workstation: git clone https://github.com/joel515/CourseProject.git One the package is cloned, cd into the CourseProject directory and run the setup.py installation script: python setup.py install This should pull the necessary dependencies to run the mixture model. If this fails, or if you prefer to install libraries manually, the list of dependencies is as follows: numpy pandas nltk The latest versions of each should suffice. Test Usage To use, you will either need to cd into the CourseProject/cplsa folder, or use the relative or absolute path to the CourseProject/cplsa/cplsa.py script. The output from either command will be a file titled ""CPLSA-<timestamp>.out"" containing run metadata and a full list of topic/view coverages. Quick and Dirty To run the script using the data provided and achieve the optimal coverage results, at least the best results that I had achieved, run the following command: python cplsa.py ../data/all_abstracts.csv ""author==1:author==2"" ""year<=1992:year>=1993 and year<=1999:year>=2000"" -t 20 -p 100 -wi 50 -th 0.3 --noSTEM Note that this will take some time to run on a normal workstation (roughly an hour). It will find an optimal solution for the 12 views with 20 different topics. Word stemming is omitted from the vocabulary preprocessing. By default, it will run 50 different ""warmup"" runs to find the optimal starting point (up to 50 iterations each) with an artificially large prior of 100 set on the global view to ""ensure a strong signal from global themes"", as per the original research paper. Each warmup run will iterate until the mean global view probability reaches 0.3 (slightly modified from the paper, which suggests 0.1), or until it hits 50 iterations. The optimal starting point (the one with the largest MLE) is ""pickled"" and then restarted for the full, 1000 iteration analysis. Convergence is reached when the difference between the previous log-likelihood and the current one is less than 0.001 (around 380 iterations or so for this dataset). Quicker and Dirtier You can also run a less optimal set of iterations to simply check that the package is running correctly: python cplsa.py ../data/all_abstracts.csv ""author==1:author==2"" ""year<=1992:year>=1993 and year<=1999:year>=2000"" -t 5 -w 2 -th 0.15 -e 0.1 --noSTEM This should run in much less time, but will give less than optimal, but decent, results. This time we are only asking to evaluate 5 topics with only 2 warm up runs, iterating until the mean global view probability reaches 0.15. The convergence criterion (i.e., the difference between the previous and current log-likelihood) is now only 0.1, so it should converge much quicker. General Usage What follows is a description of the inputs and arguments of the FC-CPLSA package for a general analysis. Inputs There are 3 required inputs to run this context mixture model: a CSV file containing the documents and associated metadata, and two strings containing Boolean operations to categorize two columns of the metadata, separated by colons. CSV File The CSV file can contain any amount of information, as long as there is one column labeled text containing the documents to be evaluated, and two additional labeled columns containing metadata to use as context. For instance, the data associated with this project looks like the following: | id | author | year | text | | --- | -- | -------- | -------------------- | | 1 | 1 | 2005 | Graphs have become ... | | ... | ... | ... | ... | | 363 | 2 | 1985 | The performance of ... | The text column contains the abstracts from either author 1 or 2. It is important to note the spelling and case of the column titles for the contextual metadata (author and year in this case), as they will be used in the following inputs to generate the views. Note that the id column is ancillary in this case and hence ignored. View Specification The next two inputs are used to generate the various contextual views to use in the mixture model. The format for the inputs should be strings enclosed in double quotes. Each input will refer to only one of the metadata columns and contain multiple Python-formatted Boolean operations to perform on that column's metadata, with each operation separated by a colon. Each Boolean operation in the string is used to extract a one-feature view. The code will then combine the different combinations of Boolean operations from the two inputs to extract two-feature views. In our example, the second input is ""author==1:author==2"", which consists of two valid Python Boolean operations to perform on the author column of metadata. The input will create two views, one consisting of the text from the author labeled 1, and the other consisting of the text from the author labeled 2. Likewise, the third input performs similar Boolean operations, this time on the year column (""year<=1992:year>=1993 and year<=1999:year>=2000""). This input contains three valid Python Boolean operations which will result in three one-feature views. Specifically, one view containing text from years prior to 1993 (year<=1992), one view containing text from the period between 1993 and 1999 (year>=1993 and year<=1999), and one view containing text after 1999 (year>=2000). The code will automatically create two-feature views through merging each combination of Boolean operations with a logical ""AND"". So in our example, the code will create the following six additional operations: author==1 and year<=1992, author==1 and year>=1993 and year<=1999, author==1 and year>=2000, author==2 and year<=1992, author==2 and year>=1993 and year<=1999, and author==2 and year>=2000 In total, if there are n operations specified in the first input string, and m operations specified in the second string, we will end up with n + m + nm + 1 views. In our example, we will have one global view, five one-feature views, and six two-feature views, for a total of 12 views. Numerical Arguments -w, --warmup [Integer, default=20] The number of warm-up E-M runs to perform to discover the best starting point. The mixture model will initialize the probability matrices randomly, leading to potentially local maxima. To find the optimal result, the code starts at different random points and uses the initial run that gives the maximum log-likelihood. -p, --prior [Float, default=1.0] Prior to assign to the global view probabilities. According to Mei et. al., to ensure a strong signal from the global themes, we need to assign an artificially large prior to the global view probabilities. -th, --threshold [Float, default=0.1] Mean global view probability threshold for warm-up convergence. The warm-up iterations will run until the mean probability for all of the global views falls below this value. -wi, --warmup_iter [Integer, default=25] Maximum number of warm-up E-M iterations per run. It is possible for the mean global view probability to converge higher than the supplied threshold. In this case, a maximum number of iterations is specified to kill that warm-up run. This starting point will be discarded. -t, --topics [Integer, default=10] Number of global topics/themes to extract. -i, --iterations [Integer, default=1000] Maximum number of E-M iterations if mixture model convergence cannot be obtained. -e, --epsilon [Float, default=0.001] Minimum log-likelihood estimate error for convergence. E-M convergence occurs when the difference between the prior log-likelihood estimate and the current log-likelihood estimate falls below this value. Flag Arguments -s, --save If specified, will save out the Corpus object containing the vocabulary and final matrix values as a pickled file upon E-M completion. This pickled file can be extracted later for further examination. Preprocessor Flags By default, the code will preprocess the vocabulary by performing a lower-case transformation, stopword removal, and Porter word stemming. The code will also automatically remove any non-ASCII characters, numbers, and any punctuation except -. The following flags can be used to override this automation. -noASCII, --noASCII Switch off non-ASCII character removal. -noLC, --noLC Switch off lower-case transformation. -noPUNC, --noPUNC Switch off punctuation removal. -noNUM, --noNUM Switch off number removal. -noSTEM, --noSTEM Switch off Porter word stemming (uses the PorterStemmer functionality from the nltk.stem library). -noSTOP, --noSTOP Switch off stopword removal (uses the English stopwords list from the nltk.corpus library). Results At a glance, the code seems to do a good job at finding general themes throughout the abstracts. With enough topics specified (in this case 20), it will capture a global ""frequent pattern mining"" theme similar to what was presented in the paper by Mei, et. al. | Topic: 15 | View: global | | --------- | ------------ | | mining | 0.06549781470307586 | | patterns | 0.04130088878399839 | | pattern | 0.031002929953296773 | | sequential | 0.017802974506818257 | | frequent | 0.015000041355054937 | | structures | 0.01246208861695447 | | algorithms | 0.010986754324057484 | | approach | 0.0106817830308471 | | efficient | 0.010681189139252758 | | information | 0.008901491869253191 | It also captures themes that are representative of their context - temporal and author context is captured by the view coverage. In this case, coverages for the ""frequent pattern mining"" global topic for author 1 published after 2000 matched up decently with the paper. Indeed, author 1 does seem to spend more time covering frequent pattern mining during this timeframe. | Topic: 15 | View: author==1 and year>=2000 | | --------- | ------------ | | mining | 0.0597014953071478 | | pattern | 0.03731343384903124 | | frequent | 0.029850750414016154 | | patterns | 0.029850745701384865 | | frequent-pattern | 0.022388049893103487 | | databases | 0.018656738476720114 | | examine | 0.018656723360443182 | | effectiveness | 0.018656708244252907 | | sequential | 0.01492538594417156 | | study | 0.014925382719363917 | There are instances that the linkage between views and global themes is not always fully captured, however. Many of the views also seem to be very localized - only giving coverages of 1 or 2 abstracts - albeit within the proper context. For instance, it seems that the author 2 view captures some themes of frequent pattern mining (""segmentation"" does appear in frequent pattern mining abstracts for author 2), but a closer look shows that the overall theme for this view seems to be leaning toward segmentation approaches to proxy caching. The following coverage seems to be very specific to one particular abstract: | Topic: 15 | View: author==2 | | --------- | ------------ | | media | 0.07857142857142857 | | caching | 0.06428571428571428 | | segmentation | 0.02857142857142857 | | large | 0.02857142857142857 | | cache | 0.02142857142857143 | | segments | 0.02142857142857143 | | size | 0.02142857142857143 | | whole | 0.02142857142857143 | | proxy | 0.02142857142857143 | | objects | 0.02142857142857143 | The reason for this may be my application of the global view prior. The prior is applied by replacing the values for each document for the global view in the view probability matrix with the specified prior. The view probability matrix is subsequently normalized. It is possible that this is not the best way to do this, or, my priors were too high, giving too strong of a signal to the global view. Multiple priors were tried, and it did seem that the higher priors performed slightly better, however. Another issue may be my implementation of the maximization step, or specifically how I formulated the coverage distribution. In general, any summations over coverages were removed, since we are using a fixed coverage approximation. Additionally, the coverage distrubtion under this assumption becomes the probability of a topic given a document's coverage. Therefore, for the formulation for p(l\|kD), I removed the summation over all documents/contexts, leaving a matrix of size number_of_topics by number_of_documents. This essentially gives us the document coverage probability from the vanilla PLSA formulation. I am not 100% sure that this would be the correct formulation."
https://github.com/wfcwfcwfcwfc/CourseProject	CS 410 Course Project - Final.pdf	"CS 410 Course Project - Final Report Source code and test set predictions Code: https://github.com/wfcwfcwfcwfc/CourseProject/blob/main/cs410-classification-contest.ipynb Test set predictions: https://github.com/wfcwfcwfcwfc/CourseProject/blob/main/cs410-classification-contest-result.txt Explain your model, and how you perform the training. Describe your experiments with other methods that you may have tried and any hyperparameter tuning. The classifier uses BERT transformer deep learning framework at its core. BERT is a recent NLP framework based on Transformer and self-attention architecture. It serves the ""encoder"" in the transformer model and is widely adopted in text generation and text classification. Training a BERT model generally has two stages: pre-training and fine-tuning. Pre-training aims at providing BERT a general understanding of a language. This step builds the embeddings and trains the parameters. Fine-tuning is optimizing BERT for certain specific tasks. Pre-training requires large language corpus and tremendous computing power. A common practice is to use existing pre-trained model and fine-tuning for the specific task. In this scenario, I use ""bert-large-cased-whole-word- masking"" from hugging-face as pre-trained model, then fine-tuned with the training data provided. After fine-tuning, the model is capable to perform predictions. The performance with default parameters beats the baseline. I also explored other pre-trained models like ""distilbert-base-uncased"", ""roberta-base"", ""xlnet-base- cased"". They all have smaller number of parameters compared to ""bert-large-cased-whole-word- masking"". The performance is good on training set but does not pass the baseline in test data. On engineering side, the model was implemented with PyTorch and deployed on Microsoft Azure ML Studio. It provided convenient middleware for ML tasks for easy deployment and prototyping. All the fine-tuning was done on a single compute, GPU instance and running time is less than 10 minutes. Compute GPU instance pre-installed with CUDA 10.1. All code is contained in the notebook shown in the beginning of this document. I used the ""NLP Best Practices"" library as well as ""NLP Utilities"" library to build the classifier. These libraries also provided example templates which is referenced in this project. Demo and Tutorial Environment Setup Sign up for Microsoft Azure. Create a subscription that allows you to use GPU instances. Student email get $100 free credit. Create resource group and ML workspace In the created workspace, launch ML studio. Create GPU compute instance and start. Upload notebook file, data file and clone the NLP Library. Update Conda: conda update -n base -c defaults conda Generate conda env config: cd nlp-recipes python tools/generate_conda_file.py --gpu Open nlp_gpu.yaml, update pytorch version from 1.4.0 to 1.5.1 Create Anaconda env conda env create -n nlp_gpu -f nlp_gpu.yaml conda activate nlp_gpu Register this virtual env to notebook python -m ipykernel install --user --name nlp_gpu --display-name ""Python (nlp_gpu)"" The environment setup is complete at this point. Running the notebook Open notebook and set kernel as 'nlp_gpu'. Run the notebook, and the result shows up in ""answer.txt"" Intermediate output can be seen in the notebook. Performance passed the baseline."
https://github.com/wfcwfcwfcwfc/CourseProject	CS 410 Course Project Progress Report.pdf	CS 410 Course Project Progress Report Classification Competition - Twitter Sarcasm Detection Which tasks have been completed? Overall Design, implementation and testing. BERT was decided as the core language model. Using pre-training then fine-tuning is the primary strategy. PyTorch as the implementation framework. Microsoft Azure ML platform as the deployment environment. Which tasks are pending? Fine-tuning data to beat the baseline. Using context data to improve the scores. Are you facing any challenges? No
https://github.com/wfcwfcwfcwfc/CourseProject	Proposal.pdf	CS 410 Project Proposal Fengchao Wang, fw9@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Fengchao Wang - Captain 2. Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection 3. 1. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. N/A 2. If you choose the classification competition, are you prepared to learn state-of- the-art neural network classifiers? Yes 3. Name some neural classifiers and deep learning frameworks that you may have heard of. RNN, LSTM, GAN, CNN. Tensorflow, Keras, PyTorch 4. Describe any relevant prior experience with such methods Used LSTM model to predict stock price and performed operational metric anomaly detection. Implemented with Facebook's prophet package. 4. Which programming language do you plan to use? Python
https://github.com/wfcwfcwfcwfc/CourseProject	README.md	CourseProject Feel free to reach out if any explnation needed. Slack: fw9 Final Report Code Result Demo Video
https://github.com/darrenmuliawan/CourseProject	Project Progress Report.pdf	Project Progress Report Darren Muliawan I have been able to get the Python script to run forever on the Cloud. I am also able to run the ExpertSearch system on my local machine. I am currently still working on implementing the service to monitor the faculty websites and automatically add new faculty members to the dataset. So far, I have not faced any challenges.
https://github.com/darrenmuliawan/CourseProject	Project Proposal.pdf	Project Proposal Darren Muliawan - darrenm2 (captain) For the final project, I choose to work on improving the ExpertSearch System, focusing on automatically crawling faculty webpages. The ExpertSearch's data right now is coming from the MP2 submissions of the previous course offering. I am planning to improve the system by creating a service that keeps on monitoring the faculty homepages from the MP2.1 signup sheets to look for a new faculty members that was added after the MP2 submissions of the previous course offering. To show that my implementation works better, I need to show that there are actually new faculty members that is not included in the ExpertSearch's dataset. This crawler will be a separate system that will update the dataset that ExpertSearch uses. I am going to use Python to complete this project. The main tasks that need to be completed for this project includes, 1. Automatic crawling for the faculty homepages 2. Ability to detect changes of faculty members 3. Updating the dataset that ExpertSearch uses These tasks should be able to be completed in ~20 hours.
https://github.com/darrenmuliawan/CourseProject	README.md	"Project Overview This project is made to improve the existing ExpertSearch system. My goal was to update the dataset used by ExpertSearch by creating a script that checks the URLs in the data/MP2_Part1 Signup - Sheet1.csv if they have new faculty members that wasn't added to the current ExpertSearch's dataset. Implementation In order to find new faculty members, first I get the original email list that the current ExpertSearch used from data/emails. I used faculty member's email to identify unique faculty member, since each person will most likely only have 1 email, and store it in Python dictionary for fast lookup. The next step is to crawl each faculty homepage URL from data/MP2_Part1 Signup - Sheet1.csv, and scrape each page to get the list of faculty member's email addresses. If the email address does not exist in the dictionary key, then I assume that this faculty member was new (added after ExpertSearch was created). For each new faculty member, the crawler will get their bio URL, updates the dataset, and update other files which then will be used to create the index. Algorithm Get the list of faculty homepage URLs from data/MP2_Part1 Signup - Sheet1.csv Scrape each URL and grab the a element that has href mailto: prefix, which is an indicator for an email address. Look for their personal page URL by recursively check its HTML structure. The crawler will use the email_element found in step 1, grab its parent, then iterate its children to find another a element with href that starts with either / or http which, possibly, contains the link for their personal page. Codeblock for recursively checks HTML structure, ``` for a in soup.select(""a[href^=\""mailto:\""]""): email = a[""href""].split(""mailto:"")[1] if email not in email_table: # NEW FACULTY MEMBER FOUND, FIND ITS PAGE LINK element = a.parent while True: a2 = element.find(""a"") # CHECK IF THE LINK IS (MAYBE) A FACULTY MEMBER'S PAGE if a2.has_attr('href'): href = a2['href'].encode('ascii') if a2 != a and (href.startswith(""/"") or href.startswith(""http"")): new_faculty_bios_url.append(href) emails.append(email) new_unis.append(uni) new_depts.append(dept) found += 1 break element = element.parent if element is None: break 4. Scrape the new member's bio page and store it indata/compiled_bios/n.txt, where n is some number. 5. Updatedata/email,data/depts,data/location, anddata/uniswith the new member's information 6. Use the modifiedextraction/extract_names.pyto updatedata/names.txt7. Use the modifiedwrite_file_names.pyto updatedata/compiled_bios/dataset-full-corpus.txtanddata/compiled_bios/metadata.dat8. Rebuild the index withmetapy.index.make_inverted_index(searchconfig)``` Limitations There are some limitations for this improvement due to the limitation of time since I am working on this project solo, such as, When the crawler looks for the faculty member's bio URL, it assumes that the URL is located somewhere in other a element's href. If the link is not in the href, then it won't be able to find the bio URL. It may also find an incorrect URL if the link is not in other element's href. ExpertSearch's script to get the faculty member's name uses Stanford Named Entity Recognizer (NER) Tagger, which doesn't 100% correctly detect the faculty member's name ExpertSearch's script to get the location relies on Google Maps API which may not be free since we are dealing with thousands of faculty members. So for the new faculty members, I set the location to be UNKNOWN, United States There is a mismatch in the number of records for data/urls Need to restart the app everytime the crawler finished to view the new faculty members How to run the code Note: Make sure you have both Python 2.7 and Python3 since some of ExpertSearch script does not work on Python 2.7 and some of them does not work on Python 3. For reference, I am using Python 2.7.16 and Python 3.7.7 Run crawler scripts python crawler.py [max_found] [run_forever] max_found is the maximum number of new faculty members that you want to find before it updates the dataset. run_forever sets to true if you want the crawler to loop forever. For example, python crawler.py 10 true For continuous checking and without limit, either set max_found to be -1 and run_forever to be true or run python crawler.py. Run ExpertSearch app gunicorn server:app -b 127.0.0.1:8095 How to check this project Open the original ExpertSearch http://timan102.cs.illinois.edu/expertsearch// and open the updated ExpertSearch at localhost:8095. Find the name of recently added faculty members by looking at new_bios/trial-n.txt and open the bio URL. The new members should only appear in the updated ExpertSearch. Video Link https://mediaspace.illinois.edu/media/t/1_ybtlfoxk"
https://github.com/TomTJarosz/CourseProject	CS410Proposal.pdf	Thomas Jarosz NetId: Tjarosz2 UIN: 669902044 CS410 Final Project Proposal For my final project, I have chosen Option 1: Reproducing A Paper. Questions: 1a) What are the names and NetIDs of all your team members? Thomas Jarosz: Tjarosz2 1b) Who is the captain? Tjarosz2 2) Which paper have you chosen? I have chosen to reproduce Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505. 3) Which programming language do you plan to use? I plan to use Python. 4) Can you obtain the datasets used in the paper for evaluation? Yes; the authors of this paper provide a link to the datasets.
https://github.com/TomTJarosz/CourseProject	ProgressReport.pdf	Thomas Jarosz Progress Report 1) Which tasks have been completed? I have read through the paper. 2) Which tasks are pending? Implementing the paper. 3) Are you facing any challenges? Yes; understanding the paper is difficult.
https://github.com/TomTJarosz/CourseProject	README.md	CourseProject This repo contains the work for Team Foo's CS410 final project. Proposal Contained within the file CS410Proposal.pdf is the team Foo's project proposal.
https://github.com/sbitra2/CourseProject	CS-410-Presentation.pptx	Text Classification Sarcasm Detection UIUC: CS-410 Course Project Introduction Problem: Generate a model to detect the sarcasm from the list of tweets. Approach: I have taken a language representation model BERT(Bi-directional Encoder Representations from Transformers) with pretrained deep bi-directional representations to train the model by using Google Tensorflow and Keras Deep learning APIs Results: With this approach I have managed to achieve the training accuracy to ~91% and validation accuracy to ~76%, On the competition leaderboard F1 Score: 0.7472 Code WalkThrough Link: https://www.youtube.com/watch?v=dXuhx-kpfaE&feature=youtu.be Demo Link: https://youtu.be/Z3Yu8VzgWMw Resources https://github.com/google-research/bert https://www.tensorflow.org/tutorials/text/classify_text_with_bert https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03 https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad
https://github.com/sbitra2/CourseProject	Project Proposal.pdf	Project Proposal: Name: Sirish C Bitra NetIDs: sbitra2 Captain: Sirish C Bitra Team Type: Individual Planned Competition: Text Classification Competition: Twitter Sarcasm Detection. Proposal: Currently, I'm going through the GloVe model with my Technology Review assignment and doing some research on how it suits with the Text classification competition. I believe with the use of neural network classifiers like Bi-directional LSTM with GloVe Embeddings I can achieve the expected results with this competition. Prior Experience: I don't have any prior experience in any of these models. Programming Language: Python
https://github.com/sbitra2/CourseProject	Project-Progress-Report.pdf	Project: Text Classification Competition: Twitter Sarcasm Detection Status: 1. Using Google Collab and TensorFlow2: Tried couple of models with Pre trained BERT and different loss entropy like binary Cross Entropy and Sparse Categorical Entropy. 2. Need to fine tune the model using parse categorical loss entropy which gave better results for me but it is very slow. 3. WIP - performance improvement.
https://github.com/sbitra2/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/rosed2/CourseProject	Progress Report.pdf	1. Progress made thus far a. So far, we have been able to preprocess our data so we can determine the aspect weight and aspect ratings. We were also able to determine the seed words in order to create our aspect segmentation and we will be using this when creating the rating regression. 2. Remaining tasks a. We have to still implement the rating regression model and determine if we were able to reproduce similar results to the research paper. We also need to finish implementing the sentiment analysis on each aspect which will lead to the creation of the aspect rating. 3. Any challenges/issues being faced a. Right now we are struggling with determining the term weights and trying to find an algorithm which will accurately assign weights to each term. We are looking into some of the lecture videos for additional help and will reach out to our TA if we are still struggling.
https://github.com/rosed2/CourseProject	Project Proposal.pdf	"Option chosen: Reproducing a Paper 1. Team members a. Archisha Majee, majee2 b. Captain: Rose Dinh, rosed2 2. Paper chosen: ""Latent aspect rating analysis without aspect keyword supervision."" Under subtopic: Latent aspect rating analysis 3. Programming language: python 4. Yes"
https://github.com/rosed2/CourseProject	README.md	"CS 410 CourseProject Project Proposal: We will be recreating the paper ""Latent Aspect Rating Analysis Without Aspect Keyword Supervision."" Code Overview: This code tries to implement Latent Aspect Rating Analysis (LARA) as a reproduction of research paper ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"" by Hongning Wang, Yue Lu, and ChengXiang Zhai. It takes a set of reviews and a list of aspects/topics covered within them. It also takes a list of feature words for each topic. Then, it finds the reviewer's ratings on these aspects and the weights the reviewer placed on these aspects to form the overall rating. This code can be used to analyze TripAdvisor reviews to find ratings on the following topics: Value, Rooms, Location, Cleanliness, Check in/front desk, Service, and Business service; and what weights the reviewer placed on each topic to construct the overall rating. Implementation: The code first preprocesses the reviews by removing stop words and stemming. Then the code assigns what topic each sentence in each review is about by comparing it to a list of user-defined topic feature words. The sentence is assigned the topic whose feature words it has the most of. Then, it uses sentiment analysis to determine topic ratings for each topic in each review. Next, it uses random variable initialization to determine the weights placed on each topic by the reviewer. We calculated which set of random weights returned the highest probability of getting the reviewer's overall rating from a Normal distribution whose mean was the weighted mean of the aspect ratings we found. Software Usage Tutorial Presentation: https://illinois.zoom.us/rec/share/aHaip21p63f29jFbE96x2s2LgFm9d6Pa4qIdLK3IwpMeqnh-pJqWV9ZXdbejbLHJ.ThV8W0Z9PVQhu4fv?startTime=1607902233000 Usage Notes: The code requires nltk, scipy, and numpy. Install using pip or other preferred python installation method. To use this code, download the repository, cd into the folder, and run the python file ""code.py"". Main Results: We found the aspect ratings and aspect weights for every review we parsed in the file ""test_result.txt"". To evaluate, we found the mean squared error of the aspect ratings was 2.383. In the paper, the desired result of the MSE was 2.130. Our result differed from the desired result by a very small amount. This difference could be due to the fact that we might've used a different sentiment analysis library and we weren't sure how to handle large amounts of neutral words. We also weren't sure how to determine the aspect weights, so we instead used a brute force approach where we tested random weights while the paper used gradient optimization but we couldn't get that to work. Team Contributions: Rose Dinh: Worked on preprocessing, assigning topics to each sentence, and determining ratings of topics in each review Archisha Majee: Worked on determining topic weights and finding MSE to evaluate the results"
https://github.com/samphadnis/TeamTextDragons	CS410 Project Progress Report.pdf	Progress Report: Text Classification Competition: Twitter Sarcasm Detection Group Name: Text Dragons Team members: Chen Yuan (cheny9), Email: cheny9@illinois.edu Sameer Phadnis (phadnis3) - Team Leader, Email: phadnis3@illinois.edu Abhishek Shinde (ashinde2), Email: ashinde2@illinois.edu Overview: We will be joining the text classification competition. The object of the competition is to identify sarcasm from a set of Twitter responses. The given dataset is split into two: the train dataset (5000 observations) and the test dataset (1800 observations). For the training dataset, we are given the response (which is the Tweet to be classified), the context (which is the conversation of the context) and the label. We will be using the training dataset to build the models and making predictions based on the test set. Tasks Completed: So far we have outperformed the baseline model with only a few attempts. Models we have tried so far: 1. Fasttext (did not outperform the baseline) 2. RoBERTa + simple neural networks model (outperforms the baseline) Tasks Pending: 1. Writing a documentation for our models 2. Organize and submit code Challenges: The challenge about computation power as indicated in the project proposal was addressed by using google colab. Therefore, there are no remaining challenges.
https://github.com/samphadnis/TeamTextDragons	CS410_ Project Proposal_TextDragons.pdf	"1. Our project team members are Chen Yuan (cheny9), Sameer Phadnis (phadnis3), and Abhishek Shinde (ashinde2). Team name is ""Text Dragons"" and Sameer Phadnis is team captain. 2. We plan to join the text classification competition. 3. Yes, we are prepared to learn state-of-the-art classifiers. Some popular neural classifiers consist of LeNet, AlexNet, and GoogLeNet. For a computer vision project, Abhishek's team utilized a modified LeNet convolutional neural network to classify images of sign language into the letters of the alphabet.We may also explore the below neural classifiers and deep learning frameworks for the project: Feed Forward Neural networks: * Deep Average Network * fastText RNN (Recurrent Neural Network) based models: * Tree-LSTM * Multi-Timescale LSTM CNN (Convolutional Neural Network) based models: * Dynamic CNN Capsule Neural Network: * CapsNET Transformers: * BERT 4. We will use Python as the programming language for the project Project Proposal: Text Classification Competition: Twitter Sarcasm Detection Group Name: Text Dragons Team members: Chen Yuan (cheny9), Email: cheny9@illinois.edu Sameer Phadnis (phadnis3) - Team Leader, Email: phadnis3@illinois.edu Abhishek Shinde (ashinde2), Email: ashinde2@illinois.edu Overview: We will be joining the text classification competition. The object of the competition is to identify sarcasm from a set of Twitter responses. The given dataset is split into two: the train dataset (5000 observations) and the test dataset (1800 observations). For the training dataset, we are given the response (which is the Tweet to be classified), the context (which is the conversation of the context) and the label. We will be using the training dataset to build the models and making predictions based on the test set. Objective: We will be using Python as the main programming language for the project. We will try different models with state-of-the-art classifiers. Our potential candidates includes: 1. LeNet 2. AlexNet 3. GoogLeNet 4. ResNet 5. LSTM 6. Fasttext Abhishek's team has had previous experience on a computer vision project, which utilized a modified LeNet convolutional neural network to classify images of sign language into the letters of the alphabet. Chen has had previous experience building Wide-ResNet models to classify street view house numbers. Challenges: There are several challenges that we need to address throughout our project: 1. The Tweets may contain emojis and spam information that we need to deal with at the data preprocessing step. 2. The power of a CPU may not be sufficient to train deep neural network models. Therefore, we will need to leverage some cloud computing resources, such as google colab."
https://github.com/samphadnis/TeamTextDragons	Project Documentation.pdf	"Documentation for Text Classification Project I. Overview This program implements a model for detecting sarcasm in text. The training data consists of twitter feeds having context text, where the response to the context is labelled as sarcasm or not sarcasm. The objective is to define a model based on this training data that detects sarcasm in text, and use it to classify the responses in the test data as sarcasm or not sarcasm. II. Data Profiling We conducted data profiling on both the training data and test data. The datasets provided are in json format. The training data contains the following columns: * Label: Indicates whether the response is sarcasm or not sarcasm. * Response: A string which contains the Tweet response to be classified. * Context: A list which contains the conversation history in context of response. The order of the list is the same as the order of the dialogue (i.e. response directly replies to the last element of the list) The training dataset contains 5000 Tweets in total and 2500 are labeled as sarcasm and the rest are labeled not. Therefore, the training dataset is considered as a balanced dataset and no oversampling is required. We also conducted analysis on the response text length on both the training set and the testing set. The distribution shows that the training set and the testing set are similar in terms of response length distribution, and therefore, models based on the training set might be suitable for predicting the testing set. In addition to response length, we also conducted an analysis on the number of contexts. The distribution shows that for both the training set and the testing set, the majority of the responses have two corresponding contexts. Based on our calculation, the average number for contexts for the training set is 3.87 and for the testing set is 3.16. During our data preprocessing step, we figured that keeping all contexts will draw a warning message which indicates that ""Token indices sequence length is longer than the specified maximum sequence length for this model"". Therefore, we decided to use the last three available contexts. Percentile Characters(Train) Characters(Test) 0.1 71.0 59.0 0.2 85.8 76.0 0.3 97.0 92.0 0.4 108.0 108.0 0.5 117.0 127.0 0.6 126.0 149.4 0.7 145.0 176.0 0.8 190.0 214.0 0.9 254.0 265.0 III. Data Cleaning The following approaches were tried to clean up the data to see if it improved computation efficiency and\or classification accuracy: 1. Convert all text to lowercase 2. Remove all punctuations (except apostrophe) 3. Remove stop words 4. Stemming of words A separate script clean.py was written to process the input data and write the cleaned up data in the jsonl format. However, we found that the cleaning of the data actually resulted in a loss of accuracy as compared to the original data. This could be because the cleanup process affected the sarcasm detection learning model. Therefore the decision to clean the data must be carefully weighed to see if it adversely impacts the learning model, and experimentation with\without cleaning is essential. IV. Model Architecture There are multiple ways to build contextual sentiment analysis models. Our final decision is to use the simple method: combine contexts with the response to form a single string as input to the model(Amardeep Kumar, 2020). Throughout the process, we have considered the following models: 1. Fasttext 2. Roberta + 1-hidden layer neural network + ReLU 3. Roberta + 1-hidden layer neural network + Softmax 4. XLMRoberta + 1-hidden layer neural network + ReLU 5. Albert + 1-hidden layer neural network + ReLU We also tried different learning rates for these models. The result shows that model #2 combined with a learning rate of 1e-5 gives us the best performance on the testing set with a f1 score of 0.787. The model architecture is as below: V. Implementation The program uses Google Colab + Jupyter notebook to take advantage of the Google GPU for accelerated data processing. The training and test data is stored on Google Drive. The PyTorch library is used, which enables the usage of GPU as well as the use of various implementations of state-of-the-art NLP transformer libraries i.e. Pre-trained Language Models (PLMs).Transformers allow for parallelization, which makes it possible to efficiently train very big models on large amounts of data on GPU clusters.Transformer-based PLMs use much deeper network architectures, and are pre-trained on much larger amounts of text corpora to learn contextual text representations by predicting words based on their context. One of the most popular transformers is BERT, developed by Google. RoBERTa (developed by Facebook) is an extension that is more robust than BERT, and is trained using much more training data and dynamic masking. To investigate a lighter model, we researched and tested out Albert, which is essentially a lite-BERT. The benefits for training are evident; it provides two parameter reduction techniques to improve memory usage and BERT speed. According to Hugging Face, these techniques are forming two smaller matrices from an embedding matrix and providing repeated layers split on groups. The experience with this consisted of beating the baseline, but not being one of our best models. The RoBERTa PyTorch transformer library is selected in our implementation as our top model. The data is loaded using a batch size of 16. The model is trained on the training data set using epoch count of 12. The resulting model is serialized and saved in the PyTorch model format. The test data is then evaluated against the model to classify the tweets as sarcasm\not sarcasm (stored in the generated answers.txt file). VI. Instructions to Run Code 1. In Colab Notebooks under Google Drive, create a directory called TextClassification . 2. Within the TextClassification  directory, create another directory called data. 3. Include the data files (test.jsonl and train.jsonl) in this data  directory. 4. From the GitHub repository (https://github.com/samphadnis/TeamTextDragons), open roberta_no_pretrain.ipynb from the code  directory into Colab Notebooks. 5. Since the saved model is too large to be uploaded to Github, we created a link to google drive and shared it on Github readme page. Download the model and save it under the TextClassification  directory. 6. If you want to run the whole model, including the training portion, in the navbar, navigate to ""Runtime"" and select ""Run all"". The pipeline should run, and the results answer.txt should be generated under Colab Notebooks. 7. If you only want to generate the results with the saved model, you may skip the Model Training portion of the code. VII. Conclusion The program is able to beat the baseline. The state-of-the-art Pre-trained Language Models such as RoBERTa are powerful tools for text classification. VII. Team Contributions Sameer Phadnis (phadnis3@illinois.edu): Team Leader - Worked on and tested variations of data cleaning processes and documentation Chen Yuan (cheny9@illinois.edu) - Led the development work by setting up project infrastructure, implementing data profiling, classification pipeline, presentation and documentation Abhishek Shinde (ashinde2@illinois.edu) - Worked on training and testing various models, and improving the classification pipeline and documentation VIII. References - Kozlov, Alexander. ""Fine-Tuning BERT and RoBERTa for High Accuracy Text Classification in PyTorch."" Medium , Towards Data Science, 7 Sept. 2020, towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classificatio n-in-pytorch-c9e63cf64646. - The code repository we relied on can be found here: https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20fo r%20text%20classification.ipynb - ""Transformers."" Transformers - Transformers 4.0.0 Documentation , huggingface.co/transformers/index.html. - Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov: ""RoBERTa: A Robustly Optimized BERT Pretraining Approach"", 2019; http://arxiv.org/abs/1907.11692 arXiv:1907.11692 - Amardeep Kumar, Vivek Anand: ""Transformers on Sarcasm Detection with Context"", 2020; https://www.aclweb.org/anthology/2020.figlang-1.13.pdf"
https://github.com/samphadnis/TeamTextDragons	README.md	"Contextual Twitter Sarcasm Detection CS 410: Text Information Systems Final Project This repository contains the project code, data, report and video presentation for text classification competition. Team members Sameer Phadnis: phadnis3@illinois.edu Abhishek Shinde: ashinde2@illinois.edu Chen Yuan:cheny9@illinois.edu Code environment We are running our code on the google colab platform. You will need: - Python 3.x - transformers - torch - pandas - sklearn All packages are pre-installed except transformers. Model Due to the limit of file size on Github, we are not able to upload our final saved model to the github repository. Therefore, we created a google drive link to share our final model: https://drive.google.com/file/d/1bFc4HK1N7pneVfDeVwESM3TehEK4o44w/view?usp=sharing Setup In Colab Notebooks under Google Drive, create a directory called TextClassification. Within the TextClassification directory, create another directory called data. Include the data files (test.jsonl and train.jsonl) in this data directory. From the GitHub repository (https://github.com/samphadnis/TeamTextDragons), open roberta_no_pretrain.ipynb from the code directory into Colab Notebooks. Since the saved model is too large to be uploaded to Github, we created a link to google drive and shared it on Github readme page. Download the model and save it under the TextClassification directory. If you want to run the whole model, including the training portion, in the navbar, navigate to ""Runtime"" and select ""Run all"". The pipeline should run, and the results answer.txt should be generated under Colab Notebooks. If you only want to generate the results with the saved model, you may skip the Model Training portion of the code. The subfolders should be organized as below: . +-- ... +-- Colab Notebooks | +-- TextClassification | | +-- data | | | +--train.jsonl | | | +--test.jsonl | | +-- roberta_no_pretrain.ipynb | | +-- model_RoBERTa_relu_nopretrain.pkl | +-- +-- Presentation We created a video presentation to walk through the code and show how to reproduce the results. The video can be found: https://mediaspace.illinois.edu/media/1_79uj7ghe. File Description Reports: CS410_ Project Proposal_TextDragons.pdf - Project proposal CS410 Project Progress Report.pdf - Project progress report Code: code/Profiling.ipynb - Pre-modeling analysis code/roberta_no_pretrain.ipynb - Our best model, which uses roberta + relu code/roberta_no_pretrain_softmax.ipynb - roberta + softmax code/xlmroberta_no_pretrain.ipynb - xlmroberta + relu code/albert.ipynb - albert + relu code/Fasttext.ipynb - Fast text model code/clean.py - Data preprocessing script Data: data/train.jsonl - labeled training data data/test.jsonl - unlabeled testing data Reference Kozlov, Alexander. ""Fine-Tuning BERT and RoBERTa for High Accuracy Text Classification in PyTorch."" Medium, Towards Data Science, 7 Sept. 2020, towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646. - The code repository we relied on can be found here: https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20for%20text%20classification.ipynb ""Transformers."" Transformers - Transformers 4.0.0 Documentation, huggingface.co/transformers/index.html. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov: ""RoBERTa: A Robustly Optimized BERT Pretraining Approach"", 2019; http://arxiv.org/abs/1907.11692 arXiv:1907.11692 Amardeep Kumar, Vivek Anand: ""Transformers on Sarcasm Detection with Context"", 2020; https://www.aclweb.org/anthology/2020.figlang-1.13.pdf"
https://github.com/Jagaskak/CourseProject	Causal Topic Modeling with Time Series Feedback.pptx	"Causal Topic Modeling with Time Series Feedback Akshaya Jagannadharao (akshaya2) Heidi Toussaint (heidist2) Hari Venkitaraman (hv4) Pre-setup -- Data This project requires access to New York Times data. There are two methods to collect articles published by New York Times from May 2000-Oct 2000. Gain access to the LDC corpus. You should parse the documents and only collect relevant articles (i.e. keep paragraphs that contain ""Bush"" or ""Gore). You can also collect the data by scraping the New York Times website. Details on how to do so can be found in the README file in the Data folder of the project repo. Ensure the data is stored locally Pre-setup -- Jupyter Notebook For both methods below, you will need to have a version of the data stored locally. We strongly suggest using Google Colab as it reduces the overhead of installing packages. To run on Google Colab: Click on link at top of Jupyter Notebook that says ""Open in Colab"" Make sure that the notebook is not in your Shared Drive Edit the paths to the New York Times and IEM datasets in the 3rd cell To run locally: Clone GitHub Open up Jupyter Notebook locally and open file ("""") How to install the software In Google Colab: Run cell block two (everything is pretty much already installed) Locally: Install the following libraries using pip (if you have not done so already) Gensim Spacy pyLDAvis Scipy Statsmodels Numpy Pandas Nltk For stopwords, run python -m nltk.downloader stopwords 2. How to use the software Run each cell block in the notebook. If you'd like to try out different parameters (i.e. change number of topics, decay param, etc.) you can edit the block and rerun it. 3. Example use case Improve topic modeling by using a causal reference (e.g. analyzing topics with external time series variables such as stock prices) Video Link: CS410_TeamEastToWest_FinalProject - Illinois Media Space"
https://github.com/Jagaskak/CourseProject	Final Writeup.pdf	"Team EastToWest Akshaya Jagannadharao (akshaya2) | Heidi Touissant (heidist2) | Hari Venkitaraman (hv4) 1) An overview of the function of the code (i.e., what it does and what it can be used for). We are reproducing a paper in this project: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback. We are only reproducing the first experiment where the authors examine the 2000 U.S. Presidential Election. The code we implement here can be used to find causal relationships between two different datasets. To be more specific, we are trying to find a causal relationship between new articles and stock market prices. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Data: New York Times: There are two methods to collect articles published by New York Times from May-Oct 2000. One is to gain access to the LDC corpus. You should parse the documents and only collect relevant articles (i.e. keep paragraphs that contain ""Bush"" or ""Gore). If the corpus is unavailable to you, it is still possible to collect the data by scraping the New York Times website. Details on how to do so can be found in the README file in the Data folder. Iowa Electronic Market (IEM) Time Series: The procedure to collect the IEM data is detailed in the README file in the Data folder. Code: We are running everything inside a Jupyter Notebook (with Python version 3.7.5) so that we can easily create and display visualizations. The code can be broken down into 3 sections (we have taken the initiative to section it off into three cells). The first cell contains all the import statements. We have detailed the usage of software in section 3 of this document for further perusal. The second cell loads the New York Times dataset and the Iowa Electronic Market Time Series Data into data tables and creates BOW (bag of words) representations by article and date. You should only have to run this cell once unless you want to change the BOW representation. The third cell contains the code to run the topic modeling with time series feedback. There is an example use case documented in the README file as well as an example all in the jupyter notebooks. Challenges + Improvements + Differences between our results and theirs: * Delay in obtaining dataset from NYT corpus * We were unsure if we were able to obtain the dataset because of copyright permissions. Due to this, we were unable to start our project until 1 month before the deadline (and one week was technically break). * No efficient way to write code as a group remotely * We struggled to find a collaborative Jupyter Notebook environment. Jupyter Notebook is an important tool for our use to easily visualize the different outputs and data structures we were generating throughout the code (i.e. visualization for LDA model, visualization of results). * Missing dates in Time Series * There are multiple ways to handle missing dates in time-series data. Namely, take the last price, take the logistic regression, take an average, drop the missing date. # We took the easiest route and simply used the previous day's prices for the missing dates as there were only two (6/7 & 6/8) * Calculating mu remained unclear after referring to the referenced paper, attending multiple office hours, and consulting with other classmates. * We decided to go with a similar parameter already integrated with Gensim's LDA model, decay. * Some material was outside the scope of the course. We needed to understand how Granger Causality worked and the constraints of a time series feedback (namely the data should be stationary). * A typographical error in the paper was discovered while attempting to reproduce Table 1 (p. 3, table 1) * This caused confusion amongst the group and we spent some time trying to understand it. After attending office hours, the professor, a co-author of the paper, ultimately decided this was a typographical error * Our results do not replicate the results achieved in the paper. There is a clear difference in the progression of confidence and purity levels through multiple iterations. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Additional Instructions can be found in the README files in the GitHub repository. You can find a short demo on Mediaspace on how to set up the code: CS410_TeamEastToWest_FinalProject - Illinois Media Space. To view a run of our code with the graphs generated at the end, visit CS410_TeamEastToWest_FinalProject. If you would like to see the code in action, please reach out to the contributors (Akshaya, Heidi, or Hari) for a demo/tutorial. Due to copyright restrictions on the data, it is not possible to run this code yourself. We use the following package and import statements: pip install pyLDAvis import re import numpy as np import numpy.linalg as la import pandas as pd from pprint import pprint import datetime # NLTK import nltk nltk.download('stopwords') from nltk.corpus import stopwords from nltk import ngrams # Gensim import gensim from gensim import models import gensim.corpora as corpora from gensim.utils import simple_preprocess from gensim.models import CoherenceModel from gensim.models import Phrases # TODO: to create bigrams with # spacy for lemmatization import spacy # Plotting tools import pyLDAvis import pyLDAvis.gensim # don't skip this import matplotlib.pyplot as plt import statsmodels from statsmodels.tsa.stattools import grangercausalitytests import warnings warnings.filterwarnings('ignore') References: Mei, Q, Ling, X, Wondra, M, Su H., and Zhai, C. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171-180, New York, NY, USA, 2007. ACM. Mupfururirwa, W. (2019, September 19). Retrieved from https://stackoverflow.com/questions/58005681/is-it-possible-to-run-a-vector-autoregression-analysis- on-a-large-gdp-data-with Prabhakaran, S. Time Series Analysis in Python, Retrieved from https://www.machinelearningplus.com/time-series/time-series-analysis-python Sarit, M. (2019, October 7) Time Series Forecasting using Granger's Causality and Vector Auto-regressive Model, Retrieved from https://towardsdatascience.com/granger-causality-and-vector-auto-regressive-model-for-time-series- forecasting-3226a64889a6 4) Brief description of the contribution of each team member in case of a multi-person team. We initially tried to code in real-time as a group across various notebook environments but could not find an efficient solution that would have allowed us to access the data at the same time (we could have potentially hosted the data on the cloud but that would have required additional charges). We ultimately decided that Akshaya would share her screen via Zoom video call while she coded and Hari and Heidi worked through understanding the steps of the paper together as well as communicated with TAs and Prof during office hours. Essentially pair programming with one person as driver and two people as navigators. During the week, Heidi and Hari would go through the paper and figure out the next steps. Akshaya would code during the week. During the meeting, we would review what Akshaya did and find any gaps in logic or deviations from the paper/catch for mistakes. After reviewing the code, Hari and Heidi would go through the next steps that Akshaya would code up later in the week. We split up the work like this because the LDA model would take a significant time to run during the meeting. If there was time remaining in our meeting, we would take some time to code as a team. If there was a report due, we would complete it as a team during the meeting. To make the best use of our time since the algorithm takes a significant amount of time to run, we decided on this strategy. When issues came up that we could not agree on a solution as a team, Hari and Heidi would take the time during the week to talk with the TAs during office hours. A good example of this situation in our group would be that Hari found a typo in the paper that caused a lot of confusion. Heidi and Hari went to office hours to clarify with the TA. And when there was no resolution, Heidi and Akshaya spoke with the professor during office hours and finally received an answer."
https://github.com/Jagaskak/CourseProject	Progress Report.pdf	Team East to West Reproduce results from paper: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 1. Which tasks have been completed? Implemented one iteration of the paper. 2. Which tasks are pending? * Need to understand how the Mu weight works * Implement feedback loop * Calculate purity * Use purity as stopping criteria 3. Are you facing any challenges? a. Calculating/implementing mu? b. Granger Causality covariance warning i. Calculating the word stream leads does not make a full rank matrix. We think this makes sense that this happens, but the paper doesn't mention what they do
https://github.com/Jagaskak/CourseProject	Project Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Akshaya Jagannadharao, akshaya2 [Captain] * Heidi Toussaint, heidist2 * Hariharan Venkitaraman, hv4 2. Which paper have you chosen? Causal topic modeling Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? Sort of. New York Times Dataset (preferred dataset if we get access): * https://catalog.ldc.upenn.edu/LDC2008T19 Iowa Electronic Markets (IEM) 2000 Presidential Winner-TakesAll Market: * https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?marke t_ID=29 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? New York Times Articles * https://spiderbites.nytimes.com/2000/ * It has all the articles written in 2000. We just need to scrape the page ourselves and aggregate the useful text (which we would have to do using the LDC corpus anyway). 6. If you answer ""no"" to Questions 4 & 5, how are you gohttps://spiderbites.nytimes.com/2000/ing to demonstrate that you have successfully reproduced the method introduced in the paper? Our results shouldn't differ from the method in the paper because we are using the same exact data. The paper didn't use any of the other meta information from the LDC corpus, only the date (which we can find in the article) and the text (which is available). Since we have both pieces of information, we should be able to reproduce the method introduced in the paper."
https://github.com/Jagaskak/CourseProject	README.md	"Overview This project attempts to reproduce the paper Mining causal topics in text data: iterative topic modeling with time series feedback. We are only reproducing the first experiment where the authors examine the 2000 U.S. Presidential Election. The code we implement here can be used to find causal relationships between a textual dataset and non-textual dataset. We prove that by using the idea of causality, we can acheive better topic mining results than the baseline model (i.e. without time series feedback). Each folder in this repo contains a README file that will explain more about the contents of the directory. Repository Structure Parent Directory The top level directory contains the jupyter notebooks with our code. There is further documentation in the jupyter notebook as well. To view a demo on how to setup the notebook, visit Example Run for Causal Topic Modeling with Time Series Feedback. To view a run of our code with the graphs generated at the end, visit CS410_TeamEastToWest_FinalProject. Data Folder The Data folder provides an overview/methodology of how to access the data and structure it to run our code. Results Folder The Results folder contains images of our results as well as interactive visualizations in the form of html files. We also provide a README file in this directory that analyzes the differences between our results and the papers and why this may be so. Run the code We strongly suggest using Google Colab as it reduced the overhead of installing packages. The other option is to download the Jupyter Notebook and set it up on your local machine. Make sure the data is stored locally. To run on GoogleColab (will save a copy in your Google Drive) Open the Jupyter notebook titled ""ITMTF_GoogleColab.ibynb"" At the top of the notebook there is a link that says ""Open in Colab"" Click the link and follow the instructions to give permissions to GoogleColab The notebook should be located in My Drive. Edit the paths in the first cell to point to your data on your Google Drive Run the cells To run locally Clone this reproduce Open the Juptyer notebook titled ""ITMTF.ipynb"" Run the cells Code Examples There is documentation for the main function (ITMTF()) which implements the topic modeling. An example of how to call it is below as well as in the notebook. ITMTF Function Parameters: paramCtrl -- str (either 'k' or 'decay'), tells function to test various k or decay values params -- list of integers, each val in list is either a k or decay values to test. For k values, the last k value will be used to test a variable number of topics starting at k topics. In every iteration of ITMTF, k will increase by the number of causal topics found + some constant decay -- (0.5, 1] decay parameter to use when running lda (effective only when paramCtrl='k') k -- Interger representing number of topics to use when running lda (effective only when paramCtrl='decay') iterations -- Integer number of iterations to run ITMTF const_k_increase -- Integer number of topics to increase (in addition to number of causal topics found) each iteration when running varying number of topics (effective only when paramCtrl='k') Default is 0 verbose -- True/False Print out purity and confidence every iteration Returns: k_lda_model -- The final lda_model k_avg_purities -- Average purity of all causal topics in each iteration k_avg_conf -- Average confidence of all causal topics in each iteration Example Calls: k_lda_model, k_avg_purities, k_avg_conf = ITMTF(""k"", params=[10, 20, 30, 40, 10]) mu_lda_model, mu_avg_purities, mu_avg_conf = ITMTF(""decay"", params=[0.51, 0.6, 0.7, 0.8, 0.9, 1]) Standard Graph Visualizations These visualizations are built using matplotlib.plotly. The function show_plot() is a wrapper for the plotly function plot(). ``` k_labels = [""10"", ""20"", ""30"", ""40"", ""Varying Number of Topics""] show_plot(range(1, 6), k_avg_purities, ""Iteration"", ""Average Purity"", k_labels, xticks=range(1, 6), yaxisrange=[0, 120], title=""Average Purity for Different Number of Topics"", legend_title=""Number of Topics"", saveAs=""purity_k.png"") show_plot(range(1, 6), k_avg_conf, ""Iteration"", ""Average Confidence"", k_labels, xticks=range(1, 6), yaxisrange=[95.5, 100], title=""Average Confidence for Different Number of Topics"", legend_title=""Number of Topics"", saveAs=""confidence_k.png"") mu_labels = [""0.51"", ""0.6"", ""0.7"", ""0.8"", ""0.9"", ""1""] show_plot(range(1, 6), mu_avg_purities, ""Iteration"", ""Average Purity"", mu_labels, xticks=range(1, 6), yaxisrange=[0, 120], title=""Average Purity for Different Decay Values"", legend_title=""Decay Values"", saveAs=""purity_mu.png"") show_plot(range(1, 6), mu_avg_conf, ""Iteration"", ""Average Confidence"", mu_labels, xticks=range(1, 6), yaxisrange=[95.5, 100], title=""Average Confidence for Different Decay Values"", legend_title=""Decay Values"", saveAs=""confidence_mu.png"") ``` Interactive Visualizations These visualizations are built using pyLDAvis. The below example also includes a line showing how to save the visualization. ``` pyLDAvis.enable_notebook() vis = pyLDAvis.gensim.prepare(k_lda_model, corpus, id2word) pyLDAvis.save_html(vis, 'LDAvis_k.html') pyLDAvis.enable_notebook() vis = pyLDAvis.gensim.prepare(mu_lda_model, corpus, id2word) pyLDAvis.save_html(vis, 'LDAvis_mu.html') ```"
https://github.com/srzn/CourseProject	Documentation.pdf	"Project Documentation Srujan Netid:ssg7 LiveLab ID:srzn December 13, 2020 Included is the project flow, definition and function of each module and usage details 1 Project Flow I participated in the IR competition. I used MP2.4 code template to implement various state of the art ranking functions for IR evaluation and ranking 1.1 Converting Data Format of the data provided is incompatible with that of my code. So, the first step is data conversion. Queries are provided in xml and need to be converted into a text file, each query per line. Actual documents are provided as a csv file with duplicate entries per document and a lot of irrlevant information. Each document is identified by an alphanumeric id. Relevant data from the csv file need to be extracted and populated as document data with each document occupying one line in the final .dat file. My code takes document ids as numbers. So, a dictionary with uid to number mapping needed to be built. The entire process needed to be implemented for both training and testing data. This concludes data conversion. 1.2 Training Step I used 6 IR ranking functions to train on the dataset to extract optimal param- eters for each of the ranking functions. The user running my training program is given a choice/option to select any one of these ranking functions. One of the ranking function,InL2 ranker, was overloaded with my custom scoring func- tion. I swept each ranking function with a number of parameters over the given training dataset and labels. The combination of parameters that lead to the best NDCG@20 were saved to a file named ""Option< num >.txt"" These were the ranking functions used:  Option 0 : BM25 1  Option 1: InL2 ranker  Option 2: InL2 ranker  Option 3: Jelinek-Mercer Smoothing  Option 4: Dirichlet Prior Smoothing  Option 5: Absolute Discounting Smoothing 1.3 Testing Step From the ""Option< num >.txt"" I obtained from the training step, we use corresponding ranking functions to obtain ranking scores of each document w.r.t a query. Test data is slightly different from training dataset. So, inverted indices were built separately for training dataset as well. 2 Module Definitions The code files in both testing and training have similar names and functions. Following code files can be found in my software package: 2.1 queryextr.py This file is used to convert query.xml into queries.txt and queries-test.txt 2.2 convert-dat1.py This file is used to extract data from metadata.csv and document jsons to build SarsCov.dat file which is inturn used to build inverted index and used to evaulate the ranking functions 2.3 uidmap.py This file is used to build dictionaries ""uidmap.txt"", ""uidrevmap.txt"". These dictionaries contain docid to uid mappings and vice-versa. 2.4 search eval.py This is the main program that runs the training phase to output best parameters that generate the highest NDCG 2.5 search test.py This is the main program that ranks and scores each document from test set against the test queries and outputs a ""testpredictions.txt"" output which has docids instead of uids 2 2.6 predictgen.py Uses the dictionaries obtained from ""uidmap.py"" and produces the final output ""predictions.txt"" that has uids 3 Usage 3.1 Training In the directory that has search eval.py, run python search eval.py config.toml 'Option' where 'Option' has the same range as described in ""Training step"". You'll obtain a text file ""Option0.txt"" (if you chose Option 0). Please copy this file over to the ""test"" directory that has ""search test.py"" 3.2 Testing After the above step, in the directory that has ""search test.py"", run python search test.py config-test.toml 'Option' where 'Option' can take any integer value between 0 and 5. Please refer the documentation to know more about each choice. For example, if you chose Option 0 then you'll run python search test.py config-test.toml 0 And then run, python predictgen.py The above code must be executed to obtain the final predictions in a text file. If you chose Option 0 you would obtain ""prediction0.txt"" as your final predictions for all the queries capping at 1000 top documents per query. 4 Leaderboard 3 Figure 1: Leaderboard as of 12/13/2020 9:38 PM EST 4"
https://github.com/srzn/CourseProject	progressreport.pdf	"Project Progress Report Srujan Netid:ssg7 November 29, 2020 1 Introduction to Project I chose to participate in the Information Retrieval competition. I proposed using a Learning-to-Rank methodology for IR as opposed to standalone rankers. I also proposed to first explore evaluating using combination of rankers rather than one ranking methodology to produce rankings. Finally, I proposed using SV Mmap, a supervised learning based ranking technique for IR. 2 Estimated steps in the project timeline 2.1 Step 1: Choosing metapy for a preliminary analysis Status: Completed On: 11/27/2020 I decided to use metapy to get an understanding for the data (CORD-19 set) and how classical rankers we used through the course behave with this enormous dataset. This means using the ranking template from MP2.4 to see how that implementation performs with the new dataset (includes train and test folders from here on) instead of cranfield data. 2.2 Step 2: Converting data into MeTA format Status: Completed On: 11/29/2020 Using metadata.csv and documents folder, the dataset needs to be created as a "".dat"" file with documents separated by a new line. Queries should be extracted from the given xml document and converted into a text file that metapy.index.IREval can read. Relevance judgement file must have ""doc id"" 1 as uint64 instead of the alphanumeric format given in the CORD-19 dataset. Finally, building an inverted index to be used by metapy rankers. Figure 1: Extracted queries from xml Figure 2: Extracted data in MeTA format 2.2.1 Step 3: Evaluating with known rankers Status: Pending Estimated Completion: 12/04/2020 After completing the data conversion, using classical and custom rankers 2 (BM25+, InL2 ranker etc.) to evaluate their performance with CORD-19 dataset. 2.2.2 Step 4: Evaluate with a combination of rankers Status: Pending Estimated Completion: 12/04/2020 Depending on the individual rankers' performance, choose a combination of top rankers based on the data at hand. Theoretically, this implementation changes the combination weights depending on the dataset. 2.3 Step 5: Convert data for SV M map compatibility Status: Pending Estimated Completion: 12/09/2020 2.4 Step 6: Evaluate using SV M map Status: Pending Estimated Completion: 12/09/2020 2.5 Step 7: Produce final results based on Steps 3, 4, and 6 Status: Pending Estimated Completion: 12/12/2020 3 Challenges Data format conversions are pretty challenging owing to lot of exception handling. The final results need to be in the competition format which means the data need to be converted back again. It's difficult to comprehend the embeddings data without going through SPECTER project implementation. So, the data will have to go unused if I rely entirely on introduction section for each paper. There are also a few missing data fields in the dataset collection which need to be handled. The implementation would still work if I ignore missing data but would be incomplete. 3"
https://github.com/srzn/CourseProject	Project Proposal.pdf	"Project Proposal for CS 410 October 25, 2020 1. Name, NetId and Captain *This is a one member team* Name: Sai Srujan Gudibandi NetId: ssg7 Captain: Sai Srujan Gudibandi 2. Competition selection I choose to participate in the Information Retrieval Competition 3. Implementation and ideas for the project I plan to use a supervised learning approach owing to the fact that IR is more amenable to empirical tuning over a completely black-box approach of constructing an unsupervised mathematical model i) From the experience of constructing an IR system for MP2.4, I have realized that Okapi-BM25 per- formed best for the cranfield dataset. While constructing different rankers, I have cycled through various models like Pivoted Length Normalization, INL2 ranking, Zhai's BM25+, Jelinek-Mercer, and Dirichlet smoothing models. After extensive and exhaustive tuning on each and every model, Okapi-BM25 emerged victorious over the others. I, however, didn't have the chance to implement learning-to-rank methods by combining more than one ranking models to obtain a superior perfor- mance. I would start with implementing one of the learning-to-rank methodologies described in the optional module of the course to pitch it against individual models to compare their performances. ii) In implementing learing-to-rank, I wish to choose a classification based learning to rank from among the different approaches. For this purpose, I'm currently exploring SVMmap. I may change my choice of LTRs and choose another model if I'm not satisfied with the performance of a previously chosen one. In addition to working on the datasets provided for the competition, I plan on using LETOR as a benchmark dataset for my model. iii) 4. Programming language option I wish to implement the majority of the project in Python. However, I may use C++ and/or R in the due course of the project if need be the details of which I will thoroughly document in the user guide. References Liu,""Learning to rank for Information Retrieval"", Foundations and Trends(r) in Information Retrieval, Vol. 3 No. 3 (2009) pp. 225-331, 2009 http://projects.yisongyue.com/svmmap/ https://www.microsoft.com/en-us/research/publication/letor-benchmark-collection-research-learning-rank- information-retrieval/ 1"
https://github.com/srzn/CourseProject	README.md	"CourseProject Project Proposal.pdf is the Project Proposal Document progressreport.pdf is Project Progress Report as of 11/29/2020 9:00 PM ET ==========For running the code================= All the software content is in the file Code.zip. The link to the same is https://drive.google.com/file/d/1Xep48h-O4VUR2WZna2LtugpVpYauIyWU/view?usp=sharing Due to GitHub restrictions, I was unable to upload the zip file directly here. Please use the link above to get the file. Thanks! When you uncompress Code.zip you'll have two folders - train and test. Please do not delete any file. Important files are illustrated below ---train--- queryextr.py uidmap.py convert-dat1.py search-eval.py ---test--- queryextr.py uidmap.py convert-dat1.py search_test.py predictgen.py ====Usage=== If you're skipping the training, run the following from ""test"" folder on your command prompt: python search_test.py config-test.toml 'Option' where 'Option' can take any integer value between 0 and 5. Please refer the documentation to know more about each choice. For example, if you chose Option 0 then you'll run python search_test.py config-test.toml 0 And then run, python predictgen.py The above code must be executed to obtain the final predictions in a text file. If you chose Option 0 you would obtain ""prediction0.txt"" as your final predictions for all the queries capping at 1000 top documents per query. If you want use the training phase: From ""train"" folder, run python search_eval.py config.toml 'Option' where 'Option' has the same range as described above. You'll obtain a text file ""Option0.txt"" (if you chose Option 0). Please copy this file over to the ""test"" folder and repeat the instructions above to obtain final predictions. ---End"
https://github.com/armhoeft/CS410Fall2020-CourseProject	README.md	"CourseProject Enhance MeTA and Metapy Usability: Python 3.9 For required project submissions, see project-artifacts. Final Submission Details The first body of interesting code lies in an adjacent repo (https://github.com/armhoeft/metapy-container) which I built strictly for this project to separate my deliverables from an artifact that can live on beyond the scope of this course. The second body of interesting code lies in the sample-assignment directory, where I've provided an example of how to use this container for course assignments. The final (and perhaps most important) is artifact is a functioning Docker container (armhoeft/metapy-container) that can be found here (https://hub.docker.com/r/armhoeft/metapy-container) or by executing the following command. This container contains a Python 3.9 runtime environment and a compiled version of MeTA and metapy. {bash} docker pull armhoeft/metapy-container:0.2.13 Recommendations for a Reviewer Watch the video: https://drive.google.com/file/d/19wAVeWPICQYA1XFTOhIhv-6bQbbniu69/view?usp=sharing Look around this repo and: https://github.com/armhoeft/metapy-container. Read: https://github.com/armhoeft/metapy-container/blob/main/README.md Read: https://github.com/armhoeft/CS410Fall2020-CourseProject/blob/main/sample-assignment/README.md Download: https://hub.docker.com/r/armhoeft/metapy-container Have fun and be generous in your grading/feedback! Closing Thoughts After many hours of fighting with MeTA and metapy, I realized that literally building the project on Python 3.9 wasn't especially remarkable. You struggle through a series of errors (most of which were the product of dead links or the repository falling into relative disrepair as the software world keeps chugging along), search the web for ways to resolve them, and try building again until you hit the end. This realization caused me to revector my efforts after succeeding in my initial task and incorporate some of my ""stretch goals"": namely, building a Docker container and making this class-ready. In my view, the biggest outcomes of this assignment are: A better working knowledge of how MeTA and metapy are constructed. A documented process for using Docker to make standardized build and execution environments for all students (which should help with those who have difficulty getting environment consistency between their local setup and the autograder). A published Docker container on DockerHub! A path to replacing the typical autograders with Kubernetes (since k8s likes containers)! I don't pretend to know how our autograders work, but I do know from personal and professional experience that elastically scaling Kubernetes pods are the future and could lead to substantial cost savings if not already implemented by the school."
https://github.com/chanwoo321/CourseProject	Final Project Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. Which paper have you chosen? 3. Which programming language do you plan to use? 4. Can you obtain the datasets used in the paper for evaluation? 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? The group captain of this project is Jonathan Kim (jck3) and the other two members of the group are Michael Xiang (mx9) and Tyler Ruckstaetter (thr2). The paper we have chosen is "" A cross-collection mixture model for comparative text mining."" The language that we will be using for this project is python. Since the paper is from 2004, it seems like the exact datasets used in the paper will be unavailable for us. However, a similar dataset will be obtainable since the nature of the data is very publicly available. As of right now, we can still use reviews of laptops as used in the paper as well as more modern laptops. War news is available on many news sites, so while the exact documents from the paper may not be directly used, they will be very similar in nature."
https://github.com/chanwoo321/CourseProject	Progress Report.pdf	Progress Report So far, we have researched the sources as mentioned in the paper. In order to keep this project as similar to the paper as possible, articles were pulled from the websites mentioned in the paper. This includes war news from CNN and BBC sites online and getting reviews from epinions.com. Since epinions no longer exists as a domain, an archive of the internet was used in order to extract data from the approximate time period the paper was published. This led to a couple of complications: it was unclear in the paper what articles/reviews in particular were used despite mentioning the website they were taken from. While internet archives are available to extract data from that period of time, the archives are not comprehensive. Thus, we were not able to get all of the reviews mentioned in the paper (though we believe a sufficient number of them have been obtained to still run the model in a meaningful manner). Additionally, since the specific articles/reviews were not released in the paper, it is unclear whether our extracted data matches the data used for the model in the paper. Regardless, we expect the content to be very similar, so this should not affect the results. The only task that is left to be done is to implement the model itself. This involves two parts: creating the naive mixture model and creating the mixture model described in the paper. This will likely be in a format similar to that of MP3, so it should be a doable amount of time.
https://github.com/chanwoo321/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Demo Video Note that the video was not recorded with the final edits to the repository. The actual results may differ/be more accurate. Table of Contents Introduction Brief Overview Obtaining and Organizing the Data Running the Software Code Information Baseline Model Cross-Collection model Introduction Hello! Our group consists of three people: Jonathan Kim, Michael Xiang, and Tyler Ruckstaetter. This repository was created for the final project for the class CS 410: Text Information Systems at the University of Illinois at Urbana-Champaign. The purpose of this project was to reproduce the Comparatrive Text Mining model described in the paper ""A Cross-Collection Mixture Model for Comparative Text Mining"", which can be found here. Brief Overview The model in the paper was created in order to solve the novel text mining problem of ""Comaparative Text Mining"". This problem consists of trying to find common themes across some collections of texts and to summarize the similarities and differences between these collections. Thus, a generative probabilistic mixture model was proposed. It performs cross-collection clustering and within-collection clustering. This is done to find themes across collections and utilize the fact that each collection may have information on a similar topic to the other collections as opposed to a completely different one. The data used in the paper and the model implemented in this repository were laptop reviews and war news. In particular, the laptop reviews analyzed the Apple iBook Mac Notebook, the Dell Inspiron 8200, and the IBM ThinkPad T20 2647. The war news covered the Iraq war and the Afghanistan war. To verify the validity of this model, a simple baseline mixture model was also implemented that takes the data of all of the laptops or all of the wars and tries to cluster documents without utiliing the differences in different collections. This cross-collection mixture model works notably better than the baseline. Obtaining and Organizing the Data In order to keep the study as close as possible to the paper, the reviews available in this repository were collected based on the description of the paper. The war news was collected from articles from BBC and CNN for one year starting from November 2001. On the other hand, the laptop reviews were pulled from epinions.com. However, epinions.com is no longer available at the time this project was completed. In order to maintain as accurate of a reproduction of this model as possible, a internet archiving website was used to see what was available on epinions.com in 2001. The data is organized inside of the ""data"" folder of this repository. Each text file in the data folder represent data collected for each individual laptop and war. Inside the data folder, there is another folder called ""combined"". This folder contains two files: laptops.txt and wars.txt. Laptops.txt contain all of the laptop reviews in one file and wars.txt contain all of the war articles in one file. This was done so that the baseline model could access all of the needed reviews or articles as necessary. Running the software This software uses Python3 and uses numpy and pandas. To run the baseline mixture model, ensure that numpy, pandas, and python are properly installed (pip install numpy/pip install pandas) and run the following code in the terminal: python main.py This will run the baseline model on data/combined/laptops.txt by default. If another data set should be analyzed, provide the file location as a parameter. For example, if analysis wants to be run on war models, run the following: python main.py 200 ./data/combined/wars.txt The format being: python main.py [iterations] [dataset] To run the cross-collection mixture model, run the following code in the terminal: python cross_collection_model.py The above will run the Cross Collection model on the default dataset of the laptop reviews with the collections being mac.txt, inspiron.txt, and thinkpad.txt. If you want to run it with another set of data, you will want to run the following command: python cross_collection_model.py 10 ./data/combined/laptops.txt ./data/inspiron.txt ./data/mac.txt ./data/thinkpad.txt The format being: python cross_collection_model.py [iterations] [full dataset] [collections files separated by spaces] Note that running either model may take a long amount of time because of how much data the model is processing. We noticed that you could see pretty realistic word-probability assignments in as little as 5 iterations of testing. Code Information Baseline model The code from the baseline model is primarily based on the PLSA algorithm as used in MP3 of CS 410. This model was estimated using the EM (Estimation-Maximization) algorithm. Here is a quick overview of the functions provided: normalize(input_matrix) Normalizes the rows of a 2d input_matrix so they sum to 1. class NaiveModel(object) Class that actually runs the baseline mixture model. Includes the following methods: build_corpus(self) Fills in self.documents with a list of list of words by reading from the document path build_vocabulary(self) Constructs a list of unique works in the whole corpus and updates self.vocabulary build_term_doc_matrix(self) Constructs a term document matrix where each row represents a document, and each column represents a vocabulary term. initialize_randomly(self, number_of_topics) Randomly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize_uniformly(self, number_of_topics) Uniformly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize(self, number_of_topics, random=False) Sets up the matrices of the model using initalize_randomly or initialize_uniformly. expectation_step(self) Runs the expectation_step as part of the EM algorithm. maximization_step(self, number_of_topics) Runs the maximization_step as part of the EM algorithm. calculate_likelihood(self, number_of_topics) Calculates the log-likelihood of the model using the model's updated probability matrices. Used to determine when the EM algorithm is complete/converged. naivemodel(self, number_of_topics, max_iter, epsilon) Runs the model in its entirety on self.document_path and the provided parameters. show_top_10(matrix, model) Displays the top 10 probabilities of a topic-word-probability matrix given a model's vocabulary main(documents_path) This is the default function used when running from the terminal. Runs the model with default parameters. Cross Collection mixture model The code from the Cross Collection model is also primarily based on the PLSA algorithm as used in MP3 of CS 410 with modifications according to the paper's given formulae. This model was estimated using the EM (Estimation-Maximization) algorithm. Here is a quick overview of the functions provided (functions shared with baseline are omitted): class CCModel(object) Class that actually runs the Cross Collection mixture model. Includes the following methods: build_corpus(self) Fills in self.documents with a list of list of words by reading from the document path build_vocabulary(self) Constructs a list of unique works in the whole corpus and updates self.vocabulary build_term_doc_matrix(self) Constructs a term document matrix where each row represents a document, and each column represents a vocabulary term. initialize_randomly(self, number_of_topics) Randomly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize_uniformly(self, number_of_topics) Uniformly sets the normalized matrices self.document_topic_prob, self.topic_word_prob, and self.background_prob. initialize(self, number_of_topics, random=False) Sets up the matrices of the model using initalize_randomly or initialize_uniformly. expectation_step(self) Runs the expectation_step as part of the EM algorithm. maximization_step(self, number_of_topics) Runs the maximization_step as part of the EM algorithm. calculate_likelihood(self, number_of_topics) Calculates the log-likelihood of the model using the model's updated probability matrices. Used to determine when the EM algorithm is complete/converged. naivemodel(self, number_of_topics, max_iter, epsilon) Runs the model in its entirety on self.document_path and the provided parameters. show_top_10(matrix, model) Displays the top 10 probabilities of a topic-word-probability matrix given a model's vocabulary main(documents_path) This is the default function used when running from the terminal. Runs the model with default parameters."
https://github.com/chanwoo321/CourseProject	Research Paper.pdf	"A Cross-Collection Mixture Model for Comparative Text Mining ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Atulya Velivelli Department of Electrical and Computer Engineering University of Illinois at Urbana Champaign Bei Yu Graduate School of Library and Information Science University of Illinois at Urbana Champaign ABSTRACT In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent com- mon themes across all collections as well as summarize the similarity and differences of these collections along each com- mon theme. This general problem subsumes many interest- ing applications, including business intelligence and opinion summarization. We propose a generative probabilistic mix- ture model for comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algo- rithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. Categories and Subject Descriptors: H.3.3 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Comparative text mining, mixture models, clus- tering 1. INTRODUCTION Text mining is concerned with extracting knowledge and patterns from text [5, 6]. While there has been much re- search in text mining, most existing research is focused on one single collection of text. The goals are often to extract basic semantic units such as named entities, to extract rela- tions between information units, or to extract topic themes. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'04, August 22-25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00. In this paper, we study a novel problem of text mining re- ferred to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differ- ences of these collections along each common theme. Specif- ically, the task involves: (1) discovering the different com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. The need for comparative text mining exists in many different applica- tions, including business intelligence, summarizing reviews of similar products, and comparing different opinions about a common topic in general. In this paper, we study the CTM problem and propose a generative probabilistic mixture model for CTM. The model simultaneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The mixture model is based on component multinomial distribution models, each characterizing a different theme. The common themes and collection-specific themes are explicitly modeled. The pro- posed model can be estimated efficiently using the Expectation- Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. The rest of the paper is organized as follows. In Section 2, we briefly introduce the problem of CTM. We then present a baseline simple mixture model and a new cross-collection mixture model in Section 3 and Section 4. We discuss the experiment results in Section 5. 2. COMPARATIVE TEXT MINING 2.1 A motivating example With the popularity of e-commerce, online customer eval- uations are becoming widely provided by online stores and third-party websites. Pioneers like amazon.com and epin- ions.com have accumulated large amounts of customer input including reviews, comments, recommendations and advice, etc. For example, the number of reviews in epinions.com is more than one million[4]. Given a product, there could be up to hundreds of reviews, which is impossible for the readers to go through. It is thus desirable to summarize a collection of reviews for a certain type of products in order to provide the readers the most salient feedbacks from the peers. For review summarization, the most important task is to identify different semantic aspects of a product that the reviewers mentioned and to group the opinions accord- ing to these aspects to show similarities and differences in the opinions. For example, suppose we have reviews of three different brands of laptops (Dell, IBM, and Apple), and we want to summarize the reviews. A useful summary would be a tab- ular representation of the opinions as shown in Table 1, in which each row represents one aspect (subtopic) and differ- ent columns correspond to different opinions. Table 1: A tabular summary Subtopics Dell IBM Apple Battery life long enough short short Memory good bad good Speed slow fast fast It is, of course, very difficult, if not impossible to pro- duce such a table completely automatically. However, we can achieve a less ambitious goal - identifying the semantic aspects and identifying the common and specific character- istics of each product in an unsupervised way. This is a concrete example of comparative text mining. 2.2 The general problem The example above is only one of the many possible appli- cations of comparative text mining. In general, the task of comparative text mining involves: (1) discovering the com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. It is very hard to precisely define what a theme is, but it corresponds roughly to a topic or subtopic. The granularity of themes is application-specific. CTM is a fundamental task in ex- ploratory text analysis. In addition to opinion comparison and summarization, it has many other applications, such as business intelligence (comparing different companies), cus- tomer relationship management (comparing different groups of customers), and semantic integration of text (comparing component text collections). CTM is challenging in several ways: (1) It is a completely unsupervised learning task; no training data is available. (It is for the same reason that CTM can be very useful for many different purposes - it makes minimum assumptions about the collections and in principle we can compare any arbitrary partition of text.) (2) We need to identify themes across different collections, which is more challenging than identifying topic themes in one single collection. (3) The task involves a discrimination component - for each discov- ered theme, we also want to identify the unique information specific to each collection. Such a discrimination task is dif- ficult given that we do not have training data. In a way, CTM goes beyond the regular one-collection text mining by requiring an ""alignment"" of multiple collections based on common themes. Since no training data is available, in general, we must rely on unsupervised learning methods, such as clustering, to perform CTM. In this paper, we study how to use prob- abilistic mixture models to perform CTM. Below we first describe a simple mixture model for clustering, which repre- sents a straightforward application of an existing text min- ing method, and then present a more sophisticated mixture model specifically designed for CTM. 3. CLUSTERING WITH A SIMPLE MIXTURE MODEL   th  th  th "" $ % th ' th Figure 1: The Simple Mixture Model A naive solution to CTM is to treat the multiple collec- tions as one single collection and perform clustering. Our hope is that some clusters would represent the common themes across the collections, while some others would rep- resent themes specific to one collection (see Figure 1). We now present a simple multinomial mixture model for clus- tering an arbitrary collection of documents, in which we assume there are k latent common themes in all collections, and each is characterized by a multinomial word distribu- tion (also called a unigram language model). A document is regarded as a sample of a mixture model with these theme models as components. We fit such a mixture model to the union of all the text collections we have, and the obtained component multinomial models can be used to analyze the common themes and differences among the collections. Formally, let C = {C1, C2, ..., Cm} be m comparable col- lections of documents. Let th1, ..., thk be k theme unigram language models and thB be the background model for all the collections. A document d is regarded as a sample of the following mixture model (based on word generation). pd(w) = lBp(w|thB) + (1 - lB) k j=1 [pd,jp(w|thj)] where w is a word, pd,j is a document-specific mixing weight for the j-th aspect theme, and k j=1 pd,j = 1. lB is the mix- ing weight of the background model thB. The log-likelihood of all the collections C is log p(C|L) = m i=1 dCi wV [c(w, d) x log(lBp(w|thB) + (1 - lB) k j=1 (pd,jp(w|thj)))] where V is the set of all the words (i.e., vocabulary), c(w, d) is the count of word w in document d, and L = ({thj, pd,j}k j=1 is the set of all the theme model parameters. The purpose of using a background model is to ""force"" clustering to be done based on more discriminative words, leading to more informative and more discriminative component models. We control this effect through thB. The model can be estimated using any estimator. For example, the Expectation-Maximization (EM) algorithm [3] can be used to compute a maximum likelihood estimate with the following updating formulas: p(zd,w = j) = p(n) d,j p(n)(w|thj) k j'=1 p(n) d,j'p(n)(w|thj') p(zd,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j p(n)(w|thj) p(n+1) d,j = wV c(w, d)p(zd,w = j) j' wV c(w, d)p(zd,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,w = B))p(zd,w = j) w'V m i=1 dCi c(w', d)(1 - p(zd,w' = B))p(zd,w' = j) This mixture model is closely related to the probabilis- tic latent semantic indexing model (PLSI) proposed in [7] and treats CTM as a single-collection text mining problem. However, such a simple model is inadequate for CTM for two reasons: (1) We have completely ignored the structure of collections. As a result, we may have clusters that repre- sent only some, not all of the collections. (2) There is no easy way to identify which theme cluster represents the common information across collections and which represents specific information to a particular collection. Below we present a more sophisticated coordinated mixture model, which is specifically designed for CTM and addresses these two defi- ciencies. 4. CLUSTERING WITH A CROSS- COLLECTION MIXTURE MODEL  th th    th     th    th    th    th    th    th Figure 2: The Cross-Collection Mixture Model 4.1 The model Our main idea for improving the simple mixture model for comparative text mining is to explicitly distinguish com- mon theme clusters that characterize common information across all collections from special theme clusters that char- acterize collection-specific information. Thus we now con- sider k latent common themes as well as a potentially dif- ferent set of k collection-specific themes for each collection (illustrated in Figure 2). These component models directly correspond to all the information we are interested in discov- ering. The sampling distribution of a word in document d (from collection Ci) is now collection-specific. Specifically, it involves the background model (thB), k common theme models (th1, ..., thk), and k collection-specific theme models (th1,i, ..., thk,i), which are to capture the unique information about the k themes in collection Ci. That is, pd(w|Ci) = (1 - lB) k j=1 [pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))] +lBp(w|thB) where lB is the weight on the background model thB and lC is the weight on the common theme model thj (as opposed to the collection-specific theme model thj,i). Intuitively, when we ""generate"" a word, we first decide whether to use the background model thB according to lB; the larger lB is, the more likely we will use thB. If we decide not to use thB, then we need to decide which theme to use; this is controlled by pd,j, the probability of using theme j when generating words in d. Finally, once we decide which theme to use, we still need to decide whether we should use the common theme model or the collection-specific theme model, and this is con- trolled by lC, the probability of using the common model. The weighting parameters lB and lC are intentionally to be set by the user, and their interpretation is as follows. lB reflects our knowledge about how noisy the collections are. If we believe the text is verbose, then lB should be set to a larger value. In our experiments, a value of 0.9 - 0.95 often works well. lC indicates our emphasis on the commonality, as opposed to the speciality in comparative text mining. A larger lC would allow us to learn a richer common theme model, whereas a smaller one would learn a weaker com- mon theme model, but stronger special models. The optimal value depends on the specific applications. According to this generative model, the log-likelihood of the whole set of collections is log p(C) = m i=1 dCi wV [c(w, d) log[lBp(w|thB) +(1 - lB) k j=1 pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))]] 4.2 Parameter estimation We estimate the background model thB using all the avail- able text in the m text collections. That is, ^p(w|thB) = m i=1 dCi c(w, d) m i=1 dCi w'V c(w', d) Since lB and lC are set manually, this leaves us with the following parameters to estimate: (1) the common theme models, th = {th1, ..., thk}; (2) the special theme models for each collection Ci, thCi = {th1,i, ..., thk,i}; and (3) the theme mixing weights for each document d: pd = {pd,1, ..., pd,k}. p(zd,Ci,w = j) = p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) k j'=1 p(n) d,j'(lCp(n)(w|thj') + (1 - lC)p(n)(w|thj',i)) p(zd,Ci,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) p(zd,Ci,j,w = C) = lCp(n)(w|thj) lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i) p(n+1) d,j = wV c(w, d)p(zd,Ci,w = j) j' wV c(w, d)p(zd,Ci,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)p(zd,Ci,j,w = C) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)p(zd,Ci,j,w' = C) p(n+1)(w|thj,i) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)(1 - p(zd,Ci,j,w = C)) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)(1 - p(zd,Ci,j,w' = C)) Figure 3: EM updating formulas for the cross-collection mixture model As in the simple mixture model, we can also use the EM algorithm to compute a maximum likelihood estimate. The updating formulas are shown in Figure 3. Each EM iteration involves scanning all the text once, so the algorithm is quite scalable. 4.3 Using the model Once the model is estimated, we will have k collection- specific models for each of the m collections and k common theme models across all collections. Each of these mod- els is a word distribution or unigram language model. The high probability words can characterize the theme/cluster extracted. Such words can often be used directly as a sum- mary or indirectly (e.g., through a hidden Markov model) to extract relevant sentences to form a summary of the cor- responding theme. The extracted word distributions can also be used in many other ways, e.g., to classify other text documents or to link the related passages in the text collec- tions so that a user can navigate the information space for comparative analysis. We can input our bias for CTM through setting lB and lC manually. Specifically, lB allows us to input our knowledge about the noise (stop words) in the data - if we know the text data is verbose, then we should set lB to a high value, whereas if the data is concise and mostly content-bearing keywords, then we need to set lB to a smaller value. Sim- ilarly, lC allows us to input a trade-off between extracting common theme models (setting lC to a higher value) vs. ex- tracting collection-specific models (setting lC to a smaller value). Such biases cannot be learned by the maximum like- lihood estimator. Indeed, maximizing the data likelihood is only a means to achieve our ultimate goal, which is why we want to regularize our model in a meaningful way so that we can impose certain preferences while maximizing the data likelihood. The flexibility and control provided by lB and lC make it possible for a user to control the focus of the results of comparative text mining. 5. EXPERIMENTS AND RESULT ANALYSIS We evaluated the Simple Mixture model (SimpMix) and the Cross-Collection Mixture model (CCMix) on two do- mains - war news and laptop reviews. 5.1 War news The War news data consists of news excerpts on two com- parable events: (1) Iraq war and (2) Afghanistan war, both of which occurred in the last two years. The Iraq war news excerpts were a combination of 30 articles from the CNN and BBC web sites over the last one year span. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov. 2001. Our goal is to compare these two wars and find out their common and specific characteristics. The results of using either the simple mixture model or the cross-collection mixture model are shown in Table 2, where the top words of each theme model are listed along with their probabilities. We set lB = 0.95 for SimpMix and set lb = 0.9, lC = 0.25 for CCMix; in both cases, the number of clusters is fixed to 5. Variations of these parameters are discussed later. We see that although there are some interesting themes in the results of SimpMix (e.g., cluster3 and cluster4 appear to be about American and British inquiry into the pres- ence of weapons in Iraq, respectively, while cluster2 suggests the presence of British soldier in Basra, a town in southern Iraq), they are all about Iraq war. We do not see any obvi- ous theme common to both Iraq war and Afghanistan war. This is expected given that SimpMix pools all documents together without exploiting the collection structure. In contrast, the results of CCMix explicitly suggest the common themes and the corresponding collection-specific themes. For example, cluster3 clearly suggests that in both wars, there has been loss of lives. Furthermore, the top words in the corresponding Iraq theme include names of some key defense people that are involved in the Iraq war (e.g., ""Hoon"" is the last name of the british defense secre- tary and ""Sanchez"" is the last name of the U.S General in Iraq). In comparison, the top words in the corresponding Afghanistan theme includes the name of the U.S Defense secretary who had an important role in the Afghan war. Cluster4 and cluster5 are also meaningful themes. The common theme captured in Cluster4 is the Monday briefings by an official spokesman of a political administration during both wars; the corresponding special themes indicate the dif- ference in the topics discussed in the briefings (e.g., weapon inquiry for Iraq war and Bin Laden for Afghanistan war). The common theme of Cluster5 is about the diplomatic role Table 2: War news results using SimpMix model (top) vs. CCMix model (bottom) Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common will 0.019 british 0.017 weapons 0.022 inquiry 0.052 countries 0.026 theme let 0.012 soldiers 0.015 kay 0.021 intelligence 0.036 contracts 0.023 words united 0.012 baghdad 0.015 rumsfeld 0.017 dossier 0.024 allawi 0.012 god 0.011 air 0.011 commission 0.014 hutton 0.021 hoon 0.012 inspectors 0.011 basra 0.011 group 0.014 claim 0.019 russian 0.010 your 0.010 mosque 0.010 senate 0.011 wmd 0.019 international 0.010 nation 0.010 southern 0.01 survey 0.010 mps 0.018 russia 0.009 n 0.010 fired 0.010 paper 0.010 committee 0.017 reconstruction 0.009 Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common us 0.042 mr 0.029 killed 0.036 monday 0.036 united 0.042 theme nation 0.030 marines 0.025 month 0.032 official 0.032 nations 0.04 words will 0.024 dead 0.023 deaths 0.023 i 0.029 with 0.03 action 0.022 general 0.022 one 0.023 would 0.028 is 0.025 re 0.022 defense 0.019 died 0.022 where 0.025 it 0.024 border 0.019 key 0.018 been 0.022 do 0.025 they 0.023 its 0.017 since 0.018 drive 0.018 spokesman 0.022 diplomatic 0.023 ve 0.016 first 0.016 according 0.015 political 0.021 blair 0.022 Iraq god 0.022 iraq 0.022 troops 0.016 intelligence 0.049 n 0.03 theme saddam 0.016 us 0.021 hoon 0.015 weapons 0.034 weapons 0.024 words baghdad 0.013 baghdad 0.017 sanchez 0.012 inquiry 0.028 inspectors 0.023 your 0.012 nato 0.015 billion 0.01 commission 0.017 council 0.016 live 0.01 iraqi 0.013 spokeswoman 0.008 independent 0.016 declaration 0.015 Afghan paper 0.021 story 0.028 taleban 0.026 bin 0.031 northern 0.040 theme afghan 0.019 full 0.026 rumsfeld 0.020 laden 0.031 alliance 0.040 words meeting 0.014 saturday 0.016 hotel 0.012 steinberg 0.027 kabul 0.030 euro 0.012 e 0.015 front 0.011 taliban 0.023 taleban 0.025 highway 0.012 rabbani 0.012 dropped 0.010 chat 0.019 aid 0.020 played by the United Nations (UN). The corresponding spe- cial themes again suggest the difference between the two wars. The Iraq theme indicates the role of UN in sending weapon inspectors to Iraq; the Afghanistan theme refers to Northern Alliance that received aid from the UN and came to power in Afghanistan after the defeat of Taliban. 5.2 Laptop customer reviews This data set was constructed to test our models for com- paring opinions of customers on different laptops. We man- ually downloaded the following 3 review sets from epin- ions.com [4], filtering out the misplaced ones: Apple iBook (M8598LL/A) Mac Notebook (34 reviews), Dell Inspiron 8200 (8TWORH) PC Notebook (22 reviews), IBM ThinkPad T20 2647 (264744U) PC Notebook (42 reviews). The results on this data set are generally similar to those on war news. Due to the limit of space, we only show the CCMix results in Table 3, which are obtained by setting lC=.7 and lB=.96 and fixing the number of clusters to 8. Here we again see many very interesting common themes; in- deed, the top two words in the common themes can provide a very good summary of the themes (e.g., ""sound and speak- ers"" for cluster1, ""battery hours"" for cluster5, and ""Mi- crosoft Office"" for cluster8). However, the special themes, although suggesting some differences among the three lap- tops, are much harder to interpret. This may be because there is a great deal of variation in product-specific opin- ions in the data, which makes the data extremely sparse for learning a coherent collection-specific theme for each of the eight themes. 5.3 Parameter tuning When we vary lB and lC in CCMix, the results are gen- erally different. Specifically, when lB is set to a small value, non-informative stop words tend to show up in common themes. A reasonable value for lB is generally higher than 0.9 - in that case, the model automatically eliminates the non-informative words from the theme clusters, allowing for more discriminative clustering. Indeed, in all our experi- ments, we have intentionally retained all the stop words, and the model is clearly able to filter out non-informative words, though in some cases, they still show up as top words in the common themes of the news data. They can be ""eliminated"" by using an even higher lB, but then we may end up having insufficient information to learn a common theme reliably. lC affects the vocabulary allocation between the common and collection-specific themes. In the news data experiments, when we change lC to a value above 0.4, the collection-specific terms would dominate the common theme models. In the laptop data experiments, when lC is less than 0.7, we lose many content keywords of the com- mon themes to the corresponding collection-specific themes. Both lB and lC are intentionally left for a user to tune so that we can incorporate application-specific bias into the model. 6. RELATED WORK The most related work to our work is the coupled clus- tering method presented in [8], which appears to be one of the very few studies considering the clustering problem in multiple collections. They extend the information bottle- neck approach to discover common clusters across different collections. Comparative text mining goes beyond this by analyzing both the similarities and collection-specific differ- ences. We also use a completely different approach based on probabilistic mixture models. Another related work is [10], where cross-training is used for learning classifiers from mul- tiple document sets. Our work differs from it in that we per- form unsupervised learning. The aspect models studied in [7, 2] are also related to our work but they are closer to our baseline model and are not designed for comparing multiple collections. There are many studies in document clustering [1]. Again, the difference lies in that they consider only one collection and thus are similar to the baseline model. Our work is also related to document summarization, es- pecially multiple document summarization (e.g.,[9, 12]). In- deed, we can the results of CTM as a special form of sum- mary of multiple text collections. However, an important difference is that while a summary intends to retain the ex- plicit information in text (to maintain fidelity), CTM aims at extracting non-obvious implicit patterns. 7. CONCLUSIONS AND FUTURE WORK In this paper, we define and study a novel text mining problem referred to as comparative text mining. It is con- Table 3: Laptop review results using CCMix model Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Cluster6 Cluster7 Cluster8 C sound 0.035 port 0.023 ram 0.105 m 0.027 battery 0.129 t 0.039 cd 0.095 office 0.037 O speakers 0.035 jack 0.021 mb 0.037 trackpad 0.018 hours 0.080 modem 0.017 drive 0.076 microsoft 0.021 M playback 0.034 ports 0.018 memory 0.034 chip 0.013 life 0.060 internet 0.017 rw 0.055 little 0.018 M feel 0.019 will 0.018 256mb 0.027 improved 0.012 5 0.038 later 0.014 dvd 0.049 basic 0.015 O pros 0.017 your 0.017 128mb 0.021 volume 0.012 end 0.016 configuration 0.014 combo 0.025 6 0.014 N cons 0.017 warm 0.013 tech 0.020 did 0.011 3 0.016 free 0.013 drives 0.023 under 0.013 market 0.017 keep 0.012 128 0.020 latch 0.011 high 0.015 vga 0.012 rom 0.020 mhz 0.012 size 0.014 down 0.012 support 0.018 make 0.010 processor 0.014 were 0.012 floppy 0.017 word 0.011 D rests 0.026 banias 0.019 options 0.039 inspiron 0.061 dells 0.032 fans 0.019 apoint 0.017 0 0.046 E palm 0.022 svga 0.014 sodimm 0.025 pentium 0.052 ran 0.017 shipping 0.017 blah 0.015 angle 0.018 L 9000 0.020 record 0.014 eraser 0.021 8200 0.03 prong 0.015 2nd 0.016 hook 0.011 portion 0.0154 L smart 0.018 supposedly 0.013 crucial 0.018 toshiba 0.027 requiring 0.014 tracking 0.015 tug 0.011 usb 0.0153 reader 0.018 rebate 0.013 sdram 0.018 440 0.026 second 0.011 spoke 0.015 2499 0.011 specials 0.014 A magazine 0.011 osx 0.040 macos 0.019 macos0.016 g4 0.016 iphoto 0.031 airport 0.075 appleworks 0.060 P ipod 0.010 quartz 0.015 personal 0.018 netscape 0.013 interlaced 0.016 itunes 0.027 burn 0.035 word 0.021 P strong 0.01 instance 0.014 shield 0.016 apache 0.009 mac 0.016 import 0.021 4x 0.018 result 0.016 L icon 0.009 underneath 0.012 airport 0.016 ie5 0.008 imac 0.014 book 0.018 reads 0.014 spreadsheet 0.013 E choppy 0.008 cooling 0.012 installation 0.015 ll 0.008 powermac 0.012 quicktime 0.016 schools 0.013 excel 0.012 I technology 0.023 rj 0.033 exchange 0.023 company 0.021 thinkpad 0.077 thinkpads 0.020 t20 0.04 list 0.015 B outdated 0.020 chik 0.018 hassle 0.016 570 0.017 ibm 0.047 connector 0.018 ultrabay 0.030 factor 0.013 M surprisingly 0.018 dsl 0.017 disc 0.015 turn 0.017 covers 0.029 connectors 0.018 tells 0.021 months 0.013 trackpoint 0.014 45 0.015 t23 0.012 buttons 0.015 lightest 0.028 bluetoot 0.018 device 0.021 cap 0.013 recommend 0.013 pacbell 0.012 cdrw 0.015 numlock 0.012 3000 0.027 sturdy 0.011 number 0.020 helpdesk 0.0128 cerned with discovering any latent common themes across a set of comparable collections of text as well as summariz- ing the similarities and differences of these collections along each theme. We propose a generative cross-collection mixture model for performing comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. We define the model and present the EM algorithm that can estimate the model ef- ficiently. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method based on a simple mixture model. Experiment results show that the cross-collection mixture model is quite effective in dis- covering the latent common themes across collections and performs significantly better than the baseline simple mix- ture model. The proposed model has many obvious applica- tions in opinion summarization and business intelligence. It also has many other less obvious applications in the general area of text mining and semantic integration of text. For example, our model can be used to compare the course web pages from the major computer science department web sites to discover core computer science topics. It can also be used to compare literature collections in different communities to support concept switching [11]. The work reported in this paper is just an initial step toward a promising new direction. There are many interest- ing future research directions. First, it may be interesting to explore how we can further improve the CCMix model and its estimation. One interesting direction is to explore the Maximum A Posterior (MAP) estimator, which would allow us to incorporate more prior knowledge in a princi- pled way. For example, a user may already have certain thematic aspects in mind. With MAP estimation, we can easily add that bias to the component models. Second, we can generalize our model to model semi-structured data to perform more general comparative data mining. One way to achieve this goal is to introduce additional random variables in each component model so that we can model any struc- tured data. Finally, it would be very interesting to explore how we could exploit the learned theme models to provide additional help to a user who wants to perform comparative analysis. For example, the learned common theme models can be used to construct a hidden Markov model (HMM) to identify the parts in the text collections about the common themes, and to connect them through automatically gener- ated hyperlinks. This would allow a user to easily navigate through the common themes. 8. REFERENCES [1] D. Baker and A. McCallum. Distributional clustering of words for text classification. In Proceedings of ACM SIGIR 1998, 1998. [2] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statist. Soc. B, 39:1-38, 1977. [4] epinions.com, 2003. http://www.epinions.com/. [5] R. Feldman and I. Dagan. Knowledge discovery in textual databases. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, 1995. [6] M. A. Hearst. Untangling text data mining. In Proceedings of ACL'99, 1999. [7] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR'99, pages 50-57, 1999. [8] Z. Marx, I. Dagan, J. Buhmann, and E. Shamir. Coupled clustering: a method for detecting structural correspondence. Journal of Machine Learning Research, 3:747-780, 2002. [9] K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. E. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI-99. [10] S. Sarawagi, S. Chakrabarti, and S. Godbole. Cross-training: Learning probabilistic mappings between topics. In Proceedings of ACM SIGKDD 2003. [11] B. R. Schatz. The interspace: Concept navigation across distributed communities. Computer, 35(1):54-62, 2002. [12] H. Zha. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of ACM SIGIR 2002."
https://github.com/SphtKr/MeTAPyquod	MeTAPyquod_Progress_20201127.pdf	Matt Williamson -- mdw8 (lead/captain) CS410 November 27, 2020 MeTAPyquod Progress Report As of the end of November, the metapyquod-dev container image is essentially complete and working--at least for x86_64 platforms. Work on metapyquod-server is in its early stages. metapyquod-dev Initial creation of the container definition went smoothly and operates as expected on macOS--in fact, a surprising result was observed that running a working implementation of MP2.4 natively on a macOS 11 host and then in the metapyquod-dev container resulted in a consistently faster execution time in the container than natively: Two adjustments from the initial proposal are likely necessary for the metapyquod-dev portion of the project: 1. A desired goal was to include a GUI IDE for Python development in the container-- however, this was seen as high-complexity and added risk, especially on non-Linux platforms. However, in the process of researching options the web-pdb Python module was discovered, METAPYQUOD - PROGRESS REPORT 1 which provides a full debugging environment within a web browser. The need for a robust debugger was the primary motivator for providing a GUI IDE, and this module provides the visual debugging capability with far lower complexity. Therefore, the final solution will map the host working directory into the container to allow any editor (PyCharm, VSCode, Spyder, etc.) to work directly with the files on the host, but provide debugging via web-pdb. 2. Another desired goal was to support armv7l/armhf and armv8/aarch64 systems, mainly because of the popularity of Raspberry Pi and similar SBC hardware. However, attempts to do so have been beset with build problems, despite the fact that the base official Python 3.6 image being used supports those architectures. First, scipy/numpy and related dependencies are not well-supported on these systems, though this may be surmountable-- and if so, the benefit may be significant because the time required to build these dependencies on these platforms is a large investment, and a prebuilt container image would eliminate this cost. Second, and more difficult, the MeTA build system itself fails on aarch64 (at least) when trying to download and build icu4c--the same problem encountered by many users on many different platforms--and the solution to this problem (or even it's exact root cause) remains elusive. This may be resolvable but would be likely to consume more time than is available within the project scope. METAPYQUOD - PROGRESS REPORT 2 It may be noted that--in this student's opinion--the present state of support of MeTA/metapy jeopardizes the viability of continuing to base the course content on these tools. It appears that no updates have been made to the tool in two years, and as Python and other dependencies continue to evolve, the surface area of supported configurations will continue to shrink. Additionally, it may be particularly noteworthy that the build problems described above with 64- bit ARM systems are likely to also occur with new Apple ARM processor (M1) systems, which will eventually include all new Apple systems going forward. metapyquod-server Work on metapyquod-server is just beginning, but will benefit both from work already done on metapyquod-dev and the tech review completed on AWS Lambda--which shares some implementation details including the OpenAPI specification. Given difficulties with metapyquod- dev, it is likely that some descoping may occur here as well, for instance the feedback or metrics endpoints. Remaining technical unknowns have to do with the behavior of MeTA inverted disk indexes being updated--e.g. it is unclear whether a total re-index will be necessary when starting the container or whether the existing index can be loaded and new documents added. Based on experience with metapyquod-dev, the metapyquod-server container will only target x86_64 platforms, though this is less of an issue for its use case. METAPYQUOD - PROGRESS REPORT 3
https://github.com/SphtKr/MeTAPyquod	MeTAPyquod_Proposal_20201025.pdf	"Matt Williamson -- mdw8 (lead/captain) CS410 October 25, 2020 MeTAPyquod Containerized metapy Appliances The object of this project is to simplify the use of MeTA and the metapy bindings by creating pre-packaged docker containers ready to use with all dependencies for different use cases. The result will be at least the following two reusable containers: metapyquod-dev This container will contain the metapy libraries and a functional development environment including a known good version of Python, metapy dependencies and useful related libraries (e.g. scipy, numpy), Git version control binaries, and a Linux shell, suitable for experimentation and basic development (i.e., all the CS410 programming assignments should be able to be completed with only the tools in the container). This container (or a derivative) may also include a GUI IDE such as Spyder and its dependencies to better facilitate development work. A particular goal of this container is to simplify use of the toolchain on Windows with Docker (and/or Windows Subsystem for Linux), and the hope is that with Docker's new ""multi-arch"" features that this and running on ARM devices (e.g. Raspberry Pi) will be possible. If this goal is achieved, such users will be able to instantiate a full working environment for metapy development with a single ""docker run ..."" command. metapyquod-server This container will provide a simple search engine core as a microservice behind a REST API. The goal is for this container to be capable enough to serve in basic production use as a simple search appliance, for instance in an intranet search scenario. It will also facilitate experimentation with different metapy components in a real world use case. The search service will be intended for use with English language corpora. METAPYQUOD 1 This container will expose a Volume where it expects to find a web mirror filesystem structure according to the conventions produced by the wget spider process. This will allow the user to use the basic but flexible wget tool (likely in another container) to crawl and download web content for ingest by the metapyquod-server appliance. Since wget supports timestamping of downloaded files (and requery culling based on Last-Modified headers, the appliance will monitor the Volume for newly modified files, gaining some efficiencies. In addition to providing sane defaults, this container will also expose a volume with configuration control data (e.g. a config.toml or other relevant files) to facilitate customization of or experimentation with the search appliance. The search service itself will be written in Python (probably with an existing HTTP framework like Django) and will expose at least the following three capabilities as HTTP REST services: 1. A search service, with a text string parameter and pagination parameters. This service will return JSON-formatted search results including the URL, and will include a query identifier for potential use with a feedback mechanism. 2. A feedback service for registering click-throughs for implicit feedback. The viewed URL and the query identifier from the search service will be received as inputs. This service will likely have no effect in the initial iteration, but is established for future implementation of feedback incorporation. 3. A telemetry service providing basic information about the search index for diagnostic or engagement purposes (e.g. last 10 URLs indexed, total documents/terms in index, etc.) A Swagger/OpenAPI specification will be provided such that a compliant client should be able to interact with the service--it should be possible to test the service using a generic Swagger/OpenAPI client. If time permits, a simple example web frontend demonstrator for the service may be provided. Estimates metapyquod-dev: Design and build container specification with dependencies, deploy and test on all target environments, create documentation: 4-6 hours. metapyquod-server: Develop and implement REST services: 10-15 hours. Package and test containerization, create documentation: 4-8 hours. METAPYQUOD 2"
https://github.com/SphtKr/MeTAPyquod	README.md	"MeTAPyquod Proposal PDF Progress Report PDF Containerized metapy Appliances The object of this project is to simplify the use of MeTA and the metapy bindings by creating pre-packaged docker containers ready to use with all dependencies for different use cases. The result is three reusable containers: metapyquod-dev With this container users can instantiate a full working environment for metapy development with a single ""docker run ..."" command, instead of fighting python versions and build issues. This container includes the metapy libraries and a functional development environment including a known good version of Python, metapy dependencies and useful related libraries (e.g. scipy, numpy), Git version control binaries, and a Linux shell, suitable for experimentation and basic development (e.g., all the UIUC CS410 programming assignments should be able to be completed with only the tools in this container). This container builds on multiple architectures including i386, amd64, armv7l and aarch64--so it will run on Windows, macOS, Linux, and Raspberry Pi 2, 3, and 4, for 32-bit and 64-bit OS's! (It's also believed to be ready for Windows ARM64 and MacOS M1 ARM64 processors when Docker supports them!) metapyquod-server This container provides a simple search engine core as a microservice behind a REST API. This server is almost capable enough to serve in basic production use as a simple search appliance in an intranet search scenario, but lacks some robustness. Mainly it can facilitate experimentation with different metapy components in a real world use case. The search service is intended for use with English language corpora. metapyquod-indexer This container expedites the creation of MeTA indexes from a directory structure like that produced by wget --recursive. It captures a few useful fields in the index metadata including the original URL, the title from HTML documents, and the modification time of the file (which can be made to correspond to the modification date of the web page when retrieving with wget). This is intended to be used in conjunction with metapyquod-server, but may be useful in other contexts. What Docker/Why Docker? Docker is perhaps many things, but you can view it as yet another attempt to deliver ""Write Once Run Anywhere"" capability for software. Java did this by abstracting away the operating system and hardware and presenting a new, ""virtual machine"" (the Java VM runtime) that never existed before in concrete form. Hardware virtualization (e.g. VMWare, VirtualBox, KVM) does this by presenting a software-based abstraction of existing physical hardware, upon which you can run any whole OS and software. Docker does something in between, by creating a virtual OS running inside a ""container"" within another running OS (usually Linux) that to your software looks like a dedicated instance of an OS, and--crucially--provides efficient mechanisms to package that virtual OS and all of the dependencies necessary to run your software. In this way, software can be packaged, delivered, and used in a way that is predictable and repeatable regardless of the system on which it is run. With the above explanation, it may be clear why Docker can be useful for distributing and using software like MeTA/metapy, especially for learning or research: it has dependencies or proclivities for certain versions of python, certain operating systems (i.e. Windows) present problems, and gathering and/or building the software and its dependencies can be tedious and error prone and may require a level of proficiency in a number of different technical areas. For the containers in this project, one essentially needs only to install Docker and run a single command to get started. Helpful Background While it is not necessary to have a thorough understanding of Docker to use these containers (especially metapyquod-dev), the following references are helpful in understanding key concepts: Docker Simplified: A Hands-On Guide for Absolute Beginners (text) Learn Docker in 12 Minutes (video) Other References Install Docker Desktop on Windows (Windows Home specific instructions) Install Docker Desktop on Mac Install Docker Engine on Ubuntu (other distros also described on docs.docker.com, and pay special notice to the Optional post-installation steps) Installing Docker on the Raspberry Pi How were these images built? The Dockerfile within each directory defines the build steps necessary for each container, and you should be able to reproduce the build by using (or reading) these Dockerfiles. Note, however, that images for each container have been pushed to Docker Hub, so you should not have to build them yourself (e.g. docker build . -t metapyquod-indexer) unless you want to--you should be able to simply run them (e.g. docker run --rm sphtkr/metapyquod-indexer --help). The images on Docker Hub were built as ""multiarch"" images using buildx/BuildKit, using an amd64 host, a Raspberry Pi 3 running Raspbian Buster as an arm7l (32-bit ARM) build host and a Raspbery Pi 4 running Ubuntu as an aarch64 (64-bit ARM) build host. The result is that you should be able to use a single tag to pull/run any of these images from any x86_64 Linux Docker host (including macOS's HypervisorKit driver and Windows' WSL driver) or any ARMv7 or ARMv8 system supported by Docker (e.g. Raspberry Pi 2, 3, and 4 systems). As the above link points out, this is probably also crucial to support use on Windows and Mac systems with ARM processors, though Docker is not yet supported on these systems. The base image for all containers are the [official Python images], which are (thankfully) already multiarch images, simplifying the build process. Since the MeTAPyquod containers rely on the build system components being in the base image, the ""full"" official images are used and not the slim or alpine variants, which does result in a larger image/download size. (This could be improved for -server and -indexer in the future with a multi-stage build.) One notable aspect of all three Dockerfiles is that they build metapy from source with a small patch (instead of using pip install metapy). In short, all current branches of MeTA include a specified URL for downloading the ICU library, but the ICU project has changed all their download links to a new location on GitHub. This problem should be avoidable by installing libicu and libicu-dev via the package manager (apt), but on some architectures the Cmake build scripts for MeTA force a static build of ICU (for reasons that are not entirely clear), which forces the use of the download URL. Furthermore, the metapy in pip and at the head of the master branch in GitHub builds from a specific commit of MeTA that is no longer at the head of any branch, so to fix the problem both repositories would have to be updated. Therefore, to simplify the cross-architecture build process, we simply checkout the metapy source from GitHub recursively (including the specific commit of MeTA), patch the broken URL (the MD5 hash still matches) and build from that patched source. What's With the Name? The Pequod was the ship sailed by Captain Ahab in Moby Dick. The Whale mascot in the Docker Logo is named Moby Dock. So, ba dum tss (or womp womp)."
https://github.com/ccasey645/CourseProject	CS_410_Project_Proposal.pdf	"Team Members: NetID's of all team members: cdblair2. Team of 1. cdblair2 (Casey Blair) is the captain. Topic: I have chosen to update Metapy's Python version compatibility to Python 3.8 on both Windows 10 and OSX 10.15. Description: Metapy currently will not install on Windows 10 or Mac OSX 10.15 with Python 3.8.*. My project will focus on updating Metapy's code to be compatible with Python 3.8 as it is the most stable recent version of Python. For security reasons, Python codebases around the world have been upgraded from Python 2.* to Python 3.* since Python 2.*'s deprecation in January of 2020. Python 2 is no longer being maintained and using Python 2.* code in production is a security risk. However, when running the Metapy toolkit on my assignments in this class, my classmates and I were only able to run Metapy with Python 2.7 (there were posts in Piazza and Slack about using Python 2.7 exclusively as Python 3.* compatibility was reported broken by everyone who attempted to use Python 3). If I want to use this immensely useful toolkit in production code, I'm going to have to update the Metapy toolkit's Python compatibility to Python 3.* to run with any production Python server/project, so I would like to contribute to this open source toolkit by updating the repo to work with Python 3.8. Justifying the workload is at least 20 hours: I've attempted to run Metapy on OSX and Windows 10 using Python 3.8 and neither would install correctly: both OS's failed to install Metapy during the setup.py file's when using the command line command ""pip install metapy"" with Python 3.8. Both OS's threw errrors related to the C bindings. I'm going to have to research which C bindings are broken, research Pybind11 and get familiar with C bindings for Python, and possibly research OS related issues to get Metapy to work with both Windows 10 and OSX 10.15. The issues with C bindings may take a while to fix as it appears that import of various CMake libraries in Windows and OSX have changed and will need substantial refactoring to work. Also I will need to get familiar with Pybind11 and C bindings for Python. And there will be a lot of testing required to make sure functionality is working as expected: example programs will need to be run (if they exist), and tests will need to be updated."
https://github.com/ccasey645/CourseProject	Final Project Progress Report.pdf	"CS 410 Casey Blair Final Project Progress Report 11/29/2020 My final project is to update the Metapy GitHub repo to allow for installation with Python 3. Currently, when trying to install Metapy using Pip on both Windows 10 and Mac OSX, if using a Python 3 environment, the setup wheel fails to install the Metapy package due to errors. However, if using a Python 2.7 environment on both Windows 10 and OSX, the Metapy library will install correctly. Since Python 2 was deprecated in January of 2020, updating the Metapy code base to work with Python 3.* environments is vital for using Metapy in production code bases. I have been researching Pybind11 and how Python can be used with C and C++ code to understand how the Metapy Git repo works. Metapy is a Python wrapper for the Meta Github repo (https://github.com/meta-toolkit/meta). The Meta code base is written in C++, and when the Metapy package is installed with Pip, the Meta C++ code is downloaded and installed then a Python interface is constructed so the code can be used from a Python interpreter. When installing Metapy in a Python 2.7 environment, the Pip installer downloads a tar.gz file from the Github repo's Releases page with the Metapy code already compiled for a Python 2.7 environment, which is why it can be installed correctly with a Python 2 environment. All the necessary files, including third party libraries, are already downloaded and bundled in the tar.gz file that is then used to install Metapy. However, when installing Metapy using Python 3.*, instead of using this tar.gz file from the GitHub repo's Releases page, the Pip setup wheel builds the Metapy code from scratch, which requires redownloading all third party libraries and compiling the C++ code in the Meta Git submodule. The third party library download step fails because a library the Meta repo uses is failing to download from the URL specified in the Meta Makefile build steps. The following error is repeated over and over again and the installation ultimately fails after 5 retries of downloading the missing package: -- Downloading... dst='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz' timeout='none' -- Using src='http://download.icu-project.org/files/icu4c/58.2/icu4c-58_2-src.tgz' -- [download 100% complete] * Closing connection 2 * Closing connection 0 * Closing connection 1 -- verifying file... file='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz' -- MD5 hash of /Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-58.2/icu4c-58_2-src.tgz does not match expected value expected: 'fac212b32b7ec7ab007a12dff1f3aea1' actual: 'ac2ff460bdda9c70e6ed803f5e4d03fb' -- Hash mismatch, removing... -- Retrying... First, I had to determine which errors were occurring on both operating systems (Windows 10 and OSX) to determine if the installation errors were related or operating system dependent. It turns out the same error is occurring on both Windows 10 and OSX which led me to believe the issue was not operating system dependent and that I needed to follow the build steps to determine why this third party library was failing to install. I had to figure out why the Metapy package was installing correctly for Python 2 but not Python 3 as it didn't make sense that if the C++ code was being compiled on my laptop why it would compile correctly for Python 2 but not Python 3 since the installer would be using the same C++ compiler. Now that I understand that this ""icu4c"" library is included in the tar.gz file downloaded for Python 2 environments I now I understand that the C++ compile step would fail for both a Python 2 and Python 3 environment because this third party library is unable to be downloaded from this broken link. After researching why this ""icu4c"" package was failing to download repeatedly, I found out that the URL used to download the ""icu4c-58_2-src.tgz"" no longer has this tgz file at this URL. I had to step though the setup.py file's build steps and figure out what exactly the build steps were. The setup.py file from the Metapy repo is actually running a C++ make file that downloads the Meta git repo (https://github.com/meta-toolkit/meta), then runs a Makefile to compile the C++ code for the host machine. The icu4c-58_2-src.tgz file download is performed in the Meta C++ Makefile system, so I need to figure out how to update this file download in this second Git repo. While fixing this icu4c-58_2-src.tgz is definitely an important part of fixing the issues with the setup.py file's installation issues, it is unclear what further steps I will need to take to fix the Python 3.* installation at this time. I will not know what other steps I will have to take to fix the C++ makefile compile steps since I'm blocked from progressing further though the compilation steps at this time until I can fix the icu4c-58_2-src.tgz file download. However, figuring out this third party download issue is vital to fixing the Python 3 installation issues as the URL currently in the Meta Git repo's Makefile build steps to download the icu4c file no longer works. But it is possible I will run in to operating system dependent issues when the C++ code is able to be compiled. But until I can get to the point where the C++ compiler is able to attempt to compile the code, I have to fix these other issues that are preventing C++ code compilation from even being started."
https://github.com/ccasey645/CourseProject	Final_Project_Final_Report.pdf	"Casey Blair Team Member Net ID's: cdblar2 CS 410 Final Project Fix Broken Metapy Installation in Python 3.8 Environments My final project is to update the Metapy GitHub repo to allow for installation with Python 3.8. Currently, when trying to install Metapy using Pip on Windows 10, Mac OSX, and Linux if using a Python 3.8 or higher environment, the install setup Python wheel fails to install the Metapy package due to errors. However, if using a Python 3.7 - 2.7 environment the Metapy library will install correctly. Since It is vital to keep this library functioning with the latest version of Python to allow this library to be used in production code bases. I have been researching Pybind11 and how Python can be used with C and C++ code to understand how the Metapy Git repo works. Metapy is a Python wrapper for the Meta Github repo (https://github.com/meta-toolkit/meta). The Meta code base is written in C++, and when the Metapy package is installed with Pip, the Meta C++ code is downloaded and installed then a Python interface is constructed so the code can be used from a Python interpreter. When installing Metapy in a Python 3.7 - 2.7 environment, the Pip installer downloads a tar.gz file from the Github repo's Releases page with the Metapy code already built for that specific Python version's environment, which is why it can be installed correctly with a Python 3.7 - 2.7 environments. The maintainers of the repository stopped making new releases after they published the Python 3.7 version in August 2018. If there is not a pre-built Tar file for the specific Python version, then Pip will use Python's installation wheel system to manually build the version on the client's computer to do the installation of Metapy. The Metapy repository is a python wrapper around the Meta library (https://github.com/meta- toolkit/meta). This Meta project's code is written in C++. The PyBind11 library allows for a Python interface to be constructed between C++ code and Python to allow for C++ code to be used in a Python interpreter. I next researched the PyBind11 Python package and learned how C++ code could be used from a Python interpreter as I have never written a Python program that used C++ code before so I needed to understand this system to understand how Metapy worked. At this point I was unsure if the C++ code was failing because the C++ code needed to be updated to be compatible with a new version of the C++ compiler, or maybe a dependency in the C++ code had changed unexpectedly which was causing the issue. I spent 3-4 hours reading the PyBind documentation so I understood how this library was linking the C++ code from the Meta submodule to the Metapy python code. I also didn't know if Metapy simply needed to be updated to be Python 3.8 compatible. When I first used this library, my Mac had 0Python 3.8 installed as well as Python 2.7 and since a release compatible with Python 3.8 has not been released it caused the setup.py script to be used instead of the just downloading the prebuilt Tar file. Because the C++ files in the Meta file needed to be compiled using the CMake files I also brushed up on C make files as it has been years since I've written one. I then downloaded the Metapy code and ran the manual build steps outlined here: (https://github.com/meta-toolkit/metapy#getting-started-the-hard-way). I ran in to many issues trying to get my C++ compiler to compile the Meta. I also had to brush up on C++ make files as the Meta repo's C++ build steps were failing, so I needed to figure out how the system was being built and it required me to refresh my knowledge of C++ make files as I have not written one in over 5 years. After this initial research, which took 7-8 hours, I started stepping though the C++ compiler errors one at a time. The problem with debugging C++ compiler errors is that when the compiler encounters the first error, it exits the compilation. So the developer doesn't know how many more errors there may be until after the fix the previous error: there can only be one error encountered at a time since the compiler exits the compilation after the first encountered error. So I knew I needed to fix all the errors the C++ compiler encountered here to allow the system to build correctly for Python 3.8. I attempted to install the Metapy library in my Python 3.8 environment using the following command (which failed): sudo python3.8 -m pip install metapy The first error I ran in to was due to a missing C header file on my operating system. This was to be expected as I just created this virtual machine with a clean install of Ubuntu and it did not have python 3.8 installed on it by default, so I had installed Python 3.8 but forgot to install some development tools associated with python. I needed to install the Debian package python3.8-dev with the following command: Sudo apt-get install python3.8-dev To install the missing Python.h header file that the C++ compiler was throwing an error because this file was misssing from my operating system. I had to research the error being thrown to figure out that the python3.8-dev package was needed to fix the missing Python.h file error being thrown by the Metapy install wheel. After fixing the Python.h missing file issue, rerunning the Metapy's installation command would fail with the following error repeating five times before exiting the installation: -- Downloading... dst='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz' timeout='none' -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'' -- [download 100% complete] * Closing connection 2 * Closing connection 0 * Closing connection 1 -- verifying file... file='/Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz' -- MD5 hash of /Users/caseyblair/Documents/Classes/CS_410/metapy/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz does not match expected value expected: 'fac212b32b7ec7ab007a12dff1f3aea1' actual: 'ac2ff460bdda9c70e6ed803f5e4d03fb' -- Hash mismatch, removing... -- Retrying... It was very strange that this ICU Tar file download was producing a different MD5 hash after each attempt to download the Tar file. I decided to investigate this further as if the Tar file had been updated, but the expected MD5 hash had not been, then at least the same incorrect MD5 hash should be displaying each time. However, the actual MD5 hash was a different hash each time, which made me suspect a different error was occurring. I decided to attempt to visit the URL the C++ code was attempting to download the dependency from (http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz). After pasting this URL in to my web browser, instead of a TGZ file download, my browser took me to an HTML page listing all the releases of this ICU dependency. Puzzled why this URL was not a download link anymore, I navigated around this website until I found an answer. After going to the ""Source Repository"" page, at the top of this page I found my answer (http://site.icu-project.org/repository http://site.icu-project.org/repository). The code repository that used to host this ICU dependency was migrated from Subversion to GitHub. Therefore, the previous download link would not work anymore, and all pre-built TGZ files were now on GitHub. I then returned to the ICU version release page and found my version 61 listed and clicked on that link. I was redirected to a page listing the change log for this new version and found the link to the Git repo where the code now resides: https://github.com/unicode-org/icu/releases/tag/release-61-1. I navigated to the GitHub repo to find many pre-built TGZ files. I had to step though the setup.py file's build steps and figure out what exactly the build steps were ad where this ICU package was being downloaded at in the code. The setup.py file from the Metapy repo is actually running a C++ make file that downloads the Meta git repo (https://github.com/meta-toolkit/meta), then runs a Makefile to compile the C++ code for the host machine. The icu4c-61_1 -src.tgz file download is performed in the Meta C++ Makefile system, so I need to figure out how to update this file download in this second Git repo. It took me a while to search though all the C++ code in the Meta repo to find the correct location where the ICU package was being downloaded. I finally found the line in the Meta repo where the ICU package was downloaded. I tried each of the TGZ files until once successfully downloaded and the C++ compiler progressed past that point of the installation. Each attempt took 20 minutes as the C++ compiler is quite slow, so this was a manual process to debug. At this point, I was not sure how many other issues I would encounter. I didn't know if I would need to fix many more of these external dependency url's that were broken, or if this was the only one. Fortunately, this was the only broken URL and the rest of the compilation succeeded. I successfully fixed the Python 3.8 build system as the C++ compiler was able to compile the Meta repo successfully, then install the Metapy library using Pip. I did have to make forks of the Metapy and Meta repos in GitHub as my GitHub account does not have write access to the main Metapy or Meta repos. I tried to make a new branch on the Metapy and Meta repos to update the code, however, I was met with 403 unauthorized errors each time. So I needed to fork the repos and make my own version, then make merge requests from my forked repo back to the parent repo. I currently have three open GitHub pull requests to fix this issue in the original parent repos: https://github.com/meta-toolkit/meta/pull/222, https://github.com/meta-toolkit/meta/pull/220, and https://github.com/meta-toolkit/metapy/pull/10. All three pull requests are currently waiting review by the code repo's members so I have no idea when or if they will approve these changes as none of them have made an update to the repo since August 2018. However, you can currently use my forked version of Metapy to correctly install Metapy for Python 3.8 with the following command: sudo python3.8 -m pip install git+https://github.com/ccasey645/metapy.git This command, on Ubuntu 18.04, will build and install the Metapy library correctly. This command will instead download my forked version of Metapy from my GitHub repo and the installation succeeds. I did not have time to attempt the build on OSX and Windows 10 too as spent well over 20 hours figuring out the code changes needed to fix the C++ compiler errors. However, when I was attempting to run this on OSX and Windows 10 my C++ compiler installations on these OS's were in a very broken state: Windows 10 needed some new version of Visual Studio C++ library installed that would break other projects I was working on, and my OSX's clang compiler was throwing errors saying that it was not allowed to compile C files using a C++ compiler so I decided to focus just on the code changes needed to fix the errors and leave the OSX and Windows 10 TGZ building to the Metapy code repo owners. When the Metapy code repo owners accept my GitHub pull requests, there is a make-release.sh script in the Metapy repo that will build the appropriate Tar files for all three platforms, so each client computer will not have to have a 100% configured C++ compiler just to install the Metapy library."
https://github.com/ccasey645/CourseProject	README.md	"CourseProject Team Name Game of Threads NetID's cdblair2 (Casey Blair) Topic: Update Metapy repo's code to fix errors when installing with Python 3.8 environment Final Project Documentation/Report File Name: Final_Project_Final_Report.pdf link: https://github.com/ccasey645/CourseProject/blob/main/Final_Project_Final_Report.pdf Presentation Video File: link: https://drive.google.com/file/d/1Ty8QGFlu-29FeX0yPSJ502Wzu_YwygVt/view?usp=sharing On Google Drive because Git's file size limit prevented me from uploading it here. Installation Demo Video File: Mentioned this in Presentation Video File This 12 minute video demonstrated how ""sudo python3.8 -m pip install metapy"" fails, but ""sudo python3.8 -m pip install git+https://github.com/ccasey645/metapy.git"" succeeds"" link: https://drive.google.com/file/d/1cb1YbEJkYavFmk_LiHOLWqlwR72bLMAS/view?usp=sharing On Google Drive Source Code: CourseProject/metapy Note: this is a Git Submodule that will redirect you to another GitHub repository. This is where all my code is since I was updating the Metapy repo: I needed to make a fork of the original repo."
https://github.com/retrouvailles0/CourseProject	CS410FinalProjectProposal.pdf	CS410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rui Liu (ruiliu7) Captain Zhenzhou Yang (zy29) 2. What system have you chosen? Which subtopic(s) under the system? We choose Option 2.2 Expert Search System. Both. We plan to finish an application where users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. 3. Briefly describe the datasets, algorithms or techniques you plan to use. * The positive datasets are the faculty web pages given by students used for mp2.1 (https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVx ixU5vvMuUaM/edit#gid=0). The negative datasets would be non-directory web pages we collect online. Their ratio would be close to 1:1. * We will preprocess the html data and get their text. Using TF-IDF to filter the text and select features, vectorize the data into the input of our models. * We will train models using Python sklearn packages, such as linear regression, logistic regression, naive bayes, SVM, nearest neighbors and decision trees. Then we will choose the one yielding the highest accuracy as our final model. * If none of the models in sklearn perform well, we may build a neural network model including word embedding layer and linear layers. * If all the models failed to yield good accuracies, we may adopt several improving methods such as limiting the domains that users are intended to search. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We add a function to enable users to enter a URL link and do the crawling automatically. The way we demonstrate that it works as expected is that users will get direct feedback from our system if the entered URL is a directory page or not. If yes, the system will continue to search all URL embedded in this directory website and classify if any of them is a faculty URL page. If yes, we will add this website to the final structured output. Then users will be able to check the results from the automatic crawler. Besides, there are things we would like to improve. While we researched the provided system, we found out there are bugs that have not been catched. For example, if I enter some arbitrary meaningfulness word, the system returns back some information that is not related at all. What's more, there are times that the faculty names are not displayed properly and listed empty. Therefore we would like to catch these bugs and fix them. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly. We plan to maintain a similar user interface as the given system. However, our system will focus more on automating the crawling process of MP2.1. Users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. In order to do the identification tasks, we plan to preprocess the text information on the input URL and do vectorization using TF-IDF, then train various models using sklearn packages and then select the best one as our model for the system. By doing this, our system will be able to classify any URL given by the users. To display the structured faculty information, we plan to utilize the format of the given system. Instead of using the interface directly, we will debug it first to take care of the blocks that should not be displayed when users enter meaningfulness words. What's more, we will fill in the information provided by our system to demonstrate our work. 6. Which programming language do you plan to use? * Machine Learning: Python scikit-learn package and pytorch * Web Crawling: Python selenium package * User Interface: Python in the given Github repo. * Supplement: Python tkinter, Javascript packages (haven't been decided) 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Get familiar with the current system: 4hrs * Read related docs. * Clone repo and run the code. Get the dataset: 4hrs * Collect 900 non-directory dataset to make the ratio of positive and negative in directory dataset 1:1. * Collect 900 non-faculty dataset to make the ratio of positive and negative in faculty URL dataset 1:1. Preprocess data: 10hrs * Implement TF-IDF with various parameters to vectorize the data as the format to fit into the models and select the most optimal parameters. Develop models: 30 - 35hrs * Train models using Python sklearn packages, including linear regression, logistic regression, naive bayes, SVM, nearest neighbors and decision trees. * Build an neural network model using pytorch (haven't been decided) * Fine tune the models and do cross validation and calculate accuracy for every model. * Compare accuracy of different models and take the one with highest accuracy as the final model in our system. Design and Implement User Interface: 5hrs * Since the user interface of the current system was designed for words search, we need to customize the interface to better serve our needs. Meetings: 10hrs * Weekly meetings to keep both of the members on the same page.
https://github.com/retrouvailles0/CourseProject	Project Progress Report.pdf	Project Progress Report Recalling from our project proposal, we plan to finish an application where users can enter a URL link and our system will identify faculty directory pages, identify faculty webpage URLs on the directory website, and format a structured faculty list with relevant information from faculty bios if the given URL is valid. Besides that, based on the feedback of the proposal, we decided to put on more effort on the faculty directory training model. By now, the progress we made includes one training model on faculty directory and negative dataset collection. We have downloaded the positive datasets from https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVx ixU5vvMuUaM/edit#gid=0 and collected 300 negative data points by hand. We have finished preprocessing html data and use text on the website as the input for our model. We removed the header and the redundant spaces in the text. And use Tfidftransformer from sklearn to do the feature vectorization and build vocabulary for us. Currently we have only implemented the SVM model for the directory page classification task. The model yields a pretty optimistic accuracy data around 95% using 5-fold cross validation. Therefore we have no plan on adding a neural network or limiting the domains now. Difficulties: We found out that collecting negative datasets is time exhausting. Since we would like our negative data points to be as diverse as possible, we collected every one of the website URLs by hand. We collected 300 negative data points instead of 800 and the accuracy is pretty optimistic, therefore, we have no plan on adding more negative data for now. We are still in the process of implementing other models and choosing the most optimal one in the near future. We also have the second task which is identifying whether the page contains the profile links for the faculty and crawling the data. Currently we have not yet come up with a strategy to conquer it.
https://github.com/retrouvailles0/CourseProject	README.md	Course Project Introduction This repo is for CS410 course project option 2.2 Expert Search System. \ Our system is part of the automatic crawler for MP2, using which users can enter a URL link and the system will identify if the entered URL is a valid faculty directory page. In order to finish the task, we retrieve the information on those websites and process data using text retrieval methods. Then, we train several several machine learning models using the processed text data from labeled websites(positive/negative). After training, we do test and cross validation and then select the optimal model to save for future prediction. When the user enters a URL, the saved model will be retrieved and a result will be predicted using that model. Author Zhenzhou Yang(zy29@illinois.edu) \ Rui Liu(ruiliu7@illinois.edu) Dataset Positive datasets are the faculty web pages given by students used for mp2.1 ( https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVx ixU5vvMuUaM/edit#gid=0 ). The data is saved in positive_link.csv Negative datasets are non-directory web pages we collected online. The data is saved in negative_info.csv Positive/Negative Ratio 3:1 Method Process the html data of labeled data set and get their text. Use TF-IDF to filter text, select features, vectorize the data into the input of our models. Training models (sklearn) SVM SVC LinearSVC Naive Bayes MultinomialNB Tree DecisionTreeClassifier RandomForest RandomForestClassifier Linear SGDClassifier Adaptive Boosting AdaBoostClassifier Save model (joblib) User interface (tkinker) Run The code was tested using Python 3.7. Train the model angular2html python3 select_model.py Predict a URL angular2html python3 predict.py Demo https://uofi.box.com/s/apzzkkdm40upm5rvkh3qpg8sob23o4a8
https://github.com/juan4bit/CourseProject	README.md	"Pattern annotation Table of Contents vim-markdown-toc GFM Introduction Prerequisites Installation Implementation Selecting DBLP inproceedings (conferences) Pattern extraction and compression Semantic annotation Tutorial Selecting DBLP inproceedings (conferences) Pattern extraction and compression Semantic annotation Presentation vim-markdown-toc Introduction In this project I will try to reproduce the results of the following paper: * Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 This paper proposes using paradigmatic and syntagmatic relationships from text data in order to annotate non-text data with semantic context in the same way a dictionary would define a word with synonyms (paradigmatic patterns) and examples of the word being used in a context (syntagmatic context). In this particular implementation, the non-text data represents items from a transactional database, instantiated as authors from major computer conferences, and the text data represents the titles from such conferences. Prerequisites You will need a Unix machine to run this tool, I used Ubuntu 20.04 at the time of developing and testing. You will also need to install Python 3 and the latest Java Runtime Environment. Installation Clone the project from the code repository: sh git clone https://github.com/juan4bit/CourseProject.git cd CourseProject Once in the project folder, install the following Python dependencies: sh python -m pip install -U pip wheel Optional: You may additionally want to setup a virtual environment for this project by running: sh python -m venv ~/.envs/CourseProject source ~/.envs/CourseProject/bin/activate Finally, install the following Python dependencies: sh pip install -r requirements.txt And download the SPMF tool and place the jar file in the ./lib folder of the project directory. You may also need to give this file executable permissions: sh chmod +x spmf.jar Implementation I decided to implement this paper in three stages that I describe in detail below. Selecting DBLP inproceedings (conferences) The original paper tests the algorithm by selecting a subset of conferences from the DBLP dataset, a 3GB+ XML file with bibliographic entries on major computer science journals, theses and proceedings. For the purpose of this paper, we are only interested on the proceedings (conferences). Furthermore, because the amount of conferences is too large, I also decided to select only those falling in a given date range (determined by year). There are existing tools to manipulate and filter XML files, namely XSL templates, but because of the large size of the original DBLP dataset, I was not able to get any of them working so instead I decided to implement my own script which incrementally reads the DBLP file and selects only the items and fields I'm interested in, that is, the conference titles and its authors. Additionally, I also preprocess the title text through stemming and removal of stop words as described in the original paper. This stage can be run via the python main.py scrape command which I will explain how to use in the next section. Pattern extraction and compression Before extracting paradigmatic and syntagmatic relationships, we first need to mine patterns from the non-text data (authors) and text data (titles). The paper suggests using FP-Close algorithm for the former and CloSpan for the latter which instead of implementing them myself I decided to use a third party tool, SPMF, that I call internally from my script code, please make sure the dependency is installed as described in the previous section. After the Closed Frequent Patterns are extracted, the paper suggests two compression algorithms to reduce the overall number of entries for future stages in the pipeline, aiming to reduce redundancy among the patterns in each dataset. I decided to implement one of the algorithms, One-pass Microclustering, where you calculate the Jaccardian distance between each pair of patterns in a given dataset and one by one, it starts to either append a pattern to an existing cluster if the Jaccardian distance to the farthest item in the group falls below a threshold or creates a brand new cluster for the pattern. From each cluster then I select the pattern that is, on average, closer to the other patterns in the group, that is, the medoid pattern. This stage can be run via the python main.py mine command which I will explain how to use in the next section. Semantic annotation Given a preprocessed DBLP dataset and a list of Frequent Patterns for conference authors and titles, the last stage of the pipeline is a script that expects a query pattern as input (either an author, a list of authors or a text phrase) and returns its: Syntagmatic context. Given the definition of Mutual Information , where x and y are binary variables that represent each whether a pattern shows up in the database or not and P represents either the single or joint probability of these events happening, the algorithm scores each pattern from the list against the query and selects the top N as its context. Mutual Information can be understood as the reduction of uncertainty from a pattern once another one (e.g. the query) is given, the more reduction, the more likely these two patterns are related. Paradigmatic patterns. Given the set of patterns determined in the previous step, the algorithm calculates vectors of Mutual Information scores from each pattern in the original list against the patterns from the query context. Then it computes the cosine similarity between these vectors and the one from the query context and selects the top N patterns as paradigmatic relations. The rationale is that if the context patterns from the query reduce uncertainty in a similar way for another pattern, then it's likely that these two patterns are similar given a context. Paradigmatic transactions. The last step is similar to the previous one except in how the context of a transaction is calculated. For each pattern of the query context, the algorithm checks whether it appears in a transaction or not, if it does then it scores 1, otherwise -1. The original paper assigns 0 for a non-match but in practice I found out this value didn't yield good results. This stage can be run via the python main.py annotate command which I will explain how to use in the next section. Tutorial As mentioned in the previous section, there are three stages in the pipeline that can be run as individual commands. The tool provides help messages when running python main.py --help or python main.py cmd --help, where cmd can be either scrape, mine or annotate. In this section, I'll explain in detail these help messages and provide examples but before you start, make sure to download the DBLP dataset and unzip the XML file together with its DTD definition file. Selecting DBLP inproceedings (conferences) Running python main.py scrape --help yields the following output: ```sh usage: main.py scrape [-h] --dblp_file DBLP_FILE --article_file ARTICLE_FILE [--from_year FROM_YEAR] optional arguments: -h, --help show this help message and exit --dblp_file DBLP_FILE REQUIRED: the path to the DBLP input file --article_file ARTICLE_FILE REQUIRED: the path where the selected articles will be printed in XML format --from_year FROM_YEAR selects articles from no earlier than the provided year ``` So you can select conferences from the DBLP dataset by running a command similar to this one: sh python main.py scrape --dblp_file dblp.xml --article_file articles.xml --from_year 2010 This will read dblp.xml, select all inproceedings (conferences) no earlier than 2010 and print the preprocessed items in articles.xml in the following format: ```xml xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""? Fast multipoint evaluation and interpolation of polynomials in the LCH-basi s over F 2020 fast multipoint evaluate interpolation polynomial lch-basis f axel mathieu-mahias michael quisquater ... ``` The title field stores the original title of the conference while label stores its preprocessed version (after stemming and then removing stop words). Pattern extraction and compression Running python main.py mine --help yields the following output: ```sh usage: main.py mine [-h] --dblp_file DBLP_FILE --title_file TITLE_FILE --author_file AUTHOR_FILE --title_support TITLE_SUPPORT --author_support AUTHOR_SUPPORT --title_distance TITLE_DISTANCE --author_distance AUTHOR_DISTANCE optional arguments: -h, --help show this help message and exit --dblp_file DBLP_FILE REQUIRED: the path to the DBLP input file --title_file TITLE_FILE REQUIRED: the path where the title patterns will be printed --author_file AUTHOR_FILE REQUIRED: the path where the author patterns will be printed --title_support TITLE_SUPPORT REQUIRED: the minimum support [0, 1] for title patterns. --author_support AUTHOR_SUPPORT REQUIRED: the minimum support [0, 1] for author patterns. --title_distance TITLE_DISTANCE REQUIRED: the Jaccard threshold [0, 1] to use when compressing title patterns --author_distance AUTHOR_DISTANCE REQUIRED: the Jaccard threshold [0, 1] to use when compressing author patterns ``` So you can mine and compress patterns by running a command similar to this one: sh python main.py mine --dblp_file articles.xml --title_file titles.txt --author_file authors.txt --title_support 0.003 --author_support 0.001 --title_distance 0.9 --author_distance 0.9 This will read a preprocessed DBLP dataset (it assumes a label field exists for each inproceedings element which is not the case in the original DBLP file) and print to titles.txt and authors.txt the list of titles and authors respectively that were mined as frequent patterns. Title subsequences (that's what CloSpan generates) will be space separated while author itemsets will be semicolon separated. Title and author support represent the coverage percentage that a pattern needs to exhibit to be considered frequent, in this case, a title subsequence needs to show up in 0.3% of the transactions and an author itemset 0.1%. Title and author distances are the Jaccardian threshold described in the previous section, used to compress the pattern list by removing redundancy, the larger the threshold, the more agressive the compression is. Semantic annotation Running python main.py annotate --help yields the following output: ```sh usage: main.py annotate [-h] --db_file DB_FILE --title_file TITLE_FILE --author_file AUTHOR_FILE -q QUERY --type {author,title} -n1 N_CONTEXT -n2 N_SYNONYMS -n3 N_EXAMPLES optional arguments: -h, --help show this help message and exit --db_file DB_FILE REQUIRED: the XML input file with all the transactions --title_file TITLE_FILE REQUIRED: the input file that stores the patterns for titles --author_file AUTHOR_FILE REQUIRED: the input file that stores the patterns for authors -q QUERY, --query QUERY REQUIRED: the query pattern to enrich with semantic annotations --type {author,title} REQUIRED: the type of the query pattern -n1 N_CONTEXT, --n_context N_CONTEXT REQUIRED: the number of context indicators to select -n2 N_SYNONYMS, --n_synonyms N_SYNONYMS REQUIRED: the number of semantically similar patterns to select -n3 N_EXAMPLES, --n_examples N_EXAMPLES REQUIRED: the number of representative transactions to select ``` So you can find the semantic annotations of a given query pattern by running a command similar to this one: sh python main.py annotate --dblp_file articles.xml --title_file titles.txt --author_file authors.txt -q ""network"" --type title -n1 7 -n2 5 -n3 3 DBLP file, title file and author file paths point to the preprocessed DBLP dataset and the list of title and author patterns respectively. The query type can be either title or author, for the former just write any phrase surrounded by double quotes and for the latter write a semicolon-separated list of authors (casing doesn't matter but letter matching has to be identical so beware typos) also surrounded by quotes. N1, N2 and N3 represent the number of syntagmatic patterns and the number of paradigmatic patterns and transactions to retrieve respectively. The output will look something like the XML below: xml <definition> <pattern> <title>network</title> </pattern> <context> <pattern> <title>deep neural</title> </pattern> <pattern> <title>convolution neural</title> </pattern> <pattern> <title>network image</title> </pattern> <pattern> <title>graph neural</title> </pattern> <pattern> <title>generative adversarial</title> </pattern> <pattern> <title>graph convolution</title> </pattern> <pattern> <title>network 3d</title> </pattern> </context> <synonyms> <pattern> <title>method</title> </pattern> <pattern> <title>optimize</title> </pattern> <pattern> <title>deep learning</title> </pattern> <pattern> <title>explore</title> </pattern> <pattern> <title>evaluate</title> </pattern> </synonyms> <examples> <transaction> <title>Dual-domain Deep Convolutional Neural Networks for Image Demoireing.</title> <author>vien gia an</author> <author>hyunkook park</author> <author>chul lee</author> </transaction> <transaction> <title>A topological encoding convolutional neural network for segmentation of 3D mu ltiphoton images of brain vasculature using persistent homology.</title> <author>mohammad haft-javaherian</author> <author>martin villiger</author> <author>chris b. schaffer</author> <author>nozomi nishimura</author> <author>polina golland</author> <author>brett e. bouma</author> </transaction> <transaction> <title>FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary.</ title> <author>yingzhen yang</author> <author>jiahui yu</author> <author>nebojsa jojic</author> <author>jun huan</author> <author>thomas s. huang</author> </transaction> </examples> </definition> Presentation You can find a recorded version of the tutorial here."
https://github.com/alex-pi/CourseProject	README.md	CourseProject Automatic Crawler of Faculty Pages Tool is live on this URL: https://sxxgrc.github.io/faculty-scraper/ Demo video under: doc/UsageDemo.mp4 Directory structure faculty_scraper_ui.py >> flask client for GUI cli.py >> command line utility to use the faculty scraper globals.py >> common python definitions README.md +---data finalized_model.sav >> svm model data output.zip >> output example TrainingDataSetTest.csv TrainingTestingDataSet.csv +---output >> csv files will be generated here, one for positive cases, another for all URLs tested .gitignore README.md +---doc Automatic crawler of faculty pages - Project Proposal.pdf ProgressReport.pdf FinalReport.pdf UsageDemo.mp4 +---spiderbot scrapy_spider.py +---ui >> web content for GUI such as JS, html and css +---public +---src faculty_scraper.js >> Main search page for UI, handles accessing the flask client and polling results.js >> Main results page which displays the JSON result from the Flask client +---urlclassification url_classification.py >> model training and classification
https://github.com/chuanyueshen/CourseProject	CS410_FinalProjectProposal_TEAMPYTHON.pdf	CS 410 Text Information System: Final Project Proposal TEAM PYTHON Team members: * Chuanyue Shen (NetID: cs11, team leader) * Jianjia Zhang (NetID: jianjia2) * Runpeng Nie (NetID: runpeng3) Project name: In-class Text Classification Competition Answer required questions: * Are you prepared to learn state-of-the-art neural network classifiers? o Yes. * Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. o Neural classifiers: DNN, RNN, LSTM, and CNN. o Deep learning frameworks: TensorFlow, PyTorch * Models we may explore: o We may start with a self-developed DNN architecture to get to know about the dataset. Then we may move on to implement some state of art CNN architectures and tune the learning parameters to achieve a higher accuracy. We may also use some pretrained model to expediate the model training. Relevant prior experience: The team members are familiar with Python language and have some limited experience with mini machine learning projects during Machine Learning related courses. We chose this competition to gain some hands on in text classification and to explore machine learning approaches. Programming language: Python Timeline: * Model buildup: week 11 - 12 * Training and testing: week 12-14 * Model improvement: week 14-15 * Final Report: week 15-16
https://github.com/chuanyueshen/CourseProject	CS410_project_progress.pdf	"CS 410: Project Progress Report TEAM PYTHON Team members: * Chuanyue Shen (NetID: cs11, team leader) * Jianjia Zhang (NetID: jianjia2) * Runpeng Nie (NetID: runpeng3) Tasks have been completed: 1. Data preprocessing: Training data and test data store in JSON line file. Each data item has three fields. The first field of the training data is a label, indicating whether the response of this data is sarcasm or not. The first field of the test data is its ID. Both training data and test data have a response field and a context field. The response field stores the tweet to be classified, and the context field stores the conversation context of the response relatively. Both response and context are string data. Because the data is tweets, so there are lots of emoji objects and @USER marks. The first step is removing the emojis and @USER. First of all, the regular expression package is used since all emojis are encoded in Unicode. By using re.compile() function the emoji Unicode pattern is defined, and emojis are removed through re.sun() function. @USER is relatively easier to remove. Just like what we learned from lectures, there are lots of meaningless stop words like ""the"", ""a"" etc, they are almost useless for text classification. The NLTK package provides an English stopwords list, by adding ""@USER"" to the list and removing all words that appear in the list from response and context. Besides, for text classification, the punctuation character is useless too since the punctuation does not hold sentiment like normal words. Words are combined except punctuation through using string.punctuation. After these operations, the data is basically cleaned. Only having a clean dataset is not enough. In English, with the change of grammar and context, there are many words with similar roots that have a similar meaning. For instance computer, computing, and computational. Their appearance increases the complexity of matrix operations. In order to improve performance and raise classification accuracy, words need to be grouped/replaced by their stem word. NLTK's PortStemmer tool is very useful. By replacing some words with their stem word, the computation complexity dropped significantly. 2. Model implementation * Naive Bayes o For this model, we choose the Gaussian distribution to fit the distribution of the count of each word. Then use Naive Bayes classifier to fit the data. o Result: # Precision = 0.5371 # Recall = 0.7967 # F1 = 0.6416 * SVM o For this model, we preprocess the data using TF-IDF methods. Due to the reason that we have too many unique words and which will result in around 20,000 features, but the training data has only 5000 rows which will possibly result in underfitting. We remove the words with term count less than 3. After the data cleaning, the unique words are around 6,000, which is reasonable compared to original unique words. o Parameters: # min_df = 3 o Result: # Precision = 0.5829 # Recall = 0.8633 # F1 = 0.6959 * LSTM (Passed the baseline) o For this model, we use three densely-connected layers and use Sigmoid as the activation function in the output layer. We choose binary cross entropy as the loss function, and RMSprop as the optimizer. We take the 5000 strings as the input and for each string we choose at most max_len = 100 words from the right side to left side. o Parameters: max_words = 5000, max_len = 100, batch_size = 128, epochs = 50, dropout_value = 0.5 o Result # Precision = 0.6068 # Recall = 0.8967 # F1 = 0.7238 * CNN model (in progress): o We start with a basic CNN model based on PyTorch framework. The model uses three convolutional layers with LeakyReLU and BatchNorm2d, and Sigmoid as the activation function in the output layer. We use SGD as the optimizer, and NLLLoss as the loss function. With proper fine tuning, the model is expected to pass the baseline and beat the LSTM model. Tasks are pending: * Finish the CNN model and fine-tune the learning parameters * Implement some other state of art models/methods, such as transformer-based machine learning technique BERT Challenges: * During tuning LSTM models, we found it is not easy to improve the precision value. * Current completed models cannot achieve much higher F1 scores. We will try to achieve the F1 scores by using CNN model and/or BERT model and finetuning the parameters."
https://github.com/chuanyueshen/CourseProject	README.md	"CS 410 Final Project - Classification Competition This is TEAM PYHTON's repo for CS 410 final project classification competition. This competition is about Twitter Sarcasm Detection. We implemented Naive Bayes, SVM, and LSTM to classify a Tweet as Sarcasm or Not sarcasm. LSTM model gave the best prediction for the test dataset. Team members: Chuanyue Shen (cs11), Jianjia Zhang (jianjia2), Runpeng Nie (runpeng3) Prerequisite Please use Python3 and install the following packages: numpy jsonlines nltk string re autocorrect Keras sklearn Pandas It is recommended to run our program on machines with GPU. Run the code Data cleaning and preprocessing Type python preProcessData.py Train the model and make predictions To run the LSTM model, type python lstm.py To run the SVM model, type python svm.py To run the Naive Bayes model, type python navieBayes.py Test dataset prediction After running the model, the test dataset prediction will be saved in the local directory, named answer.txt Reference https://www.kaggle.com/kredy10/simple-lstm-for-text-classification Presentation https://youtu.be/IC9ncGVvbcQ More details about the project Source code Please refer to the Source code part to the ""Run the code"" part mentioned above. The test set prediction of our best results can be found in answer.txt. The F1 score of one of our best results using LSTM beat the baseline and can be found in the Livedatalab leaderboard under the name of cs11 and/or jianjia2. Implementation details Data preprocessing: Training data and test data store in JSON line file. Each data item has three fields. The first field of the training data is a label, indicating whether the response of this data is sarcasm or not. The first field of the test data is its ID. Both training data and test data have a response field and a context field. The response field stores the tweet to be classified, and the context field stores the conversation context of the response relatively. Both response and context are string data. Because the data is the tweets, so there are lots of emoji objects and @USER marks. The first step is removing the emojis and @USER. First of all, the regular expression package is used since all emojis are encoded in Unicode. By using re.compile() function the emoji Unicode pattern is defined, and emojis are removed through re.sun() function. @USER is relatively easier to remove. Just like what we learned from lectures, there are lots of meaningless stop words like ""the"", ""a"" etc, they are almost useless for text classification. The NLTK package provides an English stopwords list, by adding ""@USER"" to the list and removing all words that appear in the list from response and context. Besides, for text classification, the punctuation character is useless too since the punctuation does not hold sentiment like normal words. Words are combined except punctuation through using string.punctuation. After these operations, the data is cleaned. Only having a clean dataset is not enough. In English, with the change of grammar and context, there are many words with similar roots that have a similar meaning. For instance computer, computing, and computational. Their appearance increases the complexity of matrix operations. To improve performance and raise classification accuracy, words need to be grouped/replaced by their stem word. NLTK's PortStemmer tool is very useful. By replacing some words with their stem word, the computation complexity dropped significantly. With the steps mentioned above, we can generate a n * 3 numpy array. The 3 columns store label, response, and context, respectively. The array is saved in 'dataStep1.csv' file. Using the n * 3 numpy array, we can build a term document matrix that counts the frequency of each vocabulary in each tweet. The term document matrix is saved in 'term_doc_matrix.csv' file. 'dataStep1.csv' is mainly used in LSTM model implementation with some further processing. 'term_doc_matrix.csv' is mainly used in Naive Bayes and SVM models. Model implementation and test result * Naive Bayes For this model, we choose the Gaussian distribution to fit the distribution of the count of each word. Then use the Naive Bayes classifier to fit the data. Result: # Precision = 0.5371 # Recall = 0.7967 # F1 = 0.6416 * SVM For this model, we preprocess the data using TF-IDF methods. Due to the reason that we have too many unique words and which will result in around 20,000 features, but the training data has only 5000 rows which will possibly result in underfitting. We remove the words with term count less than 3. After the data cleaning, the unique words are around 6,000, which is reasonable compared to original unique words. In the training process, we shuffle the training dataset and split it as training data (0.8) and validation data (0.2). Result: # Precision = 0.5829 # Recall = 0.8633 # F1 = 0.6959 * LSTM (best model) For this model, we create a neural network with LSTM, and word embeddings were learned while fitting the neutral network. We try to manipulate the number of layers and layer depths to find the best model architecture. Our final best RNN with LSTM model is composed of an embedding layer, a LSTM layer, 3 densely-connected layers, and activation and dropout layers in between. 'ReLu' is used for the activation function. Dropout = 0.5 is used to reduce overfitting. We adopt Sigmoid as the activation function in the output layer. In the training process, we try to compare different loss functions (mse, binary cross entropy), optimizers (Adam, SGD, RMSprop), and batch sizes (16,32,64,128). Our final best model using binary cross entropy as the loss function, RMSprop as the optimizer, and batch size = 128. Also, we shuffle the training dataset and split it as training data (0.8) and validation data (0.2). For the data input, we further process the 'dataStep1.csv'. At first, we take the 5000 strings as the input and for each string, we choose at most max_len = 100 words from the right side to the left side. And then, based on the vocabulary, we store the index of each word for each string in the table (matrix_sequences) with its size as 5000 by 100. Overall, the data input for the LSTM model would be a matrix with a size of 5000 by 100. Result # Precision = 0.6068 # Recall = 0.8967 # F1 = 0.7238 Contribution All team members made equal contribution to the project and commited 20 hours+ per person to this project."
https://github.com/rupsis/CourseProject	final report_documentation (3).docx	"IR Competition Description This is the final course project for CS 410 - Text Retrieval & Mining course at the University of Illinois at Urbana-Champaign. For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Project Team: Nathaniel Rupsis (NetID: nrupsis2) - Team Captain Hoa Le (NetID: hle30) Video Presentation Link to our video presentation for this project can be found below: https://mediaspace.illinois.edu/media/t/1_cgdhmw3n Prerequisites Packages Metapy We use Metapy to build and evaluate our search engi # Ensure your pip is up to date pip install --upgrade pip # install metapy! pip install metapy pytoml # Ensure your pip is up to date pip install --upgrade pip # install metapy! pip install metapy pytoml ne. argparse The argparse module makes it easy to write user friendly command line interfaces. # Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work # Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work argparse should work on Python 2.3 and newer. Running the program Directory: | - /test \- line.toml ... data | - /train \- line.toml ... data A test and train directory need to exist with a line.toml file which specifies the metadata. Note: the test and train dataset are too large to be uploaded to this repo. Please see the link below to download the dataset. You need to put line.toml file in the downloaded train dataset in order for build_courpse.py to create the index. The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing // line.toml type = ""line-corpus"" metadata = [{name = ""uid"", type = ""string""}] To build the index, simply run: // remove the index else the evaluator will use a cached version rm -rf idx/ && python3 build_courpse.py train where the first argument is either ""test"" or ""train"" (need to have corresponding test / train files for MetaPy to work) To run the evaluator: python search_eval.py config_train.toml where the config argument is either config_train.toml or config_test.toml File & Code Walkthrough search_eval.py This is our main file containing most of the crucial functions used for creating ranking algorithms and searching an index as well as evaluate and writing the results. Below is the list of functions in the search_eval.py file: expand_query(query): Description: This function is a simple approach to query expansion. It expands on some important keywords such as covid-19, coronavirus, test, mark, spread, etc... It also contains some pre-processing methods such as removing punctuation, converting to lower case, filtering out common words, etc... Parameter: query What the function returns: return list of query words containing new keywords. load_queries(): Description: This function retrieves query from the xml file. Parameter: None What the function returns: return tuple array of queryID and query pairs. load_ranker(cfg_file): Description: This function returns the Ranker object to evaluate Parameter: cfg_file is the path to a configuration file used to load the index. What the function returns: Ranker object. saveResults(prediction_results): Description: This function writes the results to ""predictions.txt"" file. Parameter: prediction_results (a list of 3 elements returned from running ranking algorithms) What the function returns: return list of query words containing new keywords. runQueries(queries): Description: This is our main function for creating inverted index and ranking algorithms. Parameter: it takes the output which is the query from load_queries function as input. What the function returns: The function saveResults mentioned above is called to store the output. build_courpse.py This function contains code to extract information such as title, abstract, introduction, etc... from metadata.csv using example code. It also writes the data to .dat file and create metadata file for metapy. config.toml We have two separate config file : config_test and config_train. Each file contains general settings that deal with paths related to the its corresponding dataset. We specified some details about the corpus and its analyzer. For example: dataset = ""train"" # name of the dataset corpus = ""line.toml"" # type of corpus query-judgements = ""qrels.txt"" # path to query judgement file [[analyzers]] method = ""ngram-word"" # set of co-occurring words ngram = 1 # specified to one word aka unigram filter = ""default-unigram-chain"" stopwords.txt A text file contains list of common English words that do not add much meaning to a sentence. For example: ""the"",""a"",""an"",etc...They can be safely ignored, and this is part of pre-processing to filter out useless information. train & test folders Contain the cord-19 dataset and metadata.csv file. The train data also contains query relevance file. predictions.txt This is a text file that has the same format as relevance judgment file which is (query_ID doc_uid relevance_score). It contains our final top 1000 documents per query and is used for computing nDCG score on the leaderboard. Implementation Overview Task Definitions Collecting data Unzip and extract data from the provided train.zip Collect and store information from metadata.csv such as title, abstract, introduction, etc... Building a corpus Create train/test.dat file. Each line in .dat file represents one document. Create metadata.dat file containing the collected information from step 1 part ii. Create line.toml configuration file containing corpus input formats. Implementing simple query expansion Collect query from xml file. Query are stored in a tuple array (queryID, query) Create query expansion function with some preprocessing methods. Create an inverted index Set up the config file. This file contains fields that need to be specified and makes references/pathing to several files. Write code to create inverted index. Searching the index Create bm25 ranker. Run query to search the index and returns top 1000 documents per query as sorted vector pairs (doc_id, double). Store the query-ids, results and the corresponding uid (document_id retrieved from metadata file) and write to predictions.txt file. Optimize nDCG score Submit predictions.txt file and check the nDCG score on the leaderboard. Address potential problems and identify methods to improve nDCG score. Potential Issues Encounter If UnicodeEncodeError: 'charmap' codec can't encode characters error appears, running python3 seems to fix that. One solution which we will add to our code is to write (encoding = ""utf-8"") when reading .json or .dat file. We ran into some disk problem when implementing BERT-large method for reranking. We decided to exclude it from our final code to avoid this potential issue. Key Findings & Potential Improvements Even though we beat the baseline and made it to leaderboard, we think that the result could be further enhanced. Below is what we identify that might help with improving the result: Add more information to the index (actual documents) Weight the keywords better since relevancy will definitely be based on the keywords (covid, alcohol, spread, African American, health, etc...) Further expanding on query. Query expansion has helped enhancing our result greatly. Implementing BERT large for reranking. We ran into disk problems when trying to implement it. Citation https://github.com/meta-toolkit/metapy Bhavya, Dec (2020) MP 2.4 [Source code] https://github.com/CS410Fall2020/MP2.4 Contact nrupsis@gmail.com hle027@gmail.com"
https://github.com/rupsis/CourseProject	NA Coders Project Progress Report.pdf	Team: NA coders Members: Hoa Le (NetID: hle30), Nathaniel Rupsis (NetID: nrupsis2) Project Progress Report For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Progress Made Thus Far We break down the project into smaller tasks and milestones: extracting, preprocessing, indexing, retrieving and ranking. In the extracting step, we collected Titles, Abstracts, and Introductions of papers using the Python code sample provided. Then, we fed the extracted data to the preprocess function we created. The preprocess function contains basic preprocessing tasks such as removing special characters, text.split(), stemming, etc... For data indexing, we've taken two different approaches, and each individual is working on their own implementation. By doing this, we'll have some options and flexibility when tweaking the overall system. The first approach is to build inverted indices from scratch. It will take 4 inputs (uid, title, abstract and keywords) or a dataframe containing those 4 elements and output a dictionary of inverted indices. These keywords are based on their tf-idf scores. We used the stop word file for previous assignments to obtain a list of words that will not be indexed. As for the query, we decided to go for the query field of the topic. They are easily preprocessed as the other two fields(question and narrative) contain uppercase and more special characters. The Second approach is to build a corpus .dat file with a pre-processor, and then utilizing the metapy indexing algorithm. By doing this, it allows us to focus on what to include in the index (title, author, etc), while freeing us from having to worry about the performance of the index. With both index approaches, our team has a firm handle on the data preparation, and the next task is to implement the retrieval model. For the ranking algorithm, we are looking to use okapi BM25 and combine it with other state-of-the-art methods. Remaining Tasks Even though we haven't gotten results from our okapi BM25 ranking algorithm, we are 90% sure that it won't be enough to beat the baseline. Thus, the remaining tasks would be to finalize all the steps we mentioned, testing BM 25, fine-tune various parameter settings and then look to combine it with different methods to enhance our result. We are currently looking at some of the live systems that use BM25 such as Neural Covidex. We are also exploring LDA(Latent Dirichlet Allocation) which studies semantic relationships. Challenges that We Encountered We had trouble getting started because we felt overwhelmed with the massive dataset which contains over 57000 articles. However, once we started to break down the project into smaller tasks, it became much more manageable. The preprocessing proved to be a bit of a tough task since the dataset contains highly technical papers with scientific terms. Another big challenge is to build an inverted index from scratch. I think it's extremely worthwhile and we can learn a lot from the process. Final challenge to find the advanced ranking methods to pass the baseline.
https://github.com/rupsis/CourseProject	NA_Coders_Final_Project_Proposal.pdf	Team: NA coders Members: Hoa Le (NetID: hle30), Nathaniel Rupsis (NetID: nrupsis2) Project Proposal The NA Coders will be competing in the IR (Information Retrieval) competition. Our team is prepared to research, test, and implement state of the art search methods and techniques. For this project, we'll be using python, and focusing on implementing and tuning the methods mentioned in the proposal(query expansion, feedback, rank fusion, etc). We will dig deeper into more advanced ranking techniques such as learning to rank, and Okapi BM25. We have experienced Okapi BM25 in the previous assignments and it's truly a state-of-the-art technique. I think it'll help with the project using different parameters. Learning to rank is a ranking technique where it learns to directly rank items by training a model to predict the probability of one item over another. We will be testing 3 different learning to rank algorithms: RankNet, LambdaRank and LambdaMART. We are prepared to learn more about these techniques to help us reach the baseline and achieve high standing position in the leaderboard. For query expansion, we'll be looking into utilizing WordNet with the Natural Language Toolkit to help test variations of query expansion to aid in the text retrieval model. For ranking fusion, since we are trying different IR models, it's best to have a method that can combine these models together so the overall probability that the document is relevant can be higher. Finally, time permitting, we'll test out and incorporate additional retrieval methods listed here, such as Compound term processing and Contextual Searching.
https://github.com/rupsis/CourseProject	README.md	"Team NA Coders Project team: Nathaniel Rupsis (team captain) Hoa Le Project details: This is the final course project for CS 410 - Text Retrieval & Mining course at the University of Illinois at Urbana-Champaign. For the project our team decided to participate in the CORD-19 Open Research Dataset IR Competition. The CORD-19 dataset contains over 57000 scholarly articles available for the global research community. Our goal is to build a live system that supports search on this massive dataset. Our team is prepared to research, test, and implement state of the art search methods and techniques. For this project, we'll be using python, and focusing on implementing and tuning the methods mentioned in the proposal(query expansion, feedback, rank fusion, etc). Time permitting, we'll test out and incorporate additional retrieval methods listed here, such as Compound term processing and Contextual Searching. The complete project proposal can be found here. Video Presentation: Link to our video presentation for this project can be found here: https://mediaspace.illinois.edu/media/t/1_cgdhmw3n Prerequisites Packages Metapy We use metapy to build and evaluate our search engine. If you have not installed metapy so far, use the following commands to get started. ```bash Ensure your pip is up to date pip install --upgrade pip install metapy! pip install metapy pytoml ``` argparse The argparse module makes it easy to write user friendly command line interfaces. argparse should work on Python 2.3 and newer. ```bash Try one of the following installation methods: pip install argparse python setup.py install easy_install argparse putting argparse.py in some directory listed in sys.path should also work ``` Running the program Directory: | - /test \- line.toml ... data | - /train \- line.toml ... data A test and train directory need to exist with a line.toml file which specifies the metaData. Note: the test and train dataset are too large to be uploaded to this repo. Please see the link below to download the dataset. You need to put line.toml file in the downloaded train dataset in order for build_courpse.py to create the index. The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing ``` // line.toml type = ""line-corpus"" metadata = [{name = ""uid"", type = ""string""}] ``` To build the index, simply run: // remove the index else the evaluator will use a cached version rm -rf idx/ && python3 build_courpse.py train where the first argument is either ""test"" or ""train"" (need to have corresponding test / train files fro MetaPy to work) To run the evaluator: python search_eval.py config_train.toml where the config argument is either config_train.toml or config_test.toml File & Code Walkthrough search_eval.py This is our main file containing most of the crucial functions used for creating ranking algorithms and searching an index as well as evaluate and writing the results. Below is the list of functions in the search_eval.py file: expand_query(query): Description: This function is a simple approach to query expansion. It expands on some important keywords such as covid-19, coronavirus, test, mark, spread, etc... It also contains some pre-processing methods such as removing punctuation, converting to lower case, filtering out common words, etc... Parameter: query What the function returns: return list of query words containing new keywords. load_queries(): Description: This function retrieves query from the xml file. Parameter: None What the function returns: return tuple array of queryID and query pairs. load_ranker(cfg_file): Description: This function returns the Ranker object to evaluate Parameter: cfg_file is the path to a configuration file used to load the index. What the function returns: Ranker object. saveResults(prediction_results): Description: This function writes the results to ""predictions.txt"" file. Parameter: prediction_results (a list of 3 elements returned from running ranking algorithms) What the function returns: return list of query words containing new keywords. runQueries(queries) : Description: This is our main function for creating inverted index and ranking algorithms. Parameter: it takes the output which is the query from load_queries function as input. What the function returns: The function saveResults mentioned above is called to store the output. build_courpse.py This function contains code to extract information such as title, abstract, introduction, etc... from metadata.csv using example code. It also writes the data to .dat file and create metadata file for metapy. config.toml We have two separate config file : config_test and config_train. Each file contains general settings that deal with paths related to the its corresponding dataset. We specified some details about the corpus and its analyzer. For example: dataset = ""train"" # name of the dataset corpus = ""line.toml"" # type of corpus query-judgements = ""qrels.txt"" # path to query judgement file [[analyzers]] method = ""ngram-word"" # set of co-occurring words ngram = 1 # specified to one word aka unigram filter = ""default-unigram-chain"" stopwords.txt A text file contains list of common English words that do not add much meaning to a sentence. For example: ""the"",""a"",""an"",etc...They can be safely ignored, and this is part of pre-processing to filter out useless information. train & test folders Contain the cord-19 dataset and metadata.csv file. The train data also contains query relevance file. predictions.txt This is a text file that has the same format as relevance judgment file which is (query_ID doc_uid relevance_score). It contains our final top 1000 documents per query and is used for computing nDCG score on the leaderboard. Implementation Overview Task Definitions Collecting data i. Unzip and extract data from the provided train.zip ii. Collect and store information from metadata.csv such as title, abstract, introduction, etc... Building a corpus i. Create train/test.dat file. Each line in .dat file represents one document. ii. Create metadata.dat file containing the collected information from step 1 part ii. iii. Create line.toml configuration file containing corpus input formats. Implementing simple query expansion i. Collect query from xml file. Query are stored in a tuple array (queryID, query) ii. Create query expansion function with some preprocessing methods. Create an inverted index i. Set up the config file. This file contains fields that need to be specified and makes references/pathing to several files. ii. Write code to create inverted index. Searching the index i. Create bm25 ranker. ii. Run query to search the index and returns top 1000 documents per query as sorted vector pairs (doc_id, double). iii. Store the query-ids, results and the corresponding uid (document_id retrieved from metadata file) and write to predictions.txt file. Optimize nDCG score i. Submit predictions.txt file and check the nDCG score on the leaderboard. Address potential problems and identify methods to improve nDCG score. Potential Issues Encounter If UnicodeEncodeError: 'charmap' codec can't encode characters error appears, running python3 seems to fix that. One solution which we will add to our code is to write (encoding = ""utf-8"") when reading .json or .dat file. We ran into some disk problem when implementing BERT-large method for reranking. We decided to exclude it from our final code to avoid this potential issue. Key Findings & Potential Improvements Even though we beat the baseline and made it to leaderboard, we think that the result could be further enhanced. Below is what we identify that might help with improving the result: 1. Add more information to the index (actual documents) 2. Weight the keywords better since relevancy will definitely be based on the keywords (covid, alcohol, spread, African American, health, etc...) 3. Further expanding on query. Query expansion has helped enhancing our result greatly. 4. Implementing BERT large for reranking. We ran into disk problems when trying to implement it. Citation https://github.com/meta-toolkit/metapy Bhavya, Dec (2020) MP 2.4 [Source code] https://github.com/CS410Fall2020/MP2.4 Contact nrupsis@gmail.com hle027@gmail.com"
https://github.com/toskaushik/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Junting98/CourseProject	ProgressReport.pdf	1. Done data preprocessing and have implemented CNN and bidirectional LSTM. Cannot beat the baseline but have come pretty close. The general pipeline of the classification model is completed. Might need some fine-tuning. 2. Beat the benchmark result, submit the code with documentation, and prepare for the presentation. 3. Mainly have concerns with data processing. Not sure what kind of words to filter and what preprocessing will have the best result.
https://github.com/Junting98/CourseProject	ProjectProposal.pdf	1 Captain: Junting Wang(junting3) Member: Tianwei Zhang(tianwei4) 2 Text classification competition 3 We are prepared to learn state-of-the-art neural network classifiers. I know pytorch and tensorflow. I have completed several research projects using pytorch and tensorflow. 4 I plan to use python
https://github.com/Junting98/CourseProject	README.md	Documentation In the following, we will describe how the text classification model is build and its major components. Project Overview In this project, our team is doing the text classification competition. The specific task is sarcasm detection, given a specific Twitter response, we are trying to tell if the response is sarcasm. Each data point in the given dataset consist of the response tweet and also the context of the response. The goal of this project is to develop a robust model that can perform well in telling whether the given Twitter response is sarcasm or not. DEMO Link https://mediaspace.illinois.edu/media/1_pa3h5d8y Classification Model We have experimented with many different models. In the end, we reached the competition benchmark score by fine-tuning a pre-trained BERT, distilled BERT in specific. Data Preprocessing For data cleaning, we remove punctuations and any other special characters in the Twitter response. Also, we expand abbreviations such as can't and won't into can not and will not respectively. We also remove the heading of each response. After cleaning the data, we use a pre-trained DistilBertTokenizer to tokenize the cleaned response data. Then, the tokenized responses are used as the input to our model. Model Architecture The general idea of our model is to fine-tune the pre-trained distilled BERT for text classification. We achieved this by adding two extra fully connected linear layers to the BERT output and fixing the parameters for the BERT model. The general pipeline of the model is that given tokenized responses as input, we first put the inputs to the distilled BERT base model to get high-dimensional representations of the responses. Then the response representations are input to the two linear layers to get the final prediction of whether the response is sarcasm. Between the linear layers, we used ReLU as activation. We also applied dropout to both the output of the base model and the output of the linear layers. Model Training Given raw Twitter response, we first preprocessing it following the data preprocessing steps to get tokenized responses. Then, the tokenized responses are used as the input to the BERT base model, which will give high-dimensional representations of the responses. Then, those representations are put into several linear layers to generate the final prediction. The loss function we use is a NLL loss. After getting the training prediction, together with the ground truth labels that indicate whether the response is sarcasm or not, we put them to the NLLLoss and performs back propagation on the computed loss. Evaluation During training, we split the train dataset into two subset, one for actual training, one for validation. The percentage of the validation set is 20% of the data point in the original training dataset. In each epoch, we evaluate the F1 score of the model on the validation set save the model having the best F1 score. For the actual prediction task on the test set, we use the saved model during training for the actual prediction. Previous Attempts We have come a long way to the model we have right now. We first thought of models based on CNN and RNN. But after actually implemented them, those models did not give results good enough to beat the competition benchmark. Apart from distilled BERT, we have also experimented with the full BERT, which gives decent result, but it tends to overfit and takes a lot more time to run. We have also tried to vary the number of linear layers and the dimension of those layers used to fine-tune the model, we have tried to add 3 or 4 linear layers and many other different combinations of dimension, but we finalized to 2 linear layers, which are of size(768, 256) and (256, 2). In terms of different activation function, we have tried Tanh, PReLU, and LeakyReLU. Though they all give very similar results, we choose ReLU in the end. We have also experimented the dropout ratio in the range [0, 0.5]. We observed that with 0.5 dropout, the model reaches best performance on the test set. We also tried to tune the learning rate in the range[0.00001, 0.01]. For data preprocessing, we found that removing stopwords and stemming the words has negatively affected the performance of our model. Expanding abbreviations seems to have improve the performance of the model by reducing overfitting. Removing punctuations and special characters generally gives cleaner data for the tokenizer. Therefore, it helps both with the model training and model testing. Dependencies Python Json PyTorch Skit-Learn Transformers To install dependencies, you can use the included environment.ysml in the code directory to create a virtual environment with Anaconda. Installation reference can be found here. A detailed tutorial is also included in the DEMO. Contributions Junting Wang: Team Leader. Implemented the model and written up the code documentation. Tianwei Zhang: Helped with experiments, model testing and project DEMO.
https://github.com/chungfaith1/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. For the final project submission, please go to the ""documentation"" folder to read the Final Project Report and Presentation. The link to the demo is here: https://www.youtube.com/watch?v=K2cQH4pPyBk&feature=youtu.be"
https://github.com/luoxix/CourseProject	documentation.pdf	"PROJECT DOCUMENTATION Reproduce A Paper: A cross-collection mixture model for comparative text mining Overview This code is used to discover latent themes across collections. We provide two models, the simple mixture model (simplemix.py) and the cross-collection mixture model (ccmix.py). The simple mixture model treats multiple collections as one single collection and discovers k latent common themes in it. The Cross-collection mixture model explicitly distinguishes themes from different collections. It not only captures k common themes that characterize common information across all collections but also k collection-specific themes for each collection. The value of k is set by users and each theme is characterized by a multinomial word distribution. Implementation Dataset The paper uses 2 datasets: war news, and laptop reviews. We are not able to obtain exactly the same datasets as those are used in the paper. For the war news, we manually searched and downloaded 30 news articles from BBC or CNN for each of the two wars, published in one-year span (May 2003 - April 2004 for Iraq war, Nov 2001 to Oct 2002 for Afghanistan war) to approximate the war news dataset that is used in the paper. (code/data/war_dataset.txt) For the laptop reviews, we chose 3 laptops from the Amazon.com best sellers in laptop computers: Acer Aspire 5 Slim Laptop, Apple MacBook Air, HP Chromebook 14-inch HD Laptop, and manually downloaded the top 40 reviews from Amazon.com. (code/data/laptop_reviews.txt) Each document occupies one line in the .txt file, prepended by an integer that indicates which collection the document is from. Preprocessing We preprocessed the data by removing stop words (including punctuation marks), words that contain less than 3 characters, and stemming the documents to map inflected words to their stems. Preprocessing is important to ensure that the resulting clusters after applying the mixture model would contain more meaningful words for evaluation. Initialization SimpMix Model (baseline) CCMix Model In SimpMix model, there are two types of themes: the background theme, and the shared themes (between collections) In the CCMix model, there are three types of themes: the background theme, the common themes throughout all collections, and themes specific to the collections. Both SimpMix and CCMix models use the EM algorithm to find the clusters iteratively. The two models also share the same background theme (topic_word_prob_background, dimension: 1 * vocabulary_size) which is calculated by taking the maximal likelihood of each word in the whole corpus (includes all the collections) before the iterative EM steps. The other parameters !!(lambda_B) and !""(lambda_C) which are the probability of selecting the background theme and the common theme respectively. We used 0.95 for !!and 0.25 for !""as the paper suggested. Expression Name in code Initialization function Dimension !! lambda_B Scalar, 0.95 !"" lambda_C Scalar, 0.25 !(#, %) term_doc_matrix build_term_doc_matrix (self) This function counts the term frequency in each document, and computes the background distribution. number_of_collections * number_of_documents * vocabulary_size '(#|)!) topic_word_prob_bac kground 1 * vocabulary_size ""#,% document_topic_prob initialize_randomly (self, number_of_topics) This function randomly initialize number_of_collections * number_of_documents * number_of_topics '(#|)%) topic_word_prob number_of_topics * vocabulary_size '(#|)%,&) topic_word_prob_col lection_specific number_of_collections * number_of_topics * vocabulary_size # epsilon Scalar, 0.00001 The EM algorithm The implementation of the baseline SimpMix model is very similar to MP3 except for introducing a background model. Therefore, in this section, we will focus on how to implement the EM algorithm for the CCMix model. E step E step is to estimate the hidden variables: probability of selecting a theme in a mixture model. Expression Name in code Calculation function Dimension '(*#,""!,' = ,) Collection specific theme topic_prob_j expectation_step (self, number_of_topics, verbose) This function performs E step number_of_collections * number_of_documents * vocabulary_size * number_of_topics '(*#,""!,' = -) Background theme topic_prob_ B number_of_collections * number_of_documents * vocabulary_size * 1 '(*#,""!,' = .) Common theme topic_prob_ C number_of_collections * number_of_documents * vocabulary_size * number_of_topics M step M step is to use the hidden variables to re-estimate the distributions in each theme, maximizing the likelihood. Expression Name in code Calculation function Dimension ""#,% ()*+) document_topic_prob maximization_step(self, number_of_topics, verbose) This function performs M step number_of_collections * number_of_documents * number_of_topics '()*+)(#|)%) topic_word_prob number_of_topics * vocabulary_size '()*+)(#|)%,&) topic_word_prob_colle ction_specific number_of_collections * number_of_topics * vocabulary_size /01 '(.) likelihoods calculate_likelihood(self) This function calculates likelihood List of scalars The iteration of E-M steps continues until the difference between likelihood in adjacent iterations is less than #or the maximum iteration number is reached. Usage Python version Python 3.6 Download Repo git clone https://github.com/luoxix/CourseProject.git Install Dependencies pip install metapy pip install numpy Run code To run the simple mixture model: python simplemix.py --input_path ./data/laptop_reviews.txt -- output_path ./result/result_simple_laptop.txt --lambda_b 0.95 --max_iterations 500 -- number_topics 5 --number_top_words 8 --verbose True To run the cross-collection mixture model: python ccmix.py --input_path ./data/laptop_reviews.txt -- output_common_path ./result/common_laptop.txt -- output_specific_path ./result/specific_laptop.txt --lambda_b 0.95 --lambda_c 0.25 -- max_iterations 1000 --number_topics 5 --number_top_words 8 --verbose True python ccmix.py --input_path ./data/war_dataset.txt -- output_common_path ./result/common_war.txt --output_specific_path ./result/specific_war.txt -- lambda_b 0.95 --lambda_c 0.25 --max_iterations 1000 --number_topics 5 --number_top_words 8 --verbose True Meaning of each argument input_path: the path of the input file which contains all collections. Each line contains a document and the first number denotes which kind of collection it is from. output_path: the path of the output file which contains k themes, for each theme, several top words with highest probability are shown. output_common_path: the path of the output file which contains common themes output_specific_path: the path of the output file which contains specific themes lambda_b: the weight of the background model lambda_c: the weight of common theme max_iterations: the number of iterations for EM algorithm number_topics: the number of latent themes number_top_wods: the number of top words which are shown in the output file verbose: whether to output the immediate information Results and Evaluations Run the code with instructions described above until the likelihood value converges. Laptop Reviews Cluster 1 Cluster 2 Cluster 3 Common theme words catalina0.0198 remov 0.0148 harddriv 0.0148 ad 0.0148 new 0.0102 second 0.0102 internet 0.0099 found 0.0099 samsung 0.0169 2400 0.0169 numer 0.0169 tech 0.0169 top 0.0169 edg 0.0169 left 0.0169 hour 0.0148 fan 0.0311 2020 0.0311 temperatur 0.0271 thermal 0.0271 cpu 0.0216 zoom 0.0203 extern 0.0203 bar 0.0203 Cluster 1 Cluster 2 Cluster 3 Common air 0.0252 connect 0.024 2020 0.022 thermal 0.0165 pictur 0.0137 poor 0.0125 bar 0.0124 new 0.0124 mode 0.0247 app 0.0179 differ 0.0172 side 0.0169 2400 0.0162 samsung 0.0162 support 0.014 download 0.013 call 0.0304 cpu 0.0162 amazon 0.0156 charg 0.0155 thermal 0.0153 brand 0.015 noth 0.012 bar 0.0115 Acer drive 0.0274 ad 0.0201 harddriv 0.0197 remov 0.0194 click 0.0178 pictur 0.0175 new 0.0165 second 0.0142 remov 0.0204 mode 0.0163 harddriv 0.0146 ad 0.0146 wouldn't 0.0129 veri 0.0121 second 0.0121 case 0.0114 call 0.0354 amazon 0.0191 did 0.0166 tech 0.0157 bla 0.0155 brand 0.015 wait 0.0139 minut 0.0135 HP connect 0.0347 amaz 0.0221 pictur 0.0206 love 0.017 unit 0.016 chrome 0.0158 drive 0.0151 nice 0.0151 cuz 0.0413 love 0.028 i'm 0.0207 didn't 0.0207 daughter 0.0207 polici 0.0207 glad 0.0207 aren't 0.0207 call 0.0306 charg 0.0204 brand 0.0197 amazon 0.0163 noth 0.0157 minut 0.0145 did 0.0139 differ 0.0136 Macbook Air upgrad 0.1062 hard 0.0704 16gb 0.0551 drive 0.0522 probabl 0.0475 beauti 0.0475 instal 0.0452 suppos 0.0448 receiv 0.0258 lock 0.0234 dissapoint 0.0162 possibl 0.0162 owner 0.0162 anazon 0.0162 recoveri 0.0162 immedi 0.0162 call 0.0304 cpu 0.0162 amazon 0.0156 charg 0.0155 thermal 0.0153 brand 0.015 noth 0.012 poor 0.0115 From the result, we can see that SimpMix model is only able to find the themes in the whole corpus. The themes are shared among collections. It tells about what topics the overall corpus covers, but it is not able to identify topic differences between different collection. On the other hand, the CCMix model is able to find the common themes throughout the collections, and is also able to identify the different specific themes in each collection. For example, in Cluster 2, all three collections share the same common theme. However, there is a high frequency of ""love"" in HP collection, whereas there is a high frequency of ""disappoint"" in the Macbook Air collection. We may infer these two opposite attitudes maybe towards the same topic in the common theme. Probably this HP laptop provides app or support that buyers love, whereas Macbook Air disappointed buyers in these two aspects. Therefore, we can conclude that, in reviews evaluation, the CCMix model is able to identify different performance of similar products on the same aspects. Another observation is that not all specific themes are well distinguished from the common theme / other specific themes within the same cluster (e.g., Cluster 3). This is probably because we use a uniform !"" for all clusters. However, for some clusters, there are more overlaps in topics among collections and less differences, and !"" should be larger to account for the common topics. The data we use are from Amazon, and many of them are expression of feelings, and purchase experience with Amazon, instead of technical reviews. Therefore, the results are more on customer satisfaction. On the other hand, performing CCMix model on technical reviews will find more about the performance of each laptop. War Dataset Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Common theme words khalifa 0.0307 o'neil 0.0199 newsweek0.0174 quran 0.0154 hous 0.0141 dyke 0.0113 cbs 0.0102 kennedi 0.0092 mirror 0.0161 threat 0.0139 palac 0.0134 wmd 0.0122 religi 0.0113 morgan 0.0092 nuclear 0.0087 pictur 0.0076 flag 0.0475 gun 0.0158 design 0.0091 nasratullah0.0088 kit 0.0088 equip 0.0087 zardad 0.0083 leak 0.0072 woodward0.0247 powel 0.0220 kerri 0.0210 marin 0.0201 matti 0.0174 gen 0.0125 clinton 0.0114 bandar 0.0110 draft 0.0237 opium 0.0211 hamdi 0.0202 rape 0.0169 wolfowitz0.0149 farmer 0.0149 poppi 0.0132 erad 0.0114 Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Common theme words Flag 0.0450 chang 0.0158 design 0.0131 women0.0096 repres 0.0095 new 0.0086 threat 0.0085 equip 0.0084 gun 0.0332 draft 0.0296 leak 0.0173 kennedi 0.0158 katharin 0.0154 o'neil 0.0143 prosecut 0.0130 secret 0.0128 Wolfowitz 0.0403 soro 0.0370 group 0.0304 rumsfeld 0.0277 independ 0.0230 money 0.0220 rais 0.0185 moveon.org 0.0168 mirror 0.0277 religion 0.0218 god 0.0207 zardad 0.0195 koran 0.0164 morgan 0.0164 dearing 0.0158 cramer 0.0135 marin 0.0178 woodward 0.0165 powel 0.0147 coalit 0.0145 matti 0.0123 gen 0.0122 opium 0.0101 sunday 0.0095 Iraq theme words flag 0.0450 chang 0.0158 design 0.0133 women 0.0096 repres 0.0095 new 0.0088 threat 0.0085 equip 0.0084 gun 0.0338 draft 0.0295 leak 0.0172 kennedi 0.0158 katharin 0.0153 o'neil 0.0142 prosecut 0.0129 secret 0.0128 wolfowitz 0.0403 soro 0.0370 group 0.0304 rumsfeld 0.0277 independ 0.0230 money 0.0220 rais 0.0185 moveon.org 0.0168 mirror 0.0277 religion 0.0218 god 0.0207 zardad 0.0195 koran 0.0164 morgan 0.0164 dearing 0.0158 cramer 0.0135 zapatero 0.0295 spain 0.0269 spanish 0.0167 coalit 0.0134 sunday 0.0132 marin 0.0124 woodward 0.0115 powel 0.0102 Afghan theme words women 0.0326 rape 0.0119 elect 0.0118 soviet 0.0117 khalifa 0.0112 live 0.0099 nasratullah 0.0092 villag 0.0091 kerri 0.0389 hamdi 0.0382 hous 0.0315 clinton 0.0217 cohen 0.0186 wednesday 0.0154 foam 0.0152 polystyren 0.0133 money 0.1082 group 0.1033 rumsfeld 0.0746 million 0.0695 rais 0.0515 candid 0.0456 link 0.0384 campaign 0.0375 mirror 0.0654 zardad 0.0472 morgan 0.0396 daili 0.0285 tortur 0.0244 qlr 0.0216 pictur 0.0212 goldsmith 0.0202 newsweek 0.0554 quran 0.0489 magazin 0.0274 toilet 0.0228 desecr 0.0228 isikoff 0.0196 dirita 0.0196 investig 0.0151 Similarly, the result for the war dataset also demonstrates the ability of CCMix in differentiating the specific themes between collections. For example, in Cluster 1, we can see that in Iraq war news, people are more interested in reporting flag and mental changes, whereas in Afghanistan war news, women raped were reported in the highest frequency. In Cluster 2, Iraq war news reported more on gun and draft, while Afghanistan war news reported more on the two persons: Kerry and Hamdi. Another observation with the war dataset is that in Cluster 1 to 4, the common theme has high similarity with the Iraq theme, and has much smaller overlap with the Afghan theme. This is probably because the Iraq specific theme is a very tight cluster where the top words have very high frequencies, such that the common theme is only able to account for partial frequencies of the top words. Only in Cluster 5, both themes are very different from the common theme. Conclusion In conclusion, we can see that CCMix model is able to address the task of comparative text mining by its capability to discover the latent common themes across all collections, and to summarize the similarity and differences of the collections along each common theme. However, the performance of CCMix varies on the choice of !! and the characteristics of the dataset that it is applied on. Contribution Xi Luo (xiluo4) Yuheng Zhang (yuhengz2) Algorithm implementation: CCMix Documentation: - Implementation - Results and Evaluations - Conclusion Algorithm implementation: SimpMix Documentation: - Overview - Usage Tutorial presentation"
https://github.com/luoxix/CourseProject	progress report.pdf	PROGRESS REPORT Reproduce A Paper: A cross-collection mixture model for comparative text mining 1. Data Collection (Partially Completed) Dataset 1: Laptop reviews (Completed) Dataset 2: War news (Not Started) This should be done by manually searching and downloading 30 news articles from BBC or CNN for each of the two wars, published in one year span (May 2003 - April 2004 for Iraq war, Nov 2001 to Oct 2002 for Afghanistan war). This would be a very close proximation of the war news dataset that is used in the paper. 2. Algorithm Implementation (Completed) Implementation of the CCMix model is fully done, and EM converges with the laptop reviews dataset. 3. Experimental Results Gathering (Partially Completed) The experimental results should be collected and summarized in a table of theme clusters as the same way in the paper to facilitate comparison. 4. Results Comparison and Evaluation (Not Started) Baseline model (SimpMix) should be implemented, and the results from the CCMix model should be compared with those from the baseline model to evaluate the performance of CCMix model and compare with the conclusions from the paper. 5. Preparation of Documentation and Presentation (Not Started) A readme file and a video presentation should be prepared for the final submission.
https://github.com/luoxix/CourseProject	proposal.pdf	"PROJECT PROPOSAL Reproduce A Paper: A cross-collection mixture model for comparative text mining 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Xi Luo (Captain): xiluo4 Yuheng Zhang: yuhengz2 2. Which paper have you chosen? ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 3. Which programming language do you plan to use? python 4. Can you obtain the datasets used in the paper for evaluation? The paper uses 2 datasets: war news, and laptop reviews. We are not able to obtain exactly the same datasets as those are used in the paper. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? For the war news, we will manually search and download 30 news articles from BBC or CNN for each of the two wars, published in one year span (May 2003 - April 2004 for Iraq war, Nov 2001 to Oct 2002 for Afghanistan war). This would be a very close proximation of the war news dataset that is used in the paper. For the laptop reviews, we choose 3 laptops from the Amazon.com best sellers in laptop computers: Acer Aspire 5 Slim Laptop, Apple MacBook Air, HP Chromebook 14-inch HD Laptop, and will manually download the top 40 reviews from Amazon.com. This would be comparable to the laptop reviews dataset that is used in the paper. 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? (Not applicable)"
https://github.com/luoxix/CourseProject	README.md	CourseProject Project Topic Reproduce A Paper: A cross-collection mixture model for comparative text mining Tutorial Link https://www.youtube.com/watch?v=QebNvPrvisk Documentation documentation Team members Xi Luo (xiluo4) Yuheng Zhang (yuhengz2)
https://github.com/ss129/CourseProject	Course Project Document.pdf	"Text Classification Competition: Twitter Sarcasm Detection CS410 - COURSE PROJECT DOCUMENT Team - SSW Classifiers * Saravana Somasundaram (ss129@illinois.edu) * Shashivrat Pandey (spandey6@illinois.edu) * Walter Tan (wstan2@illinois.edu) Table of Contents 1) Introduction to Team: ......................................................................................................................... 2 2) Selection of project: ............................................................................................................................ 2 3) Project Background: ............................................................................................................................ 2 4) Steps followed during implementation: ............................................................................................. 2 5) Code walkthrough:.............................................................................................................................. 3 6) Steps to run the application: ............................................................................................................... 3 7) References: ......................................................................................................................................... 3 1) Introduction to Team: We formed a team of three people to work on this project. All of us are passionate about the Data mining concepts were interested on exploring more knowledge around data mining and apply our learning from this course. Our team includes below set of people from fall season of course CS410 from University of Illinois at Urbana-Champaign: * Saravana Somasundaram (ss129) * Shashivrat Pandey (spandey6) * Walter Tan (wstan2) 2) Selection of project: After discussions, as a team we decided to proceed with Text Classification Competition: Twitter Sarcasm detection project. Other project topics that we considered are: * Automatically crawling faculty webpages * Extracting relevant information from faculty bios 3) Project Background: In this project there were two sets of data file provided to us to use in our application: * Training data (train.jsonl) * Test data(test.jsonl) We are supposed to build an application to do a prediction for Sarcasm or Not Sarcasm. The data files provided to us are twitter responses. These response texts are in context to some conversation happening in twitter feed. We were supposed to use these two datasets for training our model for this classification. Since this was a classification task, the training file also had the label for each data point as ""SARCASM"" or ""NOT_SARCASM"". Our job was to predict the same thing for all the records present in the testing file. Training file had 5000 labelled dataset and testing set had 1800 dataset. The project required some machine learning task to train the model using the training data and finally predict the outcome for test-data set using that trained classification model. The output of the application will be a text file that capture all the 1800 rows from test-data and SARCASM Vs NOT_SARCASM label. The name of the file is supposed to answer.txt. 4) Steps followed during implementation: * Analyzed the requirements for the project completion captured under document (CS 410 Project Topics - Google Docs) provided by instructors * Setup environment to execute the projects o Download train.jsonl and test.jsonl data under a folder called data o Install all the packages required for project using pip install command # Sklearn # Pandas # Nltk # Numpy * Design a framework to read train.jsonl and test.jsonl files and parse data * Process the data to remove words such as '@USER' and use the lemmatize function to clean up the data * Initialize TfidfVectorizer with the appropriate parameters * Initialize GaussianNB model and train the model using the training data * Perform predictions of labels using the test data * Write the results into answer.txt 5) Code walkthrough: We created and uploaded a video under github account that covers step by step walkthrough of our code implementation. Below is the URL and Name of the file for code walkthrough video: * URL: GitHub - ss129/CourseProject * Name: CourseProject_Demo.mp4 6) Steps to run the application: Please follow the below steps to run the application and generate the results. * Clone the below GitHub repository into your local machine - GitHub - ss129/CourseProject * Install the below packages using pip install command o Sklearn o Pandas o Nltk o Numpy * Execute the python program twitter_sarcasm_classification.py * The results will be available in the file data/answer.txt 7) References: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://realpython.com/python-keras-text-classification/#convolutional-neuralnetworks-cnn c. https://towardsdatascience.com/classification-using-neural-networks-b8e98f3a904f d. https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis- 84cb013bb6db"
https://github.com/ss129/CourseProject	CS410 - Twitter Text Classifier.pdf	Project Proposal - Text Classification Competition 1. Team Name: SSW Classifiers Members: * Saravana Somasundaram (captain) - ss129 * Shashivrat Pandey - spandey6 * Walter Tan - wstan2 2. Competition - Text Classification 3. Neural Networks we are looking to explore for Text classification: a. Convolutional Neural Network (CNN) b. Recurrent Neural Network (RNN) c. Hierarchical Attention Network (HAN) 4. References we have looked so far: a. https://keras.io/examples/nlp/text_classification_from_scratch/ b. https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han- f0e887214d5f c. https://realpython.com/python-keras-text-classification/#convolutional-neural- networks-cnn d. https://towardsdatascience.com/classification-using-neural-networks-b8e98f3a904f 5. The programming language we will be using is Python. Our group is prepared to learn how text classification algorithms can be implemented using state-of-the art neural networks such as Convolutional Neural Network, Recurrent Neural Network and Hierarchical Attention Network. We are excited to learn techniques to improve our ML skillset and will apply what we learned to our current/future work projects. Some Deep Learning frameworks we've heard of include PyTorch, TensorFlow, Keras, and Sonnet. TensorFlow and PyTorch seem to be the most popular and used by many users and institutions worldwide. Our group has never worked with these technologies, but are excited to learn these new technologies for this competition. We will be using Python for this project and we are confident that we will come up with an optimized code to improve the performance of application.
https://github.com/ss129/CourseProject	ProjectProgressReport.pdf	Project Progress Report: Below is the progress we made so far on our project: 1) Which tasks have been completed? * Read through the instructions for text classification project. * Had few meetings with all group member to understand the requirements and instructions * Created a Github repository for project, cloned it from original repository and read through Readme.md file * Analyzed train and test data 2) Which tasks are pending? * Working on framework to read the file and scan through each JSON document * Next step is to apply a text classification logic to decide each document as Sarcasm Vs Not_Sarcasm 3) Are you facing any challenges? * We did not face any challenges as of now
https://github.com/ss129/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/tmm-ai/CourseProject	CS 410_ Final Project Progress Report_ .docx	"CS 410: Final Project Progress Report: Sarcasm Detection in Twitter Posts Date: November 29, 2020 Team: Tardy Slackers Progress Thus Far: We have a decent ""first draft"" python code that is working with the provided training set. For the input of the model, we are combining the response and context tweet. We then use a huggingface tokenizer to get the tokens, masks, and special token objects. We started working with the RoBERTa model that was developed by Google. Since we were working with the free version of Google Colab, we froze all the parameters to use as embeddings and then ran it with another LSTM layer, which means we are only changing LSTM weights at this point, to fit the model into memory. The results of this locally got a .8 F1 but on the official test set, we only got around .65 F1 which is about 0.08 below the baseline score. Remaining Tasks Upgrade to a paid plan of Google Colab GPU so we get more processing capability Run our program with the full BERT/ROBERTA model, and possibly other models. Experiment with different values for epochs, multiple GPUs, various learning rates, stacking models together, and using external data to see what delivers better/best performance to reach the baseline threshold. Current Challenges The familiarity and comfort with Google Colab, BERT/ROBERTA model, and NLP programming tasks such as this project vary significantly across team members. Those will less familiarity will be focused on getting more fully up to speed in the coming weeks and adjusting our model to achieve SOTA."
https://github.com/tmm-ai/CourseProject	CS410 Final Project_ Text Classification _ Twitter Sarcasm (1).pdf	"CS410 Final Project: Text Classification / Twitter Sarcasm Team: The Tardy Slackers Wei Dai:  weidai6@illinois.edu Michael Huang:  mh54@illinois.edu Tom McNulty:  tmcnu3@illinois.edu Demo video / code walkthrough:  https://youtu.be/OUu71EapO5U An overview of the function of the code (i.e., what it does and what it can be used for). The function of the code provided is text classification. Specifically, the model is meant for classifying whether a response tweet given its context is sarcastic or not. While this code can be used for classifying any type of text into any number of classifications with some minor adjustments, the code has been specifically designed to fit the text in the provided testing and training files. The program takes in the provided 3-part Twitter interactions made up of 1 tweet that is a potentially sarcastic response and two tweets that occurred just before the response. These two tweets are collectively the ""context"". We use the RoBERTa pretraining language model. RoBERTa builds on the original BERT program which stands for Bidirectional Encoder Representations from Transformers, or BERT, is a revolutionary self-supervised pretrained architecture that has learned to predict intentionally hidden (masked) sections of text. RoBERTa builds on BERT's language masking strategy and modifies key hyperparameters in BERT, including removing BERT's next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT. After reading in and preparing the data, we tokenize the data, train the RoBERTa model, do a prediction on the validation set, prepare the test input, perform a prediction on the test set and then prepare the output, which is a text document simply listing out where each line in the test file is sarcasm or not. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Code: All code is labeled with comments for easy understanding. We extensively use the HuggingFace library for tokenization and model loading. We use Tensorflow to train the model. Data Preparation: 1. Have train.jsonl and test.jsonl in folder called data in present working directory 2. We used an 80-20 train-val split 3. We simply combined the context together and appended it to the response tweet and used that as a string, using only the first 128 tokens in our model as max input sequence length Training: 1. Tokenize data with huggingface tokenizer to get encodings, then put in a tensorflow dataset for training 2. Start training from pretrained roberta-large and finetune with our dataset for 3 epochs with Adam optimizer with learning rate of 2e-5 with batch size of 16. Validation and Test results: 1. Predict on validation and get f1 score, then predict on test set and write to answer.txt file and upload to leaderboard 2. Model achieves .84 F1 score on validation set(internally) and .78396226 F1 on test set / leaderboard(under tmmai) Experimentation: For the parameters, we adjusted the max length for tokenization, training encodings and validation encodings. We tried lengths of 64, 80, 128, 150, 256. We also experimented with adjusted the learning rates from 2e-5 up and down just slightly, such as 1e-5 and 3e-5 but these met with poor results. We also tried Bert-large, but this gave worse results than roberta-large. Maxlength of 128 for tokenization, training and validation with a learning rate of 2e-5 delivered the best results. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run the software, whichever is applicable. Two ways to run code: as python scripts or on google colab as notebook For running python scripts: 1. Clone the repo 2. Cd into repo and run pip install --upgrade -r requirements.txt, make sure you are using python3 3. Run python cs410_FINAL.py for training and testing. If you want to simply test on validation data and test data, then run python cs410_FINAL_EVAL.py 4. Both codes will evaluate on validation set and print f1 score 5. It will also write a data/answer.txt file with predictions on tests 6. If you use the training script, it will also save the model in huggingface hf format as data/roberta_model. In this folder, it will contain the tensorflow .h5 model 7. If using validation script, download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2 . Place these files in a folder called roberta_model within the data subdirectory i.e. data/roberta_model For running the colab notebook: 1. Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2 . 2. Get our train.jsonl and test.jsonl files and place them in your google drive in subdirectory called data i.e. drive/MyDrive/data/train.jsonl 3. Run through our notebook, https://colab.research.google.com/drive/1zcMMw8xe6vh9rMPlBB_i-HZGrxsk5UJj#scroll To=Qbr6a4LYcl_w , but skip training step. Instead, load our model into your drive and run on validation and test set. Then, submit answer.txt from last cell to livedatalab. Link to the demonstration video / code walkthrough: https://youtu.be/OUu71EapO5U Brief description of the contribution of each team member in case of a multi-person team. Wei Dai: Researched ideas, attempted to develop code in parallel to the team. Studied theories under BERT and RoBERTa. Actively discuss questions with other team members. Learned tensorflow, transformers and other libraries. Tried to discover parameters following Michael and Tom's draft. Michael Huang: Researched ideas/models and determined RoBERTa was likely our best model to use. Developed the best working code of the team, trained the RoBERTa model to beat baseline, and provided good resources for the team to learn more about implementing BERT and RoBERTa. Also created python script to run model and contributed heavily to documentation showing how to run software. Tom McNulty: Team captain, set up meetings, drafted the first drafts of all deliverables and requested team members to edit. Researched initial ideas and models, tried to develop a decent draft of text classification code in parallel with other team members but Michael developed the superior draft. Extensively experimented with various hyper parameter and Colab settings to get to optimal results. Contributed heavily to final documentation. Citations / Resources RoBERTa model:  https://arxiv.org/abs/1907.11692 BERT model:  https://www.aclweb.org/anthology/N19-1423/ Huggingface blog / libraries:  https://huggingface.co/transformers/model_doc/roberta.html Keras:  https://keras.io/api/ ** A lot of code utilizes huggingface and keras api's"
https://github.com/tmm-ai/CourseProject	CS410_Project_Proposal_Tardy_Slackers.docx	CS410 Final Project Proposal 10/25/2020 Team name: The Tardy Slackers What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Tom McNulty - Captain NetID: tmcnu3 tmcnu3@illinois.edu Wei Dai NetID: weidai6 weidai6@illinois.edu Michael Huang NetID: mh54 mh54@illinois.edu Which competition do you plan to join? The text classification competition - identifying sarcasm on Twitter Are you prepared to learn state-of-the-art neural network classifiers?: Yes - of course! Name some neural classifiers and deep learning frameworks that you may have heard of. We have done some research to identify good classifiers, but have not chosen exactly which one(s) we will actually use yet. Those we have heard of are Transformer-based models such as BERT, ALBERT, RoBERTa, XLNet, UniLM, Multi-Task DNN LSTMs, GRU, RNN Word Embeddings i.e. word2vec, glove, ELMO CNNs Describe any relevant prior experience with such methods Michael has experience with using these architectures. In undergrad, he participated in research projects that utilized LSTMs for NER tasks and also CNNs for some computer vision research. Also at work, he utilizes many transformer-based models to solve text-classification tasks as well as language generation tasks. (Side note: Although he does have this experience, he still finds this project appealing as it allows him to focus on improving other areas he's been meaning to work on such as writing good software, scripts, and documentation that makes it easy to install, use, and maintain) Wei has experience with data collection. At work, she utilizes many tools such as Json, goose-extractor to web scrape and crawl to build datasets for further use. Which programming language do you plan to use? - Python, most likely with TensorFlow for modeling Additional If we are able to achieve state-of-the-art performance in a reasonable amount of time, we would like to expand the capabilities of our program to do one or more of the following: Identify a rate of sarcasm or create a sarcasm score on the Twitter accounts of some well-known people such as comedians, politicians, flamboyant business people, etc. The score would be based on the person's entire tweeting history - or as much as we can get a hold of. Develop a capability to determine metaphors / idioms based on comparing figurative and literal interpretations of tweets. These concepts were presented at a recent UIUC presentation by Suma Bhat who is an Assistant Professor in ECE at Illinois https://mediaspace.illinois.edu/media/t/1_0db6ad18 Deploy our model with a REST API, so that it is easily interactable and usable by others, and it would also make for a cool demo
https://github.com/tmm-ai/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection Evaluation Please look at ""CS410 Final Project: Text Classification / Twitter Sarcasm.pdf"" for all documentation deliverables and if you want details about our code and training. This README.md is a brief summary on how to evaluate our model results. This assumes you are running our model and not training your own model to validate our results, since training your own model may give different results due to changes in initialization. More details about how to evaluate model results if anything is not clear may be present in pdf mentioned above. We provided our answer.txt in this repo. If you add this to livedatalab, you can confirm our rank on the leaderboard (under username tmmai). If you want to evaluate our results by having the model predict on the test set, then there are two options you can take: run our notebook on google colab or clone this repo and run our python scripts. Run on Google Colab Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2. Get our train.jsonl and test.jsonl files and place them in your google drive in subdirectory called data i.e. drive/MyDrive/data/train.jsonl Run through our notebook, https://colab.research.google.com/drive/1zcMMw8xe6vh9rMPlBB_i-HZGrxsk5UJj#scrollTo=Qbr6a4LYcl_w, but skip training step. Instead, load our model into your drive and run on validation and test set. Then, submit answer.txt from last cell to livedatalab. Demo video / code walkthrough of Colab verion: https://youtu.be/OUu71EapO5U Run with python scripts Clone this repo and cd into it Run pip install --upgrade -r requirements.txt (make sure you are using python3) Download our model from here: https://drive.google.com/drive/folders/1raCd-g4chwuufv1JBDNXyNxITJDMk_K2. Place these files in a folder called roberta_model within the data subdirectory i.e. data/roberta_model Run python cs410_FINAL_EVAL.py. This code will evaluate on validation set and print f1 score. It will also write in the data folder a answer.txt file with predictions on tests"
https://github.com/xw23/CourseProject	410_Project_Documentation.docx	Software Implementation: The implementation for this program comes in two parts. We decided to use a Naive Bayes methodology, which starts with a training method and then uses that data to predict classifications onto a testing dataset. We use a bag of words model in which we consider each tweet to be represented as a bag of independent words, meaning that we will ignore the position that the words appear and only deal with the frequency that they appear. Using a unigram model, the program first calls the training function found in training.py, which goes through the training data given and creates a bag of words model. Because each tweet is pre labeled as either sarcasm or not sarcasm, we can treat each tweet as a list of words using the NLTK TweetTokenizer. This method returns a list of sarcastic words and their frequencies, a list of non-sarcastic words and their frequencies, as well as totals for both sarcastic and non sarcastic words. After that, during the classification phase, the program calculates the probability for each tweet to be sarcastic or non-sarcastic based on the probabilities that were developed from the training set. The program develops a posterior probability for each tweet, and based on which is higher, classifies the tweet as either sarcastic or non-sarcastic. In order to avoid zero probabilities, we have a smoothing parameter in place, which has been optimized to give the best results. Installation and running: To install the software, first clone this git repository: https://github.com/shail4221/Classification-Competition.git Once finished, you can run the program inside by running python classify.py. This will train the data on the train.jsonl file, run the classification on the test.jsonl file, and output the results into answer.txt Description of contribution: Both members worked together mostly equally, with Xuechen focusing primarily on the training side of the program and Shail focusing on the classification side. Both team members worked together to refine and test the program for accuracy.
https://github.com/xw23/CourseProject	Progress_Report.pdf	Team25 Progress Report Team Name: Team25 Team Members: Xuechen Wang(xuechen5), Shail Desai(shailrd2) Project Topic: Reproducing A Paper * Subtopic: Pattern annotation * Paper to be recreated: Latent aspect rating analysis without aspect keyword supervision (by Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011) Progress made thus far: We retrieved the dataset, and looked in and did preliminary processing of the dataset. We read through the research paper and made some notes, and communicated our ideas about how to build the LARAM model. Remaining tasks: Write python scripts to build the LARAM model. Compare our results with the authors' results. Write and submit our project report. Challenges/issues faced: We are having a hard time to understand and implement the functions for mining latent topics and the LARA function.
https://github.com/xw23/CourseProject	Project Proposal.pdf	Team25 Project Proposal Team Name: Team25 Team Members: Xuechen Wang(xuechen5), Shail Desai(shailrd2) Captain: Xuechen Wang(xuechen5) Project Topic: Reproducing A Paper * Subtopic: Pattern annotation * Paper to be recreated: Latent aspect rating analysis without aspect keyword supervision (by Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011) Programming language to use: Python The Datasets used in the paper for evaluation: can be found in http://sifaka.cs.uiuc.edu/~wang296/Data/index.html
https://github.com/xw23/CourseProject	README.md	"CourseProject Software Implementation: The implementation for this program comes in two parts. We decided to use a Naive Bayes methodology, which starts with a training method and then uses that data to predict classifications onto a testing dataset. We use a bag of words model in which we consider each tweet to be represented as a bag of independent words, meaning that we will ignore the position that the words appear and only deal with the frequency that they appear. Using a unigram model, the program first calls the training function found in train.py, which goes through the training data given and creates a bag of words model. Because each tweet is pre-labeled as either ""SARCASM"" or ""NOT_SARCASM"", we can treat each tweet as a list of words using the NLTK TweetTokenizer. This method returns a list of sarcastic words and their frequencies, a list of non-sarcastic words and their frequencies, as well as totals for both sarcastic and non sarcastic words. After that, during the classification phase, the program calculates the probability for each tweet to be ""SARCASM"" or ""NOT_SARCASM"" based on the probabilities that were developed from the training set. The program develops a posterior probability for each tweet, and based on which is higher, classifies the tweet as either ""SARCASM""or ""NOT_SARCASM"". In order to avoid zero probabilities, we have a smoothing parameter in place, which has been optimized to give the best results. Installation and running: To install the software, first clone this git repository: https://github.com/shail4221/Classification-Competition.git Once finished, you can run the program inside by running python classify.py. This will train the data on the train.jsonl file, run the classification on the test.jsonl file, and output the results into answer.txt The video demo for running the code could be found at: https://drive.google.com/file/d/14abGsfp8Gjn4iRPimjK375e4egvLOwgn/view?usp=sharing Description of contribution: Both members worked together mostly equally, with Xuechen focusing primarily on the training side of the program and Shail focusing on the classification side. Both team members worked together to refine and test the program for accuracy. The training file is train.py, the classification file is classify.py, and the output file is answer.txt. The link to video demo is also in VideoLink.txt. Thank you."
https://github.com/JiajunWuEdu/CourseProject	Proposal.docx	Proposal of the text classification competition What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. My group has only one member. Jiajun Wu, jiajunw6. So I will be the captain myself. Which competition do you plan to join? I would like to join a text classification competition. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Though I am not very familiar with most classifiers, I decide to learn some of these and I hope they will be useful in the competition. While my interest is classification and prediction of utterances in classroom contexts, I heard of some neural network classifiers and deep learning frameworks which were employed in this area. For instance, Zhao (2016) developed a sequence predictor using three neural network classifiers (i.e. MLP, CNN, and LSTM). Donnelly et al. (2016) used the Naive Bayes classifier using the WEKA machine learning toolbox identified five key instructional segments (Question & Answer, Procedures and Directions, Supervised Seatwork, Small Group Work, and Lecture) in the classroom audio data with F1 scores ranging from 0.64 to 0.78. Suresh et al. (2019) trained a bi-LSTM network to classify the transcript of teacher-student dialogues sentence by sentence in to 6 talk moves (e.g. Restating, revoicing, pressing for reasoning, getting students to relate to another's ideas, etc.) with an F1 measure of 65%. Reference Donnelly, P.J., Blanchard, N., Samei, B., Olney, A.M., Sun, X., Ward, B., Kelly, S., Nystran, M. and D'Mello, S.K., 2016, July. Automatic teacher modeling from live classroom audio. In Proceedings of the 2016 conference on user modeling adaptation and personalization (pp. 45-53). Suresh, A., Sumner, T., Jacobs, J., Foland, B. and Ward, W., 2019, July. Automating Analysis and Feedback to Improve Mathematics Teachers' Classroom Discourse. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp. 9721-9728). Zhao, Y., Chu, S., Zhou, Y. and Tu, K., 2017, January. Sequence Prediction Using Neural Network Classiers. In International conference on grammatical inference (pp. 164-169). Which programming language do you plan to use? I plan to use Python, and C++ if it is necessary.
https://github.com/JiajunWuEdu/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/jinningli/CourseProject	CS410_Final_Report_Team_Salty_Fish.pdf	"CS410 Final Project Report Text Classification Competition: Twitter Sarcasm Detection Our Team: Team name: Salty Fish Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Introduction The final project we choose is the text classification competition. The main task of the competition is to do the sentiment analysis of a given tweet to determine whether it is sarcasm or not. We first built a model using pre-trained LSTM to do the classification. We only used the response data to train the model and we did not beat the baseline. We did not make use of the context data, and we also did some research on the state-of-art network models of the sentiment analysis. We then built a BERT model using a pre-trained BERT. We used BCEloss to finetune the parameters during training and added a fully connected layer to output the classification result. With this method we beat the baseline, and the best result we achieved is (0.6832, 0.8433, 0.7549), precision, recall and F1 score respectively. LSTM Baseline Introduction of LSTM LSTM is the abbreviation of Long short-term memory. LSTM is a special kind of RNN, which solves the deficiency of gradient vanishing and exploding during long-term training that RNNs have. LSTM consists of three gate structures:forget gate, input gate and output gate. Fig. Overall structure of the LSTM The forget gate layer is implemented by a sigmoid layer. It looks at and ,and outputs a ht-1 xt value, denoted as , between 0 and 1 for each number in the cell state , where is f t Ct-1 ht-1 the output at timestamp t-1 and is the input at timestamp t. xt The input gate layer is used to update the information in cell state. This process has two steps. The first step is to use a sigmoid layer to decide which value to get updated, denoted as , and the second step is to use a tanh layer to create a vector of new candidate values, it . Then we can update the cell state using the previous output from the forget gate and C't the input gate. The new cell state is calculated as: Ct f i Ct = t * Ct-1 + t * C't The output gate layer is to decide the output value. First, we use a sigmoid layer to decide what parts of the cell state will be output. Then, we put the cell state through tanh and multiply by the output of the sigmoid layer, denoted as , and then we can get the final ot output . ht Result of using LSTM We first tried to use the pre-trained LSTM from pytorch and we added two fully connected layers to make the classification given the output of the LSTM. In this case we get two classifications, 'SARCASM' and 'NOT_SARCASM'. The best result we get using this method is: From the result we can see that the pre-trained LSTM model without fine-tuning can not beat the baseline, this is probably because LSTM does not perform well when dealing with long term dependency problems. So we decide to try another model, BERT, to get better performances. The LSTM model is implemented by Python 3.7.0, PyTorch 1.2.0 and trained on Intel x86 CPU devices with 8 cores. The code can be found in https://github.com/jinningli/CourseProject. Please refer to the Code Manual section for the usage of our code. Precision Recall F1 Score 0.6109979633401222 0.6666666666666666 0.6376195536663125 BERT Model and Logits Ensemble BERT Model BERT is a language representation model, which is pre-trained on unlabeled text data at first and fine-tuned with an additional output layer for different NLP tasks. BERT can learn a bidirectional representation for each word on both left and right context, compared to the static word embedding learned in word2vec. BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) as shown in Fig. Fig. BERT architecture based on Transformer Encoder. Two unsupervised tasks are used to pre-train BERT. The first task is Masked language model (MLM) where some percentage of the input tokens are masked at random, and the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary for prediction. The second task is Next Sentence Prediction (NSP) where the hidden vector at [CLS] token is used to classify whether the given two sentences are continuous sentences or not. Both tasks can be trained jointly as in Fig. Fig. Two unsupervised tasks for BERT pre-training. During fine-tuning, the BERT model is first initialized with the pre-trained parameters, and a fully connected layer is added as the output layer. All of the parameters are tuned using labeled data from the downstream tasks as shown in Fig. Fig. Procedure of BERT fine-tuning. Since BERT obtained the state of the art result in eleven NLP tasks once published, variants of work are proposed based on the BERT model and push the performance of NLP tasks a big step forward such as XLNet and ALBERT. Our method is based on the BERT model. We load the weights of BERT from PyTorch-Transformers which is pre-trained on BooksCorpus (800M words) and English Wikipedia (2,500M words). A fully connected layer is added as the output layer to classify Twitter response into ""SARCASM"" or not. All the parameters are tuned in the training process with BCELoss. The Twitter dataset is splitted into training dataset with 4000 samples and validation dataset with 1000 samples. Validation dataset is used to tune the hyperparameters during training and find 2 epochs is the best since the validation loss starts to grow up after it probably due to the limited dataset. Finally, the model is trained on the whole dataset including both training dataset and validation dataset. The model is implemented by Python 3.7.0, PyTorch 1.2.0. We train the model on NVIDIA TITAN V GPU, 16 cores Intel x86 CPU machine. The code can be found in https://github.com/jinningli/CourseProject. Please refer to the Code Manual section for the usage of our code. Fig. Train Loss and Validation Loss From the figure above, we can see that the validation loss is relatively low around epoch 2-4. Then after epoch 4 the validation loss is high due to overfitting. This is mainly because the size of the training dataset is small, so in this case the model tries to overfit the data in the training dataset rather than generalize from patterns observed from the dataset. This results in the low training loss and high validation loss after epoch 4. Therefore, we only take the result of 2-4 epochs for further ensemble. In our final submission, we used all the 5000 samples (without validation) in the training dataset to train the model to get better performances. Logits Ensemble In order to further improve the performance of our model on the testset, we tried to submit several results to the LiveDataLab OnlineJudge system and collect the Precision, Recall, and F1 Score response. The scores are shown in the figure below. We find that our epoch 3 model and epoch 4 model have the similar highest F1 Score. In addition, their precisions and recalls are varied. The ensemble method should be effective by taking the average of logits values (the output of neural network). Fig. Precision, Recall, and F1 Score on Testset (Response after submission) We ensemble the logits by taking the average vglogit ogit / n a =  n i l i The prediction is the argmax of the average logits red argmax(avglogit) p = The ensemble method further improves our F1 Score from 0.7519 to 0.7549 Table 1: Ensemble Improvement Conclusion In conclusion, we built two models, an LSTM model and a BERT model to accomplish the twitter sarcasm detection. The LSTM model with a pre-trained LSTM and two fully connected layers did not beat the baseline. The BERT model with a pre-trained BERT and a fully connected layer using BCEloss beats the baseline. Code Manual Evaluate Model and Ensemble Download pre-trained model for our method Download checkpoint.zip from https://drive.google.com/file/d/1nRucz1yDqyoYR8jLeP6fKZt8FpqJBBUA/view?usp=s haring unzip checkpoint.zip mkdir checkpoint; mkdir checkpoint/run0 mv checkpoint checkpoint/run0/checkpoint Evaluate and Generating Predictions python3 evaluate.py --run run0 --use_bert --device 3 Model Precision Recall F1 Score BERT Epoch 3 0.6667 0.8622 0.7519 BERT Epoch 4 0.6876 0.8267 0.7508 BERT Epoch 3&4 Ensemble 0.6832 0.8433 0.7549 Ensemble # modify ensemble_paths in ensemble.py python3 ensemble.py All Parameters --run Evaluating on runX. e.g. --run=run1 --use_bert Use bert model. If not, using LSTM model. --device GPU device to use Train Model Start Training with Validation and Evaluation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 --use_valid --eval Start Training without Validation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 Start Training using LSTM model python3 main.py --device 2 --lr 2e-5 --epochs 20 All Parameters --epochs Number of epochs for training --batch_size Batch size --lr Learning rate for optimizer --run Continue training on runX. Eg. --run=run1 --eval Evaluate on training set --use_valid Use valid dataset. If not, using the whole dataset for training --use_bert Use bert model. If not, using the LSTM model --split_ratio When using validation, percentage of trainset --device GPU device to use Appendix Screenshot of our scores and rank:"
https://github.com/jinningli/CourseProject	CS410_Progress_Report_Team_Salty_Fish.pdf	Progress Report Team: Salty Fish Members: Jinning Li (jinning4) Jialong Yin (jialong2) Jianzhe Sun (jianzhe2) Task: Text Classification Competition (Sarcasm Detection) 1) Which tasks have been completed? 1. We have surveyed related background knowledge and previous researches about the sarcasm detection task and sentiment analysis. 2. We have implemented the code to process the tweet data. 3. We have a quick implementation on our basic LSTM model with Pytorch. The network architecture of this model includes one LSTM layers and two dense layers. We only use the response data to train the model. We currently achieve (Precision: 0.61 Recall:0.67 F1: 0.64) on the testset. 2) Which tasks are pending? 1. We are trying to develop a more complex framework which can help make use of the context data. 2. We try to make use of some popular network architectures which were proved to be effective on the sentiment analysis task. 3. Final hyper-parameter adjusting and model ensemble. 3) Are you facing any challenges? 1. The most challenge part is how to make use of the context data provided in the dataset. Every piece of tweet in the dataset is a reply to another tweet. However, sometimes they agree with each other while sometimes they are not. 2. Overfitting. Our basic model performs much better on the trainset than the testset. We have tried using regularization and reducing the parameters but the improvement is limited.
https://github.com/jinningli/CourseProject	CS410_Project_Proposal_Team_Salty_Fish.pdf	CS410 Project Proposal Team members: Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Competition to join: text classification competition Additional Questions to Answer: Q: Are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods A: The state-of-the-art neural network classifiers include Multi-layer Perceptron (MLP), Support Vector Machine (SVM), Naive Bayes, Decision Tree, Logistic Regression, etc. I have heard of some deep learning frameworks for text classification including RNN, GRU, CNN, LSTM, Bi-LSTM, Word2Vec, Conv-RNN, BERT, etc. My relevant prior experience includes training a Word2Vec model to transform the words into vectors and apply random forest and logistic regression after the embedding to classify text. Programming language: Python
https://github.com/jinningli/CourseProject	README.md	"CS410 Course Project - Text Classification Competition: Twitter Sarcasm Detection Team name: Salty Fish Team captain: Name: Jinning Li NetID: jinning4 Team member 1: Name: Jialong Yin NetID: jialong2 Team member 2: Name: Jianzhe Sun NetID: jianzhe2 Video The video to introduce our method and the usage of code can be found in Youtube https://youtu.be/eSR7B8TYnJY It's not uploaded to github because it exceeds 100 MB. Code Manual Source code can be found in ./code Evaluate Model and Ensemble Download pre-trained model Download checkpoint.zip from https://drive.google.com/file/d/1nRucz1yDqyoYR8jLeP6fKZt8FpqJBBUA/view?usp=sharing unzip checkpoint.zip mkdir checkpoint mkdir checkpoint/run0 mv checkpoint checkpoint/run0/checkpoint Evaluate and Generating Predictions python3 evaluate.py --run run0 --use_bert --device 3 Ensemble ``` modify ensemble_paths in ensemble.py python3 ensemble.py ``` All Parameters --run Evaluating on runX. e.g. --run=run1 --use_bert Use bert model. If not, using LSTM model. --device GPU device to use Train Model Start Training with Validation and Evaluation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 --use_valid --eval Start Training without Validation python3 main.py --use_bert --device 2 --lr 2e-5 --epochs 20 Start Training using LSTM model python3 main.py --device 2 --lr 2e-5 --epochs 20 All Parameters --epochs Number of epochs for training --batch_size Batch size --lr Learning rate for optimizer --run Continue training on runX. Eg. --run=run1 --eval Evaluate on training set --use_valid Use valid dataset. If not, using the whole dataset for training --use_bert Use bert model. If not, using the LSTM model --split_ratio When using validation, percentage of trainset --device GPU device to use Appendix: Competetion Data Format Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/Ehaly/CourseProject	Project Documentation.pdf	"Project Documentation Lin Yutong Yutong21@illinois.edu 1. Overview This software is designed to complete the text mining task by a contextual generative model, which is an extension of the PLSA generative model. Several texts from different collections, several parameters (lambda c, lambda b, number of clusters etc.) will be passed in, and the software will produce the matrix of clusters and top k words in each language model (common model, collection-specific models) in the form similar to the original paper shown in Figure 1. The idea is that a word could be generated from a common theme model with lambda c probability, but also has (1 - lambda c) probability to be generated from a collection specific theme model. The probability of a word given the collection is shown in Figure 2, and the mixture model is illustrated in Figure 3. Figure 1: example output Figure 2: Pd(w|Ci) Figure 3: the cross-collection mixture model 2. Implementation Details The class Corpus consists of the following functions: __init__(document_path): initialize a Corpus object. build_corpus(): read the document in document path, store the collection number and the document in self.collection and self.documents. build_vocabulary(): read the documents and build the vocabulary for the whole dataset. build_background_model(): build the background model from the whole dataset. build_term_doc_matrix(): Construct the term-document matrix where each row represents a document, each column represents a vocabulary term.self.term_doc_matrix[i][j] is the count of term j in document i. initialize(self, number_of_collections, number_of_clusters, random=True): initialize the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob. expectation_step(number_of_collections,number_of_clusters,lambda_b, lambda_c): the E-step updates the P(zd ci w = j), i.e. the topic_prob matrix, p(zd,Ci,w = B) i.e. the bg_prob matrix, and p(zd,Ci ,j,w = C ), i.e. the common_topic_prob matrix. maximization_step(number_of_collections, number_of_clusters ): the M-step updates the , and . calculate_likelihood(number_of_collections,lambda_b, lambda_c): Calculate the current log-likelihood of the model using the model's updated probability matrices. ccmm(number_of_collections, number_of_clusters, max_iter, lambda_b, lambda_c, epsilon): execute the text mining on the document passed in in max_iter times of iteration. In each iteration, execute the E-step and the M-step, calculate the likelihood. Stop when the likelihood converges and print the topic models. The function ccmm(*) is the core function of the project. The update of each matrix and the calculation of likelihood are based on the following equations. Figure 4: log-likelihood calculation Figure 5: E-step and M-step updates 3. Usage Documentation The project uses two examples to test the text mining performance, both are similar from the example in the original paper. The data in the first example was scraped from BBC and CNN websites. The news URLs were selected by the author, so the contents are different from the news used in the original paper. The data in the second example was scraped from BestBuy.com, which are customer reviews on three kinds of laptop (Macbook-air-13-3- laptop, Dell-g5-15-6-fhd-gaming-laptop, Lenovo-yoga-c940-2-in-1-14-touch-screen-laptop). The scraper code and the scraped texts could be found in the project folder. To run the scraper code, run ""jupyter notebook"". The code could be run by command ""python model.py -h"". To run the first example, ""python model.py --document wars_news.txt --clusterNumber 5 - -collectionNumber 2 --c 0.25 --b 0.91"" To run the second example, ""python model.py --document laptop_reviews.txt -- clusterNumber 4 --collectionNumber 3 --c 0.7 --b 0.96"". Notice here we set a smaller cluster number than the original paper, due to the content difference and the worse performance with 8 clusters in experiment. The result will be saved in results.txt. An example output of the first example text mining. The collection 0 is the Iraq-theme model, and the collection 1 is the Afghanistan-theme model. 4. Work Distribution This is an individual project. All work was done by the author."
https://github.com/Ehaly/CourseProject	Project Proposal.docx	"Project Proposal Yutong21 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. It's an individual project. Yutong21. Which paper have you chosen? A Cross-Collection Mixture Model for Comparative Text Mining. Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes. The data is from CNN news and from epinions.com. Though we don't know which slice exactly, I could download similar data from those sources. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N.A"
https://github.com/Ehaly/CourseProject	Project_Progress_Report.pdf	"Project Progress Report Yutong Lin NetId: yutong21 Project Introduction The project is to reproduce the paper ""A Cross-Collection Mixture Model for Comparative Text Mining"". The project is in its final phase so far. The program is improved based on the PLSA model, which includes an expectation algorithm step, a maximization algorithm step and a likelihood calculation step. The new mixture model has more matrix to update. In the PLSA model, there is a document topic coverage matrix and a topic-specific language model matrix. In the new CCMM (Cross- Collection Mixture Model) has a collection-specific topic language model. It is designed as a 3D array. I finished the code and tried running data to test its performance. The original paper used two datasets for experiments. The first is a news dataset including about 30 articles on Iraq War and 30 articles on Afghanistan War from BBC and CNN between 2003 to 2004. I wrote a scraper to scrape from BBC and CNN websites (scraper code in news_scraper.ipynb, scraped data in wars_news.txt ). The second dataset is customer reviews on three types of laptop from Apple, Dell and IBM from epinion.com. However, the epinion.com website has been shut down, so I scrape 84 customer reviews from BestBuy.com on the M1 chip Macbook Air, Dell g5 fhd gaming laptop and Lenovo Yoga c940 (scraper code in reviews_scraper.ipynb, scraped data in laptop_reviews.txt). The model performs well and could print the common theme model and the collection- specific theme model matrix in terminal by command ""python model.py"". The format is the same as the original paper. The column stands for different clusters, and the probability of the word decreases by row. Remaining Challenge The remaining challenge is to find a better set of parameters to pass in, that could make each cluster more distinct make the top 5 words more representative. At present, the clusters seem to be too much for the laptop review dataset, so each cluster is not very distinctive. Also, there remains a question in the maximization step, I shall contact the professor to check my implementation."
https://github.com/Ehaly/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Please read Project_Documentation for more details. The video could be found at : https://mediaspace.illinois.edu/media/t/1_n26fno8k To run the scraper code, run ""jupyter notebook"". The code could be run by command ""python model.py -h"". To run the first example, ""python model.py --document wars_news.txt --clusterNumber 5 --collectionNumber 2 --c 0.25 --b 0.91"" To run the second example, ""python model.py --document laptop_reviews.txt --clusterNumber 4 --collectionNumber 3 --c 0.7 --b 0.96"" The class Corpus consists of the following functions: init(document_path): initialize a Corpus object. build_corpus(): read the document in document path, store the collection number and the document in self.collection and self.documents. build_vocabulary(): read the documents and build the vocabulary for the whole dataset. build_background_model(): build the background model from the whole dataset. build_term_doc_matrix(): Construct the term-document matrix where each row represents a document, each column represents a vocabulary term.self.term_doc_matrix[i][j] is the count of term j in document i. initialize(self, number_of_collections, number_of_clusters, random=True): initialize the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob. expectation_step(number_of_collections,number_of_clusters,lambda_b, lambda_c): the E-step updates the P(zd ci w = j), i.e. the topic_prob matrix, p(zd,Ci,w = B) i.e. the bg_prob matrix, and p(zd,Ci ,j,w = C ), i.e. the common_topic_prob matrix. maximization_step(number_of_collections, number_of_clusters ): the M-step updates the matrices document_topic_prob , topic_word_prob and collection_topic_word_prob calculate_likelihood(number_of_collections,lambda_b, lambda_c): Calculate the current log-likelihood of the model using the model's updated probability matrices. ccmm(number_of_collections, number_of_clusters, max_iter, lambda_b, lambda_c, epsilon): execute the text mining on the document passed in in max_iter times of iteration."
https://github.com/dilipis/CourseProject	Final Project Proposal.pdf	Project Proposal Document 1. This will be an individual project. Name: Dilip Ravindran NetID: dilipr2 2. Text Classification Competition 3. I am prepared to learn state-of-the-art neural network classifiers. I have heard of deep learning frameworks like TensorFlow and PyTorch. I am also aware of classifiers like LSTM, GRU, BERT etc. However, I do not have any working experience in any of these tools/ frameworks. I hope to use this as an opportunity to get familiar with these frameworks and get some hands-on experience. 4. Python
https://github.com/dilipis/CourseProject	Progress Report.pdf	Progress made * Figuring out how BERT can be used for text classification * Setting up Google colab and running some sample text classifications Remaining tasks * Implement the solution * Documentation * Creating presentation Challenges * Figuring out an optimal model * Need to devote a lot of time in the coming days as progress has been slow.
https://github.com/dilipis/CourseProject	README.md	Sarcasm detection using BERT This project uses NLP techniques to classify if tweets are sarcastic or not. BERT is used to train the model and arrive at the predictions. How to run the code The executable code resides in the file Sentiment_Analysis_with_BERT.ipynb. This code needs to be directly executed from Google Colab. Click on the button below to open the file in Colab. Once in Colab, the code needs to run on a GPU. From Colab, navigate to Edit> Notebook Settings. Select GPU from the Hardware accelerator dropdown The notebook can be executed by executing all the code blocks in order by clicking on the black 'Play' button at the top of each block. In the end, all the predictions are stored in answer.txt in the output folder in the workspace. A video tutorial is available HERE How the code works This project uses BERT (Bidirectional Encoder Representations from Transformers) which is a state-of-the-art machine learning model used for NLP tasks. BERT is a pre-trained NLP model which can be further trained to solve several text classification problems. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The HuggingFace Transformers library is used to get the BERT model that works with Tensorflow. This is how the code works at the high level Copy the testing and training data from github to the Colab workspace Read the testing and training data from jsonl file and convert them into a csv file Clean the input data by removing URL and USER tags from the tweets Split the training dataset into training and validation. This will be used to train the model. Extract only the required columns for further processing. Create the BERT model and tokenizer Convert the training and validation data into the BERT format using the helper functions defined above Use model.compile to set the optimizer, loss function that BERT will use to train the model Call model.fit to actually train the model based on the training and validation data Make predictions on the test data based on the trained model. Write the resuts to answer.txt in the output folder in the workspace. Dependencies python tensorflow transformers pandas sklearn os urllib jsonlines csv References Sentiment Analysis in 10 Minutes with BERT and TensorFlow by Orhan G. Yalcin FigLang2020-Sarcasm-detection Github
https://github.com/Xinyihe123/LARA	Progress Report .pdf	1) Progress made thus far: Our group is working on reproducing a paper of Latent Aspect Rating Analysis. The progress we made thus far includes the followings. All of our group members watched the lecture videos and read through the paper in order to understand the concepts of LARA and its fundamental difference with LRR. We have found sample Java code online implementing LRR and will start from here, write our own Python code implementing LARA. We have collected data sets and finished the preprocessing: remove the reviews with missing aspect rating or document length less than 50 words, convert to lower cases, removing punctuations and stop words. 2) Remain task: We still need to read paper and understand the equations, and to finish our implement of the aspect modeling module. The model also needs validation, and we need to use it to produce results similar to the paper. 3) Any challenges/issues being faced The equations provided in the paper are complex, it takes lots of time to try to understand them. Besides, we also need to read the paper described LRR model to implement LARA. Since the functions are complex, this task requires heavily coding and debugging.
https://github.com/Xinyihe123/LARA	Proposal.pdf	Course Project Proposal Topic: Reproduce A Paper 1. Team member: a. Xinyi He, netid: xinyihe4; Dingsen Shi, netid: dingsen2; Qunyu Shen, netid: qunyus2; b. Captain: Xinyi He 2. The paper we choose is : Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3. We plan to use Java, since the model we need to reproduce according to the paper is based on LRR, whose source codes language is Java. 4. We find the datasets used in the paper from http://times.cs.uiuc.edu/~wang296/Data/
https://github.com/Xinyihe123/LARA	README.md	CS410 CourseProject -- Reproduce a paper Documentation: Team LARA Datasets from http://times.cs.uiuc.edu/~wang296/Data/ References: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. The codes in LRR are downloaded from Internet. These are references. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. Presentation: https://mediaspace.illinois.edu/media/t/1_fo2gtfej files: clean.py: data preprocess: First we remove the reviews with any missing aspect rating or document length less than 50 words (to keep the content coverage of all possible aspects). Then we convert all the words into lower cases and remove punctuations and stop words. In vocab.txt we write vocabulary appearance based on reviews. If a word appears in several times in the same review, it would only be counted as once. We then filtered out words that have less than ten occurences. load.py: build matrix for reviews and generate results. Load data for testing. lara.py: The LARA model, mainly the aspect modeling part, using EM algorithm.In this program, we implemented function such as update_mu, update_beta, E_step, M_step etc. Gererated the alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector. The overall rating for the review can be drawn from the Gaussian distribution with mean alpah.T dot product s, and variance delta. Data: The test data we use, download from http://times.cs.uiuc.edu/~wang296/Data/: TripAdvisor Data Set: JSON Results: High rating words collections and the aspect rating weight for each reviews represented as matrix. LRR : Downloaded from Internet. Source: Hongning Wang, Yue Lu and Chengxiang Zhai. Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach. The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'2010), p783-792, 2010. The LRR model implemented by Hongning Wang using Java We used some codes in LRR, since the LARA we built is a generative model of LRR. We used some parameter initilizing codes in LRR. Required packages in Python3: numpy scipy nltk Project Members: Xinyi He Weijiang Li Dingsen Shi Qunyu Shen Ziyuan Wei We decided to work with another team to build this model since the challenge we were facing when trying to understand the methods and alogrithm are extremely hard. Implementation of Model: The inputs of this model for each review d are: (epsilon, gamma, beta, mu, sigma, delta), with hidden parameters: sigma_inverse, alpha, sigma_square, eta, phi. Epsilon, gamma, beta, mu, sigma, delta, phi, alpha, eta and the rating vector r are used in calculate log-likelihood. The outputs are alpha and s, which are the review-level k dimensional( 7 for our data) aspect weight vector and rating vector, used for gererate the final overall ratings of review d. The detail of this model can be found in the paper. This model involves more than ten parameters, some of them are generated by mutivariate Gaussian distribution, variational distribution, Dirichlet distribution, and multinomial distribution, and they are updated using gradient based method, which are hard to implement and transfer the complex math equations into codes. We spend most of our time on reading and understanding the paper and the math methods in the paper. Thus although we spent more than 20*5 hours on this project, we can only produce a simple and crude model and test one dataset in the paper. Team contribution: Since each steps and parameters in the EM algorithm are closely related to each other, we usually coded and debuged together through zoom, all of our teammates contributed their 100% effort on this project.
https://github.com/mackteng/CourseProject	CS410 Project Report.pdf	CS410 Project Report Cross-collection Mixture Model for Comparative Text Mining Function Overview: This software implements the method described in the paper A Cross-Collection Mixture Model for Comparative Text Mining. The paper proposes a method for discovering common themes across all the collections and for each theme, discover what is unique to a particular theme for each collection. In other words, for k topics among c collections, k common themes are discovered along with kc special themes for each collection-theme pair. A theme is modeled as probability distribution over words. Implementation: The skeleton of the code is similar to that of MP3. Models to be learned are randomly initialized, hidden latent variables are declared and the EM algorithm is implemented according to the formula listed below (copied from paper): One of the issues we ran into when implementing were underflow errors due to very miniscule probabilities. Our workaround was to pad these very small probabilities to a predefined number (we used 1e-600). Usage: To run the code: python3 cross.py {collectionName} where collectionName is the name of the folder under data/collections. Inside each folder under data/collections, there should be N files that make up the N collections, with each line in each of the N files representing a document. Our provided data includes one for wine (pinot noir and chardonnay) and covid-related articles by region (usa, asia, europe). Words are tokenized using the nltk tokenizer and any word < 3 characters are discarded to remove too much background noise in the theme models. Example output: Contribution: Mackt2: Implementation of cross-collection mixture model - (EM algorithm) Documentation (Progress Report) Hhc3: Debugging of code - research using log-space and padding to avoid underflow errors Data scraping - Sanitization and manipulation of various datasets found on kaggle (ad-hoc scripting for various formats, so not included in code) Presentation (Video)
https://github.com/mackteng/CourseProject	Progress-Report.pdf	Progress Report Captitan: Mack Teng (mackt2) Member: Hsin Hsien Chung (hhc3) Progress: * Finished parsing and filtering the data sets * Finished building the matrice * Implementing cross-collection mixture model EM algorithm Challenges: * Originally, we were planning to crawl data from the web, but it took longer than expected. We used existing data sets from Kaggle.com instead. * Some data are duplicated * For the mobile phone review data set, some of the comparison is not valid. Because some mobile phone models are outdated, but we do not have starting sales time or review time to distinguish them, we need to come up with a way to make a valid set for comparison. Therefore, we filtered the data with price range. Remaining Tasks: * Might need to compare with filtered data sets or unfiltered data sets * Add documentations * Create presentation
https://github.com/mackteng/CourseProject	Proposal.pdf	Project Proposal Team Name: Mbouncy Team Members: Captain: Mack Teng (mackt2) Member: HsinHsien Chung (hhc3) Paper Chosen: A Cross-Collection Mixture Model for Comparative Text Mining (http://www.ifp.illinois.edu/~velivell/ctm4.pdf) Programming Language: Python Dataset: Not exactly the same as the original dataset, but we will crawl for/aggregate document collections in a similar fashion as described in the paper.
https://github.com/mackteng/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/gotplt/CourseProject	final-report.pdf	"ManBearPig: Final Report spiros thanasoulas st19@illinois.edu December 15, 2020 Description The manbearpig project is an attempt to understand parts of the mandoc system and to lay the foundations for full text search in it. Mandoc is a set of tools that display and index man or mdoc files to users of UNIX systems. The main purpose of these files are to provide documentation for commands, APIs, system components etc in a consistent way to the users. The Mdoc language and its history UNIX manual pages originally were written in a language called roff which itself was a descendant of an even earlier system called RUNOFF originally written by Jerry Saltzer for the Compatible Time Sharing System around 1964. That language provided macros that controlled the typesetting of text, like .CENTER for centering or .br for line breaking. Around 1970 it was rewritten as roff by doug mcIllroy and bob morris , and then it was ported to the UNIX system by ken thompson. There it was first used for documenting the aspects of the system and since then it has remained the preferred way for performing these tasks. The GNU system has also brought forward the info system which is similar, but it hasn't gained widespread adoption. Mdoc originally appeared as a troff macro package in 4.4BSD. It was then significantly updated by Werner Lemberg and Ruslan Ermilov in groff-1.17. The standalone implementation that is part of the mandoc utility we use for this project written by Kristaps Dzonsons and it appeared in OpenBSD 4.6. Mdoc allows the semantic annotation of words and phrases , and also supports document hyperlinking. In an mdoc document, lines beginning with the control character '.' are called ""macro lines"". The first word is the macro name. It consists of two or three letters. Most macro names begin with a capital letter. The words following the macro name are arguments to the macro, optionally including the names of other, callable macros. Lines not beginning with the control character are called ""text lines"". They provide free- form text to be printed the formatting of the text depends on the respective processing context which is controlled by the parent macro. An example some mdoc test for a fictional utlity called ""progname"" could be as follows .Dd Mdocdate .Dt PROGNAME section .Os .Sh NAME .Nm progname .Nd one line about what it does ."".Sh LIBRARY .""For sections 2, 3, and 9 only. .""Not used in OpenBSD. .Sh SYNOPSIS .Nm progname .Op Fl options .Ar .Sh DESCRIPTION The .Nm utility processes files ... Searching mdocs Traditionally the manual pages can be searched with a command names apropos (originating from the french expression `a propos, which means ""about""). Also to make things even more confusing, another way to search manual pages has been invoking the man command with a -k (keyword) flag. In the mandoc system although this syntax exists for compatibility, it just invokes the apropos command on the backend. This is a point where across UNIX systems, things can begin to diverge greatly. In the mandoc system that we are examining the apropos and whatis utilities query manual page databases generated by the makewhatis 1 command, By default, apropos searches for makewhatis databases using case-insensitive extended regular expression matching over manual names and descriptions (the .Nm and .Nd macro keys). In the mandoc system these databases are basically hashtables based on the ohash open hashing helper functions written originally by Marc Espie for OpenBSD. On other implementations though like the GNU ones in some linux distributions (again, this means not mandoc, but completely different manual page systems, which are presented here just for reference) the databases are implemented differently. Below are the options for the system shipped with debian linux of the mandb database formats and how it compares with mandoc in terms of async access, database naming and backend. Name Type Async Filename mandoc db Hashed (ohash) Yes section/$arch/title.secion debian man/Berkeley db Binary tree Yes index.bt debian man/GNU gdbm Hashed Yes index.db debian man/UNIX ndbm Hashed No index.(dir--pag) As we have seen the current search functionality for all systems allows the query of very specific keywords in very specific parts of the document. Mandoc greatly improved the state of the art when it appeared because it also allowed certain semantic search capabilities. For example you could perform and/or operations on different macros to refine your search result. But still the result would be just the manual page name and section, and the keyword would have to be in an easily indexable part of the mdoc, because as it can be seen from the example above, the free text is intermixed with typesetting information. Small steps forward Extracting text One good addition to the system would be to enable full text search. To do so we first have to extract the text in as a pure form as we can, and then somehow index it. For this task a small utlity was written (which borrows heavily from the demandoc command) to extract relevant text from an mdoc page. The current result is far from perfect as it needs to make decisions about , spaces, linebrakes , capitalization etc, but it still achieves the goal for the most part. It is able to extract text without formatting macros. The relevant code for this lives under code/extract text.c, and it works by recursively parsing the mdoc structures (they can be embedded) in order to output only the words that are not language tokens. matching using trigrams Since the makewhatis database is already in a hashtable format, it would make sense to choose a represen- tation that maps well to that backend if it is to ever be merged in the main codebase. We want the user to be able to enter a small set of words and to fetch the results of the manual page this text sequence exists in. Also we would like for the user to have the ability to find a matching line of text midsentence. Consider for example a part of a manual page stating that ""A manual page consists of several sections."". If we only kept an inverted index of words and our user wanted to look for the string ""age consist"", our system could lead him quite astray since none of the words page and consists match the query properly. Therefore as discussed before in the proposal documents, we will follow the approach that Russ Cox used while imple- menting the backend for google code search, which consists of splitting the text in tri-grams and storing their occurences. Under the trigram transformation the word ""word"" creates the set of the following trigrams w, wo, wor, ord, rd , d . An example program that perfors this transformation on its arguments can be found in code/words to trigrams.c. Running the example code Under the directory code/ there is a Makefile that builds the two binaries. Under the directory code/input/ exist some sample files for input to the test programs. For the code to be compiled the mandoc source code 2 should be compiled and existing at the same level of directory as the courseproject code. Below are some sample runs of the two provided binaries. >cd code ; >make ; cc =c =I . . / . . / mandoc/ e x t r a c t t e x t . c cc e x t r a c t t e x t . o =L . . / . . / mandoc/ =L / l i b /x86 64=linux=gnu/ =lmandoc =l z =o e x t r a c t t e x t cc =o words to trigrams words to trigrams . c >./ e x t r a c t t e x t input / apropos . 1 ; operating on f i l e input /apropos .1 October APROPOS NAME apropos whatis search manual page databases SYNOPSIS apropos afk f i l e path path outkey arch sect ion expression DESCRIPTION The apropos and whatis u t i l i t i e s query manual page databases generated by makewhatis evaluating expression f o r each f i l e in each database By default they display the names sect ion numbers and d e s c r i p t i o n l i n e s of a l l matching manuals By default apropos searches f o r makewhatis databases in the default paths s t i p u l a t e d by man and uses case=i n s e n s i t i v e extended regular expression matching over manual names and d e s c r i p t i o n s the and macro keys Multiple terms imply pairwise whatis i s synonym f or apropos The options are as f o l l o w s : Instead of showing only the t i t l e l i n e s show the complete manual pages j u s t l i k e man would I f the standard output i s terminal device and i s not $\ ldots$ \\ >./ words to trigrams the quick brown fox jumped over the lazy dog arg : the trigrams [ t ] [ t h ] [ t h e ] [ h e ] [ e ] arg : quick trigrams [ q ] [ q u ] [ q u i ] [ u i c ] [ i c k ] [ c k ] [ k ] arg : brown trigrams [ b ] [ b r ] [ b r o ] [ r o w] [ o w n ] [w n ] [ n ] arg : fox trigrams [ f ] [ f o ] [ f o x ] [ o x ] [ x ] arg : jumped trigrams [ j ] [ j u ] [ j u m] [ u m p ] [m p e ] [ p e d ] [ e d ] [ d ] arg : over trigrams [ o ] [ o v ] [ o v e ] [ v e r ] [ e r ] [ r ] arg : the trigrams [ t ] [ t h ] [ t h e ] [ h e ] [ e ] arg : lazy trigrams [ l ] [ l a ] [ l a z ] [ a z y ] [ z y ] [ y ] arg : dog trigrams [ d ] [ d o ] [ d o g ] [ o g ] [ g ] Next steps Since we now have seen that we can succesfully extract words from the mdoc format and generate trigrams for words we would need to create a hashtable using the ohash functions that will resemble the existing makewhatis databases. Currently the values of these hashes are just the name and the section of the manual page, but since we can give the ability to match anywhere in a page , it would also make sense to at least provide a file pointer to beginning of the actual text. This would be a very hard problem since we can not correlate where something is going to be rendered as a position , to where the text is on the mdoc document. Therefore it would make more sense to just give the name of the page back and then use our system's pager (less/more etc) to navigate to that exact string. 3"
https://github.com/gotplt/CourseProject	progress-report.pdf	Progress Report for ManBearPig spiros thanasoulas st19@illinois.edu November 28, 2020 Status As outlined in the project proposal the goal of this project is to document the current status and lay the foundation for providing full text search capabilities to the mandoc system. Mandoc is a modern implementation of a manual page system https://mandoc.bsd.lv and part of the project is to identify its current search capabilities (how the command apropos / man -k with a keyword works) and provide a design and parts of an implementation for a full text search option on it. Progress made so far I have analyzed the current way that apropos accepts search terms and passes them to the current database implementation. Also i have designed an initial algorithm based on n-grams and a matching database format that could replace (or go side by side) with the current database in order to provide some basic full text search capabilities. I have also written a scraper which can consume the mandoc language and extract the words that are to be fed to the database builder. Remaining tasks Actually implement a database format that conforms to the apropos search api. Connect it to the apropos code and issue queries. Write tests to make sure that the words that have been inputed have matching n-grams in the correct format in the binary database. Write the report that describes the work done Challenges faced Being a large codebase it requires many hours of familiriazing with a part of it before you can start making sense of how to interface with it. Also being a pure C project , always requires treading carefully with things like memory management, IO etc so the progress done is slower than it would be in a completely dynamic/scripting language working with a smaller framework. members st19 / solo project 1
https://github.com/gotplt/CourseProject	proposal.pdf	ManBearPig. An attempt at providing native full text search to the mandoc system. spiros thanasoulas st19@illinois.edu November 2, 2020 Description The goal of this project would be to create a report and possibly code improvements towards providing a backend that supports Full Text Search capabilities for the mandoc project (https://mandoc.bsd.lv/). Background UNIX system provide their documentation to the user through a set of tools collectively referred to as the Manual Page system. The well known man(1) command exists today on all UNIX systems but even on other platforms like MacOSX and android. Searching efficiently keywords and semantics has been of paramount importance for the user to quickly get to the relevant manual page and the command apropos(1) traditionally served that purpose, meaning doing database lookups. The databases are built with the makewhatis(1) tool. Project proposal We will investigate the C source code of the mandoc project, targeted on the modules of searching and database creation. The goal of this project would be to lay a path for full text search capabilities from the apropos command. Currently only certain words of a manual page are indexed and their semantic information stored with them, in a persisted to a file database that on the outerlevel is implemented as a hash map. To allow for the full text search capabilities we will implement a database based on trigrams keying an inverted index of the full text being contained in a manual page, after it has been parsed from the mdoc parsers and only the content remains. In detail the goal of the project would be to create the equivalent database of the makewhatis(1) db that is currently created, but which stores the trigrams. Due to lack of time no optimizations for very large databases are going to be implemented and the testing input will be constrained enough to make sure the datastructures will be able to fit in memory. The generated database will be evaluated by dumping the contents and making sure all the trigrams that should be produced and only those are contained within it. A test harness to ensure that will be provided. If time permits the search capabilities will be attempted to connect to the database through the apropos command and query for a text string. note to the reviewer: although i would love to finish the whole thing but it might be unfeasable in around 25hrs that i have budgeted it for it. My intention though is to lay the foundation so that a patch will be eventually merged in the mandoc codebase, not to demo something that noone will ever user Proposed Workflow We propose that the analysis and development will be split across 5 6hr man - days of work Day 1 Code and Documentation analysis. Reviewing the makewhatis utility and the resulting databases it creates examine the relevant code flow and find where to plugin the new functionality. Day 2 Design of the binary file format that will store to the trigrams data structure, as well as the parsing functions to extract them. Day 3 Development and Documentation Day 4 Development, Documentation and test harness Day 5 Final report 1 members st19 / solo project 2
https://github.com/gotplt/CourseProject	README.md	CourseProject for st19 ManBearPig. An attempt at providing Full Text Search to mandoc The goal of this project would be to attempt an implementation of a trigram based full text search database for the mandoc project (https://mandoc.bsd.lv/) Background UNIX system provide their documentation to the user through a set of tools collectively referred to as the Manual Page system. The well known man(1) command exists today on all UNIX systems but even on other platforms like MacOSX and android. Searching efficiently keywords and semantics has been of paramount importance for the user to quickly get to the relevant manual page and the command apropos(1) traditionally served that purpose, meaning doing database lookups. The database are built with the makewhatis(1) tool. Project proposal We will investigate the C source code of the mandoc project, targeted on the modules of searching and database building. We will understand how it works and how to extract manual page text using the mdoc library functions. Then we shall create a simplified trigram database of an an inverted index with the goal to connect it to the search capabilities. Final report and code You can read the final report in the pdf , and see the provided final-presentation.mp4 To compile the mandoc source code you can fetch it from https://mandoc.bsd.lv/ and compile it at the same directory level as this project. members st19 / solo project
https://github.com/icyguy64/CourseProject	final_report.pdf	"Text Classification Competition: Sarcasm Detection Final Report Chua Yeow Long, ylchua2@illinois.edu Introduction In this competition, we are required to perform sarcasm detection/classification of twitter tweets. The dataset consists of a response which is the tweet to be classified, context which is the conversation context of the response and the label as well as the id for identification of tweets when making submissions. The goal is to predict the label of the tweet ""response"" while optionally using context (i.e., the immediate or the full context). From the size of the dataset which is 5,000 for training and 1,800 for testing, transfer learning models will need to be used instead of building neural network models from scratch. Installation of libraries I'll use jupyter notebooks to illustrate the workflow and we'll need NLTK for text pre-processing, Keras/TensorFlow to build our neural networks and standard sklearn libraries. To install the libraries, just do a pip install or conda install (in the case of anaconda). Overall Plan I'll first preprocess the text data by removing stop-words using NLTK's stop-words and non-alphabetic characters such as symbols before feeding the data into 3 different models. The first is to use a simple word occurrence/count model and the model did not work out very well as the model predicted not-sarcasm for all the test dataset. The second is to use GLoVe embedding and adding some LSTM and dropout layers and training just the newly added layers. A 85% train and 15% validation split was performed to obtain the validation dataset. For the input text data, I have tried with and without context and there was not much a difference in the results. The last and third method is to use BERT and re-train the entire BERT layers using the dataset on a pretty, I would say beefy machine. The best F1-score I have obtained is 0.7378 with context information and 0.716 without context which is pretty significant. I have explored using additional feature engineering and GPT-2/XLM models but given the time constraints, I was not able to obtain a better F1-score. pip install tensorflow-gpu pip install nltk pip install sklearn Code Walkthrough For this section, you can just refer to the accompanied jupyter notebook or the video presentation. Results I have not included the results for the CountVectorizer model as it predicted all 0's/Not-Sarcasm. The best F1-score I have obtained using GLoVe embedding together with LSTM layers was 0.661 both with and without context. The best F1-score I have obtained using BERT is 0.7378 with context information and 0.716 without context. The baseline required is 0.723. Conclusion It was a fun journey to be playing with the state-of-the-art in NLP, playing with BERT and GLoVe embeddings as well as GPT-2/XLM. Although I was able to surpass the baseline only using BERT, I believe with more feature engineering and tinkering with the models, the baseline should be achievable. Description F1-score GLoVe embedding with LSTM layers without context 0.661 GLoVe embedding with LSTM layers 0.661 BERT without context 0.716 BERT with context 0.7378"
https://github.com/icyguy64/CourseProject	notebook_final.pdf	"notebook_fnal December 12, 2020 1 Text Classifcation Competition: Sarcasm Detection Chua Yeow Long, ylchua2@illinois.edu In this notebook, i'll frst try using a simple word count model using sklearn's CountVectorizer to perform sarcasm detection. Next, I'll use GLoVe embedding and add neural network layers towards the end, training just the added layers using the provided dataset. Finally, I'll fne tune BERT layers, just fne-tuning the entire BERT model which consists of hundreds of millions of parameters using the training dataset with a pretty beefy machine. We'll frst need to import the necessary libraries. [2]: import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np from nltk.corpus import stopwords from nltk.util import ngrams from sklearn.feature_extraction.text import CountVectorizer from collections import defaultdict from collections import Counter plt.style.use('ggplot') stop=set(stopwords.words('english')) import re from nltk.tokenize import word_tokenize import gensim import string from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from tqdm import tqdm from keras.models import Sequential from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D from keras.initializers import Constant from sklearn.model_selection import train_test_split from keras.optimizers import Adam We'll make use of pandas' read_json module to load the data into a pandas dataframe where we'll perform our work 1 [3]: df = pd.read_json('data/train.jsonl', lines=True) [4]: df.head() [4]: label response \ 0 SARCASM @USER @USER @USER I don't get this .. obviousl... 1 SARCASM @USER @USER trying to protest about . Talking ... 2 SARCASM @USER @USER @USER He makes an insane about of ... 3 SARCASM @USER @USER Meanwhile Trump won't even release... 4 SARCASM @USER @USER Pretty Sure the Anti-Lincoln Crowd... context 0 [A minor child deserves privacy and should be ... 1 [@USER @USER Why is he a loser ? He's just a P... 2 [Donald J . Trump is guilty as charged . The e... 3 [Jamie Raskin tanked Doug Collins . Collins lo... 4 [Man ... y ' all gone "" both sides "" the apoca... 1.1 Data-Preprocessing We'll just do a bit of preprocessing by removing non-alphabets and removing stopwords [6]: from nltk.corpus import stopwords import re [7]: def refineWords(s): letters_only = re.sub(""[^a-zA-Z]"", "" "", str(s)) words = letters_only.lower().split() stops = set(stopwords.words(""english"")) meaningful_words = [w for w in words if not w in stops] return( "" "".join( meaningful_words )) 1.2 With or without context We'll just combine the context into the response. I suppose having extra information/context is always good. [8]: df['response'] = df['response'].apply(refineWords) df['context'] = df['context'].apply(refineWords) df['response'] = df['context'] + ' ' + df['response'] We'll need to change the predictor variable which consists of 'SARCASM' and 'NON_SARCASM' to '1's and '0's so that we can feed them into the model 2 [9]: def sarcasm_mapping(input): if input == 'SARCASM': return 1 else: return 0 df['label'] = df['label'].apply(sarcasm_mapping) We initialize a simple countvectorizer from sklearn for our frst model. [10]: from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer(analyzer = ""word"", \ tokenizer = None, \ preprocessor = None, \ stop_words = None, \ max_features = 5000) We duplicated copies of the training dataset so we have individual copies to work on for each model. [ ]: df_tmp = df.copy() df_tmp_bert = df.copy() df_tmp_xlm = df.copy() 1.3 Data-Preprocessing We'll need to ensure our training data is in the correct format for further analysis [ ]: df[""response""] = vectorizer.fit_transform(df[""response""]).toarray() df[""context""] = vectorizer.fit_transform(df[""context""]).toarray() 1.4 Modelling We'll train our frst model which is a simple word count model. [11]: from sklearn.ensemble import RandomForestClassifier forest = RandomForestClassifier(n_estimators = 100) features_forest = df[[""response"",""context""]].values my_forest = forest.fit(features_forest, df['label']) We check the distribution of the target variable to see and indeed, the dataset is perfectly balanced. [12]: df['label'].value_counts() [12]: 1 2500 0 2500 Name: label, dtype: int64 3 1.5 Data-Preprocessing for test set We do a similar preprocessing for the test set as well. [13]: test_df = pd.read_json('data/test.jsonl', lines=True) [14]: test_df.head() [14]: id response \ 0 twitter_1 @USER @USER @USER My 3 year old , that just fi... 1 twitter_2 @USER @USER How many verifiable lies has he to... 2 twitter_3 @USER @USER @USER Maybe Docs just a scrub of a... 3 twitter_4 @USER @USER is just a cover up for the real ha... 4 twitter_5 @USER @USER @USER The irony being that he even... context 0 [Well now that ' s problematic AF <URL>, @USER... 1 [Last week the Fake News said that a section o... 2 [@USER Let ' s Aplaud Brett When he deserves i... 3 [Women generally hate this president . What's ... 4 [Dear media Remoaners , you excitedly sharing ... We simply combine the context into the response. [15]: test_df['response'] = test_df['response'].apply(refineWords) test_df['context'] = test_df['context'].apply(refineWords) test_df['response'] = test_df['context'] + ' ' + test_df['response'] We duplicate the test dataset so that we have individual copies to work on [16]: test_df_tmp = test_df.copy() test_df_tmp_bert = test_df.copy() test_df_tmp_xlm = test_df.copy() We need to make sure that the test data is in the same format as the train data so we do essentially the same analysis [ ]: test_df[""response""] = vectorizer.fit_transform(test_df[""response""]).toarray() test_df[""context""] = vectorizer.fit_transform(test_df[""context""]).toarray() The input format needs to be in a form of an array [17]: features_forest_test = test_df[[""response"",""context""]].values We perform predictions of the test features using the trained CountVectorizer model [18]: my_prediction = my_forest.predict(features_forest_test) 4 We convert the predictions into a pandas dataframe so we can use to_csv to easily output our predictions in the right format [20]: test_df['label'] = pd.DataFrame(my_prediction) We do a value_count on the predictions to check how well the model worked. Well the model predicted 100% no sarcasm which is clearly problematic. [21]: test_df['label'].value_counts() [21]: 0 1800 Name: label, dtype: int64 [22]: test_df.describe() [22]: response context label count 1800.000000 1800.000000 1800.0 mean 0.002778 0.002222 0.0 std 0.097170 0.074523 0.0 min 0.000000 0.000000 0.0 25% 0.000000 0.000000 0.0 50% 0.000000 0.000000 0.0 75% 0.000000 0.000000 0.0 max 4.000000 3.000000 0.0 After we are done making predictions using the trained model we'll need to change the 1's and 0's of the label column back to sarcasm and non-sarcasm. [23]: def sarcasm_reverse_mapping(input): if input == 1: return 'SARCASM' else: return 'NOT_SARCASM' test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping) We make use of to_csv of the pandas library to create our answer.txt [24]: #test_df[['id','label']].to_csv('answer.txt', index=False, header=None) 1.6 Modelling using GLoVe embedding and some neural network layers First, we'll need to create the corpus. We'll need the tqdm module and NLTK's word tokenize as well as stopwords to preprocess our data. [25]: def create_corpus(df): corpus=[] for tweet in tqdm(df['response']): 5 words=[word.lower() for word in word_tokenize(tweet) if((word. -isalpha()==1) & (word not in stop))] corpus.append(words) return corpus We concat/combine the training and test dataset so create a corpus of both the train and test datasets. [26]: df_tmp['response'] = df_tmp['response'].astype('string') test_df_tmp['response'] = test_df_tmp['response'].astype('string') df_new = df_tmp.append(test_df_tmp) corpus=create_corpus(df_new) 100%| | 6800/6800 [00:01<00:00, 3772.90it/s] We'll frst need to download the glove embedding and load it ensuring the correct formats, [27]: embedding_dict={} with open('data/glove.6B.200d.txt','r', encoding=""utf8"") as f: for line in f: values=line.split() word=values[0] vectors=np.asarray(values[1:],'float32') embedding_dict[word]=vectors f.close() We frst start by initializing a Keras tokenizer and train it with the corpus we obtained earlier. We perform truncating and padding to get sequences of the same length. [28]: MAX_LEN=50 tokenizer_obj=Tokenizer() tokenizer_obj.fit_on_texts(corpus) sequences=tokenizer_obj.texts_to_sequences(corpus) tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post') Our tokenizer has a word number count in the form of word_index. We'll need it later. [29]: word_index=tokenizer_obj.word_index We'll need to make sure of tqdm module and GLoVe embedding to obtain the embedding matrix to be used to create an embedding layer using Keras. [30]: num_words=len(word_index)+1 embedding_matrix=np.zeros((num_words,200)) 6 for word,i in tqdm(word_index.items()): if i > num_words: continue emb_vec=embedding_dict.get(word) if emb_vec is not None: embedding_matrix[i]=emb_vec 100%|| 34445/34445 [00:00<00:00, 569822.52it/s] We'll build our neural network sequentially. We frst add the embedding layer and add LSTM layers of decreasing nodes with dropout to reduce overftting. [31]: model=Sequential() embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN,trainable=False) model.add(embedding) model.add(SpatialDropout1D(0.10)) model.add(LSTM(128*2, dropout=0.10, recurrent_dropout=0.10, -return_sequences=True)) model.add(Dense(64*2, activation='relu')) model.add(LSTM(128, dropout=0.10, recurrent_dropout=0.10, -return_sequences=True)) model.add(Dense(64, activation='relu')) model.add(LSTM(64, dropout=0.10, recurrent_dropout=0.10, return_sequences=True)) model.add(Dense(32, activation='relu')) model.add(LSTM(32, dropout=0.10, recurrent_dropout=0.10, return_sequences=True)) model.add(Dense(16, activation='relu')) model.add(LSTM(16, dropout=0.10, recurrent_dropout=0.10)) model.add(Dense(1, activation='sigmoid')) We'll make use of the Adam optimizer with a small learning rate. We'll compile the model specifying the loss, optimizer and metrics to optimize our model for. [ ]: optimzer=Adam(learning_rate=1e-5*10) model. -compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy']) We split our data back into train and test datasets. The frst 5000 is our training data the rest is test. [32]: train=tweet_pad[:5000] test=tweet_pad[5000:] 7 We'll perform a train-validation split to obtain validation data. [33]: X_train,X_test,y_train,y_test=train_test_split(train,df_tmp['label']. -values,test_size=0.15) We can now start training our model specifying the batch size, epochs and validation datasets. [34]: history=model. -fit(X_train,y_train,batch_size=4*16,epochs=1,validation_data=(X_test,y_test),verbose=2) Train on 4250 samples, validate on 750 samples Epoch 1/1 - 11s - loss: 0.6914 - accuracy: 0.5214 - val_loss: 0.6810 - val_accuracy: 0.6293 We perform predictions using our trained model. As the predictions are not strictly 1's and 0's we'll just simply round them to the nearest integer. [35]: y_pre=model.predict(test) test_df['label'] = np.round(y_pre).astype(int) We'll need to do a mapping of 0's and 1's to 'NOT_SARCASM' and 'SARCASM' [36]: def sarcasm_reverse_mapping(input): if input == 1: return 'SARCASM' else: return 'NOT_SARCASM' test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping) We convert our predictions into a csv fle with the right format with the following. [37]: #test_df[['id','label']].to_csv('answer.txt', index=False, header=None) 2 Modelling using BERT, retraining the entire BERT architecture to the data. We'll use the offcial tokenization script from tensorfow. You can download the tokenization.py by using wget or downloading it manually. [38]: !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/ -official/nlp/bert/tokenization.py SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc syswgetrc = C:\Program Files (x86)\GnuWin32/etc/wgetrc We'll import the necessary libraries needed for this section. 8 [39]: import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.optimizers import Adam from tensorflow.keras.models import Model from tensorflow.keras.callbacks import ModelCheckpoint import tensorflow_hub as hub import tokenization We'll need to encode the input texts to BERT's input format. For each row of the training data, we frst tokenize the text using NLTK tokenizer before proceeding to convert the obtained tokens into IDs. [40]: def bert_encode(texts, tokenizer, max_len=512): all_tokens = [] all_masks = [] all_segments = [] for text in texts: text = tokenizer.tokenize(text) text = text[:max_len-2] input_sequence = [""[CLS]""] + text + [""[SEP]""] pad_len = max_len - len(input_sequence) tokens = tokenizer.convert_tokens_to_ids(input_sequence) tokens += [0] * pad_len pad_masks = [1] * len(input_sequence) + [0] * pad_len segment_ids = [0] * max_len all_tokens.append(tokens) all_masks.append(pad_masks) all_segments.append(segment_ids) return np.array(all_tokens), np.array(all_masks), np.array(all_segments) We'll create the BERT layer specifying the inputs and creation of the BERT layer using the inputs to obtain the output sequence. Since we are re-training the entire BERT architecture to the dataset, we'll just add a sigmoid dense layer to perform classifcation. We specify the BERT model and the adam optimizer, optimization loss and accuracy metrics. [41]: def build_model(bert_layer, max_len=512): input_word_ids = Input(shape=(max_len,), dtype=tf.int32, -name=""input_word_ids"") input_mask = Input(shape=(max_len,), dtype=tf.int32, name=""input_mask"") segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=""segment_ids"") _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids]) 9 clf_output = sequence_output[:, 0, :] # Without Dropout out = Dense(1, activation='sigmoid')(clf_output) model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out) model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', -metrics=['accuracy']) return model We load BERT from TensorFlow Hub. TensorFlow Hub provides pre-trained models for us to use and we load the model using TensorFlow hub module. [42]: module_url = ""https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"" bert_layer = hub.KerasLayer(module_url, trainable=True) We'll need to load the tokenizer from the BERT layer. [43]: vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case) Next, we process the text into BERT required formats such as tokens, masks and segment fags. [44]: train_input = bert_encode(df_tmp_bert.response.values, tokenizer, max_len=160) test_input = bert_encode(test_df_tmp_bert.response.values, tokenizer, -max_len=160) train_labels = df_tmp_bert.label.values This are the parameters we'll need for our BERT model. Due to limited processing capacity, there aint much room for me to play with. [45]: Max_length = 42 Dropout_num = 0 learning_rate = 6e-6 valid = 0.15 epochs_num = 5 batch_size_num = 4 ids_error_corrected = True We build our BERT model and note that we have 335 million traininable parameters. [46]: model_BERT = build_model(bert_layer, max_len=160) model_BERT.summary() Model: ""model"" ________________________________________________________________________________ 10 __________________ Layer (type) Output Shape Param # Connected to ================================================================================ ================== input_word_ids (InputLayer) [(None, 160)] 0 ________________________________________________________________________________ __________________ input_mask (InputLayer) [(None, 160)] 0 ________________________________________________________________________________ __________________ segment_ids (InputLayer) [(None, 160)] 0 ________________________________________________________________________________ __________________ keras_layer (KerasLayer) [(None, 1024), (None 335141889 input_word_ids[0][0] input_mask[0][0] segment_ids[0][0] ________________________________________________________________________________ __________________ tf_op_layer_strided_slice (Tens [(None, 1024)] 0 keras_layer[0][1] ________________________________________________________________________________ __________________ dense (Dense) (None, 1) 1025 tf_op_layer_strided_slice[0][0] ================================================================================ ================== Total params: 335,142,914 Trainable params: 335,142,913 Non-trainable params: 1 ________________________________________________________________________________ __________________ We'll train the BERT model, serializing the best model to a fle for later predictions. Finally, we get to train our BERT model. [47]: checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', -save_best_only=True) train_history = model_BERT.fit( train_input, train_labels, validation_split = valid, epochs = epochs_num, callbacks=[checkpoint], batch_size = batch_size_num ) Train on 4250 samples, validate on 750 samples 11 Epoch 1/5 4250/4250 [==============================] - 303s 71ms/sample - loss: 0.5174 - accuracy: 0.7588 - val_loss: 0.5598 - val_accuracy: 0.6080 Epoch 2/5 4250/4250 [==============================] - 278s 65ms/sample - loss: 0.4388 - accuracy: 0.8024 - val_loss: 0.4221 - val_accuracy: 0.7653 Epoch 3/5 4250/4250 [==============================] - 265s 62ms/sample - loss: 0.3094 - accuracy: 0.8727 - val_loss: 1.3605 - val_accuracy: 0.4267 Epoch 4/5 4250/4250 [==============================] - 265s 62ms/sample - loss: 0.1140 - accuracy: 0.9614 - val_loss: 2.7396 - val_accuracy: 0.3747 Epoch 5/5 4250/4250 [==============================] - 265s 62ms/sample - loss: 0.0422 - accuracy: 0.9861 - val_loss: 2.8826 - val_accuracy: 0.4853 We perform predictions by loading weights from the fle we serialized the model earlier. [48]: model_BERT.load_weights('model_BERT.h5') test_pred_BERT = model_BERT.predict(test_input) test_pred_BERT_int = test_pred_BERT.round().astype('int') We'll need to perform a mapping of the 1's and 0's back to 'SARCASM' and 'NOT_SARCASM' for submission. [49]: test_df['label'] = test_pred_BERT_int def sarcasm_reverse_mapping(input): if input == 1: return 'SARCASM' else: return 'NOT_SARCASM' test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping) We generate the 'answer.txt' in the format required for submission. [ ]: #test_df[['id','label']].to_csv('answer.txt', index=False, header=None) 12"
https://github.com/icyguy64/CourseProject	progress_report.pdf	Course Project Progress Report Chua Yeow Long, ylchua2@illinois.edu Text Classification Competition Introduction For the text classification competition on sarcasm detection, I'll utilise transfer learning using embeddings such as GLoVe and BERT which are both major milestones in modern NLP rather than training a machine learning model from scratch. GLoVe embeddings is one of the first applications of transfer learning in NLP and with the introduction of BERT by Google in 2018, it addresses the shortcomings of LSTMs and other modern NLP techniques and also more recently with huge models such as XLM and GPT-2 achieved state-of-the-art performance. Methodology & Results I'll first preprocess the text data by removing stop-words and non-alphabetic characters such as symbols before feeding the data into 3 different models. The first is to use an GLoVe embedding and adding some LSTM and dropout layers and start training the model. A 85% train and 15% validation split is performed to obtain the validation data. For the input text data, I have tried with and without context and only for the BERT case the F1-score is pretty significant. I have tried different LSTM layers and nodes as well as dropout layers and the best f1-score I have obtained is around 0.661. The second method is to use BERT and re-train the entire BERT layers using the dataset on a pretty beefy machine. The best F1-score I have obtained is 0.7378 with context information and 0.716 without context. The baseline F1-score is 0.723 and the top F1-score is by awe with 0.7653. Future-works I'll try and see if I can get a better score by exploring two different approaches: feature engineering and/or GPT-2/XLM transfer learning based methods when there is time to spare. I have included the references for BERT as well as GPT-2 in the references section below. References Language Models are Unsupervised Multitask Learners Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Iiya Sutskever Attention Is All You Need Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Description F1-score GLoVe Embedding with LSTM layers without context 0.661 GLoVe Embedding with LSTM layers 0.661 BERT without context 0.716 BERT 0.7378
https://github.com/icyguy64/CourseProject	proposal_ylchua2.pdf	Project Proposal In your project proposal, please answer the following questions: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. CHUA Yeow Long, ylchua2@illinois.edu Which competition do you plan to join? Text classification competition If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Yes, I'm prepared to learn state-of-the-art neural network classifiers. Name some neural classifiers and deep learning frameworks that you may have heard of. Deep learning frameworks such as Pytorch and TensorFlow. Artificial neural networks(ANN), LSTMs, CNN, RNN, transformers, transfer learning Describe any relevant prior experience with such methods Built machine learning models using sklearn libraries XGBoost, SVM to perform classification not limited to NLP Built deep learning neural networks using Keras/TensorFlow and PyTorch Text pre-processing using SpaCy and NLTK Transfer Learning using SimpleRepresentations or HuggingFace Which programming language do you plan to use? Python
https://github.com/icyguy64/CourseProject	README.md	Text Classification Competition: Sarcasm Detection ylchua2@illinois.edu Project Proposal - proposal_ylchua2.pdf Project Progress Report - progress_report.pdf Project Code - notebook.ipynb (working copy), notebook_final.ipynb (notebook with outputs kept), notebook_final.pdf (pdf version), answer_1p0_0p7378 (predictions that achieved 100% accuracy and F1-Score of 0.7378), glove.6B.200d.txt(https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) Documentation - final_report.pdf (Simple writeup of the final project, installation, overall plan) Presentation - https://youtu.be/dE7jiio5QAM
https://github.com/yukuo78/CourseProject	ProgressReport.pdf	Monday, November 30, 2020 Progress Report Subject: Reproducing paper: Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. 1) Progress made thus far Finished reading this paper and the original paper first paper. Acquired the datasets needed for the paper experiments. 2) Remaining tasks The remaining tasks would be actually understand the described model and try to reproduce the algorithm and experiment in time, hopefully. 3) Any challenges/issues being faced. Since previously I'm quite busy with some other study in parallel with this course. Except for keeping up with the lesson and MPs, and also the taking exam, I hadn't have enough time to work on the project, or to join a team. So I'm working alone now. This is also why I chose this reproducing paper as project, since I figured this requires least creativity thus effort, and might get some help from classmates and TAs. 1
https://github.com/yukuo78/CourseProject	ProjectProposal.pdf	"Sunday, October 25, 2020 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. kuoyu2 2. Which paper have you chosen? Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. 3. Which programming language do you plan to use? Python 4 Can you obtain the datasets used in the paper for evaluation? Maybe not all. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? I belive I can with some help from TAs. 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? 1"
https://github.com/yukuo78/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Please check the docs directory for all progress report, documentation and video presentation.
https://github.com/pritomsaha/CourseProject	Documentation.pdf	"Documentation Reproducing the Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Darius Nguyen Pritom Saha Akash Trisha Das 1) An overview of the function of the code : Many applications need textual topics to be studied together with external time series. This paper proposes a general text mining system for the discovery of this type of causal themes from the text. We implemented the algorithm presented in the paper in Python. Our implementation combines a given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time-series data. As described in the paper, we iteratively refine the discovered topics to increase the correlation with the time series. Time series data provides feedback at each iteration by imposing prior distributions on parameters. In our experiment, we examine the 2000 U.S. Presidential election campaign. The input text data is from New York Times articles from May through October of 2000. As a non-textual time-series input, we use prices from the Iowa Electronic Markets 2000 Presidential Winner-Takes-All Market. We also experimented with stock time-series data (AAMRQ vs. AAPL). Our implementation can determine causal topics efficiently. We used PLSA as our topic modeling method. Granger Test is used to find out a set of candidate causal topics with lags. Pearson Correlation is used to find out word-level causality in our implementation. Though the authors of the paper used R programming language to do the Granger Causality test, we did it in Python to make the code compact and manageable. We used only one programming language to complete the full implementation. 2) Documentation of how the software is implemented: The main parts of our code are as follows: 1. Data Preparation: We used text data from the 2000 U.S. Presidential election campaign. The input text data is from the New York Times articles from May through October of 2000. We filter them for keywords ""Bush"" and ""Gore,"" and use paragraphs mentioning one or both words. Also, we scrapped time-series data from [2][3][4]. We used the ""normalized"" price of one candidate as a forecast probability of the election outcome: (Republic AvgPrice)/(Republic AvgPrice + Democratic AvgPrice). Other data cleansing steps were also taken. We also experimented with the High and Low prices of each party. 2. Generating topics: We used PLSA(Probabilistic Latent Semantic Analysis) topic modeling method to find out representative topics from the text data. This uses the Expectation-Maximization (EM) algorithm. We used a PLSA implementation package from [1]. This is many times faster than our previous implementation in MP3. That's why we used this implementation in our program. 3. Causal analysis with time series data: We used the Granger test for measuring causality. We utilized the Python library grangercausalitytests from statsmodels.tsa.stattools for the Granger test. The output of this part gives significant causal topics with significance >95%. Also, each topic is associated with a corresponding time lag which can describe the causality of the corresponding topic the most. 4. Word level causality: We used the Pearson Correlation test for measuring the word level causality in our implementation. Each significant topic determined by the granger test is passed through this next level for finding word-level causality. Within each topic, the words with significant positive correlation and negative correlations are separated and grouped into two distributions. These distributions work as priors in the next iteration. 5. Generating Topic Priors: We generated topic priors for the causal topics and incorporated them into the next iteration PLSA. 3) Usage documentation: Our program was built in a Jupyter notebook and ran on Google Colab. Please see the comments we added inline for instructions on running specific blocks of code. The code blocks can be run sequentially from beginning to end to see the results. Please find the Jupyter notebook on our GitHub repository: (https://github.com/pritomsaha/CourseProject) 4) Participation: netid participation huy2 * Logistics: * Create project in CMT & added project meta * Group coordination/planning * Contribute to proposal/progress report/documentation/presentation * Project work: * Contribute to paper investigation, determining implementation steps, finding solution to roadblocks * Implemented stock time series cleansing and processing * Implement word-level causality modeling * Implement topic prior generation paksash2 * Logistics: * Create the project on github and upload the project proposal and project progress reports. * Contribute to proposal/progress report/documentation/presentation * Project work: * Contribute to paper investigation, determining implementation steps, finding solution to roadblocks * Extract the appropriate text data from new york time corpus [5]. * Preprocess and cleaning text data so that it can be used to train plsa model. * Finding out a working fast plsa model [1] and making necessary changes to the implementation of the plsa model so that it is appropriate for the topic modeling in the paper. * Making room for incorporating topic prior feedback to the plsa model. * Implementing the code for calculating topic coverage that is required in finding causal topics. * Combining all the modules (topic modeling, topic-level causality, word-level causality) to make it workable for running. trishad2 * Logistics * Group coordination/planning * Contributed to the proposal, progress report, presentation, and documentation References: 1. https://github.com/henryre/numba-plsa 2. https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_I D=29 3. https://thestockmarketwatch.com/stock/stock-data.aspx?symbol=AAMRQ&action=sh owHistory&page=1&perPage=25&startMonth=4&startDay=1&startYear=2000&endM onth=9&endDay=30&endYear=2020&endDateLite=11%2F15%2F2020 4. https://finance.yahoo.com/quote/AAPL/history/ 5. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19 * Project work: * Contributed to the paper investigation, determining implementation steps, finding the solution to roadblocks * Stock time series cleansing and processing * Scraped time-series data(Iowa Electronic Markets (IEM)3 2000 Presidential Winner-Takes-All Market, AAMRQ, AAPL stock price data) from websites [2][3][4] * Implemented topic-level causality using Granger test * Worked on hyperparameter tuning * Wrote the documentation and created the PowerPoint presentation * Fixed bugs in code"
https://github.com/pritomsaha/CourseProject	Final Project Proposal-Team PDT.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Ans: Team members: Name NetID Darius Nguyen (Captain) huy2 Pritom Saha Akash pakash2 Trisha Das trishad2 2. Which paper have you chosen? Ans: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 3. Which programming language do you plan to use? Ans: Python 4. Can you obtain the datasets used in the paper for evaluation? Ans: We are trying to get the dataset. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?"
https://github.com/pritomsaha/CourseProject	ProgressReport-TeamPDT.pdf	CS 410 - Final Project Progress Report Team PDT 1. Steps completed: a. Dataset collection: Both text and time series datasets are collected. b. Text data preprocessing: We have preprocessed text data for using it into the PLSA algorithm. c. Topic modeling: We have applied the PLSA algorithm to find topics from text data. d. Causal topic filtering: We have applied the granger test for finding significant casual topics based on time-series data. e. Word-level causality modeling: We have implemented the part for filtering causally related words (positive and negative) for each causally significant topic. 2. Steps outstanding: a. Topic prior generation b. Apply topic prior feedback to topic modeling 3. Challenges: a. We are facing some problems in understanding which distribution( negative or positive or both) for each causally significant topic to apply as prior to topic modeling. Name NetID Darius Nguyen (Captain) huy2 Pritom Saha Akash pakash2 Trisha Das trishad2
https://github.com/pritomsaha/CourseProject	README.md	CourseProject:- Reproducing the Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Team members Darius Nguyen Pritom Saha Akash Trisha Das Our program was built in a Jupyter notebook and ran on Google Colab. There are two colab file: 1. Corpus Processing: It is used to get text corpus and preprocessing the text. 2. ITMTF: This is the main colab where the ITMTF model is run sequencially. Please see the comments we added inline for instructions on running specific blocks of code. The code blocks can be run sequentially from beginning to end to see the results. Please read the Documentation.pdf for more details. Presentation https://illinois.zoom.us/rec/play/i-3hI-q_f39vHiVYxZJbjuZqNIwKxF-3-qz1Lo8t3VftqgSmI5hJuGkqAuRgABcRNnumMCWDhfHfh5PK.UiSG0GrcsaiEr6gK?continueMode=true&_x_zm_rtaid=DKz7zUCcQWy8BRcZYpWxJQ.1607897144160.874f474aff50d1bfcba2cf282ecb81a9&_x_zm_rhtaid=454
https://github.com/ashpradhan01123/CourseProject	CS410 Project.pdf	"CS-410 Text Information Systems Final Project Text Miners Ashish Kumar Pradhan (apradh6@illinois.edu) Kirti Magadum (magadum2@illinois.edu) Bhuvaneswari Periasamy (bp14@illinois.edu) Improving EducationalWeb System 1. Introduction The EducationalWeb System is the term given to the Web of Slides (WOS) created at the University of Illinois at Urbana Champaign by Sahiti Labhishetty, Bhavya, Kevin Pei, Assma Boughoula and Prof. Chengxiang Zhai. It aimed to link all the lecture slides of a course which would be easily navigable and searchable. Not only that, with Machine Learning, it is also able to provide a list of related slides which has some similarity to the current slide displayed. It is created as a Python Flask Web App with an interface which is similar to opening up a PowerPoint presentation. Navigation can be primarily done by clicking through the previous and next buttons in the interface as well as from the list of similar slides provided by the system. A separate system allows the search ability through which the navigation can be done as well. This project consisted of adding a few features which would enhance the System so that it can be used in a more widespread manner. Keeping true to the vision which was to create a system which would interact with each of the slides in a way that the current internet interacts, the modifications are aimed that doing just that. 2. Background The existing code consists of two major components: a. The related slides code which computes the similarity between slides using Deep Learning https://github.com/Bhaavya/mooc-web-of-slides b. The structural part of the application which consists of the rest of the Python Flask Web App. https://github.com/CS410Fall2020/EducationalWeb Initially, we thought of incorporating both parts of the code into one and then including the modifications but due to the computationally heavy part of the Deep Learning component, we decided to exclude this part completely and focus on adding the new components to the Web App. 3. Code Explanation Before incorporating the modifications, installing the existing code from the repository was necessary. We have detailed the following steps taken for installing the code in our Windows Machines. a. Download the Download the code from the github page ( https://github.com/CS410Fall2020/EducationalWeb) to a folder EducationalWeb in your local machine b. Elastic Search Elastic Search was installed by downloading the zip file from the Elastic Search Website. https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.10.1-windows- x86_64.zip. Unzip the folder in a folder ElasticSearch and then run the following code in the command prompt after changing the directory to that location. .\bin\elasticsearch.bat This will start the elastic search in your local machine. Fig 1 Fig 2 c. Create index (need to be done only once) Run the following script after navigating to the EducationalWeb folder in the command prompt python create_es_index.py Fig 3 d. Download tfidf_outputs.zip folder Download it from the following link, unzip it and save it in EducationalWeb/static https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing e. Download the lecture slides Use the following link to download the lecture slides and then unzip it to EducationalWeb/pdf.js/static/slides/ https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing f. Install gulp server Follow the steps in this page ( https://gulpjs.com/docs/en/getting-started/quick-start/) to install gulp server g. Run Gulp Server Open a separate terminal window and navigate to EducationalWeb/pdf.js/build/generic/web and execute the following command to run gulp server gulp server Fig 4 h. Run the application In another terminal window, navigate to the EducationalWeb folder and run the following script to start the code. Make sure that all the python libraries present in the code are installed before the script is run. python app.py i. Launch the application The application should open in http://localhost:8096/ 4. Modifications to the code Our goal is to add additional courses to the existing code so that users can navigate between courses and utilize the features available to maximize the learning ability from those courses. In order to achieve that, we have made two additions to the code, namely: a. Created a web crawler which would take as input the url of the webpage which contains all the lecture content and create a list of urls which contains the pdf links b. Download the pdfs, and subsequently create a repository like the existing one for CS410, containing folders for each lecture pdfs which will contain single page pdfs split automatically by the code. c. These courses are then incorporated into the existing code, bringing in all the course slides into the EducationalWeb which can then be navigated like the existing CS410 course Web Crawler An automatic Web Crawler is essential to any web application which needs to add new content from the web. A web crawler essentially follows all the links from a starting page and then retrieves information from the pages as desired. Our web crawler recursively follows all the links which are contained from a starting page. If it finds a dead end in any of the pages, i.e. there are no additional pages to scrape from that page, it will just go one step up and then continue the scraping of the pages from there on. The creation of the web crawler was done by using the very useful Beautiful Soup library which parses content from HTML pages using a tree structure. It is very useful in navigating, searching and modifying a parse tree of an html page, all very essential parts of web scraping. Download the lecture content After the web crawler scrapes through all the URLs containing lecture slides in each webpage, we need to download them and structure them in such a way so that it fits the educational web code. The existing code handles only single page PDFs which are then navigated and searched by the system. To achieve that for the newly added courses, these are the steps that were taken: 1. Download the pdfs from the URLs scraped by the web crawler. 2. Scrape the lecture title from the webpage 3. Create a folder structure like the existing 410 slides. There needs to be one folder for every Lecture pdf 4. Split the pdfs into single page pdfs and then add them to that Lecture's folder. For our project, we have included 3 additional University of Illinois courses whose lecture slides are publicly available. Those are: 1. ECE 313 (https://courses.engr.illinois.edu/ece313/sp2013/Slides.html) 2. CS 425 (https://courses.engr.illinois.edu/cs425/fa2019/lectures.html) 3. CS 554 (https://solomonik.cs.illinois.edu/teaching/cs554/index.html) Add the slides to the existing code After the slides have been created according to the configuration of the code, modifications had to be made to add them under the drop downs under the ""Courses"" section. We specifically parsed texts from these webpages to add the details to that course section in the system. The details were present in various tags which needed to be extracted so that we get the relevant titles for the slides. Running the Modified Code The code has been integrated in the existing system in such a way that it does not need any additional input from the user. Changes were made inside the model.py and app.py scripts with the additional functions which were defined being referenced from within the existing functions, thus reducing the need for any user input specifically. To run the modified code, all that's required is to run the app.py function again which will start the web scraping from the webpages and then create the folder structures for each of the subject and then integrate them into the existing EducationalWeb System. Fig 5 Fig 6 Fig 7 Fig 8 5. Limitations and Scope for Further Work 1. Due to an absence of a training dataset, a classification model could not be created to correctly classify a webpage as one which contains lecture slides. Due to this we had to explicitly input the starting URL for web scraping. Creating that model required much more time than what could be allotted to this project. 2. The related slides section which uses Deep Learning to find out the similarity between the slides could not be incorporated. Additional work needs to be done to integrate it into the existing code 3. Documentation of the code and how to navigate between the scripts would have been helpful to understand the code in the initial stages and not spend a lot of time in that. 6. Contribution of Team Members Team Member Contribution Ashish Pradhan (Captain) Web Crawler, Documentation Kirti Magadum PDF splitting code, integration to the existing code Bhuvaneswari Periasamy Modeling code, Text parsing code 7. Conclusion The EducationalWeb is a powerful tool which can assist the learners from learning in an optimized manner without spending a lot of time in searching and navigating through slides across different topics. There is tremendous potential in this application and the scope of incorporating a lot of features is immense. The modifications which we added to the existing code paves the way for additional work which would improve the system. As stated in the scope above, creating a classification model and adding it to our web scraper would make it largely automated and be able to scrape the pdf urls from anywhere in the web"
https://github.com/ashpradhan01123/CourseProject	README.md	CourseProject Please install the Educational Web as mentioned below from the CS410 Repository and then overwrite the two python scripts (app.py and model.py) from our project repository. The git push from the original repo was showing an error when we were trying to bring in the entire code. We have tested the code using Python 3.6. Please let us know if you need a demo or require any assistance installing the code in your local machine for testing. EducationalWeb Installation Steps The following instructions have been tested with Python2.7 on Linux and MacOS 1. Download EducationalWeb code from https://github.com/CS410Fall2020/EducationalWeb You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server Please install the libraries mentioned in app.py before you run the code. In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/
https://github.com/ashpradhan01123/CourseProject	Test Miners Project.docx.pdf	"Text Miners Team Project 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team member NetId Ashish Pradhan [Captain] apradh6@illinois.edu Kirti Magadum magadum2@illinois.edu Bhuvaneswari Periasamy bp14@illinois.edu 2. What system have you chosen? Which subtopic(s) under the system? We have chosen to enhance the Educational Web System and intend to add new features, focusing on the following subtopics: * Scale up the current system Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. We intend to create an automatic crawler which could classify a webpage containing slides correctly and subsequently download them. * Allow downloading slides in bulk: Downloading the entire collection of slides for a particular course or interest. Our goal is to primarily focus on creating a successful crawler and then aim for creating the ability to allow slides in bulk after that. 3. Briefly describe the datasets, algorithms or techniques you plan to use Dataset Create a dataset of all the UIUC pages which has slides available for download and also some random pages which are ""negative"" examples. Algorithms Dirichlet prior, EM Algorithm, All algorithms currently in use. For classification, we intend to use all the standard available ones such as SVM, Clustering, Logistic Regression to achieve maximum accuracy. Techniques Automatic web crawler. All techniques currently in use. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? - Scale up the current system We are adding an automatic web crawler. We will crawl data related to selected courses from the mentioned dataset. Users should be able to select newly added courses and our updated code should be able to display relevant data to users. Also upon selection of a newly added course, options related to course should get updated. # Courses [Newly added courses] # Recently Visited slides # Lectures # Search result. * Allow downloading slides in bulk We will enhance the existing functionality from downloading a single slide into downloading multiple slides that fits under the same course/slide heading. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will aim to automate the crawling to feed slides directly into the system so that users will not have to create a zip file and then manually upload it into the system. For the second part, we will aim to provide the user two options: to download the slide for a particular lecture or the whole course (as a zip file). 6. Which programming language do you plan to use? We will be writing our code predominantly in Python but might implement Javascript, CSS and HTML if needed. 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. * Scale up the current system Task Hours ( hours * N team member) Detailed understanding code for existing system 6 (2 * 3 ) User Interface changes 3 (1 * 3 ) Automatic Web crawling (Including creation of training dataset and a classification model) 33 (11* 3) Deployment and testing 12 (4 * 3) Bug Fix 3 ( 1 * 3) Performance Test 3 (1 * 3) Total 60 Hours 8. Allow downloading slides in bulk NOTE: Any remaining time from the above section will be utilized to complete this task."
https://github.com/ashpradhan01123/CourseProject	Text Miners Team Project - Progress Report.docx	Text Miners Team Project: Progress made thus far: Scale up the current system: POC to read the pdf file(slides) from UIUC url and download it in the required format is complete. Implemented the changes to show the newly added courses for users to select on test data. Remaining tasks: Creation of Automatic Web Crawler by leveraging the POC that we completed now. Implementing the code changes for search result and recently visited slides. Unit Testing and Integration Testing Any challenges/issues being faced: Determining the reference link on Course era was not straight forward as the links contained sections which could not be parsed via code. We need the slides in single-page pdf as input to the educational web search program. Also, it requires additional authentication, ultimately downloading the pdf from cloudfront.net. As we don't have access to cloudfront, we decided to refer slides from UIUC site as input data to our program.
https://github.com/adeetikaushal/CourseProject	CS410_Voltron_Final_Project_Proposal.pdf	Team: Voltron Topic: Book Recommendation System 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name NetIds Captain Adeeti Kaushal Adeetik2 Yes Vivek Bansal Vivekb3 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic: Book recommendation system. Description: In the course, we discussed about Recommendation systems and Search. There are two main recommendation system approaches that were discussed further, Content-based filtering and Collaborative based filtering. In this project, we are focusing on simple content-based book recommendation system. The content will be downloaded from public sites (Project Gutenberg). Tasks involved: The tasks involve loading the content of book into python after downloading it. Search for relevant content/words in the loaded data. Tokenize the corpus and perform stemming on the tokenized corpus. Next step would involve building bag of words model and find stop words, build term frequency-inverted document frequent model and show the results of tf-idf model. Compute the distance between texts. Look into search criteria and find similar books matching the content using Cosine Similarity. Important and Interesting: This topic is important and interesting because big corporations like Facebook, Amazon, Apple, Netflix, Google (FAANG) and others big companies use recommendation system to show/target the content based on similarities in content. This is important for business to present or build the resources that users are searching and show them similarities between other books. Planned Approach: The approach that we plan to take involves collecting the books from Project Gutenberg which offers free books and that will be used as our dataset. We will find a topic/search term or title that will be used to create the model and then then find similarities in other books. Python libraries will be used to perform various tasks like stemming, tokenizing etc. The outcome would involve books with similarities. Tools, Systems, Dataset: Python Libraries that can be used for NLP, tokenization, stemming, parsing, classification etc. which are important tasks. Jupyter Notebook will be used to read the dataset and execute the python program. From Project Gutenberg, free books would be downloaded to build the dataset. Expected outcome: Being able to retrieve/recommend similar books based on the content. Evaluation: Results retrieved from the program should give recommendation of books that are matching or similar in some way or form. For example, a crime mystery novel should not return any match with children book. 3. Which programming language do you plan to use? We are planning to use Python. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. We are planning to cover the following topics in the project. Tasks Estimated Time (hrs Learning Python 8* Research/Build dataset 5 Code for tokenization, Stemming on corpus 4 Build bag of words, find stop words 2 Build tf-idf 4 Build Similarity matrix 2 Full end to end integration, tuning 6 Testing 5 Visual representation of results (matplotlib) 6 Other use case - Content based Similarity 6 Report 4 Total 52 N=2 20*2 hours = 40 hrs project Estimated hours = 52 * Since, both need to learn Python, so we have added 8 hrs for learning but without that the efforts required for project are above the required hours.
https://github.com/adeetikaushal/CourseProject	CS410_Voltron_Final_Project_Report.pdf	"CS-410 Text Information System Book Recommendation System Team: Voltron Team Member Email Captain Adeeti Kaushal adeetik2@illinois.edu Yes Vivek Bansal vivekb3@illinois.edu Table of Contents INTRODUCTION ............................................................................................................................................................ 3 FLOW DIAGRAM .......................................................................................................................................................... 3 OVERVIEW OF TASKS .................................................................................................................................................... 4 ABOUT THE DATASET ................................................................................................................................................... 4 CODE .......................................................................................................................................................................... 4 SETUP ....................................................................................................................................................................... 4 Step by Step Code details ......................................................................................................................................... 5 SET SEARCH CRITERIA: .................................................................................................................................................. 5 LOAD DATASET AND SORT ............................................................................................................................................ 5 LOAD TITLES AND TEXT IN OBJECTS ................................................................................................................................. 6 LOAD INDEXES ............................................................................................................................................................. 7 TOKENIZE THE CORPUS ................................................................................................................................................. 7 PICKLING THE TOKENIZED CORPUS ................................................................................................................................. 8 STEMMING OF TOKENIZED CORPUS ............................................................................................................................... 9 BUILDING A BAG-OF-WORDS MODEL .............................................................................................................................. 9 VISUALIZE THE MOST COMMON WORDS ....................................................................................................................... 10 BUILD A TF-IDF MODEL ............................................................................................................................................... 11 RESULTS OF THE TF-IDF MODEL .................................................................................................................................... 12 COMPUTE DISTANCE BETWEEN TEXTS ........................................................................................................................... 13 SIMILAR BOOKS ......................................................................................................................................................... 14 BOOKS WITH SIMILAR CONTENT .................................................................................................................................. 15 INTRODUCTION The Book recommendation system aims to provide a selection to user based on the user's taste. Generally, a system would rely on user's metadata (for ex. Author of the book, theme etc.) to determine which book would user enjoy the most. However, when we take the same approach with full text search or full content, it becomes a very heavy dataset. In this project, we will try to demonstrate the same by providing recommendation based on the content. FLOW DIAGRAM Above picture shows the entire flow of the project. In dataset layer, we first downloaded the books (public) in text form from glutenberg.org. For our sample, we took 25 books and processed. In the data processing layer, we loaded the books as text and titles and performed clean up. We went through removal of stop words, grouped them together (created stem). Using the stem words and dictionary, BagofWords were created for each book. Finally, in data processing layer, we built the inverted Index model. In the end, we did the analysis using similarity calculations by creating the matrix and using the matlib, created the visualization and horizontal dendrogram. OVERVIEW OF TASKS Following tasks were performed to complete this project. Research/Build dataset Code for tokenization, Stemming on corpus Build bag of words, find stop words Build tf-idf Build Similarity matrix Full end to end integration, tuning Testing Visual representation of results (matplotlib) Other use case - Content based Similarity Report In this report, we will walk through the code step by step and also showcase the output of each step. ABOUT THE DATASET The dataset is collection of books that is manually downloaded from Project Gutenberg. For this project, around 20 books were downloaded and used to find the content by searching the Text using similarity matrix. CODE The code is written in Jupyter notebook and python3. Following are the 2 components: Recommender.ipynb - The ipynb is iPythonNotebook file (Jupyter file) that contains the code. In the step 1, we will start with setting up the search criteria. The criteria for searching book of interest is set here. Library - Downloaded data from Project Gutenberg SETUP Install the following tools/lang: * Jupyter * Python 3 Install following python/machine learning libraries * Glob: This is used fir filename and pattern matching. * Re: This is used for regular expression matching. * Nltk: Natural Language toolkit * Os: IT consists of functions interacting with operating system * Genism: It is the natural language processing library used for unsupervised topic modeling * Pandas: most important library used by data scientist for data analysis. * Matplotlib: Used for visualization * Scipy: used for numerical integration and optimization. Step by Step Code details SET SEARCH CRITERIA: bookInterstedIn = RelativityandGravitation LOAD DATASET AND SORT * Download the dataset and store it in a folder ""library"". * Read the .txt files from the library folder and load it into the memory using glob library. * Sort the files using sort(). Following Output was obtained after reading the files from the ""library"" folder ['library/Relativity.txt', 'library/ExperimentalMechanics.txt', 'library/ThePoetryofScience.txt', 'library/TheEinsteinTheoryofRelativityAConciseStatement.txt', 'library/TheGravityBusiness.txt', 'library/FromNewtontoEinstein.txt', 'library/The BoyPlaybookofScience.txt', 'library/SidelightsonRelativity.txt', 'library/RelativityTheSpecialandGeneralTheory.txt', 'library/TheEinsteinSeeSaw.txt', 'library/AetherandGravitation.txt', 'library/The EarthBeginning.txt', 'library/specialtheoryRelativity.txt', # The folder created below folder = ""library/"" # List all the .txt files files = glob.glob(folder + ""*.txt"") 'library/TheTheoriesof DarwinandTheirRelationtoPhilosophyRelig ionandMorality.txt', 'library/RelativityandGravitation.txt', 'library/ThoughtsonArt.txt', 'library/EinsteinTheoriesofRelativityandGravitation.txt', 'library/TheJuniorClassics.txt'] LOAD TITLES AND TEXT IN OBJECTS * Next step requires converting the data as information instead of string. For that purpose, open the files and encode them with UTF-8 signature (utg-8-sig). When reading the file using utf-8-sig, it will treat BOM as file info. * Further clean up the file and remove the non-alphanumeric characters. * After reading the files, store the text and tiles of the books in two lists and save them as titles and txts. * To remove the folder name and .txt extension from the file name, use the os.path.basename() and replace() functions. #define objects to hold text and titles content_txts = [] book_titles = [] #loop through each, read, encode, remove txt extension for n in files: f = open(n, encoding='utf-8-sig') val = re.sub('[\W_]+', ' ', f.read()) content_txts.append(val) book_titles.append(os.path.basename(n).replace("".txt"", """")) [len(t) for t in content_txts] Here is the Output of above code: [24297, 519663, 875023, 59083, 56364, 169246, 953601, 69853, 24297, 69011, 941247, 600351, 197572, 669416, 548935, 273437, 548935, 716050] LOAD INDEXES * In the next step, we need to store the index of the interested title from the ""titles"" list to a variable ""typeofBook"" * To verify, print the content of the ""typeofBook"" variable. # the list contains all the book titles for i in range(len(book_titles)): if(book_titles[i]==bookInterstedIn): typeOfBook = i print(str(typeOfBook)) Output: 14 TOKENIZE THE CORPUS Now that the information has been collected, we will tokenize the corpus and transform into list of individual words. This is important step as we will now perform following steps in this: * To filter our words for processing, we need to define the stop words * Use lower() method to convert the contents in ""content_txts"" * Breakdown the lower case text into individual words and python provides a method ""split()"" for the same. * Store the split word into another variable ""txts_split"". * Remote the list of stop words in ""stoplist"". * Store the resulting list into another variable ""texts"" * Print first few tokens for the searched books. # Define a list of stop words stoplist = set('for w a of the and to in to be which some is at that we i who whom show via may my our might as well project by gutenberg ebook'.split()) # Convert the text to lower case * The output containing 20 tokens is below: PICKLING THE TOKENIZED CORPUS * In this step, we will generate a stem for each token. * Further, we used the pickle library of python for serializing. Python pickle module is used for serializing and de-serializing a Python object structure. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. txts_lower_case = [txt.lower() for txt in content_txts] # Transform the text into tokens txts_split = [txt.split() for txt in txts_lower_case] # Remove tokens which are part of the list of stop words texts = [[word for word in txt if word not in stoplist] for txt in txts_split] # Print the first 20 tokens texts[typeOfBook][0:20] ['einstein', 'theories', 'relativity', 'gravitation', 'malcolm', 'united', 'states', 'other', 'parts', 'world', 'cost', 'restrictions', 'whatsoever', 'away', 'or', 're', 'under', 'terms', 'license', 'included'] # Create an instance of a PorterStemmer object porter = PorterStemmer() # For each token of each text, we generated its stem texts_stem = [[porter.stem(token) for token in text] for text in texts] # Save to pickle file pickle.dump( texts_stem, open( ""library/porterstem.p"", ""wb"" ) ) STEMMING OF TOKENIZED CORPUS Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Generally, it is also a part of queries and Internet search engines. In our use case, the words related to the concept of selection would be gathered under the select stem. As we are analyzing 25 full books, the stemming algorithm can take several minutes to run. We will then load the final results from a pickle file and review the method used to generate it. Output: BUILDING A BAG-OF-WORDS MODEL Now, we need to build the model using the stemmed tokens, it will be used by algorithms in next part. * We created a dictionary that contains universe of all words in our corpus of books. * Then, using the stemmed tokens and the dictionary, we will create bag-of-words models (BoW) of each of our texts. * The BoW models will represent our books as a list of all unique tokens they contain associated with their respective number of occurrences # Load the stemmed tokens list from the pre generated pickle file texts_stem = pickle.load(open(""library/porterstem.p"", ""rb"" ) ) # Print the 20 first stemmed tokens from texts_stem[typeOfBook][0:20] ['einstein', 'theori', 'rel', 'gravit', 'malcolm', 'unit', 'state', 'other', 'part', 'world', 'cost', 'restrict', 'whatsoev', 'away', 'or', 're', 'under', 'term', 'licens', 'includ'] # Create a dictionary from the stemmed tokens Output: VISUALIZE THE MOST COMMON WORDS For better understanding and interpret the results returned by BoW model, there is a need for visualization. This will help understand which stemmed tokens are present in given book and how many occurrences are found. To visualize the content, we need to transform the content into DataFrame using the libraries and display 10 most common stems for the book as searched book ""search criteria"". dictionary = corpora.Dictionary(texts_stem) # Create a bag-of-words model for each book, using the previously generated dictionary bows = [dictionary.doc2bow(text) for text in texts_stem] # Print the first five elements using BoW model bows[typeOfBook][0:5] [(0, 1), (1, 1), (5, 51), (6, 5), (13, 3)] # Convert the BoW model into a DataFrame df_bow_origin = pd.DataFrame(bows[typeOfBook]) # Add the column names to the DataFrame df_bow_origin.columns = [""index"", ""occurrences""] # Add a column containing the token corresponding to the dictionary index df_bow_origin[""token""] = [dictionary[index] for index in df_bow_origin[""index""]] # Sort the DataFrame by descending number of occurrences and print the first 10 values df_bow_origin.sort_values(by=""occurrences"", ascending=False).head(10) Output: BUILD A TF-IDF MODEL Next, we need to generate the TF-IDF (term frequency-inverse document frequency) model from BoW model using the library function gensim's (TfidfModel()). TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This model defines the importance of each word depending on how frequent it is in this text and how infrequent it is in all the other documents. As a result, a high tf-idf score for a word will indicate that this word is specific to this text. We will compute the score and print the results from model. # Generate the tf-idf model model = TfidfModel(bows) # Print the model model[bows[typeOfBook]] Output: RESULTS OF THE TF-IDF MODEL In order to interpret the results of TF-IDF model, we will display the 10 most specific words for a book. In our case, we used ""Relativity"" book. # Convert the tf-idf model into a DataFrame df_tfidf = pd.DataFrame(model[bows[typeOfBook]]) # Name the columns of the DataFrame id and score df_tfidf.columns=[""id"", ""score""] # Add the tokens corresponding to the numerical indices for better readability df_tfidf['token'] = [dictionary[i] for i in list(df_tfidf[""id""])] # Sort the DataFrame by descending tf-idf score and print the first 10 rows. df_tfidf.sort_values(by=""score"", ascending=False).head(10) Output: COMPUTE DISTANCE BETWEEN TEXTS Stemmed token that are specific to each book are returned by TF-IDF model. The topics defined on the book ""Relativity"" can be seen now (like, gravitation etc). With this, we have a model associating tokens to how specific they are to each book, we can measure how related to books are between each other. #Compute similarity matrix sims = similarities.MatrixSimilarity(model[bows]) # Transform results to DF sim_df = pd.DataFrame(list(sims)) # Add book_titles of the books as columns and index of DF sim_df.columns = book_titles sim_df.index = book_titles # Print matrix sim_df Output: SIMILAR BOOKS The output we have contains the matrix containing all the similarity measures between any pair of books from the library. This matrix will be useful to quickly extract the information like distance between two books or more. This is needed to display plots in a notebook %matplotlib inline # Select the column corresponding to v = sim_df[bookInterstedIn] # Sort by ascending scores v_sorted = v.sort_values(ascending=True) # Plot this data has a horizontal bar plot v_sorted.plot.barh(x='lab', y='val', rot=0).plot() # Modify the axes labels and plot title for better readability plt.xlabel(""Cosine distance"") plt.ylabel("""") plt.title(""Most similar books to ""+ bookInterstedIn) Output BOOKS WITH SIMILAR CONTENT This project/approach is a good fit and of use, if we want to determine similar books that match user's interest. For example, if user picked up the book ""Relativity,"" user can read books discussing similar concepts such as ""Special Theory of Relativity"" or ""Relativity and Gravitation If you are familiar with Einstein's work, these suggestions will likely seem natural to you. However, we now want to have a better understanding of the big picture and see how Einstein's books are generally related to each other (in terms of topics discussed). To this purpose, we will represent the whole similarity matrix as a dendrogram, which is a standard tool to display such data. This last approach will display all the information about book similarities at once. For example, we can find a book's closest relative but, also, we can visualize which groups of books have similar topics # Compute the similarity matrix the WVMA (Ward variance minimization algorithm) Z = hierarchy.linkage(sim_df, 'ward') # Display the results in horizontal dendrogram a = hierarchy.dendrogram(Z, leaf_font_size=10, labels=sim_df.index, orientation=""right"") Output:"
https://github.com/adeetikaushal/CourseProject	CS410_Voltron_Process_Report.pdf	CS-410 Text Information System Fall 2020 Book Recommendation System Progress Report Team: Voltron Team Member Email Captain Adeeti Kaushal adeetik2@illinois.edu Yes Vivek Bansal vivekb3@illinois.edu Progress Report 1) Which tasks have been completed? * All tasks are complete, and report and presentation is submitted for review. 2) Which tasks are pending? * No task pending from the list and if any feedback is received, we will make changes accordingly. 3) Are you facing any challenges? * No. Tasks Status Learning Python Completed Research/Build dataset Completed Code for tokenization, Stemming on corpus Completed Build bag of words, find stop words Completed Build tf-idf Completed Build Similarity matrix Completed Full end to end integration, tuning Completed Testing Completed Visual representation of results (matplotlib) Completed Other use case - Content based Similarity Completed Report Completed - Submitted for Review Presentation Completed - Link checked-in to repo Total Completed
https://github.com/adeetikaushal/CourseProject	README.md	Project Code: https://github.com/adeetikaushal/CourseProject/tree/main/code Project Report: https://github.com/vivekb3Illinois/CourseProject/blob/main/CS410_Voltron_Final_Project_Report.pdf Project Progress Report: https://github.com/vivekb3Illinois/CourseProject/blob/main/CS410_Voltron_Process_Report.pdf Project Video: https://mediaspace.illinois.edu/media/t/1_h258rjku
https://github.com/mzhai20/CourseProject	Documentation.pdf	"1 Documentation for CS410 Final Project: Improving ExpertSearch System Mengyu Zhai (NetID: mzhai) Fall 2020 1. Overview 1.1 Background This project is for Fall 2020 CS410 Text Information Systems final project option 2.2 ExpertSearch System. The information quoted below is from CS410 course project instructions and what contained is the foundation of the current project: ""The ExpertSearch system (http://timan102.cs.illinois.edu/expertsearch//) was developed by some previous CS410 students as part of their course project! The system aims to find faculty specializing in the given research areas. The underlying data and ranker currently come from the MP2 submissions of the previous course offering. You can read more about it here (Sections 3.6 and 4: Project are especially relevant). The code is available here."" 1.2 Functions of Code The current project is trying to enhance the utility of the ExpertSearch System by extracting relevant information from faculty bios. More specifically, the code uses techniques for extracting other information, i.e., faculty research interests, than what is already provided in the original system. Simply put, topic mining is performed on bios and top-keywords are found and shown as the common research areas, in addition to the bio information shown in the search result. This added function does not influence the ways that users can use to conduct searches in the system. Users can search the expert by whatever search key word they want to use and applying whatever location or university filters they choose. Now if the user is especially interested to know which experts work in a certain research area and what their respective research interests are, they can directly search by that research area they have in mind. What is improved is that in the search result, rather than just showing the matching parts in the bio of an expert (or matching parts in the bios of all the experts in the search results) as before, a set of highly possible research interests (using top-keywords) of the expert will also be 2 provided. The method is not perfect as we assume top-keywords are common research areas but cannot guarantee that is always the case. 2. Implementation 2.1 What Are Used * Python * LDA topic model: generative statistical model - to detect topics * gensim API: to lemmatize and add bi-gram tokenization (build dictionary), and train LDA model * pyLDAvis: to provide interactive visualization of LDA topic results * word_cloud: to visualize corpus key words * Other major tools/ packages involved: Jupyter notebook, numpy, pandas 2.2 Steps * With jupyter notebook, utilizing gensim API and LDA to build the topics from the combined bios (10 for each bio). o Preprocess data o Lemmatize and stop word removal o Build dictionary and use word_cloud to visualize the corpus o LDA modeling o Visualize LDA topics with pyLDAvis * Then also within juyter notebook, results are saved into a researchinterest file under ExpertSearch\data. * Finally modify the original ExpertSearch web application source code to add and display top topics as research interests. 2.3 Results The corpus outlook built using word_cloud: 3 pyLDAvis output example (interactive and topic num = 10): An example of some selected topics for a bio: 4 Another example of some selected topics for a different bio: The top topics with the highest weight get saved to researchinterest.txt and will be the one to display in expert search results. Search by key words, you will be able to see the results similar to image below: 5 You can also filter by locations and/or universities: 6 Now compare with the original system's results below, which doesn't provide top topic key-words: 3. Installation and Run Codes uploaded to github include ExpertSearch_LDA.ipynb and web application codes in ExpertSearch.zip. Be aware that the web application codes does not work on Windows (mainly because gunicorn is not supported on Windows)! 3.1 Simple Instructions (If you are familiar with the project) To run the web application code, run the following command from ExpertSearch (work with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ 3.2 More Detailed Instructions (If you are not familiar with the original project) * Download ExpertSearch.zip. * Unzip it in Python2.7 on MacOS and Linux. * Go to the folder and run the following command: 7 [ExpertSearch]$ gunicorn server:app -b 127.0.0.1:8095 * You should see something similar to below once the website is running: [ExpertSearch]$ [2020-12-11 03:07:57 +0000] [17325] [INFO] Starting gun icorn 19.10.0 [2020-12-11 03:07:57 +0000] [17325] [INFO] Listening at: http://127.0. 0.1:8095 (17325) [2020-12-11 03:07:57 +0000] [17325] [INFO] Using worker: sync [2020-12-11 03:07:57 +0000] [17329] [INFO] Booting worker with pid: 173 29 * The site should be available at http://localhost:8095/. Otherwise, you can open the port 8095 so people can access it from http://yourdomain:8095, for example: References: 1. [Reference code]: https://github.com/CS410Fall2020/ExpertSearch/ 2. [Reference code]: https://github.com/TeddyWang0202/BeyondLD 3. [Reference file]: https://bhaavya.github.io/files/SIGCSE2020.pdf"
https://github.com/mzhai20/CourseProject	Project Progress Report - 11-29-2020.pdf	Progress Report 11/29/2020 CS410 Fall 2020 Final Project: Improving Expert Search System Mengyu Zhai Progress made thus far: * Spent a lot of time (much more than planned) acquiring and setting up the running environment that the original code requires, which is different from what I had. * Studied the code and made necessary modifications to make the code run locally. Remaining tasks: * Continue working on the new function code and testing * Make the final demo * Upload all files Challenges/issues being faced: * None now
https://github.com/mzhai20/CourseProject	Project Proposal - Mengyu Zhai.pdf	CS410 Fall 2020 Final Project Proposal: Improving Expert Search System 1. I am doing this project individually. My name is Mengyu Zhai and NetID is mzhai. 2. In this project, I choose to improve the Expert Search System (option 2.2), and more specifically, the subtopic of Extracting relevant information from faculty bios. I plan to add the function of extracting faculty research interests, so users can search faculty by their research interests. 3. Dataset to be used is the bios available under MP2.3 on Coursera Week 5: complied_dataset.zip. According to the introduction on the Coursera page, all the faculty bio submissions from MP2.1 that were submitted by Sep 20 were compiled into this dataset, and there are a total of 16492 unique faculty bios. Algorithms or techniques to be used is topic mining on the bios available under MP2.3 on Coursera. The top-keywords per topic are the common research interests in Engineering and Science. 4. I am adding a function that is not in the original system, so it is relatively easy to demonstrate that it works as expected - In the final demo, I will perform some search with some research interests and show that it can really extract the relevant results. 5. I plan to utilize the system and code given, and then add & modify codes that can achieve the added function. 6. As the original code is in Python, I plan to use Python as the programming language. 7. As an individual project, its planned workload is about 20 hours, not including the time used for this proposal. I may spend more hours because I am not a Python expert, but I wouldn't count much of that into the workload of the project. Main Tasks Estimated Time Cost Manually pick common research interest areas (topic terms) from a typical Engineering and Science department (plan to use UIUC CS department faculty's research interests pool https://cs.illinois.edu/about/people/all-faculty ) 3 hours Study original system code and system 2 hours Work on the new function code and testing 13 hours Make the final demo 1 hour Upload all files (proposal, progress report, project code and documentation, project presentation/demo) 1 hour
https://github.com/mzhai20/CourseProject	README.md	CourseProject Final project submissions include: - Documentation.pdf - Code - ExpertSearch_LDA.ipynb - ExpertSearch.zip (in Releases on the right side of the page, under About section) - Software usage tutorial presentation link: https://youtu.be/WXJgNFB4Nxo
https://github.com/mzhai20/CourseProject	Software Usage Tutorial Presentation.md	Software usage tutorial presentation link: https://youtu.be/WXJgNFB4Nxo
https://github.com/kn13-kiran/CourseProject	cs410_CourseProject_MenuCrawler.pptx	RESTAURANT MENU CRAWLER & CLASSIFIER Kiranmayee Nimashakavi (cs410 COURSE PROJECT) PROJECT OVERVIEW Context & Objective A restaurant review company would like to provide recommendation on menu items across the world. To accomplish this goal, the first stage of the pipeline is to identify the URLs that relevant (i.e. find the URLs that contain menu information) This tool crawls the URLs, classifies the pages that contain menu. Components Scrapy An open source and collaborative framework (python based) for extracting the data you need from websites. BeautifulSoup Beautiful Soup is a Python library for pulling data out of HTML and XML file Metapy Python bindings for MeTA toolkit that tokenizes documents, creates indexes, inverted indexes, classifies, creates topic models etc., Scrapy architecture Project design 1. List of URLs Main.py Duplink Remover PageDataOutput Scrapy Engine Pipelines Menu Spider Spiders Meta Py Menu Classifier 2b. Downloaded Content 4. Classify using Page Data 6. Uses pre-indexed training data set 5. Uses MetaPy to classify Training Dataset + Indexes Beautiful Soup 3. Extract Text data from Page 7. Save Menu Links, processed links Down loader 2a. Download pages Final results 8. Save Menu Links, processed links Design/code level overview Contains all scrapy plug-ins Contains custom spider code Stores log files Contains the pre-classified seed data Contains All URLs that were crawled URLs classified as MenuPages demo
https://github.com/kn13-kiran/CourseProject	ProjectStatusReport.docx	WebCrawler for Restaurant Menus: From a given list of URLs, this crawler looks for pages that are relevant to restaurants, pages that contain restaurant menu and downloads the menus and drops the remaining pages. The high-level process can be summarized as: From a Base URL, crawl the home page. Using title, head, page tags identify whether that site will contain Restaurant menu or not? Only for Restaurant menu related sites, find all links Classify the pages that contain the menu and keep the menu pages. What is completed: Architecture Design Finalized tools and technologies. Design of base classes Key Components: Classifier: Classifier uses title, head and page tags to classify relevance of the page to find out whether a page is a restaurant page or not. If it is a restaurants page, then further crawling is performed. WebCrawler: WebCrawler is used to crawl main pages, links under that page etc. It stops crawling page if relevance score goes to zero or lesser (i.e., that site doesn't contain menu). Web crawler takes inputs seed data and classifier instance which has to inherit from web classifier. Using seed URLs, it starts extracting title, head, page tags identify whether that category is restaurant, or the page contains menu etc., Which tasks are pending? Training - manually create training data set by using existing known restaurant sites, sites containing menu information, non-restaurant pages and converting them to metapy format. Code, Integration testing - Developing the scraping code, classifier code and integrate them and test it end2end. Are you facing any challenges? Integrating classifier with Scrapy. Identifying pages that contains menus from other pages in a site.
https://github.com/kn13-kiran/CourseProject	README.md	Restaurant Menu Crawler & Classifier A restaurant review company would like to provide recommendation on menu items across the world. To accomplish this goal, the first stage of the pipeline is to identify the URLs that relevant (i.e. find the URLs that contain menu information). This project focuses on the first stage. The summary of the video can be found herehere Components There are two major components of this project: Menu classifier - which uses anchor text, page title head>title, and body>p data to classify relevance of the page. This uses NaiveBayes classification and provides score either 0 or 1. Menu Spider - Web crawler that downloads, parses and extract the data from the links. Training data set for classifier - I've manually created, curated and labelled menu data from menupages.com dataset/sample/menu.txt - contains menu data from different types of restaurants in different cities. dataset/sample/other.txt - contains non menu relevant data. Functional Flow main.py: The driver code that triggers the crawling process. This component takes target URLs (URL that needs to be checked for Menu information) as an input. First, MenuClassifer is initialized using the training dataset, creates indexes and inverted indexes using MeTA. On target urls, driver code starts crawling using Scrapy engine. Scrapy Engine has a call back mechanism that calls two main components Menu Spider and Pipelines for every page that is downloaded. MenuSpider: Responsible for extracting, parsing page content and holds them in the plain text fields. This plain content is passed to the MenuClassifier and score is calculated. Based on the score, further processing of the links with in that page is determined. If the score is <=0 further parsing of the links is abandoned. Crawling ends when all the links are exhaused or when the score reaches 0. Pipelines: DuplicateLinkRemover: Tracks already processed URls and removes circular links so that they don't get processed again. PageDataOutput : Stores the pageURL, scores to finalresults directory. Project Dependencies This projects uses some third party components, you have to install these components first to run this project * conda - to create python 3.5 virtual environment * python 3.5+ * scrapy 2.3.0 * metapy * Beautiful Soup version 4+ Install scrapy Crawler depends scrapy. Before running this project, install scrapy using following commands. pip install scrapy Install metapy We are using metapy---Python bindings for MeTA. Install metapy using the following commands. pip install metapy Install Beautiful Soup pip install beautifulsoup4 Running the project There is main.py file which needs to be invoked by passing target_urls (comma separated urls). python main.py url1,url2,...,urln For example - python main.py https://papillonrestaurant.com,http://www.cnn.com This command will generate output to finalresults folder. restaurant_menu_crawler_all_links.txt - holds all the urls that were crawled. restaurant_menu_crawler_menu_links.txt - holds the urls that contain menu informaiton. menucrawler/log -> contains log file. Termination This project will keep crawling until resources is exhausted or no more relevant url to crawl.If you want to stop crawling immediately then press [ctrl^c] which unsafely stop crawler immediately. Future work Improving Sample Data - I could add additional data to include different types of cuisines. Improving Classification - Current classification is binary , it can be further improved to include type of cuisine. Tracking Zipcodes - The next part of this pipeline requires us to store the zipcode information associated with the restaurant. Currently, zipcode information is not saved, but, it can also be added to the output.
https://github.com/kn13-kiran/CourseProject	The proposal.docx	What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This is a solo project. My name is Kiranmayee Nimashakavi. My NetId is kn13@illinois.edu. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? WebCrawler and classifier for MenuPages.com or some site that offers restaurant menus. The objective is to crawl the entire site and collect all pages and then classify the pages that has menu information. This information can be used to create a dish specific search on the restaurant menu. My approach is to build a non-recursive crawler that extracts all pages from a given list of sites and use some classifier as (naive bayes) to classify the pages. I've not decided on the tools to use on this project yet. The specific set of systems that are used are the list of websites that will be given as an input to the program. Expected outcome is to store the Menu pages to a specific directory. I will manually test the output data by supplying some random collection of URLs. If time permits, I will implement the dish search on the dataset that was classified as a menu page. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The following activities has to be performed to accomplish this project. Design and implementation of the non-recursive crawler. - 5 hours Design and implementation of the classifier - 3 hours Unit and manual testing of the crawler and classifier - 5 hours Performance tuning of crawler and classifier - 3 hours Documentation of the project - 4 hours Presentation of the project -2 hours
https://github.com/nadiawoodninja/CourseProject	CS410ProjectFinalReport.docx	"Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 Final Project Netid: nadiaw2 Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 Final Project Netid: nadiaw2 Table of Contents Abstract 2 Video 2 Code Repository 2 Demo App 2 Technical Architecture 3 Setting up Elastic Search in Google Cloud. 3 Data Pipeline 10 Fine Tuning the Engine 11 Creating a search UI to search data 12 Setting a development Environment locally on your computer. 12 Creating a UI for search experience by using App Search packages 14 Creating a search experience 14 Deploying the React app to Google Cloud Platform 17 Create the app on the App Engine 17 Deploy the app 21 Abstract In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1. Video Code Repository Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n Technical Architecture Figure 1 Setting up Elastic Search in Google Cloud. Go to https://console.cloud.google.com/ and create a project on Google Cloud. Choose Elasticsearch Service. The only reason I chose this is to get some free credit to do my project work. You can create a separate account on Elasticsearch if you want to but the trial only lasts for 14 days. A little bit about, Elasticsearch Service on Google Cloud: The service offers seamless integrated billing through your Google Cloud account for simple management and powerful customization. Once the service is setup, you can click on manage on provider, to go directly to Elastic Cloud, to manage and create a search deployment on the cloud. https://cloud.elastic.co/ Once you login, you will be taken to the Elastic Cloud dashboard. Here you can create your ""deployments"" . When you create a deployment, you are given a choice of selecting from pre-configured environments for your need. In my case, I chose the Elastic Enterprise Search solution to allow me to create a search experience on my web app. Elastic technology provides the following stack options. For my project I am using the stack outlines in red. This stack gives me the Elastic Cloud, which gives me the ability to make RESTFUL API calls to my search engine. Once your deployment is created you will be taken to the deployment dashboard. In this project, we will be focusing on using Enterprise search capability. Once you launch Enterprise Search, it will give you an option to select a product. For this project, I used App Search. An overview of the architecture of App Search stack is below. I have highlighted the architectural components which are being utilized for this project. Once the App Search is launch, it gives you an option to create a search engine. For this project, I created an engine called environmentaldata. This search engine allows a user to search through documents which contain sensor data e.g. humidity, temperature, light, battery info etc. Currently the engine contains xxxx documents. The documents were loaded into the engine by uploading JSON file to the engine. The engine requires a specific formatting for the json files to adhere to. The JSON field names have to be all lowercase or be separated by underscore. This created a need to automate the conversion of existing json files to be converted to the required format. Data Pipeline Data Ingestion program in C#: In order to quickly load json files to the engine, I created a program in C# to convert existing files to a proper json file so that it can be imported into App Search. This code can be run if you have Visual Studio free community version installed. The program requires to have a ""data"" folder where the files needed to be converted need to stored. The converted files are stored in the ""data/converted"" folder. I have included some converted file in the repo as well: https://github.com/nadiawoodninja/CourseProject/tree/main/data/converted Fine Tuning the Engine Once the documents are loaded into then engine, you can index any JSON object. The json object will become a search-optimized document within your Engine. A schema is created for you when you index your data - you do not need to specify any schema or alter your data before uploading. You can alter your schema later to set the appropriate data types. You also have the option to refine search by using features like, Relevance Tuning, Synonyms & Curations. For this project I utilized the Synonym feature as we may have data from different sensors and the same datapoint maybe spelt differently or represented differently. Creating a search UI to search data Setting a development Environment locally on your computer. Download and install Node.js from https://nodejs.org/en/ Once installation is complete run this command. We are going to use this to create a react app. npm i -g create-react-app Once the package is installed create the react app by running the command below. create-react-app 410-search-ui This command installs a light weight web server, webpack to bundle our code for deployment and Babel for compiling our JavaScript code. Once the app is created go to folder 410-search-ui and run this command. This will launch our development server on localhost:3000 npm start Creating a UI for search experience by using App Search packages Install React Search UI and the App Search connector by running these commands npm install --save @elastic/react-search-ui @elastic/search-ui-app-search-connector Creating a search experience I use Atom as my editor for React apps. The app folder contains src folder which contains all the source code. App.js is the main file where the program starts execution. The src folder also has a config folder which contains engine.json. This file contains all the configuration needed to configure your search UI. In this file you can define your ""facets"", the fields which will be displayed on your results page, your sort fields etc. Figure 2: engine.json The ability to define these configurations are provided by the packages which were installed above. Figure 3: App.js Deploying the React app to Google Cloud Platform Create the app on the App Engine Go to Google's App Engine Console and create a new project: Once the project is created, create an App Engine application. Select a region Select Node.js and standard environment Clone our app's source code from GitHub Activate the shell by clicking git clone https://github.com/nadiawoodninja/CourseProject.git Install npm by running and install other elastic search packages npm i npm install @elastic/search-ui-app-search-connector npm install @elastic/react-search-ui Build our app for deployment To do this, simply go into your app's root folder (where your ""src"" folder is), cd CourseProject cd 410-search-ui And type the following command: npm i npm run build This creates a folder named ""build"" in our root directory. Delete every thing else besides the build folder. Get rid of everything else, except for the build folder. Use these commands to remove files and folders rm <file-to-remove> rm -r <remove-recursively-like-directories-inside-directories> Add an app.yaml and deploy In the same folder where we have our ""build"" folder, create a new file named app.yaml. By the end of this step, the only things left should be the ""build"" folder and ""app.yaml"". That's all the App Engine will need to run our app. touch app.yaml nano app.yaml And add the following to its content: runtime: nodejs12 handlers: # Serve all static files with url ending with a file extension - url: /(.*\..+)$ static_files: build/\1 upload: build/(.*\..+)$ # Catch all handler to index.html - url: /.* static_files: build/index.html upload: build/index.html Deploy the app Deploy the app using the following command gcloud app deploy The app is running here https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n"
https://github.com/nadiawoodninja/CourseProject	CS410ProjectFinalReport.pdf	"Search engine for indoor environment data using ElasticSearch and front end search UI using React. CS410 FINAL PROJECT NETID: NADIAW2 Table of Contents ABSTRACT ............................................................................................................................................ 2 DEMO APP ........................................................................................................................................... 2 TECHNICAL ARCHITECTURE .................................................................................................................. 3 SETTING UP ELASTIC SEARCH IN GOOGLE CLOUD. ................................................................................ 3 DATA PIPELINE ................................................................................................................................... 10 FINE TUNING THE ENGINE .................................................................................................................. 11 CREATING A SEARCH UI TO SEARCH DATA .......................................................................................... 12 SETTING A DEVELOPMENT ENVIRONMENT LOCALLY ON YOUR COMPUTER. ........................................................... 12 CREATING A UI FOR SEARCH EXPERIENCE BY USING APP SEARCH PACKAGES ......................................................... 14 CREATING A SEARCH EXPERIENCE ............................................................................................................... 14 DEPLOYING THE REACT APP TO GOOGLE CLOUD PLATFORM .............................................................. 17 CREATE THE APP ON THE APP ENGINE ......................................................................................................... 17 DEPLOY THE APP .................................................................................................................................... 21 Abstract In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1. Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Technical Architecture Figure 1 Setting up Elastic Search in Google Cloud. 1. Go to https://console.cloud.google.com/ and create a project on Google Cloud. 2. Choose Elasticsearch Service. The only reason I chose this is to get some free credit to do my project work. You can create a separate account on Elasticsearch if you want to but the trial only lasts for 14 days. 3. A little bit about, Elasticsearch Service on Google Cloud: The service offers seamless integrated billing through your Google Cloud account for simple management and powerful customization. 4. Once the service is setup, you can click on manage on provider, to go directly to Elastic Cloud, to manage and create a search deployment on the cloud. https://cloud.elastic.co/ 5. Once you login, you will be taken to the Elastic Cloud dashboard. Here you can create your ""deployments"" . 6. When you create a deployment, you are given a choice of selecting from pre-configured environments for your need. In my case, I chose the Elastic Enterprise Search solution to allow me to create a search experience on my web app. Elastic technology provides the following stack options. For my project I am using the stack outlines in red. This stack gives me the Elastic Cloud, which gives me the ability to make RESTFUL API calls to my search engine. 7. Once your deployment is created you will be taken to the deployment dashboard. In this project, we will be focusing on using Enterprise search capability. 8. Once you launch Enterprise Search, it will give you an option to select a product. For this project, I used App Search. 9. An overview of the architecture of App Search stack is below. I have highlighted the architectural components which are being utilized for this project. 10. Once the App Search is launch, it gives you an option to create a search engine. 11. For this project, I created an engine called environmentaldata. This search engine allows a user to search through documents which contain sensor data e.g. humidity, temperature, light, battery info etc. Currently the engine contains xxxx documents. The documents were loaded into the engine by uploading JSON file to the engine. 12. The engine requires a specific formatting for the json files to adhere to. The JSON field names have to be all lowercase or be separated by underscore. This created a need to automate the conversion of existing json files to be converted to the required format. Data Pipeline 13. Data Ingestion program in C#: In order to quickly load json files to the engine, I created a program in C# to convert existing files to a proper json file so that it can be imported into App Search. This code can be run if you have Visual Studio free community version installed. The program requires to have a ""data"" folder where the files needed to be converted need to stored. The converted files are stored in the ""data/converted"" folder. I have included some converted file in the repo as well: https://github.com/nadiawoodninja/CourseProject/tree/main/data/converted Fine Tuning the Engine 14. Once the documents are loaded into then engine, you can index any JSON object. The json object will become a search-optimized document within your Engine. A schema is created for you when you index your data - you do not need to specify any schema or alter your data before uploading. You can alter your schema later to set the appropriate data types. 15. You also have the option to refine search by using features like, Relevance Tuning, Synonyms & Curations. For this project I utilized the Synonym feature as we may have data from different sensors and the same datapoint maybe spelt differently or represented differently. Creating a search UI to search data Setting a development Environment locally on your computer. 16. Download and install Node.js from https://nodejs.org/en/ 17. Once installation is complete run this command. We are going to use this to create a react app. npm i -g create-react-app 18. Once the package is installed create the react app by running the command below. create-react-app 410-search-ui This command installs a light weight web server, webpack to bundle our code for deployment and Babel for compiling our JavaScript code. Once the app is created go to folder 410-search-ui and run this command. This will launch our development server on localhost:3000 npm start Creating a UI for search experience by using App Search packages 19. Install React Search UI and the App Search connector by running these commands npm install --save @elastic/react-search-ui @elastic/search-ui-app-search-connector Creating a search experience 20. I use Atom as my editor for React apps. The app folder contains src folder which contains all the source code. App.js is the main file where the program starts execution. 21. The src folder also has a config folder which contains engine.json. This file contains all the configuration needed to configure your search UI. In this file you can define your ""facets"", the fields which will be displayed on your results page, your sort fields etc. Figure 2: engine.json The ability to define these configurations are provided by the packages which were installed above. Figure 3: App.js Deploying the React app to Google Cloud Platform Create the app on the App Engine 22. Go to Google's App Engine Console and create a new project: 23. Once the project is created, create an App Engine application. 24. Select a region 25. Select Node.js and standard environment 26. Clone our app's source code from GitHub 27. Activate the shell by clicking git clone https://github.com/nadiawoodninja/CourseProject.git 28. Install npm by running and install other elastic search packages npm i npm install @elastic/search-ui-app-search-connector npm install @elastic/react-search-ui 29. Build our app for deployment To do this, simply go into your app's root folder (where your ""src"" folder is), cd CourseProject cd 410-search-ui And type the following command: npm i npm run build This creates a folder named ""build"" in our root directory. 30. Delete every thing else besides the build folder. Get rid of everything else, except for the build folder. Use these commands to remove files and folders rm <file-to-remove> rm -r <remove-recursively-like-directories-inside-directories> 31. Add an app.yaml and deploy In the same folder where we have our ""build"" folder, create a new file named app.yaml. By the end of this step, the only things left should be the ""build"" folder and ""app.yaml"". That's all the App Engine will need to run our app. touch app.yaml nano app.yaml And add the following to its content: runtime: nodejs12 handlers: # Serve all static files with url ending with a file extension - url: /(.*\..+)$ static_files: build/\1 upload: build/(.*\..+)$ # Catch all handler to index.html - url: /.* static_files: build/index.html upload: build/index.html Deploy the app 32. Deploy the app using the following command gcloud app deploy 33. The app is running here https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n"
https://github.com/nadiawoodninja/CourseProject	CS410ProjectProgressReport.docx	Project Progress Report CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: Architecture Diagram Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Hours Status Determine how to set up, configure and deploy ElasticSearch on a cloud platform : The cloud platform determination will base on the cost difference between Azure, Google and AWS. 15 hours Completed. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. Migration of raw to json data to Elastic search server : 3 hours Completed. Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. Development and deployment of Web application: 10 hours Started. 8 hours' worth of work left. Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. Test and measure outcomes: 4 hours Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React.
https://github.com/nadiawoodninja/CourseProject	CS410ProjectProgressReport.pdf	"Project Progress Report CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application to use ElasticSearch to search content from a set of documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Architecture Diagram Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Hours Status 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 15 hours Completed. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. 2. Migration of raw to json data to Elastic search server : 3 hours Completed. Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. 3. Development and deployment of Web application: 10 hours Started. 8 hours' worth of work left. Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. 4. Test and measure outcomes: 4 hours Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React."
https://github.com/nadiawoodninja/CourseProject	EnvData_ElasticSearch_ProjectProposal.docx	Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web application in C#.net to use ElasticSearch to search content from a database of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in C# will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: Figure 1 The estimated time to complete this project is about 30-40 hours. Tasks and time: Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours The cloud platform determination will base on the cost difference between Azure, Google and AWS. Development and deployment of Web application: 10 hours Migration of SQL database to Elastic search server : 3 hours Test and measure outcomes: 4 hours
https://github.com/nadiawoodninja/CourseProject	EnvData_ElasticSearch_ProjectProposal.pdf	"Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch NetID: nadiaw2. I will be working on this project individually. Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster.1 I will be creating a web application in C#.net to use ElasticSearch to search content from a database of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data transfer it to ElasticSearch server. The front end written in C# will allow users to search for data. Measurable outcomes are going to be the amount of time it takes to run a query against a traditional RDMS database vs. using ElasticSearch. The planned architecture is shown in Figure1: 1 Gujarat, India, Darshita Kalyani, and Dr. Devarshi Mehta, ""Paper on Searching and Indexing Using Elasticsearch,"" International Journal Of Engineering And Computer Science, June 30, 2017, https://doi.org/10.18535/ijecs/v6i6.45. Figure 1 The estimated time to complete this project is about 30-40 hours. Tasks and time: 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 2. Development and deployment of Web application: 10 hours 3. Migration of SQL database to Elastic search server : 3 hours 4. Test and measure outcomes: 4 hours"
https://github.com/nadiawoodninja/CourseProject	README.md	CourseProject Project Proposal CS 410 - Text Information Systems N. Wood (nadiaw2) Search engine for indoor environment data using ElasticSearch Presentation Video Part 1: https://uofi.box.com/s/a9p26mi8eym9i0n3ql8d83jke6ztzjbz Part 2: https://uofi.box.com/s/gvsrrqsf9rpqbr48ha20slvg4m5eovky Final Report https://github.com/nadiawoodninja/CourseProject/blob/main/CS410ProjectFinalReport.pdf Demo App https://cs410-env-search-app.uc.r.appspot.com/?size=n_20_n NetID: nadiaw2. Individual Abstract: In the age of sensors, devices and platforms collecting millions of datapoints every second, it comes necessary to be able to sift through all the data to develop insights efficiently Big data offers the solution for analyzing large amount of data and using the technique of Elasticsearch, access to data can be made faster. I will be creating a web front end in React to use ElasticSearch's App Search to search documents of environmental datapoints collected by sensors (indoor air, humidity, temp etc). Currently, it is difficult to search for data in a RDMS database and it takes significant time using traditional SQL queries. The project will take the data, transfer it to ElasticSearch platform. The front end written in React will allow users to search for data. Measurable outcomes are going to be the amount of effort it takes to search for data in traditional RDMS database vs. using ElasticSearch's platform. The planned architecture is shown in Figure1: Figure 1 ============================== The estimated time to complete this project is about 30-40 hours. Tasks and time: 1. Determine how to set up, configure and deploy ElasticSearch on a cloud platform : 15 hours a. The cloud platform determination will base on the cost difference between Azure, Google and AWS. 2. Development and deployment of Web application: 10 hours 3. Migration of SQL database to Elastic search server : 3 hours 4. Test and measure outcomes: 4 hours PROGRESS REPORT Progress Report The estimated time to complete this project is about 30-40 hours. Tasks and time: Task Determine how to set up, configure and deploy ElasticSearch on a cloud platform : The cloud platform determination will base on the cost difference between Azure, Google and AWS. Created an account on Google and hosted ElasticSearch as a hosted service on ElasticCloud. Hours 15 hours Status Completed Task Migration of raw to json data to Elastic search server : Hours 3 hours Status Completed Task Created a data ingestion pipeline to read in a file which contains json per line and convert the file into an array of jsons. Then uploaded the data to ElasticSearch Cloud. 3. Development and deployment of Web application: Creating a CRUD application in REACT using the API to communicate with the ElasticSearch Engine. Hours 10 hours Status Started. 8 hours' worth of work left. Task Test and measure outcomes Hours 4 hours Status Not started. Issues: Originally the plan was to develop a C#.NET application, but further research supported developing a front end using Node.js or React. I will be creating A CRUD (Create, read, update, delete) client using React. Understanding the ElasticSearch Stack takes a lot longer than expected. Had to make very conscious decisions what to implement.
https://github.com/kaipak/CourseProject	project_report.pdf	Team Kayak Karlin Dye and Kai Pak CS 410 Text Information Systems, Fall 2020 December 13, 2020 BERT For 100% Accuracy (Sarcasm) 13th December, 2020 Introduction In this paper, we discuss the implementation of a pre-trained BERT NN model to a binary classification problem based on NLP characteristics to detect sarcasm in a dataset consisting of a series of Twitter messages (tweets). We find that the BERT model performs exceedingly well with minimal data processing and hyperparameter tuning. Through repeated experiments, the team consistently scored .71-.75 on F1 with a final F1 score on the leaderboard of 0.745 and beating the baseline. The following sections provide some information on the BERT language model, the infrastructure, language, and libraries for this project, the code, a discussion about the training process and results, and instructions on how to reproduce our final output. Introduction to BERT Language models are key components for applications ranging from speech recognition to information retrieval. In the realm of information retrieval, often a unigram model or bag of words representation is employed. However, there are much more advanced language models that employ deep neural networks that can take word context into account and output embeddings of words in a continuous space. BERT, or Bidirectional Encoder Representations from Transformers, is one of these deep learning language models that is able to include context from both the left and right sides of words. BERT was developed by researchers at Google AI Language. More specifically the BERT language model is a pre-trained neural network that through transfer learning can be refined for a particular corpus of new documents. Since the output of word embeddings from the BERT language model includes contextual understanding the embeddings perform well in the realm of text classification. The BERT neural network architecture consists of 24 layers of transformer blocks, 16 attention heads, and 340 million parameters. The BERT model employs a preprocessing step for input text that can take into account a pair of sequential sentences and includes information about the word positions, sentence positions, and the words themselves. This preprocessing step for text input is an important component of the model training process. Once input is properly preprocessed the BERT model uses some novel techniques as part of the training process. One challenge involved is how to include both left and right context in a final output embedding. The BERT researchers solved this problem by utilizing a technique called Masked LM (MLM) which masks 15% of input tokens at random and then uses a classification layer to predict the masked token. An additional strategy that the BERT model uses to help encode context is a technique called Next Sentence Prediction (NSP). The process involves an additional classification layer that is given pairs of sentences and is trained to predict whether one of the sentences in the pair is subsequent from the first. This technique is reliant on the preprocessing of text input that provides information about sentence positions. During training a random sentence from the corpus is paired with an input sentence 50% of the time and the other 50% of the time is paired with the actual subsequent sentence pair from the input. BERT has been pre-trained on an extremely large corpus of text and is often fine-tuned for a specific application. BERT can be used in a wide range of applications including sentiment analysis, next sentence classification, question answering tasks, named entity recognition (NER), and many more. The use of BERT for many of these tasks have achieved state-of-the-art results. There are many options for using BERT for text classification tasks. One common approach is the use of the PyTorch Transformers library from HuggingFace. This library contains pre-trained BERT models of different sizes and for different languages and use-cases. The full documentation for how to use the library is available at: https://huggingface.co/transformers/model_doc/bert.html Model Code Two classes were written to handle data processing and the model. Utilizing huggingface package with Python and Pytorch as deep learning framework, we wrote the following classes: DataPrep The DataPrep class is used to process the train.jsonl and test.jsonl files and write out three CSV files that are used for training the classification model and one CSV file that will be used for getting predictions for submission. The response_only  argument can be used when instantiating the class to either write out the tweet response by itself or both the response and context tweets concatenated together. The train_test_split  method is used to split the train.jsonl data into a training, validation, and test set to be used for model training. When training the neural network the validation set is used to monitor progress. After training is complete the test set is used to do a final evaluation of the models performance. The write_data  method writes out three CSV files for the training, validation, and testing along with a file that will be used to generate the predictions for submission. SarcasmDetector The SarcasmDetector class is used to instantiate, train, evaluate, and perform predictions with the BERT based neural network model. For the sake of brevity not all methods and attributes will be described in this paper. A sample notebook in the repository will provide additional detail on how to properly use this class. The primary usage of the class for model training, evaluation, and prediction uses the following sequence of methods: 1. Instantiate the class with arguments indicating the directories where the CSV data resides, the model weights should be saved, and the training tensorboard logs should be saved. 2. Use the tokenize_data  method to tokenize the training, validation, and test text data appropriately for use in the BERT pre-trained model. The names of each file will need to be passed along with an appropriate batch size for training (if you are running out of memory when attempting to train reduce this value). 3. The tune  method is then utilized to perform a gridsearch for the optimal learning rate hyperparameter. A list of learning rates and a list of the number of epochs to train for must be supplied as arguments when using this method. 4. The evaluate  method can then be used to provide precision, recall, and f1 metrics for the trained model using the test set. 5. Lastly, the predict  method uses the trained model to create predictions on the tweets supplied for submission. The output of this method is a dataframe that can then be written to disk for submission. Infrastructure We trained our models on the Collab Google environment which is a virtualized jupyter notebook utilizing Tesla GPUs for training. Results TensorBoard was used to log training and validation loss during the hyperparameter tuning process. The screenshot below shows the results from a number of training runs utilizing different learning rates and number of epochs. It turned out that using the tweet responses concatenated with context and a learning rate of 8e-7 was the best performing model. Below are the precision, recall, and f1 scores for this model on the held out test data.
https://github.com/kaipak/CourseProject	README.md	CourseProject - BERT For 100% Accuracy (Sarcasm) This readme serves as technical documentation and overview of the project code. Please review the PDF included in this repository for more detailed information on the BERT model, results, and technical details. Technical Requirements The code has been tested on Python 3.8.x and requires the following core packages. torch >= 1.5 transformers >= 3.5.1 tensorflow >= 2.3.1 pandas >= 1.1.4 numpy >= 1.18.5 scikit-learn matplotlib seaborn It is highly recommended to use virtual environments for running this code, or on a dedicated cloud environment such as Google Colab. Running in Google Colab This is probably the easiest way to get things going. The TeamKayak_SarcasmDetectorDemo.ipynb notebook located in this repo gives a good overview of the environment set up steps you'll need to run this repo. Install required packages You will want to be able to sync to Google Drive so you can check out this repo and have it available to Colab. Running the cell below will install the Google package and mount your GDrive. Then, run the sample git clone command replacing my_directory to a location of your convenience. from google.colab import drive drive.mount('/content/drive', force_remount=True) !git clone https://github.com/kaipak/CourseProject.git my_directory !pip install transformer Set up Environment You will need to set up some directories in your GDrive to hold the input datasets and so the code can also output processed data, model checkpoints, metrics, etc. You can use the following cell as a template. ``` Set Paths for Google Drive source_folder = '/content/drive/My Drive/Data' destination_folder = '/content/drive/My Drive/Model/Response' code_folder = '/content/drive/My Drive/CourseProject/src' train_log_dir = '/content/drive/My Drive/logs/tensorboard/train/' ``` Then, import needed packages to your notebook: ``` import sys, os from pathlib import Path import random import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sys.path.append(code_folder) from model import SarcasmDetector from data_prep import DataPrep ``` Prepare Data The input data for the model can be processed using our data_prep class: ``` Prepare Data data_prepper = DataPrep(train_path=source_folder + '/train.jsonl', sub_path=source_folder + '/test.jsonl', response_only=False) data_prepper.train_test_split() data_prepper.write_data(datapath=source_folder) ``` Most of the parameters have sane default values, but you can see the docstrings to alter if you like. Train a Model The model should be ready to train now. ``` Instantiate Model SarcModel = SarcasmDetector(input_dir=source_folder, output_dir=destination_folder train_log_dir = train_log_dir ``` Tokenize text Then, tokenize the input data: ``` SarcModel.tokenize_data(train_fname = 'train.csv', validate_fname = 'validate.csv', test_fname = 'test.csv', batch_size = 8 ``` Start up Tensorboard This will give you feedback on how training is going. Run the magic command. %tensorboard --logdir Train! There's a tune() method that does grid search to find best set of hyperparameters but just running on lists of singletons is essesntially as running train(). ``` learning rate list lr_list = [5e-7] number epochs list num_epochs_list = [15] tune hyperparameters SarcModel.tune(lr_list=lr_list, num_epochs_list=num_epochs_list ``` Evaluate You can then run the trained model against the test set to get some metrics on how well it does on unseen observations. SarcModel.evaluate(model_name = 'lr_5e-07_epochs_15_')
https://github.com/kaipak/CourseProject	team_kayak_progress_report.docx	Kai Pak and Karlin Dye November 29, 2020 CS410 Team Kayak Project Progress Report Progress made thus far We have implemented a BERT based pretrained model using huggingface transformers on Pytorch framework. We have already beat baseline and our score is currently #18 on the leaderboard (kaipak) Model has been deployed on Google Collab as well as on local workstation with GPU. Remaining tasks We plan on continuing fine tuning the model including experimenting with combinations of context and response. Develop systematic way of searching for optimal hyperparameter tuning including finding optimal learning rate Clean code Write documentation Create short presentation Any challenges/issues faced Investigate variances in performance when no parameters are modified Could use better GPU
https://github.com/kaipak/CourseProject	team_kayak_project_proposal.docx	CS 410 Team Kayak Course Project Proposal Project Topic: Text Classification Competition - Twitter Sarcasm Detection What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Kayak Names NetIDs Kai Pak - Captain kaipak2 Karlin Dye karlind2 Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Neural Network Architectures: RNN, CNN, LSTM, Bi-LSTM, C-LSTM, BERT Deep learning frameworks experience: Tensorflow, Pytorch, Keras, Fastai Platforms: Google Colab, AWS Sagemaker and AI/ML tools Past Experience: Have used neural networks for image classification, segmentation, and entity detection and classification. Which programming language do you plan to use? Python
https://github.com/RyoTakaki/CS410_TIS_CourseProject	Progress Report.pdf	"CS410 Text Information Systems (Fall 2020) Project Progress Report Text Classification Competition: Twitter Sarcasm Detection Ryo Takaki 1. Progress made thus far 1.1 Project outline: The goal of this project is to classify tweets into two labels(""SARCASM"" and ""NOT_SACASM""). A performance of the classified label will be evaluated by using F1 score, and will be compared to a baseline performance generated by a state-of-the-art model. 1.2 Progress: I decided to use a BERT(Bidirectional Encoder Representations from Transformers) which is known to have a good performance with text classification problems. Fortunately, an implementation of the BERT model was easy and a performance for our task(twitter sarcasm detection) was good enough to beat the baseline. Specifically, although it was the first try to train the model, the F1 score was 0.744 (Baseline: 0.723). 2. Remaining tasks 2.1 Model update: As I have already outperformed the baseline performance, additional parameters tuning for the model training is not necessary. 2.2 Presentation preparation: For the final presentation, remaining tasks below will be done. * Add comments to the code * Make a project presentation video 3. Any challenges/issues being faced Currently, I do not have any issues. Therefore, no additional support is needed. 1"
https://github.com/RyoTakaki/CS410_TIS_CourseProject	Project Final Report.pdf	"CS410 Text Information Systems (Fall 2020) Project Final Report Text Classification Competition: Twitter Sarcasm Detection Ryo Takaki 1. Overview of functions 1.1 Twitter Sarcasm Detection The main function of this system is to predict a label(SARCASM or NOT_SARCASM) of input tweets. The input tweets include response tweets and its context tweets. We are given 5,000 training data sets and 1,800 test data sets. The prediction performance will be evaluated using F1 score and compared to the baseline score. Input : Pairs of the tweets(response and context tweets) Output : Predicted labels of the response tweets(SARCASM or NOT_SARCASM) Process : Pre-trained BERT model + single layer classifier 1.2 Code outline The code of the twitter sarcasm detection system mainly consists of the components below. Detailed instructions of how to implement those components are explained in the next section. 1 No Component Function outline 1 Library installation Install the Hugging Face Library for BERT model 2 Context selection Derive the latest tweet from context tweets 3 Removing @USER Remove unnecessary words from the input tweets 4 Tokenization Convert the input text to the tokenized IDs 5 Formatting Reshape the input IDs size with fixed length 6 Data split Split the data into training and validation data set 7 Model training Build and train the BERT model with recommended parameters 8 Label prediction Predict the labels using test set data CS410 Text Information Systems (Fall 2020) 2. Implementation 2.1 Library Installation BERT pre-trained model and its library can be used by just installing the library. https://huggingface.co/transformers/ 2.2 Context selection As you can see that some contexts have several context tweets(e.g. 20), however the majority have only two to five context tweets. Although we can see that at least two tweets are included in the context information, I decided to use only one latest context tweet as a first step. In the actual code below, the derived latest context tweet is stored as a new column(pre_comment). 2 CS410 Text Information Systems (Fall 2020) 2.3 Removing @USER As you can see, the original response and context include unnecessary words like ""@USER"". The unnecessary word(@USER) is removed by this function. 2.4 Tokenization In order for the BERT model to handle the input tweets, the original tweets sentences will be converted to tokenized words and its IDs. 2.5 Formatting In order to define the BERT model's input dimensions, we need to identify the maximum length of the input tweets(response + context). Thanks to the library, what we need to do to create the fixed size input data is to set the maximum input length when we encode the data. Even if the actual input data length is shorter than the maximum input length, remaining parts will be automatically filled with zero. 3 CS410 Text Information Systems (Fall 2020) 2.6 Data split 5,000 Data sets are divided into 90% of training sets and 10% of validation sets randomly(although it is random selection, it is reproducible as the seed value is specified). 4 CS410 Text Information Systems (Fall 2020) 2.7 Model training 2.7.1 Model definition The model here consists of mainly two parts. One is the pre-trained BERT model and the other is single layer linear classification for this tweet classification application. In order to build the model, we only need to import ""BertForSequenceClassification"" from transformers like below, and specify some parameters. In our case, num_lebels is 2 as we want to classify the tweets into SARCASM or not. 5 CS410 Text Information Systems (Fall 2020) 2.7.2 Trainable parameters setting Finally, I decided not to change the trainable parameters of the BERT model as the performance with fixing the BERT model was not good enough. However, those parameters can be changed by enabling the cell below if needed for other purposes. 2.7.3 Model training with recommended parameters The batch size, the learning rate, the epsilon, and the number of epochs were determined based on the recommendation of the author of the BERT papar. Although I have not tried to change the parameters, It worked well at the first trial. 6 CS410 Text Information Systems (Fall 2020) Although I only tried to use the pre-trained BERT model + single linear classifier with recommended parameters, the training time was faster than expected and also the performance was good enough to beat the baseline(See 2.9). 7 CS410 Text Information Systems (Fall 2020) 2.8 Label prediction on the test data By applying the same preprocessing as for the training/validation data set, the test data can also be inputted to the trained model. And finally, we can get the predicted labels by applying the softmax function to predicted values. 2.9 Competition result The F1 score (0.737) beat the baseline score(0.723) as below. 2.10 Reproducing the same prediction After re-running all the code, a reproducibility test will be done automatically as below by downloading the actual submitted prediction file and comparing them to the prediction labels generated in your environment. 8 CS410 Text Information Systems (Fall 2020) 2.11 Reference The project code was inspired by a UIUC's tech review and a public website below. * https://github.com/zen030/tech_review/blob/master/techreview.pdf * https://mccormickml.com/2019/07/22/BERT-fine-tuning/ 3. Usage 3.1 Open ""SarcasmClassification.ipynb"" 9 CS410 Text Information Systems (Fall 2020) 3.2 Follow the setup instruction 1.1 and 1.2 below. 3.3 Run the code After setting up the google colab GPU setting above, you can run all the code by clicking the ""Runtime -> Run all"" below, or simply run all the cells one by one. 10 CS410 Text Information Systems (Fall 2020) 4. Contribution As my team member is only me, everything is done by myself. 11"
https://github.com/RyoTakaki/CS410_TIS_CourseProject	Project Proposal.pdf	CS410 Text Information Systems (Fall 2020) Project Proposal and Team Formation Submission for Grading Ryo Takaki In your project proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Ryo Takaki (Captain) NetID: rtakaki2 2. Which competition do you plan to join? Text classification competition 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods No. Methods Experience 1 CNN Through Udacity's Self-Driving Car Nanodegree and Computer Vision Nanodegree course, I have learned the basics of CNN, RNN and how to use PyTorch. Computer Vision Nanodegree course syllabus Automatic Image Captioning: Combine CNN and RNN knowledge to build a deep learning model that produces captions given an input image. Image captioning requires that you create a complex deep learning model with two components: a CNN that transforms an input image into a set of features, and an RNN that turns those features into rich, descriptive language. In this project, you will implement these cutting-edge deep learning architectures. 2 RNN 3 LSTM 4 PyTorch 5 BERT No actual experience with this method, but I would like to try this first just because the BERT is well-known. If the BERT is not good enough, then I will try other methods. 4. Which programming language do you plan to use? Python
https://github.com/RyoTakaki/CS410_TIS_CourseProject	README.md	CourseProject Classification Competition Twitter Sarcasm Detection Project proposal Project proposal Project progress report Project progress report Project final report Project final report Code Prediction results Answer.txt Tutorial movie UIUC media space
https://github.com/hc2111/CourseProject	progress report.pdf	Progress Report: 1. Progress made thus far a. Implementation of classifier and training on google cloud. 2. Remaining tasks, a. Improve model b. Complete Submission Documentation c. Complete Project Demo Vid 3. Any challenges/issues being faced. a. N/A
https://github.com/hc2111/CourseProject	Project Proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Harry Chen => NetID: hchen223 => Captain 2. Which competition do you plan to join? a. Text Classification 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods a. Yes, I am prepared to learn state of the art neural network classifiers. Some classifiers and frameworks I have heard of include SVM, perceptron network, Logistic regression classifiers, Deep neutral neural network classifier. I've has experiences with svm's in the past and look forward to learning more about other classifiers. 4. Which programming language do you plan to use? a. I plan to use python
https://github.com/hc2111/CourseProject	README.md	CourseProject Explain your model For the competition I used the BERT(Bidirectional Encoder Representations from Transformers) model. This model is a cutting edge model for classification. BERT's main innovation is applying the bidirectional training of Transformer, a popular attention model, to language modeling. How I performed the training: Since BERT training on CPU was incredably slow, I utilized a google cloud VM to train the model on the gpu, this enabled much faster training speeds and more experimentation with parameters and tuning. Experiments with other methods: Prior to using BERT, I tried to create my own model and tunings, however they failed to come close to the baseline, so I expanded my options and opted to utilize BERT. How to run: Download the Data folder from this link: https://drive.google.com/file/d/1OqLtj9BTnob45huOsFN_fMN23hL_WrGi/view?usp=sharing and the jupyter notebook, and then run all the cells. demo video: https://drive.google.com/file/d/1LIkJRzyLRYKK2roMzF5CP208Hlj24ehw/view?usp=sharing
https://github.com/thecheebo/NLP-Classification-Competition-w-BERT	CS410 Classification Competition Final Report.pdf	"Jie Zheng - jzheng5 Yuanming Mao - ym18 Albert Yeh - ayeh2 CS410 - Classification Competition Final Report Summary Our team was formed over slack when we were looking for fellow students to work with in the same metropolitan area. We had wanted to have the option to meet up in person, as we are all from southern California. Unfortunately, due to COVID 19, all of our meetings ended up just being virtual meetups. When one of us stumbled across the suggested tech review suggestions, BERT/ALBERT/Transformers caught our attention and appeared to be a perfect solution for the classification competition. Because the competition had suggested ""You will need to research by yourselves some cutting-edge models that are more recent than those introduced in the lectures"" and ""achieve the state-of-the-art performance"", we quickly realized that BERT and its many variants are going to be the perfect solution for the classification task at hand. Initial Research Each of our members spent many hours reading and researching much of the prerequisite knowledge required before understanding how to use BERT or how BERT worked. Starting from text preprocessing, tokenization, to word2vec/Glove/word embeddings, then understanding the use of artificial neural networks to recurrent neural networks, and LSTMs. After the initial research, we were finally able to understand the landmark paper, ""Attention is all you need.""1 Transformers, a key part of all the variations of BERT and transformer-like models, required many hours of research. Additionally, we read ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding""2. More importantly, our group was able to find multiple 1 https://arxiv.org/abs/1706.03762 2 https://arxiv.org/abs/1810.04805 papers on similar twitter classification problems which provided a source of inspiration on which models to use and how to preprocess the data.3 Trial and Error Our team attempted a conquer and divide method for solving the classification competition. However, we were in constant communication with each other and ultimately one of the team members was able to achieve a score beating the baseline. When the team member who came up with the solution that beat the baseline, we decided to regroup and focus on the best solution. We attempted to both learn from and improve on the best solution to increase the F1 score. Ultimately, the best score was about 2 points higher than the baseline of .723. Much of the learning happened throughout the trial and errors of each student. One of the approaches ended up with a F1 score of ~.68 and while ultimately scrapped, proved a useful learning tool. This method started using the pre-processing library ""ekphrasis"" found at https://github.com/cbaziotis/ekphrasis. This library offered the following: * Social Tokenizer A text tokenizer geared towards social networks (Facebook, Twitter...), which understands complex emoticons, emojis and other unstructured expressions like dates, times and more. * Word Segmentation. You can split a long string to its constituent words. Suitable for hashtag segmentation. * Spell Correction. You can replace a misspelled word, with the most probable candidate word. The pre-processing tools were very useful and was the biggest increase in F1 score from an initial score of around F1 50, to the mid 60's. In this approach, we learned about overfitting as using higher epochs, was not generally successful, and while it increased model training times, it did not do much for the score, or often ended up lowering it. We found this to be likely an overfitting of the model. We also considered the sample size of 5000 training inputs a potential factor as well. This approach also tried BERT's cased and uncased models but we did not find a significant difference in 3 https://www.aclweb.org/anthology/2020.figlang-1.13.pdf and https://arxiv.org/pdf/2005.05814.pdf the F1 score. Additionally, this approach found that the stopwords library from nltk.corpus had a marginal effect on the F1 score. A second approach that the team took was using the RoBERTa model for solving the classification model but ultimately it also came up short with an F1 score of ~69. This approach attempted to follow the guide at https://rsilveira79.github.io/fermenting_gradients/machine_learning/nlp/pytorch/text_clas sification_roberta/. Our attempts to remove stem words, numbers, URLs, did not have significant impacts to our score. This approach also attempted to change the test_train_split size of .7 compared to .8 but did not lead to much success. We noticed that after removing punctuation during the preprocessing phase, that the classification accuracy decreased. We hypothesize this was due to the fact that punctuation plays a critical role in understanding the nuance of the english language, such as the question mark or exclamation mark, especially when it comes to training a language model to understand something as complex as sarcasm. Last but not least, the final approach which was fully inspired by one of the many tutorials that our teams attempted to replicate, was found at https://analyticsindiamag.com/step-by-step-guide-to-implement-multi-class-classification -with-bert-tensorflow/. This approach also used the bert library optimization, tokenization as well as the library ""tweet-preprocessor"" and ""preprocessor"" which appeared to be extremely helpful. We used preprocess to clean up Emoji's, @mentions, hashtags and more. Additionally, we added a custom preprocessor, to reverse the context, concatenation, and cleaned up some spacing issues. With this set of preprocessing, ultimately our approach that solved the classification competition used mostly the default hyperparameters from the BERT model (max_epochs = 3, learning_rate of .00001). Despite the group's best effort of trying to increase our F1 score past 74, we were unable to increase it significantly past this point. We believed that there was a ceiling to training the model because of overfitting. Conclusion Overall, the classification competition proved to be a worthy assignment for the group. As suggested by the initial competition requirement to use ""cutting-edge models that are more recent than those introduced in the lectures"", we realized quickly that the lectures from ~2012 were likely to be quite dated, so we were excited to learn something new. The field of NLP has advanced leaps and bounds in just the last few years, especially with the introduction of CNN's, LSTMs, and Transformers. Despite being dated, the vast majority of the lectures were fundamental in providing the foundation for our understanding of the state-of-the art NLP models. From bag-of-words in lecture 1 to TF-IDF weighting to understanding sentiment analysis and classification, each student was well prepared to digest the papers on transformers and BERT. We understand that while there was certainly more room to improve both the parameters for this competition or potentially concatenate additional preprocessors/models to get a better score, the fundamental ideas were well learned by the group."
https://github.com/thecheebo/NLP-Classification-Competition-w-BERT	CS410 Project Proposal - SoCal Classifiers - Google Docs.pdf	CS410 Project Proposal - SoCal Classifiers 1. The names and netIDs of our team members are: Yuanming Mao - ym18@illinois.edu, Jie Zheng - jzheng5@illinois.edu, Albert Yeh- ayeh2@illinois.edu. Albert Yeh is the captain. 2. We are choosing the text classification competition for our project. 3. The team is committed to learning state of the art classification for the competition. We are researching the most recent state of the art technologies which rely on the idea of transformers which is introduced in 2017 in the paper Attention is all you need. https://arxiv.org/pdf/1706.03762.pdf . The idea of transformers supersedes the previous models of RNN, CNN, LSTM, and GRU. Then Google released the paper about BERT, https://arxiv.org/pdf/1810.04805.pdf . This was built on transformers which is a much larger improvement on current methods. No one in our group has any relevant prior experience with any of these models or other classifiers. We understand that we will also learn about text preprocessing using technologies such as word2vec and avgword2vec. 4. Because we will primarily be focusing on BERT, we will be programming in python which is the language it is written in.
https://github.com/thecheebo/NLP-Classification-Competition-w-BERT	CS410_progress_report.pdf	Yuanming Mao - ym18 Jie Zheng - jzheng5 Albert Yeh - ayeh2 CS410 - Progress Report Our team decided on the Classification competition for the class project. We have beaten the baseline with an F1 Score over 74%. Although, the team each took different approaches to solving the classification problem, each member used some variation of BERT as we stated in our project proposal. The BERT model is significantly more effective in classification problems, using transformers over previous CNN/LTSM type models. Fortunately, there were a lot of resources online, and each of us were able to learn the ins and outs of BERT and transformers, as well as follow examples to solve this classification problem. The entire group had zero experience in sentiment analysis and text classification going into this project so we were able to get an idea of how to start this competition by reading a few papers that were specifically about Twitter Sarcasm Sentiment Classification. 12 Each of these papers were examples that were working with the same exact dataset, so there was a lot of useful discussions on how to pre-process the data, and the test results of which transformer models produced the best F1 scores for classification. After reading the papers, we were able to create a path forward for our team to attempt to work together in solving the classification competition. After our second meeting we were able to get to 68% for the F1 score, but not enough to pass the competition. However, after our most recent meeting, we were able to solve and pass the baseline. We tried different variations of BERT and Roberta, and ended up just using BERT large uncased. We also tried many different pre-processors both individually and using different libraries (such as ekphrasis), but ultimately settled on tweet-preprocess library. We found most of our inspiration from a multi-classification guide3. We played around with the parameters and were able to finally pass the baseline after many different various attempts. We have not only learned how BERT works, but all of us have learned the meaning of parameters and how to solve a problem like text classification using state of the art algorithms. Lastly, the team will attempt to increase our F1 score by continuing to tweak the parameters, but the majority of the work has been done. There are a few things we can test out, such as single sentence (response w/ context), or combine with LSTM, or other optimizers. 1 https://www.aclweb.org/anthology/P11-2102.pdf 2 https://arxiv.org/pdf/1911.10401.pdf 3 https://analyticsindiamag.com/step-by-step-guide-to-implement-multi-class-classification-with-bert-tensorflow
https://github.com/thecheebo/NLP-Classification-Competition-w-BERT	README.md	CS410 Classification Competition For our reviewers: Please see the colab linked here and if you would like to, run each line step by step to get the same txt file, be aware that training will take hours. https://colab.research.google.com/drive/1PCRQ3qv32JF6tPBE7r5pWswcMIDlwIRe#scrollTo=DxW6mBl5A-1X Otherwise, please read our detailed report to see how our group came about to solving the competition. Here is the link and screenshot of us beating the baseline. http://livelab.centralus.cloudapp.azure.com/project/leaderboard/303268761 (You will have to log in). The image is found in the repository: livelab.JPG If you have any further questions after reading our report, please contact any of the group members. Thank you for your time and consideration.
https://github.com/lipingxie/CourseProject	CS410 Project Code Demo Video and Contact Info.pdf	CS410 Project Code Demo Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Demo Link: CS410 Course Project Demo - YouTube We recorded our demo and uploaded the video onto YouTube, please click on the above link to watch the video. If you experience any issues running our code or you have any questions regarding to our project details, please send us an email to organize a Zoom Meeting. Thanks. Contact Emails: Liping Xie: lipingx2@illinois.edu Zijing Chen: zijingc3@illinois.edu Zuliang Weng: zwe@illinois.edu
https://github.com/lipingxie/CourseProject	CS410 Project Detail.pdf	"1 CS410 Project Detail 1. Team Information Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) 2. Project Overview 2.1 Project Topic Text Classification Competition: Twitter Sarcasm Detection. The Classification task is to classify the list of Tweets into two categories: ""SARCASM"" or ""NOT_SARCASM"". There are 2 datasets: Training set which with 5000 Tweets and Test set which with 1800 Tweets. We researched and tried many cutting-edge models that are suitable for classifying ""SARCASM"" or ""NOT_SARCASM"" in this project. There are two files provided: train.jsonl and test.jsonl. The model is trained with the data provided in train.jsonl , and test.jsonl contains the tweets that we need to classify with the trained model. The classification result is reported with the provided id. All the results are stored in the answer.txt file. 2.2 Overview of the Function of the Code For this classification task, we used PyTorch as the Deep learning framework, Python(version 3.6.9) as the Programming Language and Pre-trained BERT as the State-of-the-art neural network classifier. The main function of the code is to classify the list of Tweets into two categories: ""SARCASM"" or ""NOT_SARCASM"". To be more specific, we applied the pre-trained Twitter-specific BERT model, roBERTa-base model on the training data to fine-tune the model and then predicted on the testing data to generate final results. This code can only be used to analyze or classify Tweets data due to the Twitter-specific BERT model we used. 2 Our code is in file ""Final BERT Twitter Sarcasm Classification.ipynb"", here is the detail mappings of the code section and the content: Code Section No. Content 1 How to use Google Colab for training the model and perform the test under python 3.6.9 environment and install the Hugging Face Library 2 How to load data from the provided jsonl files and parse the data 3 How to use the pre-trained BERT model. How to formatting the data, tokenize Dataset and split the training data into training set and validation set for the use of BERT 4 How to train and fine-tune the model with different batch size, learning rate and epochs, and how to evaluate the result 5 How to use the trained model to predict the tweets provided in the test.jsonl. How to save the prediction result. 3. Software Implementation Details 3.1 Classifier Selection and Introduction We have investigated different types of Classifiers which include Word2Vec, FastText and BERT. We found BERT (Bidirectional Encoder Representations from Transformers) is the State-Of-The-Art Neural Network Classifier. According to Wikipeda, BERT is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT is a model that broke several records for how well models can handle language-based tasks(Jay Alammar, 2018). This model would look like this: (Above information and image from:http://jalammar.github.io/illustrated-bert/) To train such a model, we mainly have to train the classifier, with minimal changes happening to the BERT model during the training phase. This training process is called Fine- Tuning (Jay Alammar, 2018). BERT makes use of the encoder mechanism of Transformer, 3 an attention mechanism that learns contextual relations between words (or sub-words) in a text. At the moment, the Transformers package from Hugging Face PyTorch library is regarded as the most widely accepted and powerful pytorch interface for working with BERT. (Above information and image from:http://jalammar.github.io/illustrated-bert/) BERT's clever language modeling task masks 15% of words in the input and asks the model to predict the missing word (Jay Alammar, 2018). (Above information and image from:http://jalammar.github.io/illustrated-bert/) 3.1.1 Model Selection 4 Training a BERT model takes huge time, we can use the pre-trained model, and fine-tune it with our training data. With the consideration of classifying Twitter text, we used the pre- trained Twitter-specific BERT model - roBERTa-base model. The reason why we choose Twitter-specific BERT model is that the characteristics of Tweets are significantly different from traditional text such as research papers or articles. Tweets are generally short, and include frequent use of informal grammar as well as irregular vocabulary such as abbreviations. Thus, it would be inappropriate to apply language models such as bert-base- uncased, bert-large-uncased that are pre-trained on traditional text with formal grammar and regular vocabulary to analyze Tweets data (Nguyen et al., n.d.). Our testing result confirmed that the roBERTa-base model outperforms bert-base-uncased and bert-large-uncased in this classification task. The architecture for BERTweet is very similar to BERTbase, which is trained with a masked language modeling objective (Devlin, 2019). BERTweet pre-training procedure is based on RoBERTa which optimizes the BERT pre-training approach for more robust performance (Liu, 2019). For pre-training data, BERTweet uses an 80GB pre-training dataset of uncompressed texts, which contain 850M Tweets (Nguyen et al., n.d.). This is very similar to the training and testing dataset since our data is also Tweets data. Under https://huggingface.co/models, we can find there are different sub models, we compared the performance between twitter-roberta-base and twitter-roberta-base-irony, we found twitter-roberta-base performs a little bit better on the prediction task (we have provided the detail test result in section 3.7.2 ), we decided to select twitter-roberta-base in our final version code. 3.2 Environment setup 3.2.1 Setup Colab Since we need to train a large neural network, Google Colab offers free GPUs and TPUs, we used Colab to train our model to shorten our training time. Steps for setting up Colab: 1. Add Google Colab as an app to Google Drive. 2. Set the runtime type to GPU. To be more specific, we went to ""Edit"" -> ""'Notebook Settings"" -> ""Hardware accelerator"" -> ""GPU"". If you want to check if the GPU is detected, execute code in section 1.1 to confirm 3. We need to identify and specify the GPU as the device in order for torch to use the GPU. As a user, you need to install the package torch If you have not installed it before. After importing the package torch, you can run the second section of code in 1.1 to specify the GPU as the device. 3.2.2 Install the Hugging Face Library We have selected the Transformers package from Hugging Face PyTorch library, so we need to install the transformers package from Hugging Face which gives us a pytorch interface for working with BERT. You can run the code in section 1.2 to install the Hugging Face Library. 5 3.3 Data Analysis and Preparation There are 2 json format datasets provided: * train.jsonl: 5000 Tweets * test.jsonl: 1800 Tweets In each tweet in test.jsonl, it contains ""id"", ""response"" and ""context"": * Id: String identifier for sample. This id will be required when making submissions. * Response: the Tweet to be classified * Context: the conversation context of the response. The context is an ordered list of dialogue In each tweet in train.jsonl, it contains ""label"", ""response"" and ""context"": * Label: SARCASM or NOT_SARCASM * Response: the Tweet to be classified * Context: the conversation context of the response. The context is an ordered list of dialogue The length of all responses are less than 150 words, but for the context, some tweets with very long context and exceed the maximum supported input size (512) of BERT, that means we cannot use the full content of context for classification. So, we have tried two strategies: 1. Classify based on Responses only 2. Classify based on Responses and part of Context For strategy 2, according to our test, we only can include two dialogues (we have tried the last two items in each context), if we selected 3 dialogues, it reports ""CUDA out of memory"" error. So, we just use the last two items in each context as part of the content for classification, then compare the f1 result (We are intended to compare the result of two strategies, and we will detail how to get the f1 value in later sections). From above, we can see that adding the content for classification doesn't improve the prediction. So, we have decided not to include the ""content"" in each tweet for our classification task. 6 Please refer to code section 2.1 for how to load the data into Colab for test, and section 2.2 for how to read and parse the data for later use. 3.4 Tokenization 3.4.1 BERT Tokenizer There are many different pre-trained BERT models available. Each model comes with its own tokenizer. We need to make sure we use the correct tokenizer as we experiment with different models. We defined which pre-trained BERT model we will use in this step and if we need to change the model, we just need to simply update the model name. Please refer to code section 3.1 for more details. 3.4.2 Required Formatting BERT requires tokens to fit certain format: 1. Add special tokens to the start and end of each sentence. o For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence. This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output. o At the end of every sentence, we need to append the special [SEP] token. 2. Pad & truncate all sentences to a single constant length. 3. Explicitly differentiate real tokens from padding tokens with the ""attention mask"". BERT has two constraints: 1. All sentences must be padded or truncated to a single, fixed length. o For any sentences that are less than the fixed length, we need to PAD with the same special token. o For any sentences that are more than the fixed length, we need to truncate them to the fixed length, otherwise the system will report the errors. 2. The maximum sentence length is 512 tokens. 3.4.3 Tokenize Dataset For saving the memory in the model training, we set the max_length of the input token based on the max length of all the input sentences. We used encode_plus methods for token padding and attention masking, it automatically adds ""special tokens"" which are special IDs the model uses. We need to convert the lists into tensors in order to use the Pytorch properly. Please refer to the code section 3.3 for the detailed implementation. 3.4.4 Training & Validation Split and Batch Size Before training our data using BERT, data must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. 7 For the provided training data, we split them into training set and validation set, the proportion of the training set will impact the prediction accuracy of the model. We have tried the training:validation ratio as 9:1, 99:1 and 8:2. We observed when the training:validation ratio is 9:1 provides the best prediction result. Please refer to section 3.7.2 for the detail test result. We used DataLoader to help us save memory, and we define the batch size for this step. Please refer to the code section 3.4 for the detailed implementation of data split, the usage of dataloader and the setting of batch size. 3.5 Fine-tune BERT Models First we load the defined pre-trained BERT model, examine the result, then we set the key values for training the model: * Batch size (set when creating our DataLoaders) * Learning rate * Epochs Then we train the model with the training loop and provide the training result statistic (accuracy and validation lost). We use the output of the fine-turn model to predict the tweets in the test.jsonl. After that we update the parameters and train the model again. When we have a satisfied result for the pre-trained model under test, we change to other models with the same parameters. Here is the a list that we have experimented: * Bert Models: o Bert-base-uncased o bert-large-uncased * Twitter-roberta models: o cardiffnlp/twitter-roberta-base o cardiffnlp/twitter-roberta-base-irony o cardiffnlp/twitter-roberta-base-offensive Based on our test result, twitter-roberta models outperforms Bert Models on this task, we have decided not to use this model in early stage, so our testing is mainly focused on the twitter-roberta related models. Comparing the performance of three twitter-roberta models, cardiffnlp/twitter-roberta-base provides the best performance with the following settings: * Batch size: 32/64 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4/6 3.5.1 Download the Model Please refer to the code section 4.1. 3.5.2 Optimizer & Learning Rate Scheduler We use AdamW optimizer and define the epochs in the scheduler. 8 Impact analysis on the parameters: * Batch size: The batch size is the number of words to be used for calculation and updating the weight once, it impacts the training time, memory usage and model's prediction accuracy. The larger the batch size is, the more memory it will consume. When the batch size is too small, the result may not converge, and when the batch size is too big, it will cause over fitting or out of memory. Based on our test, the batch_size = 32/64 provides the best performance. * Learning rate: We have tried three learning rates: 2e-5, 1e-4, 1e-6. We noticed that when the learning rate is large, it cannot converge, and when the learning rate is small, it overfits. learning rate = 2e-5 provides the best prediction result. * Epochs: Epochs value need to be set correctly, if the value is too small, the gradient descent will not converge, if the epochs value is bigger than required, we will overfit the model and decrease the accuracy of the prediction result. We use the accuracy and validation lost in each epoch's validation result. When we notice when the epochs number increases, the accuracy increases, we increase the epoch value to test again. If the accuracy decreases or there is not much change, we decrease the epochs to the point when accuracy is not increased. Based on our test, the epochs = 4/6 provides the best performance. Please refer to the code section 4.2 for the implementation. Please refer to section 3.7.3 for the detail test result. 3.5.3 Training Loop We define and create the training loop based on the contribution of Stas Bekman. The training loop has a training phase and a validation phase. It also detects over-fitting by using validation loss. After defining the training loop, we start to fine-tune the model. Please refer to the code section 4.3. 3.6 Predict on the test data 3.6.1 Data Preparation Prepare the test data just as how we prepare the training data. Please refer to the code section 5.1. 3.6.2 Predict on the test set Generate the final prediction based on the score, and download the result as answer.txt. Submit answer.txt to livelab for the result. When the f1 result is not good, we change the parameters or pre-trained model as mentioned in 3.5. Please refer to the code section 5.2. 9 3.7 Results 3.7.1 Final Result Chosen Model: cardiffnlp/twitter-roberta-base Parameters setting: * Batch size: 32 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4 Model Training Result: The Best Prediction Result with the above parameters: * Precision: 0.7577319587628866 * Recall: 0.8166666666666667 * F1: 0.7860962566844919 3.7.2 Result Discussion We observed that when using the same parameters to fine-tune the pre-trained model, the prediction result is not the same for each time. We have confirmed that even we set seed for the random sampling and using SequentialSampler in dataloader, we cannot make the prediction consistent. Based on the information provided in Paperswithcode, the issue is caused by the Attention Dropout and Dropout: * Attention Dropout: It is a type of dropout used in attention-based architectures, where elements are randomly dropped out of the softmax in the attention equation. * Dropout: It is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability (a common value is ). At test time, all units are present, but with weights scaled by (i.e. becomes ). The idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks. According to Huggingface BertConfig, the default dropout rate is set to 0.1: * hidden_dropout_prob (float, optional, defaults to 0.1) - The dropout probability for all fully connected layers in the embeddings, encoder, and pooler. * attention_probs_dropout_prob (float, optional, defaults to 0.1) - The dropout ratio for the attention probabilities. 10 Considering the above factors and the test result, we observe the following parameters' settings ensures we can get the F1 > 0.75 which is well above the baseline: * Batch size: 32/64 in training, and 32 in prediction * Learning rate: 2e-5 * Epochs: 4/6 Here is the test result for these settings: epoch Training Split Precision Recall F1 Comments 4 0.9-0.1 0.71468927 0.843333333 0.773700306 batch_size=64 Sentences=Response 4 0.9-0.1 0.69802867 0.865555556 0.77281746 batch_size=80 Sentences=Response 6 0.9-0.1 0.72380952 0.844444444 0.77948718 batch_size=64 Sentences=Response 8 0.9-0.1 0.73635427 0.794444444 0.764297167 batch_size=64 Sentences=Response 8 0.8-0.2 0.73236515 0.784444444 0.75751073 batch_size=64 Sentences=Response 4 0.8-0.2 0.70119157 0.85 0.768458061 batch_size=64 Sentences=Response 6 0.9-0.1 0.74476987 0.79111 0.767241379 batch_size=100 Sentences=Response 8 0.9-0.1 0.747288 0.76555 0.75631174 batch_size=100 Sentences=Response 5 0.9-0.1 0.72727272 0.80888 0.7659 batch_size=100 Sentences=Response 6 0.9-0.1 0.7391304 0.79333 0.76527 batch_size=80 Sentences=Response 6 0.9-0.1 0.74545454 0.77444 0.759673024 batch_size=64 Sentences=R learning rate=5e-5 6 0.9-0.1 0.73473 0.77555 0.75459 batch_size=64 Sentences=R learning rate=1e-5 6 0.9-0.1 0.7433264 0.80444 0.77267 learning rate=2e-5, batch_size=32,200 Sentences=Response 6 0.9-0.1 0.73313783 0.833333333 0.780031201 learning rate=2e-5, batch_size=64,32 Sentences=Response 4 0.9-0.1 0.74543611 0.816666667 0.77942736 learning rate=2e-5, batch_size=32,32 Sentences=Response 6 0.9-0.1 0.73615307 0.812222222 0.77231907 learning rate=2e-5, batch_size=32,32 Sentences=Response 4 0.9-0.1 0.75773196 0.816666667 0.786096257 learning rate=2e-5, batch_size=32,32 Sentences=Response 3.7.3 Testing Records Below is the table that listed all the models we've tried with some parameters and statistics. The thirteenth line achieved the best result, which ranked the second in the leadership board. epoch Training Split Precision Recall F1 Comments 0.591836735 0.805555556 0.682352941 no fine-tuning 4 0.9-0.1 0.74522293 0.78 0.762214984 11 4 0.99- 0.01 predict all tweets as sarcasm. results not usable 4 0.95- 0.05 0.722109534 0.791111111 0.755037116 4 0.9-0.1 0.74522293 0.78 0.762214984 4 0.9-0.1 0.691049086 0.797777778 0.740587932 Sentences=Response + Context[- 2:] 4 0.9-0.1 0.648626817 0.892222222 0.751169317 Sentences=Response + Context[- 2:] 4 0.9-0.1 N/A N/A N/A Sentences=Response + Context[- 3:],CUDA out of memory. 4 0.9-0.1 N/A N/A N/A batch_size=64 Sentences=Response + Context[- 2:], CUDA out of memory 4 0.9-0.1 0.714689266 0.843333333 0.773700306 batch_size=64 Sentences=Response 4 0.9-0.1 N/A N/A N/A batch_size=100 Sentences=Response,CUDA out of memory. 4 0.9-0.1 0.698028674 0.865555556 0.77281746 batch_size=80 Sentences=Response 6 0.9-0.1 0.723809524 0.844444444 0.77948718 batch_size=64 Sentences=Response 8 0.9-0.1 0.736354274 0.794444444 0.764297167 batch_size=64 Sentences=Response 8 0.8-0.2 0.732365145 0.784444444 0.75751073 batch_size=64 Sentences=Response 4 0.8-0.2 0.701191567 0.85 0.768458061 batch_size=64 Sentences=Response 6 0.9-0.1 0.768564356 0.69 0.727166276 batch_size=64 Sentences=Response 6 0.9-0.1 0.732291667 0.78111 0.7559 batch_size=64 Sentences=Response 2 0.9-0.1 0.5 1 0.6667 batch_size=64 Sentences=Response 6 0.9-0.1 0.74476987 0.79111 0.767241379 batch_size=100 Sentences=Response 8 0.9-0.1 0.747288 0.76555 0.75631174 batch_size=100 Sentences=Response 5 0.9-0.1 0.72727272 0.80888 0.7659 batch_size=100 Sentences=Response 6 0.9-0.1 0.7391304 0.79333 0.76527 batch_size=80 Sentences=Response 6 0.9-0.1 0.74545454 0.77444 0.759673024 batch_size=64 Sentences=R learning rate=5e-5 6 0.9-0.1 0.73473 0.77555 0.75459 batch_size=64 Sentences=R learning rate=1e-5 6 0.9-0.1 0.5 1 0.6667 learning rate = 4e-4 batch_size=64 Sentences=Response 6 0.9-0.1 0.665354331 0.563333333 0.610108303 learning rate = 1e-6 batch_size=64 Sentences=Response 6 0.9-0.1 0.73193 0.7766 0.7536 learning rate=2e-5, batch_size=64 Sentences=Response 6 0.9-0.1 0.751054852 0.791111111 0.770562771 learning rate=2e-5, batch_size=64,200 Sentences=Response 12 6 0.9-0.1 0.7433264 0.80444 0.77267 learning rate=2e-5, batch_size=32,200 Sentences=Response 6 0.9-0.1 0.73313783 0.833333333 0.780031201 learning rate=2e-5, batch_size=64,32 Sentences=Response 4 0.9-0.1 0.745436106 0.816666667 0.77942736 learning rate=2e-5, batch_size=32,32 Sentences=Response 6 0.9-0.1 0.736153072 0.812222222 0.77231907 learning rate=2e-5, batch_size=32,32 Sentences=Response 4 0.9-0.1 0.757732 0.81666667 0.7860963 learning rate=2e-5, batch_size=32,32 Sentences=Response 4. Instructions for Using the Software 4.1 Setup and Installation For this classification task, we used PyTorch as the Deep learning framework, Python as the Programming Language and pre-trained Twitter-specific BERT model, roBERTa-base model as the State-of-the-art neural network classifier. Here are the steps to execute the classifier: 1. Download these files from https://github.com/lipingxie/CourseProject : a. Final BERT Twitter Sarcasm Classification.ipynb b. test.jsonl c. train.jsonl 2. Add Google Colab as an app to Google Drive. 3. Upload ""Final BERT Twitter Sarcasm Classification.ipynb"" into Google Colab and open the file in Colab. Our code includes all the required commands to setup the environment, Colab provides the default Python(version 3.6.9)environment. 4. In Colab, in the top menu, select ""Runtime"" -> ""Change runtime type"" -> In ""Hardware accelerator"", select ""GPU"" -> click ""SAVE"" 5. You can run all the code in sequence via the top menu, select ""Runtime"" -> ""Run all"". When the code in section 2.1 is being executed, you need to upload the file manually. The upload button will be enabled once this code is being executed. Upload test.jsonl and train.jsonl at the same time. Once the files are uploaded successfully, the remaining code will continue to be executed automatically. 6. Wait for the code to be finished, it may take a long time around 15-60 minutes which depends on Colab performance on that time period. 7. The answer.txt file will be downloaded automatically (If the download action is permitted in your machine.). Sometimes the answer.txt file cannot be automatically downloaded due to your local environment issue. You can click on the folder button on the left-side bar, the file will be displayed there and you can download it manually. 8. If you would like to verify the prediction result, you need to upload the file to livelab- ClassificationCompetition project with your own account with the similar process of MP2.4. As mentioned in section 3.7.2, due to the BERT default dropout, you may not 13 get the same result as our report or on the board, but we guarantee the result can pass the baseline. 9. You can check our best prediction result of our fine-tuned model in livelab -> ClassificationCompetition project -> Leaderboard -> Record uploaded by ""Liping Xie"" 4.2 Troubleshooting If you would like to execute the code from the beginning again or load the model again, it may fail or you will have memory issues during the training stage. It seems it is caused by the Colab. The following steps can help to solve the problem: 1. Refresh the page 2. Click on the ""Additional Connection Options"" button (displayed as a small triangle icon that is next to ""RAM"" and ""Disk"" and located on top right corner. 3. Select ""manage sessions"" 4. Select the active section and click ""TERMINATE"" 5. Execute the code from the beginning again Due to the dependency of the code, it may generate unexpected results if you re-execute part of the code. Please terminate the session and run from the beginning again. 5. Team Member Contribution In this project, we all contributed a lot to each of the tasks, including writing the project proposal, researching cutting-edge pre-trained models, implementing the models on training and testing data, writing the progress report, finalizing the source code, and writing the documentation. 6. Reference * Nguyen, Dat Quoc. BERTweet: A pre-trained language model for English Tweets. Retrieved from: https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf * Jay Alammar, 2018. BERT: http://jalammar.github.io/illustrated-bert/ * Yinhan Liu, 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint, arXiv:1907.11692 * Jacob Devlin, 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, page 4186. * Paperswithcode BERT: https://paperswithcode.com/method/bert * Huggingface BertConfig: https://huggingface.co/transformers/model_doc/bert.html * Model reference: https://huggingface.co/models * Transformer coding reference: https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813 037968a9e58/examples/run_glue.py#L109 * Model training reference: https://mccormickml.com/ 14 * Pytorch coding reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner- blitz-cifar10-tutorial-py * Stas Bekman: https://github.com/stas00?tab=repositories"
https://github.com/lipingxie/CourseProject	CS410 Project Progress Report.pdf	"CS410 Progress Report Team Information Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Project Topic Competition - Text Classification Project Progress Tasks have been completed: 1. Made decisions based on the options in our proposal: * State-of-the-art neural network classifier: BERT * Deep learning frameworks: PyTorch * Programming Language: Python 2. Our testing result has passed the baseline: o precision =0.7333333333333333 o recall = 0.7211111111111111 o f1 = 0.727170868347339 3. Draft version code is ready, here is what we have done related to coding: We modified and fine-tuned BERT to train the text classifier. To be more specific, we tried some pre-trained BERT models. The reason why we chose a pre-trained BERT model is that the pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes less time to train our fine-tuned mode. First of all, we installed the transformers package from Hugging Face which gave us a pytorch interface for working with BERT. Next, in order to apply the pre-trained BERT, we used the tokenizer provided by the model, and we tried ""bert-base- uncased"" and ""bert-base-cased"". Since BERT has very specific formatting requirements, we loaded the data from the file and formatted it to match its requirements, such as added special tokens to the start and end of each sentence; truncated all sentences to the same length, etc. Then we used ""BertForSequenceClassification"" to train the model. This is the normal BERT model with an added single linear layer on top for classification that we used as a sentence classifier. Then we applied our model to generate predictions on the test set. Tasks are pending: 1. Use Google Colab to train and fine-tuned our model. Training a large neural network in Google Colab can save some training time. 2. Try some other pre-trained model such as bert-large-uncased. 3. Fine-tuning the model, try different batch size, learning rate, and number of epochs. 4. Discuss how much context data will be used in training, since if we use all the context data, it will take a long time to train. Challenges we are facing: 1. It takes too long to train the model locally since the context is very long. 2. The testing result is still not good enough, we need another way to improve the result."
https://github.com/lipingxie/CourseProject	Execution Result.docx	Execution Result: Result: Result:
https://github.com/lipingxie/CourseProject	README.md	"Team Name: Team Commonwealth Team Members: 1. Zuliang Weng / zwe 2. Zijing Chen / zijingc3 3. Liping Xie / lipingx2 (captain) Project Topic: Competition - Text Classification Since it is a Text Classification Competition Project, as the instruction provided by TA, the project details will be provided in ""CS410 Project Detail.pdf"" instead of the README.md file. Please refer to ""CS410 Project Detail.pdf"" under https://github.com/lipingxie/CourseProject . Thanks. For all other information, please refer to the following documents in https://github.com/lipingxie/CourseProject: 1.Demo and Contact Information: CS410 Project Code Demo Video and Contact Info.pdf Project Code: Final_BERT_Twitter_Sarcasm_Classification.ipynb Project related datat files: result file: answer.txt input data: train.jsonl, test.jsonl If you experience any issues running our code or you have any questions regarding to our project details, please send us an email to organize a Zoom Meeting. Thanks. Contact Emails: Liping Xie: lipingx2@illinois.edu Zijing Chen: zijingc3@illinois.edu Zuliang Weng: zwe@illinois.edu ==========Updated on 15th December ========= Some reviewers reported the code will fail in the upload file step, it seems it is related to the Broswer settings or Broswer limitations. To ensure the code can be executed correctly in Colab, please use Chrome normal window. If execute the code in Chrome incognito window, the file upload button will not be enabled and just fail the step. ""Execution Reslut"" file is attached to confirm the code can be executed correctly."
https://github.com/lipingxie/CourseProject	Team Commonwealth CS410 Course Project Proposal.docx	CS410 Course Project Proposal October 25th 2020 Team Information Team Name: Team Commonwealth Team Members: Zuliang Weng / zwe Zijing Chen / zijingc3 Liping Xie / lipingx2 (captain) Project Topic Competition - Text Classification Learning and Investigation Plan We are planning to learn the following state-of-the-art neural network classifier: BERT We do not have relevant prior project experience with this classifier. We are planning to learn the following deep learning frameworks: TensorFlow PyTorch We have basic knowledge with PyTorch, and we have used it in our self-study exercises. We will try PyTorch in our project first, if there is any issue that can only be resolved in TensorFlow, we will switch to TensorFlow. Programming Language We plan to use Python for this project.
https://github.com/rakesh-patnaik/CourseProject	demo_presentation.pdf	"Rakesh Patnaik (rakeshp2@illinois.edu) 12/13/2020 Reproducing a Paper: Latent Aspect Rating Analysis without keyword supervision Visual depiction of the task * Input * Review texts * Overall rating * Assumed aspects in the review (Location, Room, Service etc) * Output * Latent aspects (Topic model used to extract text from review corresponding to a topic) * Rating associated to each latent aspect * Weight associated to each latent aspect * Validation * Mean squared error from ground truth overall rating. Stages in the process * Pre-processing (preprocessing_Sec5_1.py) * Lowercase * Remove punctuation characters * Remove stop words * Lemmatize * Processing and Analyzing (Main.py) * Model topics based on ""Service"", ""Cleanliness"", ""Overall"", ""Value"", ""Location"", ""Rooms"", ""Sleep Quality"" * Identify words that correlate to model topics * Use regression to identify topic rating to maximize probability to ground truth latent ratings * Use regression to identify topic weights to maximize probability to ground truth overall rating * Calculate mean squared error to ground truth ratings * Output results to results/results.txt and MSE to stdout. How to run the code * git clone https://github.com/rakesh-patnaik/CourseProject.git * cd CourseProject * python3 -m venv env * source env/bin/activate * pip install --upgrade pip * python -m pip install -r requirements.txt * python -m nltk.downloader stopwords * python -m nltk.downloader punkt * python -m nltk.downloader wordnet * python preprocessing_Sec5_1.py * python Main.py Results * Results will be output to results/results.txt * Mean Squared Error will be output to stdout * (env) rakesh@Rakeshs-MacBook-Pro-4.local:~/work/uiuc-mcsds/cs410-fall2020/CourseProject$ python preprocessing_Sec5_1.py (env) rakesh@Rakeshs-MacBook-Pro-4.local:~/work/uiuc-mcsds/cs410-fall2020/CourseProject$ python Main.py Total reviews: 183 MSE: 2.99805326964421"
https://github.com/rakesh-patnaik/CourseProject	project_progress_report_w14.pdf	Project Progress Report - 11/28/2020 - week 14 1) Which tasks have been completed? * Test DataSet * Preprocessing Test DataSet - section 5.1 of paper 2) Which tasks are pending? * Aspect Identification * Aspect rating prediction * Aspect weight prediction 3) Are you facing any challenges? * Identifying visualization methods and parameters to visualize. I will be using python matplotlib but will need to identify the correct parameters to plot. * Have yet to test if the code will run on local laptop with the entire dataset
https://github.com/rakesh-patnaik/CourseProject	project_proposal.pdf	Following is the proposal to execute CS410 final project. Team Details I would be working as a individual to execute the final project NetIDs: * rakeshp2 Topic Of the Project This project would try to reproduce the following paper on the topic of Latent aspect rating analysis without aspect keyword supervision * Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Review comments by customers and users are a valuable source of feedback for businesses. Mining information and quantifying a customer review can help reduce human effort. A generic review usually has the following components: * topics or aspects such as location, service, cleanliness, specific amenities, food etc * A relative weight placed on each of the topics. Some topics might carry more weight to a certain customer and hence determines the final rating. latent aspect rating analysis ( lara ) refers to the task of inferring both opinion ratings on topical aspects ( e.g. , location , service of a hotel ) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings If a system is fed the aspects to look for in a review it would need human intervention and hence defeating the purpose of large scale data mining on review texts. A generative model that identifies the topics and weights associated with each of the topics would make the system function without supervision and hence scale up. Hence this topic of LARA without aspect keyword supervision is valuable and interesting. Implementation technology Python3 Dataset: http://times.cs.uiuc.edu/~wang296/Data/ Project Tasks * Design * Implementation * Testing
https://github.com/rakesh-patnaik/CourseProject	README.md	Latent Aspect Rating Analysis without aspect keyword supervision Implementation for Paper https://www.cs.virginia.edu/~hw5x/paper/p618.pdf Project Topic This project would try to reproduce the following paper on the topic of Latent aspect rating analysis without aspect keyword supervision Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Abstract from the paper Mining detailed opinions buried in the vast amount of review text data is an important, yet quite challenging task with widespread applications in multiple domains. Latent Aspect Rating Analysis (LARA) refers to the task of inferring both opinion ratings on topical aspects (e.g., location, service of a hotel) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings. A major limitation of previous work on LARA is the assumption of pre-specified aspects by keywords. However, the aspect information is not always available, and it may be difficult to pre-define appropriate aspects without a good knowledge about what aspects are actually commented on in the reviews. In this paper, we propose a unified generative model for LARA, which does not need pre-specified aspect keywords and simultaneously mines 1) latent topical aspects, 2) rat- ings on each identified aspect, and 3) weights placed on dif- ferent aspects by a reviewer. Experiment results on two dif- ferent review data sets demonstrate that the proposed model can effectively perform the Latent Aspect Rating Analysis task without the supervision of aspect keywords. Because of its generality, the proposed model can be applied to ex- plore all kinds of opinionated text data containing overall sentiment judgments and support a wide range of interest- ing application tasks, such as aspect-based opinion summa- rization, personalized entity ranking and recommendation, and reviewer behavior analysis Derived Abstract Review comments by customers and users are a valuable source of feedback for businesses. Mining information and quantifying a customer review can help reduce human effort. A generic review usually has the following components: topics or aspects such as location, service, cleanliness, specific amenities, food etc A relative weight placed on each of the topics. Some topics might carry more weight to a certain customer and hence determines the final rating. latent aspect rating analysis ( lara ) refers to the task of inferring both opinion ratings on topical aspects ( e.g. , location , service of a hotel ) and the relative weights reviewers have placed on each aspect based on review content and the associated overall ratings If a system is fed the aspects to look for in a review it would need human intervention and hence defeating the purpose of large scale data mining on review texts. A generative model that identifies the topics and weights associated with each of the topics would make the system function without supervision and hence scale up. Hence this topic of LARA without aspect keyword supervision is valuable and interesting. Run the project ```shell script Running the project git clone https://github.com/rakesh-patnaik/CourseProject.git cd CourseProject python3 -m venv env source env/bin/activate pip install --upgrade pip python -m pip install -r requirements.txt python -m nltk.downloader stopwords python -m nltk.downloader punkt python -m nltk.downloader wordnet python preprocessing_Sec5_1.py python Main.py ``` Demo https://github.com/rakesh-patnaik/CourseProject/blob/main/demo_presentation.pdf Implementation technology Python3 Dataset subset of TripAdvisor data from http://times.cs.uiuc.edu/~wang296/Data/
https://github.com/akhilbhamidipati5/CourseProject	CS410Project_ProgressReport_copy.pdf	Progress Report Our group has made some reasonable progress on our project, which is reproducing the paper outlined in Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback.  Right now, we have our response data: our time series data which we arbitrarily chose to be the closing price of Facebook, Apple, Microsoft, Tesla, and American Airlines stock from 1/2/2018 to 10/30/2020. We have also decided what we will be using as our text data: the most popular tweets surrounding a ticker symbol tag on twitter. In our case, $FB, $AAPL, $MSFT, $TSLA, and $AAL. To get our document collections, we have begun using Tweepy, a twitter API, to create a collection of top tweets containing each ticker symbol along with the day they were tweeted. We have also written out the pseudocode for the Iterative Topic Modeling with Time Series Feedback algorithm so that we can begin with our model soon. The major tasks that we still have to carry out are finalizing the document collection, writing the code for our topic modeling of the tweets, deciding what our causality measure will be and which testing strategy (Granger or Pearson) we will use to evaluate significance, deciding how strong of an effect we want our prior to have, and writing code to perform sentiment analysis on tweets and words. One challenge we are working through is people who tag several ticker symbols in their tweet to try to make it more popular. These tweets often are not focused on the stock we are trying to observe and will create unnecessary noise. Another challenge is getting a complete understanding of how to use the prior in the iterative topic modeling and writing out the code for this process. A third problem we are facing comes in the presence of pictures. Oftentimes people will tweet a picture of a stock chart or the picture will contain essential information without which the tweets itself may seem out of context or to be missing information. Therefore, we are trying to think about what the best way to handle pictures will be. Right now we are thinking about filtering images out and ignoring all non-text data, but should we find that these tweets contain crucial text data, we may try to include them somehow.
https://github.com/akhilbhamidipati5/CourseProject	CS410_Project_Proposal.pdf	"Paper we will be reproducing: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Project Proposal Our team is StonksOnlyGoUp and Akhil Bhamidipati (akhilsb2) will be the team captain along with team members Angeeras Ramanath (ar13) and Josh Perakis (perakis2). We will be reproducing an algorithm outlined in the paper Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback  to observe the impact of news and tweets on the stock prices of Facebook, Apple, Microsoft, Tesla, and American Airlines. These specific stocks have been chosen for certain reasons: Apple and American Airlines stock prices were used as response data in the original paper and using this same data again after several years will likely yield quite interesting results; Facebook, Tesla and Microsoft have been extremely popular stocks over the past few years and analyzing the prices of those stocks will best serve the needs of our ideal users. The goal of our project will be to implement the ITMTF algorithm to determine words from our sources linked causally to stock price changes--relevant positive words which are correlated with increasing stock price and relevant negative words which are correlated with decreasing stock price. The important concept is that the time-series data, stock prices in this case, has to change after a certain time delay after the relevant data has been observed. Once we have our topic mining done, we will evaluate the effectiveness of our model on our time-series stock market data by using a significance test to compare the model to the actual prices of the respective stocks during those time periods. To carry out this project, we will use Python and several of its libraries for the development of the model and then use R for parts of our statistical analysis process when needed. For our data, we will also use Tweepy (a twitter API to get our Twitter input data from select accounts), web scrape news headlines and rumors from select pages, and use Finnhub to get our time series stock price data. Once we are finished, we will demonstrate the usefulness of our model by trying to evaluate it over a future series after the algorithm is developed and using a significance test to estimate its effectiveness in that window. This test should give us a baseline on whether our model is good enough to ""put our money on"" or not. The people who will benefit most from our model will be common Robinhood investors, investment bankers, and traders who can try to capitalize on market volatility induced by news or tweets. While similar tools do already exist, there are not many which show a causal relationship between news and actual changes in price which makes our tool unique in that sense. Also, because our tool is focused on a relatively simple set of inputs and response data, it will provide starting investors with a comprehensible algorithm which they can use to judge investor sentiment and make informed trades."
https://github.com/akhilbhamidipati5/CourseProject	README.md	"Topic Modeling and Causality Evaluation An evaluation of how coverage of topics in tweets with cashtags ($TSLA, $PLTR, $NFLX) are causally linked to the price of the respective stock. Team Members: Captain -> Akhil Bhamidipati (akhilsb2), Angeeras Ramanath (ar13), Joshua Perakis (perakis2) Introduction We proposed to implement the paper titled Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback. In this paper they used AAL and AAPL stock as well as Presidential Probability odds as time series data and New York Times text data. The topic modeling is generic, however, in the paper they only implement the PLSA (Probabilistic Semantic Analysis) method. The paper uses both Pearson correlation coefficient and Granger causality to quantitatively evaluate the correlations. Our project was to implement the iterative topic modeling with time series feedback (ITMTF) algorithm to identify which words from tweets are linked causally to stock price changes. Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 ITMTF Algorithm Below is a general summarization / pseudocode of the ITMTF algorithm which helped us understand the paper: Identify our time series response data (X = x1, ..., xn) with timestamp t1, ..., tn 1a. Stock data Identify collection of documents form same time period D = {(d1,td1), ..., (dm, tdm)} 2a. Twitter tweets Use a topic modeling method to generate topics for each doc T1, .., Ttn 3a. This topic modeling method is M 3b. Going to apply M to D, Find topics with high causality For each topic apply C to find most significant causal words in the top words of the topic and get the impact of these significant words Separate positive impact terms and negative impact terms Assign prior probabilities according to significance level Use the prior to repeat until we reach good topic quality Purpose The goal of this project was to find relevant words which where causally linked with price movements so that we could use the document collection in the future to predict trends. Ultimately, what our code does is evaluate which words' coverages over time are most strongly causally linked to changed in price within a 5-day lag. Further improvements on our project will be able to more effectively find significantly causal words which are linked to movements in price and more accurately predict changes in stock prices based on the coverage of topics in tweets with the respective cashtag. Implementation We begin by using Tweepy, a twitter querying API to retreive tweets with the cashtags $TSLA, $PLTR, and $NFLX and format them into files with their date and the tweet. Parse the files from step 1 to make document collections Create corpuses to maintain a vocabulary and calculate word coverage over time in the time series Use Granger Testing to test for a causality relationship where the coverage of a word over time in the given corpus ""Granger causes"" the change in the stock's price within a lag of 5 days. Evaluate words which are significantly causal and possible implications/inferences. A Walk-Through of our Project Take a look at our demo video on Youtube: https://youtu.be/yu4mr-RQW80 In the file twitter_stock.py you will find the first steps of our project which involved retrieval of tweets. We then generated the tweets we retreived into the files tweet_data_tsla.txt, tweet_data_pltr.txt, and tweet_data_nflx.txt. At this point we were ready for the main portion of our project which can be found in doc_collection_topic_modeling.html or doc_collection_topic_modeling.ipynb. In this step, we first parsed the data from the previous files to create comprehensive document collections and then went ahead and intialized corpuses for all of these document collections while ignoring stopwords. After intializing these corpuses, we performed topic modeling calculations (which are further documented in doc_collection_topic_modeling.html) to understand the coverage of the most highly covered words at any point in our time series over time. Once we had narrowed down our list of words for every corpus along with their coverage over time, we converted these data frames into csv's so that we could import them in R and perform Granger tests. The final step of our project can be observed in grangertesting.html or grangertesting.Rmd and what it essentially comprises of is hand-selecting topics from the top 200 topics that we had filtered for in the previous step, and then performing a Granger test for causality from the coverage of that topic in the time series to movements in that stock's price within 5 days. After completing that last step, we were able to find a few words who's coverage over time was causally linked to changes in the stock's price. A lot of our documentation was best suited to be in docstring format within our files. Please take a look at the specified files for more details. Some Additional Information We decided to use the closing prices of stocks $TSLA, $PLTR, and $NFLX as our time series data. A reason we decided to choose these stocks was due to their liquidity, popularity, and unique ticker symbols which allowed for easy parsing. For our document collection we scraped twitter using Tweepy between 11/20/2020 and 12/11/2020 as there was increased volatility due to the US Presidential election and easy scrapers to aid in building the document collection. After creating the document collection, we created parsers using the PSLA algorithm to determine the probabilities of words in different ranges of vocabulary. The probabilities we calculated include a word in a tweet, a word in a tweet within a day, etc. We used stopwords as well to minimize the noise in our document collection. Additionally, our parser identified non-english tweets and scrapped them from the document collection. We used the same R library used by the paper used for determining Granger causality with the top 200 words for each of our three stock tickers. We experimented with different time lags between the range of 3-6 days to find the best results. Contributions of Each Member The project began with data collection by Joshua. The team decided that Twitter is a good platform to retrieve data from. After setting up a developer account, he began to pull tweets that contained PLTR, NFLX, and TSLA. 100 tweets were pulled from every day for the last month all using Twitter's API. These tweets were written into a respective .txt file and each tweet was treated as a document. Angeeras led the algorithms for topic modeling. Akhil contributed to the topic modeling as well. On top of that, he implemented Granger tests in R. The dataset for these algorithms to be run on came from the tweets that Joshua provided as well as the stock data that Akhil provided from Yahoo Finance. Overall the project was split very well. The contributions made by all members were all equally important in completing the project and also a great learning experience in applying class material to real world analysis. Conclusions & Further Research Things we learned: Creating a Twitter Document collection was quite difficult Excess noise in tweets makes it hard to scrape a good document collection Iterative topic modeling is complicated Pictures are very difficult to model Creating topic models for short documents is tricky Potential Future Extensions of this Project: Try to model more stocks and see what words we find as causally linked for these tickers Implement the feedback loop to allow our causal topics/words to guide our model and make it much more oaccurate Use the data from the project to created predictive models based on sentiment and topic coverage"
https://github.com/violetta-ta/CourseProject	FinalProjectProposal.pdf	Final project - Proposal Team information: Team Name: The West Coasters Team Members * Marina Polupanova - marinap2 [Captain] * Tirthankar Bhakta - tbhakta2 * Savita Manghnani - savitam2 Selected competition: Option4 - Text Classification competition State and goals of the project: Our team has almost no experience with any of the neural networks and machine learning frameworks. For this program we intend to explore on following techniques and Machine Learning frameworks: * Random Forest * Logistic Regression * Support Vector Machine * Recurrent Neural Network Machine Learning Frameworks * Tensorflow Tensoflow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that let researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. The system is general enough to be applicable in a wide variety of other domains, as well. We will pick up one ML model in the TensorFlow library for our classification task. * Keras The Keras neural networks library supports both convolutional and recurrent networks that are capable of running on either TensorFlow or Theano. Keras deep learning framework was built to provide a simplistic interface for quick prototyping by constructing active neural networks that can work with TensorFlow. In a nutshell, Keras is lightweight, easy-to-use, and has a minimalist approach. These are the very reasons as to why Keras is a part of TensorFlow's core API. The primary usage of Keras is in classification, text generation, and summarization, tagging, translation along with speech recognition, and others. * Scikit-learn Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. It is licensed under a permissive simplified BSD license and is distributed under many Linux distributions. We are planning to use the Scikit's robust set of algorithm for: SS Regression: Fitting logistic regression models. For Logistic regression we will as well try GLMnet library, and will submit the results of the library which will look more competitive. SS Decision Trees: Tree induction and pruning for both classification and regression tasks SS SVMs: for learning decision boundaries Programming language: * Python
https://github.com/violetta-ta/CourseProject	Project_progress_report.pdf	"CS 410: Project Progress Report The West Coasters | 28-Nov-2020 Text Classification Competition Team: The West Coasters 2020 Project Progress Report CS410: Text Information Systems Final Project: Text Classification Competition - Sarcasm detection Date: Saturday, Nov. 29, 2020 Team members: * Marina Polupanova ( University of Illinois at Urbana-Champaign) - marinap2@illinois.edu * Savita Manghnani ( University of Illinois at Urbana-Champaign) - savitam2@illinois.edu * Tirthankar Bhakta ( University of Illinois at Urbana-Champaign) - tbhakta2@illinois.edu Approaches considered for 'Sarcasm Detection': We have considered the following ideas to implement text classification in order to detect Sarcasm. 1. Text classification and sarcasm detection with CNN: We want to try a model, which can be made with parallel convolutional neural networks, where different kernel sizes used in parallel convolutional layers. This gives a multichannel input of text, that in fact uses different n-gram sizes. The network type was chosen, as over all the tutorials, it gave one of the best result on the text data, and also, logically, it should grab maximum information having in mind using different n-ngrams. The neural network will be built using TensorFlow and Keras libraries. 2. Text classification and sarcasm detection with BERT: The idea here is to build a classifier with BERT. BERT or Bidirectional Encoder Representations from Transformers is considered as a choice to do text classification as it can do compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after. We plan to use Tensorflow and Keras libraries to create the basic machine learning models and then pre-train the dataset with BERT, fine-tune the model and use it on the test data. 3. Text classification and sarcasm detection with DNN: Tensorflow library contains multiple estimators which can be used without building the complex models. Here I tried one such pre-made estimator, DNNClassifier, available in tensorflow python library for the classification task. DNNClassifier is an Tensorflow implementation of Deep Neural Network model. It is capable of accepting text in its raw format than required preprocessing to convert in numbers. Paired with Adagard, a gradient based optimization, DNNClassifier provides high recall. 4. Text classification and sarcasm detection with Bidirectional LSTM Long Short Term Memory networks - usually just called ""LSTMs"" - are a special kind of RNN, capable of learning long-term dependencies. The classification task requires the model to learn from various language usages, tones in the sentences, etc. The amount of learning for this supervised task requires, neural network needs to memorize the decisions from short term to long term. LSTM, as the name suggests fits the requirements very well. As there is no pre-estimator or readily available model available for LSTM in Tensorflow, requires creation of neural network model using Tensorflow library. Implementation & Current Status: 1. Text classification and sarcasm detection with CNN: The implementation steps for the model include following: a) Import required libraries - Tensorflow, Keras, Numpy, Pandas, Json, re, string b) Load the 'train.jsonl' and 'test.jsonl' file and read the file c) Create the train and validation labels and feature arrays from 'train.jsonl', and test label and feature arrays from 'test.jsonl'. d) Convert all the resulting arrays to tensors e) Pre-process all the feature train, test, and validation tensors to get a numeric input to Embedding layer and CNN. f) Create a model with Embedding and CNN layers g) Use model to train the data h) To evaluate the model i) To test the model on test data, and check the results vs Leaderboard. Status: Currently, the steps up to ""e"" are passed, now we are in the process of model creation and training. Challenges: There was a challenge to load the data from jsonl files to tensors, and it was multiple time failing as it was the first our experience with Pandas and Tensorflow data structures. After some efforts, the expected input was received. Current challenge is to create the Embedding/CNN of a structure, which will count towards the highest score. Possible issue can be that we have chosen to merge ""response"" and ""context"" in one variable, but so far we haven't yet understood how to make it a multichannel tensor. 2. Text classification and sarcasm detection with BERT: The idea is to use Tensorflow and Keras to create a ML model and use BERT pre- training models to perform pre-training on the data, fine tune it, optimize the output and create the final model. Here are the implementation steps taken into consideration: j) Import required libraries - Tensorflow, Keras, Numpy, Pandas, Jsonlines, Tensorflow-Hub k) Load the 'train.jsonl' file and read the file l) Create the train dataset from 'train.jsonl'. m) Split the dataset into train and validation datasets n) Load models from Tensorflow Hub o) Choose BERT models and determine the best fit to fine-tune p) Use BERT models to Pre-train the data q) Build own model by combining BERT with a classifier r) Train a model, including the preprocessing module, BERT encoder, data, and classifier s) Use an Optimizer like Adaptive Moments to fine-tune the model t) Run the test data and analyze data. Status: Currently, we are in the process of creating the dataset to train the model. The BERT models for pre-training and training are selected and also the model is built. Challenges: Using Tensorflow-Hub was giving an error - ""ImportError: cannot import name 'feature_column_v2' from 'tensorflow-hub"". Resolved the error by setting up a new environment and installing Tensorflow ver. 1.14. The current challenge is to create the expected dataset that BERT encoder and classifier can understand. 3. Text classification and sarcasm detection with DNN Current State: DNNClassifier is a pre-made estimator available in tensorflow library. The classifier is capable of supporting multiple classes for categorization. Implementation steps: a) Import required libraries, as it's a pre-made estimator not many libraries are required for making it work. Libraries it requires are tensorflow, numpy, pandas, sklearn, tensorflow_hub. b) Load the training dataset and split it into training and validation datasets as Pandas dataframes. c) Use TFHub sentence encoder to encode the datasets from Pandas dataframes. d) Created embeddings from above step are fed to DNNClassifier for training and evaluation. e) Use the trained model, categorize the test data set and generate answer.txt file. The latest submission with DNNClassifier, LiveLab reports F1 measure of 0.7073684210526316. Challenges: One of the challenge I faced with the approach was to embed text data to the classifier. Next steps: Train the model with ""Attention"" to improve on the performance. 4. Text classification and sarcasm detection with Bidirectional LSTM Current State: Unlike DNNClassifier where one does not have to build a model, LSTM requires creation of a neural network model. To create the model, we use tensorflow keras APIs. The generic APIs accept the data set in numeric format, thus preprocessing of input will not just involve cleaning of data but also converting data from text to numeric representation. Following steps were performed to achieve the task: a) Import required libraries like tensorflow, numpy, sklearn, TextVectorization, keras layers and losses. b) Load the training data set and split the data set into training and validation data sets. c) Preprocess data set to remove unwanted characters and words; convert the data set into numeric format as expected by keras APIs. d) Build neural network model with different layers including bidirectional LSTM. e) Train and evaluate the model. f) When satisfied with evaluation, use the model to predict for test data set. The latest submission with this model resulted in F1 measure of 0.6917372881355932. Challenges: Once the DNNClassifier implementation was done, this implementation did not take much time. The only challenge I faced was to vectorize the text in format understood by keras APIs. Next Steps: Train the model with ""Attention"" to improve on the performance."
https://github.com/violetta-ta/CourseProject	README.md	"CourseProject The project for Text Classification Competition from the team ""The West Coasters"": Tirthankar Bhakta tbhakta2@illinois.edu Marina Polupanova marinap2@illinois.edu Savita Manghnani savitam2@illinois.edu We planed to classify tweets to predict the ones which can be qualified as sarcasm using following methods: * Random Forest * Logistic Regression * Support Vector Machine * Recurrent Neural Network Instead, after feedback on the Project Proposal submission, we used following methods: * LSTM * DNN Classifier with DAN sentence encoder (hit the LiveDataLab threshold). Please note, that in the demo video we have told that we use Transformer sentence encoder, but in fact the one which hit the performance baseline was DAN sentence encoder, and it is posted in the dnn.py file. * Multi-channel CNN model * BERT Project Tutorial: https://mediaspace.illinois.edu/media/1_uscvryhp Project Documentation file: Documentation/ProjectDocumentation.pdf Installation instructions: Documentation/Installation_and_execution_instructions.txt The account on the LiveDataLab for submitting the ""answers.txt"" was ""marina_polupanova""."
https://github.com/danco14/CourseProject	ProgressReport.pdf	Progress Report 1) Progress made thus far So far, I have developed two different text classification models. Initially, I created an iPython notebook that is used to run all of the code. Two different models are then trained on the datasets The first model is LSTM, which is an RNN, and the second model is BERT. Both of the architectures are implemented and are running in the iPython notebook. Currently, I am developing both of these models for the competition, and I will choose which one to submit based on which has the better results. Both models have been both pre-trained and trained on the provided training dataset. The model files are imported into the notebook and then run. I am currently using the pytorch data loader to run the datasets. Since I do not have a GPU, the models are being run on google colab. 2) Remaining tasks I still need to pass the baseline accuracies. Each model still has to have its hyperparameters tuned to perform better on the test dataset. The architecture of the models may also have to be modified/developed if the resulting accuracy does not pass the baseline after some grid searching. 3) Any challenges/issues being faced A challenge that I am currently facing is having the models not overfit the training data. Most overfitting must also be changed by reducing the model architecture, and having to change the provided BERT model is tedious.
https://github.com/danco14/CourseProject	ProjectProposal.pdf	Project Proposal Name: Darren Anco (Captain) netid: danco2 Competition * Text Classification I am prepared to learn state-of-the-art neural network classifiers for this project. Neural Network Classifiers and Frameworks Some neural network classifiers that I have heard of are: * AlexNet * VGGNet * GoogLeNet * ResNet * ResNeXt * DenseNet I have used AlexNet before in MPs for other classes. For the rest, I learned about their structures and features from CS 498DL, but I have not used them in any projects. For frameworks, I have used: * PyTorch * Tensorflow I have a decent amount of experience with both of these frameworks. I have used them before in CS 440, CS 498DL, and CS 498AML. I have also worked with these frameworks on various projects outside of class. Programming Language For this project, I plan to use python.
https://github.com/danco14/CourseProject	README.md	Text Classification Competition Project Team: Darren Anco (danco2)
https://github.com/pmanden-uiuc/CourseProject	CourseProject-Progress-Report-11-28-20.pdf	"Course Project Progress Update (11/28/20) pmanden2@illinois.edu Objectives Objective of my project is to improve the ExpertSearch system. I will attempt to achieve the following enhancements in this project, as mentioned in the project proposal. 1. Given a URL, use Naive Bayes classifier to classify it as a directory page or a non-directory page 2. Given a URL use Naive Bayes classifier to classify it as a faculty page or a non-faculty page Which tasks have been completed? Getting the baseline code to work I have already spent quite a bit of time trying to get the baseline ExpertSearch code on Git to work. After trying different python versions, python library versions, trying on Linux and Windows and various experiments and debugging, I got it to work on Python 2.7 on Windows with some code changes. Now I have the baseline to do the actual implementation. Generating negative samples for directory and faculty pages I have written web scraping code to achieve the following: 1. Wrote code to get a list of all universities in US (from here - http://doors.stanford.edu/~sr/universities.html) 2. Wrote code to scrape each of the university in the list above, and identified 10 links 3. Wrote code to clean up the list to exclude directory/faculty URLs, so that it can be used as ""negative"" samples for directory and faculty classes. Now I have the positive and negatives samples for the directory and faculty URLs. Core Naive Bayes classifier code Core code that implements Naive Bayes classifier has been written. It can accept file names of positive and negative samples, load data, create term document matrix etc. Also provides a function for classification. Which Tasks are pending? 1. Need to get all the code to run on one version of Python, that I haven't been able to do so far. 2. Clean up code, better documentation I also will attempt to do the following (I am not very familiar with javascript/UI, so I may not be able to do this): 1. Change the UI so that an additional option can be added to the UI to type in a directory page or a faculty page so that classification results can be seen visually in UI Are you facing any challenges? As indicated above, I have managed to get the baseline ExpertSearch code to work on Python 2.7. However, my code (classifier etc.) doesn't run on 2.7 (it runs on 3.8). Need to figure out a way to get all code to run on one version. Dealing with Python versions and libraries continue to be a pain. Thoughts on work beyond the scope of this project In order to achieve full automation of identifying and extracting faculty pages, we will first need to automatically identify the directory and faculty pages on a university website, given a root URL for the university. This can be done if a list of university websites is available (one such list of universities in US is available here - http://doors.stanford.edu/~sr/universities.html) With full automation of identifying faculty web pages will look as below: 1. For each university a. Get list of all URLs on the website by: i. Finding the sitemap file (sitemap.xml, ...) ii. Or by crawling the website and generating a full list of all URLs on the website 2. From the list of URLs generated above a. Create 2 training sets i. One for directory pages classification (using the directory pages listing from Coursera?) ii. One for faculty pages classification (using faculty URLs provided) 3. Use Naive Bayes Classifier to identify URLs that are directory pages (If the ultimate objective is to find the faculty pages, there is really no need to find the directory pages, as the sitemap will contain the full faculty pages. And hence this step can be eliminated) 4. Use Naive Bayes classifier to identify URLs that are faculty pages 5. Scrape each faculty page classified as a faculty page My project implements part of this work. Maybe the rest can be done by a future student of this class! References https://sebastianraschka.com/Articles/2014_naive_bayes_1.html https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b https://towardsdatascience.com/implementing-a-naive-bayes-classifier-for-text-categorization-in-five- steps-f9192cdd54c3 https://www.xml-sitemaps.com/ https://code.google.com/archive/p/sitemap-generators/wikis/SitemapGenerators.wiki http://doors.stanford.edu/~sr/universities.html https://pagedart.com/blog/how-to-find-the-sitemap-of-a-website/"
https://github.com/pmanden-uiuc/CourseProject	CourseProject-Proposal-PrakashManden.pdf	"Course Project Proposal Prakash Manden Pmanden2@illinois.edu Note: I thought did everything as per the documentation by 10/25 deadline. However, I was not aware that a separate proposal was to be uploaded until I saw some notes on Piazza today. Instruction are a bit all over the place, and I wasn't aware of the need, otherwise I would have submitted it before the deadline. Improving a System - ExpertSystem Search I plan to implement the following project mentioned in the 'Course Project Topic' (text copied as is from the document) The ExpertSearch system (http://timan102.cs.illinois.edu/expertsearch//) was developed by some previous CS410 students as part of their course project! The system aims to find faculty specializing in the given research areas. The underlying data and ranker currently comes from the MP2 submissions of the previous course offering. You can read more about it here (Sections 3.6 and 4: Project are especially relevant). The code is available here. Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms from scratch. Automatically crawling faculty webpages Recall that you developed scrapers for faculty web-pages in MP2.1, which, in general, can be a time- consuming task. So, the question is can we automate this process? Some challenges include: * Identifying faculty directory pages: First, we need to identify the pages from where faculty web- pages can be mined. In MP2.1, we used faculty directory pages as the starting point to find faculty webpages. So, given a university website, can we automatically identify the directory pages? This can be posed as a classification task, i.e. classify a URL into a directory page vs. non-directory page. We have a huge resource of directory page URLs available in the sign-up sheet. These can be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs(e.g. other URLs on the university websites, product websites, news sites,etc.). These would be the ""negative"" examples. * Identifying faculty webpage URLs: Next, we need to extract the faculty webpages from the directory pages. This can again be posed as a classification task. Given a URL, can we identify whether it is a faculty webpage or not? We have a huge resource of faculty webpage URLs (available under MP2.3 on Coursera). These would be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs (e.g. other URLs on the university websites, product websites, news sites, etc.) to get the ""negative"" labels."
https://github.com/pmanden-uiuc/CourseProject	ProjectDocumentation.pdf	Improving a System - ExpertSystem Search CS410 Course Project, Fall 2020 pmanden2@illinois.edu Objectives From Improving a System - ExpertSystem Search 1 2 Objectives  Given a URL classify it as directory vs non-directory page  Given a URL classify it as faculty or non-faculty page [This project was done by myself as a one person team] Naive Bayes classifier Credit - https://sebastianraschka.com/Articles/2014_naive_bayes_1.html Text representation - Bag of words Implementation * 2 Classifiers built * Directory link classifier * Faculty link classifier * Directory link classifier Training data * Positive Samples collected from MP2.1 signup spreadsheet (as suggested) * 900 Samples are available (in file 'directory-positives.txt' in source code) * Negative Samples * A scraper utility was written to collect links from university webpages, which excluded all links with keywords such as 'directory', 'staff' etc. to ensure negative samples * 6592 samples are available (in file 'directory-negatives.txt' in source code) Implementation * Faculty link classifier Training data * Positive samples * Faculty pages from MP2.3 data on Coursera was used as suggested * 16492 samples are available (in file 'faculty-pages-positives.txt') * Negative samples * The same scraper was used to generate links from university websites. Links with keywords such as 'faculty', 'staff' are removed to ensure negative links * 6592 samples available (in file 'faculty-pages-negatives.txt') Source code - (Core file) * Classifier.py * Implements Naive Bayes Classifier in 'class naive_bayes_classifier' * Input * Name of file that contains positive samples * Name of file that contains negative samples * Number of samples to be used for training * 'initialize_classifier' method * Loads the samples from the specified files * Calculates and saves term document matrix, term frequency, no of terms, total counts for each terms for both positive and negative samples * 'classify' method * Accepts a url * Calculates probability of the words in url with laplace smoothing for both positive and negative classes * Returns True if the positive class probability is > negative class probability * It can be run to test the code independently * To test, Run - 'python Classifier.py', tests model accuracy and shows results Model Accuracy * Directory Classifier (with 800 samples, 100 test data) * Precision 0.96 * Recall 0.94 * F1 score 0.94 * Faculty Classifier (with 6000 samples, 300 test data) * Precision 0.99% * Recall 0.98 * F1 score 0.99 Source Code - Utilities * scraper.py : Scrapes all universities listed at * http://doors.stanford.edu/~sr/universities.html * 1,088 universities available * Identify 10 links from the home page of each university * Removes all links with key words that identify directory pages, such as 'directory', etc. * Removes all links with key words that identify faculty pages, such as 'faculty' etc. * Generates a list that is used as negative samples for both classifiers * Run - python scraper.py * Will take 3-4 hours to run. (Change 'no_universities_to_scrape' line 137 to test on limited number of universities) Source Code - UI, Data files * ExpertSearch app UI was modified to test URLs * UI changes are only for testing & and not relevant to improving the system! * templates/index.html * Modified to support a new pull down menu * static/index.js * Java script code changes to interface with backend * Data files * directory_positives.txt (Directory positive samples) * directory_negatives.txt (Directory negative samples) * faculty_positives.txt (Faculty positive samples) * faculty_negatives.txt (Faculty negative samples) Running the Application * To run ExpertSearch App run * python server.py * Open a browser and point to http://localhost:8095/ * Demo! - Play ProjectDemo.mp4
https://github.com/pmanden-uiuc/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Guanhuali2/CourseProject	Code Documentation.pdf	Documentation: Functions of our code: * Given a root website (University's Web Page URL) and its name (e.g. UIUC, UCB ....), the program will automatically find its Faculty directory page. * After getting the Faculty directory page, the program will automatically find all of its faculty homepages. * Then according to the user's instructions, the program will output a given number of Faculty Information including their Phone, Email, past College, personal bio, Research Area, and earned Awards. * This program can be used for CS students to search desired colleges' professors, and get their information in a convenient way (the program will filter useful information for them). How code is implemented: * For the first part, we use Selenium to find and interact with College's search bar in order to find candidates for the Faculty directory page url (directory.py). Since some college's search system is not perfect, we combine search results from google to make sure our candidate list includes the true Faculty directory page url. Then we build Random Forest Classifier (rfclassifier1.py, one model - rfclassifier1) with manually collected training data to classify the correct directory page url.(getfeature.py) * Second part, we keep using Selenium and Requests/Bs4 to find candidates' url list of Faculty Homepage. Since there is too much candidate url, we use some conditions to filter out as much as possible url. Then building another Random Forest Classifier (rfclassifier2.py, one model - rfclassifier2) to find corrected Faculty Homepages. (getfeature.py) * Inside each Faculty Homepage, we first get the html file and filter out useless information (dataprocess.py), and make all useful information into a list. Then we build a tfidf vectorizer as the feature to train our text classifier (Random Forest.py, two models - Vectorizer and text_classifier). Then we use the classifier to classify our desired information (bio,education,awards,research interests). How to run the code: * All the main code is inside directory.py. In order to run it, just need to run ./directory.py and make sure the computer has the correct version of browser to fit with Selenium. And then input College Url, College Name, and desired number of output. * Environment requirement: beautifulsoup4, selenium, numpy, pandas, seaborn, scikit-learn, matplotlib Contribution of Team member: * Guanhua Li: Implementing the Selenium/Web crawler part and building models for classifiers. * Ruoyu Zhu: Collect dataset and find features for models. * Ziqi Li: Find features for models , scrape features from webpages and function1 coding.
https://github.com/Guanhuali2/CourseProject	LZL Project Proposal.pdf	Team Name: LZL Team Member: Guanhua Li(guanhua2@illinois.edu),Ruoyu Zhu(ruoyuz3@illinois.edu),Ziqi Li(ziqili3@illinois.edu) Team Project: Improving ExpertSearch System Primary Language: Python Team Leader: Ruoyu Zhu * What is the function of the tool? Given a university website as the root URL, our tool is able to classify the correct faculty directory pages. Then we are able to classify the correct faculty home webpage according to the directory pages. Thus, our tool can further extract useful information from faculty webpages(Bios, Awards, Teaching Courses, Email...) * Who will benefit from such a tool? College students that want to get detailed information about certain professors. * Does this kind of tools already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? Our tool will try to achieve higher accuracy and efficiency. * What existing resources can you use? In order to build classifier, Python packages such as scikit-learn and Pytorch In order to extract useful features and information from given URL website, Python packages such as Selenium and Requests/Bs4 * What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) Using Web Crawler technique to get some features (or URL features) for building and training classifiers (Decision Trees - Random Forest Classifier). Classify the correct Faculty Directory pages and Faculty Home Webpages. Using Text Retrieval Technique learned from CS410 courses to get relevant useful text from Faculty Home Webpages. * How will you demonstrate the usefulness of your tool? It can efficiently get detailed and correct faculty information. * A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) By November, Finish building the basic classifier for Faculty Directory pages and the training process for it, and starting to evaluate the classifier for further improvement. By 11/10, Finish building the Basic classifier for Faculty Home Webpages, starting the evaluation process, at the same time, starting building proper text retrieval algorithm for extracting useful info from Faculty Homepages. By 11/15 Finish all the classifiers, and make sure the high accuracy of each classifier. By December, Finish the whole project
https://github.com/Guanhuali2/CourseProject	Progress report.pdf	1) Progress made thus far * Given the correct specific department of University, we are able to recognize correct faculty directory pages using classifiers. * Extract faculty web pages according to different directory pages. 2) Remaining tasks * Given correct Faculty pages, we still need to get useful information using some textual information retrieved methods. * Integrate all the code for different parts * Preparing a short presentation for the overall project 3) Any challenges/issues being faced * Since faculty directory pages are different among different departments, do we need to identify all departments' faculty directory pages given a University website, or particular department website will be given. * Since we are using classifiers to classify the correct website pages, we cannot ensure 100% accuracy on all the University.
https://github.com/Guanhuali2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Team Name : LZL Team Member: Guanhua Li, Ruoyu Zhu, Ziqi Li Team Leader: Guanhua Li, Ruoyu Zhu directory.py is the main program that needs to run. Documentation is inside the code dumentation.pdf file Demo video is inside the Project Video Link file In order to run the program, you need to allow browser remote automation, Safari menu->Developer->allow remote automation
https://github.com/steve303/CourseProject	README.md	CourseProject Summary In this project we trained a pre-built model (BERT) using the transformers library from Hugging Face on a dataset of labeled tweets, labeled for sarcasm (sarcastic/not sarcastic). We then used this model to predict the class (sarcastic/not sarcastic) of a set of provided unlabeled tweets for comparison to a competitive baseline score. We were able to beat the baseline with our model. See ./Project Documentation/final_summary.pdf for a full report. Repository Contents ./Alternative Methods & Models/: Contains additional models that we built and trained but which were unsuccessful at beating the baseline score. ./data/: Contains the test and train data provided for the competition. ./Project Documentation/: Contains the final report, demo, and other project deliverables. answer.txt: Our final file containing the classification of the test tweets which outperformed the F1 score of the baseline. Bert.ipynb: Our notebook file in which we build, train, and test the model. TEXT_PREPROCESSING.py: A dependecy of Bert.ipynb, used to preprocess the text for tokenization.
https://github.com/sairanga123/CourseProject	Improving ExpertSearch Progress Report.docx	"Improving ExpertSearch Progress Report Plan 2 Automate scraping process 2 Perform topic mining 2 Additional improvement 2 Improve UI 2 Progress 2 Automated scraping process 3 Deliverables: 3 Outputs: 3 Challenges: 3 Automated Scraper 4 Directory URL classification 5 1. Dataset preparation 5 2. Scraper 5 3. Text classification 6 Faculty URL classification 6 1. Dataset preparation 6 2. Scraper 6 3. Text classification 7 Topic Mining 7 Deliverables: 7 Outputs: 7 Challenges: 7 Topic Miner: 7 1. Corpus preparation 7 2. Model creation 8 3. Term extraction 8 Improved Email Extraction 9 Deliverables: 9 Regex Improvement: 9 UI Improvements 9 Deliverables: 9 Challenges: 9 Info Button: 9 Top 5 Topics Display: 9 Email Automation: 9 Plan Automate scraping process To identify faculty directory pages To identify faculty home pages Perform topic mining To identify top-k topics associated with each faculty Additional improvement To improve email extraction for each faculty Improve UI To display top-5 topics associated with each retrieved faculty To allow search based on any of the topics from the displayed topic cloud To prepopulate email content when clicked on a faculty's email address Progress Item Owner Status Automated Scraping Mriganka Sarma Completed: Automated scraper Data Handler Scraper Text Classifier Remaining: Optimizing parameters for classification Integration testing Challenges: None Topic Mining Zacharia Rupp Completed: Topic model Function to return top-10 words associated with query topic Remaining: Further exploration of best topics Clean up code Integration testing Challenges: Inferring topics takes considerable processing time Improved Email Extraction Improved UI Sai Ranganathan Completed: To display top 5 topics associated with each faculty member To prepopulate email field when clicked on email address To improve email extraction part 1. Challenges: None More detailed description is provided in the below sections. Automated scraping process Deliverables: Automated Scraper (auto_scraper.py) Data Handler (data_handler.py) Scraper (scraper.py) Text Classifier (text_classifier.py) Outputs: Corpus of classified Faculty Directory URLs Corpus of classified Faculty Bio URLs Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs Challenges: None Automated Scraper Automated scraper module (auto_scraper.py) automates the process in the following way: Uses the data handler to prepare a train and test set of Faculty Directory URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of directory contents Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" Saves the classified directory URLs to a file Uses the data handler to prepare a train and test set of Faculty Bio URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" Saves the classified bio URLs to a file Uses the scraper to scrape the faculty bios from the classified bio URLs Generates one document per faculty bio and saves under ExpertSearch/data/compiled_bios The automated scraper can be invoked as follows: $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t -d option specifies to generate/regenerate the train and test dataset. The dataset will be generated even if -d is not provided if the dataset doesn't exist yet. When -d is not provided, the existing dataset will be used. -t option specifies to retrain the Doc2Vec model on the train dataset. The model will be trained even if -t is not provided if the model wasn't trained and saved yet. When -t is not provided, the saved model will be loaded. The following sections describe the text classification tasks for Faculty Directory URLs and Faculty Bio URLs. Directory URL classification Dataset preparation First we need to prepare the dataset for training and testing the model. The following approach was used to prepare the dataset. Downloaded the known faculty directory pages from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. Collected top URLs from Alexa. These will serve as the ""negative"" examples. Collected the global top-50 pages of Alexa. Collected the top-50 pages for different countries. Manually verified that the pages are in English. About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. URLs for total 14 countries + top-50 global URLs from Alexa were collected. This gave 750 ""negative"" URLs. Wrote a python module (data_handler.py) for data handling that does the following: Converts the MP 2.1 sign-up data from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. Combines the top-50 Alexa URLs for 5 countries and labels them as ""test_dir"". Uses these 250 pages for testing. Mix the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Remove duplicates if any. This gives 734 URLs as the final training URLs. Mix the 400 Faculty Directory test URLs with the 250 Alexa training URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. Scraper Wrote a python module (scraper.py) for scraping the URLs collected from the above step. The scraper does the following: Gets the contents of each URL as text. Performs clean-up of non-ascii characters from the content. Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". Writes contents of each webpage as a single line of space separated words in a file meant to be the final corpus. This is done to prepare both the training corpus (""train_dataset.cor"") and the test corpus (""test_dataset.cor""). Text classification Wrote a python module (text_classifier.py) for performing the text classification task of identifying valid Faculty Directory pages from the test corpus. The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Dataset preparation The following approach was used to prepare the dataset: Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the train URLs. Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. Use the data handling module to do the following: Tag the faculty bio URLs as ""faculty"" and save to a file. Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. This will be the final file with all the train URLs. Scraper Since the ExpertSearch project already contains the faculty bios as documents, the top 1000 faculty bios are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: Scrapes the remaining train URLs from the train URLs file and appends to the train corpus (""train_bio_dataset.cor""). Uses the classified Faculty Directory URLs from the classified Directory URLs file above and gets all embedded potential faculty bio URLs as the test URLs. Scrapes the test URLs from above and adds to a test corpus (""test_bio_dataset.cor""). Text classification The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_bio_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Topic Mining Deliverables: Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Outputs: Trained topic model ('lda_mallet_model') Bag-of-words representation of corpus to be used with miner.py ('corpus_dictionary') Challenges: Inferring topics takes considerable processing time. Topic Miner: The topic miner uses gensim and mallet to create a model from the entire corpus. The process is as follows: Corpus preparation Read in compiled bios as strings Filter the string representation of each bio to: Remove stop words Extract HTML tags and elements Strip non-alphanumeric characters Strip numbers Strip words that exist in lists of terms extracted from the bios Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) Remove words shorter than four characters Split all words into a list of tokens Create list of documents which is comprised of lists of tokens for each document as described above Append bigrams and trigrams to each token list for each document Create a gensim dictionary from the above documents Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below: Create a general model with gensim.models.ldamodel.LdaModel class with 10 models Visually inspect term clusters to ensure they were meaningful Visualize clusters with pyLDAvis to assess clusters If the above criteria were not satisfactory: Tweak corpus construction After the above criteria was deemed satisfactory: Using gensim.models.wrappers.LdaMallet with the mallet library, I: Varied number of topics to create new model Assessed coherence of each model with varying number of topics Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains Chose the best model according to above criteria and saved it and the created dictionary for query inference Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query. Improved Email Extraction Deliverables: email-extraction.py Regex Improvement: There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Deliverables: Server (server.py) UI Front (index.js) Challenges: None Info Button: Information button is created at the top of each of the retrieved faculty. When the button is clicked there a table pops up that appears below the selected retrieved faculty The table will contain additional information regarding the research topics that the faculty does When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: Display the top 5 topics from the preview for each of the faculty. Display these topics in a table format when the information button is clicked Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Email Automation: Email comes pre-populated with a set subject and body. The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. 1"
https://github.com/sairanga123/CourseProject	Improving ExpertSearch Progress Report.pdf	"1 Improving ExpertSearch Progress Report Plan ........................................................................................................................................................................................ 2 Automate scraping process ....................................................................................................................................... 2 Perform topic mining ................................................................................................................................................... 2 Additional improvement .............................................................................................................................................. 2 Improve UI ....................................................................................................................................................................... 2 Progress ............................................................................................................................................................................... 2 Automated scraping process .................................................................................................................................... 3 Deliverables: ............................................................................................................................................................. 3 Outputs: ..................................................................................................................................................................... 3 Challenges: ................................................................................................................................................................ 3 Automated Scraper ................................................................................................................................................. 4 Directory URL classification ................................................................................................................................. 5 1. Dataset preparation .................................................................................................................................. 5 2. Scraper ........................................................................................................................................................ 5 3. Text classification ..................................................................................................................................... 6 Faculty URL classification ..................................................................................................................................... 6 1. Dataset preparation .................................................................................................................................. 6 2. Scraper ........................................................................................................................................................ 6 3. Text classification ..................................................................................................................................... 7 Topic Mining ................................................................................................................................................................... 7 Deliverables: ............................................................................................................................................................. 7 Outputs: ..................................................................................................................................................................... 7 Challenges: ................................................................................................................................................................ 7 Topic Miner: .............................................................................................................................................................. 7 1. Corpus preparation .................................................................................................................................. 7 2. Model creation ........................................................................................................................................... 8 3. Term extraction ......................................................................................................................................... 8 Improved Email Extraction ......................................................................................................................................... 9 Deliverables: ............................................................................................................................................................. 9 Regex Improvement: .......................................................................................................................................... 9 UI Improvements ........................................................................................................................................................... 9 Deliverables: ............................................................................................................................................................. 9 Challenges: ................................................................................................................................................................ 9 Info Button: ........................................................................................................................................................... 9 Top 5 Topics Display: ........................................................................................................................................ 9 Email Automation: .............................................................................................................................................. 9 2 Plan Automate scraping process - To identify faculty directory pages - To identify faculty home pages Perform topic mining - To identify top-k topics associated with each faculty Additional improvement - To improve email extraction for each faculty Improve UI - To display top-5 topics associated with each retrieved faculty - To allow search based on any of the topics from the displayed topic cloud - To prepopulate email content when clicked on a faculty's email address Progress Item Owner Status Automated Scraping Mriganka Sarma Completed: - Automated scraper - Data Handler - Scraper - Text Classifier Remaining: - Optimizing parameters for classification - Integration testing Challenges: - None Topic Mining Zacharia Rupp Completed: - Topic model - Function to return top-10 words associated with query topic Remaining: - Further exploration of best topics - Clean up code 3 - Integration testing Challenges: - Inferring topics takes considerable processing time Improved Email Extraction Improved UI Sai Ranganathan Completed: - To display top 5 topics associated with each faculty member - To prepopulate email field when clicked on email address - To improve email extraction part 1. Challenges: - None More detailed description is provided in the below sections. Automated scraping process Deliverables: - Automated Scraper (auto_scraper.py) - Data Handler (data_handler.py) - Scraper (scraper.py) - Text Classifier (text_classifier.py) Outputs: - Corpus of classified Faculty Directory URLs - Corpus of classified Faculty Bio URLs - Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs Challenges: - None 4 Automated Scraper Automated scraper module (auto_scraper.py) automates the process in the following way: - Uses the data handler to prepare a train and test set of Faculty Directory URLs - Uses the scraper to scrape these URLs to prepare the train and test corpus - Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of directory contents - Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" - Saves the classified directory URLs to a file - Uses the data handler to prepare a train and test set of Faculty Bio URLs - Uses the scraper to scrape these URLs to prepare the train and test corpus - Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios - Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" - Saves the classified bio URLs to a file - Uses the scraper to scrape the faculty bios from the classified bio URLs - Generates one document per faculty bio and saves under ExpertSearch/data/compiled_bios The automated scraper can be invoked as follows: $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t -d option specifies to generate/regenerate the train and test dataset. The dataset will be generated even if -d is not provided if the dataset doesn't exist yet. When -d is not provided, the existing dataset will be used. -t option specifies to retrain the Doc2Vec model on the train dataset. The model will be trained even if -t is not provided if the model wasn't trained and saved yet. When -t is not provided, the saved model will be loaded. The following sections describe the text classification tasks for Faculty Directory URLs and Faculty Bio URLs. 5 Directory URL classification 1. Dataset preparation First we need to prepare the dataset for training and testing the model. The following approach was used to prepare the dataset. o Downloaded the known faculty directory pages from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. o Collected top URLs from Alexa. These will serve as the ""negative"" examples. SS Collected the global top-50 pages of Alexa. SS Collected the top-50 pages for different countries. Manually verified that the pages are in English. o About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. o URLs for total 14 countries + top-50 global URLs from Alexa were collected. This gave 750 ""negative"" URLs. o Wrote a python module (data_handler.py) for data handling that does the following: SS Converts the MP 2.1 sign-up data from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". SS Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. SS Combines the top-50 Alexa URLs for 5 countries and labels them as ""test_dir"". Uses these 250 pages for testing. SS Mix the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Remove duplicates if any. This gives 734 URLs as the final training URLs. SS Mix the 400 Faculty Directory test URLs with the 250 Alexa training URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. 2. Scraper Wrote a python module (scraper.py) for scraping the URLs collected from the above step. The scraper does the following: o Gets the contents of each URL as text. o Performs clean-up of non-ascii characters from the content. o Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. o Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". 6 o Writes contents of each webpage as a single line of space separated words in a file meant to be the final corpus. SS This is done to prepare both the training corpus (""train_dataset.cor"") and the test corpus (""test_dataset.cor""). 3. Text classification Wrote a python module (text_classifier.py) for performing the text classification task of identifying valid Faculty Directory pages from the test corpus. The classification module does the following: o Uses gensim to build a Doc2Vec model for feature vector representation of each document. o Uses the train_dataset.cor to build the vocabulary and train the model. o Saves the model so that it can be reloaded while running next time on the same dataset. o Uses LogisticRegression as the classifier from scikit-learn module. o Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification 1. Dataset preparation The following approach was used to prepare the dataset: o Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the train URLs. o Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. o Use the data handling module to do the following: SS Tag the faculty bio URLs as ""faculty"" and save to a file. SS Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. SS This will be the final file with all the train URLs. 2. Scraper Since the ExpertSearch project already contains the faculty bios as documents, the top 1000 faculty bios are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: o Scrapes the remaining train URLs from the train URLs file and appends to the train corpus (""train_bio_dataset.cor""). o Uses the classified Faculty Directory URLs from the classified Directory URLs file above and gets all embedded potential faculty bio URLs as the test URLs. 7 o Scrapes the test URLs from above and adds to a test corpus (""test_bio_dataset.cor""). 3. Text classification The classification module does the following: o Uses gensim to build a Doc2Vec model for feature vector representation of each document. o Uses the train_bio_dataset.cor to build the vocabulary and train the model. o Saves the model so that it can be reloaded while running next time on the same dataset. o Uses LogisticRegression as the classifier from scikit-learn module. o Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Topic Mining Deliverables: - Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Outputs: - Trained topic model ('lda_mallet_model') - Bag-of-words representation of corpus to be used with miner.py ('corpus_dictionary') Challenges: - Inferring topics takes considerable processing time. Topic Miner: The topic miner uses gensim and mallet to create a model from the entire corpus. The process is as follows: 1. Corpus preparation o Read in compiled bios as strings o Filter the string representation of each bio to: - Remove stop words - Extract HTML tags and elements - Strip non-alphanumeric characters 8 - Strip numbers - Strip words that exist in lists of terms extracted from the bios - Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) - Remove words shorter than four characters - Split all words into a list of tokens o Create list of documents which is comprised of lists of tokens for each document as described above o Append bigrams and trigrams to each token list for each document o Create a gensim dictionary from the above documents o Create a bag-of-word representation of our documents: this will be our corpus. 2. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below: o Create a general model with gensim.models.ldamodel.LdaModel class with 10 models o Visually inspect term clusters to ensure they were meaningful o Visualize clusters with pyLDAvis to assess clusters o If the above criteria were not satisfactory: a. Tweak corpus construction o After the above criteria was deemed satisfactory: a. Using gensim.models.wrappers.LdaMallet with the mallet library, I: i. Varied number of topics to create new model ii. Assessed coherence of each model with varying number of topics iii. Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains iv. Chose the best model according to above criteria and saved it and the created dictionary for query inference 3. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query. 9 Improved Email Extraction Deliverables: - email-extraction.py Regex Improvement: - There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting - Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Deliverables: - Server (server.py) - UI Front (index.js) Challenges: - None Info Button: - Information button is created at the top of each of the retrieved faculty. - When the button is clicked there a table pops up that appears below the selected retrieved faculty - The table will contain additional information regarding the research topics that the faculty does - When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: - Display the top 5 topics from the preview for each of the faculty. - Display these topics in a table format when the information button is clicked - Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Email Automation: - Email comes pre-populated with a set subject and body. - The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic."
https://github.com/sairanga123/CourseProject	Project Report - Improved ExpertSearch.docx	"PROJECT REPORT Improved ExpertSearch System (Team ZMS) Zacharia Rupp (zrupp2@illinois.edu) Mriganka Sarma (ms76@illinois.edu) Sai Ranganathan (sr50@illinois.edu) Introduction 4 1. Functional Overview 4 2. Implementation Details 4 2.1. Automated scraping process 4 2.1.1. Inputs: 5 2.1.2. Outputs: 5 2.1.3. Deliverables: 5 2.1.4. Component design / Code workflow: 5 Automated Scraper (auto_scraper.py) 5 Directory URL Classification 6 Data Handler (data_handler.py) 7 Scraper (scraper.py) 8 Text Classifier (text_classifier.py) 9 Faculty URL classification 9 Data Handler (data_handler.py) 9 Scraper (scraper.py) 11 Text Classifier (text_classifier.py) 11 2.2. Topic Mining 12 2.2.1. Inputs: 12 2.2.2. Outputs: 12 2.2.3. Deliverables: 12 2.2.4. Component design / Code workflow: 12 Topic Miner (miner.py) 12 Corpus preparation 12 Model creation 13 Term extraction 14 2.3. Improved Email Extraction 16 2.3.1. Inputs: 16 2.3.2. Outputs: 16 2.3.3. Deliverables: 16 2.3.4. Component design / Code workflow: 16 Regex Improvement: 16 2.4. UI Improvements 16 2.4.1. Inputs: 16 2.4.2. Outputs: 16 2.4.3. Deliverables: 16 2.4.4. Component design / Code workflow: 17 Info Button: 17 Top 5 Topics Display: 17 Email Automation: 17 3. Usage Details 17 3.1. Setup Guide (Mac) 17 3.1.1. Repo setup 17 3.1.2. Project environment setup 18 3.2. Setup Guide (Windows) 19 3.3. Usage Guide 19 3.3.1. Running the Automated Scraper 19 3.3.2. Running the Topic Miner 20 3.3.3. Running the Backend Server 20 3.3.4. Running Faculty Search from the UI 21 3.4. Example Use Cases: 21 3.4.1. Use Case 1 - Basic use to search faculties 21 3.4.2. Use Case 2 - Find research interests of the faculty 21 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results 21 3.4.4. Use Case 4 - Connecting to faculty 21 4. Contributions 22 Introduction The ExpertSearch system is a system to search faculties who are experts in certain research areas or topics from university websites crawled from the web. The goal of our project is to improve this ExpertSearch system in a few ways, including automating the scraping process, adding topic mining for finding faculty research topics, and improving the UI to give improved visualizations and query refinement options. Functional Overview The improved ExpertSearch system enables the following functionalities: Automatic scraping of websites to identify faculty directory webpages and non-directory webpages Automatic scraping of the classified faculty directory webpages to further identify faculty bio webpages and non-bio webpages Automatic scraping of faculty bio webpages to generate one bio document per faculty and adding to the compiled bios Topic Mining from the compiled bios to extract research topics of the faculties Display top-5 research topics associated with each retrieved faculty Improved email extraction for each faculty Refine search query using any of the topics from the displayed topic cloud Prepopulate email content when clicked on a faculty's email address The Automated Scraper improves the faculty bio generation process from a vast collection of websites. The Topic Miner adds more structure to the unstructured faculty website data retrieved from a query. The enhanced UI enables succinct visualization of the structured faculty results and provides shortcuts for additional search filters and faculty connection. Together, these new features improve the utility of the ExpertSearch system to the user. Implementation Details Automated scraping process The Automated Scraper takes a set of known University websites and top 500 Alexa websites as input, performs a series of operations to classify the directory URLs and then to classify the faculty homepages. The automated scraper then scrapes the classified faculty homepages to generate faculty bio documents and adds the bios to the collection. Inputs: University websites Top-500 Alexa websites Outputs: Corpus of classified Faculty Directory URLs (classified_dir_urls.cor) Corpus of classified Faculty Bio URLs (classified_faculty_urls.cor) Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs (e.g. 6530.txt) Deliverables: Automated Scraper (auto_scraper.py) Data Handler (data_handler.py) Scraper (scraper.py) Text Classifier (text_classifier.py) Component design / Code workflow: Automated Scraper (auto_scraper.py) The Automated Scraper module automates the whole flow of generating the faculty bios from the input mixture of ""positive"" and ""negative"" URLs in the following sequence of steps: Uses the data handler (data_handler.py) to prepare a train and test set of Faculty Directory URLs Uses the scraper (scraper.py) to scrape these URLs to prepare the train and test corpus Uses the text classifier (text_classifier.py) to build and train a Doc2Vec model on the documents in the train corpus of directory contents Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" Saves the classified directory URLs to a file (classified_dir_urls.cor) Uses the data handler to prepare a train and test set of Faculty Bio URLs Uses the scraper to scrape these URLs to prepare the train and test corpus Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" Saves the classified bio URLs to a file (classified_faculty_urls.cor) Uses the scraper to scrape the faculty bios from the classified bio URLs Generates one document per faculty bio (e.g. 6530.txt) and saves under ExpertSearch/data/compiled_bios The auto_scraper.py module is the entry point for the complete automatic scraping and bio generation task. The following figure (Fig. 1) shows the complete automation flow starting with the input websites till the bio generation completion. Fig. 1: Automation Control Flow / Module interactions Directory URL Classification First, let's explain the Directory URL Classification task with the help of the modules. Data Handler (data_handler.py) The Data Handler module first takes the University websites and Alexa websites as input, mixes them and partitions them into test and train URLs. Then uses the scraper module to extract the URL contents into test and train corpus as shown in the figure below (Fig. 2). Fig. 2: Dataset Preparation for Directory URL Classification Here's a detailed explanation of the approach used by the Data Handler module to prepare the URLs for the scraper. Downloaded the known faculty directory URLs from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. Collected top URLs from Alexa. These will serve as the ""negative"" examples. Collected the global top-50 pages of Alexa. Collected the top-50 pages for 14 different countries. Manually verified that the pages are in English. This gave 750 ""negative"" URLs. When the AutoScraper is launched, it invokes the DataHandler which mixes and partitions the above URLs into train and test URLs as follows: Training URLs Converts the MP 2.1 sign-up sheet from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. Mixes the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Removes duplicates if any. This gives 734 URLs as the final training URLs. The training URLs are saved in the file train_urls.cor. Test URLs Combines the top-50 Alexa URLs for 5 countries. Uses these 250 pages for testing. Mixes the 400 Faculty Directory test URLs with the 250 Alexa test URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. The test URLs are saved in the file test_urls.cor. Scraper (scraper.py) The Scraper module scrapes the contents from the above train and test URLs and prepares the train and test corpus for the classification task. The scraper does the following: Gets the contents of each URL as text. Performs clean-up of non-ascii characters from the content. Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". Writes contents of each training URL as a single line of space separated words to the training corpus (""train_dataset.cor""). Similarly, writes contents of each test URL as a single line of space separated words to the test corpus (""test_dataset.cor""). Text Classifier (text_classifier.py) The Text Classifier module uses the train and test dataset from above step to classify the Faculty Directory URLs. The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Next, let's look at the Faculty URL Classification task. Data Handler (data_handler.py) The Data Handler module now takes the existing project's known Faculty Bio URLs and mixes with some Alexa URLs to prepare the train dataset. Uses the classified Faculty Directory URLs from the above step to extract potential Faculty Bio URLs to prepare the test dataset. Then uses the scraper module to extract the URL contents into bio test and bio train corpus as shown in the figure below (Fig. 3). Fig. 3: Dataset Preparation for Faculty Bio URL Classification The following approach was used to prepare the dataset: Training URLs Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the ""positive"" train URLs. Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. Combine them. Tag the faculty bio URLs as ""faculty"" and save to the file ""train_bio_urls.cor"". Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. This will be the final file with all the train URLs. Test URLs Use the Classified Faculty Directory URLs obtained from the Directory URL Classification task above. Use the scraper to find all potential faculty bio URLs from each of these Directory URLs. Save all these potential faculty bio URLs to the file ""test_bio_urls.cor"". This will be the final file with all the test URLs. Scraper (scraper.py) Since the ExpertSearch project already contains the faculty bios as documents, the contents of the top 1000 faculty bios (0.txt ... 999.txt) are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: Scrapes the URLs from line no. 1000 till the end from the file train_bio_urls.cor and appends to the train corpus (""train_bio_dataset.cor""). Scrapes the contents of the test URLs (i.e. potential faculty bio URLs) from the test_bio_urls.cor file above and adds those contents to the test corpus (""test_bio_dataset.cor""). Text Classifier (text_classifier.py) The classification module does the following: Uses gensim to build a Doc2Vec model for feature vector representation of each document. Uses the train_bio_dataset.cor to build the vocabulary and train the model. Saves the model so that it can be reloaded while running next time on the same dataset. Uses LogisticRegression as the classifier from scikit-learn module. Uses LogisticRegression to predict the categories (bio or non-bio) of the test URLs given the test dataset. Finally, the Scraper module scrapes these classified bio URLs and saves the contents of each bio URL to a new file under ExpertSearch/data/compiled_bios. Topic Mining Inputs: Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) Outputs: Trained topic model (lda_mallet_model) Bag-of-words representation of corpus to be used with miner.py (corpus_dictionary) Text representation of corpus (lda_corpus) Deliverables: Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) Component design / Code workflow: Topic Miner (miner.py) The topic miner pulls a topic distribution from a document already mined if the model was trained on the document, otherwise it uses gensim and mallet to create a model from the entire corpus. The process is described below: Corpus preparation Read in compiled bios as strings Filter the string representation of each bio to: Remove stop words Extract HTML tags and elements Strip non-alphanumeric characters Strip numbers Strip words that exist in lists of terms extracted from the bios Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) Remove words shorter than four characters Split all words into a list of tokens Create list of documents which is comprised of lists of tokens for each document as described above Append bigrams and trigrams to each token list for each document Create a gensim dictionary from the above documents Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below (Fig. 4): Create a general model with gensim.models.ldamodel.LdaModel class with 10 models Visually inspect term clusters to ensure they were meaningful If the above criteria were not satisfactory: Tweak corpus construction After the above criteria was deemed satisfactory: Using gensim.models.wrappers.LdaMallet with the mallet library, I: Varied number of topics to create new model Assessed coherence of each model with varying number of topics Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains Chose the best model according to above criteria and saved it and the created dictionary for query inference Fig. 4: Workflow for building an optimal topic model. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query as shown below (Fig. 5). Fig. 5: Extracting topics from documents not included in training set. Improved Email Extraction Inputs: Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) Outputs: Extracted emails for the faculties Deliverables: Updated email extractor (extract-email.py) Component design / Code workflow: Regex Improvement: There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu UI Improvements Inputs: Query terms in the search box Topics mined by the topic miner Outputs: Updated UI showing top-5 research topic per faculty Updated UI showing topic cloud Clickable topic terms for query refinement Prepopulated email template on-click email icon Deliverables: Updated server endpoints (server.py) Updated UI (index.js) Component design / Code workflow: Info Button: Information button is created at the top of each of the retrieved faculty. When the button is clicked there a table pops up that appears below the selected retrieved faculty The table will contain additional information regarding the research topics that the faculty does When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: Display the top 5 topics from the preview for each of the faculty. Display these topics in a table format when the information button is clicked Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. Clicking on the ""Add to Query"" button will refine the current search query to include this topic. Email Automation: Email comes pre-populated with a set subject and body. The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. Usage Details The modified project has been tested on Mac and Windows with Python 2.7. Here are the setup instructions for each of these platforms. Setup Guide (Mac) Repo setup Run the following command on a terminal to clone the github repository. $ git clone https://github.com/sairanga123/CourseProject.git The directory structure of the project is as below (listing only the files/folders relevant to this project): CourseProject |__________ ExpertSearch |__________ AutoScraper | |__________ data | |__________ auto_scraper.py | |__________ data_handler.py | |__________ scraper.py | |__________ text_classifier.py | |__________ d2v.model | |__________ d2v-bio.model | |__________ data | |__________ compiled_bios | |__________ expertsearch | |__________ mallet-2.0.8 | |__________ model_files | |__________ corpus_dictionary | |__________ lda_mallet_model | |__________ lda_corpus | |__________ miner.py | |__________ extraction |__________ mallet-2.0.8 |__________ static |__________ server.py Project environment setup The project has been tested on python 2.7. Please setup a python 2.7 environment for running the project. Creating an environment from Anaconda will make many common packages available. So a quick way to start would be to setup a python 2.7 environment from Anaconda. May need to install many or all of the following python packages depending on what packages the python environment already has. gunicorn=19.10.0 flask=1.1.2 metapy=0.2.13 requests=2.25.0 pytoml=0.1.21 gensim=3.8.3 nltk=3.4 bs4=0.0.1 lxml=4.6.2 numpy=1.16.6 sklearn=0.0 Setup Guide (Windows) Windows is currently not supported. If you want to build in Windows, use Windows Subsystem for Linux and follow the steps above. Usage Guide Running the Automated Scraper Run the AutoScraper to generate the bio documents. This step has been already performed and the generated bio documents have already been added to the ExpertSearch/data/compiled_bios folder. Here are the instructions for running the AutoScraper if it needs to be run again with additional input. To run the automated scraper, first go the ExpertSearch/AutoScraper directory. Then the Automated Scraper can be invoked as follows: -d option specifies to generate/regenerate the train and test dataset. If -d switch is used: If the dataset already exists, it will be regenerated If the dataset doesn't yet exist, it will be generated If -d switch is not used: If the dataset already exists, the existing dataset will be used in the subsequent flow If the dataset doesn't yet exist, it will be generated even if -d switch is not used -t option specifies to train/retrain the Doc2Vec model on the train dataset. If -t switch is used: If a trained and saved model already exists, the model will be retrained and saved again If a trained and saved model doesn't yet exist, it will be trained and saved If -t switch is not used: If a trained and saved model already exists, the saved model will be loaded and used for inference in the subsequent flow If a trained and saved model doesn't yet exist, it will be trained and saved even if -t switch is not used Running the Topic Miner The steps for creating the topic model are documented in ExpertSearch/data/expertsearch/LDATopicModeling.ipynb. The model construction is not something that can necessarily be automated because relying on perplexity and coherence scores alone often results in topics that don't make any meaningful sense to a human. Once the topic model is constructed and saved, miner.py allows the server to load the model and make inferences. Running the Backend Server Once, the topic model has been built, we can start the server. To start the server, go to the ExpertSearch folder. Then run the following command: $ gunicorn server:app -b 127.0.0.1:8095 Running Faculty Search from the UI Now, launch a web browser and type the following URL: localhost:8095 Example Use Cases: Use Case 1 - Basic use to search faculties Let's assume that we want to find the faculties that are working on ""text mining"". Then we'd go to the UI and enter our search string as ""text mining"". The existing system would retrieve the top ranked faculty results working on ""text mining"". Use Case 2 - Find research interests of the faculty Now, the improved system will also provide an info button which will bring up an additional table of information for each faculty. This table shows the top 5 research topics the faculty is associated with. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results We now maybe interested in learning more about who are the faculties that are working on any of these research topics. We can quickly search for all the faculties working on this new research topic by simply clicking on the ""Add to Query"" button for that research topic. This will automatically modify our search query by including that new research topic without having to type it in the search box. The retrieved faculty results will show the list of faculties working on that research topic. Use Case 4 - Connecting to faculty Another way we could use the system is to click on the email icon to send an email to the faculty's email address. While we may be at a loss of words for that first email, the system will provide a pre-populated template email which will automatically address the faculty's name and also include reference to the faculty's research area. This will make connecting to an expert faculty just one click away. Contributions Item Sub-items Contributor Automated Scraping Automated Scraper to automate the complete process Data Handler to prepare the datasets for the text classification tasks Scraper to scrape the URLs Text Classifier to classify directory and bio URLs Function to generate bio documents and add to compiled bios Mriganka Sarma Topic Mining Topic model Function to return top-10 words associated with query topic Zacharia Rupp Improved Email Extraction Added regular expressions to extract emails with atypical forms (e.g. person at place dot com) Sai Ranganathan Zacharia Rupp Improved UI Display top 5 topics associated with each faculty member Display cloud of topics Pre-populate email field when clicked on email address Improve email extraction part 1. Sai Ranganathan 1"
https://github.com/sairanga123/CourseProject	Project Report - Improved ExpertSearch.pdf	"1 PROJECT REPORT Improved ExpertSearch System (Team ZMS) Zacharia Rupp (zrupp2@illinois.edu) Mriganka Sarma (ms76@illinois.edu) Sai Ranganathan (sr50@illinois.edu) 2 Introduction ................................................................................................................................. 4 1. Functional Overview ......................................................................................................... 4 2. Implementation Details .................................................................................................... 4 2.1. Automated scraping process ..................................................................................... 4 2.1.1. Inputs: ........................................................................................................................ 5 2.1.2. Outputs: .................................................................................................................... 5 2.1.3. Deliverables: ............................................................................................................ 5 2.1.4. Component design / Code workflow: ................................................................ 5 Automated Scraper (auto_scraper.py) ......................................................................... 5 Directory URL Classification .......................................................................................... 6 Data Handler (data_handler.py) ................................................................................. 7 Scraper (scraper.py) ..................................................................................................... 8 Text Classifier (text_classifier.py) ............................................................................. 9 Faculty URL classification .............................................................................................. 9 Data Handler (data_handler.py) ................................................................................. 9 Scraper (scraper.py) ................................................................................................... 11 Text Classifier (text_classifier.py) ........................................................................... 11 2.2. Topic Mining .................................................................................................................. 12 2.2.1. Inputs: ...................................................................................................................... 12 2.2.2. Outputs: .................................................................................................................. 12 2.2.3. Deliverables: .......................................................................................................... 12 2.2.4. Component design / Code workflow: .............................................................. 12 Topic Miner (miner.py) ................................................................................................... 12 Corpus preparation ..................................................................................................... 12 Model creation .............................................................................................................. 13 Term extraction ............................................................................................................ 14 2.3. Improved Email Extraction ........................................................................................ 16 2.3.1. Inputs: ...................................................................................................................... 16 2.3.2. Outputs: .................................................................................................................. 16 2.3.3. Deliverables: .......................................................................................................... 16 2.3.4. Component design / Code workflow: .............................................................. 16 Regex Improvement: ................................................................................................... 16 2.4. UI Improvements .......................................................................................................... 16 3 2.4.1. Inputs: ...................................................................................................................... 16 2.4.2. Outputs: .................................................................................................................. 16 2.4.3. Deliverables: .......................................................................................................... 16 2.4.4. Component design / Code workflow: .............................................................. 17 Info Button: ................................................................................................................... 17 Top 5 Topics Display: ................................................................................................. 17 Email Automation: ....................................................................................................... 17 3. Usage Details .................................................................................................................... 17 3.1. Setup Guide (Mac) ....................................................................................................... 17 3.1.1. Repo setup ............................................................................................................. 17 3.1.2. Project environment setup ................................................................................. 18 3.2. Setup Guide (Windows) .............................................................................................. 19 3.3. Usage Guide .................................................................................................................. 19 3.3.1. Running the Automated Scraper ...................................................................... 19 3.3.2. Running the Topic Miner .................................................................................... 20 3.3.3. Running the Backend Server ............................................................................. 20 3.3.4. Running Faculty Search from the UI ............................................................... 21 3.4. Example Use Cases: ................................................................................................... 21 3.4.1. Use Case 1 - Basic use to search faculties ................................................... 21 3.4.2. Use Case 2 - Find research interests of the faculty .................................... 21 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results ...................................................................................... 21 3.4.4. Use Case 4 - Connecting to faculty ................................................................. 21 4. Contributions .................................................................................................................... 22 4 Introduction The ExpertSearch system is a system to search faculties who are experts in certain research areas or topics from university websites crawled from the web. The goal of our project is to improve this ExpertSearch system in a few ways, including automating the scraping process, adding topic mining for finding faculty research topics, and improving the UI to give improved visualizations and query refinement options. 1. Functional Overview The improved ExpertSearch system enables the following functionalities: * Automatic scraping of websites to identify faculty directory webpages and non- directory webpages * Automatic scraping of the classified faculty directory webpages to further identify faculty bio webpages and non-bio webpages * Automatic scraping of faculty bio webpages to generate one bio document per faculty and adding to the compiled bios * Topic Mining from the compiled bios to extract research topics of the faculties * Display top-5 research topics associated with each retrieved faculty * Improved email extraction for each faculty * Refine search query using any of the topics from the displayed topic cloud * Prepopulate email content when clicked on a faculty's email address The Automated Scraper improves the faculty bio generation process from a vast collection of websites. The Topic Miner adds more structure to the unstructured faculty website data retrieved from a query. The enhanced UI enables succinct visualization of the structured faculty results and provides shortcuts for additional search filters and faculty connection. Together, these new features improve the utility of the ExpertSearch system to the user. 2. Implementation Details 2.1. Automated scraping process The Automated Scraper takes a set of known University websites and top 500 Alexa websites as input, performs a series of operations to classify the directory URLs and then to classify the faculty homepages. The automated scraper then 5 scrapes the classified faculty homepages to generate faculty bio documents and adds the bios to the collection. 2.1.1. Inputs: o University websites o Top-500 Alexa websites 2.1.2. Outputs: o Corpus of classified Faculty Directory URLs (classified_dir_urls.cor) o Corpus of classified Faculty Bio URLs (classified_faculty_urls.cor) o Documents of bios for each faculty generated by scraping the classified Faculty Bio URLs (e.g. 6530.txt) 2.1.3. Deliverables: o Automated Scraper (auto_scraper.py) o Data Handler (data_handler.py) o Scraper (scraper.py) o Text Classifier (text_classifier.py) 2.1.4. Component design / Code workflow: Automated Scraper (auto_scraper.py) The Automated Scraper module automates the whole flow of generating the faculty bios from the input mixture of ""positive"" and ""negative"" URLs in the following sequence of steps: o Uses the data handler (data_handler.py) to prepare a train and test set of Faculty Directory URLs o Uses the scraper (scraper.py) to scrape these URLs to prepare the train and test corpus o Uses the text classifier (text_classifier.py) to build and train a Doc2Vec model on the documents in the train corpus of directory contents o Uses the text classifier to predict the category of the test URLs as ""Directory"" or ""Non-Directory"" o Saves the classified directory URLs to a file (classified_dir_urls.cor) o Uses the data handler to prepare a train and test set of Faculty Bio URLs o Uses the scraper to scrape these URLs to prepare the train and test corpus 6 o Uses the text classifier to build and train a Doc2Vec model on the documents in the train corpus of faculty bios o Uses the text classifier to predict the category of the test URLs as ""Faculty"" or ""Non-Faculty"" o Saves the classified bio URLs to a file (classified_faculty_urls.cor) o Uses the scraper to scrape the faculty bios from the classified bio URLs o Generates one document per faculty bio (e.g. 6530.txt) and saves under ExpertSearch/data/compiled_bios The auto_scraper.py module is the entry point for the complete automatic scraping and bio generation task. The following figure (Fig. 1) shows the complete automation flow starting with the input websites till the bio generation completion. Fig. 1: Automation Control Flow / Module interactions Directory URL Classification First, let's explain the Directory URL Classification task with the help of the modules. 7 Data Handler (data_handler.py) The Data Handler module first takes the University websites and Alexa websites as input, mixes them and partitions them into test and train URLs. Then uses the scraper module to extract the URL contents into test and train corpus as shown in the figure below (Fig. 2). Fig. 2: Dataset Preparation for Directory URL Classification 8 Here's a detailed explanation of the approach used by the Data Handler module to prepare the URLs for the scraper. * Downloaded the known faculty directory URLs from the sign-up sheet for MP 2.1. These will serve as the ""positive"" examples. o About 900 URLs were obtained from the sign-up sheet data, which was partitioned into 500 for training and 400 for test data. * Collected top URLs from Alexa. These will serve as the ""negative"" examples. o Collected the global top-50 pages of Alexa. o Collected the top-50 pages for 14 different countries. Manually verified that the pages are in English. o This gave 750 ""negative"" URLs. * When the AutoScraper is launched, it invokes the DataHandler which mixes and partitions the above URLs into train and test URLs as follows: o Training URLs * Converts the MP 2.1 sign-up sheet from csv to a file containing only the directory URLs. Performs any cleanup as necessary and labels them as ""directory"". * Combines the top-50 Alexa URLs for 10 countries and labels them as ""alexa_dir"". Uses these 500 pages for training. * Mixes the 500 Faculty Directory training URLs with the 500 Alexa training URLs. Removes duplicates if any. This gives 734 URLs as the final training URLs. * The training URLs are saved in the file train_urls.cor. o Test URLs * Combines the top-50 Alexa URLs for 5 countries. Uses these 250 pages for testing. * Mixes the 400 Faculty Directory test URLs with the 250 Alexa test URLs. Remove duplicates if any. This gives 548 URLs as the final test URLs. * The test URLs are saved in the file test_urls.cor. Scraper (scraper.py) The Scraper module scrapes the contents from the above train and test URLs and prepares the train and test corpus for the classification task. The scraper does the following: * Gets the contents of each URL as text. * Performs clean-up of non-ascii characters from the content. 9 * Performs other clean-ups such as substituting newlines, tabs, multiple whitespaces into single whitespace. * Substitutes contents such as ""403 Forbidden"", ""404 Not found"", etc. with ""Error: Content Not Found"". * Writes contents of each training URL as a single line of space separated words to the training corpus (""train_dataset.cor""). * Similarly, writes contents of each test URL as a single line of space separated words to the test corpus (""test_dataset.cor""). Text Classifier (text_classifier.py) The Text Classifier module uses the train and test dataset from above step to classify the Faculty Directory URLs. The classification module does the following: * Uses gensim to build a Doc2Vec model for feature vector representation of each document. * Uses the train_dataset.cor to build the vocabulary and train the model. * Saves the model so that it can be reloaded while running next time on the same dataset. * Uses LogisticRegression as the classifier from scikit-learn module. * Uses LogisticRegression to predict the categories of the test URLs given the test dataset. Faculty URL classification Next, let's look at the Faculty URL Classification task. Data Handler (data_handler.py) The Data Handler module now takes the existing project's known Faculty Bio URLs and mixes with some Alexa URLs to prepare the train dataset. Uses the classified Faculty Directory URLs from the above step to extract potential Faculty Bio URLs to prepare the test dataset. Then uses the scraper module to extract the URL contents into bio test and bio train corpus as shown in the figure below (Fig. 3). 10 Fig. 3: Dataset Preparation for Faculty Bio URL Classification 11 The following approach was used to prepare the dataset: * Training URLs o Use the top 1000 URLs from the currently existing Faculty Bio URLs in the ExpertSearch project as the ""positive"" train URLs. o Use 250 URLs from the Alexa test URLs set as the ""negative"" train URLs. o Combine them. o Tag the faculty bio URLs as ""faculty"" and save to the file ""train_bio_urls.cor"". o Tag the Alexa URLs as ""alexa_faculty"" and save to the same file. o This will be the final file with all the train URLs. * Test URLs o Use the Classified Faculty Directory URLs obtained from the Directory URL Classification task above. o Use the scraper to find all potential faculty bio URLs from each of these Directory URLs. o Save all these potential faculty bio URLs to the file ""test_bio_urls.cor"". o This will be the final file with all the test URLs. Scraper (scraper.py) Since the ExpertSearch project already contains the faculty bios as documents, the contents of the top 1000 faculty bios (0.txt ... 999.txt) are copied to the train corpus file (""train_bio_dataset.cor""). Then the scraper does the following: * Scrapes the URLs from line no. 1000 till the end from the file train_bio_urls.cor and appends to the train corpus (""train_bio_dataset.cor""). * Scrapes the contents of the test URLs (i.e. potential faculty bio URLs) from the test_bio_urls.cor file above and adds those contents to the test corpus (""test_bio_dataset.cor""). Text Classifier (text_classifier.py) The classification module does the following: * Uses gensim to build a Doc2Vec model for feature vector representation of each document. * Uses the train_bio_dataset.cor to build the vocabulary and train the model. 12 * Saves the model so that it can be reloaded while running next time on the same dataset. * Uses LogisticRegression as the classifier from scikit-learn module. * Uses LogisticRegression to predict the categories (bio or non-bio) of the test URLs given the test dataset. Finally, the Scraper module scrapes these classified bio URLs and saves the contents of each bio URL to a new file under ExpertSearch/data/compiled_bios. 2.2. Topic Mining 2.2.1. Inputs: o Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) 2.2.2. Outputs: o Trained topic model (lda_mallet_model) o Bag-of-words representation of corpus to be used with miner.py (corpus_dictionary) o Text representation of corpus (lda_corpus) 2.2.3. Deliverables: o Python script to create topic model and retrieve top-10 terms associated with query topic (miner.py) 2.2.4. Component design / Code workflow: Topic Miner (miner.py) The topic miner pulls a topic distribution from a document already mined if the model was trained on the document, otherwise it uses gensim and mallet to create a model from the entire corpus. The process is described below: Corpus preparation * Read in compiled bios as strings * Filter the string representation of each bio to: o Remove stop words o Extract HTML tags and elements o Strip non-alphanumeric characters 13 o Strip numbers o Strip words that exist in lists of terms extracted from the bios o Strip words that exist in a manually defined list of words that were creating incoherent topic clusters (unwanted_words.txt) o Remove words shorter than four characters o Split all words into a list of tokens * Create list of documents which is comprised of lists of tokens for each document as described above * Append bigrams and trigrams to each token list for each document * Create a gensim dictionary from the above documents * Create a bag-of-word representation of our documents: this will be our corpus. Model creation Model creation required a good deal of manual work to ensure that the term clusters were understandable. The process consisted of a lot of trial and error, using the steps below (Fig. 4): * Create a general model with gensim.models.ldamodel.LdaModel class with 10 models * Visually inspect term clusters to ensure they were meaningful * If the above criteria were not satisfactory: o Tweak corpus construction * After the above criteria was deemed satisfactory: o Using gensim.models.wrappers.LdaMallet with the mallet library, I: a. Varied number of topics to create new model b. Assessed coherence of each model with varying number of topics c. Manually inspected output of models with high coherence, looking for term clusters that made intuitive sense and that appeared distinct given knowledge of the separate domains 14 d. Chose the best model according to above criteria and saved it and the created dictionary for query inference Fig. 4: Workflow for building an optimal topic model. Term extraction With a model and a dictionary, I wrote a method that can infer the topic of a given query and fetch the top-10 terms associated with that topic, a method that can infer the topic of a single document and fetch the top-10 terms associated with 15 that document's topic, and a method that can infer the topics of multiple documents and fetch the top-10 terms associated with each document's topic. These terms will eventually be pushed to the user to help them potentially refine their query as shown below (Fig. 5). Fig. 5: Extracting topics from documents not included in training set. 16 2.3. Improved Email Extraction 2.3.1. Inputs: o Generated Faculty Bio documents from the AutoScraper (e.g. 6530.txt) 2.3.2. Outputs: o Extracted emails for the faculties 2.3.3. Deliverables: o Updated email extractor (extract-email.py) 2.3.4. Component design / Code workflow: Regex Improvement: o There are certain edge cases that we had noticed in some of the email web pages where the format was different than the traditional email formatting o Added more regex matches in order to match with these edge cases such as ex. rohini[@]buffalo[DOT]edu 2.4. UI Improvements 2.4.1. Inputs: o Query terms in the search box o Topics mined by the topic miner 2.4.2. Outputs: o Updated UI showing top-5 research topic per faculty o Updated UI showing topic cloud o Clickable topic terms for query refinement o Prepopulated email template on-click email icon 2.4.3. Deliverables: o Updated server endpoints (server.py) o Updated UI (index.js) 17 2.4.4. Component design / Code workflow: Info Button: o Information button is created at the top of each of the retrieved faculty. o When the button is clicked there a table pops up that appears below the selected retrieved faculty o The table will contain additional information regarding the research topics that the faculty does o When one of the info buttons associated with a faculty is clicked the other one will close, and the new one will open. Top 5 Topics Display: o Display the top 5 topics from the preview for each of the faculty. o Display these topics in a table format when the information button is clicked o Underneath each topic is a 'Learn More' button which when clicked leads you to a page talking more about the topic in detail from the web. o Clicking on the ""Add to Query"" button will refine the current search query to include this topic. Email Automation: o Email comes pre-populated with a set subject and body. o The body talks about one of the research topics that were extracted from the top 5 topics and how the user would like to connect with the faculty regarding research in this topic. 3. Usage Details The modified project has been tested on Mac and Windows with Python 2.7. Here are the setup instructions for each of these platforms. 3.1. Setup Guide (Mac) 3.1.1. Repo setup Run the following command on a terminal to clone the github repository. 18 $ git clone https://github.com/sairanga123/CourseProject.git The directory structure of the project is as below (listing only the files/folders relevant to this project): CourseProject |__________ ExpertSearch |__________ AutoScraper | |__________ data | |__________ auto_scraper.py | |__________ data_handler.py | |__________ scraper.py | |__________ text_classifier.py | |__________ d2v.model | |__________ d2v-bio.model | |__________ data | |__________ compiled_bios | |__________ expertsearch | |__________ mallet-2.0.8 | |__________ model_files | |__________ corpus_dictionary | |__________ lda_mallet_model | |__________ lda_corpus | |__________ miner.py | |__________ extraction |__________ mallet-2.0.8 |__________ static |__________ server.py 3.1.2. Project environment setup The project has been tested on python 2.7. Please setup a python 2.7 environment for running the project. Creating an environment from Anaconda will make many common packages available. So a quick way to start would be to setup a python 2.7 environment from Anaconda. 19 May need to install many or all of the following python packages depending on what packages the python environment already has. - gunicorn=19.10.0 - flask=1.1.2 - metapy=0.2.13 - requests=2.25.0 - pytoml=0.1.21 - gensim=3.8.3 - nltk=3.4 - bs4=0.0.1 - lxml=4.6.2 - numpy=1.16.6 - sklearn=0.0 3.2. Setup Guide (Windows) Windows is currently not supported. If you want to build in Windows, use Windows Subsystem for Linux and follow the steps above. 3.3. Usage Guide 3.3.1. Running the Automated Scraper Run the AutoScraper to generate the bio documents. This step has been already performed and the generated bio documents have already been added to the ExpertSearch/data/compiled_bios folder. Here are the instructions for running the AutoScraper if it needs to be run again with additional input. To run the automated scraper, first go the ExpertSearch/AutoScraper directory. Then the Automated Scraper can be invoked as follows: 20 -d option specifies to generate/regenerate the train and test dataset. If -d switch is used: o If the dataset already exists, it will be regenerated o If the dataset doesn't yet exist, it will be generated If -d switch is not used: o If the dataset already exists, the existing dataset will be used in the subsequent flow o If the dataset doesn't yet exist, it will be generated even if -d switch is not used -t option specifies to train/retrain the Doc2Vec model on the train dataset. If -t switch is used: o If a trained and saved model already exists, the model will be retrained and saved again o If a trained and saved model doesn't yet exist, it will be trained and saved If -t switch is not used: o If a trained and saved model already exists, the saved model will be loaded and used for inference in the subsequent flow o If a trained and saved model doesn't yet exist, it will be trained and saved even if -t switch is not used 3.3.2. Running the Topic Miner The steps for creating the topic model are documented in ExpertSearch/data/expertsearch/LDATopicModeling.ipynb. The model construction is not something that can necessarily be automated because relying on perplexity and coherence scores alone often results in topics that don't make any meaningful sense to a human. Once the topic model is constructed and saved, miner.py allows the server to load the model and make inferences. 3.3.3. Running the Backend Server Once, the topic model has been built, we can start the server. To start the server, go to the ExpertSearch folder. Then run the following command: $ gunicorn server:app -b 127.0.0.1:8095 21 3.3.4. Running Faculty Search from the UI Now, launch a web browser and type the following URL: localhost:8095 3.4. Example Use Cases: 3.4.1. Use Case 1 - Basic use to search faculties Let's assume that we want to find the faculties that are working on ""text mining"". Then we'd go to the UI and enter our search string as ""text mining"". The existing system would retrieve the top ranked faculty results working on ""text mining"". 3.4.2. Use Case 2 - Find research interests of the faculty Now, the improved system will also provide an info button which will bring up an additional table of information for each faculty. This table shows the top 5 research topics the faculty is associated with. 3.4.3. Use Case 3 - Find faculties working on similar topics as the faculty from the initial search results We now maybe interested in learning more about who are the faculties that are working on any of these research topics. We can quickly search for all the faculties working on this new research topic by simply clicking on the ""Add to Query"" button for that research topic. This will automatically modify our search query by including that new research topic without having to type it in the search box. The retrieved faculty results will show the list of faculties working on that research topic. 3.4.4. Use Case 4 - Connecting to faculty Another way we could use the system is to click on the email icon to send an email to the faculty's email address. While we may be at a loss of words for that first email, the system will provide a pre-populated template email which will automatically address the faculty's name and also include reference to the faculty's research area. This will make connecting to an expert faculty just one click away. 22 4. Contributions Item Sub-items Contributor Automated Scraping * Automated Scraper to automate the complete process * Data Handler to prepare the datasets for the text classification tasks * Scraper to scrape the URLs * Text Classifier to classify directory and bio URLs * Function to generate bio documents and add to compiled bios Mriganka Sarma Topic Mining * Topic model * Function to return top-10 words associated with query topic Zacharia Rupp Improved Email Extraction * Added regular expressions to extract emails with atypical forms (e.g. person at place dot com) Sai Ranganathan Zacharia Rupp Improved UI * Display top 5 topics associated with each faculty member * Display cloud of topics * Pre-populate email field when clicked on email address * Improve email extraction part 1. Sai Ranganathan"
https://github.com/sairanga123/CourseProject	README.md	"Improved ExpertSearch Project Source Code The project's source code is available under the ExpertSearch folder. Project Documentation The project's detailed report is available as: ""Project Report - Improved ExpertSearch.pdf"" The project's tutorial presentation video is available as: ""Video Presentation - Improved ExpertSearch.mp4"" PLEASE READ ""Project Report - Improved ExpertSearch.pdf"" FOR THE REPORT PLEASE VIEW ""Video Presentation - Improved ExpertSearch.mp4"" FOR THE PRESENTATION AND DEMO Improved ExpertSearch Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team Captain: Sai Ranganathan (sr50) Mriganka Sarma (ms76) Zacharia Rupp (zrupp2) What system have you chosen? Which subtopic(s) under the system? We have chosen to improve ExpertSearch. Briefly describe the datasets, algorithms or techniques you plan to use Datasets: Faculty dataset scraped from MP2.1 for positive examples. Scrape of Alexa Top 500 Domains for negative examples. Techniques: Topic Mining If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Additional functionality: Topic Mining If it works, users will be able to refine queries by selecting topics from a topic cloud Impact on ndcg@k Improved functionality: Email extraction If it works, more faculty members will have email addresses associated with them. Improved UI: More granular query refinement Top-k associated topics listed under individual faculty members How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Our code will build on the ExpertSearch code by: adding a topic mining function improving email extraction automating scraping process improving UI Which programming language do you plan to use? Python JavaScript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main tasks: Automatic crawler to identify faculty directory pages (10+ hrs) Automatic crawler to identify faculty webpage URLS (10+ hrs) Improving functionality: Email extraction (10+ hrs) Adding functionality: Topic mining (10+ hrs) UI Improvements: Query refinement options (10+ hrs) Topic cloud from mined topics associated with retrieved faculty members. Top-5 topics associated with faculty member (5+ hrs) Displayed at the top of the bio excerpt Prepopulated email content when a user clicks on a faculty member's email address (5+ hrs) E.g. ""Dear Faculty Name, It's a pleasure to have gone through some of your research articles. I'd like to connect with you for discussing some ideas in the Research Area. I hope to hear from you soon."""
https://github.com/sairanga123/CourseProject	TIS Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members.  Team Captain: Sai Ranganathan (sr50)  Mriganka Sarma (ms76)  Zacharia Rupp (zrupp2) 2. What system have you chosen? Which subtopic(s) under the system?  We have chosen to improve ExpertSearch. 3. Briefly describe the datasets, algorithms or techniques you plan to use  Datasets:  Faculty dataset scraped from MP2.1 for positive examples.  Scrape of Alexa Top 500 Domains for negative examples.  Techniques:  Topic Mining 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better?  Additional functionality:  Topic Mining - If it works, users will be able to refine queries by selecting topics from a topic cloud o Impact on ndcg@k  Improved functionality:  Email extraction - If it works, more faculty members will have email addresses associated with them.  Improved UI:  More granular query refinement  Top-k associated topics listed under individual faculty members 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly  Our code will build on the ExpertSearch code by:  adding a topic mining function  improving email extraction  automating scraping process  improving UI 6. Which programming language do you plan to use?  Python  JavaScript 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task.  Main tasks:  Automatic crawler to identify faculty directory pages (10+ hrs)  Automatic crawler to identify faculty webpage URLS (10+ hrs)  Improving functionality: - Email extraction (10+ hrs)  Adding functionality: - Topic mining (10+ hrs)  UI Improvements: - Query refinement options (10+ hrs) o Topic cloud from mined topics associated with retrieved faculty members. - Top-5 topics associated with faculty member (5+ hrs) o Displayed at the top of the bio excerpt - Prepopulated email content when a user clicks on a faculty member's email address (5+ hrs) o E.g. ""Dear <Faculty Name>, It's a pleasure to have gone through some of your research articles. I'd like to connect with you for discussing some ideas in the <Research Area>. I hope to hear from you soon."""
https://github.com/sairanga123/CourseProject	Tutorial Presentation - Improved ExpertSearch.pptx	Improved Expert Search Team ZMS (Zacharia Rupp, Mriganka Sarma, Sai Ranganathan) Z M S Z M S Introduction What is ExpertSearch System The System As It Is Z M S The System As It Is Z M S Challenges of Current System Manual Process for Directory URL Identification Manual Process for Faculty Webpage Identification Need for Structuring of Faculty Information Need for Improved Extraction Methods Z M S Proposed Improvements Automatic Scraping of Faculty Directory and Faculty Webpages Mining Top Research Topics of Faculty Improved Email Extraction Improved UI for Better Visualization of Structured Data Z M S Setup Instructions Clone the project repository $ git clone https://github.com/sairanga123/CourseProject.git Create Anaconda python 2.7 environment Install python packages gensim nltk gunicorn bs4 metapy Z M S Usage Instructions Main Functional Categories: Automatic Scraping Topic Mining Faculty Search in UI Z M S Usage Instructions (contd.) Automatic Scraping $ cd ExpertSearch/AutoScraper $ python ./auto_scraper.py -d -t Z M S Usage Instructions (contd.) Topic Miner Work in jupyter notebook (ExpertSearch/data/expertsearch/LDATopicModeling.ipynb) Build corpus and dictionary, and create topic model Manually inspect topics with high coherence Check against known document Save best topic model, corpus, dictionary server.py accesses model with miner.py Z M S Usage Instructions (contd.) Email Extraction $ cd ExpertSearch/extraction $ python extract_email.py Different Email Formats To Cover: Eugene dot agichten at emory dot edu Rohini [@] buffalo [DOT] edu Z M S Usage Instructions (contd.) Running the Backend Server $ gunicorn server:app -b 127.0.0.1:8095 Running Faculty Search from the UI localhost:8095 Z M S Example Usage Z M S Example Usage (contd.) Z M S Example Usage (contd.) Z M S Thank You Z M S Z M S
https://github.com/rixu1/CourseProject	Final Project Proposal.pdf	Project Proposal Project Name: 2.2 ExpertSearch System Team member: Ri Xu (Captain), NetID: rixu2 Jinou Yang, NetID: jinouy2 The system we are choosing is Option2: Improving A System, specifically the ExpertSearch system. The main dataset we will be using is the faculty bio from MP2.3 . Our project will make improvements to the ExpertSerach system in the following ways: 1. Add extractions on experts' phone numbers using regex pattern matching. (5 hours ) 2. Add extractions on experts' research interests in the format of keywords using topic mining. (10 hours ) 3. Improve search results preview by adding phone numbers and keywords. (5 hours ) 4. Build experts recommendation system based on users' search queries and browsing history. Users' interaction data can be stored and retrieved from browser cookies, BM25 can be used to find the top relevant experts to recommend. (20 hours ) We will implement web scraping scripts to extract phone numbers in python and store them in a local file, which will be shown in the preview on frontend. Another python script is to perform topic mining on faculty bio and extract top research interest keywords and store them in a local file. Search related function and ranker will also be implemented in python. For web frontend improvements, HTML, CSS and Javascript will be used to achieve the objectives.
https://github.com/rixu1/CourseProject	Progress Report (As of Nov 29th).pdf	Progress Report (As of Nov 29th) Current Progress: Backend: We have finished writing up scripts to extract phone numbers and research interests all experts and dumped the result into files. Later we ran the script to recreate the new data file (metadata.dat) to include phone numbers and research interests for the frontend to display. Frontend: Refactored code for existing project to modularize some of the components for reusing. Integrate the phone number and research interests in expert preview cards. Remaining Tasks 1. Build recommendation system based on users' search history. Need to look into how to use cookies to store search info and recommend related experts using BM25. 2. Look into improving name recognition to reduce empty names in search results. 3. Look into improving email recognition to reduce empty email information. Challenges The biggest challenge with this project is getting familiar with the existing code base and iterating based on that. Also when extracting research interests for experts, some unrelated information like school names, locations may interfere with the results.
https://github.com/rixu1/CourseProject	README.md	"Software Usage Tutorial Presentation Video link: https://www.youtube.com/watch?v=qGx1IDdoyLw Documentation An overview of the function of the code The existing ExpertSearch system is a web application where users can search related experts. We've made several improvements to the existing system. To be more specific, the improvements include: - Show experts' areas of interests in search results preview. - Show experts' phone number in search results preview. - Improve name matching; increase name recognition ratio from 88% to 96%. This reduces the chance when users see empty expert names in their search results. - Build a simple recommendation system based on users' past search queries. Replace the empty home page with recommendation feed. How the software is implemented The web application is implemented in the following way: server.py This is the main Flask server file which contains all backend APIs and page handlers. - /search This API accepts a few parameters such as search query, number of results expected. Metapy library and BM25 is used to query the dataset, then fetch additional information from metadata.dat for frontend to display preview. - /recommend Similar to /search, this API accepts search query keywords and returns up to 5 recommended experts per query. BM25 is also used for finding experts to recommend. If more than 5 experts matched with a given query, 5 experts are randomly selected to encourage exploration. index.js This is the main javascript file which contains most of the frontend logic. - recommend() This function is called during window.onload. It reads users' past search queries from browser cookies and talks with the backend (/recommend endpoint) to fetch recommended experts for each query term. Then it will display the experts' preview for each search query. The latest search query will show up on the top of the feed. - doSearch() This function is called when users have clicked the search button. If the query is non-empty, it talks with the backend(/search endpoint) to fetch the results. It will also store users' search query in the browser cookies for the recommendation system to pick up. - docDiv() This function will return a html div object given all preview data. It will render the expert's name, phone number, email, areas of interest, university information etc. It is used by both recommend() and doSearch() when rendering queried experts. Material icons ( https://material.io/icons/) are used for visibility. - setCookie() and getCookie() These two helper functions are used for storing and retrieving cookies stored in the browser. In this case, users' search keywords are stored in the ""history"" field in the format of comma separated strings. extraction/extract_interest.py This script is used to generate experts' area of interests. Nltk, Gensim, SnowballStemmer are used for extraction. For each document, tokenization is performed followed by stemming and lemmatization. Then it uses nltk to run POS tagging on each token and extract all the noun words. Finally we run through all noun words with a predefined word mapping to compute the final areas of interests for all documents. Results are then exported to data/interests. extraction/extract_phone_number.py This script is used to extract experts' phone numbers. Regex pattern matching is used for phone-number extraction. Results are then exported to data/phone_numbers. extraction/extract_names.py & extract_names_spacy.py & merge_name.py These three scripts are used to improve experts' names extraction. - We improved the original script extract_names.py to use the latest version of stanford-ner. Results are exported to /data/names.txt - In extract_names_spacy.py, we used the spacy NLP framework to run through all documents and extracted named entities. Results are exported to /data/names_secondary.txt - Finally we use merge_name.py to combine names from /data/names.txt and /data/names_secondary.txt . If we cannot find a name using stanford-ner, we will check and use the result from the secondary file. With both methods combined, we're able to improve the name recognition coverage from 88% to 96%. Final results then exported to a new file new_names.txt. extraction/write_file_names.py This script combines all data files (interests/phone/email/names/...) generated by extraction scripts and writes to the dataset file metadata.dat for metapy to index and rank. How to install and run the software To run the software, simply clone the repository from Github. There are a few dependency packages required to install. Following are the commands to run: pip install metapy pip install gunicorn pip install spacy pip install nltk cd to /CourserProject gunicorn server:app -b 127.0.0.1:8095 Then you should be able to access http://localhost:8095/ from your browser. Chrome browser is recommended to use. Description of contribution of each team member Team member: Ri Xu Responsible changes related to Flask servers and frontend javascript. Complete the Project Progress report. Team member: Jinou Yang Responsible for extraction scripts development and improvements. Demo video. Documentation."
https://github.com/srivardhansajja/tangy	ProjectDocumentation.pdf	"Text Information Systems : CS410 Project Documentation ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Team O Navyaa Sanan (navyaas2) O Srivardhan Sajja (sajja3) : Team Captain Overview In our project, we were able to augment the ExpertSearch system by adding functionality which makes the system automatically crawl through faculty webpages given the primary university link/URL (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project has two main components. First, we implemented our own classifier, which given any URL uses text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we find all directory pages associated with that primary URL. We used the classifier built in part 1 for implementing part 2. Software Implementation Datasets: To train the extension, we used 800 manually labelled URLs. To test the extension, we used another 800 manually labelled URLs to check for accuracy. We were able to achieve 83.875% Accuracy, 89.28% precision, 77% recall and 82.68% F1-score. Algorithms/Techniques: To get the list of all URLs from a university website, we used spider from scrapy package. We implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit can be set manually. To make the classifier, we used classification techniques like using stop words and filter words. We also used statistical indicators like mean length of URLs and standard deviation of the training set URLs. Lastly, we used a dictionary to look at the most common words in positive training samples to help us build a classification model. Additionally, we would like mention that our extension is independent of the system. We consider this project to be an independent feature addition, which is inspired by the ExpertSearch System but does not directly rely on any preexisting code. We drew inspiration from MP2 and used the techniques taught to us in this course, but we did not have any direct reliance on any preexisting code whatsoever (aside from external Python packages). Software Implementation Details Crawler: To get the list of all URLs from a university website, we used spider from scrapy package. We implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit for the crawler can be set manually. Model Training: To train the model, we looked at used classification techniques like using stop words and filter words. We also used statistical indicators like mean length of URLs and standard deviation of the training set URLs. Lastly, we used a dictionary to look at the most common words in positive training samples to help us build a classification model. We have a list of words we ignore in model_train/crawler.py. Any URL that contains a word also contained in ignore is automatically pulled out of consideration. The rest of the training is done in model_train/trainer.py and model_train.py where we calculate the statistical indicators and return those URLs which have the same words contained as the most popular words from the positively labelled training data. To train the extension, we used 800 manually labelled URLs. To test the extension, we used another 800 manually labelled URLs to check for accuracy. We were able to achieve 83.875% Accuracy, 89.28% precision, 77% recall and 82.68% F1-score. Model Deploying: To deploy the model, most of the work is done in model_deploy /model_dep.py. There we use the model generated by us (model_deploy/ model_testing.json) and carry out the same classification process as we did to test our classification model. Flask App: The Flask app refers mostly to the frontend work we did. It connects the deployed model to the website we made and provides a framework for piping input and output. Installation and setup Clone repository on your local machine and enter the project directory: - git clone https://github.com/srivardhansajja/CourseProject.git - cd CourseProject/ Setup a virtual environment. Executing the following two lines in a terminal will set up an environment using venv within the project directory. - python 3 -m venv venv - source venv/bin/activate We have used Python3 for running and testing the project. Install all required packages by executing: - python3 -m pip install -r requirements.txt Change line 11 in crawler/crawler_handler.py to the python version that you are using. For example, python3.8, python3, python3.6 or py. The project is now set up for you to use and test. Execute - python3 main.py from the primary project directory and access the locally hosted flask website by visiting http://127.0.0.1:3000/ from a browser window. You can enter university domain names and the appropriate faculty directory URLs will be shown to you along with the crawling statistics. The program by default uses our pre-generated model. If you wish to update the parameters or tweak the model, go to model_train/trainer.py, make your required changes, and run - python3 model_train/model_train.py This will output your testing statistics, including accuracy, precision, recall and F1 score, and generate model_testing.json. Once you are satisfied with your changes, if you wish to use your model in the crawling process instead, replace model_testing.json in line 50 of model_train/model_train.py with model.json and rerun the above statement in your terminal. Be careful as this will replace our original model, and re-cloning the project is the only way to revert it, unless you make a backup of it. Project Structure * Source Code: o Crawler SS /crawler/crawler.py SS /crawler/crawler_handler.py o Model Training SS /model_train/trainer.py SS /model_train/model_train.py SS /model_train/train_data.txt SS /model_train/dev_data.txt o Model Deployment SS /model_deploy/model_dep.py SS /model_deploy/model.json o Flask App SS /main.py SS /templates/ SS /static/ * Documentation: o ProjectProposal.pdf o ProjectProgressReport.pdf o ProjectDocumentation.pdf * README.md o / Team contributions Throughout the course of this project, the team worked very closely and even though most of the tasks were divided by person both of us ended up working on everything in some capacity. Srivardhan focused more on implementing the web crawler, setting up the website, and making training/testing data sets. Navyaa focused mainly on doing research on URL classification, building the machine learning model, and integrating the classification model into the website."
https://github.com/srivardhansajja/tangy	ProjectProgressReport.pdf	"Text Information Systems : CS410 Project Progress Report ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Progress made thus far * Enlisted some common faculty directory URLs to give us a general sense of what they look like. This would later help us in identifying some common features. * Determined some common features of faculty directory URLs which act as starting points for the classifier we are writing. We took the main features of URLs like word staff or directory in them and started with these obvious features. We also identified words and features that would never be part of a faculty directory page URL. * Implemented a spider using the python scrapy package to recursively crawl through and identify all pages of a website with either 'faculty' or 'staff' in the URL, given the primary URL of the university (example: illinois.edu, berkeley.edu). Parameters such as time limit, page crawl limit, and results count limit can be set manually. * Added some preliminary filters within the spider to not allow URLs with certain keywords such as 'mail', 'publish', 'calendar', 'research', etc., to make crawling process more efficient. * Developed a website using the flask web framework to showcase model's results, and to amplify user experience. * Looked at some research done in similar areas (classification of URLs) but were not able to find much that is directly relevant. The main issue we face is figuring out common features of a general faculty directory page (length of the URL, important terms to look for etc.) Remaining tasks * Create a testing / training data set for training our model. * Finalizing some research papers which would be the inspiration for building our classifier. * Base our classifier off of what results we see in the training data set. * Fine tune our classifier to tell with greater confidence whether a website is a faculty directory URL or not. Issues and challenges faced * Tons of literature, scholarly articles and papers on the internet - there are tons of papers which seem relevant prima facie but turn out to be not as useful when we go into the details. This has made the task of searching for what we need for sure longer than we had anticipated. * Too many potential URLs associated with a primary URL. A primary URL like ""Illinois.edu"" has several valid URLs associated with it so we have to manually stop looking at valid URLs after a certain point. * Scraping websites with a lot of pages takes time so fine-tuning certain aspects becomes a very time-consuming task."
https://github.com/srivardhansajja/tangy	ProjectProposal.pdf	"Text Information Systems : CS410 Project Proposal ""2.2 ExpertSearch System"" -- Navyaa Sanan (navyaas2) Srivardhan Sajja (sajja3) Team  Navyaa Sanan (navyaas2)  Srivardhan Sajja (sajja3) : Team Captain System 2.2 ExpertSearch System: Automatically crawling faculty webpages We plan on augmenting the ExpertSearch system by adding functionality which would make the system automatically crawl through faculty webpages given the primary university link/url (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project will have two main components. First, we plan to implement our own classifier, which given any URL will use text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we will find all directory pages associated with that primary URL. We will be using the classifier built in part 1 for implementing part 2. Datasets, Algorithms and Techniques Datasets: To train the extension, we shall be using the starting half of urls list fro mthe Google Sheets spreadsheet of MP2.1 as positive examples, and automatically generated non-faculty directory pages of university websites as negative examples. Similarly, to test our extension, we shall be using the 2nd half of the urls in the spreadsheet for positive examples, and randomly generated real urls as negative examples. Algorithms/Techniques: To get the list of all urls from a university website, we shall use simple web crawling techniques using the built-in python packages but might dip into external packages if required. To make the classifier, we shall be using basic classification techniques and algorithms taught in this course. If we feel the need to use algorithms and techniques that are not mentioned in lectures, we will be sure to use techniques taught to us in the MPs. Overall, we will ensure that any algorithms used by us are directly or indirectly related to this course. Primary Function We are adding a function to automatically compile a list of all faculty directory pages from a given university home page url. This can be tested by comparing the generated list of urls with a list of manually compiled faculty directory page urls from different universities. The closer the pre- compiled list is to the generated one, the higher the accuracy of the web crawler and classifier is. Communication with the system Our extension is independent of the system. We consider this project to be an independent feature addition, which is inspired by the goal of this project but does not directly rely on any preexisting code. We will draw inspiration from MP2 and use the techniques taught to us in this course, but we will not have any direct reliance on any preexisting code whatsoever. Programming Language We plan to use Python3 for all developmental activities, including building the classifier and the web crawler. Work Justification Since there are 2 team members in this group, we planned this project to be 40 hours of work. Here is a breakdown of all tasks with the estimated time we expect each task to take: 1. Developing a classifier which, given a URL tells us if the URL is that of a faculty directory web page (23 hours) a. Subtask 1: Research and enlist at least 200 types of faculty directory URLs (2.5 hours) b. Subtask 2: Determine common/ identifying features of these faculty directory URLs (3 hours) c. Subtask 3: Look at research papers/scholarly texts that deal with similar classification problems and choose a classifier accordingly. Potentially write some pseudo code/ starter code for our classifier (7 hours) d. Subtask 4: Develop a training and testing data set and manually flag all websites we use. (2.5 hours) e. Subtask 5: Write the code of our classifier based on the reference. (5 hours) f. Subtask 6 : Based on the results, modify and fine tune our classifier. (3 hours) 2. Given a primary url (eg. Illinois.edu) identify all possible faculty directory pages. (17 hours) a. Subtask 1: Make a training testing data set of at least 10 primary URLs and as many directory URLS associated with the primary URL (2 hours) b. Subtask 2: Identify all the possible directory page associated URLs with our training/testing data set. (3 hours) c. Subtask 3: Read research papers / scholarly literature on how to tell whether a given URL is valid or not (4 hours) d. Subtask 4: Devise a method to find all valid URLs associated with the given primary URL. (5 hours) e. Subtask 5: Find a way to determine which subset of valid URLs are faculty directory pages. (should be just using the classifier that we built in Task 1 but since generating all possible combinations and testing out whether they are valid or not can take time, 3 hours to run the whole thing)"
https://github.com/srivardhansajja/tangy	README.md	University Faculty Directory Page Crawler This is the course project for CS 410 Text Information Systems course at the University of Illinois at Urbana Champaign. Usage tutorial: https://youtu.be/tN551flUyks Overview We were able to augment the ExpertSearch system by adding functionality which makes the system automatically crawl through faculty webpages given the primary university link/URL (illinois.edu, berkeley.edu, etc.), instead of having to explicitly identify them. Our project has two main components. First, we implemented our own classifier, which given any URL uses text classification techniques mentioned in this course to judge whether the given URL is that of a faculty directory page. Second, given a primary university link we find all directory pages associated with that primary URL. We used the classifier built in part 1 for implementing part 2. Team Srivardhan Sajja Navyaa Sanan Project Setup Instructions Clone repository on your local machine and enter the project directory: git clone https://github.com/srivardhansajja/CourseProject.git cd CourseProject/ Setup a virtual environment. Executing the following two lines in a terminal will set up an environment using venv within the project directory. python3 -m venv venv source venv/bin/activate We have used Python3 for running and testing the project. Install all required packages by executing: python3 -m pip install -r requirements.txt Change line 11 in crawler/crawler_handler.py to the python version that you are using. For example, python, python3.8, python3, python3.6 or py The project is now set up for you to use and test. Execute python3 main.py from the primary project directory and access the locally hosted flask website by visiting http://127.0.0.1:3000/ from a browser window. You can enter university domain names and the appropriate faculty directory urls will be shown to you along with the crawling statistics. The program by default uses our pre-generated model. If you wish to update the parameters or tweak the model, go to model_train/trainer.py, make your required changes, and run python3 model_train/model_train.py This will output your testing statistics, including accuracy, precision, recall and F1 score, and generate model_testing.json. Once you are satisfied with your changes, if you wish to use your model in the crawling process instead, replace model_testing.json in line 50 of model_train/model_train.py with model.json and rerun the above statement in your terminal. Be careful as this will replace our original model, and re-cloning the project is the only way to revert it, unless you make a backup of it Project Structure Source Code: Crawler /crawler/crawler.py /crawler/crawler_handler.py Model Training /model_train/trainer.py /model_train/model_train.py /model_train/train_data.txt /model_train/dev_data.txt Model Deployment /model_deploy/model_dep.py /model_deploy/model.json Flask App /main.py /templates/ /static/ Documentation: ProjectProposal.pdf ProjectProgressReport.pdf ProjectDocumentation.pdf README .md / Demo and Tutorial We made a tutorial for ease of use of the software, and is hosted at https://youtu.be/tN551flUyks Model Statistics These are the statistics of our generated model, based on training and testing data sets with 800 URLs each. Accuracy: 0.83875 F1-Score: 0.8268456375838926 Precision: 0.8927536231884058 Recall: 0.77
https://github.com/sjma3/CourseProject	Project Progress Report.pdf	1) Which tasks have been completed? As of now, little actual code has been written, but solid foundational knowledge has been established in improving this UI with past examples that are similar in scope/nature. In particular, the JavaScript plugin Infinite Scroll comes to the foreground as a solid candidate to work off of. However, it remains to be seen if Infinite Scroll can work feasibly in the actual implementation of this improvement. 2) Which tasks are pending? Feasibility has yet to be confirmed with Infinite Scroll as a plug-in for the UI implementation/improvement. However, if Infinite Scroll cannot be integrated with this program, a similarly functioning minimally-viable solution will be created or utilized/cited in the native language of the ExpertSearch system. After developing a solution it will be rigorously tested to ensure that it works for as many edge cases as the scope of this project allows before the final deadline. 3) Are you facing any challenges? As of now there are no particular challenges that are outside of the bounds of expectations for a project of this nature. If there are any in the future, they may be noted when the time comes.
https://github.com/sjma3/CourseProject	Project Proposal.pdf	"Improvement of the ExpertSearch System Group: Steven Ma (Captain) NetID: sjma3 This project qualifies under option 2, improving an existing system. For this project, I would like to add a function to the ExpertSearch System. This added function is auto-scroll of results when a user looks through the query. This means that when the user reaches the bottom of the page, the page will automatically populate new results onto the end of it, resulting in a much smoother experience than having to repeatedly click ""Load more"" for the results. The function will work if a user is able to enter their query and get a small batch of results, then when reaching the bottom of their first batch of results, get more results automatically. This will continue until no more results remain to be shown. In reality, we will be getting a comprehensive list of results and simply adding these results in chunks to the page for the user to see. I do not want the web page to pre-load all of them beforehand and have it hidden; that would be an unrefined and expensive implementation. Rather, I wish to add to the end of the page as the user needs. Further research into this will dictate whether such an approach is feasible or not. To implement such functionality successfully, I currently plan to use the same code as the original project, Python. Ideally, I would be able to work through the current repository and extract out the necessary files, make changes to them, and return the results. I believe this implementation will take five hours to search through/comprehensively understand the working code, five hours to research similar approaches like mine and determine what is and isn't feasible for the working library, another three or four hours to build out the functionality, and an additional six or seven hours to make tests and ensure proper functionality. Thus, this will take 20 hours to complete, in my reasonable belief."
https://github.com/sjma3/CourseProject	README.md	"Note that this project falls under 3.3, ""Adding an unlisted function to a listed system."" ""At the final stage of your project, you need to deliver the following: Your documented source code. Explain how your code communicates with or utilize the system. A demo that shows your implementation actually works."" ExpertSearch Setup (Copy-Paste from https://github.com/CS410Fall2020/ExpertSearch/) To run the code, run the following command from ExpertSearch (tested with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ Live Demo There will not be live demos scheduled for this project, unless a reviewer is absolutely unable to get the code running on their own. Instead, there is a video available to view with UIUC school credentials at https://drive.google.com/file/d/10HNOJZRPQ2lLqYDfIC213dDoGAChQEv7/view?usp=sharing. Please note the quality through video preview is quite low; downloading the video will result in higher quality results, though core functionality can still be verified at lower quality. It should be noted that the page that is shown in the first part is the current ExpertSearch website http://timan102.cs.illinois.edu/expertsearch/. The added functionality program is run locally through my personal computer. We can see how the new functionality allows users the convenience of not having to click the ""Load More"" button, while the program is simultaneously not burdensome by loading all the results at once. If reviewers want to personally run the code, they can simply download the repository and run it per the instructions above (may need to pip install some packages) to achieve/replicate the results. If issues occur, please message/email me as the author and we will get things sorted out. Please not that you should absolutely use Linux or MacOS, as gunicorn will not run in Windows, and install Python 2.7 (not any Python3 distributions). I personally used Ubuntu in developing this project. If, for any reason, one needs to access the ExpertSearch Github, it can be found at https://github.com/CS410Fall2020/ExpertSearch/ Explanation of how code utilizes system The code's primary functionality is in-built to the system, such that if pushed to the master project of ExpertSearch, it would integrate perfectly with no issues. The simple change made was to add a function that checked for if the user's screen was close to the bottom; if so, it would scroll automatically. One caveat of this approach is that it will not work until the user first clicks the ""Load More"" button. However, after contemplating, it was decided this was actually the correct functionality; if a user clicks load more, we can assume that only then they want to see more results. Perhaps some users may only wish to see the first five results and not be bombarded with an excessive number of results until they click the ""Load More"" button themselves."
https://github.com/skuretski/CourseProject	Course Project Progress Report.pdf	"Susan Kuretski skure2@illinois.edu CS410 - Fall 2020 Course Project Progress Report Overview This course project uses a dataset from Kaggle [https://www.kaggle.com/sayangoswami/reddit-memes-dataset] to perform optical character recognition on Reddit memes and do cluster analysis. The original proposal stated doing sentiment analysis, but as the reviewer suggested I have switched to cluster analysis via K-means since upvotes and downvotes are affected by multiple factors. I hope cluster analysis on these memes will uncover unlikely themes. Tasks Completed Tasks completed to this day are: * Download and clean data * Uploaded images to Google Cloud Platform storage * Performed optical character recognition with Google Cloud Vision and Translate to determine which language * Stored results in GCP Pending Tasks * Evaluation of OCR data * Processing data to do cluster analysis - make each meme into a term vector * Perform cluster analysis * Plot out clusters * Evaluation * Determining a ""good"" number of clusters via elbow method * Silhouette analysis to see how ""good"" the clusters are Challenges My biggest challenge is my unfamiliarity with K-means and unsupervised learning. I chose K-means since I don't have a ground truth or labelled dataset. I pondered halving the dataset of ~3,000 images to label it, but I don't see it as the best use of my time for this project. While K-means may seem straightforward for those experienced with it, I would imagine it will take some time for me to fine tune the number of clusters, perform evaluation, and plot the results out. As it stands now, I have spent about four hours doing the completed tasks. Another challenge is working with OCR text data which has originated from memes, which often is sarcastic, misspelled, and infers cultural or societal knowledge. There are multiple layers of ambiguity and room for error."
https://github.com/skuretski/CourseProject	Course Project Proposal.pdf	Susan Kuretski skure2@illinois.edu CS410 - Fall 2020 Course Project Proposal Team Members Team of one: Susan Kuretski (de facto captain) - skure2@illinois.edu Description - Option 5: Free Topic Task The objective of this project is to do sentiment analysis on images, also known as memes, from Reddit. Using Google Vision OCR, the characters from the image will be processed to tokenized strings/words, essentially this transforms the text from the meme into a bag of words. Using a probabilistic approach to sentiment analysis, I would ideally like to use naive Bayes with Laplace estimation to avoid the assignment of zero probabilities. I have never implemented an application using naive Bayes, so if this ends up being a rabbit hole, I may have to switch gears to a probabilistic semantic analysis (PLSA). Importance/Interesting In general, sentiment analysis is useful in determining users' feelings and attitudes towards certain items, whether it is a review, comment, or product. In regards to Reddit, it would be interesting to see which memes gather positive or negative sentiment and whether it correlates to upvotes or downvotes. Planned Approach (Tools, systems, datasets, evaluation) with Time Estimates Task Time Estimate 1. Get dataset from Kaggle here: https://www.kaggle.com/sayangoswami/reddit-memes-data set - 2. Do a data cleaning pass to ensure all URLs of images are seemingly correct in terms of structure, downvotes and upvotes are integers >= 0. 0.5 hour 3. The dataset has 3327 images to be downloaded and/or stored in the cloud. Iterate through each entry in the dataset to fetch the image and store. Discard invalid images. 3 hour 4. Run images through Google Vision OCR. https://cloud.google.com/vision/docs/ocr 3 hour 5. Evaluate accuracy 6. Store results of OCR. I want to minimize the need for repeat OCR processing since it can become expensive. 1 hour 7. Transform data from OCR to usable dataset for naive Bayes (bag of words, maybe try n-grams). 4 hour 8. Define and fit the model. Use scikit-learn for Python. 10 hour ** No previous experience with naive Bayes or scikit-learn 9. Evaluate model using scikit-learn metrics and comparing with upvote/downvotes. 4 hour ** No previous experience with evaluation of this 10. Attempt to make the model better (iterate steps 6 - 8). 10+ hours? Total: 35.5 hrs Expected Outcomes The expected outcomes are: 1. Have a final percentage of accuracy from using naive Bayes with hopes of it being greater than 50% 2. Implement improvements to accuracy if initial accuracy <= 60% 3. Possible factors affecting results: slang and intentional misspellings in memes, inaccuracy of OCR Programming Languages and Systems Python with scikit-learn AWS s3 storage or Google Cloud Storage Google Vision API
https://github.com/skuretski/CourseProject	README.md	"CS410 Fall 2020 Course Project Written by Susie Kuretski (skure2@illinois.edu) Video Link here YouTube (https://www.youtube.com/watch?v=kLedDGQIZyA) The best way to contact me is via Slack in #cs-410-text-info-syst @Susie Kuretski You can also contact me at skuretski@gmail.com or skure2@illinois.edu (not as quick). Topic Utilizing Google Vision's optical character recognition to perform K-means cluster analysis on Reddit memes Overview From this Kaggle dataset, which includes data for ~3300 Reddit memes, I extracted the images and uploaded them to a Google Cloud Platform storage bucket. Then, I ran these images through optical character recognition and translation using Google Vision and Translate. Link to API docs here. Once this was done, the results were stored in a GCP bucket as JSON data. An example would be: The starting image The JSON result after OCR { ""src_lang"": ""en"", ""text"": ""When you don't study for a test\nand get all the answers right\nSo this is the power of Ultra Instinct?\n"", ""file_name"": ""85805u.jpg"", ""id"": ""85805u"" } The goal after this was to perform K-means cluster analysis to discover groups or common themes in the memes. Step-by-Step Details For steps 3-7, I used code from Kaggle - Use DFoly1 1. Preprocessing the Data Once the Kaggle set was downloaded, I did a quick check on the data to make sure each row had an ID and link to an image with Tableau Prep. I did not find any rows without these properties. Then, I downloaded all the images to my local machine and uploaded them to Google Cloud Platform storage. Because of the way I uploaded them, I had to fix the directories with script.sh. This was a minor setback which took a couple hours to complete. 2. Performing Optical Character Recognition Once the images were in storage, I wrote a Python script to perform OCR and translation -- OcrProcessing.py. I decided to do translation as well so that I could filter out non-English text. Out of the 3327 images, 3031 met the criteria of non-null English text. Some of the memes were just images with no text in them, or they had another language as their primary text. 3. Cleaning Text Data After doing OCR, it was important to do some regular expressions to clean up things like: - contractions - newlines and whitespace - numbers - non-alphabetical characters - commonly found slang or misspellings e.g. shes -> she is or ur -> you are 4. Stop Words and Stemming For stop words and stemming, I used Natural Language Toolkit (NLTK) for Python. I did add some additional stop words based on my findings with the memes, like ""meme"" and ""rdankmemes"" which was a Reddit tag. For stemming, I used a Porter stemmer since it is relatively fast and works well with the English language. The Porter stemmer is: ""Based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes."" 1 5. TF-IDF Vectorization Once stop words and stemming was done, I transformed the text from the memes into a TF-IDF vector using sklearn. This method has many options like n-gram range, maximum or minimum document frequency, maximum features, smoothing, and using sublinear term frequency. I tried different variations of TF-IDF vectorization to see how that would affect K-means clustering. With this specific dataset, the maximum number of features seemed to be optimal around 1000-1500. Anything beyond this would cause the clustering to have a lot of outliers, resulting in imbalanced and poorly grouped clusters. I also tried the sublinear term frequency option, but that caused some irregularities as well, similar to increasing number of features. 6. Principal Component Analysis Before doing K-means, I did do PCA to reduce dimensionality in the TF-IDF vector. If we look at the TF-IDF vector, the X axis is the meme text and the Y axis is one of the 1500 features or terms. Where [X,Y] meet is the term frequency. With 1500 features and 3031 meme text data segments, it's useful to construct a new feature subspace to reduce the risk of overfitting because the data is too generalized. 7. K-means K-means is an algorithm very similar to EM algorithm where there is an assignment-like phase and then a maximize phase. In K-means, we first initialize cluster centroids randomly. Then, repeat this until convergence: - For every data point, assign to nearest centroid via Euclidean distance - Move the centroids to the center of data points assigned to it For K-means clustering, I did do multiple runs with 2-6 clusters. I set maximum iterations to 600, but generally, it converged in < 100 iterations. With sklearn, it can do multiple randomized initializations in order to find the best possible local maxima, which may or may not be the global maxima. The default is 10, but I tried different ranges which usually didn't have much variance. 8. What is a good number of clusters? I used 2 methods in determining what might a good number of clusters look like: 1. Elbow method 2. Silhouette analysis The Elbow method is a heuristic approach in which the number of clusters is plotted against the function of variance. A ""good"" number is where the curve has a definitive bend, resembling the shape of a human arm with an elbow. Generally, 3 clusters provided the best elbow. However, with some different variations of the TF-IDF vector and K-means, the bend was not explicit and was actually a smooth curve, which is one of the drawbacks of using the Elbow method. The Silhouette analysis measures the separation distance between clusters. The range of silhouette analysis can range from -1 to 1. - A value of 1 suggests the sample is far away from neighboring clusters. - A value of zero suggests the sample is very close to the boundary between two clusters. - A negative value suggests the sample may have been assigned to the wrong cluster. My results usually had values of > 0.75 for n clusters of 2-3, while it dropped off to < 0.5 for > 5 clusters. I did not observe any negative or zero values. I used code from scikit learn to do my silhouette analysis. With the Elbow method and silhouette analysis, it seemed that 3 was a good number of clusters for this data. 9. Evaluation and Results Overall, 3 clusters seemed to be the magic number based on evaluation. Selecting features > 1500 seemed to be detrimental to clustering where the clusters were very skewed and had many outliers, even with PCA dimensionality reduction. The 3 clusters had these top words: 1. will, people, now, see, man, know, time 2. win, boi, years, body, entire, million, master 3. pm, likes retweets, trump, follow, donald, will While some themes were not extremely clear like in cluster 1 and cluster 2, the third cluster was quite clear in terms of having a social media vibe. Other top words in this third cluster were realdonaldtrump, elonmusk, and replying. For improvements, it might be useful to try different stemming methods and adding more stopwords like ""will"" or ""us."" But here is where ambiguity comes into play. Without looking at each meme individually, it's hard to tell if will was in the context of an auxillary verb like ""will travel,"" or a noun like a legal document. The same goes for ""us."" Does this mean us, like the group of us, or US like the United States? It would also be interesting to see bi-grams of this. When I did the TF-IDF vectorization, I stuck with unigrams since I just wanted to use bag of words representation before getting ahead of myself. Overall, this project has been a great learning experience in terms of working with real data, using Google Vision, seeing how K-means works especially after doing EM algorithm work, and evaluation of clusters. Despite deviating from the original plan of sentiment analysis, I did get the general outcomes I wanted with cluster analysis. It would have been nice to see more clusters or more clearly defined feature words, but I think that would have come with more refinement. How to Run Anaconda If you're interested in setting this up yourself, some test data is provided. Here is what my environment looks like: - Anaconda v4.9.2 - Download here - Python v3.7.9 - Anaconda environment file here - OS: Windows Subsystem Linux 18.04 Ubuntu (optional) Git clone the repository or download the ZIP. Navigate to the directory where it is saved. With the environment.yml file, change the prefix to where your Anaconda environments are stored. For me, it is /home/skuretski/anaconda3/envs/cs410. So for you, it might be /your/directoryToAnacondaEnv/anacondaVersion/envs/cs410 Run command conda env create -f environment.yml Run conda activate cs410 Run jupyter notebook Navigate to whatever URL the jupyter notebook command logged. It is usually something like http://localhost:8888/?token=someStringHere Navigate to KMeans.ipynb from the localhost page. I've included a directory called test_data which includes some resulting OCR JSON files locally. It is not all of them, but will give you a sense of how the code works. Make sure the first cell is selected and then hit Run. Continue this in sequence. Without Anaconda Without Anaconda is possible, however, you will have to globally install some dependencies. - Python v3.7.9 - Matplotlib v3.3.2 - Numpy v1.19.2 - Pandas v1.1.3 - Seaborn v0.11.0 - NLTK v3.5 - scikit-learn v0.23.2 - scipy v1.5.2 - Wordcloud v1.8.1 - Jupyter Notebook This project is powered by Python, those listed libraries, and Jupyter notebook."
https://github.com/JackDeDobb/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/oboffil/CourseProject	Documentation.pdf	"Coordinator: Omar N Boffil NetID: Oboffil2 Oboffil2@illinois.edu Group Name: Oboffil Topic: Option 2.1 MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Metapy and its toolkit are among the best-known libraries used in python for Text mining and another usability. Still, this library is not compatible will all python versions, making it challenging to create an environment where you can use its power to build good code. In this documentation, I will give you all the steps and code to create an environment using anaconda to install Metapy and other useful libraries that you can combine with Metapy to make your code more efficient. This process will use one of the most recent python versions that will benefit users who like to work in more updated environments and let you use newer python packages and combine them with Metapy. We will do this process as simple as possible and avoiding complicated code and frustration. This documentation will work for Windows and macOS operator systems. Also, we will use anaconda GUI to make it as simple as possible. The code to install Metapy and its useful toolkit will be included in an anaconda file and a text file with all the libraries' needs. To ensure that this software works and created the right result, make sure to read the code's comment, and follow the steps described below. This will guarantee the success of this environment for Metapy. 1) Installation process: First, we will need to install anaconda if you don't have it yet. (In case you have it installed already on your computer, you can move to step 2). Otherwise, you will need to: 1.1) Go to https://docs.anaconda.com/anaconda/install/ 1.2) Select your operator system for Windows or macOS 1.3) Fallow the instruction on anaconda website to complete the installation by using the GUI or the terminal on your computer After you have installed anaconda in your computer, we can move on and create an environment that works with Metapy 2) Creating an environment with Python 3.7. Now that anaconda software is installed on your computer. You can open it and fallows: 2.1) Click on ""environment"" 2.2) Click on ""Create"" to create a new environment where we are going to work 2.3) Add a name to your environment, select Python version 3.7, and click on ""create"". This will take a few seconds, and it will create a python environment where you can start running code 3)- Getting the source code and resources: Now that we created the environment with Python 3.7 we can implement the code that will let us use the different libraries, but first, we need to download the source code and packages from https://github.com/oboffil/CourseProject.git Download the download resources.zip from the repository above and unzip it to get the folder that contains the source code (Implementation.ipynb), and the text file (package-list.txt) with the Toolkit packages that will help you to set up Metapy and other useful libraries. 4)- Implementing the source code and packages. With all tolls needed, we can complete the installation of Metapy in our environment: 4.1) Go back to anaconda and click on ""Home"" 4.2) On ""Application on"" select the environment that you just created. This will take a few seconds 4.3) Install and launch Jupyter notebook. If you haven't installed it yet, it will take a few seconds; otherwise, it will launch automatically. 4.3) Look for the directory where you saved the unzipped download resource folder mentioned in step 3 and click on Implementation.ipynb. 4.4) This contains the source code to install the libraries and packages for this tutorial. Read the comment of the source code and make sure you delete the comments before running each line. NOTE* if you get an error trying to run the code directly from the file, go back to the directory, create a new Python 3 and copy and paste the code into the newly created file. Just make sure this new file is in the same directory as the package-list.txt. This will install Metapy and some of their useful toolkit in the environment. After the installation is completed, you can import the libraries in any other user interface that you prefer in anaconda as long as you use the environment where you installed the packages and libraries initially. For example, you could import these libraries on JupyterLab, Qt Console, Jupyter Notebook, etc."
https://github.com/oboffil/CourseProject	Project Progress.pdf	Coordinator: Omar N Boffil Oboffil2@illinois.edu Group Name: Oboffil Topic: MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Project Progress Which tasks have been completed? The environment test that can handle Metapy and the toolkits is done. Also, the code with the libraries need is complete. Other codes to import the libraries and install the different tools are completed as well. 2) Which tasks are pending? I need to complete the documentation and create the tutorial video to guide the user to develop these libraries' in an anaconda environment. 3) Are you facing any challenges? Yes, I haven't found a way to run the installation of anaconda and the environment in just one run, also creating the environment by code has some issues. I'm thinking of using the GUI to implement this step and install anaconda directly from their website and then use the code to install the different packages.
https://github.com/oboffil/CourseProject	Project proposal.pdf	Coordinator: Omar N Boffil NetID: Oboffil2 Oboffil2@illinois.edu Group Name: Oboffil Topic: Option 2.1 MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability The Metapy installation and usability took me more than 4 hours to figure out how to install a compatible version of python that I can use to complete the assignment in this course. My goal with this project is to create a system and a tutorial for installing and using the tool on different platforms that will help future students complete this installation and used the toolkits without feeling the same frustration I felt at the beginning of this class. I will show how to set up an environment in the Windows and macOS operation system using anaconda and the latest Python versions to complete the different tasks and integrate the other popular toolkits. Also, I will create a system in python that will contain all necessaries steps to install this environment and the different packages need it
https://github.com/oboffil/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Coordinator: Omar N Boffil Oboffil2@illinois.edu Group Name: Oboffil Topic: MeTa Toolkit Project Topic: Enhance MeTA and Metapy usability Tutorial Video: https://youtu.be/m9CEqIl3ADg
https://github.com/oboffil/CourseProject	Tutorial video Link.docx	https://youtu.be/m9CEqIl3ADg
https://github.com/oboffil/CourseProject	Tutorial video Link.pdf	https://youtu.be/m9CEqIl3ADg
https://github.com/pshreyareddy/CourseProject	CS410FinalProjectDocumentation.pdf	"1 PROJECT DOCUMENTATION REPORT CS 410 TEXT INFORMATION SYSTEMS FALL 2020 TEXT CLASSIFICATION COMPETITION Twitter Sarcasm Detection Team Member Email Shreya Reddy Peesary peesary2@illinois.edu 2 INTRODUCTION Recognizing sarcasm in text is an important task for Natural Language processing to avoid misinterpretation of sarcastic statements as literal statements. The use of sarcasm is prevalent across all social media, micro-blogging and e-commerce platforms. Sarcasm detection is imperative for accurate sentiment analysis and opinion mining. It could contribute to enhanced automated feedback systems in the context of customer-based sites. Twitter is a micro-blogging platform extensively used by people to express thoughts, reviews, discussions on current events and convey information in the form of short texts. Twitter data provides a diverse corpus for sentences which implicitly contain sarcasm. The aim of this project is to classify the tweets in the given dataset as SARCASM or NOT_SARCASM and beat the base line score of F1: 0.723. DATASET FORMAT Train.jsonl: Shape: 5000 rows x 3 columns Train dataset is balanced with 2500 SARCASM and 2500 NOT_SARCASM samples. Test.jsonl: Shape: 1800 rows x 3 columns Column Definitions: * response : The Tweet to be classified * context : the conversation context of the response . The context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. * label : SARCASM or NOT_SARCASM * id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , 3 you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). SYSTEM DESIGN * The code for this project was done using Google Collaboratory (Using GPU Run Time Type). * The source code can be directly run from Collaboratory using the link by executing the cells step by step from the jupyter notebook link below: https://colab.research.google.com/github/pshreyareddy/CourseProject/blob/main/FinalSu bmissionForBERT.ipynb 1. Click on open in collab button in FinalSubmissionForBERT.ipynb 2. Sign in to collab 3. Go to Runtime -- Change Runtime Type -- Select Hardware Accelerator as GPU PACKAGES USED * Python 3.6.9 * Pandas: Python data analysis library. * Numpy: Python library for working with arrays. * Re: This is used for regular expression matching. * String: For common string operations * Sklearn: Python machine learning library * TensorFlow2.1.0: an open source software library for high performance numerical computation * Keras: a deep learning API written in Python, running on top of the machine learning platform TensorFlow. * TensorFlow Hub: a repository of trained machine learning models. DATA PREPROCESSING: * Converted the context column into comma separated string. * Removed @USER from response and context columns * Removed <URL> from response and context columns. * Removed digits (0-9) * Removed special characters ,white space characters and allowing only alphanumeric characters. 4 * Converted the response and context columns to lowercase. BERT MODEL: In order to beat the base line score used state of the art language models in BERT (Bidirectional Encoder Representations from Transformers). BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT's goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata: 1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence. 2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences. 3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence. PROCESS : * Added a Keras Bert layer using Bert uncased L-12_H-768_A-12. 5 * Tokenization approach using Bert full tokenizer followed as per standard usage in tensor flow hub (https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1). * Define an Encode function to separate text into tokens, masks and segments * Split the train data set into training and validation * Apply the encode function on both response and context. 6 * Build Model with inputs as an array of context and response input word ids, mask and segment ids and output layer is a simple neural network with 1 neuron and activation function : sigmoid which is used for classification. * Hyperparameters: Optimizer : Adam learning rate : 1e-6 , Loss : Binary Cross Entropy 7 * Model layout : * Training the model : train_input (Array of train_context and train_reponse with in-turn contains input word ids,mask and segment ids ) val_input (Array of val_context and val_reponse with in-turn contains input word ids,mask and segment ids ) used epochs = 3 and batch_size = 3 Process takes around 30-40 min in Collaboratory using GPU. * Predictions: Predictions are saved in answer.txt with id and target label. 8 RESULTS: Got an F1 score of 0.7402464065708418 using this approach beating the base line score. OTHER APPROACHES : * Same Bert process with response only input without context.(F1score: 0.6905005107252298 * Simple LSTM with Stanford Glove embeddings (F1 score : 0.6645126548196015) 9 REFERENCES: * Documentation of BERT on TFHub * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding * https://www.analyticsvidhya.com/blog/2020/10/simple-text-multi-classification-task- using-keras-bert/ * https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision- and-recall-for-a-keras-model 10"
https://github.com/pshreyareddy/CourseProject	Project Proposal.pdf	PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Shreya Reddy Peesary (NetID : peesary2) (1-person team) 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods I have some basic hands-on and working knowledge on artificial neural networks namely CNN,RNN,LSTM,GRUs using TensorFlow and Keras. I'm willing to learn more recent state of the art techniques like Google Research's BERT etc and experiment with these techniques. 4. Which programming language do you plan to use? Python
https://github.com/pshreyareddy/CourseProject	ProjectProgressReport.pdf	PROJECT PROGRESS REPORT CS 410 Text Information Systems Fall 2020 Completed Tasks * Became familiar with BERT and its usage for text classification problems. * Worked on cleaning of tweets by removing urls , emojis, punctuations, special characters etc. which may not help in classification. * Implemented an approach using BERT in google collab using Tensor Flow. * Fine-tuned the model and was able to beat the baseline score. Pending Tasks * Source code refactoring, optimization and final submission. * Documentation. * Tutorial presentation. Challenges/Issues No current issues. Initially, faced issues with setup and computing power with my current laptop to use tensor flow for running BERT. So, started working on google collab for coding and running and was successful in making submissions beating baseline score.
https://github.com/pshreyareddy/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities Project Final Documentation Software Project Demo Video Link Text Classification Competition: Twitter Sarcasm Detection (https://github.com/CS410Fall2020/ClassificationCompetition) Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/savigovindarajan/CourseProject	kdd06-patann.pdf	"Generating Semantic Annotations for Frequent Patterns with Context Analysis Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Urbana,IL 61801 { qmei2, dongxin, hcheng3, hanj, czhai }@uiuc.edu ABSTRACT As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Re- search in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important next step - interpreting the discovered frequent patterns. Although some recent work has studied the com- pression and summarization of frequent patterns, the pro- posed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns. In this paper, we propose the novel problem of generat- ing semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such an annotation for a frequent pattern by con- structing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has po- tentially many applications such as generating a dictionary- like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on differ- ent datasets show that our approach is effective in generating semantic pattern annotations. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications - Data Mining General Terms: Algorithms Keywords: frequent pattern, pattern annotation, pattern context, pattern semantic analysis 1. INTRODUCTION With its broad applications such as association rule min- ing [2], correlation analysis [4], classification [6], and cluster- ing [19], discovering frequent patterns from large databases has been a central research topic in data mining for years. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'06, August 20-23, 2006, Philadelphia, Pennsylvania, USA. Copyright 2006 ACM 1-59593-339-5/06/0008 ...$5.00. Various techniques have been developed for mining frequent item sets [2, 8], sequential patterns [3], graph patterns [22], etc. These techniques can usually output a large, complete set of frequent patterns efficiently, and provide basic statis- tic information such as support for each pattern. However, the excessive volume of the output pattern set and the lack of context information has made it difficult to interpret and explore the patterns. In most cases, a user only wants to explore a small set of most interesting patterns, and before exploring them, to have a rough idea about their hidden meanings or why they are interesting. This is analogous to literature reviewing. Before deciding whether to read through a paper, a reader often wants to first look at a short summary of the main ideas of the paper. Similarly, it is also highly desirable to have such a summary for a fre- quent pattern to explain or indicate the potential meanings of the pattern and to help a user decide whether and how to explore the pattern. Therefore, a new major challenge in frequent pattern mining has been raised by researchers, which is how to present and interpret the patterns discov- ered, in order to support the exploration and analysis of individual patterns. To meet this challenge and facilitate pattern interpretation, we need to annotate each frequent pattern with semantically enriched, in-depth descriptions of the pattern and its associated context. Researchers have employed concepts like closed frequent pattern [15], and maximum frequent pattern [16] to shrink the size of output patterns and provide more information beyond ""support"". Recently, novel methods have been pro- posed either to mine a compressed set of frequent patterns [20] or to summarize a large set of patterns with the most representative ones [21]. Both of them employ extra infor- mation of frequent patterns beyond the simple information of support, which is either transaction coverage [20] or pat- tern profiles [21]. These methods can successfully reduce the number of output patterns and present only the most inter- esting ones to the user. However, the information that these methods use to annotate a frequent patten is restricted to the morphological information or simple statistics (e.g. sup- port, transaction coverage, profile); from such an annota- tion, users could not infer the semantics, or hidden meanings of the pattern, thus still have to look through all the data transactions in which a pattern occurs in order to figure out whether it is worth exploring. In this paper, we study the problem of automatically gen- erating semantic annotations for frequent patterns, by which we mean to extract and provide concise and in-depth infor- mation for a frequent pattern, which indicates the semantics, or hidden meanings, of the pattern. What is an appropriate semantic annotation for a frequent pattern? Generally, the hidden meaning of a pattern can be inferred from the patterns with similar meanings, the data objects co-occurring with it, and the transactions in which the pattern appears. In principle, we expect such an annotation to be compact, well structured, and indicative of the meanings of the pattern. This criterion is analogous to dictionary entries, which annotate each term with structured semantic information. Example 1: An example of a dictionary entry1 Dictionary Term: ""pattern"" [""paet@n], noun, ... definitions: 1) a form or model proposed for imitation 2) a natural or chance configuration 3) ... example sentences: 1)... a dressmaker's pattern... 2)... the pattern of events ... synonym or thesaurus: model, archetype, design, exemplar, motif, etc In Example 1, we see that in a typical dictionary entry, the annotation for a term is structured as follows. First, some basic non-semantic information is presented. Second, a group of definitions are given, which suggests the semantics of the term, followed by several example sentences, which show the usage of this term in context. Besides, a set of synonyms, thesaurus or semantically similar terms are pre- sented, which have similar definitions with this term. Analogically, if we can extract similar types of semantic in- formation for a frequent pattern and provide such structured annotations to a user, it will be very helpful for him/her to interpret the meanings of the pattern and further ex- plore it. Given a frequent pattern, it is trivial to generate non-semantic information such as basic statistics and mor- phological information, so the main challenge is to generate the semantic descriptions of a pattern, which is the goal of our work. First, we should ideally provide precise semantic definitions for a pattern like those in a dictionary. Unfortu- nately, this is not practical without expertise of the domain. Thus we opt to look for information that can indicate the semantics of a frequent pattern, which presumably can help a user infer the precise semantics. Our idea is inspired from natural language processing, where the semantics of a word can be inferred from its context, and words sharing similar contexts tend to be semantically similar [13]. Specifically, by defining and analyzing the context of a pattern, we can find strong context indicators and use them to represent the meanings of a pattern. Second, we also want to extract the data transactions that best represent the meanings of the pattern, which is analogical to the example sentences. Finally, semantically similar patterns (SSPs) of the given pattern, i.e., patterns with similar contexts as the original pattern, can be extracted and presented. This is similar to the synonyms or thesauri of a term in dictionary. There- fore, an example of semantic pattern annotation (SPA) can be shown as follows: Example 2: An example of annotating a frequent pattern 1The example is selected from Merriam-Webster's Collegiate Dictionary & Thesaurus Pattern: ""frequent pattern"" sequential pattern; support = 0.1%; closed context indicators: ""mining"", ""constraint"", ""Apriori"", ""FP-growth"" ""rakesh agrawal"", ""jiawei han"", ... example transactions: 1)mining frequent patterns without candidate... 2)... mining closed frequent graph patterns semantically similar patterns: ""frequent sequential pattern"", ""graph pattern"" ""maximum pattern"", ""frequent close pattern"", ... The term ""frequent pattern"" in this example is itself a fre- quent itemset, or a frequent sequential pattern in text. This dictionary-like annotation provides semantic information re- lated to ""frequent pattern"", consisting of its strongest con- text indicators, the most representative data transactions, and the most semantically similar patterns. Despite its importance, to the best of our knowledge, the semantic annotation of frequent patterns has not been well addressed in existing work. In this work, we define the novel problem of generating semantic annotations for fre- quent patterns. We propose a general approach to auto- matically generate structured annotations as shown in Ex- ample 2, by: 1) formally defining and modeling the context of a pattern; 2) weighting context indicators based on their strength to indicate pattern semantics; and 3) ranking trans- actions and semantically similar patterns based on context similarity analysis. Empirical experiments on three different datasets show that our algorithm is effective for generating semantic pattern annotations and can be applied to various real world tasks. The semantic annotations generated by our algorithm have potentially many other applications, such as ranking pat- terns, categorizing and clustering patterns with semantics, and summarizing databases. Applications of the proposed pattern context model and semantical analysis method are also not limited to pattern annotation; other example appli- cations include pattern compression, transaction clustering, pattern relations discovery, and pattern synonym discovery. The rest of the paper is organized as follows. In Section 2, we formally define the problem of semantic pattern annota- tion and a series of its associated problems. In Section 3, we introduce how the pattern context is modeled and instanti- ated. Pattern semantic analysis and annotation generation is presented in Section 4. We discuss our experiments and results in Section 5, the related work in Section 6, and our conclusions in Section 7, respectively. 2. PROBLEM FORMULATION In this section, we formally define the problem of semantic pattern annotation (SPA). Let D = {t1, t2, ..., tn} be a database containing a set of transactions ti, which can be itemsets, sequences, or graphs, etc. Let pa be a pattern (e.g., an itemset, a subsequence, or a subgraph) in D and PD = {p1, ..., pl} be the set of all such patterns. We denote the set of transactions in which pa appears as Da = {ti|pa  ti, ti  D}. Definition 1 (Frequent Pattern): A pattern pa is fre- quent in a dataset D, if |Da| |D| >= s, where s is a user-specified threshold and |Da| |D| is called the support of pa, usually de- noted as s(a). Definition 2 (Context Unit): Given a dataset D and the set of frequent patterns PD, a context unit is a basic ob- ject in D which carries semantic information and co-occurs with at least one pa  PD in at least one transaction ti  D. The set of all such context units satisfying this definition is denoted as UD. With this general definition, a context unit can be an item, a pattern, or a transaction in practice, depending on the specific task and data. Definition 3 (Pattern Context): Given a dataset D and a frequent pattern pa  PD, the context of pa, denoted as c(a), is represented by a selected set of context units Ua  UD such that every u  Ua co-occurs with pa. Each selected context unit u is also called a context indicator of pa, associated with a strength weight w(u, a), which measures how well it indicates the semantics of pa. The following is an example of the context for an item- set pattern in a small dataset with only two transactions. The possible context units for this dataset are single items, itemsets and transactions, and the context indicators of the itemset pattern are selected from the context units appear- ing with it in the same transactions. Example 3: An example of pattern context Transactions: t1 = {diaper, milk, baby carriage, baby lotion, ... } t2 = {digital camera, memory disk, printer, ... } Context Units: 1) items: diaper, milk, printer, ... 2) patterns: {diaper, baby lotion}, ... 3) transactions: t1, t2, ... An exemplary frequent pattern (s = 0.5) p = {diaper, milk} Context indicators of p: diaper, baby carriage, {milk, baby lotion}, t1, ... With the definitions above, we now define the concept of semantic annotation for a frequent pattern and the related 3 subproblems. Definition 4 (Semantic Annotation): Let pa be a frequent pattern in a dataset D, Ua be the set of context indicators of pa, and P be a set of patterns in D. A semantic annotation of pa consists of: 1) a set of context indicators of pa, Ia  Ua, s.t. u  Ia and u'  Ua - Ia, w(u', a) <= w(u, a); 2) a set of transactions Ta  Da, s.t.t  Ta and t'  Da - Ta, t is more similar to c(a) than t' under some similarity measure; and 3) a set of patterns P '  P s.t. p  P ' and p'  P - P ', c(p) is closer to c(a) than c(p'). Definition 5 (Context Modeling): Given a dataset D and a set of possible context units U, the problem of Context Modeling is to select a subset of context units U, define a strength measure w(*, a) for context indicators, and construct a model of c(a) for each given pattern pa. Definition 6 (Transaction Extraction): Given a dataset D, the problem of Transaction Extraction is to define a sim- ilarity measure sim(*, c(*)) between a transaction and a pat- tern context, and to extract a set of k transactions Ta  Da for frequent pattern pa, s.t.t  Ta and t'  Da - Ta, sim(t', c(a)) <= sim(t, c(a)). Definition 7 (Semantically Similar Pattern (SSP) Extraction): Given a dataset D and a set of candidate pat- terns Pc, the problem of Semantically Similar Pattern (SSP) Extraction is to define a similarity measure sim(c(*), c(*)) be- tween the contexts of two patterns, and to extract a set of k patterns P '  Pc for any frequent pattern pa, s.t. p  P ' and p'  Pc -P ', sim(c(p'), c(a)) <= sim(c(p), c(a)), where c(a) is the context of pa. With the definitions above, we may define the task of Semantic Pattern Annotation (SPA) as to: 1) select context units and design a strength weight for each unit to model the contexts of frequent patterns; 2) design similarity measures for the contexts of two pat- terns, and for a transaction and a pattern context; 3) for a given frequent pattern, extract the most significant context indicators, representative transactions and semanti- cally similar patterns to construct a structured annotation. This problem is challenging in various aspects. First, we do not have prior knowledge on how to model the context of a pattern or select context units when the complete set of possible context units is huge. Second, it is not immediately clear how to analyze pattern semantics, thus the design of the strength weighting function and similarity measure is nontrivial. Finally, since no training data is available, the annotation must be generated in a completely unsupervised way. These challenges, however, also indicate a great advan- tage of the SPA techniques we will propose - they do not depend on any domain knowledge about the dataset or the patterns. In the following two sections, we present our approaches for modeling the context of a frequent pattern and annotat- ing patterns through semantic context analysis. 3. MODELING PATTERN CONTEXTS In this section, we discuss how to model pattern contexts through selecting appropriate context units and defining ap- propriate strength weights. Given a dataset D and a set of frequent patterns PD, our goal is to select a set of context units which carry semantic information and can discriminate the meanings of the frequent patterns. The discriminating power of each context unit will be captured by its strength weights. Vector Space Model (VSM) [17] is commonly used in nat- ural language processing and information retrieval to model the content of a text. For example, in information retrieval, a document and a query are both represented as term vec- tors, where each term is a basic concept (i.e., word, phrase), and each element of the vector corresponds to a term weight reflecting the importance of the term. The similarity be- tween documents and queries can thus be measured by the distance between the two vectors in the vector space. For the purpose of semantic modeling, we represent a transac- tion and the context of a frequent pattern both as vectors of context units. We select VSM because it makes no assump- tion on the vector dimensions and gives the most flexibility to the selection of dimensions and weights. Formally, the context of a frequent pattern is modeled as follows. Context Modeling: Given a dataset D, a selected set of context units {u1, ..., um}, we represent the context c(a) of a frequent pattern pa as a vector <w1, w2, ..., wm> , where wi = w(ui, a) and w(*, a) is a weighting function. A transaction t is represented as a vector <v1, v2, ..., vm> , where vi = 1 iff ui  t, otherwise vi = 0. The two key issues in a VSM are to select the vector dimensions and to assign weights for each dimension [17]. Specifically, the effectiveness of context modeling is highly dependent on how to select context units and design the strength weights. Actually, due to the generality of VSM, the proposed vector-space pattern context model is quite general and covers different strategies for context unit selec- tion and weighing functions. In the following subsections, we first discuss the generality of the context model, and then discuss specific solutions for the two issues respectively. 3.1 The Generality of Context Modeling Some existing work has explored non-morphological infor- mation of frequent patterns with some concepts related to the ""pattern context"" defined above. We now show that the notion of ""pattern context"" is more general and can cover those concepts as special cases. In [21], Yan et al. introduced the profile of an itemset for summarizing itemset patterns, which is represented as a Bernoulli Distribution Vector. In fact, this ""profile"" of a fre- quent itemset a can be written as a vector <w(o1, a), w(o2, a), ..., w(od, a)>  over all the single items {oi} in D. Here w(oi, a) =  tj Da ti j |Da| , where ti j = 1 if oi  tj and 0 oth- erwise. This shows that this ""profile"" is actually a special instance of the context model as we defined, where single items are selected as context units. Xin and others proposed a distance measure for mining compressed frequent-pattern sets, which is computed based on the transaction coverage of two patterns [20]. Inter- estingly, the ""transaction coverage"" is also a specific in- stance of ""pattern context"". Given a frequent pattern pa, the transaction coverage of pa can be written as a vector <w(t1, a), w(t2, a), ..., w(tk, a)>  over all the transactions {ti} in D, where each transaction is selected as a context unit, and w(ti, a) = 1 if pa  ti and 0 otherwise. Covering the concepts in existing work as specific instances, the pattern context model we proposed is general and has quite a few benefits. First, it does not assume pattern types. The pattern profile proposed in [21] assumes that both trans- actions and patterns are itemsets, thus does not work for other patterns such as sequential patterns and graph pat- terns. Second, the pattern context modeling allows different granularity of context units and different weighting strate- gies. In many cases, single items are not informative in terms of carrying semantic information (e.g., single nucleotides in DNA sequences), and the semantic information carried by a full transaction is too complex and noisy (e.g., a text document). The context modeling we introduced bridges this gap by allowing various granularity of semantic units, and allows the user to explore the pattern semantics at the level that corresponds to their beliefs. Furthermore, this model is adaptive to different strength weighting strategies for context units, where the user's prior knowledge about the dataset and patterns can be easily plugged in. 3.2 Context Unit Selection With the general definition presented in Section 2, the selection of context units is quite flexible. In principle, any object in the database that carries semantic information or serves to discriminate patterns semantically can be a context unit, thus context units can be single items, transactions, patterns, or any group of items/patterns, depending on the characteristics of the task and data. Without losing generality, in our work we assume a pat- tern is the minimal units which carries semantic information in a dataset, and thus select the context units as patterns. All kinds of units can be considered as patterns with a spe- cific granularity. For example, in a sequence database, every single item can be viewed as a sequential pattern of length 1, and every transaction can be viewed as a sequential pattern which is identical to the transactional sequence. The choos- ing of patterns as context units is task dependent, and can usually be optimized with prior knowledge about the task and the data. For example, we can use words as context units in a text database, and in a graph database, we pre- fer subgraph patterns to be context units, since single items (i.e., vertices and edges) are noninformative. This general strategy gives much freedom to select context units. However, selecting patterns of various granularity may cause the redundancy of context because these patterns are highly redundant. As discussed in previous sections, we expect the context units not only to carry semantic informa- tion but also to be as discriminative as possible to indicate the meanings of a pattern. However, when various granu- larity of patterns are selected as context units, some units will become less discriminative, and more severely, some be- comes redundant. For example, when the pattern ""mining subgraph"" is added as a context unit, the discriminating power of other units like ""mining frequent subgraph"" and ""subgraph"" would be weakened. This is because the trans- actions containing the pattern ""mining subgraph"" always contain ""subgraph"", and likely also contain ""mining frequent subgraph"", which means that these patterns are highly de- pendent and not discriminative to indicate the semantics of the frequent patterns co-occurring with them. This re- dundancy also brings a lot of unnecessary dimensions into the context vector space where the dimensionality is already very high. This redundancy in dimensions will affect both the efficiency and accuracy of distance computation between two vectors, which is essential for SPA. In our work, we ex- amine different techniques to remove the redundancy of con- text units without losing the semantic discriminating power. 3.2.1 Redundancy Removal: Existing Techniques One may first think of using existing techniques such as pattern summarization and dimension reduction to remove the redundancy of context units. While the context units can be any patterns in principle, we are practically not interested in those with very low fre- quency in the databases. Therefore, the context units we initially include are frequent patterns. There exist meth- ods for summarizing frequent patterns with k representative patterns [21], but they only work for itemset patterns and are not general enough for our purpose. Some techniques such as LSI [5] have been developed to reduce the dimensionality in high dimensional spaces, espe- cially for text data. However, these techniques aim to mit- igate the sparseness of data vectors by reducing the dimen- sionality, and are not tuned for removing the ""redundant"" dimensions. This is because all these dimensionality reduc- tion techniques consider that each dimension is ""important"" and the information it carries will always be preserved, or propagated into the new space. This is, however, different from our goal of redundancy removal. For example, if d1 and d2 correspond to the patterns ""AB"" and ""ABC"" re- spectively, and if we consider d2 to be redundant w.r.t d1, we do not expect the information of d2 to be preserved after the removal of d2. 3.2.2 Redundancy Removal: Closed Frequent Pat- tern Since neither the pattern summarization nor the dimen- sionality reduction technique is directly applicable to our problem, we examine alternative strategies. Noticing that the redundancy of context units is likely to be caused by the inclusion of both a frequent pattern and its sub patterns, we explore closed frequent patterns [15] and maximum frequent patterns [16] to solve this problem. A maximal frequent pattern is a frequent pattern which does not have a frequent super-pattern. It is easy to show that maximum frequent pattern is not appropriate for this problem since it may lose important discriminative units. For example, the frequent pattern ""data cube"", although not a maximum frequent pattern, indicates different semantics from the frequent pattern ""prediction data cube"", and thus should not be removed. Definition 8 (Closed Frequent Pattern): A frequent pattern pa is closed if and only if there exists no super- pattern pb of pa, s.t. Da = Db. We assume that a context unit is not redundant only if it is a closed pattern. This assumption is reasonable because pa  PD, if pa is not closed, there is always another fre- quent pattern pb  PD, where pa  pb and ti  D, we have pa  ti = pb  ti. This indicates that we can use pb as a representative of pa and pb without losing any se- mantic discriminating power. Therefore, in our work we use closed frequent patterns as our initial set of context units. The algorithms for mining different kinds of closed frequent patterns can be found in [15, 23]. 3.2.3 Redundancy Removal: Microclustering However, as stated in [21], a small disturbance within the transactions may result in hundreds of subpatterns that could have different supports, which cannot be pruned by closed frequent pattern mining. Those subpatterns are usu- ally with supports only slightly different from that of the master pattern. Therefore, their discriminating power for the semantics of the frequent patterns is very weak when their master patterns are also included as a context unit. We present clustering methods to further remove redundancy from the closed frequent patterns. Microclustering is usually employed as a preprocessing step to group data points from presumably the same cluster to reduce the number of data points. In our work, we first introduce a distance measure between two frequent patterns and then introduce two microclutering algorithms to further group the close frequent patterns. Definition 9 (Jaccard Distance): Let pa and pb as two frequent patterns. The Jaccard Distance between pa and pb is computed as: D(pa, pb) = 1 - |Da  Db| |Da  Db| Jaccard Distance [10] is commonly applied to cluster data based on their co-occurrence in transactions. Our need is to group the patterns that tend to appear in the same transac- tions, which is well captured by Jaccard Distance. Jaccard Distance has also been applied to pattern clustering in [20]. With Jaccard Distance, we expect to extract clusters such that the distances between inner-cluster units are bounded. We present two microclustering algorithms as follows: In the Hierarchical Microclustering method presented as Algorithm 1, we iteratively group two clusters of patterns with the smallest distance, where the distance between two Algorithm 1 Hierarchical Microclustering Input: Transaction dataset D, A set of n closed frequent patterns, P = {p1, ..., pn} Threshold of distance, g Output: A set of patterns, P' = {p' 1, ..., p' k} 1: initialize n clusters Ci, each as a closed frequent pattern; 2: compute the Jaccard Distance dij among {p1, ..., pn}; 3: set the current minimal distance d = min(dij); 4: while (d < g) 5: select dst where (s, t) = argmini,jdij; 6: merge clusters Cs and Ct into a new cluster Cu; 7: foreach Cv = Cu 8: compute duv = max(dab) where pa  Cu, pb  Cv; 9: foreach Cu; 10: foreach pa  Cu; 11: compute -da = avg(dab) where pb  Cu; 12: add pa into P', where a = argmini( -di); 13: return Algorithm 2 One-pass Microclustering Input: Transaction dataset D, A set of n closed frequent patterns, P = {p1, ..., pn} Threshold of distance, g Output: A set of patterns, P' = {p' 1, ..., p' k} 1: initialize 0 clusters; 2: compute the Jaccard Distance dij among {p1, ..., pn}; 3: foreach (pa  P) 4: foreach cluster Cu 5: ~da,u = max(dab) where pb  Cu; 6: v = argminu( ~da,u); 7: if( ~da,v < g) 8: assign pa to Cv 9: else 10: initialize a new cluster C = {pa} 11: foreach Cu; 12: foreach pa  Cu; 13: compute -da = avg(dab) where pb  Cu; 14: add pa into P', where a = argmini( -di); 15: return clusters are defined as the Jaccard distance between the far- thest patterns in the two clusters. The algorithm termi- nates when the minimal distance between clusters becomes larger than a user-specified threshold g. The second algo- rithm, which we call One-Pass Microclustering, iteratively assigns a closed frequent pattern pa to its nearest cluster if the distance is below g, where the distance between pa and a cluster C is defined as the Jaccard distance between pa and its farthest pattern in C. Both algorithms give us a set of microclusters of closed frequent patterns. They both guarantee that the distance between any pair of patterns in the same cluster is below g. Only the medoid of each cluster is selected as a context unit. By varying g, a user can select context units with various levels of discriminating power of pattern semantics. It is clear that Algorithm 2 only passes the pattern set once and thus is more efficient than the hierarchical algorithm, at the expense that the quality of clusters depends on the order of patterns. The performance of these two methods are compared in Section 5. 3.3 Strength Weighting for Context Units Once the context units are selected, the remaining task is to assign a weight to each dimension of the context model, which represents how strong the context unit corresponding to this dimension indicates the meaning of a given pattern. Intuitively, the strongest context indicators for a pattern pa should be those units that frequently co-occur with pa but infrequently co-occur with others. Practically, many types of weighting functions can be used to measure the strength of a context indicator. For example, we can assign the weight for a context indicator u for pa as the number of transactions with both u and pa. However, in principle, a good weighting function is expected to satisfy several constraints: Given a set of context indicator U and a frequent pattern pa, a strength weighting function w(*, pa) is good if ui  U 1. w(ui, pa) <= w(pa, pa): the best semantic indicator of pa is itself; 2. w(ui, pa) = w(pa, ui): two patterns are equally strong to indicate the meanings of each other; 3. w(ui, pa) = 0 if the appearance of ui and pa is inde- pendent: ui cannot indicate the semantics of pa. An obvious choice is co-occurrences, which however, may not be a good measure. One one hand, it does not satisfy constraints 3. On the other hand, we want to penalize the context units that are globally common patterns in the col- lection. Which means, although they may co-occur many times with pa, it may still not be a good context indica- tor for pa because it also co-occurs frequently with others. In general, the context units that are strongly correlated to pa should be weighted higher. In our work, we introduce a more principled measure. Mutual Information (MI) is widely used to measure the mutual independency of two random variables in informa- tion theory, which intuitively measures how much informa- tion a random variable tells about the other. The definition of mutual information is given as Definition 10: (Mutual Information). Given two fre- quent patterns pa and pb, let X = {0, 1} and Y = {0, 1} be two random variables for the appearance of pa and pb respectively. Mutual information I(X; Y ) is computed as: I(X; Y ) =  xX  yY P(x, y)log P(x, y) P(x)P(y) where P(x = 1, y = 1) = |DaDb| |D| , P(x = 0, y = 1) = |Db|-|DaDb| |D| , P(x = 1, y = 0) = |Da|-|DaDb| |D| , and P(x = 0, y = 0) = |D|-|DaDb| |D| . In our experiments, we use stan- dard Laplace smoothing to avoid zero probability. It can be easily proved that Mutual Information satisfies all the three constraints and favors the strongly correlated units. In our work, we use mutual information to model the indicative strength of the context units selected. Given a set of patterns as candidate context units, we ap- ply closeness testing and microclustering to remove redun- dant units from this initial set. We then use mutual infor- mation as the weighting function for each indicator selected. Given a frequent pattern, we apply semantic analysis with its context model and generate annotations for this pattern, as discussed in the following section. 4. SEMANTIC ANALYSIS AND PATTERN ANNOTATION Let U = {u1, u2, ..., uk} be a selected set of k context units and w(*, pa) be the unit weighting function w.r.t. any frequent pattern pa, i.e. I(*; pa). The context model, or con- text vector c(a) for pa is <w(u1, pa), w(u2, pa), ..., w(uk, pa)> . As introduced in Section 1, we make the assumption that the frequent patterns are semantically similar if their con- texts are similar to each other. In our work, we analyze the semantics of frequent patterns by comparing their context models. Formally, Definition 11 (Semantical Similarity): Let pa, pb, pd be three frequent patterns in P and c(a), c(b), c(d)  Vk be their context models. Let sim(c(*), c(*)) : Vk xVk -- R+ be a similarity function of two context vectors. If sim(c(a), c(b)) > sim(c(a), c(d)), we say that pb is semantically more sim- ilar to pa than pd w.r.t. sim(c(*), c(*)). Cosine is widely used to compute the similarity between two vectors, and is well explored in information retrieval to measure the relevance between a document and a query if both are represented with a vector space model [17]. In our work, we use cosine similarity of two context vectors to mea- sure the semantic similarity of two corresponding frequent patterns. Formally, the cosine similarity of two context vec- tors is computed as sim(c(a), c(b)) = k i=1 ai * bi k i=1 a2 i * k i=1 b2 i where c(a) = <a1, a2, ..., ak>  and c(b) = <b1, b2, ..., bk> . With the context model and the semantical similarity measure, we now discuss how to generate semantic anno- tations for frequent patterns. 4.1 Extracting Strongest Context Indicators Let pa be a frequent pattern and c(a) be its context model, which is defined in this work as a context vector <w1, w2, ..., wk>  over a set of context units U = {u1, u2, ..., uk}. As defined in Section 2, wi is a weight for ui which tells how well ui indicates the semantics of pa. Therefore, the goal of extracting strongest context indicators is to extract a sub- set of k' context units Ua  U such that ui  Ua and uj  U - Ua, we have wi >= wj. With a strength weighting function w(*, pa), e.g., mutual information as introduced in Section 3, we compute wi = w(ui, pa), rank ui  U with wi in descending order and select the top k' ui's. 4.2 Extracting Representative Transactions Let pa be a frequent pattern, c(a) be its context model, and D = {t1, ...tl} be a set of transactions, our goal is to select kt transactions Ta  D with a similarity function s(*, pa), s.t. t  Ta and t'  D - Ta, s(t, pa) >= s(t', pa). To achieve this, we first represent a transaction as a vec- tor in the same vector space as the context model of the frequent pattern pa, i.e., over {u1, u2, ..., uk}. Then, we use the cosine similarity presented in Section 3 to compute the similarity between a transaction t and the context of pa. The rest is again a ranking problem. Formally, let c(t) = <w' 1, w' 2, ..., w' k>  where w' i = 1 if ui  t and w' i = 0 otherwise. We compute sim(c(t), c(a)) for each t  Ta, rank them in descending order and select the top kt t's. 4.3 Extracting Semantically Similar Patterns Let pa be a frequent pattern, c(a) be its context model, and Pc = {p1, ..., pc} be a set of frequent patterns which are believed to be good candidates for annotating the se- mantics of pa, i.e., as synonyms, thesauri, or more generally as SSPs. Our goal is to extract a subset of kc patterns P ' c  Pc whose contexts are most similar to pa. Formally, let {c(p1), ..., c(pc)} be the context vectors for {p1, ..., pc}. We compute sim(c(pi), c(a)) for each pi  Pc, rank them in descending order, and select the top kc pi's. Note that the candidate SSP set for annotation is quite flexible. It can be the whole set of frequent patterns in D, or a user-specified set of patterns based on his prior knowledge. It can be a set of homogenous patterns with pa, or a set of heterogenous patterns. For example, it can be a set of pat- terns or terminology from the domain that a user is familiar with, and is used to annotate patterns from an unfamiliar domain. This brings great flexibility to apply the general SPA techniques to different tasks. By exploring different types of candidate SSPs, we can find quite a few interest- ing applications of semantic pattern annotation, which are discussed in Section 5. 5. EXPERIMENTS AND RESULTS In this section, we present experiment results on three different datasets to show the effectiveness of the semantic pattern annotation technique for various real-world tasks. 5.1 DBLP Dataset The first dataset we use is a subset of the DBLP dataset2. It contains papers from the proceedings of 12 major con- ferences in Database and Data Mining. Each transaction consists of two parts, the authors and the title of the cor- responding paper. We consider two types of patterns: (1) frequent co-authorship, each of which is a frequent itemset of authors and (2) frequent title terms, each of which is a frequent sequential pattern of the title words. The goal of experiments on this dataset is to show the effectiveness of the SPA to generate a dictionary-like annotation for frequent patterns. Our experiments are designed as follows: 1) Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most repre- sentative titles from their publications, and the co-authors or title patterns which are most semantically similar to them. Note that the most representative titles do not necessarily mean their most influential work, but rather the titles which best distinguish their work from others' work. 2) Given a set of title terms (sequential patterns), anno- tate each of them with their strongest context indicators, the most representative titles, the most similar terms, and the most representative author/co-authors. Note again that the most representative author/co-authors are not necessar- ily the most well-known ones, but rather the authors who are most strongly correlated to the topics (terms). In both experiments, we use the tools FP-Close [7] and CloSpan [23] to generate closed frequent itemsets of co- authors and closed sequential patterns of title terms respec- tively. The title words are stemmed by Krovertz stemmer [12], which converts the morphological variations of each English word to its root form. We set the minimum sup- port for frequent itemset as 10 and sequential patterns as 4, 2http://www.informatik.uni-trier.de/~ley/db/ which outputs 9926 closed sequential patterns. We use the One-Pass microclustering algorithm discussed in Section 3 to remove redundancy from those sequential patterns and get a smaller set of 3443 patterns, with g = 0.9 (the average Jaccard distance between these patterns is > 0.95). Medoids Cluster Members mine data, mine, data mine mine associate rule, associate, associate rule, mine rule rule mine associate, mine associate rule mine stream mine data, mine stream, data stream, mine data stream Table 1: Effectiveness of Microclustering Table 1 shows the medoids and cluster members of three microclusters generated by the One-Pass microclustering al- gorithm discussed in Section 3, all of which begin with the term ""mine"". We see that different variations of the same concept are grouped into the same cluster, although all of them are closed patterns. This successfully reduces the pat- tern redundancy. It is interesting to see that the pattern ""data mine"" and ""mine data"" are assigned to different clus- ters, which cannot be achieved by the existing pattern sum- marization techniques such as [21]. The results generated by hierarchical microclustering are similar. In Table 2, we selectively show the results of semantic pattern annotations. We see that the SPA system can auto- matically generate dictionary-like annotations for different kinds of frequent patterns. For frequent itemsets like co- authorship or single authors, the strongest context indica- tors are usually their other co-authors and discriminative ti- tle terms that appear in their work. The semantically similar patterns extracted also reflect the authors and terms related to their work. However, these SSPs may not even co-occur with the given pattern in a paper. For example, the pattern ""jiayong wang"", ""jiong yang&philip s yu&wei wang"" actu- ally do not co-occur with the pattern ""xifeng yan&jiawei han"", but are extracted because their contexts are similar. For a single author, whose context is usually more diverse, the SSPs are more likely to be title terms instead of authors. We also present the annotations generated for title terms, which are frequent sequential patterns. Their strongest con- text indicators are usually the authors who tend to write them in the titles of their papers, or the terms that tend to co-appear with them. Their SSPs usually provide inter- esting concepts or descriptive terms which are close to their meanings, e.g. ""information retrieval - information filter"", ""xquery - complex language, function query language"". In both scenarios, the representative transactions extracted give us the titles of papers that well capture the meaning of the given patterns. We only show the title words in Table 2 for each transaction. These experiments show that the SPA can generate dic- tionary like annotations for frequent patterns effectively. In the following two experiments, we quantitatively evaluate the performance of SPA, by applying it to two interesting tasks. 5.2 Matching Motifs and GO Terms A challenging and promising research topic in computa- tional biology is to predict the functions for newly discovered protein motifs, which are conserved amino acid sequence patterns characterizing the function of proteins. To solve this problem, researchers have studied how to match Gene Pattern Type Annotations xifeng yan I graph; philip s yu; mine close; mine close frequent; index approach; graph pattern; sequential pattern jiawei han T gspan graph-base substructure pattern mine T mine close relational graph connect constraint (SSP set = T clospan mine close sequential pattern large database co-author patterns) S jiawei han&philip s yu; jian pei&jiawei han; jianyong wang; jiong yang&philip s yu&wei wang I spiros papadimitriou; fast; use fractal; graph; use correlate; christos faloutso T multiattribute hash use gray code (SSP set = T recovere latent time-sery their observe sum network tomography particle filter title term patterns) T index multimedia database tutorial S use fractal; fast data mine; data graph; efficient time sequence; spatial access method; discovery correlate information I w bruce croft; web information; monika rauch henzinger; james p callan; full-text; retrieval T web information retrieval T language model information retrieval S information use; web information; probabilist information; information filter; text information I xquery stream; murali mani; jens teubner; tree efficient xquery T implement xquery T xquery query language xml S xquery stream; stream xml; complex language; function query language; estimate xml; Table 2: Annotations Generated for Frequent Patterns in DBLP Dataset Note: ""I"" means context indicators; ""T"" means representative transactions; ""S"" means semantically similar patterns. We exclude 12 most frequent and non-informative English words from the collection when extracting frequent patterns. Ontology(GO) terms with motifs [18]. Usually, each protein sequence, which contains a number of motifs, is assigned a set of GO terms that annotate its functions. The goal of the problem is to automatically match each individual mo- tif with GO terms which best represent its functions. In this experiment, we formalize the problem as: Given a set of transactions D (protein sequences with motifs tagged and GO terms assigned), a set P of frequent patterns in D to be annotated (motifs), and a set of candidate patterns Pc with explicit semantics (GO terms), our goal is for pa  P, find P ' c  Pc which best indicate the semantics of pa. We used the same data set and judgments (i.e., gold stan- dard) as used in [18]. The data has 12181 sequences, 1097 motifs, and 3761 GO terms. We also use the same perfor- mance measure as in [18] (i.e., a variant of Mean recip- rocal rank (MRR) [11], notated as MRR in the following sections for convenience) to evaluate the effectiveness of the SPA technique on the Motif - GO term matching problem. Let G = {g1, g2, ..., gc} be a set of GO terms. Given a motif pattern pa, G' = {g' 1, g' 2, ..., g' k}  G is a set of ""cor- rect"" GO terms for pa in our judgement data. We rank G with the SPA system and pick the top ranked terms, where G is treated as either context units or semantically similar patterns to pa. This will give us a rank for each gi  G, say r(gi). MRR (w.r.t. pa) is then computed as MRRa = 1 k k  i=1 1 r(g' i) where r(g' i) is the ith correct GO term for pa. If g' i is not in the top ranked list, we set 1/r(g' i) = 0. We take the average over all the motifs, MRR = 1/m  PaP MRRa to measure the overall performance, where m is the number of motifs in our judgement file. Clearly, 0 <= MRR <= 1. A higher MRR value indicates a higher precision, and the top-ranked GO terms have the highest influence on MRR, which is intuitively desirable. If we are ranking the full candidate GO set for annotation, a ""lazy"" system may either just give them the same rank or rank them randomly. It is easy to show that the expected MRR score for these two cases are the same, which is E[MRR] = 1 |G| |G|  i=1 1 r(gi) where |G| is the number of GO terms in G. E[MRR] drops monotonously when |G| increases, which indicates the larger the candidate set is, the more difficult is the ranking task. We use this value as the baseline to compare our results. We employ all the motifs and GO terms as context units. Since these patterns are not overlapping with each other, we do not use microclustering to preprocess the context units. We compare the ranking of GO terms either as context in- dicators or as SSPs. We also compare the use of Mutual Information and co-occurrence as strength weight for con- text units. These strategies are compared in Table 3: MRR Use MI Use Co-occurrence Context Strength 0.5877 0.6064 Semantical Similarity 0.4017 0.4681 Random (|G| = 3761) 0.0023 Table 3: MRR of SPA on Motif-GO matching We see that SPA is quite effective in matching motifs with GO terms, consistently outperforming the baseline. Rank- ing GO terms as context units achieves better results than ranking them as SSPs, which is reasonable because a GO term usually describes only one aspect of a motif's function and is shared by a number of motifs, thus its context is likely quite different from that of a motif. Interestingly, we notice that although Mutual Information is a better measure for the strength weight in principle, in this specific problem, using MI as strength weight for con- text units is not as good as using simple co-occurrence. This may be because there are hardly many Go terms that are globally very common in this dataset, and therefore MI over penalizes the frequent patterns. A detailed discussion on why co-occurrence measure outperforms MI on Motif-GO matching problem is given in [18]. 5.3 Matching Gene Synonyms As discussed in Section 4.3, the algorithm for extracting semantically similar patterns aims at finding patterns whose meaning is very close to the pattern to be annotated. Ideally, they would be synonyms, or thesauri of the given pattern. These patterns may not ever co-occur with the given pattern but tend to have similar contexts, thus cannot be extracted as strong context indicators. We do another experiment to test the performance of SPA on extracting SSPs. In biomedical literature, it is common that different terms or aliases are used in different studies to denote the same gene, which are known as gene synonyms (see e.g., Table 4). These synonyms generally do not appear together but are ""replaceable"" with each other. Detecting them can help many literature mining tasks. In this experiment, we test the application of SPA to matching gene synonyms. Gene id Gene Synonyms FBgn0000028 abnormal chemosensory jump 6; acj 6; ipou; i pou; cg 9151; ti pou; twin of i pou; FBgn0001000 female sterile 2 tekele; fs 2 sz 10; tek; fs 2 tek; tekele; Table 4: Examples of gene synonym patterns We construct the synonym list for 100 fly genes, which are randomly selected from the data provided by BioCre- AtIvE Task 1B3. Ling et al. collected 22092 abstracts from MEDLINE4 which contain the keyword ""Drosophila"" [14]. We extract the sentences from those abstracts which con- tain at least one synonym in the synonym list. Only the synonyms with support >= 3 are kept, which gives us a small set of 41 synonyms. We then mix those synonyms which belong to different genes and use the algorithm of extract- ing SSPs to recover the matching of synonyms. Specifically, given a synonym from the mixed list, we rank all synonyms with the SSP extraction algorithm. The performance of the system is evaluated by comparing the ranked list with the correct synonyms for the same gene. We also use MRR as the evaluation measure. The results are shown as follows. Context min No Micro- One-pass Hierarchical Units sup Clustering g = 0.9 g = 0.9 Closed 0.15% 0.5108 0.5161 0.5199 Sequential 0.18% 0.5140 0.5191 0.5225 Patterns 0.24% 0.5220 0.5245 0.5301 0.3% 0.5281 0.5292 0.5281 Single Words 0.4774 Random 0.1049 (|G| = 41) Table 5: MRR of SPA on gene synonym matching 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.52 0.522 0.524 0.526 0.528 0.53 0.532 0.534 g MRR 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1000 2000 3000 4000 5000 6000 7000 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1000 2000 3000 4000 5000 6000 7000 Seconds (s) TIME:HIER MRR:HIER MRR:ONEP TIME:ONEP Figure 1: Effect of microclustering algorithms HIER: hierarchical microclustering; ONEP: one-pass microclus- tering; minsup = 0.3% Avg. g = 0.96; From Table 5, we see that the SPA algorithm is also ef- fective for matching gene synonyms, which significantly out- performs the random baseline. When using closed sequential patterns as context units, we always achieve better results than using single words (items) as context units, where a higher minimum support (minsup) usually yields better re- sults. When closed sequential patterns are used, further microclustering indeed improves the performance of the sys- tem. However, when the minsup is higher, this improvement 3http://www.pdg.cnb.uam.es/BioLINK/BioCreative.eval.html 4http://www.ncbi.nlm.nih.gov/entrez/query.fcgi is decaying. This is reasonable because when the minsup is higher, there is less redundancy among the output closed patterns. Using hierarchical microclustering is slightly bet- ter than using the one-pass algorithm, but not always. Finally, we discuss the performance of microclustering in removing redundant context units. The effectiveness and ef- ficiency are shown in Figure 1. Both microclustering meth- ods improve the precision (MRR score) when more redun- dant patterns are grouped into clusters. However, when g is set too large, the precision decreases. This indicates that we may have over penalized the redundancy and lost useful context units. A good g for this task is around 0.8. Although the cluster quality may not be optimized, the performance of one-pass microclustering is comparable to hi- erarchical microclustering on this task. While in principle, the hierarchical clustering is not efficient, the early termi- nation by using a small g saves a lot of time. The one-pass algorithm is more efficient than the hierarchical clustering, and is not affected by g. The overhead that both algorithms suffer is the computation of Jaccard distances for all pairs of patterns, i.e., O(n2) where n is the number of patterns. However, this computation can be coupled in frequent pat- tern mining, as discussed in [20]. 6. RELATED WORK To the best of our knowledge, the problem of semantic pat- tern annotation has not been well studied in existing work. Most frequent pattern mining work [2, 8, 3, 22] focuses on discovering frequent patterns efficiently from the database, and does not address the problem of pattern postprocess- ing. To solve the problem of high redundancy in patterns discovered, closed frequent pattern [15], maximum frequent pattern [16] and top-k closed pattern [9] are proposed to shrink the size of output patterns while keeping the impor- tant ones. However, none of this work provides additional information other than simple statistics to help users inter- pret the frequent patterns. The context information for a pattern tends to be ignored. Recently, researchers develop new techniques to approx- imate, summarize a frequent pattern set [1, 21], or mine compressed frequent pattern sets [20]. Although they ex- plored some kind of context information, none of the work can provide in-depth semantic annotations for frequent pat- terns as we do in our work. The context model proposed in our work covers both the pattern profile in [21] and trans- action coverage in [20] as special cases. Context and semantic analysis are quite common in natu- ral language and text processing (see e.g., [17, 5, 13]). Most work, however, deals with non-redundant word-based con- texts, which are quite different from pattern contexts. In specific domains, people have explored the context of specific data patterns to solve specific problems [18, 14]. Although not optimally tuned, the general techniques pro- posed in our work can be well applied to those tasks. 7. CONCLUSIONS Existing frequent pattern mining work usually generates a huge amount of frequent patterns without providing enough information to interpret the meanings of the patterns. Some recent work introduced postprocessing techniques to sum- marize and compress the pattern set, which shrinks the size of the output set of frequent patterns but does not provide semantic information for patterns. We propose the novel problem of semantic pattern an- notation (SPA) - generating semantic annotations for fre- quent patterns. A semantic annotation consists of a set of strongest context indicators, a set of representative transac- tions, and a set of semantically similar patterns (SSPs) to a given frequent pattern. We define a general vector-space context for a frequent pattern. We propose algorithms to exploit context modeling and semantic analysis to generate semantic annotations automatically. The context modeling and semantic analysis method we presented is quite gen- eral and can deal with any types of frequent patterns with context information. The method can be coupled with any frequent pattern mining techniques as a postprocessing step to facilitate interpretation of the discovered patterns. We evaluated our approach on three different dataset and tasks. The results show that our methods can generate se- mantic pattern annotations effectively. As shown in our ex- periments, our method can be potentially applied to many interesting real world tasks through selecting different con- text units and focusing on candidate patterns for SSPs. Although the proposed SPA framework is quite general, in this paper, we only studied some specific instantiation of the framework based on mutual information weighting and cosine similarity measure. A major goal for future research is to fully develop the potential of the proposed framework by studying alternative instantiations. For example, we may explore other options for context unit weighting and seman- tic similarity measurement, the two key components in our framework. 8. ACKNOWLEDGMENTS We thank Tao Tao and Xu Ling for providing the datasets of Motif-GO matching and gene synonym matching, respec- tively. This work was in part supported by the National Science Foundation under award numbers 0425852. 9. REFERENCES [1] F. Afrati, A. Gionis, and H. Mannila. Approximating a collection of frequent sets. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 12-19, 2004. [2] R. Agrawal, T. Imieliski, and A. Swami. Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACM SIGMOD international conference on Management of data, pages 207-216, 1993. [3] R. Agrawal and R. Srikant. Mining sequential patterns. In Proceedings of the Eleventh International Conference on Data Engineering, pages 3-14, 1995. [4] S. Brin, R. Motwani, and C. Silverstein. Beyond market baskets: generalizing association rules to correlations. In Proceedings of the 1997 ACM SIGMOD international conference on Management of data, pages 265-276, 1997. [5] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391-407, 1990. [6] M. Deshpande, M. Kuramochi, and G. Karypis. Frequent sub-structure-based approaches for classifying chemical compounds. In Proceedings of ICDM'03, page 35, 2003. [7] G. Grahne and J. Zhu. Efficiently using prefix-trees in mining frequent itemsets. In FIMI'03 Workshop on Frequent Itemset Mining Implementations., 2003. [8] J. Han, J. Pei, Y. Yin, and R. Mao. Mining frequent patterns without candidate generation: A frequent-pattern tree approach. Data Min. Knowl. Discov., 8(1):53-87, 2004. [9] J. Han, J. Wang, Y. Lu, and P. Tzvetkov. Mining top-k frequent closed patterns without minimum support. In Proceedings of ICDM'02, 2002. [10] P. Jaccard. Nouvelles recherches sur la distribution florale. Bull. Soc. Vaudoise Sci. Nat., 44:223C-270, 1908. [11] P. Kantor and E. Voorhees. The TREC-5 confusion track: Comparing retrieval methods for scanned text. Information Retrieval, 2:165-176, 2000. [12] R. Krovetz. Viewing morphology as an inference process. In Proceedings of SIGIR '93, pages 191-202, 1993. [13] D. Lin and P. Pantel. Induction of semantic classes from natural language text. In Proceedings of KDD'01, pages 317-322, 2001. [14] X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai, and B. Schatz. Automatically generating gene summaries from biomedical literature. In Proceedings of Pacific Symposium on Biocomputing, pages 40-51, 2006. [15] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discovering frequent closed itemsets for association rules. In Proceeding of the 7th International Conference on Database Theory, pages 398-416, 1999. [16] J. Roberto J. Bayardo. Efficiently mining long patterns from databases. In Proceedings of the 1998 ACM SIGMOD international conference on Management of data, pages 85-93, 1998. [17] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613-620, 1975. [18] T. Tao, C. Zhai, X. Lu, and H. Fang. A study of statistical methods for function prediction of protein motifs. Applied Bioinformatics, 3(2-3):115-124, 2004. [19] K. Wang, C. Xu, and B. Liu. Clustering transactions using large items. In Proceedings of CIKM'99, pages 483-490, 1999. [20] D. Xin, J. Han, X. Yan, and H. Cheng. Mining compressed frequent-pattern sets. In Proceedings of VLDB'05, pages 709-720, 2005. [21] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing itemset patterns: a profile-based approach. In Proceeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 314-323, 2005. [22] X. Yan and J. Han. gspan: Graph-based substructure pattern mining. In Proceedings ICDM'02, pages 721-724, 2002. [23] X. Yan, J. Han, and R. Afshar. Clospan: Mining closed sequential patterns in large datasets. In Proceedings of SDM'03, pages 166-177, 2003."
https://github.com/savigovindarajan/CourseProject	Project Overview.docx	Reproducing Paper on Generating Semantic Annotations for Frequent Patterns with Context Analysis Project Overview: In this project, we have tried to reproduce the model and results from the following published paper on Pattern Annotation. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. This model will automatically generate such annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary like description for a pattern, finding synonym patterns, discovering semantic relations, ranking patterns, categorizing and clustering patterns with semantics, and summarizing semantic classes of a set of frequent patterns. Experiment from the paper reproduced: Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most representative titles from their publications, and the co-authors and title patterns which are most semantically similar to them. Implementation Approach: Here, the general approach taken to automatically generate the frequent patterns and structured annotations for them by the following steps: 1. Derive the frequent patterns Author/Co-Author pattern from the Database using FP Growth algorithm. 2. Define and model the context of the pattern: Derive the context units for the Author/Co-Authors by mining the Closed Frequent Pattern to avoid any redundant pattern Derive the context units for the Titles by mining the Sequential Closed Frequent Pattern using Prefix Span Algorithm Select context units and design a strength weight for each unit to model the contexts of frequent pattern 3. Derive the list of representative transaction by finding the cosine similarity between the Frequent Pattern & the Frequent Transactions 4. Derive the Semantically Similar Author Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Frequent Author/Co-Author 5. Derive the Semantically Similar Title Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Title Words Installation and Usage: The following packages must be installed for the successful execution: https://test.pypi.org/simple/ krovetz Prefixspan, re, csv, pandas, numpy, mlxtend, sklearn Note: To use the Krovetz Stemmer installed, Visual Studio C++ is needed to be installed. Team members had issues running the program without it. Input data: Here the input dataset (DBLP Dataset) is in a specific format. DBLP dataset (a subset of around 12k transactions/titles papers from the proceedings of 12 major conferences in Data Mining; around 1k latest transactions from each of such conference). Each row represents a title presented in a conference. It has 3 fields - id (numeric), title (String) and MergedAuthors (string). The authors and co-authors associated with the title has been merged into a single column for easy analysis. Output: Once the program patternAnnotation.py is executed, it takes the DBLP_Dataset.csv as the input and generates the output.txt file in the same path where source code exists. This output file contains all the closed frequent patterns and their most representative Context Indicators, most representative transactions (capped as 4), Semantically Similar Patterns (SSPs) as per co-author patterns and title term patterns. Each record in the output file represents one closed frequent pattern and their associated details. Manually converted output to output.xlsx, with summary information of the annotated pattern is available as well: I - Context Indicator; T - Representative Transactions; SSPA - Semantically Similar Author Pattern; SSPT - Semantically Similar Title Pattern Implementation Details: Derive the frequent itemsets of Author/Co-Author: Algorithm used: FP Growth Output: 64 Frequent Author/Co-Author itemsets (Removed any frequent author without co-Author) Define and model the context of the pattern: For each of the 64 Frequent Itemsets from the above step: Derive the context units for the Author/Co-Authors: Mined closed Author/Co-Author itemsets from already mining frequent itemsets by removing any itemset with redundant support Derive the context units for the Title: Algorithm used: Prefix Span; We got limited Title Patterns, hence we did not need micro-clustering to further reduce the titles. Final Context Indicator: Merged the context units of the above two steps Context Indicator Weighting: Here we slightly modified our approach to weighting as the mutual information value was not giving the appropriate weightage, based on our analysis & understanding. We use an approach similar to IDF, where when a context unit is present in more transaction, the weightage is reduced & any context unit that uniquely appears in the transaction will have the highest weightage. Derive the list of representative transaction: Based on the context indicators identified for each of the Frequent Itemset, we identified top 4 transactions with the highest Cosine Similarity Derive the list of Semantically Similar Author/Co-Author pattern: Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their Author/Co-Author as the SSP Derive the list of Semantically Similar Author/Co-Author pattern: Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their frequent title pattern as the SSP Team Contribution: Name ID Contribution Bipin Chandra Karnati karnati3 Analysis, Implementation of Extracting Title Patterns of Frequent Authors, Context Indicator Identification, Code Integration Savitha Govindarajan savitha3 Analysis, Implementation of Extracting Itemsets of Frequent Authors, SSP Author Pattern, Co-ordination, Code Integration Utpal Mondal umondal2 Analysis, Data Extraction & Clean Up, Implementation of Cosine Similarity, Detailed analysis on MI, Documentation , Code Integration References: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 FP Growth Algorithm implementation: https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3 Prefix Span: https://pypi.org/project/prefixspan/ Cosine Similarity: https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ DBLP Dataset: 12 Major Conferences on Data Mining. 1000 latest titles from each conference ACL - Annual Meeting of the Association for Computational Linguistics (https://dblp.uni-trier.de/db/conf/acl/) ADBIS - Symposium on Advances in Databases and Information Systems (https://dblp.uni-trier.de/db/conf/adbis/) CIKM - International Conference on Information and Knowledge Management (https://dblp.uni-trier.de/db/conf/cikm/) ECIR - European Conference on Information Retrieval (https://dblp.uni-trier.de/db/conf/ecir/) ICDE - IEEE International Conference on Data Engineering (https://dblp.uni-trier.de/db/conf/icde/) ICDM - IEEE International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/icdm/) KDD - Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/kdd/) PAKDD - Pacific-Asia Conference on Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/pakdd/) SDM - SIAM International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/sdm/) SIGIR - Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (https://dblp.uni-trier.de/db/conf/sigir/) WSDM - Web Search and Data Mining (https://dblp.uni-trier.de/db/conf/wsdm/) WWW - The Web Conference (https://dblp.uni-trier.de/db/conf/www/)
https://github.com/savigovindarajan/CourseProject	Project Progress Report.pdf	Reproducing the paper on Pattern Annotation Team Members: Bipin Chandra Karnati (karnati3@illinois.edu ) Savitha Govindarajan (savitha3@illinois.edu ) Utpal Mondal (umondal2@illiniois.edu) Team Leader: Savitha Govindarajan (savitha3@illinois.edu ) Team Name: Avengers Progress Report: 1. Which tasks have been completed?  Reviewed the paper to understand what is implemented, how it is implemented and the experiments in the paper  Made decision on which of the experiments need to be implemented using the code  Collected the data required to reproduce the paper & working on the required data clean up  Organized a plan and the work split up amongst each team member 2. Which tasks are pending?  Code Implementation to reproduce the paper  Validating the code to conclude the same results as in the Paper 3. Are you facing any Challenges?  No specific open challenges at this point
https://github.com/savigovindarajan/CourseProject	Project Proposal_Reproducing paper on Pattern Annotation_Team Avengers.docx	Reproducing the paper on Pattern Annotation Team Members: Bipin Chandra Karnati (karnati3@illinois.edu ) Savitha Govindarajan (savitha3@illinois.edu ) Utpal Mondal (umondal2@illiniois.edu) Team Leader: Savitha Govindarajan (savitha3@illinois.edu ) Team Name: Avengers Paper Chosen: Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 Authors: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai Year Published: 2006 Implementation Language: Python Dataset used in paper: DLBP data set Availability of dataset: Yes. We have the data subset (https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html) & working on getting the exhaustive data set.
https://github.com/savigovindarajan/CourseProject	README.md	CourseProject: CS410 - Text Information Systems Generating Semantic Annotations for Frequent Pattern with Context Analysis Project Overview: In this project, we have tried to reproduce the model and results from the following published paper on Pattern Annotation. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. This model will automatically generate such annotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary like description for a pattern, finding synonym patterns, discovering semantic relations, ranking patterns, categorizing and clustering patterns with semantics, and summarizing semantic classes of a set of frequent patterns. Experiment from the paper reproduced: Given a set of authors/co-authors, annotate each of them with their strongest context indicators, the most representative titles from their publications, and the co-authors and title patterns which are most semantically similar to them. Implementation Approach: Here, the general approach taken to automatically generate the frequent patterns and structured annotations for them by the following steps: 1. Derive the frequent patterns Author/Co-Author pattern from the Database using FP Growth algorithm. 2. Define and model the context of the pattern: * Derive the context units for the Author/Co-Authors by mining the Closed Frequent Pattern to avoid any redundant pattern * Derive the context units for the Titles by mining the Sequential Closed Frequent Pattern using Prefix Span Algorithm * Select context units and design a strength weight for each unit to model the contexts of frequent pattern 3. Derive the list of representative transaction by finding the cosine similarity between the Frequent Pattern & the Frequent Transactions 4. Derive the Semantically Similar Author Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Frequent Author/Co-Author 5. Derive the Semantically Similar Title Pattern by finding the cosine similarity between the Context Units of the Frequent Pattern with the other context units and annotating it with contextually similar Title Words Installation and Usage: The following packages must be installed for the successful execution: https://test.pypi.org/simple/krovetz Prefixspan, re, csv, pandas, numpy, mlxtend, sklearn Note: To use the Krovetz Stemmer installed, Visual Studio C++ is needed to be installed. Team members had issues running the program without it. Input data: Here the input dataset (DBLP Dataset) is in a specific format. DBLP dataset (a subset of around 12k transactions/titles papers from the proceedings of 12 major conferences in Data Mining; around 1k latest transactions from each of such conference). Each row represents a title presented in a conference. It has 3 fields - id (numeric), title (String) and MergedAuthors (string). The authors and co-authors associated with the title has been merged into a single column for easy analysis. Output: Once the program patternAnnotation.py is executed, it takes the DBLP_Dataset.csv as the input and generates the output.txt file in the same path where source code exists. This output file contains all the closed frequent patterns and their most representative Context Indicators, most representative transactions (capped as 4), Semantically Similar Patterns (SSPs) as per co-author patterns and title term patterns. Each record in the output file represents one closed frequent pattern and their associated details. Implementation Details: Derive the frequent itemsets of Author/Co-Author: * Algorithm used: FP Growth * Output: 64 Frequent Author/Co-Author itemsets (Removed any frequent author without co-Author) Define and model the context of the pattern: For each of the 64 Frequent Itemsets from the above step: * Derive the context units for the Author/Co-Authors: Mined closed Author/Co-Author itemsets from already mining frequent itemsets by removing any itemset with redundant support * Derive the context units for the Title: Algorithm used: Prefix Span; We got limited Title Patterns, hence we did not need micro-clustering to further reduce the titles. * Final Context Indicator: Merged the context units of the above two steps * Context Indicator Weighting: Here we slightly modified our approach to weighting as the mutual information value was not giving the appropriate weightage, based on our analysis & understanding. We use an approach similar to IDF, where when a context unit is present in more transaction, the weightage is reduced & any context unit that uniquely appears in the transaction will have the highest weightage. Derive the list of representative transaction: * Based on the context indicators identified for each of the Frequent Itemset, we identified top 4 transactions with the highest Cosine Similarity Derive the list of Semantically Similar Author/Co-Author pattern: * Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their Author/Co-Author as the SSP Derive the list of Semantically Similar Author/Co-Author pattern: * Based on the context indicators identified for each of the Frequent Itemset, we identified which of the other 63 context indicators are similar using Cosine Similarity. We took the top 3 similar Context Indicators & took their frequent title pattern as the SSP Video Presentation: https://mediaspace.illinois.edu/media/t/1_hdhp3434 References: Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 FP Growth Algorithm implementation: https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3 Prefix Span: https://pypi.org/project/prefixspan/ Cosine Similarity: https://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ DBLP Dataset: 12 Major Conferences on Data Mining. 1000 latest titles from each conference ACL - Annual Meeting of the Association for Computational Linguistics (https://dblp.uni-trier.de/db/conf/acl/) ADBIS - Symposium on Advances in Databases and Information Systems (https://dblp.uni-trier.de/db/conf/adbis/) CIKM - International Conference on Information and Knowledge Management (https://dblp.uni-trier.de/db/conf/cikm/) ECIR - European Conference on Information Retrieval (https://dblp.uni-trier.de/db/conf/ecir/) ICDE - IEEE International Conference on Data Engineering (https://dblp.uni-trier.de/db/conf/icde/) ICDM - IEEE International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/icdm/) KDD - Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/kdd/) PAKDD - Pacific-Asia Conference on Knowledge Discovery and Data Mining (https://dblp.uni-trier.de/db/conf/pakdd/) SDM - SIAM International Conference on Data Mining (https://dblp.uni-trier.de/db/conf/sdm/) SIGIR - Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (https://dblp.uni-trier.de/db/conf/sigir/) WSDM - Web Search and Data Mining (https://dblp.uni-trier.de/db/conf/wsdm/) WWW - The Web Conference (https://dblp.uni-trier.de/db/conf/www/)
https://github.com/cnj3/CourseProject	progress-report.pdf	Progress Report for Project Chameleon (Jacob Huss, Chaitanya Jujjavarapu, and Edward Park) 1. We have successfully scraped the needed IEM data from the IEM website into a CSV. We then wrote IEMcsvTranformer.py, which took this data and calculated the normalized price for each date. These normalized prices were stored in data/IEMPrices.txt. We then wrote a function in DataReader.py to read these prices into a dictionary. For the 2nd part of our scraping process, we had to create a program that fetches the data from all 47000 xml files between the dates of May 2000 and October 2000, and then parses the text of each article so that only our matching query terms were pulled out and outputted and cleaned up into a text file through a 'bag of words' approach. 2. Now that we have our data completely scraped and prepared, we need to implement the algorithm detailed in the paper. In order to do this, we will first need to break down our algorithm into smaller pieces that can be tackled by each team member. Doing this will require us to first gain a better understanding of the algorithm itself. 3. The biggest challenge that we are facing is understanding how the algorithm for our function works.
https://github.com/cnj3/CourseProject	project-proposal.md	1.Team members: a. Jacob Huss, NetID: jnhuss2 (team captain) b. Chaitanya Jujjavarapu, NetID: cnj3 c. Edward Park, NetID: edwardp3 Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback: https://dl-acm-org.proxy2.library.illinois.edu/doi/epdf/10.1145/2505515.2505612 Python Yes. We have access to all the data used in the first experiment described by the paper: a. We will scrape the New York Times data from this site: https://spiderbites.nytimes.com/2000/ b. We will scrape the IEM data from this site: https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 N/A N/A
https://github.com/cnj3/CourseProject	README.md	"CS 410 Final Project jnhuss2, cnj3, edwardp3 Overview We are working on reproducing the paper, ""Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback."" Our code scrapes two datasets, IEM Price History data and the New York Times site map data. It uses these datasets to generate common topics in New York Times articles that are correlated with change in the IEM prices. The algorithm uses an iterative approach. It first uses LDA to generate topics. It then analyzes these topics to determine which topics and words are most correlated to change in the IEM data. These results are used to generate priors, which are used as input to the next iteration of LDA. How the Software is Implemented The primary functionality of the ITMTF algorithm is in the file main.py. In the following section, we break this file down into the various functions that are used to implement it. This file data_reader.py is used by main.py to read in the IEM and NYT data from the data files IEMPrices.txt and articles.txt. Both of these txt documents are held in the data directory. article_compiler.py consists of the code that was used to scrape the NYT data and create articles.txt. iem_csv_tranformer.py consists of the code that was used to create IEMPrices.txt from the data held in IEMPrices.csv. Functions In main.py: read_data - reads in data from articles.txt and IEMPrices.txt run_itmtf - runs the ITMTF algorithm with nyt_data and runs methods to determine the significant topics get_topic_total_probability_by_date - finds the probability of topics by date and creates a multidimensional array find_significant_topics - finds the significant topics using the granger algorithm granger_significance_test - runs grainger significance test off of a multidimensional array that is inputted as a parameter get_pos_neg_words - states whether the word has a positive or negative correlation with the IEM prices determine_new_priors - creates a weight for each word to be used next time LDA is run get_avg_confidence - gets the average confidence amongst all of the words that have been chosen to be significant get_avg_purity - gets the average purity amongst all of the topics In data_reader.py get_iem_prices - reads in IEMPrices.txt and returns a dictionary of date object keys and the iem normalized price values read_data_file Usage of Software / How to Run APIs Used To program our software, we used Python. We used Python because of its simplicity and packages. For scraping the New York times article data, we used bs4's BeautifulSoup. We used the gensim library in multiple locations. In main.py, we used gensim to create a dictionary that acts as a vocabulary list for the entire corpus. The keys in the dictionary act as ids and the values are the strings of words, which is nice because each word gets an id associated with it. In data_reader.py, gensim is used to clean up the characters and words scraped from the New York times data. We used gensim methods, such as strip_punctuation and remove_stopwords. Next, we used statsmodels.tsa.stattools in main.py because it contains a method called grangercausalitytests, which we used for the granger test algorithm. We also used scipy.stats because it contains a method for the pearson correlation algorithm, pearsonr. How to Run To run the code, clone the repository and open it in your terminal. After this, install all of the needed packages and run python main.py. sh $ pip install numpy $ pip install datetime $ pip install bs4 $ pip install gensim $ pip install statsmodels $ pip install scipy $ python iem_csv_transformer.py $ python main.py Running this program took our computers about 25 minutes to complete. It runs the ITMTF algorithm a total of 4 times, each time varying the number of topics that is generated by LDA. Each time the ITMTF algorithm is run, its core functions are iterated 5 times (LDA runs 5 times). With each iteration, the program prints out the topics it generated that are significant as well as the words that were most correlated to positive and negative movement in the IEM price. Once ITMTF is run all 4 times, it prints out the average purity and causality for each iteration in each run of the algorithm. Team Member Contributions Jacob sh Scraped the IEM data. Wrote the code to read in IEM and New York Times data files. Wrote the backend code to find the significant words, differentiate if they are positive and negative, and make a list of words that ""cause"" or are related to changes in the IEM betting prices. Chaitanya sh Worked with Edward to scrape the New York Times data. Cleaned the code, worked on implementing the method to find significant words, documented the code, and wrote the documentation Edward sh Worked with Chaitanya to scrape the New York Times data. (Ghosted us for the rest of the project) Presentation You can view the final presentation using this YouTube link: https://youtu.be/5NiqwlT-tu4 It is also in the GitHub repository."
https://github.com/ssquires/CourseProject	Documentation_and_Directions.pdf	"CS410 Final Project Documentation squires4 | Samantha Squires Contents Project Summary 1 How to Run the Code 1 Project Summary I trained a BERT text classification model to identify tweets as 'SARCASM' or 'NOT_SARCASM' using python/tensorflow/keras and Google Colab. I found this tutorial very helpful in the process of writing my code, and directly used some of the code provided in the tutorial-I've also noted this directly in my code in the relevant functions. The code can be found in the Google Colab notebook in this repo (filename TextClassificationCompetitionFinal.ipynb). BERT was the first model I even attempted for this project, because I had heard of its success in text classification tasks. I didn't have much previous experience with tensorflow, so the main challenges that arose were understanding the formats and shapes required for model input and output. To complete the project, I first utilized the Tensorflow BERT tutorial mentioned above to write a function for building a keras model using a pre-trained BERT model, and then a function to fit this model to the tweet classification training set, then make predictions on the test data and return those predictions. Finally, I wrote a function to transform the model's predictions (log-odds values) to the format required by LiveDataLab, and save the result to a text file for downloading. Finally, I wrote the main code pipeline, which loads the train and test data, preprocesses it by con- catenating the 'response' and 'context' columns into a single feature and transforming the labels from 'NOT_SARCASM'/'SARCASM' into 0/1, and finally calls the functions that I wrote earlier to create, fit, and predict using the model. After the code is finished running, it outputs a file named answer.txt, which is in the proper format to be submitted via LiveDataLab. How to Run the Code All project code is located in TextClassificationCompetitionFinal.ipynb. Please run the code in Google Colab by following the steps below (or watch the demo video tutorial here): 1. Download this repo. 2. Go to http://colab.research.google.com/ and click Upload, then select the downloaded file TextClassificationCompetitionFinal.ipynb. This should open the notebook in Colab. The rest of these instructions are also included in the notebook itself, but for completeness: 3. In the Colab ""Edit"" menu, go to ""Notebook Settings"" and select ""GPU"" from the hardware accelerator dropdown. 4. Upload the train and test data files provided with the competition (make sure they're named train.jsonl and test.jsonl) by going to ""Files"" in the left-hand sidebar, clicking the upload icon, 1 and selecting train.jsonl and test.jsonl. These two data files are included in the repo, so you should already have them downloaded. 5. To run the code, select ""Runtime"" from the Colab menu and click ""Run all"". The first few cells should run quite quickly: the final cell, which trains the model and predicts labels for the test data, takes much longer (in my experience, 10-12 minutes). After a minute or so, you should start to see output tracking the training progress of the model. After the code finishes running, the output file, answer.txt, should be visible under ""Files"" in the left-hand sidebar. If you'd like to save this file, make sure to download it before the runtime disconnects. 2"
https://github.com/ssquires/CourseProject	Final_Project_Progress_Report.pdf	CS410 Final Project Progress Report squires4 | Samantha Squires Which tasks have been completed? I trained a BERT text classification model to identify tweets as 'SARCASM' or 'NOT_SARCASM' using python/tensorflow/keras and Google Colab. I found this tutorial very helpful in the process. As of today (11/29/20), my second submission has successfully passed the baseline f1 score performance on LiveDataLab, with a precision of 0.699, recall of 0.768, and f1 of 0.732. Which tasks are pending? Although my code is functional at this point, I still need to: - Clean up the code and write documentation - Automate the final step of creating answer.txt (I did this manually in excel for my first two submissions) - Create my tutorial presentation. If I have additional time, I would also like to continue to improve the performance of my model. Are you facing any challenges? Since I am fairly new to tensorflow and keras, my primary challenges so far have been getting the inputs and outputs of the model into the proper shapes and formats. I don't anticipate significant challenges moving forward with the documentation and presentation. 1
https://github.com/ssquires/CourseProject	Final_Project_Proposal.pdf	CS410 Final Project Proposal squires4 | Samantha Squires Team This will be an individual project (Samantha Squires, netid: squires4). Competition I plan to join the Text Classification Competition (Twitter Sarcasm Detection). Potential NN Classifiers / DL Frameworks I plan to look into a variety of neural-network based classifiers (CNN, LSTM, BERT, etc.). I plan to use the Keras library, which I have used before for small neural network classification projects and tutorials. Programming Language The project will be completed in Python. 1
https://github.com/ssquires/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/chengbo322/CourseProject	Individual Project Proposal.pdf	Individual Project - Free Topic Sentiment Analysis in management discussion and analysis (MD&A) from company's SEC filing Chengbo Jiang chengbo6@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Net ID: Chengbo6, this will be an individual project so Chengbo Jiang will be served as the captain and other roles. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work?  Free Topic: Sentiment Analysis in management discussion and analysis (MD&A) from company's SEC filing.  The task is to extract the management and analysis (MD&A) sections of text from public companies' 10K (Annual) and 10Q (quarterly) filings from U.S. Securities and Exchange Commission (SEC) database.  After extracting the texts, I will try to classify the sentiment whether they are positive or negative using the methods and algorithms learned from CS410.  Positive and negative can be determined via counting positive or negative words from dictionary.  Other than accounting information on those filings, text information could be valuable but overwhelming. Classifying these texts information in positivity or negativity will streamline the analytical procedures of public companies as well analysts' understanding. For example, if we can filter the negatives so that we can concentrate more on analyzing the positive ones.  The planned approach: 1. Write a web crawler to download the reports from SEC's website. 2. Extract the MD&A information sections from the downloaded reports. 3. Sentiment analysis using methods such as Naive Bayes, Multinomial Naive Bayes etc.  Tools: Python (metapy, nltk, numpy, beautifulsoup etc.)  Datasets: The EDGAR (Electronic Data Gathering, Analysis, and Retrieval) database from SEC.  Expected Outcome: Expect to accurately classify the textual data in their respective categories.  Evaluation: Since I will use different methods, I will compare the results from different methods. 3. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 1. Write a web crawler to download the reports from SEC's website. 40 Hours 2. Extract the MD&A information sections from the downloaded reports. 10 Hours 3. Sentiment analysis using methods such as Naive Bayes, Multinomial Naive Bayes etc. 40 Hours
https://github.com/chengbo322/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/vs27-illinois/CourseProject	Project Documentation.pdf	"RecipeFinder Software code submission with documentation by Team ""TasteBuddies"" Vijayaragavan Selvaraj (VS27) - Team Leader Sathyanarayanan Gokarnesan (SG53) Karthika Gopalakrishnan (KG24) 1 | Page 1. Application - Live Demo link Please see the live demo of the ""RecipeFinder"" application running at the following location. http://54.81.52.188/ 2. Overview ""RecipeFinder"" is a web-based tool built using novel information retrieval techniques to search popular recipes. It searches recipes based on the ingredients of the recipes. When the user searches with an ingredient name, it lists the top 20 recipes that contain the given ingredient. For each of the recipes in the search results, users can also see recipe details including the cooking directions, other ingredients of the recipe, nutritional values, timings, and ratings of the recipe. Users are also recommended with 4 similar recipes based on the nutritional value. Content based similarity approach has been used for this project. 3. Architecture and Source Code Following is the high-level architecture of ""RecipeFinder"". Front end Angular Web app 2 | Page Source code for the ""Recipe Finder"" application is present in the following github location: https://github.com/vs27-illinois/CourseProject.git Source code for our project can be broadly categorized into three sections.  BackEnd  FrontEnd  Infrastructure Backend: Following are the main files in the backend system. 1. Indexer.py This python file is responsible for taking the ""Recipe Finder"" dataset, parsing the csv file for different fields, creating indexes on different fields, and writing the same in an index document. We used Apache PyLucene to index the dataset. The indexed dataset contains 49,698 records in total. We used the dataset from Kaggle at the following location for our project: https://www.kaggle.com/elisaxxygao/foodrecsysv1?select=raw-data_recipe.csv Following are the columns in the index. 3 | Page S.No Column Index Type 1 id IndexOptions.DOCS 2 name IndexOptions.NONE 3 image IndexOptions.NONE 4 avg_rating IndexOptions.NONE 5 total_reviews IndexOptions.NONE 6 ingredients IndexOptions.DOCS_AND_FREQS_AND_POSITIONS 7 time_taken IndexOptions.NONE 8 nutrition IndexOptions.NONE 9 calories IndexOptions.NONE We used MMapDirectory to load the index files and indexed the ingredients and recipe id field by using EnglishAnalyzer since the dataset is in English. We stored the rest of the fields in the indexed document. Following are some of the main code snippets of indexer.py. 4 | Page 10 carbohydrates IndexOptions.NONE 11 protein IndexOptions.NONE 12 fat IndexOptions.NONE 2. Retriever.py Following are the functionalities of retriever.py. # It is responsible for taking the ""ingredient"" input from the user, searching the index for 20 popular recipes based on the ingredient, converting the results (list of recipes) into json format to be rendered in the UI. # It is responsible for taking the recipe id from the user to provide a detailed view of the recipe. # It also returns the list of top 4 recipes based on similar nutritional value for the recipe which the user wants to see the details. Following are the API calls involved. 5 | Page S.No End Point Method Output 1 http://<ipaddress>/recipe/sear ch/{ingredient} Ex: http://54.81.52.188/recipe/se arch/chocolate GET [ { ""avg_rating"": 4.34615373611, ""calories"": 274.0809, ""carbohydrates"": 45.78727, ""fat"": 10.5331, ""id"": 32482, ""image"": ""https://images.media-allrecipes.com/userphotos/ 250x250/710487.jpg"", ""ingredients"": [ ""<strong>chocolate</strong> chips"", ""powdered <strong>chocolate</strong> drink mix"", ""<strong>chocolate</strong> syrup"", ""scoops <strong>chocolate</strong> ice cream"", ""milk"", ""ice"" ], ""name"": ""Chocolate Surprise Milkshake"", ""protein"": 3.052425, ""total_reviews"": 17 },... ] 2 http://<ipaddress> /recipe/details/{recipeId} Ex: http://54.81.52.188/recipe/det ails/220725 GET { ""avg_rating"": 4.26016616821, ""calories"": 123.5964, ""carbohydrates"": 19.74722, ""directions"": [ ""Prepare the cake mix according to package directions using any of the recommended pan sizes. When cake is done, crumble while warm into a large bowl, and stir in the frosting until well blended."", ""Melt chocolate coating in a glass bowl in the microwave, or in a metal bowl over a pan of simmering water, stirring occasionally until smooth."", ""Use a melon baller or small scoop to form balls of the chocolate cake mixture. Dip the balls in 6 | Page chocolate using a toothpick or fork to hold them. Place on waxed paper to set."" ], ""fat"": 5.188236, ""id"": 67656, ""image"": ""http://images.media-allrecipes.com/userphotos/7 20x405/599097.jpg"", ""ingredients"": [ ""chocolate cake mix"", ""prepared chocolate frosting"", ""bar chocolate flavored confectioners coating"" ], ""name"": ""Cake Balls"", ""nutrition"": { ""calcium"": { ""amount"": 27.23583, ""displayValue"": ""27"", ""hasCompleteData"": true, ""name"": ""Calcium"", ""percentDailyValue"": ""3"", ""unit"": ""mg"" }, ""calories"": { ""amount"": 123.5964, ""displayValue"": ""124"", ""hasCompleteData"": true, ""name"": ""Calories"", ""percentDailyValue"": ""6"", ""unit"": ""kcal"" },.. }, ""protein"": 1.122792, ""time_taken"": [ ""Prep"", ""40 m"", ""Cook"", ""30 m"", ""Ready In"", ""3 h 10 m"" ], ""total_reviews"": 1867 } 3 http://<ipaddress>/recipe/reco mmend/{recipeId} Ex: http://54.81.52.188/recipe/rec ommend/220725 GET [ { ""avg_rating"": 3.5, ""calories"": 124.4117, ""carbohydrates"": 19.98162, ""fat"": 4.989575, ""id"": 15463, ""image"": ""http://images.media-allrecipes.com/userphotos/7 20x405/1115423.jpg"", ""ingredients"": [ ""canola oil"", ""honey"", ""packed brown sugar"", ""egg whites"", ""vanilla extract"", ""water"", ""wheat flour"", ""all-purpose flour"", ""baking powder"", ""salt"", ""ground cinnamon"", ""semisweet chocolate chips"" We maintained the IndexSearcher object as a global variable to boost the performance of the search. We used Lucene Highlighter to highlight the search terms in the search results in the website. We maintained a global variable of pandas dataframe with all the records that is the L2 normalized form of nutritional values that we used in the recommendation service. The recommendation is done based on the nutritional values of calories and macronutrients (i.e., protein, carbohydrates and fat) by calculating euclidean distance between the record of the given recipe id and the other records. Following are some of the main code snippets for retriever.py. 7 | Page ], ""name"": ""No Cholesterol Chocolate Chip"", ""protein"": 1.515088, ""total_reviews"": 15 }, 8 | Page Frontend: The frontend of the code to display the search results and recommended recipes are based on Angular 11. It makes http GET requests to the backend to retrieve the search results, recipe details and recommended recipes. The frontend is a single page application and the code is located in the ""src/app"" folder. We used the Angular Material module as a base to build the UI. Infrastructure: Following are the infrastructure related files. 1. DockerFile We used Docker to containerize our application and used the following image as the base to install PyLucene: https://hub.docker.com/r/coady/pylucene. The Dockerfile in the project folder copies all the required backend and frontend files and deploys them in a standalone container. 2. startup.sh This shell script file is responsible for creating the docker image (recipefinder:1.0) and running the image as a docker container. 9 | Page 4. Setup and Installation Instructions Following are the technologies used in the project. * Python 3.9.0 * Apache PyLucene 8.6.1 * Flask 1.1.2 * Angular 11.0 * Docker 2.5 Some of the python packages used are: * Numpy 1.19.4 * Pandas 1.1.5 * scikit-learn 0.23.2 * sklearn * scipy 1.5.4 Angular modules used: * angular-material * ng-bootstrap Since we used Docker, the project can be installed in either a local environment or on any cloud instances. Following are the steps: 1. Install and set up Docker. 2. Clone the project from the github location https://github.com/vs27-illinois/CourseProject.git 3. Open the shell script and run the following command: sh startup.sh 4. If the application is running on a cloud environment, enable the http port (80) in the host machine, so that the Flask application running in the docker container can be exposed to the internet. Sample image below. 10 | Page 5. Open your favorite browser and go to http://127.0.0.1/ (if the app is running in a local environment) or http://<ipaddress>/ (if the app is running in a cloud environment). 5. Snapshots Following are the snapshots of the ""Recipe Finder"" application. 11 | Page 12 | Page 6. Further Improvements Within the given timeframe, we implemented all the functionalities that we have initially proposed for this project. We even fairly optimized the response time of the APIs by improving the performance of the search and recommendation service of the application. As a future enhancement, the performance of the recommendation service can be improved (currently it takes ~20 seconds to provide results). Moreover, we attempted to modify the recommendation service to use other fields like ingredients and faced memory limitations in our EC2 instance (we used free tier). It can also be tried as a further development of this project. 7. References https://lucene.apache.org/core/8_6_1/ https://docs.scipy.org/doc/scipy/reference/spatial.distance.html https://www.kaggle.com/elisaxxygao/foodrecsysv1 https://github.com/coady/docker https://material.angular.io/ 13 | Page 8. Contribution of Team Members Vijayaragavan Selvaraj (VS27) - Team Leader * Retriever (Search) * Docker * Angular Sathyanarayanan Gokarnesan (SG53) * Indexer * EC2 setup * Angular Karthika Gopalakrishnan (KG24) * Retriever (Recommendation service) * Documentation * Presentation 14 | Page"
https://github.com/vs27-illinois/CourseProject	Project Progress Report.pdf	Project Progress Report: Recipe Finder Team Tastebuddies, Net IDs: vs27, kg24, sg53 1) Which tasks have been completed?  Installed Pylucene using Docker in the local environment and did a test run in EC2.  Development of python code completed for the following:  Parsing and Cleansing of recipe dataset.  Indexing and Storing of recipe dataset.  Enhanced performance of indexing using Pandas.  While implementing the indexer, we evaluated different indexing options and field types like multivalued fields given in the Lucene library.  Retriever to search the recipes based on given 'Ingredient' query term.  While implementing the retriever, we evaluated different query parser options and analyzers to search the data using a single term or a phrase.  Development of API's using Flask. 2) Which tasks are pending?  Development of web pages using Polymer is in progress. (vs27, expected to be completed by 12/02)  Development of the Content Based Recommender system is in progress. (kg24, expected to be completed by 12/03)  Deployment of Pylucene in EC2 using Docker. (sg53, expected to be completed by 12/04)  Deployment of API's and web pages in a web server in EC2. (vs27, expected to be completed by 12/05)  Documentation and presentation. (expected to be completed by 12/10)  Planning to do the final submission by 12/10. 3) Are you facing any challenges?  In our project proposal, we mentioned that we would be using Apache Solr to index and store the data. However, we later found that Apache PyLucene is sufficient for our use case and used it for indexing and storing the dataset.  We have faced a lot of issues in installing Pylucene in the local environment as well as EC2. As a solution to those issues, we found an open source docker based Pylucene repository that helped us in automating the installation of Pylucene.  We are not facing any challenges as of now.
https://github.com/vs27-illinois/CourseProject	Project Proposal.pdf	Project Proposal: Recipe Finder What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - Team: Tastebuddies - Vijayaragavan Selvaraj (vs27) - Team Lead - Karthika Gopalakrishnan (kg24) - Sathyanarayanan Gokarnesan (sg53) What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? - Free Topic: Recipe Finder - Description: It is a search tool to find the most popular recipes based on an ingredient. For each recipe in the search result, the users can see the cooking directions, nutritional value and a list of recommended recipes. - Task: We are going to use the novel Information Retrieval techniques to build a web-based search engine that allows users search for the recipes based on their preferred ingredient, and a recommendation service that suggests users a list of similar recipes based on a combination of ingredients and nutritional value. - Motivation: Allrecipes.com is a recipe curation website and has curated over thousands of recipes from all over the world. Our ultimate goal is to develop a Recipe Finder to enhance the recipe search process by retrieving top 20 popular recipes using the ingredient entered by the user and recommend similar recipes for each recipe viewed by the user. - Approach: We are going to use Apache Solr as the backend layer for the search. We will index the data in Solr and fine tune the parameters for better search experience. For the recommender system, we are going to adopt Content-based Filtering mechanism and use the columns, nutritional value and ingredients, to suggest similar recipes to the user. - Dataset: Allrecipes.com dataset from kaggle - Outcome: A website where users would be able to enter an ingredient in a search bar and the system would return the top 20 popular recipes and along with recommendations of similar recipes for each recipe in the top 20 list. - Evaluation: User needs to enter an ingredient (eg. potato) in a search bar and the system would list the top 20 popular recipes for the ingredient. If the users select a recipe from the 20 recipes, they can see the cooking directions and nutrition value for the recipe. In addition to that, the users would also see a list of similar recipes that have similar ingredients and better nutritional value. Which programming language do you plan to use? - Python, Apache Solr, Scikits (learn, surprise), Pandas, Numpy, HTML, Javascript, CSS Project Proposal: Recipe Finder Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Our estimate for the project would be 80 hours approximately and below is the breakup of the workload for the project. - Oct 26 - Nov 9: Import Dataset in Apache Solr (~20 hours) - Setup Solr in EC2 - Data Cleansing & Ingestion - Fine Tuning and Performance Analysis - Nov 10 - Nov 22: Recipe Recommendation Model (~30 hours) - Design Content-based Filtering System to recommend similar recipes to the user based on the ingredient and nutritional value - Performance Analysis and Fine Tuning of the Model - Nov 23 - Dec 4: Web Application (~25 hours) - Design Backend and Frontend for the Web application - Scenario 1: - Input: User enters Ingredient in search bar - Output: Top 20 recipes that has the ingredient - Scenario 2: - Input: User opens a recipe from the Top 20 list - Output: Cooking Directions, Nutritional Value and Recommendations of similar recipes - Dec 5 - Dec 8: Documentation & Presentation Preparation (~5 hours)
https://github.com/vs27-illinois/CourseProject	README.md	Recipe Finder Cooking has become a survival skill for everyone now and every day, people search for recipes using their preferred ingredient. AllRecipes.com is a website which has curated thousands of recipes from all over the world in different cuisines and cooking methods. Our RecipeFinder application will be an enhancement to the existing system and helps their users to search for recipes using their preferred ingredient and get recommended recipes based on the ingredient and nutritional value.
https://github.com/clairebrekken/CourseProject	ProgressReport.pdf	Progress Made Thus Far: We have started building a foundation for an application that will accurately classify sarcastic tweets. We started by brainstorming different classification modeling techniques, features to engineer, and researching existing white papers on more advanced applications. Through our initial research, we decided to implement a series of models ranging from naive to advanced including Decision Tree Classifier, Logistic Regression, Linear SVC, SVC with radial-basis kernel, K-Nearest Neighbors Classifier, Gaussian Naive Bayes, Random Forest Classifier, AdaBoost Classifier and Gradient Boosting Classifier. We first spot-checked these models (using default parameters) via 10-fold cross-validation on the entire training set. The results showed that linear models such as logistic regression and linear SVC worked as well as did the more complex nonlinear models. We then proceeded to perform hyper-parameter tuning (using 10-fold cross-validation and F1 score as the metric) to optimize the models. Equally as important as the modeling techniques, we needed to engineer features to capture the nuances that determine whether a tweet is sarcastic. So far, we have implemented features to represent the number of users tagged in the tweet, number of hashtags, length of tweet, and number of characters in the tweet. We also have tried to capture the sentiment of the tweet by creating features to represent ellipses, laughter, affirmations, negations, interjections, intensifiers, punctuation and emojis. After creating these features, we also used TSNE to visualize our features to get an idea of how they cluster and represent the data thus far. Overall, we have seen the best performance with the linear/regression models compared to more complex models such as AdaBoost or the Gradient Boosted Classifier. Our best metrics thus far are as follows: Precision = 0.6006, Recall = 0.8355 and F1 = 0.6988. Remaining Tasks: Our largest area for improvement of our current work is to engineer additional features to capture more of the nuances of sarcastic tweets. Since our complex models have not performed well thus far, we think this indicates that our features are not complex enough to capture characteristics of sarcastic tweets. We think our biggest wins will come from creating features based on the context of each tweet. Since sarcasm is often a response as part of a conversation, we are hoping this will improve model performance. We also plan on implementing features to represent text patterns in sarcastic tweets like parts of speech, n-grams, and topics represented in the tweet using LDA or a similar technique. We also think it would be interesting to implement a deep learning model after the additional feature engineering and evaluate how this performs against the other models. Additionally, we plan to explore an Ensemble model that combines the models that have performed well to see if we can optimize performance that way. In the end, we plan to have a robust suite of models that are trained on data with features that capture the nuances of sarcastic tweets. Challenges/Issues: Our main obstacle we are currently facing is that we are not reaching baseline performance. With the remaining tasks that we have detailed above, we believe we will be able to reach baseline accuracy with our final implementation. Our other challenge has been time. Between finishing up content for the course and preparing for the second exam, we have not been able to prioritize working on this project. Half of our team has now completed the exam so we are not concerned with time being a challenge as we complete this project over the next couple of weeks.
https://github.com/clairebrekken/CourseProject	project_proposal.pdf	Project proposal 1. What are the names and NetIDs of all your team members? Team members: * Claire Brekken (brekken2) - Captain * YiZi Xiao (yizix2) 2. Which competition do you plan to join? We plan to join the text classification competition. 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Yes we are prepared to learn the state-of-the-art neural network classifiers. The classifiers we have heard of and plan to explore are: * Logistic regression classifier * Deep neural network classifier (DNN) * Recurrent neural network classifiers * LSTM models * Convolutional neural network classifiers (CNN) Relevant experiences: * Logistic regression model to detect seizures in EEG data (medical device application) * Logistic regression to predict likelihood of orders happening in the future (supply chain/logistics application) * DNN for classify respiratory sounds (medical device application) * DNN for estimated time of arrival (supply chain/logistics application) * CNN for handwritten and image recognition (school work) 4. Which programming language do you plan to use? We plan to use Python.
https://github.com/clairebrekken/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection Demos: - feature_engineering_demo.zip - yizi_xiao_demo.zip - bert_demo.zip Code: Requirements: Python version >= 3.7 pip3 install -r requirements.txt Files: There are 3 Jupyter notebooks which contain the code developed for this project: feature_engineering.ipynb ml.ipynb bert.ipynb feature_engineering.ipynb This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. This notebook outputs data to be used in the conventional ML models. ml.ipynb: This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. It is also dependent on feature_engineering.py to have been run previously and that the output of that notebook has been saved. bert.ipynb This notebook is self-contained and can be run from top to bottom in sequence. The only dependency is the file utils_text_clf.py, which contains auxillary functions to process data. Project Specification Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/mlbernardoni/CourseProject	Final Submission.pdf	"CS410 Project Submission Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Link to Video: https://illinois.zoom.us/rec/play/p1k2d8OXy9zlMU52qAKeYekW6uxafweLBO2aqz6R_DGSbHkY06jjyYQT ueHspfeX-Fep54MHEkpEDyt1.ORGucOcp6XSypU7d?continueMode=true Initial Setup A lot of preparation when into getting the environment ready even before the ITMTF algorithm was analyzed in detail. First, data had to be collected, mined, prepped, and reduced into a form that could easily be loaded before each run. Furthermore topic mining and stats libraries had to be selected. Detailed analysis of these data curation steps and the library selection can be found in the Appendix. Detailed instruction of the steps to setup the python environment and the libraries used can be found in the last sections of the Appendix (and in the readme file). Creation of a Baseline After the data was procured and cleaned, and the python environment created, we first set about creating a baseline. A tricky prospect in any topic mining algorithm is selecting the number of topics. The Gensim library has logging that allowed us to take a reasonable guess at a preliminary topic number. We created baselines with 10, 15, 20, 25, and 30 topics. Using the logging module, we captured the coherence of each model. While the paper suggested that 30 topics was an appropriate number (section 5.2.3), the results of the coherence logging gave us a hint that 20 topics might also be a good number to analyze. (If interested, the logging code is in notebook: coherence_create_helper. The logging was also used to tune the number of passes and the number of iterations to show that the model would converge with our extreme settings for decay.) We then set about re-creating the algorithm in the paper. An analysis of the ""classical"" algorithm can be found in the section Classical ITMTF Algorithm. In addition to re-creating the algorithm, we set about creating an ""improved"" algorithm. 4.00E-01 4.50E-01 5.00E-01 10 15 20 25 30 Coherence Instructions on running iterations with both the classical algorithm and the ""improved"" algorithm are in the comments of the main notebook: ITMTF (Note: the ITMTF notebook is the entry point to running the algorithm. Detailed comments on the parameter set up can be found at the top of the notebook) ""Improving"" the algorithm After recreating the paper's algorithm, we set about seeing if we could improve upon it. While analyzing the paper, a sentence caught our eye. Section 4.2.3 ""While we observe correlations between non-textual series and both word streams and topic streams, we do not compute correlations for all word streams. Word level analysis would give us finer grain signals. However, generating all the word frequency time series and testing correlations would be very inefficient."" The documents are stationary, thus the word series would be static over time. The word streams, along with the Granger and Pearson statistics could be pre-processed. Our data mining had already collected the words per document, and the documents per time slice. It was not difficult to create word stream and pre-process all of the Pearson and Granger stats. (Please refer to the jupyter notebook itmtf_prerun_stats to see the python code used to pre-process the Granger and Pearson statistics.) (Please refer to the appendix for all of the libraries used in this project). In our ""classic"" algorithm, after we run the granger test on the topic coverage stream, we added one step. We multiplied the topic/word probability from the model with the p-value that we had pre- processed for the word streams. We normalized this new number. The hope was that the algorithm would ""nudge"" the model into selecting words with higher statistical relevance to the betting time series and not just the words the model had selected as the top words. This change did improve the algorithm, but we wished for something ""bigger"". (The final implementation of the classic algorithm in the project directory does contain this change.) The goal of the algorithm is to iteratively improve our confidence in the topics selected by the algorithm. The topic modeling algorithm will refine topic coherence, and the ITMTF algorithm would then use word level analysis as a prior to nudge the topic modeling software toward more significant word choice for the topic. A large part of the ITMTF algorithm is the splitting of significant topics with ""positive"" words placed in one topic, and ""negative"" words placed in another. We decide to try a different (and simpler) approach. One of the benefits of topic modeling is words can have different impact in different contexts. For example, the word ""rights"" can have one impact on the betting sequence data if used in the context discussing ""second amendment rights"", and a different impact in another context discussing ""civil rights"". The classical algorithm would separate out the word ""rights"" in one of these topics. We created a simplified algorithm where the topics created by the model were left in place, only at each iteration we would analysis on every word (based upon the pre-processed word stream statistics). How the new algorithm works: 1. Run the model 2. Run a Granger analysis on the topic coverage across the time series (same as the classical algorithm) a. note this analysis is not used to refine the model, but is used to gather the confidence score for the run b. keep the current state if the confidence score of this new run is higher than the previous high confidence score 3. For each topic, multiply each word probability by the pre-process p-value of that word from the word stream analysis, and normalize 4. Use the new topic/word probabilities as a prior into the next round We ran the algorithm using 20, 30, and 40 topics. Results are shown below: The runs resulted in remarkably similar scores. All 3 results seemed to hit a peak at the 11 or 12 iteration mark, and then all 3 models recovered to hit an ultimate peak at the14-16 iteration mark. We also ran the 30 topic model over 30 iterations, to identify if there were further peaks - there were not. Results shown below: 0.7 0.75 0.8 0.85 1 2 3 4 5 6 7 8 9 101112131415161718192021 20 Topic 0.7 0.75 0.8 0.85 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 30 Topics 0.6 0.7 0.8 0.9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 40 Topics 0.65 0.7 0.75 0.8 0.85 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 30 Iteration The next step was to analyze the word output to see what was happening to the desired result. We captured the top words for significant topics at the 11-13 peak, and the peak that occurs after 16. The top words are shown below: 11 iteration peak 16 iteration peak It became clear that after the first peak centered around 11th iterations, the model began to over fit the data with only words that were highly correlated to the betting data, such as ""gore"" and ""bush"". The conclusion was for this set of data, the peak that occurs around 11 iterations produces the best data. Next we turned our attention to the number of topics. We conducted 13 iteration runs with 40, 30, and 20 topics. The produced word data is shown below: 13 iteration runs with 20, 30, 40 topics. For this set of data, 30 topics produced the best data. With 30 topics relevant data was quite dense. Next we looked at the effect of decay. We ran 30 topic runs for 13 iterations with decay settings at .001 (strong effect from priors), .5 (the default for the Gensim library), .75 (the default for several other LDA libraries), and .9 (very low effect from priors) 0 G+: gore B+: bush G+: court G+: president G+: campaign G+: abortion G+: george G+: al B+: gores G+: justices 3 B+: bush G+: george B+: police G+: school G+: governor G+: family B+: glenn B+: crime G+: avenue G+: black 5 B+: bush G+: gore G+: education B+: tax B+: bushs G+: social G+: security G+: president B+: federal G+: george 8 B+: bush G+: gore G+: campaign B+: bushs G+: president G+: texas G+: george G+: governor G+: people B+: vice 12 B+: vidal G+: gore B+: bushnell B+: author G+: love G+: oct B+: city B+: theater B+: sex B+: page 21 G+: gore G+: clinton G+: president B+: gores G+: al G+: convention G+: campaign G+: speech B+: vice B+: bush 22 B+: bush G+: george G+: republican G+: gov G+: national G+: abortion G+: president G+: prochoice G+: forces B+: antiabortion 4 G+: gore G+: health B+: bush G+: president B+: vice G+: governor B+: care G+: children B+: lehrer B+: companies 7 B+: industry G+: aug G+: entertainmentG+: re B+: gores B+: city G+: oped G+: brooklyn G+: gorey G+: bartlett 16 B+: bush B+: bushs G+: texas G+: gore G+: governor G+: george G+: president B+: visited G+: campaign G+: hours 20 G+: president G+: gore G+: abortion B+: bush G+: court G+: rights B+: vice G+: clinton G+: al G+: george 22 B+: article B+: page B+: bush G+: front G+: george G+: oct G+: al G+: policy B+: yesterday G+: misstated 40 0 G+: gore G+: russia B+: agreement G+: arms B+: chernomyrdin B+: congress B+: weapons B+: russian G+: minister G+: law 3 G+: gore B+: industry B+: bush G+: entertainment B+: war B+: gores G+: day B+: pine G+: coffees B+: vidal 7 G+: gun B+: bush B+: control G+: gore B+: association G+: bill G+: texas G+: law G+: rifle G+: guns 8 G+: gore B+: bush G+: nader G+: campaign G+: president G+: al B+: vice B+: gores G+: george G+: vote 10 B+: bush B+: debates G+: campaign G+: gore G+: debate G+: commission G+: presidential B+: bushs G+: president B+: officials 19 B+: bush G+: texas G+: governor B+: bushs G+: governors G+: george G+: gore G+: hispanic G+: campaign G+: president 22 G+: administration B+: bush G+: gore G+: gorey B+: companies G+: issues G+: top B+: telecommunications G+: al B+: federal 24 G+: security G+: social G+: gore B+: bush B+: plan G+: campaign B+: gores B+: bushs G+: money G+: president 26 G+: gore G+: al B+: gores G+: convention G+: president G+: campaign G+: vietnam G+: democratic G+: speech G+: family 28 G+: debate B+: bush G+: gore G+: president G+: george B+: vice G+: al B+: lehrer G+: people G+: texas 29 B+: bush B+: bushs G+: george G+: friends G+: texas G+: campaign B+: father G+: family G+: people B+: time 30 B+: article G+: bushwick B+: yesterday B+: am G+: misstated G+: street G+: brooklyn G+: york G+: name B+: copies 31 G+: abortion G+: rights G+: president B+: bush G+: george G+: support G+: republican B+: decision G+: platform B+: nominee 33 B+: bush G+: gore G+: president G+: campaign G+: al B+: vice G+: george G+: clinton B+: gores G+: house 36 B+: lazio G+: clinton G+: george B+: donors B+: senate G+: republican G+: political G+: campaign G+: presidential G+: daley 30 1 B+: lazio B+: bush G+: george G+: york G+: clinton B+: lazios G+: hillary G+: gore G+: al G+: campaign 2 B+: bush G+: court G+: governor G+: george G+: gore G+: death G+: president G+: texas G+: troops G+: penalty 4 B+: drug B+: medicare B+: plan B+: prescription G+: health B+: bush B+: coverage B+: insurance B+: elderly B+: companies 10 G+: ms B+: women G+: im G+: husband G+: book G+: life G+: tipper G+: schiff G+: wife B+: bushnell 13 B+: bush B+: commercial G+: campaign G+: ad B+: advertisement G+: vietnam B+: screen B+: military B+: word B+: rats 20 G+: gore B+: bush B+: tax B+: plan B+: bushs G+: security G+: social B+: gores G+: president B+: cut 25 B+: bush G+: gore G+: george G+: al B+: letterman G+: debate B+: jokes B+: comedy G+: president G+: hes 26 G+: black G+: gore G+: al G+: president B+: article B+: bush G+: george B+: play B+: vice B+: copies 29 B+: bush B+: baseball B+: rangers G+: team B+: owners B+: war G+: george B+: owner B+: arlington G+: stadium 20 6 G+: bushwick G+: avenue B+: police G+: york G+: street B+: vidal G+: brooklyn B+: theater B+: university G+: gorey 12 G+: gore B+: bush G+: percent B+: gores G+: debate G+: voters G+: president B+: poll B+: vice G+: people 14 G+: gore G+: campaign B+: bush B+: fundraising G+: house G+: president B+: million G+: white G+: democratic B+: vice 16 B+: bush G+: texas G+: gore G+: abortion G+: george B+: issue G+: governor G+: president G+: gun G+: gov Decay output What we discovered was the new algorithm produced very similar data with differing decay settings. All runs had a peak at the 9-11 iterations, then varying fluctuations thereafter. At the lowest decay (highest impact from the prior) the fluctuations leading up to the 11th iteration peak were quite even and the can in confidence was steady. As one would expect, with the highest level of decay (lowest impact from the prior) the confidence remained quite flat. The intermediate levels of decay, .5 (the Gensim default) and .75 (the default for other LDA libraries) both showed improved confidence with a peak around the 11th iteration. The new algorithm proved to be quite effective for this set of data. Confidence steadily improved and reached a peak around the 11th iteration. After that peak, the algorithm began to over fit the data and produce topics with few, but highly significant, words. Furthermore, the new algorithm was successful without an overreliance on hyper parameter tuning. The number of topics had to be selected, but the default values for decay from the Gensim library produced results. The new algorithm did require more iterations than the classic algorithm, and the word streams had to be pre-processed. However, once tuned, the Granger analysis would only need to be conducted during the peak window (9-12 iterations) looking for the maximum confidence. 0.7 0.75 0.8 0.85 1 3 5 7 9 11 13 15 17 19 21 Decay .001 0.65 0.7 0.75 0.8 0.85 1 3 5 7 9 11 13 15 17 19 21 Decay .5 0.6 0.7 0.8 0.9 1 3 5 7 9 11 13 15 17 19 21 Decay .75 0.6 0.7 0.8 0.9 1 3 5 7 9 11 13 15 17 19 21 Decay .9 Classical ITMTF Algorithm The team was able to re-crate the classical algorithm from the paper. The only real variation was the use of LDA vs PLSA. The paper used PLSA, but did state that any topic mining algorithm would work. In fact, that was one of the thrusts of the paper, a general framework. As discussed below, Gensim LDA was selected as it performed an iteration in a reasonable time (2 minutes vs over a  1/2  hour for the python PLSA algorithm). Below are some of the plots produced by our implementation of the algorithm. (Please refer to the notebook Classic_Baseline_Plot) When compared to the graphs produced in the paper, our implantation produced similar results. The average causality confidence steadily increased with both implementations, with higher prior impact showing the best results. Both implementations had somewhat flat purity. We are unsure why the published results had such poor baseline numbers. The poor baseline numbers in the published word made the first iteration appear like a substantial jump. Our algorithm did show a large improvement for the first iteration (especially with the larger prior influence, as in the published paper), just not as dramatic. Another variable would be the data cleaning. One of the topics published has ""pres"" ""al"" ""vice"" as its top 3 words. This topic clearly is about Vice President Al Gore. It seems the words ""Gore"" and ""Bush"" were removed as part of the data cleaning for the paper. This would have a large impact on the model, as the p-values for the words ""gore"" and ""bush"" were the largest of all words. Appendix - Data mining and cleansing The python code used to clean the data can be viewed in the itmtf_cleaning jyputer notebook. Step 1: Data mining First we mined the raw xml data and produced a .txt for each document that had a paragraph with the words ""Gore"" or ""Bush"". We only included the paragraphs with the key words, but we kept the document intact, that is if a doc had 2 paragraphs with either the word ""Bush"" or ""Gore"" the output would be one document with those 2 paragraphs. Note this is just prep work and is not included in the project for size considerations. Step 2: Data cleansing - .\LDA_data\LDAData.csv For each file in the mined directory, we split the string into words. For each word we made each word lowercase, stripped out any character that was not alpha, and removed all stop words. We used stop words from: Onix Text Retrieval Toolkit Stop Word List 1: https://www.lextek.com/manuals/onix/stopwords1.html . We added the results for each document in a .csv file .\LDA_data\LDAData.csv. Each document is a row: cell 1 contains the year; cell 2 contains the month; cell 3 contains the day; cell 4 contains the cleansed text string of the document We also created a csv file .\LDA_data\vocabulary.csv which contains unique vocabulary words in cell 1 and the count of the term in cell 2. Step3: Data reduction - .\LDA_data\LDAreduced.csv Using the vocabulary csv .\LDA_data\vocabulary.csv from step 2, we removed any word that only occurred once or twice (all words with counts over 2 were kept). We produced a csv file .\LDA_data\vocabularyreduced.csv which contains the new list of unique vocabulary words. Using the new vocabulary, we created a new csv .\LDA_data\LDAreduced.csv in the same form as the un-reduced csv. Step 3: Word coverage per time slice - .\LDA_data\wordseries.csv Using the vocabularyreduced.csv and the LDAreduced.csv we pre=processed a csv that contains the word coverage per time slice - .\LDA_data\wordseries.csv. The first row is a header row that contains the unique words in the vocabulary, this row is not used in the algorithm, but makes the file human readable. The first column in each row contains the time slice. All subsequent columns contain the word coverage during that time slice. This pre-processed file will be used in the ITMTF algorithm. Current data mining and cleansing files in the project: .\LDA_data\LDAData.csv cleaned data .\ LDA_data\vocabulary.csv cleaned data's vocabulary .\ LDA_data\LDAreduced.csv removed words occurring 1 or 2 .\ LDA_data\vocabularyreduced.csv removed data's vocabulary .\ LDA_data\LDAwordseries.csv words counts per time slice Step 4: Betting information The betting data is publicly available at the following site: https://iemweb.biz.uiowa.edu/closed/pres00_WTA.html Python was used to clean the data, and smooth the data into both 3 day and 5 day averages. The python code can be viewed at the following site: Bush Vs Gore Betting Data - Google Drive Topic Mining Algorithm Selection The paper indicates that the LDA algorithm was used. As such, we attempted to us LDA. First we discovered the LDA algorithm pypi https://pypi.org/project/plsa/. The algorithm worked well in our test data sets, and had excellent data visualization techniques. We identified where to add new topics in the library's python code with the iteration feedback. However when we ran the full cleaned data, this library took over 12 hours to complete 1 model. One of our team members wrote a LDA algorithm in C++. The C++ algorithm was significantly faster. However, running the entire corpus caused memory issues. Time does not permit adding data swapping to disk. Following the lead of other teams discussed on Piazza, we then selected Gensim's LDA algorithm for topic mining https://radimrehurek.com/gensim/models/ldamodel.html#usage-examples. This algorithm does not have memory issues, and completes in a reasonable amount of time (under 10 min on one of team member's home desktop). Instructions for adding this library into an Anaconda environment is in the appendix. Appendix - Libraries used Gensim Python LDA - https://radimrehurek.com/gensim/models/ldamodel.html SciPy's pearson r - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html statsmodels granger causality tests - https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.grangercausalitytests.html pyLDAvis - https://pyldavis.readthedocs.io/en/latest/index.html glob - https://docs.python.org/3/library/glob.html Matplotlib - https://matplotlib.org/ Library tested but not used: pypi.orgs PLSA - https://pypi.org/project/plsa/ Appendix - Environment setup For Windows: Open an anaconda prompt, navigate to the project's directory and type: conda env create -f ITMTF.yml The created environment will be called ""Gensim"", when you open the notebook, you will have to change kernels to Gensim. See troubleshooting note below. Adding Gensim LDA library to an Anaconda environment manually: Optional - create a new Anaconda environment to install the Gensim package: 1. Open Anaconda Navigator 2. Select Environments 3. Create an environment (i.e. ""gensim"") Install genism in Anaconda 1. Open the Anaconda command prompt 2. If you created a new environment in the previous step: a. Activate the newly created environment if you created one (""Activate gensim"") b. Run: conda install nb_conda_kernels (Proceed Y) c. Run: python -m ipykernel install --user --name myenv --display-name ""Gensim"" (you can use any display name you wish, this is what will show up on Jupyter Notebook) d. Run: pip install environment_kernels 3. Run: pip install --upgrade genism 4. Run: pip install -upgrade pyldavis 5. Run: pip install -upgrade glob2 6. Run: pip install -upgrade matplotlib Start Jupyter Notebook in the directory you downloaded the project (if not your default) 1. Open the Anaconda command prompt 2. Start Jupyter Notebook in the directory you have downloaded this project (i.e., ""jupyter notebook c:\projects"") TROUBLESHOOTING NOTE: When you open the project in Jupyter Notebook, look to the upper right and you can see what environment the project is running. If this is not the environment you just set up for Gensim, select Kernel from the notebook menu and select Change kernel, and change to the correct kernel."
https://github.com/mlbernardoni/CourseProject	Team Progress Submission.pdf	"CS410 Project Progress Report Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Tasks completed:  Data mining and cleaning of the text documents - completed.  Data mining and cleaning of the betting probabilities - completed.  Production of the word/time slice coverage for the time period - completed.  Topic mining algorithm selection - selected Gensim LDA.  Topic mining on the entire corpus - LDA algorithm implemented, baseline created  Production of the topic coverage from one run of LDA - completed.  Adding new topics back into the LDA algorithm after iteration - completed.  Ability to iterate over the prior 3 steps for the entire time period - completed  Understanding the ITMTF algorithm - completed.  Completed the setup instructions for Gensim in anaconda - completed. Tasks to do:  Code the time sequence scoring function  Code the word analysis scoring function  Code the topic splitting  Visualization of the final data Challenges:  The article has a section on m. A m of 0 means the prior is not considered. The higher the m the stronger the prior. Gensim LDA has a concept of decay (between 0 and 1). Like a m set to 0, if the decay is set to 1, the prior is not considered (similar to a m of 0). The Gensim documentation states that a decay set between .5 and 1 will converge. We ran a test with a very small decay (.001), and the topic we added remained virtually intact. We are still discovering how the decay changes the ITMTF algorithm, hopefully similar to m.  We are still discussing the topics to be carried forward to the next iteration. The article proposes a ""variable"" topic approach and discusses keeping ""buffer"" topics. Not a challenge, but something we are looking at. Detailed discussion of these steps follow: Detailed discussion Data mining and cleansing Step 1: Data mining First we mined the raw xml data and produced a .txt for each document that had a paragraph with the words ""Gore"" or ""Bush"". We only included the paragraphs with the key words, but we kept the document intact, that is if a doc had 2 paragraphs with either the word ""Bush"" or ""Gore"" the output would be one document with those 2 paragraphs. Note this is just prep work and is not included in the project for size considerations. Step 2: Data cleansing - .\lda_data\LDAData.csv For each file in the mined directory, we split the string into words. For each word we made each word lowercase, stripped out any character that was not alpha, and removed all stop words. We used stop words from: Onix Text Retrieval Toolkit Stop Word List 1: https://www.lextek.com/manuals/onix/stopwords1.html . We added the results for each document in a .csv file .\lda_data\LDAData.csv. Each document is a row: cell 1 contains the year; cell 2 contains the month; cell 3 contains the day; cell 4 contains the cleansed text string of the document We also created a csv file .\lda_data\vocabulary.csv which contains unique vocabulary words in cell 1 and the count of the term in cell 2. Step3: Data reduction - .\lda_data\LDAreduced.csv Using the vocabulary csv .\lda_data\vocabulary.csv from step 2, we removed any word that only occurred once or twice (all words with counts over 2 were kept). We produced a csv file .\lda_data\vocabularyreduced.csv which contains the new list of unique vocabulary words. Using the new vocabulary, we created a new csv .\lda_data\LDAreduced.csv in the same form as the un- reduced csv. Step 3: Word coverage per time slice - .\lda_data\wordseries.csv Using the vocabularyreduced.csv and the LDAreduced.csv we pre-processed a csv that contains the word coverage per time slice - .\lda_data\wordseries.csv. The first row is a header row that contains the unique words in the vocabulary, this row is not needed by the algorithm but was used during debugging. The first column in each row contains the time slice. All subsequent columns contain the word coverage during that time slice. This pre-processed file generates the word coverage over time used in the ITMTF algorithm. Current data mining, cleansing, and pre-processed files in the project: .\lda_data\LDAData.csv cleaned data .\lda_data\vocabulary.csv cleaned data's vocabulary .\lda_data\LDAreduced.csv removed words occurring 1 or 2 .\lda_data\vocabularyreduced.csv removed data's vocabulary list .\lda_data\wordseries.csv words counts per time slice Topic Mining Algorithm Selection The paper indicates that any topic mining algorithm can be used, but the author used the PLSA algorithm. As such, we attempted to use PLSA. First we discovered the PLSA algorithm pypi https://pypi.org/project/plsa/. The algorithm worked well in our test data sets, and had excellent data visualization techniques. The code did not have out of the box ways to add topics and topic priors, but the code was available. So we identified where to add new topics and where to set topic priors in the library's python code. However when we ran the full cleaned data, this library took over 12 hours to complete 1 model, and we would have to iterate 6-7 times. One of our team members wrote a PLSA algorithm in C++. The C++ algorithm was significantly faster. However, running the entire corpus caused memory issues. Time does not permit adding sparse matrix processing or data swapping to disk. Following the lead of other teams' discussion on Piazza, we then selected Gensim's LDA algorithm for topic mining https://radimrehurek.com/gensim/models/ldamodel.html#usage-examples. This algorithm does not have memory issues as it works on document chunks, and completes in a reasonable amount of time due to its use of sparse matrix processing (under 10 min on one of team member's home desktop). As an added bonus, the library had a way to add new topics and priors. Instructions for adding this library into an Anaconda environment is in the appendix. Algorithm Iteration Completed The first step, loading the pre-processed files into arrays, is complete. The documents are loaded into memory. The pre-processed file had the date in the first cell. While the document is loaded into memory an array is created of the docs per time slice. This array will be used to create the topic coverage per iteration. The word coverage preprocessed file is loaded into an array that can be used by the ITMFT algorithm. The loaded docs are loaded into the Gensim's dictionary format. While loading the documents, a map is created that maps the index from the pre-processed vocabulary coverage to Gensim's token index for that word. This map is used to create the new topic matrix in the correct sequence, when new topics are returned from the ITMFT algorithm. A Gensim corpus is created from the Gensim dict. As the documents do not change during the ITMFT algorithm (only the topic and topic priors), this corpus can be re-used throughout the entire ITMFT algorithm. Running the Gensim LDA model is coded. The ITMFT iteration is coded: 1. The preprocessed word coverage is passed into the algorithm 2. The document/topic probabilities are pulled from the LDA model 3. Using the document to time slice matrix, a topic coverage matrix is created from the document/topic probabilities. 4. These 2 coverage matrices are passed into the ITMFT scoring function 5. The ITMFT scoring function will create a new matrix of new topics and word probabilities prior 6. The coding of adding this matrix back into the LDA model is complete. To do: 1. Code the ITMFT scoring function 2. Code the ITMFT topic creation function Appendix Adding Gensim LDA library to an Anaconda environment Optional - create a new Anaconda environment to install the Gensim package: 1. Open Anaconda Navigator 2. Select Environments 3. Create an environment (i.e. ""gensim"") Install genism in Anaconda 1. Open the Anaconda command prompt 2. If you created a new environment in the previous step: a. Activate the newly created environment if you created one (""Activate gensim"") b. Run: conda install nb_conda_kernels (Proceed Y) c. Run: python -m ipykernel install --user --name myenv --display-name ""Gensim"" (you can use any display name you wish, this is what will show up on Jupyter Notebook) d. Run: pip install environment_kernels 3. Run: pip install --upgrade gensim Start Jupyter Notebook in the directory you downloaded the project (if not your default) 1. Open the Anaconda command prompt 2. Start Jupyter Notebook in the directory you have downloaded this project (i.e., ""jupyter notebook c:\projects"") TROUBLE SHOOTING NOTE: When you open the project in Jupyter Notebook, look to the upper right and you can see what environment the project is running If this is not the environment you just set up for Gensim, select Kernel from the notebook menu and select Change kernel, and change to the correct kernel."
https://github.com/mlbernardoni/CourseProject	Team Project Proposal.pdf	Team Project Proposal Topic: Reproducing a Paper: Mining causal topics in text data: Iterative topic modeling with time series feedback. Team: PYM First Last email Pallavi Ravada pravada2@illinois.edu Yash Skhwani yashas2@illinois.edu Michael Bernardoni mlb12@illinois.edu Captain: Michael Bernardoni mlb12@illinois.edu GitHub fork link: https://github.com/mlbernardoni/CourseProject Reproduce the following paper:  Subtopic: Causal topic modeling o Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Programming language: The team will initially proceed programming with Python. However if performance becomes an issue C++ will be evaluated. Tableau may be used for data visualization of the resulting datasets. Dataset: The dataset can be found at the Linguistic Data Consortium ( https://www.ldc.upenn.edu ). The team captain (mlb12@illinois.edu ) has applied to the consortium and is await approval from: University of Illinois at Urbana-Champaign, Beckman Institute. This request has also been posted on Piazza. Once approval is obtained access to the dataset will be provided.
https://github.com/hetadesai26/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Redstone-WB/CS410-guava	CS410_progress_report.pdf	<CS410 : Project Progress> Project Topic  Project topic : (Option 1) Reproducing a paper: Latent Aspect Rating Analysis Project Schedule (Table 1) Date TODO 11.20 ~ 11.28 Paper review 11.29 Progress report 11.30 ~ 12.01 Source review 12.02 ~ 12.08 Source implementation 12.09 ~ 12.11 Result analysis & Documentation 12.12 Source code & Documentation submission Team members 1. Hongseok ha (netID : hh23), with administrative duties 2. Changsoo Kim (netID : ck37) Progress Report 1. Which tasks have been completed? (Progress made thus far) A. Paper review i. We went through the paper to get a holistic understanding and found that this paper suggests a unified framework LARAM, which is improves LARA and enables aspect rating analysis without knowledge of the target domain. ii. LARAM has two components: 1) an aspect modeling module, and 2) a rating analysis module similar to the Latent Rating Regression Model (LRR) used in LARA. iii. Once we infer the latent aspect assignment z and aspect weight a with the given model Th = (e, g, b, m, S, 2 ), we can estimate corpus-level parameters using the Expectation Maximization algorithm. B. Find materials that help understand the paper i. We found the author's original source from his homepage. This would be very helpful to understand the process more precisely. However, it is implemented in Java, so we need to convert it to Python. ii. URL: http://sifaka.cs.uiuc.edu/~wang296/ 2. Which tasks are pending? (Remaining tasks) A. Python code implementation i. As there are two components and we have two members, each member will be in charge of one module. 1. Hongseok Ha: Aspect modeling module 2. Changsoo Kim: Rating analysis module B. An overall schedule is shown in Table 1. 3. Which tasks have been completed? (Any challenges/issues being faced) A. Ambiguity of aspect modeling part: The aspect modeling module behaves similarly as LDA or sLDA, but it has different assumptions. We cannot spot the big difference so far, so we sent an email to the TA and we are going to ask on Piazza as well. If there are only subtle difference, we are going to use LDA instead and see the difference between the paper's result and ours. B. Python portability: We will convert the author's original code in Java to Python. These differences can lead to slight differences in results. C. Experiment results: In the paper, there are several comparisons, such as LDA vs sLDA vs LARAM, LDA+LRR vs sLDA+LRR vs LARAM, Bootstrap+LRR, LARAM, and so on. If we were to compare the number of all cases, it would take a lot of time. We will only compare the paper's suggesting result and our result.
https://github.com/Redstone-WB/CS410-guava	Execution Guidelines.docx	<Execution Guidelines> CS410 Final Project, 2020, Fall Author : Changsoo Kim, Hongseok Ha Prerequisites Python : version over 3.5 Python packages : numpy, lda If you want to use np.load to load .npy files, please use numpy version 1.16.1 Data Default data(small, to upload git repository) directory : './data/yelp_sanitation_data/' To try another dataset, please change line 16 of 'Main.py' file. Output Example LDA output (5 topics) LRR output Execution Steps Clone the repository Execute the Main.py, using command 'python Main.py' If you want to change the LDA topic parameter (k), please modify line 24 of 'Main.py' file. (n_topics)
https://github.com/Redstone-WB/CS410-guava	FinalProjectProposal.docx	<CS410 : Project Proposal> Team name : guava Team members : Hongseok ha (netID : hh23), with administrative duties Changsoo Kim (netID : ck37) Project topic : (Option 1) Reproducing a paper: Latent Aspect Rating Analysis Programming language that will be used : Python Dataset Availability Two datasets (the hotel review dataset and the Amazon Mp3 review dataset) that are used in the paper are both available in the link below. Dataset Link : http://sifaka.cs.uiuc.edu/~wang296/Data/index.html However, for the Amazon MP3 review dataset, the number of reviews are different in the paper and the actual dataset. (the paper : #16,680, the dataset : #55,740) We will see if we can get results similar to the paper. The paper (Latent Aspect Rating Analysis) deals with the methodology of mining latent topical aspects, without pre-specification of aspect keywords (hashtags, ... ). LARAM would be helpful to cluster user reviews with various aspects, making it easy for other users to retrieve topics that they want. Also, reproducing this paper would be helpful for us to understand overall contents of CS410.
https://github.com/Redstone-WB/CS410-guava	Project Review.docx	<Project Review> CS410, 2020, Fall Authors : Changsoo Kim, Hongseok Ha Overall LARAM(2011) improved LARA(2010) in that it allows finding latent topics without specifying the seed words of the topics. LARAM can be divided into two modules, which are 1) finding latent topic aspects (aspect modeling module) and 2) finding ratings on each identified aspect (rating analysis module). Although we tried to understand the details of LARAM based on the original author's source code and python code from other sources, it was difficult for us to figure out all the details of the paper. Thus, as we mentioned in our progress report, we tried to implement the paper by replacing the aspect modeling module with LDA, and the rating analysis module with LRR (Latent Rating Regression). However, it was difficult for us to link the LDA results to LRR. Also, although we converted the java code of the original author to implement a python-version of LRR (which we could not find other references), there were subtle numeric differences in our intermediate results. We could not be sure whether the previous mismatch was the fundamental cause of following failures, but many unknown causes leaded us to a problem in which LBFGS (which was used as an optimizer, to minimize beta parameters) fails in line search, so that the beta parameters could not converge. In conclusion, the beta parameters could not be updated, so that the EM algorithm could not worked properly, resulting the maximization of log-likelihood (that we expected) could not happen. We found that we should try to understand the paper more sufficiently, before starting the code implementation. Also, to save time, utilizing libraries and packages would be very helpful. With this project, we were able to empathize with researchers who were making great efforts to advance their research in their fields. Difficulties There were ambiguous words, that we could not be sure whether they should be included in stopwords. For instance, the phrase 'n't' was not included in stopwords of the nltk package. Although it seems to mean 'not' with some verbs, it would mean nothing, without any verbs. Although the python package 'scipy' provides L-BFG-S option in an optimizer, structures of parameters were different, so that using the function imported from scipy was difficult. That's why we chose to convert the original author's java code of LRR. However, it resulted in the numerical mismatch in intermediate results. In LARAM, the Z values from aspect segmentation should be utilized in LRR, to improve the overall performance, but we could not implement that part, for lack of understanding of the paper and limit of time. We found that it is a tough task to implement a paper to well-structured codes only by looking at the formula and diagram in the paper. Additional resources (such as the author's presentation materials, that provides details and easier explanation) seems to be helpful to implement the paper in detail.
https://github.com/Redstone-WB/CS410-guava	README.md	CS410 : CourseProject TERM INFO : UIUC, CS410, 2020, FALL Authors : Changsoo kim, Hongseok Ha Guideline for the execution : Execution Guidelines.docx Review for the whole project : Project Review.docx Execution Video : https://uillinoisedu-my.sharepoint.com/:v:/g/personal/ck37_illinois_edu/EbHMSN_G_cJFrKHp7k164TwBjtK73jVoCEMs8Bs7yL_DYw?e=j3w4Eq References LARAM (java) : http://sifaka.cs.uiuc.edu/~wang296/ Preprocessing : https://github.com/tonyzhang1231/LARA_Python ETC : https://github.com/ericcds/LARAM_Python
https://github.com/richameher/CourseProject	Progress Report.pdf	Richa Meherwal CS 410 Project Progress Report Progress made thus far 1. Set up Environment. Installed Python 3, made requirements.txt file and installed compatible libraries 2. Data Pre-Processing in progress- removed stop words, removed latin words. Tokenized words and removed punctuations. Performed POS tagging and extracted adjectives and nouns Remaining Tasks 1. To execute LDA and extract topics 2. To perform sentiment analysis on each topic at a dataset level Challenges faced 1. Library incompatibilities with Python version 2. UTF-8 encoding err - added diff type of encoding to read_csv in pandas 3. Spacy library incompatibility 4. No module named en-core-web-sm 5. Extracting meaningful words 6. Mapping lemmatisation functions to every row in data frames
https://github.com/richameher/CourseProject	Project Documentation.pdf	"Richa Meherwal- CS 410 Course Project Documentation Presentation Available at https://mediaspace.illinois.edu/media/t/ 1_ammbs24f An overview of the function of the code Code can be used to do an aspect based sentiment analysis. As seen in the code, we first tokenise all reviews. Then extract bigrams NN-ADJ pairs to form a word cloud and visualise the features that stand out the most. We also extract unigrams that are NN as the aspects to use for sentiment analysis. Note that in order to train our classifier , we use the Ratings column from the Hotel dataset and then label our aspects with a pos,neu,neg sentiment. We then visualise the aspects and the associated sentiments using a bar plot. We the repeat this process over Airbnb reviews dataset and we use the sentiment classifier trained before to classify the sentiments of the aspects extracted. Note that Airbnb does not have ratings so it wouldn't be possible for us to retrain the classifier. Software Implementation and Usage 1. (Optional) Create a Python3 virtual environment python3 -m venv py3-env-final-proj 2. (Optional) Activate virtual environment- source py3-env-final-proj/bin/activate 3. pip install jupyter 4. Install ipykernel in this environment- python3 -m ipykernel install --user --name=final-proj (final-proj will be used as env in jupter notebook) 5. pip install -r requirements.txt 6. Start Jupyter-notebook from shell using command : jupyter notebook 7. Download repository and open the Test.ipynb file 8. Switch to final-proj kernel defined in step 4. 9. Change the file path to where the preprocessed files are i.e. under folder data and Run all the cells in the notebook Note for testers There is a joblib file that you can use to test the sentiment classifier. The classifier has been trained on HotelReviews dataset. Check Step 5.c under Hotel Review Analysis in Final_Project V4.ipynb or 4.b in Test.ipynb. I have already shown how to use it in the video presentation. Entire Code including the preprocessing and training sentiment classifier can be found in - Final_Project V4.ipynb. You can also view this ipynb file using nbviewer - https://nbviewer.jupyter.org/ github/richameher/CourseProject/blob/main/code/Final_proj%20V4.html Modified Code for Testers can be found in- Test.ipynb Final Results Understanding Plots and Graphs [Hotel Review WordCloud] Use WordCloud to visualise bi-grams. NN-ADJ pairs are extracted from reviews and TF-IDF is used to retrieve top n bigrams. There will also be a bar-plot associating the sentiment with every unigram NN keyword extracted with the probability of the sentiment. [Hotel Review Aspect Sentiment Graph] In the Wordcloud we can observe that people tend to talk about the quality of rooms. Features like safety is usually associated with the hotels than Airbnbs. Also Hotels have their own website , so people also talk about the online booking system. As for the bar plot, we can see that hotels have ""theft"", ""suite"" aspects that have been associated with negative sentiment. Also the highest positive sentiment is observed among aspects like ""room"", ""view"" and ""manager"" What is completed and what could be better? I have successfully been able to analyze the aspects that drives people to chose Airbnb over Hotels and vice-versa. With Airbnb, like we can see in the graphs, the motivation is driven by finding an ""affordable"" yet comfortable stay. People care about ""location"" and ""worth for money"". With hotels, people expect ""luxury"" in terms of ""big rooms"", ""spacious bathrooms"", ""views"" and so on. However, the sentiment classifier could have been better if we had an available labeled dataset for airbnb reviews. I also realised that using LDA for bi-grams does not work well but specialised algorithms for Bi-gram topic extraction can be used in future. LDA for unigrams also did not group the categories very well , but top weighted words could have been considered. Therefore, I used TF-IDF to find the key aspects and only used nouns to do so."
https://github.com/richameher/CourseProject	Proposal.pdf	Richa Meherwal Project Proposal CS 410 Fall 2020 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administra<ve du<es than team members. Individual Project Name- Richa Meherwal NeAd- meherwa2 Captain-Richa Meherwal 2. What is your free topic? Please give a detailed descrip<on. What is the task? Why is it important or interes<ng? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic Using Topic Mining and SenAment Analysis to compare customer level saAsfacAon in Airbnb vs Hotels Problem Statement With the rise of Airbnb, travellers are usually choosing this new type of accommodaAon over Hotels. Similarly some travellers always chose to stay in expensive Hotels. What is it that aNracts tourists to hotels over Airbnb and vice-versa. I chose this topic because I was interested in learning what level of customer saAsfacAon present in Airbnb users vs Hotels. I parAcularly wanted to compare the customer level saAsfacAon features involved with these two types of accommodaAon using topic extracAon and senAment analysis. Task To compare the customer saAsfacAon between Airbnb and Hotels using LDA and SenAment Analysis Datasets hNps://www.kaggle.com/mrinaal007/reviews hNps://zenodo.org/record/1219899#.X5Uic0Izba4 Tools jupyter notebook Nltk toolkit genism Approach 1. Complete the pre-processing of the datasets. This includes tokenisaAon, removing stop words, normalisaAon. 2. Use LDA to first extract the common topics that the customers review about in both the datasets. 3. Do a senAment analysis on each of the sentences containing the extracted topic and assign a senAment to it. Gather the associated senAment and the topic over each dataset. 4. Now under each accommodaAon type we should be able to visualise the topics that it is posiAvely credited for by the reviewers and also negaAvely. My expected outcome is to show topic and senAment level comparisons for each type of accommodaAon (Airbnb or Hotel). The idea is to see which topics are associated posiAvely or negaAvely with each of the accommodaAon types and to gain insight into where each of these services perform beNer than the other. EvaluaAon To check the senAment analyser, I would compare the senAment associated with the sentence/review to the raAng given. To check topic extracAon worked well, I will use wor2vec on all the common topics and see if they separate well. 3. Which programming language do you plan to use? Python 4. Please jus<fy that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the es<mated <me cost for each task. Tasks and hours 1. PreProcessing data - 5-6 hrs 2. Research methods for SenAment Analysis and Topic ExtracAon and deploy it - 9-12 hrs 3. Post Processing and visualising the final data - 6-7hrs 4. Cleaning code ,demo, documentaAon - 4-5 hrs N=1 Total esAmated work hours- 24- 30 hrs
https://github.com/richameher/CourseProject	README.md	"CourseProject Code can be used for Aspect-based Sentiment Analysis Extract NN-ADJ bigrams Extract NN unigrams Extract top n-grams using TF-IDF Sentiment Anaysis on Reviews on Aspects (unigrams or bigrams) Visualize sentiments on keywords extracted using barplots/Wordcloud Presentation and Tutorial Available at - https://mediaspace.illinois.edu/media/t/1_ammbs24f?st=0 Complete documentation and self-evaluation available in Project Documentation.pdf Code folder contains Final Project V4.ipynb (whole source code) Test.ipynb (Code for testers to run) requirements.txt (libraries to install) sentiment_analyzer.joblib (trained sentiment classifier model) Data Folder contains preprocess_airbnb.csv (Airbnb data with necesaary n-grams extracted with source code to be used for topic extraction and sentiment analysis) preprocess_hotel.csv (Hotel data with necesaary n-grams to be used for topic extraction and sentiment analysis) You can view my ipynb notebooks directly using nbviewer https://nbviewer.jupyter.org/github/richameher/CourseProject/blob/main/code/Final_proj%20V4.html Software Implementation and Usage 1. (Optional) Create a Python3 virtual environment python3 -m venv py3-env-final-proj 2. (Optional) Activate virtual environment source py3-env-final-proj/bin/activate 3. pip install jupyter 4. Install ipykernel in this environment- python3 -m ipykernel install --user --name=final-proj (final-proj will be used as env in jupter notebook) 5. pip install -r requirements.txt 6. Start Jupyter-notebook from shell using command : jupyter notebook 7. Download this github repository and open the Test.ipynb file 8. Switch to final-proj kernel defined in step 4. 9. Change the file path to where the preprocessed files are i.e. under folder data and Run all the cells in the notebook Methodology (Code in code/Final Project V4.ipynb) Read Original Datasets (Check Proposal.pdf Data for the links) Clean Text- Tokenize, Remove punctuations, tabs, whitespaces, stopwords, common words Extract bigrams- Create a bigrams column to extract all afjacent pairs of bigrams from review/text Create a bigram_list column and keep bigrams that are NN-ADJ pairs Create a unigram_list column and keep only unigrams that are NN Train a Logistic Regression classifier on sentiment and reviews of the hotel dataset as only hotel dataset has ratings (map ratings to sentiments first) Use WordCloud to visualize frequent bigrams Use TF-IDF to extract top n keywords from unigrams or bigrams Use the trained sentiment classifier to classify the sentiment for the keywords Plot a bar graph, with sentiment as labels, keywords and sentiment probability/topic extent on Y-axis Results [Hotel Review Bigram WordCloud] [Hotel Review Sentiment-Topic Extent Bar Plot] In the Wordcloud we can observe that people tend to talk about the quality of rooms. Features like safety is usually associated with the hotels than Airbnbs. Also Hotels have their own website , so people also talk about the online booking system. As for the bar plot, we can see that hotels have ""theft"", ""suite"" aspects that have been associated with negative sentiment. Also the highest positive sentiment is observed among aspects like ""room"", ""view"" and ""manager"" (Note- we can also observe the sentiment probability on y-axis instead of topic extent, Check Test.ipynb) Contribution Completed By Richa Meherwal Free Topic- Using Topic Mining and Sentiment Analysis to compare customer level satisfaction in Airbnb vs Hotels"
https://github.com/liuyuxiang512/CourseProject	Progress_Report.pdf	"CS410 Progress Report Dancing Text: Yuxiang Liu (yuxiang@illinois.edu, leader), Hongfei Ma (hongfei7@illinois.edu) 1. Progress For the task of identifying in-demand skills, we have written an auto-crawler to crawl tweets related to a given subject ""Computer Science"" from a social media, Twitter, and saved attributes of tweets to a file. Due to the rules of Twitter, there is a rate limit with the crawling so we cannot get as many data as we want. However, we can obtain the most-recent tweets related to the CS subject, which helps to discover emerging keywords or topics. We have also processed the crawled tweets, and analyzed the hashtags, which represent the themes of corresponding tweets, to preliminarily extract top-rank topics of these tweets. For the task of displaying relative documents of each topic, we have finished crawling PDF slides of several courses in UIUC. However, due to the difference of course websites, crawlers are running independently in different python scripts. We have also investigated some common ranking algorithms, including BM25, Jaccard index, cosine similarity and some provided algorithms in MeTA Toolkit. 2. Challenges Since there are many tweets without hashtags, we cannot extract topics from such tweets, but it helps to utilize hashtags in extracting topics. Hence, instead of designing a topic discovery algorithm for general texts, we need to propose an algorithm which can extract topics from both tweets with and without hashtags, and combine these topics to get the final top-rank topics among these tweets. Another challenge is how to demonstrate that the extracted topics are truly popular in these tweets, or how to evaluate the performance of our algorithm. Even if we have crawled tweets from Twitter, we do not know what topics are popular in these tweets. Evaluation of different ranking algorithms seems time-consuming because it is hard to get all-round training dataset. In addition, the form of displaying our ranking results is still undecided. 3. Remaining work To identify in-demand skills, the remaining work is to propose a topic discovery algorithm specifically for tweets, and design an evaluation method to demonstrate the performance of this algorithm. To display relative documents of each topic, the remaining work is to design a workflow that can automatically crawl documents from various course websites, and evaluate the existing ranking algorithms' performance in our dataset."
https://github.com/liuyuxiang512/CourseProject	Proposal.pdf	CS410 Project Proposal Dancing Text: Yuxiang Liu (yuxiang@illinois.edu, leader), Hongfei Ma (hongfei7@illinois.edu) 1. Proposed Study We plan to improve the EducationalWeb System through automatically creating teaching materials for in-demand skills, which is an extended version of the existing system. To satisfy the increasing demands from people who are looking for high-quality education but cannot get access to it whenever necessary, we aim to identify in-demand skills and create lectures and tutorials for these skills. To identify in-demand skills, we will crawl data from social media and identify emerging keywords or topics through topic extraction. To create lectures and tutorials for skills, we will recommend most relevant slides given specific topics. In this project, we plan to mainly use python and javascript languages. 2. Impacts Of The Proposed Work Through this project, we would enable emerging topic identification and slides recommendation for topics, which will help users who are interested in getting high-quality education materials at any time. 3. Project Description 3.1 Identifying in-demand skills We plan to crawl tweets from Twitter. A hashtag is a type of tag that people usually treat as the theme of the corresponding tweet, so we would directly consider hashtags as the topic of tweets. Then, we would use features of tweets such as retweets, replies, or likes to identify which hashtags are emerging or popular keywords/topics. In particular, we may limit our topics within computer science or STEM disciplines to reduce/avoid skill-irrelevant topics. After we obtain these skill-related topics, we would use them in the next part of creating corresponding lectures and tutorials. 3.2 Creating lectures and tutorials for skills In this part, we will first crawl slides of other courses in UIUC. For these slides, we will identify the topics and generate training data. Next, we will propose an efficient ranking algorithm to automatically select relevant slides for each specific topic. Once a user selects a topic from those obtained in the first part, the system will present relevant slides to the user on the webpage. To demonstrate the performance of our algorithm, we will separate our crawled slides, together with existing slides, into positive samples and negative samples, and compare our algorithm with classical algorithms such as BM25 with this generated dataset. 4. Timeline Task Estimated Time Cost Expected Completion Time Social media crawling 5h Week 11 Paper survey 5h Week 11 Emerging topic identification 15h Week 14 Slides crawling 5h Week 11 Rank algorithm design 15h Week 13 Integration with existing system 20h Week 15
https://github.com/liuyuxiang512/CourseProject	README.md	"CourseProject Final Project for CS410 of UIUC. Project Proposal Proposal.pdf Project Progress Report Progress_Report.pdf Project Presentation Video Presentation Documentation 1. Overview This project consists of two major tasks. The first one is to identify emerging topics in Twitter within computer science field, and the second one is to recommend relevant slides of the given topics. 1.1 Identify Emerging Topics In this task, we first crawled 680k tweets from Twitter with query ""computer science"", which limited our scale of topics. Then, in order to mine topics from these tweets, we found the optimal number of topics w.r.t. coherence value and trained the LDA topic model with the optimal number of topics. Finally, we visualized topics with word cloud by analyzing hashtags. Besides, we support identifying emerging topics by crawling the latest tweets and predicting their topics with pre-trained LDA model, while these newly crawled data is used to update our LDA model. All related files are in Identify_Topics/ directory. - Crawling/: Keep crawling tweets and form training data. - data/: Store raw data (sorted_tweets.json), processed data (pre-processed.pkl), stopwords (stopwords.txt), and pictures of topics (topic_desc_fig/). - model: Store pre-trained LDA model files. - topic_discovery.py: Extract topics from crawled tweets, evaluate models with different number of topics to find the optimal, get topics and draw pictures for them with word cloud, and predict emerging topics based on pre-trained model. - topics.json: Word distributions of topics, which will feed into next part. 1.2 Recommend Slides for Topics In this task, we first crawled 100+ course slides in UIUC. Then, taking the above word distributions of different topics as input, we used BM25 to find relevant slides. All related files are in Recommend_Slides/ directory. - pdf_download.py: Scrapes slides from four fixed UIUC course websites which are CS225, CS240, CS425 and CS447. It will download all the PDF documents to a local directory ""slides"". - pdf_miner.py: Read the slides under the ""slides"" folder and use pdfminer tool to extract text from the slides. Then, write the raw text to a ""txt"" file under the folder ""raw"". For example, if it read a PDF file ""slides/Lecture1.pdf"", there will be a text file ""raw/Lecture1.txt"" which contains the text data of the original PDF file. - filter_raw.py: Read the raw text files under the ""raw"" folder and filter these texts so that they can be used in the following ranking algorithm. It removes the stop words, meaningless numbers and some other useless words. Then, it lemmatizes and stems the words so that derivative words can be treated equally. The results are saved under the ""corpus"" folder. Each file under this folder represents the abstract of a PDF file from ""slides"" folder. For example, if it read a text file ""raw/Lecture1.txt"", there will be a filtered text file ""corpus/Lecture1.txt"" which contains the cleaned text data. - bm25.py: Read the topic file ""topics.json"" and generate queries with the distributions of keywords in each topic. Each topic generates one query. Then, for each query, run the bm25 ranking algorithm to calculate the scores of this query to each documents in the ""corpus"" folder. Finally, get the top 10 documents and write the result to the target file ""result/bm25.txt"". - doc_sim.py: Similar as ""bm25.py"". The only difference is the ranking algorithm. It calculates the cosine similarity with TF-IDF weights with each pair of query and document. Then, get the top 10 documents and write the result to the target file ""result/sim.txt"". 2. Implementation 2.1 Identify Emerging Topics Tweets Crawling This part serves to generate dataset containing recent tweets from Twitter. Due to the rate limit of Twitter, we can only crawl a small amount of tweets every 15 min. Therefore, in Crawling/ directory, we implemented a crawler which can crawl tweets automatically. Twitter_crawler.py: Crawl recent tweets that don't overlap with pre-crawled tweets. utils.py: Sort crawled tweets in terms of create time, which aims to optimize crawling and saving process. Twitter_crawler.sh: Auto-crawling bash file that runs Twitter_crawler.py and utils.py repeatedly every 15 min. Topic Mining This part is to generate topics with crawled tweets. Here we applied LDA algorithm for topic mining. All related files are in TopiccDiscovery/ directory. Implementation of topic_discovery.py is as follows: Preprocess: For each tweet, we perform 1) lower; 2) remove username, hashtag, url, number, punctuation, special character, and short word; 3) tokenization; 4) remove stopwords; 5) lemmatization; and 6) stemming. Then, we save processed data data/pre-processed.pkl for training. Find optimal number of topics: We applied LDA model with different numbers of topics from 2 to 14, and found that 10 is the optimal. Training: We set number of topics as 10, trained an LDA model on 662k processed tweets, and saved model files in model/ directory. Saving Topics: We loaded pre-trained files, saved word distributions for topics, and drew word cloud figures by analyzing hashtags for all topics. Predict: With pre-trained model, we can crawl latest tweets about computer science and make predictions to find out emerging topics among all topics. Meanwhile, we use these newly crawled tweets to update the LDA model. 2.2 Recommend Slides pdf_download.py This module does the following: Given the course website page, use Soup to extract all the elements with tag ""a"". Judge the elements whether it is a link which is end with "".txt"". If yes, concatenate the prefix and the link to get the complete url. Download the PDF file with its original name and put it into the ""slides"" folder. Functions are: getTagA(root_url): Obtain all the elements with tag ""a"" and return a list of string. downPdf(root_url, prefix, list_a): Download all the PDF files in the root_url. The argument ""prefix"" is used to complete the pdf links. It varies with different course websites. getFile(url): Get the url file to the ""slides"" folder. pdf_miner.py This module does the following: Read each PDF file under the ""slides"" folder. Create a PDF Parser for each PDF file. Then parse the pdf file and extract the text data from each page. Write the raw text data to a target file under the folder ""raw"". Functions are: parse(filename): Extract text data from a PDF file and write it to a target text file (Different PDF files write into different text files). filter_raw.py This module does the following: Read each raw text file under the ""raw"" folder. Tokenlize the text data and remove short words, numbers and stop words from the text. Lemmatize and stem words. Then, write it to a text file under the ""corpus"" folder (Different raw text files write into different target files). Functions are: get_raw_data(filepath): Read a raw text file and return a list of string. Each element in this list represents a line of data in the raw text file. pre_process(data, filename): First, use ""re"" (regular expression) to remove unwanted words. Second, use ""spacy"" to lemmatize words and ""nltk.stem"" to stem words. Finally, write these stemmed words to the target file under the ""corpus"" folder. bm25.py This module does the following: Read the topic file ""topics.json"" and generate a query based on the distributions of keywords in each topic. For example, if the topic is ""{""topic1"" : {kewords1 : 0.5, keyword2 : 0.5}}"", it will generate a query like ""keyword1 keyword1 keyword2 keyword2"" which keeps to the distributions. Treat this query as a document and compute the scores of this query to each document in our corpus with the BM25 model implemented by gensim library. Get the top 10 score document names and write the result to the result file ""result/bm25.txt"". Functions are: tokenization(filename): Read the document under the ""corpus"" folder and return a list of words in this document. read_corpus(dir_path): Read all the documents under the dir_path and return a 2-dimensional list of strings. The first dimension represents each document and the second one contains the words included in each document. simulate_query_by_topics(topic_file): Generate queries with topics. In this implementation, it generates query with a word base 100. If we have keyword1 and keyword2 with distribution of 0.2 and 0.5. It will generate a query with 20 keyword1 and 50 keyword2. Node: each topic only reserves top several keywords. Their distributions may not add up to one, but it doesn't affect their relative size. doc_sim.py: This module does the following: Read the topic file ""topics.json"" and generate a query based on the distributions of keywords in each topic. For example, if the topic is ""{""topic1"" : {kewords1 : 0.5, keyword2 : 0.5}}"", it will generate a query like ""keyword1 keyword1 keyword2 keyword2"" which keeps to the distributions. Treat this query as a document and compute the cosine similarity with TF-IDF weights with each documents under ""corpus"" folder. Get the top 10 similarity document names and write the result to the result file ""result/sim.txt"". Funcrions are the similar to those in ""bm25.py"". 3. Usage Installation This software requires python 3.5+, and it also requires external libraries that can be installed by: pip install -r requirements.txt After you have installed spacy library, you also need to load en model in spacy through: python -m spacy download en_core_web_sm Now you have all the necessary packages! Before any later steps, clone this repository: git clone https://github.com/liuyuxiang512/CourseProject Usage Example cd CourseProject Identify Emerging Topics Directory Identify_Topics/ serves to identify emerging topics in Twitter. cd Identify_Topics Crawl Tweets from Twitter You could jump this step by downloading our crawled data sorted_tweets.json, which contains 680k tweets, and save the file into data/ directory. In order to crawl tweets, you first need a Twitter developer account. Then: Create a Twitter application via https://developer.twitter.com/. Create a Twitter app to access Twitter's API. Find the authentication info in the ""Keys and Access Tokens"" tab of the app's properties, including consumer_key, consumer_secret, access_token, and access_token_secret. Fill the authentication into authentication.txt in four lines. Then, you can keep crawling tweets by cd Crawling bash Twitter_crawler.sh Find Optimal Number of Topics In this step, you can try LDA model with different numbers of topics from 2 to 14, and get corresponding coherence values. A higher coherence value means a better model. If you don't want to use our processed data Identify_Topics/data/pre-processed.pkl, you may first remove it and continue, but it will take some time. To find out the optimal number of topics, run python topic_discovery.py --tune Then you will get Tuning... Number of Topics: 2 --- Coherence Value: 0.49589240555472486 Number of Topics: 3 --- Coherence Value: 0.4752864500035534 Number of Topics: 4 --- Coherence Value: 0.4844109302488787 Number of Topics: 5 --- Coherence Value: 0.5426149238108859 Number of Topics: 6 --- Coherence Value: 0.5708485237453553 Number of Topics: 7 --- Coherence Value: 0.5514423515877226 Number of Topics: 8 --- Coherence Value: 0.5778541035204716 Number of Topics: 9 --- Coherence Value: 0.566857492981066 Number of Topics: 10 --- Coherence Value: 0.5808911042666589 Number of Topics: 11 --- Coherence Value: 0.5561191556402437 Number of Topics: 12 --- Coherence Value: 0.5699566981479943 Number of Topics: 13 --- Coherence Value: 0.5522769193550581 Number of Topics: 14 --- Coherence Value: 0.5433632323040761 ... The optimal number of topics is: 10 Therefore, the optimal number of topics is 10, and we will use 10 for our LDA model in formal training. Train With 10 as number of topics, you can now train an LDA model by running python topic_discovery.py --train This step will take a long time, but you can go ahead and directly use our pre-trained model in Identify_Topics/model/ directory for subsequent steps. Display This step is to use pre-trained model to get topics and draw word cloud for these topics. python topic_discovery.py --display Or you can see what we have got after this step: word distributions of topics Identify_Topics/topics.json and word cloud figures of topics in Identify_Topics/data/topic_desc_fig/. The following are word cloud figures for 2 topics out of 10. Predict After the previous steps, you have successfully obtained processed data (Identify_Topics/data/pre-processed.pkl), word distributions of topics (Identify_Topics/topics.json), and word cloud pictures of topics in Identify_Topics/data/topic_desc_fig/. This step is a further extension of our software. You still need a Twitter developer account to crawl latest tweets. Please refer to how to get authentication info in the above ""Crawling Tweets from Twitter"" section. You can always find out emerging topics in Twitter by running python topic_discovery.py --predict It will crawl latest tweets, predict topics for then and figure out popular ones. Meanwhile, it also uses these newly crawled data to update the LDA model. We got the following results on Dec.13th 9am: Emerging Topics ID (Ordered): 8 6 0 4 1 3 To see what these topics are, you may go to data/topic_desc_fig/ directory and find corresponding word cloud! Command Line Usage of topic_discovery.py ``` python topic_discovery.py -h usage: topic_discovery.py [-h] [-i INPUT_FILE] [-n NUM_TOPICS] [-f FIELD] [-o OUTPUT_FILE] [--train] [--display] [--tune] [--predict] Identify In-demanding Skills optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE input file contains tweets crawled from Twitter - (default: data/sorted_tweets.json). -n NUM_TOPICS, --num_topics NUM_TOPICS number of topics - (default: 10). -f FIELD, --field FIELD field of subject to mine - (default: computer science) -o OUTPUT_FILE, --output_file OUTPUT_FILE output file contains term distribution of topics - (default: topics.json). --train preprocess and train --display save topics and draw pictures --tune find the optimal number of topics --predict predict emerging topics with trained model ``` Recommend Slides Directory Recommend_Slides is to recommend related slides based on topics. cd Recommend_Slides Download the slides using the pdf_download.py. But it may be slow. You can access the PDF slides with the link: Download Slides. Then, unzip it to the ""slides"" folder. bash python3 pdf_download.py Then, we need to extract raw text from PDF files and filter these raw texts. bash python3 pdf_miner.py python3 filter_raw.py Finally, using bm25.py or doc_sim.py to calculate the final results. After this step, you can see the result files under the ""result"" folder. bash python3 bm25.py python3 doc_sim.py Other Usage main.py: Users can run this script with python3. It provides 2 kinds of command. (Note: This two commands are available after filtering the raw text). bash python3 main.py The first one is ""latest"". It will automatically run the results with existing topics in ""topics.json"". The second one is ""query"". Then it will ask you to type in a query and output 10 files that are most relevant to your query. This ranking list is based on BM25 algorithm because after our mannual evaluation, BM25 ranking performs better than cosine similarity ranking. search.py: Users can run this script with python3. It provides 2 kinds of command. (Note: This two commands are available after filtering the raw text). bash python3 search.py The first one is ""bm25"". It means that the following ranking is based on BM25 algorithm. Then it will ask you to type in a query and output 10 filenames that are most relevant to your query. The second one is ""sim"". It means that the following ranking is based on document cosine similaritiy. Then it will ask you to type in a query and output 10 filenames that are most relevant to your query."
https://github.com/yangyangsquare/CourseProject	CS 410 Course Project Documentation.pdf	Classification Competition: Twitter Sarcasm Detection CS 410 Final Project Documentation Yang Yang yangy19@illinois.edu Abstract Sarcasm detection is a specific case of sentiment analysis where instead of detecting a sentiment in the whole spectrum, the focus is on sarcasm. In this Classification Competition, the task is to detect sarcasm in contextual Twitter text. In order to beat the baseline F1 score and improve the performance, the main model used in this project is one of the State-of-the-Art NLP models, BERT. I adapt the off-the-shelf BERT classifier model by Hug- gingface, modify and expand the use of fine-tuning for other BERT-based models. Further more, I investigate the BERT model performance when context information is used in different manners. The result interestingly shows that doing this specific task as a sentence pair classification outperforms it as a normal text classification. 1. Introduction With the growing role of social media across the world, Sarcasm in tweets has raised more attention. Thus, how to use NLP models to efficiently detect Sarcasm in tweets also has been a hot topic in both academia and industry. In this project, I first use a couple of BERT-based pre-trained models, such as BERT, ALBERT, DistilBERT and SqueezeBERT, to understand language and beat the baseline performance. Then I start to look at the methods of utilizing Context information in tweets. Sarcasm Detection, from the topic name itself, sounds like a very typical binary text classification. Since we have Context information together with Response, I use Context sentence(s) in three different methods to run the hyperparameter tuning under BERT pre-trained model. 2. Approach Overall, I follow the procedures below to fine-tune and improve the model performance in this project: (1) Adapt BERT classifier from Google Research and BERT example by Huggingface transformers [6]; (2) Modify BERT model code to make it applicable for other BERT-based models; (3) Load the train and test dataset and split original train dataset in 80 : 20 for train and validation; (4) Run pre-trained models from BERT, ALBERT, DistilBERT, SqueezeBERT and XLNet with same hyperpa- rameter setting and compare performances; (5) Compare BERT model performances with different Context methods; (6) Fine-tune hyperparameters for BERT model with best Context method. 2.1. BERT Classifier Adaption and Expansion Inspired by Rajapakse [3] using one of the State-of-the-Art NLP model, BERT, I first look into Google's original BERT paper [7], and then notice the off-the-shelf BERT classifier from Google Research GitHub [1]. While some TensorFlow bert packages have revision issues and haven't been solved for a while. So I switch some functions to similar ones in PyTorch to fix incompatible issues in the code, luckily because NLP researchers from Huggingface have developed a PyTorch version of BERT. 1 The next step is to expand the BERT classifier for other BERT-based model fine-tuning for model performance comparison. Thanks to PyTorch AutoClasses model, it can automatically recognize the architecture from pre-trained model id to extract tokenizer and configuration file accordingly. In order to make the training portion compatible, I dig into other four models' SequenceClassification functions on Huggingface Transformers Documentation website [2]. It turns out that not all SequenceClassification functions are able to accept same inputs: The SequenceClassification functions for BERT, ALBERT and SqueezeBERT all take input ids, attention mask and token type ids tokens. While the SequenceClassification functions for DistilBERT and XLNet both can only read input ids and attention mask tokens, no token type ids tokens. So I need to differentiate these two situations and supply different inputs. 2.2. BERT-based Models After adaption and modification, the code can run to fine-tune other pre-trained models like BERT, ALBERT, SqueezeBERT, DistilBERT and XLNet. To begin with a basic comparison, I go through the pre-trained models hub and list hosted by huggingface [4]. Based on the model description and community results, I select following models from these five architectures: 1. BERT: A transformers model pre-trained on a large corpus of English data in a self-supervised fashion. * bert-base-uncased is the most popular BERT model and trained on lower cased English text. 2. ALBERT: A lite BERT for self-supervised learning of language representations. It uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers. * albert-base-v2 is trained on ALBERT base model with no dropout, additional training data and longer training. 3. DistilBERT: A transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. * distilbert-base-uncased is distilled from bert-base-uncased checkpoint. 4. SqueezeBERT: A bidirectional transformer similar to the BERT model. The key difference be- tween the BERT architecture and the SqueezeBERT architecture is that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V and FFN layers. * squeezebert-mnli-headless is the squeezebert-uncased model finetuned on MNLI sentence pair classification task with distillation from electra-base. This pre- trained model is specifically recommended on Huggingface SqueezeBERT site [5] for best results when fine-tuning on sequence classification tasks. 5. XLNet: An extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order. * xlnet-base-cased is the XLNet base English model. 2 Architecture Model ID Model Size Hidden Layer (L) Hidden Size (H) Attention Heads (A) Parameters BERT bert-base-uncased 12 768 12 110M ALBERT albert-base-v2 12 (repeating) 768 12 11M DistilBERT distilbert-base-uncased 6 768 12 66M SqueezeBERT squeezebert-mnli-headless 12 768 12 51M XLNet xlnet-base-cased 12 768 12 110M Table 1: Model Size Summary of Selected Pre-trained Models These five pre-trained models are selected from each of the architecture because they are either the most efficient or best for the task among all in the architecture. Table 1 shows a model size summary of the pre-trained models I use in the project. 2.3. Methods to Use Context Info When adapting BERT classifier, I notice that the InputExample class has two different string attributes text a and text b for sequence text. Apparently, Response information always goes into text a. So I can implement Context information in three different methods: * Method 1 - Use No Context Info: Only use Response information in text a and ignore Context informa- tion. * Method 2 - Concatenate Context with Response (used in 2.2): Concatenate Context string after Response string in text a and run it as a normal text classification task. * Method 3 - Use Context as Separate Sentence Info: Use Response information in text a and Context information in text b. Now when running SequenceClassification, it is actually a sequence pair classification. 2.4. BERT Model Hyperparameter Fine-tuning For hyperparameter fine-tuning, I sweep the batch size in {8, 16, 32}, learning rate in {2*10-5, 5*10-5, 1*10-4}, gradient accumulation steps in {1, 2, 3}, and number of epochs in {1, 2, 3} for all three methods. 3. Experiments and Results 3.1. Data In this project, Instructors provide us the dataset from Twitter. The train dataset has 5000 sarcastic or non- sarcastic posts labeled with SARCASM or NOT SARCASM. There are in total of 2010 NOT SARCASM posts and 1990 SARCASM in train dataset, which constructs a balanced dataset for binary classification. The test dataset has 1800 posts without labels. Both datasets include Response and Context information. Response is the Tweet to be classified and the Context is the conversation context of the Response, which I place in three methods. 3.2. Evaluation Metrics For evaluation, I use F1 score as my primary metric since it is the criteria to beat the baseline. Since F1 is calculated based on Precision and Recall, I generate all three metrics in my code. 3 3.3. Performances from Different Pre-trained Models To compare the five selected models, I sweep the number of epochs in {1, 2, 3, 4, 5} for all models with same remaining hyperparameters as shown below: * Maximum Sequence Length = 128 * Warmup Proportion = 0.1 * Batch Size = 16 * Learning Rate = 2 * 10-5 * Gradient Accumulation Steps = 1 * Context Method: Concatenate after Response Figure 1 shows the learning curves on validation dataset for the selected models from five architectures. In F1 plot, BERT shows a high and stable F1 score, even though the highest F1 score is not from BERT. Although the highest F1 score happens with XLNet pre-trained model, it only happens when epochs reaches at 5 with large fluctuation. Figure 1: Learning Curves of Selected Models. Overall BERT model shows the best performance. Although the highest F1 score happens with XLNet pre-trained model, it only happens when epochs reaches at 5 with large fluctuation. Considering recall and the impact of random seeds on fine-tuning, the BERT model is better. Since it is a sarcasm detection task, the goal is to detect more positive cases. So the cost of missing a positive case is more problematic than the cost of including a negative case. That means Recall is more important Precision. Looking at the Recall plot, it is clear that BERT model is the best one. In Dodge's paper [8], he mentioned that even with the same hyperparameter values, distinct random seeds can lead to substantially different results due to weight initialization, training data order and other reasons. It tells us that users may not be able to generate such high F1 score with XLNet model every single time by looking at its unstable learning curve. Considering both Recall learning curve and the impact of random seeds on fine-tuning, I choose the BERT model bert-base-uncased as the best model among all. 3.4. Performances from Different Context Methods As mentioned in 2.3, I use three different methods for Context information, they are: * Response only denotes to Method 1: Use No Context Info * ResponseContext Connect denotes to Method 2: Concatenate Context with Response * ResponseContext Separate denotes to Method 3: Use Context as Separate Sentence Info 4 Figure 2 shows the performance metrics of three Context methods. In F1 score histogram, more than 80% of Method 3 iterations reach 0.8 of F1, which is the best method in terms of F1 score. While surprisingly Method 1, which does not use Context info is even better than Method 2 - Concatenate Context with Response. Looking at the Precision vs. Recall plot, most of Method 3 dots have both high precision and recall, which result in high F1 score, whereas dots of the other two methods have either low precision or low recall. Overall, the BERT model using Method 3 - Use Context as Separate Sentence Info, has the best performance on average. Figure 2: Performance Metrics for Three Context Methods. 3.5. BERT Model Hyperparameter Fine-tuning with Method 3 After selecting pre-trained model and Context method, I sweep hyperparameters of batch size, learning rate, number of epochs and gradient accumulation steps. Figure 3 shows the hyperparameter optimization sweep result. Figure 3: Hyperparameter Fine-tuning for BERT Model with Context Method 3. More red(ish) dots indicate higher F1 scores, whereas more blue(ish) dots indicate lower F1 scores. 5 Precision Recall F1 Validation 0.783 0.892 0.834 Test 0.708 0.827 0.763 Table 2: Final Result Based on the optimization result, I select the following hyperparameters as my final BERT model hyperparam- eters: * Maximum Sequence Length = 128 * Warmup Proportion = 0.1 * Batch Size = 16 * Learning Rate = 2 * 10-5 * Gradient Accumulation Steps = 2 * Number of Epochs = 2 By using this hyperparameter set, my model is able to reach F1 score at 0.763 for test dataset. Details are shown in Table 2. 4. Conclusion In this Classification Competition, I use pre-trained base models of BERT, ALBERT, DistilBERT, Squeeze- BERT and XLNet for contextual Twitter sarcasm Detection. Among those base models with same hyperparam- eters, BERT shows the best performance. In the further discussion of Context information use, I place Context sentences in three different methods. After hyperparameter tuning with all three, the BERT model using Context as separate sentence information shows the best performance on average. The best result from the best BERT model reaches on the test dataset 0.763 as F1 score and beat the baseline F1 score of 0.723. 5. GitHub Repo The whole project is developed with PyTorch framework in Google Colab environment. The source code, voiced presentation and this project documentation are all available in this CourseProject GitHub Repo. (https: //github.com/yangyangsquare/CourseProject). 6 References [1] google-research/bert/run classifier.py. https://github.com/google-research/bert/blob/master/ run_classifier.py. Accessed: 2020-12-10. 1 [2] Huggingface transformers documentations. https://huggingface.co/transformers/index.html. Ac- cessed: 2020-12-10. 2 [3] A simple guide on using bert for binary text classification. https://medium.com/swlh/ a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04. Accessed: 2020- 12-10. 1 [4] Transformers pre-trained models. https://huggingface.co/transformers/pretrained_models. html. Accessed: 2020-12-10. 2 [5] Transformers squeezebert model. https://huggingface.co/transformers/model_doc/ squeezebert.html. Accessed: 2020-12-10. 2 [6] transformers/examples/movement-pruning. https://github.com/huggingface/transformers/blob/ 67ff1c314a61a2d5949b3bb48fa3ec7e9b697d7e/examples/movement-pruning/masked_run_ glue.py. Accessed: 2020-12-10. 1 [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transform- ers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 1 [8] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping, 2020. 4 7
https://github.com/yangyangsquare/CourseProject	CS 410 Course Project Progress Report.pdf	CS 410 Course Project Progress Report Yang Yang (NetID: yangy19) Project Topic: Text Classification Competition 1. Which tasks have been completed? I have implemented pre-trained BERT models from PyTorch on Google Colab for Twitter Sarcasm Detection dataset and beat the baseline. 2. Which tasks are pending? I am finalizing the documented source code with explanations. Then I will focus on creating the tutorial demo. 3. Are you facing any challenges? It was taking too much time when I was trying to train the model on my laptop (CPU) due to size of the dataset. But it got fixed after I switched to Google Colab (GPU).
https://github.com/yangyangsquare/CourseProject	CS 410 Course Project Proposal.pdf	CS 410 Course Project Proposal Project Topic: Text Classification Competition 1. What are the names and NetIDs of all your team members? Who is the captain? Individual Team Name: Yang, Yang NetID: yangy19 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Neural Network Classifiers:  Fully Connected Neural Networks  CNN (Convolutional Neural Networks)  RNN (Recurrent Neural Networks)  LSTM (Long Short-Term Memory)  BERT (Bidirectional Encoder Representations from Transformers)  Attention-Based Networks  Graph-Based Networks Deep Learning Frameworks:  PyTorch  TensorFlow  Keras  Caffe  CNTK  MXNet  DL4J I don't have much practical prior experience with the methods mentioned above. I am ready to learn more state-of-the-art neural network classifiers. 4. Which programming language do you plan to use? I will use Python for this project competition.
https://github.com/yangyangsquare/CourseProject	README.md	Text Classification Competition - Twitter Scarcasm Detection This task is to detect sarcasm from contextual tweets and beat the baseline performance of F1 = 0.723. Voiced Presentation (Demo) Link https://mediaspace.illinois.edu/media/t/1_685r9kih Setup for prediction generation on test dataset Please open the Twitter_Scarcasm_Detection_Source_Code.ipynb file from Google Colab. (Link directly to Colab) Go to Runtime -> Change runtime type, and make sure it has GPU selected as Hardware accelerator and High-RAM as Runtime shape. Go to Runtime -> Run all. It takes approximately 5 minutes to complete. Before you download the answer.txt, you can also look at the validation F1 score, which is usually ~0.83. You can use the Table of contents toolbar on the left to navigate to section 7. Evaluation. Use the Files toolbar on the left, go to outputs -> Twitter_Sarcasm_Detection, and you should be able to see answer.txt. Final Result on test dataset F1 = 0.7626858 More Details Please review the project documentation file, which includes all models I have tried and three different methods I used for Context text string. It also covers model performance comparison and different method comparison specification for this task.
https://github.com/bearnomore/CourseProject	CS 410 Final Report.pdf	"CS 410 Project Report Reproduction of Paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" Team TR Squirrels: Ye Xu, Weidi Ouyang, Raj Datta 1 INTRODUCTION Our project focused on reproducing the paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" by ChengXiang Zhai et al (see reference 1.) We were interested in this paper since it has broad applicability covering a variety of data types. We were also enthused by the fact that the paper used multiple data mining techniques such as frequent patterns, clustering, and similarity functions. We believed that it would allow us to solidify our knowledge based on class materials while learning and exploring new areas as well (e.g. frequent patterns.) The paper uses frequent pattern mining as a fundamental building block. The frequent patterns that occur in datasets, however, have to be interpreted to understand their relevance and semantic applicability. The goal of the paper is to create a way to attach (annotate) meaningful information to frequent patterns that lets us understand the frequent pattern better. The analogy of a dictionary is taken whereby words that are looked up, have a description, but also have examples and other related words presented to fully understand the looked-up word. The equivalent in the frequent pattern universe would then be to identify a) the definition of the frequent pattern by its context indicators, b) representative transactions with the frequent pattern, and c) semantically similar patterns. These specifics are extracted through a series of data mining steps and algorithms which are detailed in the paper. We undertook the effort by first understanding the specifics of the paper, finalizing project scope, selecting appropriate tools and libraries, and implementing the desired result. PROJECT SCOPE When we evaluated the specifics of what the paper had accomplished, we realized that the full scope of the paper would require much more effort than what was expected for the team project and endeavored to refine the scope more clearly and realistically. In consultation and agreement with the lead TA, Bhavya, we decided on the following: * To use only one dataset, not three as in the paper * To implement only one of the two clustering algorithms for removal of title pattern redundancy. * To implement the entire sequence of steps/algorithms necessary to extract the context indicators and define the given frequent pattern in the context units space (section 4.1 of paper). * To leave extraction of representative transactions (section 4.2 of paper) and semantically similar patterns (section 4.3 of paper) as optional, to be implemented only if time allowed * We would then match our results against the paper's results for the appropriate database we decide to use (either section 5.1, 5.2, or 5.3 of the paper.) 2 IMPLEMENTATION We decided to choose the DBLP dataset for our implementation. Specifically, we started with DBLP50000 (see reference 2.) As in the paper, we decided to focus our efforts on the author data attributes for itemset frequent pattern mining, and on title data attributes for sequential frequent pattern mining. We also decided to implement the agglomerative hierarchical micro-clustering algorithm (mentioned as Algorithm 1 in the paper.) We wrote our scripts in Python and chose NLTK for stemming and stop-words removal for titles, MLXend library for frequent pattern mining of authors and PySpark library for sequential frequent pattern mining of titles, as they are more modern Python-based libraries with good adoption in the industry. These are not the same libraries used when the paper was written (which may be somewhat outdated), but using the same algorithms. For the author pattern mining, we used the FPGrowth algorithm with some code sample adjustment for finding closed frequent patterns (see reference 3). For title pattern mining we used the PrefixSpan algorithm. For stemming, we used the Porter stemmer (as opposed to Krovertz stemmer used in the paper) due to our familiarity with NLTK and possibility of higher false negative rates in Krovertz stemmer. We used the default stopword removal (for English and German languages) in NLTK (the paper only mentions the fact that 12 words were removed without mentioning which words.) Data acquisition and pre-processing After we acquired the DBLP50000 raw dataset (with 50000 transactions), we parsed it and removed transactions that had no author names, which resulted in 49233 rows. We found that some of our computations (e.g. the hierarchical clustering algorithm is not scaled up well for large data such as the gigantic distance matrix) were taking too long due to the size of the dataset. We decided to take a smaller slice of raw data only from the year 2000 (based on recommendation from Bhavya, lead TA.) This reduced our dataset size to a manageable 4004 rows of transactions. Obtaining Closed Frequent Patterns After obtaining the clean dataset, the next step is to get the closed frequent patterns for authors and titles which become the fundamental building blocks of the rest of the paper's approach. Given that we had a smaller dataset, we decided to lower the support of author count to 4 (as opposed to 10 as in the paper) and obtained 14 such closed frequent patterns. And using the same support count (4) as in paper for title frequent patterns, we obtained 1912 frequent patterns (each title has multiple word sequences and hence a higher likelihood for patterns.) As part of defining the building blocks, we also wrote scripts to find transactions related to author and title frequent patterns by building a reverse index of the transactions, which in turn helps downstream 3 computations related to clustering and building weight vectors of context units. Clustering We implemented the agglomerative hierarchical clustering algorithm with complete linkage, outlined as Algorithm 1 in the paper. We implemented the Jaccard distance measure as defined in the paper (Definition 9) for the purpose of clustering. After the visualization of the resulting dendrogram (Figure 1) and the elbow analysis of clustering iterations, we chose the threshold/cutoff of maximum depth at 0.01 to give 166 clusters. We tested a range of cutoff thresholds that gave different numbers of clusters around the elbow of the velocity curve where the clustering of branches slows down and examined the clustered words at these cutoff points. We felt that anything more than 166 clusters would have some redundancy, while less than 166 clusters would lack specificity and lose some information. Therefore, we chose 166 as our heuristic best. Figure 1: Dendrogram of Hierarchical Clustering of the 1912 sequential patterns of titles. Weighting and Context Indicators Weighting and the choice of context indicators is core to the context modeling part of the paper. For this, we chose the full set of our closed frequent patterns by combining both author and title frequent patterns into one pool of context indicators of 180 patterns. We then computed the mutual information between pairwise patterns as defined by paper and generated a 180 x 180 weight matrix of context indicators. Optional Features 4 Having achieved suitable implementation covering 4.1 of the paper, we decided to implement the optional parts of identifying example transactions (section 4.2 of paper), as well as finding semantically similar patterns (section 4.3 of paper.) After implementation, we created one example of context annotation for a given author pattern, and one example of context annotation for a given title pattern. In each example, the given pattern had top 5 weighted context indicators as its definition, 5 most representative titles from transactions and 5 most semantically similar titles or authors as synonyms to verify the soundness of the implementation. RESULTS ANALYSIS & CONCLUSION Since we had chosen the DBLP dataset for processing, our target for results comparison became what is presented in section 5.1 of the paper, which simply presents examples of the patterns and its contextual definition, representative transactions, and some semantically similar patterns (SSP's). Since we had taken a data slice from only one year (the year 2000), we knew the results wouldn't be exactly the same as in the paper, but should be indicative of the power of the approach mentioned in the paper. The following are the two examples generated from our project. As shown below, they are defining what the paper intended as contextual definition, representative examples, and similarly related patterns quite well. Having accomplished all 3 aspects (sections 4.1, 4.2, 4.3 of the paper), makes this more than what we had targeted in the scope of the project (since 4.2 and 4.3 were decided as optional.) 5 Results Example1: Context Annotation of An Author ""Ralf Steinmetz"". Author Definition Representative Titles (top 5) Synonym Authors (top5) Synonym Titles (top5) Ralf Steinmetz Ralf Steinmetz Domain Name Based Visualization of Web Histories in a Zoomable User Interface. Sanjay Kumar Madria user interfac virtuellen Intelligent graphical user interface design utilizing multiple fuzzy agents. Roberto Gorrieri process descript Sanjay Kumar Madria Realistic Force Feedback for Virtual Reality Based Diagnostic Surgery Simulators. Thomas S. Huang virtuellen workbench Techniques for simulating difficult queueing problems: adaptive importance sampling simulation of queueing networks. Edwin R. Hancock summari Edwin R. Hancock Blackboard Segmentation Using Video Image of Lecture and Its Applications. Gerald Sommer high speed 6 Results Example2: Context Annotation of A Title Title Definition Representative Titles (top5) Synonym Titles (top5) Synonym Authors (top5) virtual realiti Sanjay Kumar Madria Domain Name Based Visualization of Web Histories in a Zoomable User Interface. receiv Roberto Gorrieri Edwin R. Hancock Realistic Force Feedback for Virtual Reality Based Diagnostic Surgery Simulators. diagnost Sanjay Kumar Madria Roberto Gorrieri Blackboard Segmentation Using Video Image of Lecture and Its Applications. debug program Thomas S. Huang analysi access Techniques for simulating difficult queueing problems: adaptive importance sampling simulation of queueing networks. versu,Bharat K Bhargava Thomas S. Huang An approximate model for the computation of blocking probabilities in cellular networks with repeated calls. analysi access Bill Hancock Overall, we feel that the results are very useful and are convinced that this can help in building intelligence in applications reliant on various types of data. What we have built can easily be scaled up for larger datasets (by providing more computing power), and tuned to the specifics of the application (by modifying a variety of parameters in the algorithms implemented.) This was a useful exercise for the team to understand the importance of the mining approaches we learnt in the course. 7 REFERENCES 1. Q. Mei, D. Xin, H. Cheng, J. Han, and C. Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proc. of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06, ACM, pp. 337-346. 2. Source of DBLP50000: https://hpi.de/naumann/projects/repeatability/datasets/dblp- dataset.html Citation: Frequency-aware Similarity Measures.Lange, Dustin; Naumann, Felix (2011). 243-- 248. 3. How to Find Closed and Maximal Frequent Itemsets from FP-Growth | by Andrewngai | Towards Data Science 4. Project Github site: https://github.com/bearnomore/CourseProject 5. Presentation developed as Tutorial of Paper available on Github site: https://github.com/bearnomore/CourseProject/blob/main/Guidance%20of%20Reproducing%2 0the%20paper.pptx 6. Video of paper tutorial presentation and walkthrough of implementation details: https://mediaspace.illinois.edu/media/t/1_2uzja14v 8 APPENDIX: Setup Instructions Instruction for Reproducing paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" (Instructions are also available as a README file in the project github site (see reference 4.) 1. Overview of the github CourseProject There are three folders, Datasets, PythonCodes and JupyterNoteBookDemo, in the repository. The PythonCodes and the JupyterNotebookDemo contain the same scripts but different file formats (py file and ipynb file). Within each of these two script folders, there are 6 folders giving the execution order for reproducing the paper using the DBLP dataset (paper section 5.1). All datasets imported and generated using the scripts are located in the Datasets Folder. The input and output paths of these Datasets files need to be changed if downloaded to your local computer. Except for the raw dataset ""dblp50000.xml"", all other datasets are generated by the scripts. In addition, the final report, the link (https://mediaspace.illinois.edu/media/t/1_2uzja14v) to the video demo and the powerpoint slides of paper review and project introduction are also in the CourseProject repository. 2. Python Library and Packages numpy, scipy, pandas, nltk, csv, os, mlxend and pyspark are libraries needed for running the srcipts. Except for pyspark, all libraries can be downloaded and installed through pip or conda, depending on your preference and execution environment. The installation of pyspark (and Spark) is a bit complicated and requires some environmental configuration and functional java of version 8.0 or above. Here is the link of the tutorial how to install pyspark/Spark on Windows system: https://www.datacamp.com/community/tutorials/installation-of-pyspark 3. Script running instruction 3.1. Parse the raw data (Folder 1. RawDataParsing) Download ""dblp50000.xml"" and run script ""DBLP_raw_data_parsing.py"" or ""DBLP_raw_data_parsing.ipynb"". This generates the dataset ""DBLP2000.csv"". 3.2. Build the Context Units Space (Folder 2. ContextModeling) 3.2.1. Find closed Frequent Pattern (FP) for Authors using FPgrowth algorithm in MLXtend Lib. Run script ""Author_FP_mining.py"" or ""Author_FP_mining.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""authorsFP2000_with_index.csv"", which contains 14 closed FPs of authors and their transaction index in ""DBLP2000.csv"" (e.g. author ""Edwin R. Hancock"" is a closed FP and it showed in the 9 839th, 1119th, 1127th, 1204th and 1576th row of DBLP2000, its transaction index list is [839, 1119, 1127, 1204, 1576] ). 3.2.2. Preprocess DBLP titles Run script ""DBPL_preprocessing_titles.py"" or ""DBPL_preprocessing_titles.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""DBLP2000_preprocessed_titles.txt"". In this step, stop words are removed and the titles are stemmed. 3.2.3. Find Title sequential Pattern using PrefixSpan algorithm in PySpark Run ""titles_seqPattern_mining.py"" to import ""DBLP2000_preprocessed_titles.txt"" and to find closed sequential frequent patterns from titles of DBLP2000. I had issue with configuration of Spark in Jupyter Notebook environment therefore no corresponding script in ""ipynb"" format was put in the ""JupyterNoteBookDemo\ContextModeling"" directory . However, the python script was executed successfully in the windows cmd of my laptop. The script ""titles_seqPattern_mining.py"" generates an output folder containing the pattern file ""part-00000"". Set the ""part-00000"" file to txt format. Run ""Title_sequentialFP_processing.py"" or ""Title_sequentialFP_processing.ipynb"" to import ""part- 00000.txt"" and generate the cleaned dataset ""titlesFP2000.csv"". 3.2.4. Find transaction index of title sequential patterns Run ""Find_transaction_index_of_title_FPs.py"" or ""Find_transaction_index_of_title_FPs.ipynb"" to import ""titlesFP2000.csv""and generate ""titlesFP2000_with_index.csv"", which adds the list of transaction index to each title pattern. 3.2.5. Reduce title FP redundancy by microclustering (hierarchical clustering) Run ""Hierarchical_clustering_titleFPs2000.py"" or ""Hierarchical_clustering_titleFPs2000.ipynb"" to import ""titlesFP2000_with_index.csv"" and generate ""titlesFP2000_final.csv"". This script applies the hierarchical clustering with Jaccard Distance defined per paper and clusters 1912 title sequential patterns into 166 clusters. It chooses the most frequent pattern in each cluster as the ""centroid"" pattern to further build the context unit space. 3.2.6. Combine author FPs and title FPs to build the context units space Run ""DBLP2000_context_units_with_transaction_index.py"" or ""DBLP2000_context_units_with_transaction_index.ipynb"" to import 'authorsFP2000_with_index.csv' and 'titlesFP2000_final.csv' and to generate the final context units dataset ""DBLP2000_context_units.csv"". 3.3. Define given frequent patterns using context units defined above (Folder 3. PatternDefinition) 3.3.1. Build weight vectors of FPs in the context unit space Run ""Weighting_function.py"" or ""Weighting_function.ipynb"" to import """"DBPL2000_context_units.csv"" and ""DBLP2000.csv"" and generate ""Context_units_weights.csv"". This script generates context vectors for all context units defined in 2.2 and builds a weight matrix between the pairwised context FPs. Each element of the matrix is the Mutual Information score between the context unit pair per definition in 10 the paper. 3.3.2. Annotate the given FP (e.g. an author) by context units with highest weights Run ""Defining_pattern_with_context_units.py"" or ""Defining_pattern_with_context_units.ipynb"" to import ""Context_units_weights.csv"". In this step, we first pick an author from the author FPs and rank the weights of its context vector. The context units with top 5 weights are selected as the definition of this author and are saved as ""author_annotation_example1.csv"". Similarly, we pick a title from the title FPs and rank the weights of its context vector, and save the context units with top 5 weights as ""title_annotation_example1.csv"". 3.4. Find representative titles of the given pattern (Folder 4. RepresentativeTitles2Pattern) Run script ""Find_representative_titles_to_pattern.py"" or ""Find_representative_titles_to_pattern.ipynb"" to import ""DBLP2000_context_units.csv"", ""DBLP2000.csv"" and ""Context_units_weights.csv"". This script first generates the weight matrix of transactions (titles) in the context units space as the dataset ""transaction_weights.csv"", and then computes the cosine similarity between the transaction weight vectors and the given pattern weight vector (e.g. the same author and title chosen in 2.3.2). The similarity matrix of transaction to author FPs is saved as ""similarity_scores_of_transaction_to_author.csv"", and the similarity matrix of transaction to title FPs is saved as ""similarity_scores_of_transaction_to_title.csv"". This script then generates the top 5 representative titles with highest similarity scores to the given author and to the given title pattern as dataset ""rep_titles_author_example1.csv"" and ""rep_titles_title_example1.csv"", respectively. 3.5. Find synonyms of the given pattern (Folder 5. Synonyms2Pattern) Run ""Find_synonyms_of_pattern.py"" or ""Find_synonyms_of_pattern.ipynb"" to import ""Context_units_weights.csv"" and to compute the cosine similarity between the candidate patterns of similarity (e.g. all closed frequent patterns of authors) and the given pattern (e.g. the same author and title chosen in 2.3.2). Select the authors with the highest 5 similarity scores as the synonyms of the given author or title other than the author or title itself. This script generates 2 datasets for synonyms of author pattern: ""coauthor_to_author_example1.csv"", ""syn_titles_to_author_example1.csv"", and 2 datasets for synonyms of title pattern: ""syn_titles_to_title_example1.csv"" and ""syn_authors_to_title_example1.csv"". 3.6. A final display of the context annotation of the given pattern (Folder 6. ContextAnnotation) Finally, Run ""Author_context_annotation_example1.py"" (or ""Author_context_annotation_example1.ipynb"") and ""Title_context_annotation_example1.py"" (or ""Title_context_annotation_example1.ipnb"") to combine the output datasets generated in step 2.4, 2.5 and 2.6. This script builds the two examples of context annotation for the given author pattern and the given title pattern respectivley and fullfills the two experiments in paper section 5.1."
https://github.com/bearnomore/CourseProject	CS 410 Progress Report.pdf	"Progress Report for Reproducing the Paper ""Generating Semantic Annotations for Frequent Patterns with Context "" Raj Datta, Weidi Ouyang, Ye Xu Team TR Squirrels 1) Which tasks have been completed? * Thorough Study of Paper and understanding of options * Presentation draft prepared covering overview of paper * Tightly defined scope, in discussion with Bhavya, lead TA * To choose only one dataset (chosen to be DBLP) * To choose and use only one clustering algorithm * To target completing the context modeling and frequent pattern mining steps through part 4.1 of the paper * Consider 4.2 and 4.3 to be out of scope unless time allows after completing 4.1 * Obtained clarification on various open questions and direction from Bhavya, lead TA * Obtained the raw Dataset (DBPL.xml) * Cleaned & Parsed the Dataset into csv format with ""author"" and ""title"" columns * Decided the proper algorithm and libraries for ""Closed Frequent Pattern"" mining and Redundancy reduction (test pre-processings and pattern clustering). * Using UIUC paper authors, created a toy dataset to test the concepts, algorithms and libraries. * Initial trial on the DBLP dataset to generate author itemset frequent patterns 2) Which tasks are pending? * Finalize selection of toolset/library for closed frequent patterns mining on author list and title list * Generate formal itemsets and sequential frequent patterns for clean data set * Using hierarchical clustering algorithm, remove Redundancy from the initial closed frequent patterns * Finalize the context modeling by implementing the weighting function on context indicator and pattern pairs * Analysis of Results * Finalize presentation and report based on implementation and results 3) Are you facing any challenges? * Understanding the concepts and the algorithms in the paper * Paper was written for many general scenarios (e.g. graphs/subgraphs) which aren't necessarily applicable to our implementation; trying to make it more generic made it more difficult to understand the applicability and our relevant extracts * Some of the concepts weren't covered in depth in class (e.g. closed frequent patterns, maximal frequent patterns.) * Some ambiguity (e.g. stop word removal, specifics of laplace smoothing) * Since the paper is dated, we needed to find/explore some of our own more recent libraries and tools, hoping there would be minimal impact on the end result * Implementing weighting functions (Mutual Information) to build Context indicator vectors * Timeline for completing all remaining optional parts of the paper."
https://github.com/bearnomore/CourseProject	CS 410 Project Proposal.pdf	"CS410 Project Proposal Team TR Squirrels 20-Oct-2020 Team TR Squirrels has three members, Captain Ye Xu (Net ID: yex2), member Weidi Ouyang (Net ID: wonyan2) and Raj Datta (Net ID: datta7). We will work on reproducing the listed paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" using Python as the primary coding language. This proposal addressed all related questions from week1 guideline and topic instruction. Answers to these questions were highlighted. 1. Background of the paper This paper proposed a novel approach to generate semantic annotation for frequent patterns that can better interpret the in depth and hidden meaning of the pattern. One meaningful application of this algorithm/procedure is in the field of computational biology. With tremendous sequencing data of genes and their transcripts (e.g. mRNA sequence and protein sequence), annotation of functionality is much needed but barely and poorly supported by the lab-work evidence. Most annotations in our biological databases are counted on computational work that link the known or predicted functions to frequent patterns observed in sequence data. One example is to connect the structural motif of a peptide that describe the connectivity of secondary structural element (e.g. ""helix-turn-helix"" or ""Zinc finger"" ) to a potential function of a protein or part of the protein (e.g. DNA binding domain of a transcription regulator). Biologists, either working on ""omics"" (e.g. genomics, transcriptomics, etc.) or focusing on a particular gene cassette or metabolic pathway, all benefit from such application. A well tagged dataset leads to proficient and precise findings and this is true for applications in areas other than biological science. However, annotation of patterns is challenging and can be very labor intensive. Taking genome annotation for example, in addition to manual annotation (curation), a large part of annotation work is completed by automatic annotation tools based on different algorithms. The most basic one is the homology search tool ""BLAST"" though structural and functional annotation are usually needed to identify and tag the biological information to the genomic elements. This whole process often involves both biological experiment (lab evidence) and ""in silico"" bioinformatic analysis. This paper, however, provided a novel approach that completely based on the Text information retrieval and mining to interpret the discovered patterns and tested the procedure on three different datasets including a gene ontology annotation dataset. 2. Resources and Technique to reproduce the paper We will use the same or similar datasets to reproduce the results. The first dataset is DBLP dataset that provides bibliographical information about computer science journals and proceedings and is available for download (https://hpi.de/naumann/projects/repeatability/datasets/dblp-dataset.html). The second dataset is no longer available from the paper link but similar datasets that include the Gene Ontology terms and Motif for Drosophila are accessible from Gene Ontology Website (Annotation database). The third dataset is provided by BioCreAtIvE Task 1B and is also no longer available. However, we can apply the same approach by crawling abstracts from the MEDLINE database with query keyword ""Drosophila"" and recreate the dataset. We will apply the same technique used in the paper which include the two toolkits ""FP-Close"" and ""CloSpan"" to generate ""Closed Frequent Itemset"" and ""Krovertz stemmer"" to stem the title words. Same or similar clustering algorithms, either Hierarchical Clustering or One-Pass Clustering, will be applied for redundancy reduction. Python libraries and packages, such as ""Scipy"" and ""Scikit-Learn"" provide convenient built-in functions to implement these algorithms too. 3. Brief Timeline of the Project Course Week Dataset Acquisition Week 10 Dataset Processing based on understanding of the pattern context modeling Week 10 - 12 DataSet Oriented modeling and semantic analysis Week 13 - 14 Coding and Presentation finalization Week 15 - 16 The detailed workload and distribution will evolve as the project goes."
https://github.com/bearnomore/CourseProject	Guidance of Reproducing the paper.pptx	"Guidance of Reproducing the paper: Generating Semantic Annotations for Frequent Patterns with Context Analysis CS 410 Course Project Ye Xu, Weidi Ouyang, Raj Datta Overview of the paper Focus of the paper Not about Frequent Pattern Discovery, but about ""Interpreting the frequent pattern with semantic annotations by Constructing the context model of the frequent pattern Selecting context indicators Extracting representative transactions and semantically similar patterns Input: Frequent Pattern Output: Semantic Annotation of that pattern What is the semantic annotation of the frequent pattern 1. Semantic definition of the pattern can be inferred by its context and words sharing the similar context : Context indicator. 2. Extract the data transactions that best represent the meanings of the pattern. 3. Extract semantically similar patterns (SSPs) of the given pattern, i.e., patterns with similar contexts as the original pattern. What is the semantic annotation of the frequent pattern Definition Example sentences Synonyms or Thesaurus Definitions and Problems The Problem formulation section Some concept Transaction: A collection of itemsets, sequences, graphs... A pattern: A item, a subsequence, a subgraph... Support: Absolute number of transactions that contain the pattern The proportion of transactions containing the pattern in the entire transaction dataset Some Concepts Frequent Pattern: A pattern with support equal or larger than the specified threshold Context Unit: An object from the transaction set that carries semantic information and co-occurs with at least one pattern in the pattern set and in at least one transaction of the transaction set. Some Concepts Context indicator: A select Context Unit of a Frequent Pattern. Each of such units co-occur with the Frequent pattern and associate with a weight to measure its strength of semantic indication of the pattern Major Task Define context units to form the context vector space; Design a strength weight for each unit to model the contexts of frequent patterns in order to extract the most significant context indicators as the ""definition"" for the given pattern unit. Design similarity measures between a transaction and a pattern context and between the contexts of two patterns in order to extract representative transactions and semantically similar patterns to finalize the structured annotation of a given frequent pattern. Context Modeling Vector Space Model for Context modeling A transaction and the context of a frequent pattern both are represented as vectors of context units. Context unit selection Very flexible: Any object in the database that carries semantic information or serves to discriminate patterns semantically can be a context unit. single items, transactions, Patterns any group of items/patterns In the paper, Context unit = Pattern Redundancy Reduction  Context unit == Closed Frequent Pattern Closed Frequent Pattern: A frequent pattern is closed if there exists no super-pattern that has the same support count as this original pattern. How to Find Closed and Maximal Frequent Itemsets from FP-Growth | by Andrewngai | Towards Data Science Further Redundancy removal by micro-clustering (Both are agglomerative clustering with complete link) Hierarchical clustering One pass clustering Strength Weighting for Context Unit Intuitively, the strongest context indicators for a pattern should be those units that frequently co-occur with this pattern but infrequently co-occur with others. Extract Strongest Context Indicator As the Definition With the weighting function for each context unit (ui) of a pattern, compute wi = w(ui, pa), rank ui  U with wi in descending order and select the top k units. Semantic Analysis and Pattern Annotation Semantic Similarity For Pattern pa, its context vector is a vector of unit weighting functions c(a) = [w(u1, pa), w(u2, pa), ..., w(uk, pa)], where ui is the select context unit (closed frequent pattern). Cosine distance is used to compute the similarity between two context vectors. Extracting Representative Transactions Represent a transaction as a vector in the same vector space as the context model of the frequent pattern pa, i.e., c(t) = w1, w2, ..., wk. Compute Cosine Similarity between the transaction and the pattern: s(c(t), c(pa )). Rank the similarity in descending order and select top k transactions. Extracting Semantically Similar Patterns let Pc = {c(p1), ..., c(pc)} be the context vectors for {p1, ..., pc} which are believed to be good candidates for annotating the semantics of pa. Compute sim(c(pi), c(pa)) for each pi  Pc, rank them in descending order, and select the top k pi's. Pc can be very flexible (e.g. the whole frequent pattern set). Workflow of the DBLP dataset Parsing Raw Data Find closed FPs of Authors as Context Units Find closed Sequential FPs of Titles as Context Units Preprocessing titles FP mining Processing title FPs Combine FPs of Authors and Titles to form Context Units Space Weight Matrix of Context Units Annotation of Given Pattern by Context Units with Top Weights Find representative titles of given pattern Find synonyms of given pattern"
https://github.com/bearnomore/CourseProject	README.md	"Instruction for Reproducing paper ""Generating Semantic Annotations for Frequent Patterns with Context Analysis"" 1. Overview of the github CourseProject There are three folders, Datasets, PythonCodes and JupyterNoteBookDemo, in the repository. The PythonCodes and the JupyterNotebookDemo contain the same scripts but different file formats (py file and ipynb file). Within each of these two script folders, there are 6 folders giving the execution order for reproducing the paper using the DBLP dataset (paper section 5.1). All datasets imported and generated using the scripts are located in the Datasets Folder. The input and output paths of these Datasets files need to be changed if downloaded to your local computer. Except for the raw dataset ""dblp50000.xml"", all other datasets are generated by the scripts. In addition, the final report, the link (https://mediaspace.illinois.edu/media/t/1_2uzja14v) to the video demo and the powerpoint slides of paper review and project introduction are also in the CourseProject repository. 2. Python Library and Packages numpy, scipy, pandas, nltk, csv, os, mlxend and pyspark are libraries needed for running the srcipts. Except for pyspark, all libraries can be downloaded and installed through pip or conda, depending on your preference and execution environment. The installation of pyspark (and Spark) is a bit complicated and requires some environmental configuration and functional java of version 8.0 or above. Here is the link of the tutorial how to install pyspark/Spark on Windows system: https://www.datacamp.com/community/tutorials/installation-of-pyspark 3. Script running instruction 3.1. Parse the raw data (Folder 1. RawDataParsing) Download ""dblp50000.xml"" and run script ""DBLP_raw_data_parsing.py"" or ""DBLP_raw_data_parsing.ipynb"". This generates the dataset ""DBLP2000.csv"". 3.2. Build the Context Units Space (Folder 2. ContextModeling) 3.2.1. Find closed Frequent Pattern (FP) for Authors using FPgrowth algorithm in MLXtend Lib. Run script ""Author_FP_mining.py"" or ""Author_FP_mining.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""authorsFP2000_with_index.csv"", which contains 14 closed FPs of authors and their transaction index in ""DBLP2000.csv"" (e.g. author ""Edwin R. Hancock"" is a closed FP and it showed in the 839th, 1119th, 1127th, 1204th and 1576th row of DBLP2000, its transaction index list is [839, 1119, 1127, 1204, 1576] ). 3.2.2. Preprocess DBLP titles Run script ""DBPL_preprocessing_titles.py"" or ""DBPL_preprocessing_titles.ipynb"" to import ""DBLP2000.csv"" and generate the dataset ""DBLP2000_preprocessed_titles.txt"". In this step, stop words are removed and the titles are stemmed. 3.2.3. Find Title sequential Pattern using PrefixSpan algorithm in PySpark Run ""titles_seqPattern_mining.py"" to import ""DBLP2000_preprocessed_titles.txt"" and to find closed sequential frequent patterns from titles of DBLP2000. I had issue with configuration of Spark in Jupyter Notebook environment therefore no corresponding script in ""ipynb"" format was put in the ""JupyterNoteBookDemo\ContextModeling"" directory . However, the python script was executed successfully in the windows cmd of my laptop. The script ""titles_seqPattern_mining.py"" generates an output folder containing the pattern file ""part-00000"". Set the ""part-00000"" file to txt format. Run ""Title_sequentialFP_processing.py"" or ""Title_sequentialFP_processing.ipynb"" to import ""part-00000.txt"" and generate the cleaned dataset ""titlesFP2000.csv"". 3.2.4. Find transaction index of title sequential patterns Run ""Find_transaction_index_of_title_FPs.py"" or ""Find_transaction_index_of_title_FPs.ipynb"" to import ""titlesFP2000.csv"" and generate ""titlesFP2000_with_index.csv"", which adds the list of transaction index to each title pattern. 3.2.5. Reduce title FP redundancy by microclustering (hierarchical clustering) Run ""Hierarchical_clustering_titleFPs2000.py"" or ""Hierarchical_clustering_titleFPs2000.ipynb"" to import ""titlesFP2000_with_index.csv"" and generate ""titlesFP2000_final.csv"". This script apply the hierarchical clustering with Jaccard Distance defined per paper and clusters 1912 title sequential patterns into 166 clusters. It chooses the most frequent pattern in each cluster as the ""centroid"" pattern to further build the context unit space. 3.2.6. Combine author FPs and title FPs to build the context units space Run ""DBLP2000_context_units_with_transaction_index.py"" or ""DBLP2000_context_units_with_transaction_index.ipynb"" to import 'authorsFP2000_with_index.csv' and 'titlesFP2000_final.csv' and to generate the final context units dataset ""DBLP2000_context_units.csv"". 3.3. Define given frequent patterns using context units defined above (Folder 3. PatternDefinition) 3.3.1. Build weight vectors of FPs in the context unit space Run ""Weighting_function.py"" or ""Weighting_function.ipynb"" to import """"DBPL2000_context_units.csv"" and ""DBLP2000.csv"" and generate ""Context_units_weights.csv"". This script generates context vectors for all context units defined in 2.2 and builds a weight matrix between the pairwised context FPs. Each element of the matrix is the Mutual Information score between the context unit pair per definition in the paper. 3.3.2. Annotate the given FP (e.g. an author) by context units with highest weights Run ""Defining_pattern_with_context_units.py"" or ""Defining_pattern_with_context_units.ipynb"" to import ""Context_units_weights.csv"". In this step, we first pick an author from the author FPs and rank the weights of its context vector. The context units with top 5 weights are selected as the definition of this author and are saved as ""author_annotation_example1.csv"". Similarly, we pick a title from the title FPs and rank the weights of its context vector, and save the context units with top 5 weights as ""title_annotation_example1.csv"". 3.4. Find representative titles of the given pattern (Folder 4. RepresentativeTitles2Pattern) Run script ""Find_representative_titles_to_pattern.py"" or ""Find_representative_titles_to_pattern.ipynb"" to import ""DBLP2000_context_units.csv"", ""DBLP2000.csv"" and ""Context_units_weights.csv"". This script first generates the weight matrix of transactions (titles) in the context units space as the dataset ""transaction_weights.csv"", and then computes the cosine similarity between the transaction weight vectors and the given pattern weight vector (e.g. the same author and title chosen in 2.3.2). The similarity matrix of transaction to author FPs is saved as ""similarity_scores_of_transaction_to_author.csv"", and the similarity matrix of transaction to title FPs is saved as ""similarity_scores_of_transaction_to_title.csv"". This script then generates the top 5 representative titles with highest similarity scores to the given author and to the given title pattern as dataset ""rep_titles_author_example1.csv"" and ""rep_titles_title_example1.csv"", respectively. 3.5. Find synonyms of the given pattern (Folder 5. Synonyms2Pattern) Run ""Find_synonyms_of_pattern.py"" or ""Find_synonyms_of_pattern.ipynb"" to import ""Context_units_weights.csv"" and to compute the cosine similarity between the candidate patterns of similarity (e.g. all closed frequent patterns of authors) and the given pattern (e.g. the same author and title chosen in 2.3.2). Select the authors with the highest 5 similarity scores as the synonyms of the given author or title other than the author or title itself. This script generates 2 datasets for synonyms of author pattern: ""coauthor_to_author_example1.csv"", ""syn_titles_to_author_example1.csv"", and 2 datasets for synonyms of title pattern: ""syn_titles_to_title_example1.csv"" and ""syn_authors_to_title_example1.csv"". 3.6. A final display of the context annotation of the given pattern (Folder 6. ContextAnnotation) Finally, Run ""Author_context_annotation_example1.py"" (or ""Author_context_annotation_example1.ipynb"") and ""Title_context_annotation_example1.py"" (or ""Title_context_annotation_example1.ipnb"") to combine the output datasets generated in step 2.4, 2.5 and 2.6. This script builds the two examples of context annotation for the given author pattern and the given title pattern respectivley and fullfills the two experiments in paper section 5.1."
https://github.com/bashirpartovi/CourseProject	ExpertSearch Progress Report.pdf	"ExpertSearch Improvements Progress Report Topic Modeler Assignee: Karthik Rajagopal (kr22@illinois.edu) Updates: Challenges: Running topic modeler for all bios is computationally extensive. We are incrementally optimizing the algorithm to achieve a better performance but this has proven to be challenging. Bio Page Classifier Assignee: Bashir Partovi (partovi2@illinois.edu) Updates: Task Progress Implemented topic modeling using spaCy python library and it accurately predicts topics for individual bio pages Completed Integrating topic modeler in ExpertSearch system to display top 5 topics for each search result In Progress Task Progress Implemented a URL crawler using Scrapy to crawl Carnegie Mellon University and University of Maryland in order to generate Completed Challenges: It was difficult to understand how Keras deep neural network layers work, especially for someone who has never worked with the library before. In addition, transforming the text data into a feature vector in order to fit the model was very challenging. Automatic URL Crawler Assignee: Mohana Venkata Kalyan Cheerla (cheerla3@illinois.edu) Updates: negative labels for classifier Using Keras, implemented a deep learning layer and trained it with the compiled bios from class project and the URLs that were crawled by the Scrapy spider, achieving 99% accuracy on the test data Completed Write a wrapper to load the model and use it with the URL crawler in order to identify bio pages In Progress Task Progress Code development to dynamically route from University home page to all its subsequent web pages and scraping their content has been completed. This scraping activity covers the extraction of the entire text information of those pages along with some important metadata that can feed additional information to ""Faculty Bio Page Classifier"" beyond what it requires today to help further improvements in future. Completed Adding additional filtering criteria to the crawler to eliminate crawling of uninteresting URLs. Configuration of Crawler to be more dynamic In Progress Challenges: Crawler runs for a long time and fetches over 40K+ web pages per university. Identifying the filter criteria to reduce the false positives is challenging. is also pending along with integrating it with the classifier"
https://github.com/bashirpartovi/CourseProject	README.md	CourseProject Overview As indicated in the project proposal, we improved upon Expert Search system. The Automated Crawler requires classification model which can be generated by running the classifier Source Code and Documentation Each component includes instructions on how to setup and run * Bio Page Classifier * Automated Crawler * Topic Modeler Presentation and Demo You can find our recorded presentation here as well as our presentation slides
https://github.com/MM026184/CourseProject	Michael McClanahan Project Proposal_CS410_mjm31.docx	"Michael McClanahan NetID: mjm31 CS410: Text Information Systems Project Proposal What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Captain: Michael McClanahan (NetID: mjm31) This project will be completed individually. Which paper have you chosen? Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Which programming language do you plan to use? Python (3.6.1 via Anaconda 4.4.0) Can you obtain the datasets used in the paper for evaluation? No. The dataset from the LDC requires a license for access: https://catalog.ldc.upenn.edu/LDC2008T19 If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? I could build a scraping engine to capture all of the text from the same articles here: https://spiderbites.nytimes.com/2000/ Ideally though, I would already have access to the data in XML file format from the LDC. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A"
https://github.com/MM026184/CourseProject	mjm31 Project Presentation VOICED.pptx	"CS 410 Course Project Reproducing paper: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Michael McClanahan Online MCSDS NetID: mjm31 Overview Project Objectives Installation and Use Example Results Discussion Project Objectives Implement the Iterative Topic Modeling framework with Time Series Feedback (ITMTF) outlined by Kim et al 2013: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Specifically, this is a Python3 implementation that attempts to use the LDA topic modeling implementation from Gensim Perform the paper's 2000 Presidential Election experiment Attempt to replicate results from Table 2 and Figure 2(b) The experiment for Figure 2(a) was not attempted because Gensim's LDA model implementation does not have a u parameter Installation and Use If neccessary, install Python3 Then install the following project dependencies using pip pandas - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html gensim - https://radimrehurek.com/gensim/ nltk - https://www.nltk.org/install.html scipy - https://www.scipy.org/install.html numpy - https://numpy.org/install/ Clone the project repository, making a local copy of all files. This is also the working directory for the code, all of which is implemented in ITMTFPresidential.py. All neccessary objects to reproduce my results are in this repository and automatically consumed by the script. Navigate to your local directory to which the project was cloned and run the script with the following command: python ITMTFPresidential.py Installation and Use Analyze resulting csv files causal_topic_words.csv - contains the top five words for causal topics identified during each IMTMF iteration for each of 5 different IMTMF runs , each with a different initial number of topics identified for LDA itmtf_stats.csv - contains neccessary average causality confidence and purity statistics for each iteration of the 5 different IMTMF runs Example Results Example Results Discussion In general poorer this implementation's results were poorer than what was noted in the paper Improvements in causality confidence and purity were not observed with more iterations Top words for causal topics seemed applicable, but not all that compelling or different from one another Poorer results are likely due to differences in this implementation: Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results."
https://github.com/MM026184/CourseProject	mjm31 Project Presentation.pptx	"CS 410 Course Project Reproducing paper: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Michael McClanahan Online MCSDS NetID: mjm31 Overview Project Objectives Installation and Use Example Results Discussion Project Objectives Implement the Iterative Topic Modeling framework with Time Series Feedback (ITMTF) outlined by Kim et al 2013: Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et al 2013) Specifically, this is a Python3 implementation that attempts to use the LDA topic modeling implementation from Gensim Perform the paper's 2000 Presidential Election experiment Attempt to replicate results from Table 2 and Figure 2(b) The experiment for Figure 2(a) was not attempted because Gensim's LDA model implementation does not have a u parameter Installation and Use If neccessary, install Python3 Then install the following project dependencies using pip pandas - https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html gensim - https://radimrehurek.com/gensim/ nltk - https://www.nltk.org/install.html scipy - https://www.scipy.org/install.html numpy - https://numpy.org/install/ Clone the project repository, making a local copy of all files. This is also the working directory for the code, all of which is implemented in ITMTFPresidential.py. All neccessary objects to reproduce my results are in this repository and automatically consumed by the script. Navigate to your local directory to which the project was cloned and run the script with the following command: python ITMTFPresidential.py Installation and Use Analyze resulting csv files causal_topic_words.csv - contains the top five words for causal topics identified during each IMTMF iteration for each of 5 different IMTMF runs , each with a different initial number of topics identified for LDA itmtf_stats.csv - contains neccessary average causality confidence and purity statistics for each iteration of the 5 different IMTMF runs Example Results Example Results Discussion In general poorer this implementation's results were poorer than what was noted in the paper Improvements in causality confidence and purity were not observed with more iterations Top words for causal topics seemed applicable, but not all that compelling or different from one another Poorer results are likely due to differences in this implementation: Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results."
https://github.com/MM026184/CourseProject	Progress Report_CS410_mjm31.docx	Michael McClanahan CS 410 - Text Mining University of Illinois at Urbana Champaign (Online) MCS-DS NetID: mjm31 Final Project Progress Report Reproducing a paper Mining causal topics in text data: Iterative topic modeling with time series feedback. (Zhai et al 2013) Progress made thus far Read the paper (above) Obtained access to the New York Times Annotated Corpus from the Linguistic Data Consortium Downloaded the NYT annotated corpus Read through the overview document for the NYT corpus Remaining Tasks Loop through XML data files and create functions for text parser Parse XML data for the framework inputs: a time series dataset (list of timestamps) and a corpus consisting of a list of (Document,timestamp) tuples Select a topic modeling method (M) and implement using a standard library Implement the iterative topic modeling framework with time series feedback Train against the NYT dataset Implement the experiment in the paper and produce the same set of sample results Challenges Corpus size is significant (3.06 GB), but it should fit in memory on my laptop (which has 32 GB RAM). This will be a significant undertaking to perform myself, but I'm taking the final early to give myself more time to get this done.
https://github.com/MM026184/CourseProject	README.md	"CS 410 Course Project - Michael McClanahan (mjm31) All project deliverables (source code, documentation, and presentation) were completed by me as I was not the member of a team. Reproduction of Paper Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback (Kim et. al. 2013) Link to paper Overview of Implementation This project attempts to reproduce the 2020 Presidential Election experiment outlined in the paper above. All programming was done in Python 3.6 (specifically Anaconda distribution 4.4.0). A requirements.txt file is provided outlining all non-standard libraries utilized and their associated versions. pandas==1.1.5 gensim==3.8.0 nltk==3.4.4 scipy==1.5.4 numpy==1.19.4 All source code is contained within ITMTFPresidential.py as a series of functions. For convenience, the following objects have been serialized to files (specifically .pkl files) for easy re-use: president_norm_stock_ts : This is the non-text time series containing normalized presidential stock prices (May through October 2020 for the Bush-Gore 2000 presidential race. gore_bush_nyt_ts : This is the document time series containing NY Times articles from May through October 2020 mentioning either Bush or Gore. cleaned_doc_list: This the document collection to be analyzed (ie: a list of documents represented each as a list of tokens or words). gensim_dictionary: This is the gensim dictionary object to be analyzed (built from gore_bush_doc_list). gensim_corpus: This is the gensim corpus to be analyzed (ie: a list of documents represented each as a list of wordIDs and their counts in the document) word_impact_dict: This is a dictionary of corpus {wordID:(impact score,p-value)}. It represents the result of section 4.2.2's Word-level Causality analysis. At runtime, if the script's reload_data variable is set to False, the script will reload president_norm_stock_ts and gore_bush_nyt_ts from disk in O(1) time. If set to True, functions build_datasets() and parse_nyt_corpus_for_gore_bush() will get called and rebuild these datasets from a .csv file and the NYT corpus for XML documents, respectively. Since the NYT dataset was too large, it was not uploaded to this repository. Therefore, setting this variable to True is not recommended. Additionally, if the script's build_new_corpus variable is set to False, it will reload all of the other remaining objects from disk in O(1) time. If set to True, it will rebuild all of the other objects by rebuilding the collection, the gensim dictionary, and the gensim corpus (which is the object passed to the LdaModel() for its corpus parameter. It will then reperform the Word-level Causality Analysis from 4.2.2, storing the result per gensim dictionary word ID in a dictionary for quick lookup during ITMTF iterations. The following 4 parameters are then set and ITMTF iterations are started by calling the ITMTF() recursive function. min_significance_value = 0.8 min_topic_prob = 0.001 iterations = 5 number_of_topics = 10 causal_topics = ITMTF(gore_bush_gensim_corpus,gore_bush_gensim_dictionary,number_of_topics,number_of_topics,word_impact_dict,gore_bush_nyt_ts,president_norm_stock_ts,ts_tsID_map,min_significance_value,min_topic_prob,iterations) The ITMTF function will call itself for the number of iterations specified, each time building an LdaModel() object, passing in a 2D matrix (num_topics,num_unique_terms) matrix of re calculated prior topic word probability distributions into its eta parameter. To re-calculate the prior, causal topics are first extracted during the topic-level causality analysis (see section 4.2.1 in the paper), which is performed using a Pearson correlation against the reference Topic Stream and the presidential stock price time series. The topic-word probability prior is then calculated for top words in causal topics in the build_prior_matrix() function. This function makes use of the word_impact_dict object above for each causal topic. With each iteration, .csv files causal_topic_words.csv and itmtf_stats.csv in the working directory are appended with a list of signficant topics and their top 5 words as well as that iteration's average causality confidence and average purity, respectively. Goal The project aims to reproduce the paper's results documented in Table 2 and Figure 2(b). You will note that the u parameter from the paper is not defined prior to calling the ITMTF function above, primarily because it is not an parameter for the LdaModel() class provided by Gensim. It is for this reason that Figure 2(a) from the paper was not reproduced. How to Use Install the most recent versions of the above non-standard libraries using pip in a Python3 environment. Ex: pip install pandas Clone the repository, which contains the working directory and all dependent objects. Navigate to the working directory and run the script with python ITMTFPresidential.py Presentation Files Presentation slides Voiced Presentation - To listen, open the .pptx document in Powerpoint, then navigate to the Slide Show tab and hit the From Beginning button. The presentation should start from there. Voiced Presentation Video - If unable to listen directly in Powerpoint, you can view it as a video in the Illinois Media Space. Video Demo'ing Code Results In general this implementation's results were poorer than what was noted in the paper. Improvements in causality confidence and purity were not observed with more iterations. Top words for causal topics seemed applicable, but not all that compelling or different from one another. Poorer results are likely due to the following differences with this implementation: - Lack of a background model - Although the paper doesn't explicitly cite the use of a background model prior, results would imply they had one. My topics seem to have a lot more ""background words"" (ex: would). - Lack of a background could also be the a main reason for substantially lower purities with each iteration (0-5% vs 40-100%). - Lack of u parameter - This is likely the reason that neither purity nor causality confidence were improved with each iteration, regardless of the number of topics to begin with. Using this parameter would have ensured that prior words (and topics) appeared with the next iterations results."
https://github.com/VisualPracticeRank/CourseProject	README.md	Visual Practice Rank Goal The goal of this project is to create a visual representation of a supplied ranking function in a form that is comparable to modern search engines with the display of additional data that would be only in the background Project Presentation https://mediaspace.illinois.edu/media/t/1_3s2zdgys Getting Started Installation Create a virtual environment using conda: conda create -n [your-env-name] python=3.6 e.g. conda create -n vpr python=3.6 anaconda Activate the virtual environment conda activate [your-env-name] e.g. conda activate vpr Download the repository git clone https://github.com/VisualPracticeRank/CourseProject.git Install the packages in requirements.txt cd CourseProject pip install -r requirements.txt Running the Django server While in the CourseProject folder, cd into VPR, list of the files and folders and you will see a file called manage.py. Run the following command: python3 manage.py makemigrations python3 manage.py migrate --run-snycdb python3 manage.py runserver A browser with the application will pop-up, or you can head to 127.0.0.1:8000 If you are running this on a VM, instead of python3 manage.py runserver, you can run the following command: python3 manage.py runserver 0.0.0.0:8000 Then, head over to [your ip]:8000, e.g. 18.219.133.210:8000 Shutting down the Django server If you want to shutdown the Django server, you can do ctrl+c in the terminal to shut down the server. You can also deactivate the virtual environment with this command: conda deactivate Features Dataset You can upload your dataset by selecting Dataset on the homepage and fill in and upload the appropriate files needed: 1. Name 2. Description 3. Data 4. Qrels 5. Queries and select Upload. Model In addition to the default models available ('OkapiBM25', 'PivotedLength', 'AbsoluteDiscount', 'JelinekMercer', 'DirichletPrior'), you can upload your own custom models. You can upload your own model (ranker) by selecting Model on the homepage and fill in the textboxes: 1. Name 2. Description 3. Model and select Add. Query After selecting the dataset and model that you want to use, you can specify a query in the textbox and select Search. You will get the top 10 documents with the highest score in your model, displayed in descending order. Step Through This functionality allows you to step through the queries.txt file you specified when uploading the dataset and observe the changes in the ndcg score and other various stats. After selecting the dataset and model that you want to use, you can select Step Through. You can use the '>>' and '<<' buttons to step through and step out. How It Works Overview This program is implemented using Django (Python, HTML, SQLite3) as the frontend and metapy (and Python) as the backend. Frontend The dataset details are stored in the webui_dataset. The dataset, qrels, and queries files are stored in a folder, with a unique name generated by the system, under the dataset folder. The webui_dataset has the following fields: id (primary key), name, description, data, qrels, queries. When a dataset is being uploaded, the documents in the dataset is loaded into webui_document with the following fields: id (primary key), document_id, body, dataset_id (foreign key to webui_dataset.id). When a model is being uploaded, the model is loaded into webui_model with the following fields: id (primary key), description, model, name. Before storing the actual model, it will be encoded to base64. After you can select a dataset, model, and query, you click on Search. Then frontend (in view.py) will call the backend (search_eval.py) and pass the following variables: folder (folder where the dataset is stored), model, query. For the iteration feature, the frontend will send these information to the backend: folder (path to dataset), model. After receiving the response from the backend, the frontend will display the following information: query (as specified in the queries.txt), NDCG, a table of the following information: the score of the document, size of the document, unique terms in the document, and snippit of the body. Backend The backend (search_eval.py) will these variables from the frontend: folder, model, query. It will first change its directory to [folder_to_dataset]/datasets and build an inverted index. Then, it will determine if the model is one of the defaults ('OkapiBM25', 'PivotedLength', 'AbsoluteDiscount', 'JelinekMercer', 'DirichletPrior'). If it's not, then it will decode the string (using base64) and build the ranker. Finally, it will run the ranker with the inverted index, query for the top 10 documents and return the top 10 documents, and their score. For the iteration feature, the backend will utilize the qrels.txt and queries.txt that were uploaded when uploading the dataset. It will return a list of list: 1. results[0] = list of top k articles 2. results[1] = list of ndcg 3. results[2] = list of running avg ndcg 4. results[3] = list of queries Reference Chase Geiglem, 2017. [2-search-and-ir-eval.ipynb] (https://github.com/meta-toolkit/metapy/blob/master/tutorials/2-search-and-ir-eval.ipynb).
https://github.com/CoreyShih/CourseProject	progress_report.pdf	Progress Report: Text Classification Competition Corey Shih (coreys2@illinois.edu) Progress: So far, the data preprocessing pipeline and model definition have been completed. I decided to try using the recently developed BERT transformer encoder architecture rather than an LSTM as I originally planned for this project. Most of my time thus far has been dedicated to preprocessing the data and learning how to use BERT in Tensorflow. At present, I decided to use only the most recent context element in conjunction with the response text in the model for simplicity and to cut down on computation. Should the results of such a model not prove sufficient I will look into adding the rest of the context back into the model input. Remaining Tasks: Model training/testing and hyperparameter tuning still has yet to be done. Following that, the source code submission/documentation and the video demo will have to be completed. Challenges/Issues: At present, I am simply concatenating the response tweet with the context to feed into the BERT model as a single input. Ideally, I would like to have multiple inputs to the model, i.e. one input for the response text and one input for the context text, but I have been running into issues getting that to work in Tensorflow. I am still working on this aspect of the model.
https://github.com/CoreyShih/CourseProject	project_proposal.pdf	Project Proposal: Text Classification Competition Corey Shih (coreys2@illinois.edu) 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This will be an individual project; as such, I will be the team captain. My name and NetID are at the top of this proposal. 2. Which competition do you plan to join? I plan on joining the text classification competition. 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. I am prepared to learn deep learning techniques for state-of-the-art text classification. I am familiar with Keras and TensorFlow, having used the former for time series forecasting with recurrent neural networks and the latter for various deep learning tasks. I have previously utilized TensorFlow for projects involving audio classification and computer vision. I will likely implement an RNN or LSTM using one of these frameworks for this project. 4. Which programming language do you plan to use? I plan on using Python for this project, as both Keras and TensorFlow are Python libraries.
https://github.com/CoreyShih/CourseProject	README.md	"Text Classification Competition: Twitter Sarcasm Detection This course project is an entry into the CS 410 Fall 2020 text classification competition. More details about the competition and the relevant datasets can be found at https://github.com/CS410Fall2020/ClassificationCompetition. Implementation details This project utilizes a BERT (Bidirectional Encoder Representations from Transformers) model to classify the data. BERT is a recent NLP language model developed by Google in 2018. The specific BERT model fine-tuned in this project can be found here. The classifier model constructed in this project consists of a BERT preprocessing layer, followed by the BERT model, a Dropout layer, and a Dense layer. It uses the response tweet and last context tweet as inputs. This was done for simplicity and also because, intuitively, the tweet that the response tweet is a direct response to is the most relevant context for determining sarcasm. No preprocessing was performed on the text data itself prior to input to the BERT preprocessing model (e.g. stop word removal, stemming, etc.). The model was trained using a sparse categorical cross-entropy loss function and an AdamW optimizer. This model proved sufficient to beat the baseline provided by the competition, so no other models or methods were tried. Minimal hyperparameter tuning was performed, with only the number of training epochs being adjusted for best results and to avoid overfitting. The code for this project predominantly follows the TensorFlow tutorial found here for using BERT on TPU in Google Colab, with modifications made to accomodate for the structure of the Twitter dataset. Utility testing methods Several utility methods are included in the code to assist with running the model on the test dataset and viewing the results. ```python def preprocess_dataset(dataset_path, split): """"""Processes Twitter sarcasm data into tf.data.Dataset. Args: dataset_path: str path of jsonl dataset. split: str designating train or test dataset, either 'train' or 'test'. Returns: A tf.data.Dataset of the Twitter sarcasm data, retaining only the response tweet, the last context tweet, and the label if present. """""" ``` ```python def prepare(record): """"""Prepares records from processed dataset for prediction. Args: record: dict of str Tensors. Returns: A list of lists of str Tensors. """""" ``` ```python def get_result(test_row, model): """"""Predicts whether a Twitter sarcasm test example is sarcasm or not sarcasm. Args: test_row: list of str Tensors. model: TensorFlow SavedModel for the sarcasm classifier. Returns: A str, either 'SARCASM' or 'NOT_SARCASM', corresponding to the predicted result. """""" ``` ```python def print_result(test_row, model): """"""Prints out the context, response, and predicted label for a Twitter sarcasm test example. Args: test_row: list of str Tensors. model: TensorFlow SavedModel for the sarcasm classifier. Returns: None. """""" ``` Usage examples The following code snippets provide a few examples for running the trained classifier on the Twitter sarcasm test set. Loading the trained model python load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost') reloaded_model = tf.saved_model.load(saved_model_path, options=load_options) Downloading and preprocessing the test dataset ```python test_url = 'https://raw.githubusercontent.com/CS410Fall2020/ClassificationCompetition/main/data/test.jsonl' test_path = tf.keras.utils.get_file('test.jsonl', test_url) test_dataset = preprocess_dataset(test_path, 'test') ``` Printing some classifier results on the test set python for test_row in test_dataset.shuffle(1800).map(prepare).take(5): print_result(test_row, reloaded_model) ``` context: [b'@USER @USER @USER It \xe2\x80\x99 s obvious I \xe2\x80\x99 m dealing with a double digit IQ . Have a good life .'] response: [b'@USER @USER @USER Hahahahahah What a chump . No testicular fortitude at all . It \xe2\x80\x99 s unsurprising that liberals lose with people like this . '] prediction: SARCASM context: [b'@USER @USER asked me to respond to @USER . See attached . Thanks for the opportunity . #KXL '] response: [b'@USER @USER @USER Imagine that . A politician making baseless accusations . Because has * never * done that before .'] prediction: SARCASM context: [b'@USER @USER By all means you should initiate another failed impeachment , causing further embarrassment ( if that were even possible ) to your party , then go tear up some official documents like a toddler .'] response: [b'@USER @USER @USER Yet you have no shame in supporting the biggest criminal in White House history .'] prediction: SARCASM context: [b'@USER @USER Aaaayyyyyeeee I \xe2\x80\x99 m the Hypemobile humie . I gatchu ! ! ! \xf0\x9f\x98\x9c \xf0\x9f\x91\x8c \xf0\x9f\x8f\xbc'] response: [b'@USER @USER Oh ... OHHHH ! That \xe2\x80\x99 s how it \xe2\x80\x99 s gonna be ? My two bestfriends just gon \xe2\x80\x99 team up on me ? \xf0\x9f\x98\xa1'] prediction: NOT_SARCASM context: [b""@USER @USER @USER Thank You so much , Diablo , My Dearest Friend ! I am grateful every day , as I am happy every day . I make those choices every day for me ! It doesn't matter what is going on , my choices stick . I give the gift of positivity to myself , to everyone I touch ! LOVE U XOXO ""] response: [b'@USER @USER @USER All good things are possible when the #heart illuminates the mind #ThinkBIGSundayWithMarsha #InspireThemRetweetTuesday '] prediction: NOT_SARCASM ```"
https://github.com/alany9552/CourseProject	final project report.pdf	"Reproducing Paper: Latent Aspect Rating Analysis Chengmin Huang Ge Yu Xuehao Wang Introduction and Overview In this paper, we will introduce the steps of building Latent Aspect Rating Analysis(LARA) in order to mining the opinion ratings on topical aspects of given certain services type(Hotel reviews, restaurant reviews etc.). Specifically, it will infer the opinion ratings and relative weights focusing on the different aspects based on the reviews from the website such as Yelps, Trip Advisor, Amazon. We'll be using Trip Advisor as our dataset. The basic model of LARA will be based on the pre-defined aspect keywords, whereas the advanced LARA model won't need the supervision of predefined aspect keywords. Since it's the time where the data is everywhere , LARA is helpful for users to digest a larger amount of the online reviews about a specific entity of tropic. In our project, we use the hotel reviews dataset provided by the Trip Advisor. Nowadays, most websites already decompose the overall rating into different specific aspects. For example, the hotel reviews might have such values, rooms, cleanliness and other categories. Since different users emphasize different aspects, it might still not be informative. Our LARA model can infer the relative emphasis placed by a reviewer on different aspects by digging into their specific reviews. LARA takes review texts about an entity as an input, and will produce output as 1) the ratings on a set of predefined aspects 2) the relative weights that the user placed based on their review texts. For our implementation, we divide the LARA model into three stages: 1) Data Processing to process the raw data into the format for further processing 2) A Bootstrap algorithm to identify the aspects and segment of the processed review content 3) A Latent Rating Regression model to infer the aspect rating and weights in a review. The specific implementation will be introduced in the next section. Implementation and Documentation Data Reading and Processing In order to process the data, we developed several functions. First, we read the initialized aspect words and stop words. We downloaded the stop words from the nltk library. Then we read the reviews from the json file downloaded from the database, and call the stemming Stop Removal() function to 1)tokenize the reviews into sentences and words 2) Remove the stop words to improve the accuracy of the model 3)Add words to the vocabulary list 4) Make the sentence objects and corresponding review objects. Also, since there are words that have less frequency but could be affecting the overall results as outliers, we developed a function to remove the words that have the frequency less than 5. After the processing step for the data, we call the functions in BootStrap.py to generate the processed word lists as local files. Bootstrap Algorithm As mentioned in the paper, the main usage of bootstrapping algorithms is to identify the aspects and segment the review content. We use a bootstrapping algorithm to generate the keywords. This is the code that we used from others and changed some details in order to make it satisfy our own goal . The assignAspect function: this is basically just assigning aspects to sentences. The chiSq and calcChiSq: these two functions are used to generate the chi-square value which is used to tell you how much difference exists between the observed date and the data you would expect to get. populateLists: this function is used to generate the word list. The bootStrap function: it is used to execute the algorithm, it basically implements all the functions I mentioned above. And saveToFile function is just simply saving the file. All the files are saved in the modelData folder. wList.json is a list of words and their frequency matrix, ratingsList.json is list of ratings dictionary belong to review class, reviewIdList.json is list of review IDs, vocab.json is the list of all the vocabularies that being selected and the aspectKeywords.json is the file that contains the keywords that we obtain using the bootstrapping algorithm. Then we applied a linear rating regression model with these keywords. Linear Rating Regression After identification of aspects and segments in review content, the authors applied the Latent Rating Regression(LRR) model to complete the prediction. The LRR model mainly consists of two steps. The input of the LRR model is a list of words and their frequency in the review content. At the beginning of LRR model, the word list is separated into two subset i.e. training set and testing set. The original code separated 75% word frequency data into training set and the rest of data into testing data. In the E-step of training step, the model constrained posterior inference. To be specific, it estimated the updated states using the current parameters. In the M-step, it updated the parameter estimation and maximized the log-likelihood of the whole corpus. The model is using the ""Overall"" rating of each review as the true value and calculating the likelihood between these values and prediction values. However, review text might not be directly related to the overall rating values. One possible improvement is to replace the ""Overall"" rating with the average of all aspects numbers such as ""Service"", ""Cleanliness"", and ""Location"". Another possible improvement is that since the original model didn't use the validation dataset during the training step while the validation set could help with the optimization and convergence of parameters. Further improvement The results of this model are promising and meaningful. Even though there are some large numbers in the prediction values, the overall trend and relevant values are almost consistent with the actual values. It can be told that the prediction numbers of actual 5.0 rating is greater than the prediction numbers of actual 3.0 rating in the order of magnitudes. We think the possible reason for these large numbers might come from the bag-of-words assumption. This assumption limited the model's aspect segmentation capabilities. At the beginning of E-step and M-step, the calculation parameters Mu and Sigma are set to the large range of numbers which leads prediction values to increase cumulatively. Even after many iterations steps, it would be difficult to lower these parameters down to the reasonable range. Further improvement can focus on the normalization of these prediction values to the [0, 5] range. In this way, it would be more clear to compare these two kinds of values and evaluate the performance of this model. Furthermore, the initialization of these parameters might also help with the improvement of accuracy. Also, like mentioned in the paper, we successfully implemented the proposed method of using LRR as the model with the aspect keywords as supervision. However, we fail to implement the advanced model which has a better performance without using predefined aspect keywords as supervision. In the future, we will implement the improved model and compared with our model using bootstrap algorithm and LRR model. Usage of software This paper proposed a generative LARA model and used the model to infer the opinion rating on topic aspects. Also it improves the model by eliminating the use of pre-defined aspects of keywords. The software can be downloaded from https://github.com/alany9552/CourseProject. To run the software, users need to make sure they have installed the Python3 environment on their device. Also, the software uses nltk  stopwords so users should use import nltk, nltk.download('stopwords'), and nltk.download('punkt') to download the necessary dictionaries. After the completion of installation, users can run the software using python3 ReadData.py, python3 BootStrap.py, and python3 LRR.py sequentially. Then, the running results would show up. The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively. Also, there is a simple classification at the end of prediction that the review would be positive when the ""Predicted OverallRating"" is greater than 3.0 or negative when it is smaller than 3.0. The software running can also be customized by users in terms of ratio of training dataset and testing dataset. In the line 46 LRR.py file, the users can change the percentage of the training set. Currently, the training dataset and testing dataset are in 3:1 ratio. In addition to the training ratio, users can also specify the maximum interaction steps and coverage threshold in line 370. Moreover, if they want to change the maximum interaction steps much lower, the changing of line 339 is also needed. This model is applied to predict the review score of hotels and restaurants based on the review text. Therefore, it can be generalized to the prediction of most opinion tasks. Elimination of predefined aspects of keywords enables this model to be applied in various areas. For example, as mentioned in the paper, it can be applied to reviewer behavior analysis, topic opinion prediction, and personalization recommendations. Contribution of each team member in case It is hard to say who contributes to which part of the project since we are basically all doing work that overlapped. We first all read and understand the paper by ourselves, then we gathered and shared our understanding together. After uniting the idea, the team leader (GeYu) gave each of us different work. To be more specific, Chengmin Huang and Ge Yu contributed more in coding, and Xuehao Wang contributed more in testing and processing the data. Citations and Contributors Original Implementation of the authors: http://www.cs.virginia.edu/~hw5x/Codes/LARA.zip Since the implementation of the LRR and bootstrap algorithm, we extend existing models from the web as our model per the instructor's instructions and directions, and did some changes to fit our models better: https://github.com/redris96/LARA https://github.com/seanliu96/LARA https://github.com/biubiutang/LARA-1 Data Sources: http://times.cs.uiuc.edu/~wang296/Data/ Overall implementation and Ideas: ""Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach"", http://sifaka.cs.uiuc.edu/~wang296/paper/rp166f-wang.pdf ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"", http://sifaka.cs.uiuc.edu/~wang296/paper/p618.pdf"
https://github.com/alany9552/CourseProject	progress report.pdf	CS410 Project Progress Report Ge Yu - gey2 Chengmin Huang - ch61 Xuehao Wang - xuehaow2 * Progress made: 1. Understand the advantages of Latent Aspect Rating Analysis Model compared to previous two-step solution 2. Data from TripAdvisor has been processed into SQL database * Remaining tasks: 1. Try to pull more datas from the website to make the model more meaningful 2. Algorithms to calculate the K-means from different features 3. Still try to implement algorithms described in the paper * Challenges: 1. Fit the model with the bag-of-word assumption 2. How to determine the aspect segments and their weights 3. Acceleration on the training process for the test dataset 4. Dimensionality reduction to solve the data sparsity
https://github.com/alany9552/CourseProject	project proposal.pdf	1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Xuehao wang: xuehaow2 GeYu: gey2 Chengmin Huang: ch61 GeYu is the captain 2. Which paper have you chosen? We choose the first one Subtopic: Latent aspect rating analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 3. Which programming language do you plan to use? We plan to use python 4. Can you obtain the datasets used in the paper for evaluation? Yes, we can obtain from this website http://times.cs.uiuc.edu/~wang296/Data/
https://github.com/alany9552/CourseProject	README.md	"CourseProject for CS410 at University of Illinois at Urbana-Champaign Presentation Videos: https://mediaspace.illinois.edu/media/t/1_jj5lzils https://www.youtube.com/watch?v=o0bTraiYkNM&feature=youtu.be Citations and resources: The python implementation is following the instructions from research papers: ""Latent Aspect Rating Analysis on Review Text Data: A Rating Regression Approach"", http://sifaka.cs.uiuc.edu/~wang296/paper/rp166f-wang.pdf ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"", http://sifaka.cs.uiuc.edu/~wang296/paper/p618.pdf This implementation uses the orignal implementation given by the author (http://www.cs.virginia.edu/~hw5x/Codes/LARA.zip). Also, the Latent Rating Regression model and Bootstrap use the implementation of existing models from: https://github.com/redris96/LARA https://github.com/seanliu96/LARA https://github.com/biubiutang/LARA-1 Contributors: Chengmin Huang - ch61@illinois.edu Xuehao Wang - xuehaow2@illinois.edu Ge Yu - gey2@illinois.edu Organization of the implemenataion: src/hotelRivews: This directory includes the hotel reviews download from the database http://times.cs.uiuc.edu/~wang296/Data/ from TripAdvisor. For testing, we include only one review file. You are free to download the whole dataset to do a better training, which might take up to 30 minutes to run 10 json files. src/Settings: This is the directory to store the pre-difined laten words and stopwords downloaded from nltk libaray. src/modelData: This is the directory to store the data after processed by readData.py, the file includes the ratings, reviewID, each word's frequencyy and the aspectKeywords after reading thereviews. src This is the directory to store all of the files including three main class: readData.py that includes the data proessig methods, BootStrap.py that contains the boot strapping algorithms and LRR.py that implemented the Linear Rating Regression model. How to run the model: To run the software, users need to make sure they have installed the Python3 environment on their device. Also, the software uses nltk stopwords so users should use import nltk, nltk.download('stopwords'), and nltk.download('punkt') to download the necessary dictionaries. Step-by-step tutorial: https://mediaspace.illinois.edu/media/1_s4x9i7wo https://www.youtube.com/watch?v=sbRppuoFsgY&feature=youtu.be Background: Python 3.7 Required Packages: NLTK, numpy, panda, pandas, matplotlib Specific Step: After the completion of installation, users can run the software using python3 ReadData.py, python3 BootStrap.py, and python3 LRR.py sequentially. Then, the running results would show up. The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively in your terminal window. Customize: The software running can also be customized by users in terms of ratio of training dataset and testing dataset. In the line 46 LRR.py file, the users can change the percentage of the training set. Currently, the training dataset and testing dataset are in 3:1 ratio. In addition to the training ratio, users can also specify the maximum interaction steps and coverage threshold in line 370. Moreover, if they want to change the maximum interaction steps much lower, the changing of line 339 is also needed. Reading Results: The results will list the ""ReviewId"", ""Actual OverallRating"", and ""Predicted OverallRating"" respectively. Also, there is a simple classification at the end of prediction that the review would be positive when the ""Predicted OverallRating"" is greater than 3.0 or negative when it is smaller than 3.0."
https://github.com/codylw2/CourseProject	progress_report.docx	"Cody Webster Progress Report Progress made thus far Generic Wrote script(s) to process documents and convert to a json format for easy loading in the future. Json files contain tokenized representations of the text and queries Cranfield Wrote script to convert json text data into Cranfield datasets as required for the metapy module. Adapted the scripts for homework to find the optimal values of weights in for the JM and BM25 algorithms. Adapted the scripts for homework to ranking documents. BERT Downloaded Google's MS-Marco pre-trained BERT model. Wrote script to examine documents and determine missing vocab that is common to the corpus. Adapted the scripts in the git repo (https://github.com/cognitiveailab/ranking) to finetune and train the model with the tensorflow_ranking python module. Adapted the scripts in the aforementioned git repo to run the scripts in a Docker container running in Window's Subsystem for Linux on my desktop's GPU. Rewrote the script for running to Docker to run outside of Docker for use with Google COLAB. Wrote script to convert ranking output into a predictions file. Remaining Tasks Generic Consolidate scripts and files that are spread over several directories and add into the git repo. Cranfield Run again with titles included in the ""text"" (not likely to beat baseline) BERT Finish running the BERT model and compile results. Each document/query combination that is longer than 512 broken tokens has been into segments. I will test whether a mean, geometric mean, or max score within the segment is a better metric. Test if I can use a larger BERT model with Google Colab (currently using BERT Small) Any challenges Adapting the examples in the git repo so that I can run the BERT model on my system or in git Hardware limitations on my PC. Partially solved by using Google COLAB Scoring the documents with BERT takes an excessive amount of time due to the large number of documents in the corpus and the limit of 512 tokens per analysis run. This excessive wait on results has seriously slowed my progress. Actually beating the baseline performance."
https://github.com/codylw2/CourseProject	progress_report.pdf	"Cody Webster Progress Report 1) Progress made thus far Generic Wrote script(s) to process documents and convert to a json format for easy loading in the future. Json files contain tokenized representations of the text and queries Cranfield Wrote script to convert json text data into Cranfield datasets as required for the metapy module. Adapted the scripts for homework to find the optimal values of weights in for the JM and BM25 algorithms. Adapted the scripts for homework to ranking documents. BERT Downloaded Google's MS-Marco pre-trained BERT model. Wrote script to examine documents and determine missing vocab that is common to the corpus. Adapted the scripts in the git repo (https://github.com/cognitiveailab/ranking) to finetune and train the model with the tensorflow_ranking python module. Adapted the scripts in the aforementioned git repo to run the scripts in a Docker container running in Window's Subsystem for Linux on my desktop's GPU. Rewrote the script for running to Docker to run outside of Docker for use with Google COLAB. Wrote script to convert ranking output into a predictions file. 2) Remaining Tasks Generic Consolidate scripts and files that are spread over several directories and add into the git repo. Cranfield Run again with titles included in the ""text"" (not likely to beat baseline) BERT Finish running the BERT model and compile results. Each document/query combination that is longer than 512 broken tokens has been into segments. I will test whether a mean, geometric mean, or max score within the segment is a better metric. Test if I can use a larger BERT model with Google Colab (currently using BERT Small) 3) Any challenges * Adapting the examples in the git repo so that I can run the BERT model on my system or in git * Hardware limitations on my PC. Partially solved by using Google COLAB * Scoring the documents with BERT takes an excessive amount of time due to the large number of documents in the corpus and the limit of 512 tokens per analysis run. This excessive wait on results has seriously slowed my progress. * Actually beating the baseline performance."
https://github.com/codylw2/CourseProject	project_documentation.docx	"IR Competition Project Documentation Cody Webster Contents Overview 3 Project Demonstration 3 Project Results 3 DirichletPrior 3 JelinekMercer 4 OkapiBM25 4 BM25+ 4 BERT 5 SciBERT *difficulty running this model limited options 6 Tensorflow Ranking custom model 7 Assumptions 7 Installation Instructions 7 What to install if running locally 7 What python packages to install 8 How to Run 8 Prepare Data (Required if local) 8 (Method 1) Run BM25+ or another ranker (reproduces best results) 9 Option 1 (Google Colab) 9 Option 2 (run locally) 9 (Method 2) Run Tensorflow Ranking 9 Option 1 (Google Colab) 9 Option 2 (run locally) 9 (Method 3) Run a BERT model 9 Option 1 (Google Colab) 9 Option 2 (run locally) 10 Script API Documentation 10 General 10 Metapy (BM25, ...) 12 Tensorflow Ranking 13 BERT 16 Resources 20 Citations 20 Installation Guidance 20 Stopwords 20 BM25 21 Tensorflow Ranking 21 Tensorflow Ranking (how to serve a model) 21 Tensorflow Ranking (how to predict) 21 BERT Checkpoint Conversion 21 BERT 21 SciBERT 21 Docker 21 Overview The purpose of this project is to participate in the IR Competition. For this project I developed and extensively tested three main ways to rank documents. The first is ranking documents using a ranking function either packaged with metapy or through a custom definition implemented through metapy. Initially, the best results that I achieved with this method were through a custom implementation of BM25+. Through a longshot attempt on the last day, I actually beat all of my other rankings using metapy's BM25 implementation. The second is ranking documents using a custom model that is trained using the python library tensorflow_ranking. The third option is to use a pretrained model and it's associated vocabulary and then finetune it using tensorflow_ranking. Overall the best results that I have achieved have come from running the OkapiBM25 algorithm implemented with metapy and using parameters that were produced from a brute force optimization. Project Demonstration I have uploaded ""cs410_project_demo.mp4"" into the git repository. I have also uploaded the file to Illinois Media: https://mediaspace.illinois.edu/media/t/1_07py0q5f Project Results Unfortunately, despite my best attempt I was not able to beat the initial baseline performance score as defined by the class professor or TAs. Nobody else was able to beat the initial baseline either so it is likely that baseline was an unreasonable standard for us to achieve with our limited knowledge and experience. In this section I will briefly detail some of the variations and experiments that I attempted. It is not possible for me capture every variation that I tried but I will cover as best as I can. I do not have a consistent record of the score for every attempt so I will not be including those scores. On the last day of the competition, the class administrators lowered the score of the baseline. I decided to give it another shot because I really had nothing to lose. I was able to beat the new baseline using the standard BM25 algorithm implemented via metapy. The only difference between what I did on the last day and what I had done on previous days was that I allowed one of the parameters to vary more than I initially had. I will detail that more within the BM25 section. DirichletPrior This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. Initial attempts with this algorithm were made using the body text only. Later attempts also used the title and abstract/intro. Further experimentation with the KLDivergencePRF implementation in metapy and variations on its parameters was also performed but performance lagged behind other approaches. JelinekMercer This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non-ideal approach. This algorithm was only ever attempted with the body text and results were deemed not good enough to justify further experimentation. OkapiBM25 This algorithm was implemented using metapy. The value of the three parameters was optimized on the training dataset using a brute force approach of testing every reasonable combination of parameters and selecting the set that achieved the greatest. Due to the number of variations that I tried with this algorithm, I had to optimize the parameters a number of times and I developed a multi-processing script that was capable of doing this at a much quicker pace. Initially I only attempted to run the algorithm on the body text for each paper but the performance for that was poor. With the body text I attempted to optimize a different NDCG values, 10, 20, and 50 respectively. None of them produced significant improvements in the overall performance. Next, I attempted to use the title, abstract, and introduction along with the body text and that produce marginal improvements in the score. Another boost came once I removed the body text from the dataset and only trained on the title, abstract, and introduction. I also attempted to use metapy's implementation of Rocchio feedback. I optimized the parameters for Rocchio in a similar manner to the normal OkapiBM25 algorithm. Ultimately the Rocchio feedback produced worse results then the standalone ranker so I excluded it from further test with this algorithm. The performance was initially still inadequate so I pursued further methods. One the last day, after I noticed that they had lowered the baseline, I attempted some new variations of this algorithm. I did not run the other algorithms because they all take longer to run and I did not have time to implement anything new. The new variation that I tried on the last day was to let the k3 parameter vary. I had previously not done this due to the faulty assumption that it would only hurt my results because that was the experience that I had during MP2.2. Almost immediately while running the pooled optimization, I was able to see that the performance was superior my previous results. I selected a number of the different parameter combinations to try from the generated set. On the third try I was able to beat the baseline using k1=2.0, b=0.75, and k3=4450. I am immensely frustrated that I have wasted so much effort trying to find other ways to rank documents and the answer was so simple but I am happy to have beaten the baseline. BM25+ This algorithm was implemented with metapy's api using the definition of BM25+ from (Trotman et al, 2014). Since it was implemented in Python and C++ like the native metapy implementations, it ran slower. I optimized it in the same way that I optimized the OkapiBM25 algorithm. This implementation ultimately outperformed the native OkapiBM25 algorithm, but it did not beat the baseline. Better results were obtained when using this in conjunction with the BERT implementation. I tried numerous variations on this algorithm in an attempt to increase my score on the test dataset. Similar to the other algorithms, I initially tried to only use the body text for my analysis. This did not produce the results that I desired. I have briefly listed some of the other variations that I tried: attempted with body text only attempted with title, abstract, intro, and body text attempted with title, abstract, and intro attempted with title only attempted with Rocchio pseudo feedback, varying the parameters to Rocchio attempted to remove all urls in the corpus attempted to replace all variants of the words coronavirus, covid-19, 2019-ncov, and sars-ncov-2 with coronavirus attempted to run with query text, question text, or narrative text attempted to run with all variations of query combined attempted different variations of metapy analyzer chains attempted to remove docs with duplicate s2_id but different uids, docs were functionally duplicates attempted to use a date cutoff for document attempted to pre-tokenize the queries and documents using BERT's tokenizer attempted to use multiple datasets with various weightings and combine into one result used different variations of stopwords that were gathered online (listed in resources) Ultimately, I was not able to beat the baseline using this custom algorithm, so I continued my exploration of other possibilities. BERT I had completed my Technical Review over the BERT model and had learned through that about its superior performance when classifying documents. I chose this as the algorithm to attempt to rank with. It was a significant struggle to get BERT to work for document ranking. There are very few tutorials online for how to achieve this and I spent a significant amount of time trying to get it to work. I ultimately was able to find a tutorial provided by Peter Jansen from the University of Arizona (link in resources). This tutorial gave an example of using the tensorflow_ranking module with BERT to achieve document ranking. Unfortunately, the tutorial relied on Docker to serve a fine-tuned model. I had to do a significant amount of researching about how to get Docker to work with a tensorflow model on Windows. In order to get it to work, I had to upgrade my Windows 10 OS to the Development version so that it would support the latest Windows Subsytem for Linux (WSL). I needed the latest WSL because that was what nVidia required for their latest CUDA drivers for GPUs. I needed the latest CUDA drivers because that was the only way to get Docker to run on my GPU using WSL. I also had to install Bazel for Windows because the training script was meant to be compiled to run. This lengthy setup process is obviously unsuitable for the rapid development that I needed and because it is not realistic for reviewers to reproduce this setup on their own machines. Docker is also not supported in Google Colab so I needed to arrive at a repeatable and easily setup solution for the graders. I was able to remove the need for Docker and the need to serve the model at all by exploring the documentation for tensorflow. This exploration give me insight into how to directly load the model and use it to predict document scores. I implemented this methodology into the scripts and was thus able to create a version that can be easily ran within Google Colab for easy review. Once I had an effective way of running the model, I was able to experiment with various ways to utilized it. I attempted to run three variations of the pre-trained BERT model, all provided by Google Research, BERT-Mini, BERT-Small, and BERT-Base. The BERT-Base model was too large to effectively train without utilizing TPUs. I choose to do my testing with the BERT-Mini model because it allowed me the most flexibility with my training parameters. Some key parameters used are list size and batch size. The size of the model along with those two parameters determine what hardware is required to run the model. If you try to run on inadequate hardware than you will easily run out of memory and be unable to train the model. I attempted a number of variations in ways to score or setup the data. Ultimately, I was not able to beat either baseline when using BERT but I was able to improve upon my initial results with BM25+. I have detailed some of the variations that I tried below. varied the list size varied the batch size updated vocabulary to include most common coronavirus variants attempted to run with query, question, or narrative attempted to vary the max token sequence size 256, 512, 1024 relevance = score of first doc segment relevance = max score of all doc segments relevance = mean score of all doc segments ignore body text and only use title+abstract+intro varied the number of training steps used both NCDG approximated loss and softmax loss re-rank top 1000, 2000, 5000, and 10000 results of BM25+ SciBERT *difficulty running this model limited options This model was initially very promising but the issues with getting it to run ultimately made it insufficient for what I needed. This model was create by Iz Beltagy et al. for scientific research. This model is essentially just a BERT-Base model that was pretrained on a scientific corpus instead of the generic one. The vocabulary consists of more scientific terms as a result and should theoretically be able to perform better at ranking the scientific documents in the CORD-19 corpus. I updated a few of the unused vocabulary items to the command variants of coronavirus and included the drug remdesivir as well. Unfortunately, due to the size of the model I was very limited in the list size and batch size options that I could try. I firmly believe that with a larger list size I would have been able to adequately finetune this model and it would have performed better than the BERT model. My inability to get the model to run on the TPUs available through Google Colab prevented me from realizing this goal. Tensorflow Ranking custom model This model was created by basing it off of the example in the tensorflow ranking repository. I left the structure of the context features and the example features the same. The context features contain the query tokens and the example features contain the document tokens and when training, the relevance judgement of the file. I attempted to change the script to use the argparse module instead of the flags from the absl module but when that was attempted the script stopped producing the results of the training data to stdout. I determine that there are likely scripts deeper within the tensorflow ranking module that are using these flags to determine various facets of the training process, so I restore the flags. The model consists of 3 hidden layers at a size of 64, 32, and 16 respectively. During testing the dropout rate of the model was adjust to 0.65 from the default of 0.80. This adjustment produced better results but further drops in this dropout rate risked overfitting the model to the training data. The batch size was set to 1 and the list size maximized so that it would train on more docs for a single query at once. Based on empirical results that I observed during experimentation, this produced better results than increasing batch size and lowering the list size. The maximum limit for the list_size was 100 depending on the hardware available in Colab and if it exceeded that value then it would run out of memory when training. One other significant variable was the max sequence length to use for each example. I tested with both 512 and 1024. The results did not appear to differ significantly between the two but this variable is important because it defines how much of the document can be captured per example. For this model I only used the title, abstract, and introduction because previous experimentation on other models and rankers showed that the body text was not helpful. Ultimately the results produced with this method were worse the results obtained from the BERT models. Assumptions If you are running this locally then you are running this code on Windows 10 machine that has ample RAM and a CUDA capable gpu. Every script can run on Linux but I have not generated scripts within this repo to accommodate that. All file paths are relative to the base directory of the repository Installation Instructions What to install if running locally Acquire the datasets: Files should be downloaded and unzipped here: .\competition\datasets The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y-MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing Python Instructions: https://docs.conda.io/projects/conda/en/latest/user-guide/install/ Latest Nvidia Drivers (required to run BERT or Tensorflow Ranking on GPU) Instructions: https://docs.nvidia.com/cuda/wsl-user-guide/index.html Instructions require installation of other software and packages you must follow all of them BERT Model (required to run BERT) Due to the size of the models, they are not directly included in the repository Google BERT Models (recommended): Google offers a variety of different sizes for the models to run. I ran using a BERT Mini but could possibly use a BERT Small as well. Anything larger likely requires a TPU through Google Colab to run. https://github.com/google-research/bert SciBERT Model (not recommended, requires TPU): https://github.com/allenai/scibert/ What python packages to install To run the full capability of all scripts defined in this project it is required that you setup two environments. All but one script can run in the main environment but the BERT checkpoint convertor relies on the dev version of a module that conflicts with requirements of the other modules: Main Python Environment: This is the main environment and should be used when running most of the scripts. You can install the latest stable versions of these modules to run the project. Python Version: 3.7 Pip Packages: tensorflow_ranking metapy pytoml BERT Conversion Environment: This is required because there are conflicts in the dependencies of the modules required to convert a model's checkpoints and the modules used to run it. Python Version: 3.7 Pip Packages: tf-models-nightly How to Run **WARNING: running some of these scripts will require a large amount of RAM if you use the full dataset** * Your python environment should be activated before running any scripts Prepare Data (Required if local) Generate the json file that contains the information for each dataset. This is required in order to run any of the different methodologies. File Path: .\competition\create_bert_data.bat (Method 1) Run BM25+ or another ranker (reproduces best results) Option 1 (Google Colab) Run each cell individually from top to bottom to fully. If you view the file within Github there is a link at the top of the file to open in Google Colab. This is the only Google Colab file that does not require a Google Colab Pro account. The free account should be capable of running this notebook. File Path: .\colab_cranfield_metapy.ipynb Option 2 (run locally) Generate the Cranfield Datasets that will be required to run the ranker File Path: .\competition\cranfield_metapy\cm_create_data.bat Run the ranker of your choice (default arguments already in the file) File Path: .\competition\cranfield_metapy\cm_rank_docs.bat (Method 2) Run Tensorflow Ranking Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. T This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_tfr.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\tfr_custom\tfr_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\tfr_custom\tfr_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\tfr_custom\tfr_predict.bat (Method 3) Run a BERT model Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_bert.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\bert\bert_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\bert\bert_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\bert\bert_predict.bat Script API Documentation **Unless otherwise specified paths are relative to the root of the repository General File: ./competition/check_covid_variants.py Purpose: Check for coronavirus variants in the corpus Source: developed by project team API: variant_file: the file that the coronavirus variants will be output to. File can already exist. Existing variants will be loaded. known_variants: known variants of the coronavirus that exist in the corpus and will be used to determine other variants doc_keys: the keys to use when search the document dictionary for variants run_type: the dataset that will be searched for variants input_dir: the directory that contains the json representation of the datasets to be processed Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. It loads the json for one or both datasets and then processes the text to find variants of some predefined words in the corpus and queries. The text is processed using python multiprocessing module in order to speed up execution of the script. File: ./competition/checkpoint_converter.py Purpose: Converter BERT checkpoint files from Tensorflow v1 to v2+ Source: Tensorflow Model Garden Repository API: bert_config_file: Bert configuration file to define core bert layers. checkpoint_to_convert: Initial checkpoint from a pretrained BERT model core (that is, only the BertModel, with no task heads.) converted_checkpoint_path: Name for the created object-based V2 checkpoint. checkpoint_model_name: The name of the model when saving the checkpoint, i.e., the checkpoint will be saved using: tf.train.Checkpoint(FLAGS.checkpoint_model_name=model). converted_model: Whether to convert the checkpoint to a `BertEncoder` model or a `BertPretrainerV2` model (with mlm but without classification heads). Detailed Description: This script was not written by me but was lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script is meant to convert a BERT module that you downloaded into a version that is capable of being ran using tensorflow version 2.0+. File: ./competition/create_bert_data.py Purpose: Combine and compile the information from the dataset into and easily loadable json for use by other scripts Source: developed by project team API: vocab_file: The file containing the vocabulary to be used when tokenizing a text variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants tokenize: If use, this will store a tokenized representation of the script in the output json. Must be used with `vocab_file` input_dir: The directory that contains all of the source files for the dataset output_dir: The directory where the json representations of the documents will be stored Detailed Description: This is a python script that was tested in python 3.7. The inputs to the script will determine the exact behavior at run time but lets discuss the more complete case of the training dataset. When the training dataset is specified it will start by loading the queries from there original xml format into a python dictionary. If the script is ran with the tokenize option then each of the three variants of the query (query, question, and narrative) will be tokenized according to the vocabulary defined in the specified vocab file. This json is then dumped into a target directory. For the training dataset it will also load the query relevance judgements. A list of all of the documents and their associated uid, title, abstract, publication date, and list of files containing the text representation is pulled from the metadata.csv file. The list of documents to further process is pruned done to the same as the list of relevance judgements. For each document to process the script iterates through the list of files that contains the text representation until it finds a suitable candidate or exhausts all options. The text from the representation file is loaded in the document dictionary object. If the tokenize option is used, the documents will then be tokenized through the use of a multi-processed pool. The resulting output is then written to the disk. Metapy (BM25, ...) File: ./competition/cranfield_metapy/create_cranfield.py Purpose: Use the json representation of the dataset to create a cranfield dataset for use with metapy Source: developed by project team API: variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants query_keys: The keys that will be used when creating the cranfield query file. If multiple keys are specified, text will be combined. doc_keys: The keys that will be used when creating the cranfield document files. Multiple keys can be specified and multiple keys can be combined into a single document. If creating separate datasets, use a ';' to separate keys. If combining keys for a dataset use a ':' to separate keys: cranfield_dir: The directory to use as a base for generating the cranfield data. input_dir: The directory where the json representations of the documents are stored Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. This uses the loaded json representation of the dataset to produce a use specified cranfield dataset for use with metapy. Each cranfield dataset can be uniquely defined by the user to consist of a one-to-one relationship with keys in the document dictionary or it can consist of multiple keys combined. This allows for flexibility in how the expansive each iteration of ranking testing is. File: ./competition/cranfield_metapy/search_eval.py Purpose: Rank the documents in the corpus and create the predictions file Source: original version from MP 2.2 and heavily modified to fit use case API: config_template: The template file that will be used for creating the configs for each run of the ranker run_type: The dataset that will be searched for variants dat_keys: The keys to use that indicate the name of the cranfield dataset(s). This corresponds to the first key used for every section of 'doc_keys' parameter and the 'create_cranfield.py' script. doc_weights: The weights to use when combining the rankings of multiple datasets. ranker: The ranker to use for ranking the documents. Valid rankers can be found in the script. params: The value(s) for the ranker parameters. Multiple values should be separated by `;`. cranfield_dir: The directory that is the base for the cranfield dataset(s) predict_dir: The directory to contain the predictions file. remove_idx: Delete an existing inverted index and create a new one. If no index exists it will not fail Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to specify what ranker to run from a predefined list of rankers. The user can also run rankings over multiple datasets and specify the weight that each ranking will contribute to the final prediction rankings. File: ./competition/cranfield_metapy/search_eval_pool.py Purpose: Used to find the optimal parameters for ranking algorithms. Not for production use. Source: original version from MP 2.2 and heavily modified to fit use case API: N/A Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to attempt to optimize the parameters for a predefined list of rankers. The optimization takes place using a multi-processed pool so that it can run numerous iterations over a predefined range of values. Each iteration of the optimization loop is evaulated based on normalized cumulative gain at 20 documents. Tensorflow Ranking File: ./competition/tfr_custom/tfr_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/tfr_custom/tfr_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_tfrecord.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: data_format: Data format defined in data.py. train_path: Input file path used for training. eval_path: Input file path used for eval. vocab_path: Vocabulary path for query and document tokens. model_dir: Output directory for models. batch_size: The batch size for train. num_train_steps: Number of steps for train. learning_rate: Learning rate for optimizer. dropout_rate: The dropout rate before output layer. hidden_layer_dims: Sizes for hidden layers. list_size: List size used for training. Use None for dynamic list size. group_size: Group size used in score function. loss: The RankingLossKey for the loss function. weights_feature_name: The name of the feature where unbiased learning-to-rank weights are stored. listwise_inference: If true, exports accept `data_format` while serving. use_document_interactions: If true, uses cross-document interactions to generate scores. embedding_dim: max size of any query or document Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of tfr_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by tfr_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/tfr_custom/tfr_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. BERT File: ./competition/bert/bert_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/bert/bert_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_example.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: local_training: If true, run training locally. train_input_pattern: Input file path pattern used for training. eval_input_pattern: Input file path pattern used for eval. learning_rate: Learning rate for the optimizer. train_batch_size: Number of input records used per batch for training. eval_batch_size: Number of input records used per batch for eval. checkpoint_secs: Saves a model checkpoint every checkpoint_secs seconds. num_checkpoints: Saves at most num_checkpoints checkpoints in workspace. num_train_steps: Number of training iterations. Default means continuous training. num_eval_steps: Number of evaluation iterations. loss: The RankingLossKey deciding the loss function used in training. list_size: List size used for training. convert_labels_to_binary: If true, relevance labels are set to either 0 or 1. model_dir: Output directory for models. dropout_rate: The dropout rate. bert_config_file: The config json file corresponding to the pre-trained BERT model. This specifies the model architecture. Please download the model from the link: https://github.com/google-research/bert bert_init_ckpt: Initial checkpoint from a pre-trained BERT model. Please download from the link: https://github.com/google-research/bert bert_max_seq_length: The maximum input sequence length (#words) after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. bert_num_warmup_steps: This is used for adjust learning rate. If global_step < num_warmup_steps, the learning rate will be `global_step/num_warmup_steps * init_lr`. This is implemented in the bert/optimization.py file. Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of bert_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by bert_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/bert/bert_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. Resources Citations Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf. 2019. TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2970-2978 Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. 2019. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962v2 G. V. Cormack, C. L. A. Clarke, and Stefan Buttcher. 2009. Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods Andrew Trotman, Antti Puurula, Blake Burgess. 2014. Improvements to BM25 and Language Models Examined Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21-25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331303 Iz Beltagy, Kyle Lo, Arman Cohan. 2019. SCIBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620 Installation Guidance tensorflow gpu requirements: https://www.tensorflow.org/install/gpu CUDA: https://developer.nvidia.com/cuda-10.1-download-archive-update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal cuDNN: https://developer.nvidia.com/rdp/cudnn-archive install .deb: https://www.quora.com/Is-it-possible-to-install-a-deb-package-in-Windows#:~:text=Potentially%20yes%2C%20as%20long%20as,deb Stopwords https://www.ranks.nl/stopwords https://countwordsfree.com/stopwords BM25 https://github.com/vespa-engine/cord-19/blob/master/cord-19-queries.md https://docs.vespa.ai/documentation/reference/bm25.html Tensorflow Ranking http://cognitiveai.org/2020/09/08/using-tensorflow-ranking-bert-tfr-bert-an-end-to-end-example/ https://github.com/cognitiveailab/ranking https://github.com/tensorflow/ranking https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb Tensorflow Ranking (how to serve a model) https://www.tensorflow.org/tfx/serving/serving_basic https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#serve_your_model_with_tensorflow_serving https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel https://github.com/cognitiveailab/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_client_predict_from_json.py Tensorflow Ranking (how to predict) https://github.com/tensorflow/ranking/issues/48 https://stackoverflow.com/questions/59528975/tf-estimator-predict-slow-with-tensorflow-ranking-module BERT Checkpoint Conversion https://github.com/tensorflow/models/tree/master/official/nlp/bert BERT https://github.com/google-research/bert https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html https://arxiv.org/pdf/1812.00073.pdf SciBERT https://www.aclweb.org/anthology/D19-1371/ https://huggingface.co/gsarti/scibert-nli https://github.com/allenai/scibert/ Docker https://superuser.com/questions/1382472/how-do-i-find-and-enable-the-virtualization-setting-on-windows-10 https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled https://docs.docker.com/docker-for-windows/install/"
https://github.com/codylw2/CourseProject	project_documentation.pdf	"IR Competition Project Documentation Cody Webster Contents Overview ....................................................................................................................................................... 3 Project Demonstration ................................................................................................................................. 3 Project Results .............................................................................................................................................. 3 DirichletPrior ............................................................................................................................................ 3 JelinekMercer ........................................................................................................................................... 4 OkapiBM25 ............................................................................................................................................... 4 BM25+ ....................................................................................................................................................... 4 BERT .......................................................................................................................................................... 5 SciBERT *difficulty running this model limited options .......................................................................... 6 Tensorflow Ranking custom model ......................................................................................................... 7 Assumptions ................................................................................................................................................. 7 Installation Instructions ............................................................................................................................... 7 What to install if running locally .............................................................................................................. 7 What python packages to install ............................................................................................................. 8 How to Run ................................................................................................................................................... 8 Prepare Data (Required if local) .............................................................................................................. 8 (Method 1) Run BM25+ or another ranker (reproduces best results) ................................................... 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) ............................................................................................................................ 9 (Method 2) Run Tensorflow Ranking ....................................................................................................... 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) ............................................................................................................................ 9 (Method 3) Run a BERT model ................................................................................................................. 9 Option 1 (Google Colab)........................................................................................................................ 9 Option 2 (run locally) .......................................................................................................................... 10 Script API Documentation .......................................................................................................................... 10 General ................................................................................................................................................... 10 Metapy (BM25, ...) .................................................................................................................................. 12 Tensorflow Ranking ................................................................................................................................ 13 BERT ........................................................................................................................................................ 16 Resources .................................................................................................................................................... 20 Citations .................................................................................................................................................. 20 Installation Guidance ............................................................................................................................. 20 Stopwords ............................................................................................................................................... 20 BM25 ....................................................................................................................................................... 21 Tensorflow Ranking ................................................................................................................................ 21 Tensorflow Ranking (how to serve a model) ........................................................................................ 21 Tensorflow Ranking (how to predict) .................................................................................................... 21 BERT Checkpoint Conversion ................................................................................................................. 21 BERT ........................................................................................................................................................ 21 SciBERT.................................................................................................................................................... 21 Docker ..................................................................................................................................................... 21 Overview The purpose of this project is to participate in the IR Competition. For this project I developed and extensively tested three main ways to rank documents. The first is ranking documents using a ranking function either packaged with metapy or through a custom definition implemented through metapy. Initially, the best results that I achieved with this method were through a custom implementation of BM25+. Through a longshot attempt on the last day, I actually beat all of my other rankings using metapy's BM25 implementation. The second is ranking documents using a custom model that is trained using the python library tensorflow_ranking. The third option is to use a pretrained model and it's associated vocabulary and then finetune it using tensorflow_ranking. Overall the best results that I have achieved have come from running the OkapiBM25 algorithm implemented with metapy and using parameters that were produced from a brute force optimization. Project Demonstration I have uploaded ""cs410_project_demo.mp4"" into the git repository. I have also uploaded the file to Illinois Media: https://mediaspace.illinois.edu/media/t/1_07py0q5f Project Results Unfortunately, despite my best attempt I was not able to beat the initial baseline performance score as defined by the class professor or TAs. Nobody else was able to beat the initial baseline either so it is likely that baseline was an unreasonable standard for us to achieve with our limited knowledge and experience. In this section I will briefly detail some of the variations and experiments that I attempted. It is not possible for me capture every variation that I tried but I will cover as best as I can. I do not have a consistent record of the score for every attempt so I will not be including those scores. On the last day of the competition, the class administrators lowered the score of the baseline. I decided to give it another shot because I really had nothing to lose. I was able to beat the new baseline using the standard BM25 algorithm implemented via metapy. The only difference between what I did on the last day and what I had done on previous days was that I allowed one of the parameters to vary more than I initially had. I will detail that more within the BM25 section. DirichletPrior This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non- ideal approach. Initial attempts with this algorithm were made using the body text only. Later attempts also used the title and abstract/intro. Further experimentation with the KLDivergencePRF implementation in metapy and variations on its parameters was also performed but performance lagged behind other approaches. JelinekMercer This algorithm was implemented using metapy. The value of the parameter was optimized on the train dataset using a brute forcing approach of testing every reasonable parameter and selecting the parameter that achieved the best NCDG@20 score on the training dataset. None of the attempts for this algorithm achieved results that rivaled the BM25 algorithm so it was quickly determined to be a non- ideal approach. This algorithm was only ever attempted with the body text and results were deemed not good enough to justify further experimentation. OkapiBM25 This algorithm was implemented using metapy. The value of the three parameters was optimized on the training dataset using a brute force approach of testing every reasonable combination of parameters and selecting the set that achieved the greatest. Due to the number of variations that I tried with this algorithm, I had to optimize the parameters a number of times and I developed a multi- processing script that was capable of doing this at a much quicker pace. Initially I only attempted to run the algorithm on the body text for each paper but the performance for that was poor. With the body text I attempted to optimize a different NDCG values, 10, 20, and 50 respectively. None of them produced significant improvements in the overall performance. Next, I attempted to use the title, abstract, and introduction along with the body text and that produce marginal improvements in the score. Another boost came once I removed the body text from the dataset and only trained on the title, abstract, and introduction. I also attempted to use metapy's implementation of Rocchio feedback. I optimized the parameters for Rocchio in a similar manner to the normal OkapiBM25 algorithm. Ultimately the Rocchio feedback produced worse results then the standalone ranker so I excluded it from further test with this algorithm. The performance was initially still inadequate so I pursued further methods. One the last day, after I noticed that they had lowered the baseline, I attempted some new variations of this algorithm. I did not run the other algorithms because they all take longer to run and I did not have time to implement anything new. The new variation that I tried on the last day was to let the k3 parameter vary. I had previously not done this due to the faulty assumption that it would only hurt my results because that was the experience that I had during MP2.2. Almost immediately while running the pooled optimization, I was able to see that the performance was superior my previous results. I selected a number of the different parameter combinations to try from the generated set. On the third try I was able to beat the baseline using k1=2.0, b=0.75, and k3=4450. I am immensely frustrated that I have wasted so much effort trying to find other ways to rank documents and the answer was so simple but I am happy to have beaten the baseline. BM25+ This algorithm was implemented with metapy's api using the definition of BM25+ from (Trotman et al, 2014). Since it was implemented in Python and C++ like the native metapy implementations, it ran slower. I optimized it in the same way that I optimized the OkapiBM25 algorithm. This implementation ultimately outperformed the native OkapiBM25 algorithm, but it did not beat the baseline. Better results were obtained when using this in conjunction with the BERT implementation. I tried numerous variations on this algorithm in an attempt to increase my score on the test dataset. Similar to the other algorithms, I initially tried to only use the body text for my analysis. This did not produce the results that I desired. I have briefly listed some of the other variations that I tried: * attempted with body text only * attempted with title, abstract, intro, and body text * attempted with title, abstract, and intro * attempted with title only * attempted with Rocchio pseudo feedback, varying the parameters to Rocchio * attempted to remove all urls in the corpus * attempted to replace all variants of the words coronavirus, covid-19, 2019-ncov, and sars-ncov-2 with coronavirus * attempted to run with query text, question text, or narrative text * attempted to run with all variations of query combined * attempted different variations of metapy analyzer chains * attempted to remove docs with duplicate s2_id but different uids, docs were functionally duplicates * attempted to use a date cutoff for document * attempted to pre-tokenize the queries and documents using BERT's tokenizer * attempted to use multiple datasets with various weightings and combine into one result * used different variations of stopwords that were gathered online (listed in resources) Ultimately, I was not able to beat the baseline using this custom algorithm, so I continued my exploration of other possibilities. BERT I had completed my Technical Review over the BERT model and had learned through that about its superior performance when classifying documents. I chose this as the algorithm to attempt to rank with. It was a significant struggle to get BERT to work for document ranking. There are very few tutorials online for how to achieve this and I spent a significant amount of time trying to get it to work. I ultimately was able to find a tutorial provided by Peter Jansen from the University of Arizona (link in resources). This tutorial gave an example of using the tensorflow_ranking module with BERT to achieve document ranking. Unfortunately, the tutorial relied on Docker to serve a fine-tuned model. I had to do a significant amount of researching about how to get Docker to work with a tensorflow model on Windows. In order to get it to work, I had to upgrade my Windows 10 OS to the Development version so that it would support the latest Windows Subsytem for Linux (WSL). I needed the latest WSL because that was what nVidia required for their latest CUDA drivers for GPUs. I needed the latest CUDA drivers because that was the only way to get Docker to run on my GPU using WSL. I also had to install Bazel for Windows because the training script was meant to be compiled to run. This lengthy setup process is obviously unsuitable for the rapid development that I needed and because it is not realistic for reviewers to reproduce this setup on their own machines. Docker is also not supported in Google Colab so I needed to arrive at a repeatable and easily setup solution for the graders. I was able to remove the need for Docker and the need to serve the model at all by exploring the documentation for tensorflow. This exploration give me insight into how to directly load the model and use it to predict document scores. I implemented this methodology into the scripts and was thus able to create a version that can be easily ran within Google Colab for easy review. Once I had an effective way of running the model, I was able to experiment with various ways to utilized it. I attempted to run three variations of the pre-trained BERT model, all provided by Google Research, BERT-Mini, BERT-Small, and BERT-Base. The BERT-Base model was too large to effectively train without utilizing TPUs. I choose to do my testing with the BERT-Mini model because it allowed me the most flexibility with my training parameters. Some key parameters used are list size and batch size. The size of the model along with those two parameters determine what hardware is required to run the model. If you try to run on inadequate hardware than you will easily run out of memory and be unable to train the model. I attempted a number of variations in ways to score or setup the data. Ultimately, I was not able to beat either baseline when using BERT but I was able to improve upon my initial results with BM25+. I have detailed some of the variations that I tried below. * varied the list size * varied the batch size * updated vocabulary to include most common coronavirus variants * attempted to run with query, question, or narrative * attempted to vary the max token sequence size 256, 512, 1024 * relevance = score of first doc segment * relevance = max score of all doc segments * relevance = mean score of all doc segments * ignore body text and only use title+abstract+intro * varied the number of training steps * used both NCDG approximated loss and softmax loss * re-rank top 1000, 2000, 5000, and 10000 results of BM25+ SciBERT *difficulty running this model limited options This model was initially very promising but the issues with getting it to run ultimately made it insufficient for what I needed. This model was create by Iz Beltagy et al. for scientific research. This model is essentially just a BERT-Base model that was pretrained on a scientific corpus instead of the generic one. The vocabulary consists of more scientific terms as a result and should theoretically be able to perform better at ranking the scientific documents in the CORD-19 corpus. I updated a few of the unused vocabulary items to the command variants of coronavirus and included the drug remdesivir as well. Unfortunately, due to the size of the model I was very limited in the list size and batch size options that I could try. I firmly believe that with a larger list size I would have been able to adequately finetune this model and it would have performed better than the BERT model. My inability to get the model to run on the TPUs available through Google Colab prevented me from realizing this goal. Tensorflow Ranking custom model This model was created by basing it off of the example in the tensorflow ranking repository. I left the structure of the context features and the example features the same. The context features contain the query tokens and the example features contain the document tokens and when training, the relevance judgement of the file. I attempted to change the script to use the argparse module instead of the flags from the absl module but when that was attempted the script stopped producing the results of the training data to stdout. I determine that there are likely scripts deeper within the tensorflow ranking module that are using these flags to determine various facets of the training process, so I restore the flags. The model consists of 3 hidden layers at a size of 64, 32, and 16 respectively. During testing the dropout rate of the model was adjust to 0.65 from the default of 0.80. This adjustment produced better results but further drops in this dropout rate risked overfitting the model to the training data. The batch size was set to 1 and the list size maximized so that it would train on more docs for a single query at once. Based on empirical results that I observed during experimentation, this produced better results than increasing batch size and lowering the list size. The maximum limit for the list_size was 100 depending on the hardware available in Colab and if it exceeded that value then it would run out of memory when training. One other significant variable was the max sequence length to use for each example. I tested with both 512 and 1024. The results did not appear to differ significantly between the two but this variable is important because it defines how much of the document can be captured per example. For this model I only used the title, abstract, and introduction because previous experimentation on other models and rankers showed that the body text was not helpful. Ultimately the results produced with this method were worse the results obtained from the BERT models. Assumptions * If you are running this locally then you are running this code on Windows 10 machine that has ample RAM and a CUDA capable gpu. Every script can run on Linux but I have not generated scripts within this repo to accommodate that. * All file paths are relative to the base directory of the repository Installation Instructions What to install if running locally Acquire the datasets: Files should be downloaded and unzipped here: .\competition\datasets The test files can be obtained from https://drive.google.com/file/d/1FCW8fmcneow5yyDgApkPIGM-r2x6OFkm/view?usp=sharing The train files can be obtained from https://drive.google.com/file/d/1E_Y- MkNvoOYoCZUZZa8JJ3ExiHYTfKTo/view?usp=sharing Python Instructions: https://docs.conda.io/projects/conda/en/latest/user-guide/install/ Latest Nvidia Drivers (required to run BERT or Tensorflow Ranking on GPU) Instructions: https://docs.nvidia.com/cuda/wsl-user-guide/index.html Instructions require installation of other software and packages you must follow all of them BERT Model (required to run BERT) Due to the size of the models, they are not directly included in the repository Google BERT Models (recommended): Google offers a variety of different sizes for the models to run. I ran using a BERT Mini but could possibly use a BERT Small as well. Anything larger likely requires a TPU through Google Colab to run. https://github.com/google-research/bert SciBERT Model (not recommended, requires TPU): https://github.com/allenai/scibert/ What python packages to install To run the full capability of all scripts defined in this project it is required that you setup two environments. All but one script can run in the main environment but the BERT checkpoint convertor relies on the dev version of a module that conflicts with requirements of the other modules: Main Python Environment: This is the main environment and should be used when running most of the scripts. You can install the latest stable versions of these modules to run the project. Python Version: 3.7 Pip Packages: tensorflow_ranking metapy pytoml BERT Conversion Environment: This is required because there are conflicts in the dependencies of the modules required to convert a model's checkpoints and the modules used to run it. Python Version: 3.7 Pip Packages: tf-models-nightly How to Run **WARNING: running some of these scripts will require a large amount of RAM if you use the full dataset** * Your python environment should be activated before running any scripts Prepare Data (Required if local) Generate the json file that contains the information for each dataset. This is required in order to run any of the different methodologies. File Path: .\competition\create_bert_data.bat (Method 1) Run BM25+ or another ranker (reproduces best results) Option 1 (Google Colab) Run each cell individually from top to bottom to fully. If you view the file within Github there is a link at the top of the file to open in Google Colab. This is the only Google Colab file that does not require a Google Colab Pro account. The free account should be capable of running this notebook. File Path: .\colab_cranfield_metapy.ipynb Option 2 (run locally) Generate the Cranfield Datasets that will be required to run the ranker File Path: .\competition\cranfield_metapy\cm_create_data.bat Run the ranker of your choice (default arguments already in the file) File Path: .\competition\cranfield_metapy\cm_rank_docs.bat (Method 2) Run Tensorflow Ranking Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. T This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_tfr.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\tfr_custom\tfr_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\tfr_custom\tfr_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\tfr_custom\tfr_predict.bat (Method 3) Run a BERT model Option 1 (Google Colab) Run each cell individually from top to bottom to fully. Training data for a fine-tuned model is provided so there is not need to run training but the capability is provided. If you view the file within Github there is a link at the top of the file to open in Google Colab. This file will likely require a Google Colab Pro account for the script to work. If not using a Colab Pro account it will run out of memory. File Path: .\colab_bert.ipynb Option 2 (run locally) Convert the train data into two tensorflow example list with context (elwc) files File Path: .\competition\bert\bert_create_train_elwc.bat Train the model on the elwc files File Path: .\competition\bert\bert_train_model.bat Re-rank the output of a previous run (requires a file in the format of a predictions file, no limit on docs per query but will truncate output to 1000 File Path: .\competition\bert\bert_predict.bat Script API Documentation **Unless otherwise specified paths are relative to the root of the repository General File: ./competition/check_covid_variants.py Purpose: Check for coronavirus variants in the corpus Source: developed by project team API: variant_file: the file that the coronavirus variants will be output to. File can already exist. Existing variants will be loaded. known_variants: known variants of the coronavirus that exist in the corpus and will be used to determine other variants doc_keys: the keys to use when search the document dictionary for variants run_type: the dataset that will be searched for variants input_dir: the directory that contains the json representation of the datasets to be processed Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. It loads the json for one or both datasets and then processes the text to find variants of some predefined words in the corpus and queries. The text is processed using python multiprocessing module in order to speed up execution of the script. File: ./competition/checkpoint_converter.py Purpose: Converter BERT checkpoint files from Tensorflow v1 to v2+ Source: Tensorflow Model Garden Repository API: bert_config_file: Bert configuration file to define core bert layers. checkpoint_to_convert: Initial checkpoint from a pretrained BERT model core (that is, only the BertModel, with no task heads.) converted_checkpoint_path: Name for the created object-based V2 checkpoint. checkpoint_model_name: The name of the model when saving the checkpoint, i.e., the checkpoint will be saved using: tf.train.Checkpoint(FLAGS.checkpoint_model_name=model). converted_model: Whether to convert the checkpoint to a `BertEncoder` model or a `BertPretrainerV2` model (with mlm but without classification heads). Detailed Description: This script was not written by me but was lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script is meant to convert a BERT module that you downloaded into a version that is capable of being ran using tensorflow version 2.0+. File: ./competition/create_bert_data.py Purpose: Combine and compile the information from the dataset into and easily loadable json for use by other scripts Source: developed by project team API: vocab_file: The file containing the vocabulary to be used when tokenizing a text variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants tokenize: If use, this will store a tokenized representation of the script in the output json. Must be used with `vocab_file` input_dir: The directory that contains all of the source files for the dataset output_dir: The directory where the json representations of the documents will be stored Detailed Description: This is a python script that was tested in python 3.7. The inputs to the script will determine the exact behavior at run time but lets discuss the more complete case of the training dataset. When the training dataset is specified it will start by loading the queries from there original xml format into a python dictionary. If the script is ran with the tokenize option then each of the three variants of the query (query, question, and narrative) will be tokenized according to the vocabulary defined in the specified vocab file. This json is then dumped into a target directory. For the training dataset it will also load the query relevance judgements. A list of all of the documents and their associated uid, title, abstract, publication date, and list of files containing the text representation is pulled from the metadata.csv file. The list of documents to further process is pruned done to the same as the list of relevance judgements. For each document to process the script iterates through the list of files that contains the text representation until it finds a suitable candidate or exhausts all options. The text from the representation file is loaded in the document dictionary object. If the tokenize option is used, the documents will then be tokenized through the use of a multi-processed pool. The resulting output is then written to the disk. Metapy (BM25, ...) File: ./competition/cranfield_metapy/create_cranfield.py Purpose: Use the json representation of the dataset to create a cranfield dataset for use with metapy Source: developed by project team API: variant_file: A file containing variants of the word coronavirus or its equivalents variant_default: The value that will be used to replace all variants in the corpus run_type: The dataset that will be searched for variants query_keys: The keys that will be used when creating the cranfield query file. If multiple keys are specified, text will be combined. doc_keys: The keys that will be used when creating the cranfield document files. Multiple keys can be specified and multiple keys can be combined into a single document. If creating separate datasets, use a ';' to separate keys. If combining keys for a dataset use a ':' to separate keys: cranfield_dir: The directory to use as a base for generating the cranfield data. input_dir: The directory where the json representations of the documents are stored Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_bert_data.py in order to run. This uses the loaded json representation of the dataset to produce a use specified cranfield dataset for use with metapy. Each cranfield dataset can be uniquely defined by the user to consist of a one-to-one relationship with keys in the document dictionary or it can consist of multiple keys combined. This allows for flexibility in how the expansive each iteration of ranking testing is. File: ./competition/cranfield_metapy/search_eval.py Purpose: Rank the documents in the corpus and create the predictions file Source: original version from MP 2.2 and heavily modified to fit use case API: config_template: The template file that will be used for creating the configs for each run of the ranker run_type: The dataset that will be searched for variants dat_keys: The keys to use that indicate the name of the cranfield dataset(s). This corresponds to the first key used for every section of 'doc_keys' parameter and the 'create_cranfield.py' script. doc_weights: The weights to use when combining the rankings of multiple datasets. ranker: The ranker to use for ranking the documents. Valid rankers can be found in the script. params: The value(s) for the ranker parameters. Multiple values should be separated by `;`. cranfield_dir: The directory that is the base for the cranfield dataset(s) predict_dir: The directory to contain the predictions file. remove_idx: Delete an existing inverted index and create a new one. If no index exists it will not fail Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to specify what ranker to run from a predefined list of rankers. The user can also run rankings over multiple datasets and specify the weight that each ranking will contribute to the final prediction rankings. File: ./competition/cranfield_metapy/search_eval_pool.py Purpose: Used to find the optimal parameters for ranking algorithms. Not for production use. Source: original version from MP 2.2 and heavily modified to fit use case API: N/A Detailed Description: This is a python script that was implemented in python 3.7. This script relies on the output of create_cranfield.py in order to run. This script allows the user to attempt to optimize the parameters for a predefined list of rankers. The optimization takes place using a multi-processed pool so that it can run numerous iterations over a predefined range of values. Each iteration of the optimization loop is evaulated based on normalized cumulative gain at 20 documents. Tensorflow Ranking File: ./competition/tfr_custom/tfr_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/tfr_custom/tfr_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/tf_ranking_tfrecord .py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: data_format: Data format defined in data.py. train_path: Input file path used for training. eval_path: Input file path used for eval. vocab_path: Vocabulary path for query and document tokens. model_dir: Output directory for models. batch_size: The batch size for train. num_train_steps: Number of steps for train. learning_rate: Learning rate for optimizer. dropout_rate: The dropout rate before output layer. hidden_layer_dims: Sizes for hidden layers. list_size: List size used for training. Use None for dynamic list size. group_size: Group size used in score function. loss: The RankingLossKey for the loss function. weights_feature_name: The name of the feature where unbiased learning-to-rank weights are stored. listwise_inference: If true, exports accept `data_format` while serving. use_document_interactions: If true, uses cross-document interactions to generate scores. embedding_dim: max size of any query or document Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of tfr_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by tfr_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/tfr_custom/tfr_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. BERT File: ./competition/bert/bert_predict.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To convert json data to an elwc formatted tfrecord file for use with model training API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_train_file: The tfrecord file to be used for training output_eval_file: The tfrecord file to be used for evaluation list_size: The maximum number of documents to score per query do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries, documents, and relevance judgments for the train dataset and converts them to an example list with context files for use in training the model. The maximum number of documents associated with each query is defined by the list size parameter. If the number of documents for a query exceeds this parameter value then the document list is chunked into multiple elwc representations before being output to the files. The elwc objects are formatted for the model's specific requirements. Each query and document is limited to a maxim number of tokens as defined by sequence_length and if a document or query is longer than this value then it is truncated. File: ./competition/bert/bert_train.py Source: lightly version of file located here https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/extension/examples/tfrbert_e xample.py Purpose: To train a tensorflow model on a dataset with predefined relevance judgements. API: local_training: If true, run training locally. train_input_pattern: Input file path pattern used for training. eval_input_pattern: Input file path pattern used for eval. learning_rate: Learning rate for the optimizer. train_batch_size: Number of input records used per batch for training. eval_batch_size: Number of input records used per batch for eval. checkpoint_secs: Saves a model checkpoint every checkpoint_secs seconds. num_checkpoints: Saves at most num_checkpoints checkpoints in workspace. num_train_steps: Number of training iterations. Default means continuous training. num_eval_steps: Number of evaluation iterations. loss: The RankingLossKey deciding the loss function used in training. list_size: List size used for training. convert_labels_to_binary: If true, relevance labels are set to either 0 or 1. model_dir: Output directory for models. dropout_rate: The dropout rate. bert_config_file: The config json file corresponding to the pre-trained BERT model. This specifies the model architecture. Please download the model from the link: https://github.com/google-research/bert bert_init_ckpt: Initial checkpoint from a pre-trained BERT model. Please download from the link: https://github.com/google-research/bert bert_max_seq_length: The maximum input sequence length (#words) after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded. bert_num_warmup_steps: This is used for adjust learning rate. If global_step < num_warmup_steps, the learning rate will be `global_step/num_warmup_steps * init_lr`. This is implemented in the bert/optimization.py file. Detailed Description: This script was not originally written by me but has been lightly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of bert_convert_json_to_elwc.py in order to run. This script loads in the elwc files generated by bert_convert_json_to_elwc.py and trains the model on the data provided in the elwc files. The training will run for the ""num_train_steps"" defined by the user. The outputs of this script are stored in ""model_dir"" and you can load the output model for use in prediction of a documents relevance. File: ./competition/bert/bert_convert_json_to_elwc.py Source: modified version of file located here https://github.com/cognitiveailab/ranking Purpose: To score the documents in the test dataset against the queries in the test dataset and output a predictions file API: vocab_file: The file containing the vocabulary to be used when tokenizing a text sequence_length: The max length of any individual query or document query_file: The json file that contains all of the queries qrel_files: The file containing the training relevance judgements query_key: The type of query that will be used as context for ranking, i.e. (query, question, narrative) doc_file: The json file that contains all of the documents output_file: The file that will contain the scores model_path: The path to the saved model for use in predictions docs_at_once: The maximum number of documents to score at once rerank_file: The input file consisting of previous rankings that will be reranked with the tensorflow model do_lower_case: ensure all query and document strings are lowercase Detailed Description: This script was not originally written by me but has been highly modified for the purpose of this project. This is a python script that was tested in python 3.7. This script relies on the output of create_bert_data.py in order to run. This script loads in the queries and documents for the specified dataset. It also loads in the relevance judgements of a previous ranking and a trained model. The list of documents to rank is limited to only the files specified in the previous ranking. Each document query combination is converted into an example list with context(elwc) object and then it is passed to the loaded model. All of the documents ranked for each query is combined into a singular list that is sorted by the score and only the top 1000 documents are output to a predictions file. The predictions file output is the same as the file of the relevance judgements. Resources Citations Rama Kumar Pasumarthi and Sebastian Bruch and Xuanhui Wang and Cheng Li and Michael Bendersky and Marc Najork and Jan Pfeifer and Nadav Golbandi and Rohan Anil and Stephan Wolf. 2019. TF- Ranking: Scalable TensorFlow Library for Learning-to-Rank. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2970-2978 Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. 2019. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models. arXiv preprint arXiv:1908.08962v2 G. V. Cormack, C. L. A. Clarke, and Stefan Buttcher. 2009. Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods Andrew Trotman, Antti Puurula, Blake Burgess. 2014. Improvements to BM25 and Language Models Examined Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '19), July 21-25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331303 Iz Beltagy, Kyle Lo, Arman Cohan. 2019. SCIBERT: A Pretrained Language Model for Scientific Text. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615-3620 Installation Guidance tensorflow gpu requirements: https://www.tensorflow.org/install/gpu CUDA: https://developer.nvidia.com/cuda-10.1-download-archive- update2?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version= 1804&target_type=deblocal cuDNN: https://developer.nvidia.com/rdp/cudnn-archive install .deb: https://www.quora.com/Is-it-possible-to-install-a-deb-package-in- Windows#:~:text=Potentially%20yes%2C%20as%20long%20as,deb Stopwords https://www.ranks.nl/stopwords https://countwordsfree.com/stopwords BM25 https://github.com/vespa-engine/cord-19/blob/master/cord-19-queries.md https://docs.vespa.ai/documentation/reference/bm25.html Tensorflow Ranking http://cognitiveai.org/2020/09/08/using-tensorflow-ranking-bert-tfr-bert-an-end-to-end- example/ https://github.com/cognitiveailab/ranking https://github.com/tensorflow/ranking https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking /examples/handling_sparse_features.ipynb Tensorflow Ranking (how to serve a model) https://www.tensorflow.org/tfx/serving/serving_basic https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#serve_your_model_with_tensorf low_serving https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to- inspect-and-execute-savedmodel https://github.com/cognitiveailab/ranking/blob/master/tensorflow_ranking/extension/example s/tfrbert_client_predict_from_json.py Tensorflow Ranking (how to predict) https://github.com/tensorflow/ranking/issues/48 https://stackoverflow.com/questions/59528975/tf-estimator-predict-slow-with-tensorflow- ranking-module BERT Checkpoint Conversion https://github.com/tensorflow/models/tree/master/official/nlp/bert BERT https://github.com/google-research/bert https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html https://arxiv.org/pdf/1812.00073.pdf SciBERT https://www.aclweb.org/anthology/D19-1371/ https://huggingface.co/gsarti/scibert-nli https://github.com/allenai/scibert/ Docker https://superuser.com/questions/1382472/how-do-i-find-and-enable-the-virtualization-setting- on-windows-10 https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization-must-be-enabled https://docs.docker.com/docker-for-windows/install/"
https://github.com/codylw2/CourseProject	project_proposal.docx	Cody Webster codylw2@illinois.edu Project Proposal Team Members: codylw2 Project: I intend to participate in the Information Retrieval (IR) Competition. I am prepared to learn state of the art retrieval methods such as Reciprocal Rank Fusion or Inverse Square Rank Fusion and how to utilize machine learning through a method of machine-learned ranking. If I choose to utilize a rank fusion method, I will combine different statistical and probabilistic models that are optimized on the given data set. I have little relevant prior experience in IR outside of this course. I intend to program my project using python.
https://github.com/codylw2/CourseProject	project_proposal.pdf	Cody Webster codylw2@illinois.edu Project Proposal Team Members: codylw2 Project: I intend to participate in the Information Retrieval (IR) Competition. I am prepared to learn state of the art retrieval methods such as Reciprocal Rank Fusion or Inverse Square Rank Fusion and how to utilize machine learning through a method of machine-learned ranking. If I choose to utilize a rank fusion method, I will combine different statistical and probabilistic models that are optimized on the given data set. I have little relevant prior experience in IR outside of this course. I intend to program my project using python.
https://github.com/codylw2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/yiz9/CourseProject	410 Project Paper(2004).pdf	"A Cross-Collection Mixture Model for Comparative Text Mining ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Atulya Velivelli Department of Electrical and Computer Engineering University of Illinois at Urbana Champaign Bei Yu Graduate School of Library and Information Science University of Illinois at Urbana Champaign ABSTRACT In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent com- mon themes across all collections as well as summarize the similarity and differences of these collections along each com- mon theme. This general problem subsumes many interest- ing applications, including business intelligence and opinion summarization. We propose a generative probabilistic mix- ture model for comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algo- rithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. Categories and Subject Descriptors: H.3.3 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Comparative text mining, mixture models, clus- tering 1. INTRODUCTION Text mining is concerned with extracting knowledge and patterns from text [5, 6]. While there has been much re- search in text mining, most existing research is focused on one single collection of text. The goals are often to extract basic semantic units such as named entities, to extract rela- tions between information units, or to extract topic themes. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'04, August 22-25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00. In this paper, we study a novel problem of text mining re- ferred to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differ- ences of these collections along each common theme. Specif- ically, the task involves: (1) discovering the different com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. The need for comparative text mining exists in many different applica- tions, including business intelligence, summarizing reviews of similar products, and comparing different opinions about a common topic in general. In this paper, we study the CTM problem and propose a generative probabilistic mixture model for CTM. The model simultaneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The mixture model is based on component multinomial distribution models, each characterizing a different theme. The common themes and collection-specific themes are explicitly modeled. The pro- posed model can be estimated efficiently using the Expectation- Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. The rest of the paper is organized as follows. In Section 2, we briefly introduce the problem of CTM. We then present a baseline simple mixture model and a new cross-collection mixture model in Section 3 and Section 4. We discuss the experiment results in Section 5. 2. COMPARATIVE TEXT MINING 2.1 A motivating example With the popularity of e-commerce, online customer eval- uations are becoming widely provided by online stores and third-party websites. Pioneers like amazon.com and epin- ions.com have accumulated large amounts of customer input including reviews, comments, recommendations and advice, etc. For example, the number of reviews in epinions.com 743 Research Track Poster is more than one million[4]. Given a product, there could be up to hundreds of reviews, which is impossible for the readers to go through. It is thus desirable to summarize a collection of reviews for a certain type of products in order to provide the readers the most salient feedbacks from the peers. For review summarization, the most important task is to identify different semantic aspects of a product that the reviewers mentioned and to group the opinions accord- ing to these aspects to show similarities and differences in the opinions. For example, suppose we have reviews of three different brands of laptops (Dell, IBM, and Apple), and we want to summarize the reviews. A useful summary would be a tab- ular representation of the opinions as shown in Table 1, in which each row represents one aspect (subtopic) and differ- ent columns correspond to different opinions. Table 1: A tabular summary Subtopics Dell IBM Apple Battery life long enough short short Memory good bad good Speed slow fast fast It is, of course, very difficult, if not impossible to pro- duce such a table completely automatically. However, we can achieve a less ambitious goal - identifying the semantic aspects and identifying the common and specific character- istics of each product in an unsupervised way. This is a concrete example of comparative text mining. 2.2 The general problem The example above is only one of the many possible appli- cations of comparative text mining. In general, the task of comparative text mining involves: (1) discovering the com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. It is very hard to precisely define what a theme is, but it corresponds roughly to a topic or subtopic. The granularity of themes is application-specific. CTM is a fundamental task in ex- ploratory text analysis. In addition to opinion comparison and summarization, it has many other applications, such as business intelligence (comparing different companies), cus- tomer relationship management (comparing different groups of customers), and semantic integration of text (comparing component text collections). CTM is challenging in several ways: (1) It is a completely unsupervised learning task; no training data is available. (It is for the same reason that CTM can be very useful for many different purposes - it makes minimum assumptions about the collections and in principle we can compare any arbitrary partition of text.) (2) We need to identify themes across different collections, which is more challenging than identifying topic themes in one single collection. (3) The task involves a discrimination component - for each discov- ered theme, we also want to identify the unique information specific to each collection. Such a discrimination task is dif- ficult given that we do not have training data. In a way, CTM goes beyond the regular one-collection text mining by requiring an ""alignment"" of multiple collections based on common themes. Since no training data is available, in general, we must rely on unsupervised learning methods, such as clustering, to perform CTM. In this paper, we study how to use prob- abilistic mixture models to perform CTM. Below we first describe a simple mixture model for clustering, which repre- sents a straightforward application of an existing text min- ing method, and then present a more sophisticated mixture model specifically designed for CTM. 3. CLUSTERING WITH A SIMPLE MIXTURE MODEL   th  th  th "" $ % th ' th Figure 1: The Simple Mixture Model A naive solution to CTM is to treat the multiple collec- tions as one single collection and perform clustering. Our hope is that some clusters would represent the common themes across the collections, while some others would rep- resent themes specific to one collection (see Figure 1). We now present a simple multinomial mixture model for clus- tering an arbitrary collection of documents, in which we assume there are k latent common themes in all collections, and each is characterized by a multinomial word distribu- tion (also called a unigram language model). A document is regarded as a sample of a mixture model with these theme models as components. We fit such a mixture model to the union of all the text collections we have, and the obtained component multinomial models can be used to analyze the common themes and differences among the collections. Formally, let C = {C1, C2, ..., Cm} be m comparable col- lections of documents. Let th1, ..., thk be k theme unigram language models and thB be the background model for all the collections. A document d is regarded as a sample of the following mixture model (based on word generation). pd(w) = lBp(w|thB) + (1 - lB) k j=1 [pd,jp(w|thj)] where w is a word, pd,j is a document-specific mixing weight for the j-th aspect theme, and k j=1 pd,j = 1. lB is the mix- ing weight of the background model thB. The log-likelihood of all the collections C is log p(C|L) = m i=1 dCi wV [c(w, d) x log(lBp(w|thB) + (1 - lB) k j=1 (pd,jp(w|thj)))] where V is the set of all the words (i.e., vocabulary), c(w, d) is the count of word w in document d, and L = ({thj, pd,j}k j=1 744 Research Track Poster is the set of all the theme model parameters. The purpose of using a background model is to ""force"" clustering to be done based on more discriminative words, leading to more informative and more discriminative component models. We control this effect through thB. The model can be estimated using any estimator. For example, the Expectation-Maximization (EM) algorithm [3] can be used to compute a maximum likelihood estimate with the following updating formulas: p(zd,w = j) = p(n) d,j p(n)(w|thj) k j'=1 p(n) d,j'p(n)(w|thj') p(zd,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j p(n)(w|thj) p(n+1) d,j = wV c(w, d)p(zd,w = j) j' wV c(w, d)p(zd,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,w = B))p(zd,w = j) w'V m i=1 dCi c(w', d)(1 - p(zd,w' = B))p(zd,w' = j) This mixture model is closely related to the probabilis- tic latent semantic indexing model (PLSI) proposed in [7] and treats CTM as a single-collection text mining problem. However, such a simple model is inadequate for CTM for two reasons: (1) We have completely ignored the structure of collections. As a result, we may have clusters that repre- sent only some, not all of the collections. (2) There is no easy way to identify which theme cluster represents the common information across collections and which represents specific information to a particular collection. Below we present a more sophisticated coordinated mixture model, which is specifically designed for CTM and addresses these two defi- ciencies. 4. CLUSTERING WITH A CROSS- COLLECTION MIXTURE MODEL  th th    th     th    th    th    th    th    th Figure 2: The Cross-Collection Mixture Model 4.1 The model Our main idea for improving the simple mixture model for comparative text mining is to explicitly distinguish com- mon theme clusters that characterize common information across all collections from special theme clusters that char- acterize collection-specific information. Thus we now con- sider k latent common themes as well as a potentially dif- ferent set of k collection-specific themes for each collection (illustrated in Figure 2). These component models directly correspond to all the information we are interested in discov- ering. The sampling distribution of a word in document d (from collection Ci) is now collection-specific. Specifically, it involves the background model (thB), k common theme models (th1, ..., thk), and k collection-specific theme models (th1,i, ..., thk,i), which are to capture the unique information about the k themes in collection Ci. That is, pd(w|Ci) = (1 - lB) k j=1 [pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))] +lBp(w|thB) where lB is the weight on the background model thB and lC is the weight on the common theme model thj (as opposed to the collection-specific theme model thj,i). Intuitively, when we ""generate"" a word, we first decide whether to use the background model thB according to lB; the larger lB is, the more likely we will use thB. If we decide not to use thB, then we need to decide which theme to use; this is controlled by pd,j, the probability of using theme j when generating words in d. Finally, once we decide which theme to use, we still need to decide whether we should use the common theme model or the collection-specific theme model, and this is con- trolled by lC, the probability of using the common model. The weighting parameters lB and lC are intentionally to be set by the user, and their interpretation is as follows. lB reflects our knowledge about how noisy the collections are. If we believe the text is verbose, then lB should be set to a larger value. In our experiments, a value of 0.9 - 0.95 often works well. lC indicates our emphasis on the commonality, as opposed to the speciality in comparative text mining. A larger lC would allow us to learn a richer common theme model, whereas a smaller one would learn a weaker com- mon theme model, but stronger special models. The optimal value depends on the specific applications. According to this generative model, the log-likelihood of the whole set of collections is log p(C) = m i=1 dCi wV [c(w, d) log[lBp(w|thB) +(1 - lB) k j=1 pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))]] 4.2 Parameter estimation We estimate the background model thB using all the avail- able text in the m text collections. That is, ^p(w|thB) = m i=1 dCi c(w, d) m i=1 dCi w'V c(w', d) Since lB and lC are set manually, this leaves us with the following parameters to estimate: (1) the common theme models, th = {th1, ..., thk}; (2) the special theme models for each collection Ci, thCi = {th1,i, ..., thk,i}; and (3) the theme mixing weights for each document d: pd = {pd,1, ..., pd,k}. 745 Research Track Poster p(zd,Ci,w = j) = p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) k j'=1 p(n) d,j'(lCp(n)(w|thj') + (1 - lC)p(n)(w|thj',i)) p(zd,Ci,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) p(zd,Ci,j,w = C) = lCp(n)(w|thj) lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i) p(n+1) d,j = wV c(w, d)p(zd,Ci,w = j) j' wV c(w, d)p(zd,Ci,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)p(zd,Ci,j,w = C) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)p(zd,Ci,j,w' = C) p(n+1)(w|thj,i) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)(1 - p(zd,Ci,j,w = C)) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)(1 - p(zd,Ci,j,w' = C)) Figure 3: EM updating formulas for the cross-collection mixture model As in the simple mixture model, we can also use the EM algorithm to compute a maximum likelihood estimate. The updating formulas are shown in Figure 3. Each EM iteration involves scanning all the text once, so the algorithm is quite scalable. 4.3 Using the model Once the model is estimated, we will have k collection- specific models for each of the m collections and k common theme models across all collections. Each of these mod- els is a word distribution or unigram language model. The high probability words can characterize the theme/cluster extracted. Such words can often be used directly as a sum- mary or indirectly (e.g., through a hidden Markov model) to extract relevant sentences to form a summary of the cor- responding theme. The extracted word distributions can also be used in many other ways, e.g., to classify other text documents or to link the related passages in the text collec- tions so that a user can navigate the information space for comparative analysis. We can input our bias for CTM through setting lB and lC manually. Specifically, lB allows us to input our knowledge about the noise (stop words) in the data - if we know the text data is verbose, then we should set lB to a high value, whereas if the data is concise and mostly content-bearing keywords, then we need to set lB to a smaller value. Sim- ilarly, lC allows us to input a trade-off between extracting common theme models (setting lC to a higher value) vs. ex- tracting collection-specific models (setting lC to a smaller value). Such biases cannot be learned by the maximum like- lihood estimator. Indeed, maximizing the data likelihood is only a means to achieve our ultimate goal, which is why we want to regularize our model in a meaningful way so that we can impose certain preferences while maximizing the data likelihood. The flexibility and control provided by lB and lC make it possible for a user to control the focus of the results of comparative text mining. 5. EXPERIMENTS AND RESULT ANALYSIS We evaluated the Simple Mixture model (SimpMix) and the Cross-Collection Mixture model (CCMix) on two do- mains - war news and laptop reviews. 5.1 War news The War news data consists of news excerpts on two com- parable events: (1) Iraq war and (2) Afghanistan war, both of which occurred in the last two years. The Iraq war news excerpts were a combination of 30 articles from the CNN and BBC web sites over the last one year span. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov. 2001. Our goal is to compare these two wars and find out their common and specific characteristics. The results of using either the simple mixture model or the cross-collection mixture model are shown in Table 2, where the top words of each theme model are listed along with their probabilities. We set lB = 0.95 for SimpMix and set lb = 0.9, lC = 0.25 for CCMix; in both cases, the number of clusters is fixed to 5. Variations of these parameters are discussed later. We see that although there are some interesting themes in the results of SimpMix (e.g., cluster3 and cluster4 appear to be about American and British inquiry into the pres- ence of weapons in Iraq, respectively, while cluster2 suggests the presence of British soldier in Basra, a town in southern Iraq), they are all about Iraq war. We do not see any obvi- ous theme common to both Iraq war and Afghanistan war. This is expected given that SimpMix pools all documents together without exploiting the collection structure. In contrast, the results of CCMix explicitly suggest the common themes and the corresponding collection-specific themes. For example, cluster3 clearly suggests that in both wars, there has been loss of lives. Furthermore, the top words in the corresponding Iraq theme include names of some key defense people that are involved in the Iraq war (e.g., ""Hoon"" is the last name of the british defense secre- tary and ""Sanchez"" is the last name of the U.S General in Iraq). In comparison, the top words in the corresponding Afghanistan theme includes the name of the U.S Defense secretary who had an important role in the Afghan war. Cluster4 and cluster5 are also meaningful themes. The common theme captured in Cluster4 is the Monday briefings by an official spokesman of a political administration during both wars; the corresponding special themes indicate the dif- ference in the topics discussed in the briefings (e.g., weapon inquiry for Iraq war and Bin Laden for Afghanistan war). The common theme of Cluster5 is about the diplomatic role 746 Research Track Poster Table 2: War news results using SimpMix model (top) vs. CCMix model (bottom) Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common will 0.019 british 0.017 weapons 0.022 inquiry 0.052 countries 0.026 theme let 0.012 soldiers 0.015 kay 0.021 intelligence 0.036 contracts 0.023 words united 0.012 baghdad 0.015 rumsfeld 0.017 dossier 0.024 allawi 0.012 god 0.011 air 0.011 commission 0.014 hutton 0.021 hoon 0.012 inspectors 0.011 basra 0.011 group 0.014 claim 0.019 russian 0.010 your 0.010 mosque 0.010 senate 0.011 wmd 0.019 international 0.010 nation 0.010 southern 0.01 survey 0.010 mps 0.018 russia 0.009 n 0.010 fired 0.010 paper 0.010 committee 0.017 reconstruction 0.009 Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common us 0.042 mr 0.029 killed 0.036 monday 0.036 united 0.042 theme nation 0.030 marines 0.025 month 0.032 official 0.032 nations 0.04 words will 0.024 dead 0.023 deaths 0.023 i 0.029 with 0.03 action 0.022 general 0.022 one 0.023 would 0.028 is 0.025 re 0.022 defense 0.019 died 0.022 where 0.025 it 0.024 border 0.019 key 0.018 been 0.022 do 0.025 they 0.023 its 0.017 since 0.018 drive 0.018 spokesman 0.022 diplomatic 0.023 ve 0.016 first 0.016 according 0.015 political 0.021 blair 0.022 Iraq god 0.022 iraq 0.022 troops 0.016 intelligence 0.049 n 0.03 theme saddam 0.016 us 0.021 hoon 0.015 weapons 0.034 weapons 0.024 words baghdad 0.013 baghdad 0.017 sanchez 0.012 inquiry 0.028 inspectors 0.023 your 0.012 nato 0.015 billion 0.01 commission 0.017 council 0.016 live 0.01 iraqi 0.013 spokeswoman 0.008 independent 0.016 declaration 0.015 Afghan paper 0.021 story 0.028 taleban 0.026 bin 0.031 northern 0.040 theme afghan 0.019 full 0.026 rumsfeld 0.020 laden 0.031 alliance 0.040 words meeting 0.014 saturday 0.016 hotel 0.012 steinberg 0.027 kabul 0.030 euro 0.012 e 0.015 front 0.011 taliban 0.023 taleban 0.025 highway 0.012 rabbani 0.012 dropped 0.010 chat 0.019 aid 0.020 played by the United Nations (UN). The corresponding spe- cial themes again suggest the difference between the two wars. The Iraq theme indicates the role of UN in sending weapon inspectors to Iraq; the Afghanistan theme refers to Northern Alliance that received aid from the UN and came to power in Afghanistan after the defeat of Taliban. 5.2 Laptop customer reviews This data set was constructed to test our models for com- paring opinions of customers on different laptops. We man- ually downloaded the following 3 review sets from epin- ions.com [4], filtering out the misplaced ones: Apple iBook (M8598LL/A) Mac Notebook (34 reviews), Dell Inspiron 8200 (8TWORH) PC Notebook (22 reviews), IBM ThinkPad T20 2647 (264744U) PC Notebook (42 reviews). The results on this data set are generally similar to those on war news. Due to the limit of space, we only show the CCMix results in Table 3, which are obtained by setting lC=.7 and lB=.96 and fixing the number of clusters to 8. Here we again see many very interesting common themes; in- deed, the top two words in the common themes can provide a very good summary of the themes (e.g., ""sound and speak- ers"" for cluster1, ""battery hours"" for cluster5, and ""Mi- crosoft Office"" for cluster8). However, the special themes, although suggesting some differences among the three lap- tops, are much harder to interpret. This may be because there is a great deal of variation in product-specific opin- ions in the data, which makes the data extremely sparse for learning a coherent collection-specific theme for each of the eight themes. 5.3 Parameter tuning When we vary lB and lC in CCMix, the results are gen- erally different. Specifically, when lB is set to a small value, non-informative stop words tend to show up in common themes. A reasonable value for lB is generally higher than 0.9 - in that case, the model automatically eliminates the non-informative words from the theme clusters, allowing for more discriminative clustering. Indeed, in all our experi- ments, we have intentionally retained all the stop words, and the model is clearly able to filter out non-informative words, though in some cases, they still show up as top words in the common themes of the news data. They can be ""eliminated"" by using an even higher lB, but then we may end up having insufficient information to learn a common theme reliably. lC affects the vocabulary allocation between the common and collection-specific themes. In the news data experiments, when we change lC to a value above 0.4, the collection-specific terms would dominate the common theme models. In the laptop data experiments, when lC is less than 0.7, we lose many content keywords of the com- mon themes to the corresponding collection-specific themes. Both lB and lC are intentionally left for a user to tune so that we can incorporate application-specific bias into the model. 6. RELATED WORK The most related work to our work is the coupled clus- tering method presented in [8], which appears to be one of the very few studies considering the clustering problem in multiple collections. They extend the information bottle- neck approach to discover common clusters across different collections. Comparative text mining goes beyond this by analyzing both the similarities and collection-specific differ- ences. We also use a completely different approach based on probabilistic mixture models. Another related work is [10], where cross-training is used for learning classifiers from mul- tiple document sets. Our work differs from it in that we per- form unsupervised learning. The aspect models studied in [7, 2] are also related to our work but they are closer to our baseline model and are not designed for comparing multiple collections. There are many studies in document clustering [1]. Again, the difference lies in that they consider only one collection and thus are similar to the baseline model. Our work is also related to document summarization, es- pecially multiple document summarization (e.g.,[9, 12]). In- deed, we can the results of CTM as a special form of sum- mary of multiple text collections. However, an important difference is that while a summary intends to retain the ex- plicit information in text (to maintain fidelity), CTM aims at extracting non-obvious implicit patterns. 7. CONCLUSIONS AND FUTURE WORK In this paper, we define and study a novel text mining problem referred to as comparative text mining. It is con- 747 Research Track Poster Table 3: Laptop review results using CCMix model Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Cluster6 Cluster7 Cluster8 C sound 0.035 port 0.023 ram 0.105 m 0.027 battery 0.129 t 0.039 cd 0.095 office 0.037 O speakers 0.035 jack 0.021 mb 0.037 trackpad 0.018 hours 0.080 modem 0.017 drive 0.076 microsoft 0.021 M playback 0.034 ports 0.018 memory 0.034 chip 0.013 life 0.060 internet 0.017 rw 0.055 little 0.018 M feel 0.019 will 0.018 256mb 0.027 improved 0.012 5 0.038 later 0.014 dvd 0.049 basic 0.015 O pros 0.017 your 0.017 128mb 0.021 volume 0.012 end 0.016 configuration 0.014 combo 0.025 6 0.014 N cons 0.017 warm 0.013 tech 0.020 did 0.011 3 0.016 free 0.013 drives 0.023 under 0.013 market 0.017 keep 0.012 128 0.020 latch 0.011 high 0.015 vga 0.012 rom 0.020 mhz 0.012 size 0.014 down 0.012 support 0.018 make 0.010 processor 0.014 were 0.012 floppy 0.017 word 0.011 D rests 0.026 banias 0.019 options 0.039 inspiron 0.061 dells 0.032 fans 0.019 apoint 0.017 0 0.046 E palm 0.022 svga 0.014 sodimm 0.025 pentium 0.052 ran 0.017 shipping 0.017 blah 0.015 angle 0.018 L 9000 0.020 record 0.014 eraser 0.021 8200 0.03 prong 0.015 2nd 0.016 hook 0.011 portion 0.0154 L smart 0.018 supposedly 0.013 crucial 0.018 toshiba 0.027 requiring 0.014 tracking 0.015 tug 0.011 usb 0.0153 reader 0.018 rebate 0.013 sdram 0.018 440 0.026 second 0.011 spoke 0.015 2499 0.011 specials 0.014 A magazine 0.011 osx 0.040 macos 0.019 macos0.016 g4 0.016 iphoto 0.031 airport 0.075 appleworks 0.060 P ipod 0.010 quartz 0.015 personal 0.018 netscape 0.013 interlaced 0.016 itunes 0.027 burn 0.035 word 0.021 P strong 0.01 instance 0.014 shield 0.016 apache 0.009 mac 0.016 import 0.021 4x 0.018 result 0.016 L icon 0.009 underneath 0.012 airport 0.016 ie5 0.008 imac 0.014 book 0.018 reads 0.014 spreadsheet 0.013 E choppy 0.008 cooling 0.012 installation 0.015 ll 0.008 powermac 0.012 quicktime 0.016 schools 0.013 excel 0.012 I technology 0.023 rj 0.033 exchange 0.023 company 0.021 thinkpad 0.077 thinkpads 0.020 t20 0.04 list 0.015 B outdated 0.020 chik 0.018 hassle 0.016 570 0.017 ibm 0.047 connector 0.018 ultrabay 0.030 factor 0.013 M surprisingly 0.018 dsl 0.017 disc 0.015 turn 0.017 covers 0.029 connectors 0.018 tells 0.021 months 0.013 trackpoint 0.014 45 0.015 t23 0.012 buttons 0.015 lightest 0.028 bluetoot 0.018 device 0.021 cap 0.013 recommend 0.013 pacbell 0.012 cdrw 0.015 numlock 0.012 3000 0.027 sturdy 0.011 number 0.020 helpdesk 0.0128 cerned with discovering any latent common themes across a set of comparable collections of text as well as summariz- ing the similarities and differences of these collections along each theme. We propose a generative cross-collection mixture model for performing comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. We define the model and present the EM algorithm that can estimate the model ef- ficiently. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method based on a simple mixture model. Experiment results show that the cross-collection mixture model is quite effective in dis- covering the latent common themes across collections and performs significantly better than the baseline simple mix- ture model. The proposed model has many obvious applica- tions in opinion summarization and business intelligence. It also has many other less obvious applications in the general area of text mining and semantic integration of text. For example, our model can be used to compare the course web pages from the major computer science department web sites to discover core computer science topics. It can also be used to compare literature collections in different communities to support concept switching [11]. The work reported in this paper is just an initial step toward a promising new direction. There are many interest- ing future research directions. First, it may be interesting to explore how we can further improve the CCMix model and its estimation. One interesting direction is to explore the Maximum A Posterior (MAP) estimator, which would allow us to incorporate more prior knowledge in a princi- pled way. For example, a user may already have certain thematic aspects in mind. With MAP estimation, we can easily add that bias to the component models. Second, we can generalize our model to model semi-structured data to perform more general comparative data mining. One way to achieve this goal is to introduce additional random variables in each component model so that we can model any struc- tured data. Finally, it would be very interesting to explore how we could exploit the learned theme models to provide additional help to a user who wants to perform comparative analysis. For example, the learned common theme models can be used to construct a hidden Markov model (HMM) to identify the parts in the text collections about the common themes, and to connect them through automatically gener- ated hyperlinks. This would allow a user to easily navigate through the common themes. 8. REFERENCES [1] D. Baker and A. McCallum. Distributional clustering of words for text classification. In Proceedings of ACM SIGIR 1998, 1998. [2] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statist. Soc. B, 39:1-38, 1977. [4] epinions.com, 2003. http://www.epinions.com/. [5] R. Feldman and I. Dagan. Knowledge discovery in textual databases. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, 1995. [6] M. A. Hearst. Untangling text data mining. In Proceedings of ACL'99, 1999. [7] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR'99, pages 50-57, 1999. [8] Z. Marx, I. Dagan, J. Buhmann, and E. Shamir. Coupled clustering: a method for detecting structural correspondence. Journal of Machine Learning Research, 3:747-780, 2002. [9] K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. E. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI-99. [10] S. Sarawagi, S. Chakrabarti, and S. Godbole. Cross-training: Learning probabilistic mappings between topics. In Proceedings of ACM SIGKDD 2003. [11] B. R. Schatz. The interspace: Concept navigation across distributed communities. Computer, 35(1):54-62, 2002. [12] H. Zha. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of ACM SIGIR 2002. 748 Research Track Poster"
https://github.com/yiz9/CourseProject	Answers to the 4 questions.pdf	We will briefly address the 4 questions posted on piazza in this document. Note that this is not our project report, for a detailed report of our work, please see the Project Report file. An overview of the function of the code (i.e., what it does and what it can be used for) The python scripts in the datasets folders are used to obtain the two datasets. Using our own work in MP2, we were able to retrieve text data from news articles reporting COVID-19 and SARS. The python scripts in the models and analysis folder are used to generate and run the models. Our inputs are the datasets we obtained. Our outputs are clusters of words and their probabilities. They are presented both in the Project Report file and the models and analysis folder. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Please watch the demo provided in the video presentation. The use of our code is straight forward. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Please watch the demo provided in the video presentation. The use of our code is straight forward. Brief description of the contribution of each team member in case of a multi-person team. Each team member contributed equally to the program. We had multiple group meetings to assign tasks and made sure that everyone was on the same page. Dongni Yang: Studied the paper, obtained the COVID-19 dataset, obtained the SARS dataset, generated and improved the models, ran the model. The main contributor to the demo of our source code. Zhaoyuan Yang: Studied the paper, helped to generate the models, analyzed the results, and provided conclusion. The main contributor to the Project Report. Yi Zhou: Studied the paper, explained the EM algorithm used to improve the CCMix model, helped the running of the model, administrative tasks (CMT, Github, presentations, etc.)
https://github.com/yiz9/CourseProject	Cluster.docx	Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients
https://github.com/yiz9/CourseProject	Progress Report.pdf	"Progress Report Group Members: Yi Zhou Zhaoyuan Yang Dongni Yang The project of our group focuses on reproducing the results of the paper ""A cross-collection mixture model for comparative text mining"" by Zhai and his co-workers. In the paper, Zhai used his model to examine two sets of news data, one on the Iraq War and the other on the Afghanistan war. Specifically, the Iraq war news excerpts were a combination of 30 articles from the CNN, BBC websites over 2013-2014. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC websites for one year starting from Nov. 2001. To examine the robustness of the model, we decided to use different news data for our project. The Covid-19 pandemic is an event that we believe is both appropriate and comparable. With the help of our work and experiences from MP2, we extracted 35 articles from CNN, BBC, and HUFFPOST websites to form our dataset. (Work Completed) We have also begun to input the modeling formulas into our scripts. The large number of unfamiliar concepts and variables have caused us some problems. We also found some of the formulas difficult to understand, but we believe we can fix this problem by further studying the paper and doing research on the related topics. (Challenges Encountered) Our next steps would be to finish the modeling and run the dataset. Plotting and analyzing the data may involve the usage of R Markdown. After analyzing the data, we will check if our results correspond to the conclusions from the paper. (Work Pending) To view our work in detail, please take a look at the ""Progress Report"" folder."
https://github.com/yiz9/CourseProject	Project Report.docx	"Project Report Our project aims to reproduce the results in the paper by Zhai and coworkers, ""A cross-collection mixture model for comparative text mining"". In this paper, the author developed an efficient text mining model, the Cross-Collection Mixture Model (CCMix), using the Expectation-Maximization (EM) algorithm for Comparative Text Mining (CTM). The simple mixture model was originally used as a naive solution to CTM. This method treats the multiple collections as one single collection and performs clustering. The CCMix Model, however, uses latent common themes as well as a potentially different set of collection-specific themes for each collection. These component models directly correspond to all the information we are interested in discovering. In the paper, the authors used new articles from BBC and CNN as datasets to test this model. The comparison was taken between results from news on the Iraq War and news on the Afghanistan War. Unfortunately, we are unable to obtain the same dataset for this project. Instead, we chose the COVID-19 pandemic and the SARS outbreak in 2003 as the comparing events. We extracted 35 news articles from CNN, BBC, and HUFFPOST to form our dataset. To build the model, we used an open-source code on GitHub as the basis, with added features on theme clustering to let it behave as similarly in the paper. The paper suggested that the CCMix model can help reveal many interesting common aspects between the Iraq War and the Afghanistan War that the SimpMix model failed to do. The results presented by SimpMix were less meaningful, making it hard for people to extract useful information from the common words. In contrast, we were able to gather useful information from the CCMix results. For example, from cluster 5 we knew that the news about both events mentioned the diplomatic role played by the United Nations, and from cluster 4 we knew that there were Monday briefings by an official spokesman of a political administration during both wars. Our attempt in comparing the SimpMix and CCMix model also led to meaningful results. We set lB = 0:95 for SimpMix and set lb = 0:9, lc = 0:25 for CCMix; in both cases, the number of clusters is fixed to 5. These parameters are exactly the same as in the paper. The results are tabulated below: SimpMix Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients Although there were some interesting themes from the SimpMix results (for instance, cluster 2 mentioned the lockdown policies in Britain, and cluster 3 indicates the government's effort in inventing vaccines), we could not find common themes to both events. It appears that cluster 1 to 3 solely described themes on COVID-19 and cluster 4 and 5 only covered the SARS pandemic. The CCMix results gave us more information on the similarities between SARS and COVID-19. In cluster 1, ""vaccine"", ""firms"" and ""animals"" suggest that there had been attempts for animal trials on vaccines in both cases. In cluster 4, we were able to tell that both pandemics involved an outburst of infected cases in Hong Kong, which allowed the events to draw global attention. The common theme captured in cluster 5 simply told us that a lot of patients went to the hospitals because of the diseases. The term ""dr"" also implied that there are large portions of interviews and advice of doctors in the news articles. Overall, our reproduction work using CCmix model gave us similar results as in the paper. However, it was evident that there were aspects that our model failed to do. For example, words such as ""coffee"" and ""beans"" that were unrelated to our topic appear in the common theme words section in our results. One possible explanation is that we falsely collected words in the advertisement into our dataset. Since our new articles mainly came from BBC News, the article websites may be displaying the same advertisement on coffee which let ""coffee"" being the most popular theme word. Lacking information on specific events was another drawback of our research. In the paper, the authors were able to gather detailed information on the themes (for example, the name of the spokesman, the name of the commander, etc.). Our results, however, only contained general aspects of the events. We were unable to figure out what exactly happened by just looking at the common theme words. This might be caused by the inherent difference of our dataset compared to the one in the paper. As the geographic scale and timescale are much larger in the case of the pandemic than the war, the focus on the news articles tends to be diverse. Therefore, it can be hard for a word that is only important to a specific event to stand out in the common theme words list in our case. One possible solution we can think of is to set a lower lb value to let uncommon words more favored that set a higher lc to make the collection more collection-specific."
https://github.com/yiz9/CourseProject	Project Report.pdf	"Project Report Our project aims to reproduce the results in the paper by Zhai and coworkers, ""A cross- collection mixture model for comparative text mining"". In this paper, the author developed an efficient text mining model, the Cross-Collection Mixture Model (CCMix), using the Expectation-Maximization (EM) algorithm for Comparative Text Mining (CTM). The simple mixture model was originally used as a naive solution to CTM. This method treats the multiple collections as one single collection and performs clustering. The CCMix Model, however, uses latent common themes as well as a potentially different set of collection-specific themes for each collection. These component models directly correspond to all the information we are interested in discovering. In the paper, the authors used new articles from BBC and CNN as datasets to test this model. The comparison was taken between results from news on the Iraq War and news on the Afghanistan War. Unfortunately, we are unable to obtain the same dataset for this project. Instead, we chose the COVID-19 pandemic and the SARS outbreak in 2003 as the comparing events. We extracted 35 news articles from CNN, BBC, and HUFFPOST to form our dataset. To build the model, we used an open-source code on GitHub as the basis, with added features on theme clustering to let it behave as similarly in the paper. The paper suggested that the CCMix model can help reveal many interesting common aspects between the Iraq War and the Afghanistan War that the SimpMix model failed to do. The results presented by SimpMix were less meaningful, making it hard for people to extract useful information from the common words. In contrast, we were able to gather useful information from the CCMix results. For example, from cluster 5 we knew that the news about both events mentioned the diplomatic role played by the United Nations, and from cluster 4 we knew that there were Monday briefings by an official spokesman of a political administration during both wars. Our attempt in comparing the SimpMix and CCMix model also led to meaningful results. We set lB = 0:95 for SimpMix and set lb = 0:9, lc = 0:25 for CCMix; in both cases, the number of clusters is fixed to 5. These parameters are exactly the same as in the paper. The results are tabulated below: SimpMix Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words coffee people students sars sars students rules vaccine china china people wales firms disease health workers lockdown vaccines april outbreak universities covid pandemic travel people university government people health virus health restrictions support chinese kong beans england governments hong hong CCMIX Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common Theme words firms sars people health people vaccine china rules virus covid vaccines disease lockdown people dr animals coffee wales kong hospital governments beijing restrictions students south christmas workers government hong health prices officials support china infection pandemic authorities university canada patients Although there were some interesting themes from the SimpMix results (for instance, cluster 2 mentioned the lockdown policies in Britain, and cluster 3 indicates the government's effort in inventing vaccines), we could not find common themes to both events. It appears that cluster 1 to 3 solely described themes on COVID-19 and cluster 4 and 5 only covered the SARS pandemic. The CCMix results gave us more information on the similarities between SARS and COVID-19. In cluster 1, ""vaccine"", ""firms"" and ""animals"" suggest that there had been attempts for animal trials on vaccines in both cases. In cluster 4, we were able to tell that both pandemics involved an outburst of infected cases in Hong Kong, which allowed the events to draw global attention. The common theme captured in cluster 5 simply told us that a lot of patients went to the hospitals because of the diseases. The term ""dr"" also implied that there are large portions of interviews and advice of doctors in the news articles. Overall, our reproduction work using CCmix model gave us similar results as in the paper. However, it was evident that there were aspects that our model failed to do. For example, words such as ""coffee"" and ""beans"" that were unrelated to our topic appear in the common theme words section in our results. One possible explanation is that we falsely collected words in the advertisement into our dataset. Since our new articles mainly came from BBC News, the article websites may be displaying the same advertisement on coffee which let ""coffee"" being the most popular theme word. Lacking information on specific events was another drawback of our research. In the paper, the authors were able to gather detailed information on the themes (for example, the name of the spokesman, the name of the commander, etc.). Our results, however, only contained general aspects of the events. We were unable to figure out what exactly happened by just looking at the common theme words. This might be caused by the inherent difference of our dataset compared to the one in the paper. As the geographic scale and timescale are much larger in the case of the pandemic than the war, the focus on the news articles tends to be diverse. Therefore, it can be hard for a word that is only important to a specific event to stand out in the common theme words list in our case. One possible solution we can think of is to set a lower lb value to let uncommon words more favored that set a higher lc to make the collection more collection-specific."
https://github.com/yiz9/CourseProject	Proposal.pdf	Reproducing A Paper: Contextual text mining The team captain is Yi Zhou. Netid is yiz9. Team members are: Yi Zhou. Netid is yiz9. (Captain) Zhaoyuan Yang. Netid is zy7. Dongni Yang. Netid is dongniy2. Our chosen paper is A CrossCollection Mixture Model for Comparative Text Mining. We are going to use python in this project. We can't obtain the exact same dataset used in the paper. However, we can use other datasets to achieve similar goals.
https://github.com/yiz9/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Progress Report updated. (Used part of our work from MP2. I hope that's ok.) Project completed. Make sure you check out our Project Report and our Project Presentation, which includes a demo of our source code. Source code for each part can be found in each folder. We used our own works in MPs and also some online resources. The references can be found in the main branch. Thank you for grading! If needed, please contact us through the emails provided on the CMT. Just added an addition to our documentation, which answers the 4 questions on Piazza. Thank you for Grading!
https://github.com/bzhao10/CourseProject	README.md	CS410 CourseProject(Text Classification Competition): Twitter Sarcasm Detection Table of Contents Background Install Usage Presentation Results Contributing Citation Background This is a course project for CS410 Text Information Systems and is a part of a text classification competition, which involves twitter sarcasm detection. In this competition, it is required that you classify a twitter response in a conversation into either 'sarcasm' or 'not sarcasm'. Data The dataset is comprised of two jsonl files, including a train.jsonl for data training and a text.jsonl for text classification. Each line of the training dataset includes the following fileds: - response : the Tweet to be classified - context : the conversation context of the response - label : SARCASM or NOT_SARCASM - id: String identifier for sample. The testing dataset(text.jsonl) differs from the training dataset only in that the each line of the dataset lacks the label. The size of training dataset is 5000 and the size of the testing dataset is 1800. Output The output of the project is an anwser.txt document, each line of which includes both id of test sample and a label of either SARCASM or NOT_SARCASM predicted by the model. Install You can run the whole project on Google Colab. You don't have to install anything locally. Usage Each of the ipynb file in the Code folder represents one solution in the competition. Step 1 Download two jsonl files, including one training data file and one testing data file, from dataset. Step 2 Download one of these ipynb files and open it on Google Colab. Step 3 Upload the two jsonl files to Google Colab under the dataset folder in the project that you have opened in Step 2. Step 4 Run the code line by line using Google Colab. By following all the steps mentioned above, you will get an answer.txt file. Please refer to the presentation video for detailed instructions. Presentation Following is our presentation demo link: https://mediaspace.illinois.edu/media/1_ef6myyuo Results The following table records the best performance achieved (highest F1 score) by using each model: The score marked in bold are passing the baseline scores. | Model| Precision | Recall| F1 | |-------|-------|-------|-------| | ALBERT | 0.65814 |0.77222 | 0.71063 | | ALBERT V2 | 0.64377 |0.56222 | 0.60024 | | BERT | 0.62681 | 0.86778 | 0.72787 | | BERT(RSUP) | 0.64717 | 0.76222 | 0.7 | | BERT(R_context) | 0.66042 | 0.70444 | 0.68172 | | RoBERTa | 0.63974 | 0.89777 | 0.74711 | | RoBERTa-large | 0.54708 | 0.98778 | 0.70416 | | RoBERTa(R_context) | 0.67836 | 0.77333 | 0.72274 | | SqueezeBERT | 0.61348 | 0.91 | 0.73289 | | XML-RoBERTa | 0.61864 | 0.89222 | 0.73066 | RSUP: Remove stopwords and unnecessary puncuations R_context: Reverse Context Contributing Contributors Team Name: GFZ Team Member: Bei Zhao - beizhao3 (Captain) beizhao3@illinois.edu Ryan Fraser - rfraser3 rfraser3@illinois.edu Yiming Gu - yimingg7 yimingg7@illinois.edu Citation Tutorial: Fine-Tuning BERT for Spam Classification
https://github.com/bzhao10/CourseProject	Team GFZ - Course Project Proposal.pdf	"Text Classification Competition Proposal Team GFZ 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Bei Zhao - beizhao3 (Captain) Ryan Fraser - rfraser3 Yiming Gu - yimingg7 2. Which competition do you plan to join? Text Classification Competition 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Here are some neural classifiers and deep learning frameworks that we are going to learn and try: * Word2Vec and GloVe Word2Vec and GloVe are word-based models that can map the vocabularies and phrases into vectors of numbers, which can help us to separate the phrases and words. For the context of meaningful classification, they need to combine with neural networks to work out, such as convolutional neural networks (CNN), recurrent neural networks (RNN), artificial neural networks (ANN), etc. * BERT BERT is a context-based model, which can map the vocabularies and phrases based on the sentence position. However, this model is limited to the size of the corpus it could train compared to Word2Vec and GloVe. But it abandons the traditional RNN and CNN formula, therefore, this may perform much faster than Word2vec and GloVe implementations. * Other Models We will try other useful classifiers and frameworks in the competition procedure continuously until we find the best fit one. Here are the relevant experience of each team members: * Bei: Don't have any Machine Learning experience but will try to learn as much as possible in this competition. * Ryan: Completed Data Science Certificate from UCLA for-credit, included the ""Machine Learning Using R"" course which went through the ""Introduction to Statistical Learning"" book. I will need to take some time to get up to speed on these methods but anticipate it should not take long and am prepared to learn these frameworks. I have made progress in the fast.ai tutorials in an effort to learn more about these methods, and have heard about using TensorFlow/Keras/PyTorch to complete these types of tasks. * Yiming: Don't have any Machine Learning experience but will try to learn it in this competition. 4. Which programming language do you plan to use? The main programming language that we are going to use is Python. 5. Competition Milestones: * Text /Data processing In this step, we will prepare and process the raw data. For example, text may contain numbers, special characters, etc. We need to transform the raw data into something that could be used in the project. * Feature extracting We will extract features from the texts in order to classify them. This is important because only when the features are extracted precisely, the text could then be rightly classified. We will evaluate different models in this step as well. * Train and evaluate models We will train different models on a dataset in order to investigate their actual effectiveness. It is essential that we optimize the models according to the results. * Deploy the models in the competition * Revise and keep optimizing if necessary * Document source code and test set predictions * Document the model and the test set predictions, experiments with other methods, hyperparameter tuning, etc. * Create a demo that shows code can run on the test set and generate submitted predictions, etc"
https://github.com/bzhao10/CourseProject	Team GFZ - Final Report.pdf	"Team GFZ Final Report Bei Zhao(Captain) beizhao3@illinois.edu Ryan Fraser rfraser3@illinois.edu Yiming Gu yimingg7@illinois.edu Table of Contents: Explain what problem we are trying to solve 2 Explain our model 3 Explain how we perform the training 5 Explain any other methods tried and hyperparameter tuning 6 Results from testing various pre-trained model and hyperparameter combinations 7 Team Member Contributions 9 References 10 1 Explain what problem we are trying to solve This code attempts to detect sarcasm in tweets. This is a particularly tricky problem in Natural Language Processing (NLP) due to the nature of sarcasm itself. Sarcasm is highly dependent on the author, the specific topic in question, cultural attitudes about the topic, current events, tone, and much more. The difficulty is further compounded by the fact that sarcasm is (usually) intentionally designed to not sound sarcastic. In a simple example, a person asking his friend if an idea is good may hear that his idea is ""a really good idea,"" when in fact the friend means the opposite. In particular, this code was built for the Text Classification competition for CS 410 at the University of Illinois at Urbana-Champaign. In this competition, 2 sets of tweets are provided. The training set is pre-labeled and identifies if example tweets are sarcastic or not. The test set is not labeled. Both sets also contain the context of the tweets, which in this context means the parent tweets that the tweets in question are responding to. The goals of the competition are twofold. First, teams must outperform the baseline, which means realizing an F1 score of about 0.723 (based on current results on LiveDataLab). Second, teams must try to rank highest out of all other teams. 2 Explain our model We achieved our best results for this task by using a pre-trained deep learning model called RoBERTa. This model builds off of another well-known deep learning model called BERT, which stands for ""Bidirectional Encoder Representations from Transformers."" A few key differences between BERT and RoBERTa are outlined below: As one can see, RoBERTa takes longer to train because it has much more data. It also does not use Next Sentence Prediction, which is explained further below. At its core, BERT uses Transformers. These are a deep learning construct that ""learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms - an encoder that reads the text input and a decoder that produces a prediction for the task."" A key feature of BERT is that it is bidirectional. This means it reads all of the words at once and uses all context available. Prior models used left, right, left-right, right-left context or some combination of those. This means that BERT obtains a more holistic understanding of context versus prior methods, usually leading to better results. In very general terms, BERT uses 2 strategies to train on the data. First is Masked LM and second is Next Sentence Prediction. 3 In Masked LM, BERT hides a number of words in each sequence before training and then attempts to predict the hidden words. The loss function only considers the prediction of those hidden values. Because of this, the model is a bit slower to converge than prior models that did not make use of the full context. In Next Sentence Prediction, the BERT model also ""receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence."" Note that RoBERTa does not use Next Sentence Prediction. Both strategies are used when training the model and the goal is to balance and minimize the loss of the combined strategies. We believe that RoBERTa outperforms BERT for sarcasm detection in tweets because tweets are very short and tend to have less overall context. Therefore, it makes sense that a scoring mechanism that does not focus as much on Next Sentence Prediction will be more fit for purpose for this competition. 4 Explain how we perform the training Our very general outline of steps taken is below. To keep things concise, only high level descriptions are given. Our code is somewhat short and more detailed documentation is available in the code comments. Basic Steps: * Convert JSON data to CSV format while cleaning the labels * Create k-fold cross-validation training sets * Create a list of dictionaries for our training and test data * Create CSVs with the context data, the response data, and the labels * Convert CSVs into pandas dataframes * Split data into respective training/test sets * Select model and import into the environment * Encode the text data using the Tokenizer library * Convert data into Tensors using PyTorch * Set up model parameters * Batch Size * Wrap Tensors * Create sampler for sampling data during training * Create dataLoader for training set * Freeze parameters * Create model class and initialization logic * Initialize model * Import model optimizer from HuggingFace Transformers library * Compute starting class weights * Convert class weights to Tensors * Define number of training epochs * Define model training function * Define model evaluation function * Run and train the model 5 Explain any other methods tried and hyperparameter tuning The team spent time on a number of different tasks outside of just building the model: 1. We spent a few weeks researching the competition and educating ourselves on deep learning models. When we first started, we believed we'd use word2vec and/or sentence2vec, two approaches for NLP. However, as we did more research, we found that there were better models we could use. 2. We spent time trying various deep learning models to see how each performed. We used a library from HuggingFace that allowed us to easily swap out pretrained NLP deep learning models. Below we've outlined the performance of various pretrained model/hyperparameter combinations, which is how we ultimately decided to use RoBERTa. 3. We also spent time trying to identify which data to train the model on. The data includes both target tweets and the context surrounding them, therefore, there were 3 intuitive combinations we felt were worth trying: response-only, context-only, and a combination of both. We found that the combination of both works best. This made sense to us given the very nature of sarcasm. In fact, sarcasm is almost always an extremely context dependent response to a topic that is oftentimes serious, or not inherently sarcastic. By its nature, sarcasm taken out of context sounds like any other normal statement. (Just think of the phrase, ""good work,"" which can take on many different meanings depending on a huge number of factors. Most of the time, this should be taken as a friendly, encouraging, and positive statement. However, depending on the context, it could also be a sarcastic statement.) 6 Results from testing various pre-trained model and hyperparameter combinations Following are the different testing results we've tried so far (We only include the best scores of different setup models). Passing the baseline F1 scores are highlighted in green color. 7 Model precisio n recall f1 Model Description ALBERT BS=25 0.65294 0.74000 0.69375 ALBERT model with Batch Size 25 ALBERT BS=25 MSL= 300 0.64178 0.70667 0.67266 ALBERT model with Batch Size 25 max_seq_len=300 ALBERT BS=26 0.64400 0.75778 0.69627 ALBERT model with Batch Size 26 ALBERT BS=27 0.63944 0.75667 0.69313 ALBERT model with Batch Size 27 ALBERT BS=28 0.63520 0.77000 0.69613 ALBERT model with Batch Size 28 ALBERT BS=29 0.62868 0.76000 0.68813 ALBERT model with Batch Size 29 ALBERT BS=30 0.62960 0.79889 0.70421 ALBERT model with Batch Size 30 ALBERT BS=31 0.65814 0.77222 0.71063 ALBERT model with Batch Size 31 ALBERT BS=32 0.65145 0.69778 0.67382 ALBERT model with Batch Size 32 ALBERT BS=33 0.64371 0.77889 0.70488 ALBERT model with Batch Size 33 ALBERT BS=34 0.61558 0.82556 0.70527 ALBERT model with Batch Size 34 ALBERT BS=35 0.65490 0.74222 0.69583 ALBERT model with Batch Size 35 ALBERT V2 BS=31 0.64377 0.56222 0.60024 ALBERT model v2 with Batch Size 35 BERT BS=25 0.68477 0.71444 0.69929 BERT model with Batch Size 25 BERT BS=26 RS&UP 0.63957 0.72556 0.67985 BERT model with Batch Size 26 remove stopword and unnecessary punctuations BERT BS=27 0.65652 0.76667 0.70733 BERT model with Batch Size 27 BERT BS=27 RS&UP 0.64245 0.75667 0.6949 BERT model with Batch Size 27 remove stopword and unnecessary punctuations BERT BS=28 0.62681 0.86778 0.72787 BERT model with Batch Size 28 BERT BS=28 R_context 0.66042 0.70444 0.68172 BERT model with Batch Size 28 and Reverse Context BERT BS=28 RS&UP 0.64717 0.76222 0.7 BERT model with Batch Size 28 remove stopword and unnecessary punctuations BERT BS=29 0.6519 0.80111 0.71884 BERT model with Batch Size 29 BERT BS=29 R_context 0.67768 0.57000 0.61919 BERT model with Batch Size 29 and Reverse Context BERT BS=29 RS&UP 0.64358 0.70222 0.67163 BERT model with Batch Size 29 remove stopword and unnecessary punctuations 8 BERT BS=30 0.68614 0.66556 0.67569 BERT model with Batch Size 30 RoBERTa BS=28 0.66057 0.80222 0.72453 RoBERTa model with Batch Size 28 RoBERTa BS=29 0.64521 0.86888 0.74053 RoBERTa model with Batch Size 29 RoBERTa BS=29 R_context 0.67836 0.77333 0.72274 RoBERTa model with Batch Size 29 and Reverse Context RoBERTa BS=30 0.66324 0.78778 0.72016 RoBERTa model with Batch Size 30 RoBERTa BS=31 0.63974 0.89777 0.74711 RoBERTa-large model with Batch Size 31 RoBERTa-large BS=27 0.64571 0.62778 0.63662 RoBERTa-large model with Batch Size 27 RoBERTa-large BS=28 0.54708 0.98778 0.70416 RoBERTa-large model with Batch Size 28 RoBERTa-large BS=29 0.71308 0.37556 0.49199 RoBERTa-large model with Batch Size 29 SqueezeBERT BS=28 0.6506 0.72 0.68354 SqueezeBERT model with Batch Size 28 SqueezeBERT BS=29 0.6537 0.74667 0.6971 SqueezeBERT model with Batch Size 29 SqueezeBERT BS=30 0.65345 0.76889 0.70648 SqueezeBERT model with Batch Size 30 SqueezeBERT BS=32 0.63973 0.84444 0.72797 SqueezeBERT model with Batch Size 32 SqueezeBERT BS=33 0.61348 0.91 0.73289 SqueezeBERT model with Batch Size 33 SqueezeBERT BS=34 0.63851 0.84 0.72553 SqueezeBERT model with Batch Size 33 XML-RoBERTa BS=27 0.70183 0.34 0.45808 XML-RoBERTa model with Batch Size 27 XML-RoBERTa BS=28 0.6059 0.91222 0.72816 XML-RoBERTa model with Batch Size 28 XML-RoBERTa BS=29 0.61864 0.89222 0.73066 XML-RoBERTa model with Batch Size 29 XML-RoBERTa BS=29 MSL=150 0.64249 0.82667 0.72303 XML-RoBERTa model with Batch Size 29, max_seq_len = 150 XML-RoBERTa BS=30 0.60588 0.91556 0.72920 XML-RoBERTa model with Batch Size 30 Team Member Contributions - Bei Zhao (Team Captain): Bei served as our team captain for this project. Bei helped on all facets of the project. She wrote our initial proposal and coordinated all meeting times. Bei also spent a lot of time working to try various model/hyperparameter combinations to get the best results. - Yiming Gu: Yiming was instrumental in getting our model up and running. He was quickly and efficiently able to get us a very solid foundation to work off of, such that we could spend time focusing on the best model and hyperparameters. Yiming also spent a lot of time trying various model and hyperparameter combinations to find the best results and wrote the README for the project. - Ryan Fraser: Ryan helped the team strategize about how to approach the competition. He spent time trying to get the model to run on a GPU for faster training times, but ultimately the team decided to use CPUs. Ryan also wrote the project progress report, the documentation, and the final report. 9 References Amardeep Kumar and Vivek Anand 2020. Transformers on Sarcasm Detection with Context. Proceedings of the Second Workshop on Figurative Language Processing Debanjan Ghosh, Avijit Vajpayee, Smaranda Muresan, and Educational Testing Service 2Data Science Institute, Columbia University {dghosh, avajpayee}@ets.org smara@columbia.edu 2020. A Report on the 2020 Sarcasm Detection Shared Task. arXiv preprint arXiv:2005.05814. GLUE Benchmark Leaderboard. (n.d.). Retrieved November 11, 2020, from https://gluebenchmark.com/leaderboard Hankyol Lee, Youngjae Yu, and Gunhee Kim. Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context. arXiv preprint arXiv:2006.06259. Ingham, F. (2018, November 27). Dissecting BERT Part 2: BERT Specifics. Retrieved November 11, 2020, from https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73 Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, and Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805. Nikhil Jaiswal 2020. Neural Sarcasm Detection using Conversation Context. Proceedings of the Second Workshop on Figurative Language Processing Tanvi Dadu and Kartikey Pant 2020. Sarcasm Detection using Context Separators in Online Discourse. arXiv preprint arXiv:2006.00850. Xiangjue Dong, Changmao Li, and Jinho D. Choi 2020.Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media. arXiv preprint arXiv:2005.11424. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Conference paper at ICLR 2020 Suleiman Khan, Ph.D. ""BERT, RoBERTa, DistilBERT, XLNet - Which One to Use?"" Medium , Towards Data Science, 17 Oct. 2019, towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8. 10"
https://github.com/bzhao10/CourseProject	Team GFZ - Project Progress Report.pdf	UIUC Online MCS - CS410: Text Information Systems Fall 2020 Team GFZ (Bei Zhao, Yiming Gu, Ryan Fraser) Project Progress Report Progress Made For our project, we decided to compete in the Text Classification Competition. In summary, we have become more familiar with deep learning generally and BERT in particular, and have implemented a working model that surpasses the competition baseline. Our weekly status is outlined below for more detail: * Week 1: the team spent time individually researching various approaches to solving this problem. We explored Kaggle competition notebooks, browsed academic papers, and searched the internet for tools that could be useful. We then aggregated and summarized our findings. * Week 2: the team agreed to focus on using the BERT deep learning model for our submission. All of us are new to deep learning, so we spent this week learning more about deep learning generally and BERT in particular. * Week 3 and 4: to better understand BERT and prepare ourselves for the competition, we all individually used this time to get BERT working on our local machines. This helped us understand how data needed to be cleaned and prepared for use, and also helped us understand the behavior of the model when different parameters change. * Week 5: we are currently working on enhancing model performance. We are working to get our model running on Google Colab and we are also experimenting with a few changes to our approach. First, we are testing if we can improve performance by cleaning the dataset in different ways. Second, we are attempting to implement an enhanced version of BERT called RoBERTa to see if this helps. Remaining Tasks 1. Fine tune the model and prepare it for final submission a. Try c1+c2+c3 preprocessing of context with response and c3+c2+c1 preprocessing of context b. Try other models such as XLM-RoBERTa, ALBERT, SqueezeBERT, etc. c. Change model parameters to optimize performance d. Change train_test_split parameters and max_seq_len parameters to optimize performance 2. Document how to use the code 3. Document how the code is implemented 4. Software usage tutorial presentation Challenges/Issues Being Faced In general, things are going very smoothly. Our toughest challenge so far has been identifying a suitable solution to the problem and writing code so that it works for this specific dataset and use case. Getting up to speed quickly on deep learning, while also taking this course, has proven time consuming.
https://github.com/losaohan/CourseProject	CS 410 Project Documentation.docx	"CS410 Project Documentation - ExpertSearch System Overview of the function of the code The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Users can provide a piece of search query that contains name, university, specialization, location, etc. in the text box showing ""Enter Search Query"", and hit ""Enter"" on keyboard or click the icon on the right side of the text box. Two additional filters ""Locations"" and ""Universities"" are optional to further filter the desired faculty's geographic information and association name. The search result of faculties will display in the order of relevance below. There are two rows in the header area. In the first row, faculty name will show on the left bald-faced, and Area of interest (the new feature added in this project) aligns to the right. There can also be a envelop shape icon if email address is captured, and clicking it will evoke the system-default email application to draft an email to the faculty. The next icon will lead to the faculty bio page. In the second row, department and university name display on the left, while the state and country name show on the right. In the main body of search result, it shows a segment from detailed faculty introduction that contains the query information. Code structure and implementation details Information extraction The first step of the code is to extract information from the source data. The raw data is saved in the ./data/compiled_bios folder. Each file is named by a 0-based index and contains one line of faculty detailed introduction. The existing code has 3 .py files in the ./extraction folder, each has the Python code to extract email, names and location from the raw data. This project added a Jupyter notebook called ""named-entity-recognition.ipynb"" that can extract the research area of interest from the raw data. Below is a brief introduction of the code logic: Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence: ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] }) Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy. Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are."" Testing We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Here are a few examples of research area extracted: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')] Extracted information are saved under ./data folder, each named by the field extracted. For example, in the names.txt file, there are 6525 rows. Each row corresponds to the faculty name extracted from the raw data at the same index. Ranking function Those extracted information from last step are merged into a metadata.dat file under ./data/compiled_bios folder. The existing code already has 8 fields in each row in the metadata, so this project created a ./data/compiled_bios/merge_new_field_to_metadata.py file to append the newly extracted research area to the end of each row. In addition, it' also required to add the name and type to the end of file.toml file under the same folder, so that the code knows how to parse the new field added. The metadata file is then feed into the ranker located as ./data/expertsearch/ranker.py, configed by a config file located at ./data/compiled_bios/config.toml. Visualization webpage Visualization is handled by the ./templates/index.html file, which designs the layout and format of the ExpertSearch webpage. It's also controlled by ./static/index.js file to interact with the ranker and user. The new field ""Area of Interest"" needs to be added in the JavaScript file, in order to display on the search result. Main function The main function is called ./server.py, that is executed when the program launches. It uses flask to drive the ranker and visualization functions, so that a local server is running at http://localhost:8095/. It's also necessary tell the code to get the research interest in metadata and pass it to the Javascript. Installation and usage It's extremely easy to install and run the code on Mac or Linux, but needs some tricky settings to execute on Windows. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required. Installation on Windows Install Git (if you haven't) In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/ Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students. Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"": In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window. In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."". Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Hit Run: the image will start to build, and the container will launch in a few seconds. In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up On Mac/Linux Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter. In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: Team member contribution Bruno Seo: Focused on the modeling part: analyzed on the source data, identified the pattern of research interest key words, implemented the algorithms of Named Entity Recognition, and trained the extraction model. Joseph Angulo: Focused on the front-end visualization part: figured out how the web server work, designed the appearance of updated webpage, and implemented the changes to show the new field on the page. Xiaohan Liu (captain): Coordinate and focus on the high-level Python: solved the installation and execution across Windows/Mac/Linux systems, figured out the structure and workflow of the project, implemented code to prepare training data as well as code to create metadata, drafted the proposal / progress report / final document / presentation."
https://github.com/losaohan/CourseProject	CS 410 Project Documentation.pdf	"CS410 Project Documentation - ExpertSearch System 1 Overview of the function of the code The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Users can provide a piece of search query that contains name, university, specialization, location, etc. in the text box showing ""Enter Search Query"", and hit ""Enter"" on keyboard or click the icon on the right side of the text box. Two additional filters ""Locations"" and ""Universities"" are optional to further filter the desired faculty's geographic information and association name. The search result of faculties will display in the order of relevance below. There are two rows in the header area. In the first row, faculty name will show on the left bald-faced, and Area of interest (the new feature added in this project) aligns to the right. There can also be a envelop shape icon if email address is captured, and clicking it will evoke the system- default email application to draft an email to the faculty. The next icon will lead to the faculty bio page. In the second row, department and university name display on the left, while the state and country name show on the right. In the main body of search result, it shows a segment from detailed faculty introduction that contains the query information. 2 Code structure and implementation details 2.1 Information extraction The first step of the code is to extract information from the source data. The raw data is saved in the ./data/compiled_bios folder. Each file is named by a 0-based index and contains one line of faculty detailed introduction. The existing code has 3 .py files in the ./extraction folder, each has the Python code to extract email, names and location from the raw data. This project added a Jupyter notebook called ""named-entity-recognition.ipynb"" that can extract the research area of interest from the raw data. Below is a brief introduction of the code logic:  Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence: ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] })  Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy.  Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are.""  Testing We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Here are a few examples of research area extracted: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')] Extracted information are saved under ./data folder, each named by the field extracted. For example, in the names.txt file, there are 6525 rows. Each row corresponds to the faculty name extracted from the raw data at the same index. 2.2 Ranking function Those extracted information from last step are merged into a metadata.dat file under ./data/compiled_bios folder. The existing code already has 8 fields in each row in the metadata, so this project created a ./data/compiled_bios/merge_new_field_to_metadata.py file to append the newly extracted research area to the end of each row. In addition, it' also required to add the name and type to the end of file.toml file under the same folder, so that the code knows how to parse the new field added. The metadata file is then feed into the ranker located as ./data/expertsearch/ranker.py, configed by a config file located at ./data/compiled_bios/config.toml. 2.3 Visualization webpage Visualization is handled by the ./templates/index.html file, which designs the layout and format of the ExpertSearch webpage. It's also controlled by ./static/index.js file to interact with the ranker and user. The new field ""Area of Interest"" needs to be added in the JavaScript file, in order to display on the search result. 2.4 Main function The main function is called ./server.py, that is executed when the program launches. It uses flask to drive the ranker and visualization functions, so that a local server is running at http://localhost:8095/. It's also necessary tell the code to get the research interest in metadata and pass it to the Javascript. 3 Installation and usage It's extremely easy to install and run the code on Mac or Linux, but needs some tricky settings to execute on Windows. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required. 3.1 Installation on Windows  Install Git (if you haven't)  In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git  Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/  Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students.  Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"":  In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window.  In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."".  Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Hit Run: the image will start to build, and the container will launch in a few seconds.  In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up 3.2 On Mac/Linux  Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter.  In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt  Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: 4 Team member contribution Bruno Seo: Focused on the modeling part: analyzed on the source data, identified the pattern of research interest key words, implemented the algorithms of Named Entity Recognition, and trained the extraction model. Joseph Angulo: Focused on the front-end visualization part: figured out how the web server work, designed the appearance of updated webpage, and implemented the changes to show the new field on the page. Xiaohan Liu (captain): Coordinate and focus on the high-level Python: solved the installation and execution across Windows/Mac/Linux systems, figured out the structure and workflow of the project, implemented code to prepare training data as well as code to create metadata, drafted the proposal / progress report / final document / presentation."
https://github.com/losaohan/CourseProject	CS 410 Project Progress Report.docx	"CS 410 Project Progress Report Modification of project scope: In the original project proposal, we planned to extract and show both research interests and top 2/3 keywords in publications. After careful consideration, we think the second part will take tremendous time to download and parse publications from sources (like Arxiv or Google Scholar). Therefore, we decided to hold off the publication keywords and first focus on research interests in this project. After we finish the visualization of research interests, we may either come back to the publication keywords, or switch to another field that is contained in the faculty bio data. Progress report: The following tasks in the proposal have been finished: Digest the existing data + code + visualization and test how they work and connect (10h) Analyze the dataset for the pattern of the research of interest and publication keywords, and come up with corresponding algorithms (10h) Implement functions to retrieve those fields (15~20h) Train the new text retrieval/mining system and make some improvements (10h) The following tasks are pending: Adjust the web frontend to display the new fields (5~10h) Test the new function as well as the whole system (5~10h) Prepare the complete documentation/report (5~10h) Challenges: The biggest challenge we faced was to systematically download faculty publications using faculty name/university as the query. After spending some time engineering on this, we decided to hold this off and may either come back to it later or switch to another field that is contained in the faculty bio data. We also had some difficulties at the beginning running and debugging the program on Windows, as some of our teammates use Windows machine. With the support of Docker in Pycharm Professional version, we were finally able to run it on Windows. We are currently struggling with html and Javascript to display the new field ""research interests"" on the webpage. We think this should be manageable as it is almost the last ""technical"" part of the project. Below is a brief description of our steps to extract area of interests: Data Preparation The dataset we use is the `faculty dataset` collected from CS 410 class. While there are 6525 numbers of documents in the dataset, we randomly sampled 25 documents for training. Then, we tokenized sentences and labeled target tokens into corresponding entities. For example, we labeled 'database systems' as a `research area` in the following sentence ""His primary research interests are database systems."" ('''His primary research interests are database systems, object-oriented systems and software engineering.''', { 'entities': [(35, 51, 'AREA'), (53,76, 'AREA'), (81, 101, 'AREA'), ] }) Language Modelling While Spacy provides a pre-trained CNN model, we decided to build a new model from scratch with 70 numbers error-free entities. We used 0.5 for the drop rate and default optimizer provided in Spacy. Rule based Matching To maximize the quality of outcome, we also leverage rule-based matcher using regular expressions. Since the entities we are looking for are limited to `research interest`, we first searched a sentence that includes `research` or `interest` in the validation dataset. We observed that the entities we aim to extract have a simple structure. For example, they are ""My research interest is ..., Research interests are."" Outcome We obtained qualitative results when performing interpolation in our initial model. We will continue testing our model on the testing dataset for future work. Example: Entities [('Distributed computing', 'AREA'), ('Analysis of algorithms', 'AREA'), ('Data structures', 'AREA'), ('Computational geometry', 'AREA'), ('Graph algorithms', 'AREA')] Entities [('computer networking', 'AREA'), ('computer security', 'AREA')] Entities [('CS Education', 'AREA')]"
https://github.com/losaohan/CourseProject	CS410 Project -- Presentation Slides.pdf	"CS410 Project -- ExpertSearch System B RU N O S E O J O S E P H A N G U LO X I AO H A N L I U Introduction The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Introduction The search result of faculties will display in the order of relevance below. Another Sample Input Installation It's extremely easy to install and run the code on Mac or Linux On Windows it requires some tricky settings to execute the code. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required to run the code. Installation on Windows Install Git (if you haven't) In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/ Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students. Installation on Windows Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"": Installation on Windows In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. Installation on Windows A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window. Installation on Windows In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."". Installation on Windows Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Installation on Windows Hit Run: the image will start to build, and the container will launch in a few seconds. Installation on Windows In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up Installation on Mac/Linux Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter. Installation on Mac/Linux In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt Installation on Mac/Linux Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: Thanks!"
https://github.com/losaohan/CourseProject	CS410 Project -- Presentation Slides.pptx	"CS410 Project -- ExpertSearch System Bruno seo Joseph Angulo Xiaohan Liu Introduction The ExpertSearch system is a webpage-based search engine that provides search function for faculty information. Introduction The search result of faculties will display in the order of relevance below. Another Sample Input Installation It's extremely easy to install and run the code on Mac or Linux On Windows it requires some tricky settings to execute the code. This is because the Python webserver package Gunicorn doesn't support directly on Windows system, so Docker is required to run the code. Installation on Windows Install Git (if you haven't) In the folder you want to save the project, right click -> Git Bash Here -> type: git clone https://github.com/losaohan/CourseProject.git Download and install Docker Desktop for Windows: https://hub.docker.com/editions/community/docker-ce-desktop-windows/ Download and install Pycharm Professional for Windows: https://www.jetbrains.com/pycharm/download/#section=windows You need to apply an JetBrains account with your Illinois email to have access to the professional version: https://www.jetbrains.com/shop/eform/students. Installation on Windows Open Pycharm, open the CourseProject folder as project. On the top-left menu, hit File -> Settings -> Plugins -> search for Docker, and install it -> make sure it's ""Enabled"": Installation on Windows In the Settings -> Build, Execution, Deployment -> Docker, create a new Docker service as follows -> hit OK. Installation on Windows A new Docker service will appear on the bottom left menu. Make sure the Docker Desktop you installed in step 1 is running, and hit the green button ""connect"" on the top left: it will show ""Connected"" in the middle of the bottom window. Installation on Windows In the Project directory, open the file docker/mainapp/Dockerfile. Right click the double green arrow and select ""Edit 'mainapp/Dockerfile'..."". Installation on Windows Fill in the same setting as below in the pop up config edit window. You could make your own Image Tag and Container Name, but make sure the other settings are the same. Installation on Windows Hit Run: the image will start to build, and the container will launch in a few seconds. Installation on Windows In the attached console, you will find the code is running at 0.0.0.0:8095 in the container, which is mapped to 127.0.0.1:8095 in the host machine. If you type localhost:8095 in your browser, the expert search page will show up Installation on Mac/Linux Open Pycharm, open the CourseProject folder as project. Create a new virtual env by choosing your local Python 2.7 as base interpreter. Installation on Mac/Linux In the activated virtual env, Install the requirements.txt packages by typing: pip install -r requirements.txt Installation on Mac/Linux Type gunicorn server:app -b 127.0.0.1:8095 in the terminal and the server will launch. Type localhost:8095 in your browser, the expert search page will show up: Thanks!"
https://github.com/losaohan/CourseProject	CS410 Project Proposal.docx	CS 410 Project Proposal Team members Joseph Angulo (jangulo2) Bruno Seo (sbseo2) Xiaohan Liu (xliu120) - captain/coordinator Project topic Improve the ExpertSearch System: Extracting relevant information from faculty bios If you are adding a function, how will you demonstrate that it works as expected? In this project, we are going to add two functions to extract faculty research interests and top 2 (or 3) keywords in publications. In the search result page, the two new fields will be displayed after the name and address items (see highlighted area below). Briefly describe the datasets, algorithms or techniques you plan to use We are going to reuse the existing data of the search engine, which is part of the codebase in GitHub. We plan to use Named Entity Recognition (NER) algorithm along with other data mining techniques introduced in this course. How will your code communicate with or utilize the system? We are also going to rely the existing code framework: new functions will be added to the ./extraction folder. New results will be merged together with existing search results and displayed on the webpage. Which programming language do you plan to use? Python will be the main language for extraction while Javascript and HTML for visualization. Main tasks and estimated time cost (60~80hrs) Digest the existing data + code + visualization and test how they work and connect (10h) Analyze the dataset for the pattern of the research of interest and publication keywords, and come up with corresponding algorithms (10h) Implement functions to retrieve those fields (15~20h) Train the new text retrieval/mining system and make some improvements (10h) Adjust the web frontend to display the new fields (5~10h) Test the new function as well as the whole system (5~10h) Prepare the complete documentation/report (5~10h)
https://github.com/losaohan/CourseProject	README.md	ExpertSearch To run the code, run the following command from ExpertSearch (tested with Python2.7 on MacOS and Linux): gunicorn server:app -b 127.0.0.1:8095 The site should be available at http://localhost:8095/ For details of installation or running the code on Windows, please refer to the project documentation or presentation.
https://github.com/riyv/CourseProject	Project progress report-converted.pdf	Offensive Language Detection - Project progress report For our project we have decided to divide our entire project work into four basic modules, - View - Extractor - Analyzer - Repository View For view module we have decided on using react / dash combination, we may switch to static html webpage based on our needs, view will be used for two main reason, one from ui we will input what hashtags we want to search, that will be fed into our extractor system. Also, our ui will have separate dashboard, where will have visual elements of different datasets, like tweet counts, their overall sentiment, etc. Extractor This module will be used to get the tweets for a list of hashtags, we have decided to use Java for this module, we have signed up for Twitter developer account, we are using twitter official hbc api for getting our tweets, twitter/hbc: A Java HTTP client for consuming Twitter's realtime Streaming API (github.com) We have been able to complete the coding of this module, and we were able to get tweets from api successfully for particular hashtags, please find code snippets below, PAGE 1 And we are getting outputs like this, Now we are working on cleaning the tweets, so that we can use them directly to our analyzer module without any deformed text Analyzer We have decided on using Python for this module, there will be python script running in background, where we will feed our cleaned tweets, and we will scan for offensive words in that text and mark that tweet accordingly, also we are planning for doing sentiment analysis of the tweets, we are still deciding on that topic. Repository We plan on storing all our analyzed tweets on mongoDB on cloud, so that we use data from our UI component.
https://github.com/riyv/CourseProject	proposal.md	Sentiment and Speech Analysis By: Riya Gupta, Chitra Uppalapati, and Diptam Sarkar What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - riyag3 (captain), chitrau2, diptams2 What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Our free topic is conducting sentiment analysis as well as polite and impolite language detection on a set of around 1,000 of the top tweets from the daily trending hashtags, dynamically. We are trying to categorize the most popular tweets as positive, negative, neutral, polite, or impolite depending on the topic. This is an interesting project because it allows us to sense what the overall attitude and feeling is towards certain ideas when examining the most relevant tweets per hashtag. This can allow us to find certain patterns and sentiments in the trending hashtags, which can help us identify how people feel about popular discussions and products as well as how polarized specific topics on Twitter are. This project will be most helpful for identifying sentiment towards political discussions as well as new products. We plan to use Twitter HBC, the Java HTTP client for accessing the Twitter API, to fetch our 1,000 tweets. Then, we will build a system that classifies the tweets. When analyzing our data, we will experiment with different classifiers and evaluate our system using the standard classification evaluations metrics (Precision, Recall, and F-score). The expected outcome is to display our results on a web app that we will create using React. Which programming language do you plan to use? - Python, Java, Javascript Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: - Scrape 1,000 tweets from daily trending hashtags (20 hours) - Input hashtags from our web app - Clean our data - Query by top number of Retweets - Store in repo (MongoDB database) - Perform sentiment analysis/language detection (positive, negative, neutral, polite, impolite) (20 hours) - Feed tweets into modules for language analysis - Update tweets with language analysis information - Output results on a web app (20 hours) - Show results in graphs - Create a good UI - Add information on how to use the tool/its purpose
https://github.com/riyv/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Arnavj3/CourseProject	CS 410 Project Proposal.pdf	"CS 410 Project Proposal Song Retrieval using Sentiment Analysis The Group The group consists of Arnav Jain (Arnavj3) and Nikhil Sahni (Sahni4). The captain of the group will be Arnav Jain. Our Free Topic We plan on scraping the web for song lyrics and building an informational retrieval system. We will conduct a sentiment analysis of these lyrics and classify the songs. The user will then be able to query our tool with a combination of parameters like song sentiment, artist, genre, etc. We think this is incredibly interesting as it introduces a novel and useful way of searching for music. An example query for our tool would be ""I want a happy country song by ____ artist"" Our planned approach is to first gather the data and clean it into a processable format. We will then work on building a program to conduct sentiment analysis on the gathered lyrics. Once we have a feature set for each song's lyrics, we can start building our search engine that will allow the user to use our tool. We plan on using NLTK for sentiment analysis and text processing tasks. We will scrape and gather data using python. We are still unsure of the data storage tools we will use and plan on making that decision once we start the process of collecting data. This way we will be able to make a more informed decision based on the nature of the data we will work with. The expected outcome is a working multi-feature search engine for songs. To evaluate our search performance, two useful metrics will be precision and recall. We think a combination of analytical and human oversight will help us evaluate our tool. Programming Language We will primarily be using Python for this project. We will be using multiple packages such as NLTK, MeTAPy and Selenium to perform the sentiment analysis, create the search engine and scrape the web Workload We expect this project to take us anywhere from 50-60 hours to complete. The division of time is expected to be as follows: Phase 1: The first phase of this project is to scrape the web to create a dataset of as many songs and lyrics as we can. This would take us 10-15 hours in total. Phase 2: The second phase consists of creating the search engine to be able to give accurate recommendations of songs only based on the lyrics searched. This would take us 15 - 20 hours Phase 3: The third phase consists of adding sentiment analysis to the search engine so that users could search for genres and moods and accordingly get song recommendations. This would take us 15-20 hours."
https://github.com/Arnavj3/CourseProject	CS410ProgressReport.pdf	"Project Progress Report Progress So far, we have managed to set-up our data retrieval pipeline. We first mine billboard's website to scrape song names from their top 100 charts for a range of years. We use these song names and query Genius.com's API to obtain the lyrics for the song. We then cleaned these lyrics for processing by removing stop words and non-useful information. This way the data is in a consistent universal format throughout. We've combined all the meta-data we have on these songs with the lyrics and consolidated it into a dataframe. We are currently working on our sentiment analysis model to assign each song a sentiment value. Remaining Tasks We have part of the sentiment analysis on the lyrics remaining. We will then add this sentiment value as an additional feature in our data frame for each song. Once we are done with this, we need to create the search/recommendation engine to recommend songs based on the user's query. Challenges Faced One of the biggest challenges we faced while collecting the lyrics was that we weren't able to find any ready datasets online for lyrics. Most were bag-of-word representations or very limited. We decided to scrape the top 100 billboard website to get all top 100 songs and then we used those song names to query the ""Genius"" API to retrieve lyrics for each of the songs. This way we achieved more flexibility and customization in our retrieval."
https://github.com/Arnavj3/CourseProject	README.md	SmartLyrics Song Search System Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. To run the code follow the following steps: - Unzip the provided CSV file and upload it to your google drive - open the notebook in google drive and run the cell which reads the CSV (This will require you to mount your drive and should not take more than a few seconds) - run the cell that reads the unnormalized dataset and run all the cells that follow - the last cell will have the following search fields: artist, album, year, sentiment, profanity and lyric. You can use these search fields to find a song you're looking for. The following video explains all the different steps taken to create this project - https://www.youtube.com/watch?v=briL4ZidYhE
https://github.com/garciadiazjaime/CourseProject	progress-report.md	Progress Report CS-410 / Fall 2020 Which tasks have been completed? The following tasks marked with an x are in progress; they haven't been completed yet but a good progress has been made. [x] Instagram crawler [x] Food classifier Which tasks are pending? The following task are pending, not much progress has been made here. [ ] Food API [ ] Web application (Demo) [ ] Documentation Are you facing any challenges? Instagram crawler It took some time to build the Instagram crawler but a stable version is not in place. Food classifier Giving a supervised approached is been used, it has been time consuming traning the classifier, but good progress has been made so far.
https://github.com/garciadiazjaime/CourseProject	proposal.md	Project Proposal CS-410 / Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. jaimeg4 (Individual) What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic (Free): Chicago Food Instagram Crawler. Description: Provide an easy way to find up-to-date food options for the Chicago area using Instagram as the source. Task: Instagram has a public API that provides access to recent the posts, and with the help of the hashtags we can determine if it's a post related to food; if that's the case a crawler will extract the coordinates used for the location and a classifier will try to guess the food category, finally the information will be saved into a database and exposed throught a REST API. Important: At the moment there are a couple of alternatives like Ubereats or Yelp, however sometimes their data is outdated or their pictures are of poor quality; so the importance of the project is to provide up-to-date information and quality pictures using the Instagram community. Approach & Tools: To mention some of the components needed: a) Instagram crawler (Nodejs script). b) Food classifier (Tensorflow.js). c) Food API (Express nodejs). d) Web application (Svelte Javascript Framework). Outcome: An interactive web application that will show Chicago food options and a way to filter them by categories (classifiers). Evaluation: The project will be evaluated by the progress of the web application, which will be a Proof-of-Concept.
https://github.com/garciadiazjaime/CourseProject	README.md	Demo What to eat in Chicago? CourseProject Chicago Food Instagram Crawler. Proposal Progress Report Documentation Presentation Demo Contact details Twitter @jaumint
https://github.com/sitajothi/CourseProject	Documentation.pdf	"An overview of the function of the code Our project aims to analyze stock sentiment from Twitter data to help our users understand whether or not to buy or sell stocks. Understanding sentiment from tweets is very helpful as it provides textual context to the market outlook of different stocks. We started by running a pre-existing sentiment analyzer, Vader, to have a baseline performance. Then, we did research into different methods of sentiment analysis to build our own sentiment analyzer, which we trained on a kaggle dataset, and later tested on the Twitter API using recent tweets. We wanted our user to easily understand the sentiment of a stock's set of tweets, so we added a pie graph as a visualization for user's to make the stock purchasing decisions. How software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. We can consider the code to be in two different portions: creating and training the sentiment analyzer and actually running it with the tweets pulled from the Twitter API. For our sentiment analyzer, we used knowledge from class to create a sentiment analyzer that utilizes TF-IDF and a Bag-Of-Words model. From MP1, we realized that tokenizing and lemmatizing the words and then using n-gram on words would be effective. We tried many tutorials to understand how to use SpaCy, but utilized https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ the most as it incorporated TF-IDF and the Bag-Of-Words model. While testing the model, we also played around with the n-gram_range that was fed into the BOW-vector and found that unigrams gave the best results. Figure 1: Model Results We used a kaggle dataset to train our model, which performed above the Vader benchmark. We ended up using Logistic Regression because it gave the best result compared to the other models on Scikit-Learn. We then used the Twitter API to pull the most recent tweets from Twitter pertaining to a certain stock ticker (input as '$tickerName'), run it through the model, and then output a pie chart that shows the user the percentage of tweets that had a positive and negative connotation. Based on this chart, the user can then make a judgement as to whether or not they want to short or long the stock. In order to test the overall precision/recall/F1 of the entire program, we established the ground truths of the tweets that were pulled by ourselves and then calculated the metrics. Figure 2: Performance metrics of entire program Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Firstly, the user must have access to Jupyter notebook. There is a browser version, which the user can find with a google search for ""Jupyter notebook browser."" Our application also uses the Twitter API. To be able to receive real time information, the user must create a developer portal on Twitter and gain Access Tokens and Keys. Additionally, the user must save their credentials in a json format to a file called: twitter_credentials.json so that the application can pull them. (e.g. {""CONSUMER_KEY"": """", ""CONSUMER_SECRET"": """", ""ACCESS_TOKEN"": """", ""ACCESS_SECRET"": """"}) Once these two steps are complete, the user can proceed to the last cell of the Jupyter notebook to change the ticker name. The ticker name must be in a string format with a preceding '$'. To run our application, the user can click on Cell -> Run All, and they will see a pie graph with the sentiment distribution corresponding to that stock (an example can be seen below). Figure 3: Example of Sentiment Distribution of Tesla Brief description of contribution of each team member in case of a multi-person team. Both members of the team had a very nice and collaborative experience with this project. In the first few weeks, both Asha and Sita researched different methods of sentiment analysis and tried to find tutorials that would be most helpful for them while creating it. Once the research phase was over, Asha mainly worked with coding the first part of the project as well as finding the benchmark metrics with Vader while Sita worked with the Twitter and graphing portion as she had a Twitter account. The full breakdown of tasks can be found below. Task Asha Sita Researching about sentiment analysis/analyzer tutorials yes yes Vader benchmark with Kaggle dataset yes Coding sentiment analyzer yes yes Fine-tuning sentiment analyzer yes Research about pulling tweets yes yes Coding Real-Time Twitter Pull yes Coding Data Visualization yes Creating ground truths for testing/accuracy purposes with the Twitter results yes yes"
https://github.com/sitajothi/CourseProject	Project Progress Report.pdf	Progress made thus far: So far, we have completed what we believe to be the hardest part of the project - creating and training the sentiment analyzer and the testing model. We first ran the Vader SentimentAnalyzer on a Kaggle Dataset to use as a benchmark. Its F1 score came out to be 0.40, so we wanted to make sure that the analyzer that we create beats this score. When we used a limit of 200 out of the +5000 columns in the Kaggle Dataset, its F1 score was 0.475. We have tried to run it on the entire set and got an F1 score of 0.54! We probably will not be running it on a dataset that big again since it took a very long time, and crashed multiple times before a successful run. Remaining tasks: * Connect to Twitter API to test model on real-time data * Figure out how to filter tweets based on a stock ticker that the user inputs * Run the test model created on these tweets and find if the sentiment regarding the particular stock is positive or negative and if the user should buy/sell respectively * Verify the output of the model (will have to establish ground truths ourselves) Challenges/issues (being) faced: * Training/testing Kaggle Dataset was very large, causing the program to crash multiple times * Precision/Recall/F1_score are all turning out to be the same number on each run. We need to figure out if this is accurate.
https://github.com/sitajothi/CourseProject	Project Proposal .pdf	Project coordinator: Sita Jothishankar (slj2@illinois.edu), Asha Agrawal (ashaa2@illinois.edu) What is your free topic? Twitter sentiment analysis for stock price direction. Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? What is the function of the tool and its output? This tool will scrape Twitter for a stock ticker name and then perform sentiment analysis to see how much positive and negative news there is regarding the term. Who will benefit from such a tool? Our main audience will be banks, investors, and shareholders. After finding the ratio of positive/negative news, they can incorporate this data into their views to make more robust and preemptive decisions for different commodities, stocks, companies, etc. This can help investors either buy low - sell high, or mitigate losses early on. Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? We were not able to find an actual tool that found the sentiments, but found a study that showed that there is a correlation between between tweets and stock performance: https://towardsdatascience.com/stock-prediction-using-twitter-e432b35e14bd and http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnal ysis.pdf. With this supporting study, we believe that our project has a potential to make an impact. What existing resources can you use? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py and https://github.com/satishrath185/Movie-Review-Sentiment-Analysis/blob/master/Sentiment%20A nalysis.ipynb can give us a launching pad for a sentiment analyzer. We will be adding to it to create a more robust analyzer. What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We will use cross validation to create a sentiment analyzer on a dataset from kaggle: https://www.kaggle.com/yash612/stockmarket-sentiment-dataset Once we have created the analyzer we will use the twitter API to perform sentiment analysis on current tweets to provide users with the most up to date information. As mentioned above, we will be using existing resources and studies to help us figure out which algorithms would be best to use. How will you demonstrate the usefulness of your tool? We will want to show correlation between stock price and the sentiment of news stories. With this correlation, we will be able to demonstrate the usefulness as the analyzer will provide our audience a preemptive measure for their stocks. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. There are 2 students on this team, so we will work on this project for 40 hours. We first have to research what algorithm we plan to use to create the analyzer and then we have to implement the algorithm. Once this is done, we have to test and train our analyzer. After this, we will use our analyzer to perform sentiment analysis on current tweets and create a user interface for the user to interact with the analyzer for most up to date information. Each part of the project (listed below in the rough timeline) will be approximately 10 hours. A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) Part 1: Oct 26 - Nov 2 Research algorithms and learn more about sentiment analysis Part 2: Nov 2 - Nov 16 Create and train sentiment analyzer Part 3: Nov 16 - Nov 25 Perform sentiment analysis on current tweets using our sentiment analyzer and twitter API Part 4: Nov 25 - Dec 9 Create user interface for user to interact with analyzer + write out documentation of our project
https://github.com/sitajothi/CourseProject	README.md	"Overview of our Stock Sentiment Analyzer Our project aims to analyze stock sentiment from Twitter data to help our users understand whether or not to buy or sell stocks. Understanding sentiment from tweets is very helpful as it provides textual context to the market outlook of different stocks. We started by running a pre-existing sentiment analyzer, Vader, to have a baseline performance. Then, we did research into different methods of sentiment analysis to build our own sentiment analyzer, which we trained on a kaggle dataset, and later tested on the Twitter API using recent tweets. We wanted our user to easily understand the sentiment of a stock's set of tweets, so we added a pie graph as a visualization for user's to make the stock purchasing decisions. Stock Sentiment Analyzer Setup and Usage Firstly, the user must have access to Jupyter notebook. There is a browser version, which the user can find with a google search for ""Jupyter notebook browser."" Our application also uses the Twitter API. To be able to receive real time information, the user must create a developer portal on Twitter and gain Access Tokens and Keys. Additionally, the user must save their credentials in a json format to a file called: twitter_credentials.json so that the application can pull them. (e.g. {""CONSUMER_KEY"": """", ""CONSUMER_SECRET"": """", ""ACCESS_TOKEN"": """", ""ACCESS_SECRET"": """"}) Once these two steps are complete, the user can proceed to the last cell of the Jupyter notebook to change the ticker name. The ticker name must be in a string format with a preceding '$'. To run our application, the user can click on Cell -> Run All, and they will see a pie graph with the sentiment distribution corresponding to that stock."
https://github.com/mrente5/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	CS410-TextInformationSystem-ProjectProgressReport.pdf	"CS 410: Text Information System Expert Search System Project Progress Report Team Name: BayToBay Team Captain: Pushpit Saxena (netid: pushpit2) 1) Which tasks have been completed? - Design and architecture of the proposed modules of the project. - Extract and Crawl Web Pages - Utilized scrapy framework with Python to crawl web pages and identify connected links. - Utilized BeautifulSoup toolkit with Python to extract text from web pages. - Removed special characters using regular expressions and extracted text data from web pages. - Expert Bios page classification - PreProcessing Data - Using NLTK Python toolkit - Removed stop words - Stemming - XGBoost Text Classifier - Tf-Idf vectorizer Name Net-Id Govindan Kutty Menon gvmenon2 Harikrishna Bojja hbojja2 Pushpit Saxena pushpit2 - Assumes that tokens are stemmed and lowercase - Remove stopwords - Used the dataset from here as positive text and regular web pages as negative text. - Logistic Regression Classifier - Tf-Idf vectorizer - Assumes that tokens are stemmed and lowercase - Remove stopwords - Used the dataset from here as positive text and regular web pages as negative text. 2) Which tasks are pending? - Extract and Crawl Web Pages - Increase scalability to crawl and extract data from web page with higher data content and connected links - Multi-thread extract and crawl scripts to achieve parallel processing - Utilize a delimiter to segregate page identifier and content information - Redesign process to accept input web pages from a file and make the process more configurable - Validation of script for different University web pages - Expert Bios page classification - PreProcessing Data - Writing unit tests - Bi-LSTM text classification (using tensorflow/pytorch on Google Colab) - Use the data set from here - Compare accuracy against other classifiers - XGBoost Classifier - With word2Vec vectorizer (Glove) - Assumes that tokens are stemmed and lower case - Remove stopwords - Used the dataset from here as positive text and regular web pages as negative text. - (Stretch goal) Topic modelling on expert bios. - Integration - Integrate different components which are being developed individually - Integrate with existing functionality of the expert search system - End to end script execution/ validation - Run scripts (in order) and validate conformance to the need/ original design. - Submission/ Demo delivery - Finalize and create deliverables for project submission - Finalize and create deliverables for demo 3) Are you facing any challenges? - Extracting and crawling web pages using lower powered CPUs and less memory on personal machines is posing a challenge from performance and scalability perspective. - Extracting/ crawling web pages of different universities using a scrapy framework requires understanding of the page structure. Page structure could be different for different web pages and this is a challenge for crawling and scraping required contents. - While we were able to collect positive training set for classifiers, collecting ""quality"" negative data set could be tricky - Collected positive training set from CS410-MP2 - Trying to use general web crawled data for negative examples."
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	FinalPresentation.pdf	"* * * * * * * * * * 1) * * * * * * * * * * * * * * * * * * * * * * * 1) * * * * * * * * * * * * * * * * 1) * * * ""distilbert-base-nli-mean-to kens"" * Encoding saved at: ""BERT_encoding_classifier.be rt-embeddings-for-classifica tion.pkl"" * *     * - * - * - * * * [(0, '0.019*""graphics"" + 0.018*""paper"" + 0.015*""image"" + 0.014*""siggraph"" + 0.010*""computer""'), (1, '0.033*""conference"" + 0.027*""international"" + 0.018*""systems"" + 0.014*""proceedings"" + 0.013*""networks""'), (2, '0.015*""translation"" + 0.012*""speech"" + 0.010*""blandford"" + 0.010*""rogers"" + 0.009*""nadia""'), (3, '0.022*""research"" + 0.019*""function"" + 0.019*""study"" + 0.017*""details"" + 0.014*""state""'), (4, '0.014*""programming"" + 0.013*""system"" + 0.011*""architecture"" + 0.011*""software"" + 0.010*""memory""'), (5, '0.017*""electrical"" + 0.014*""engineering"" + 0.012*""power"" + 0.009*""control"" + 0.009*""signal""'), (6, '0.015*""learning"" + 0.008*""conference"" + 0.008*""machine"" + 0.007*""model"" + 0.007*""paper""'), (7, '0.030*""research"" + 0.026*""university"" + 0.025*""computer"" + 0.024*""science"" + 0.018*""engineering""'), (8, '0.016*""theory"" + 0.012*""algorithms"" + 0.011*""algorithm"" + 0.009*""graph"" + 0.008*""complexity""'), (9, '0.008*""guohong"" + 0.008*""ghosh"" + 0.008*""patrick"" + 0.008*""veeravalli"" + 0.008*""thomas""')] * * * * * * * *  * * * * * * *  *"
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	FinalProjectDocumentation.pdf	"Project Documentation: Web Scraping 1. Scrapy Framework - a. Utilized scrapy framework with Python to crawl web pages and identify connected links. 2. Beautiful Soup - a. Utilized BeautifulSoup toolkit with Python to extract text from web pages. b. Removed special characters using regular expressions and extracted text data from web pages. Classification: 1. Once the web pages are scraped using the 'Crawl-n-Extract' module (see run help below). The web pages text is saved in a raw text file with each web page text on one line (similar to MP2 exercise). 2. The classification task is to classify a given webpage as a Faculty Page (positive class: 1) or Non Faculty Page (negative class: 0). 3. We trained multiple classifiers as part of this exercise and results (F1-score and accuracy) can be found in our presentation. 4. We have built most of the model based on the text classification techniques that we have learnt as part of the course. We have mainly used Tf-Idf vectorizer and Bert based sen2Vec to vectorize the text of the web pages. * Logistic Regression 1. Used Scikit learn Logistic regression module as well as scikit learn tf-idf vectorizer for vectorizing the web pages. We have 2. Used nltk library to remove stopwords and for stemming. 3. F1-Score (hold out test set): 0.9702970297029703 * XGBoost 1. Used XGBoost library for training the model 2. Scikit learn tf-idf vectorizer for vectorizing the web pages 3. F1-Score (hold out test set): 0.9829683698296837 * Deep learning 1. Tensorflow - Used tensorflow to build the deep-learning model. a. Four layered neural network model(excluding input layer) b. Uses Adam optimizer with learning 0.0001 c. Loss is evaluated using Binary Cross Entropy d. F1 Score on test data set: 0.9963 2. NLTK - Utilized NLTK to pre-process the data. Pre-processing involved following list of steps a. Remove stop words b. Stemming * (Experimental) BERT encoding based Logistic Regression 1. Generated the embeddings for the web pages text using a pre-trained BERT model ('distilbert-base-nli-mean-tokens') 2. Used the transformer library to generate the embeddings 3. Built an experimental Logistic regression model a. F1 Score for this experimental model is: 0.9874055415617129 4. As, we can see there is an improvement over the Tf-IDF based Logistic Regression model, hence this experiment was successful. But we were getting better results with our custom neural network model, we didn't pursue this further. In future, we would like to expand on this idea of transferring knowledge (from BERT encoding) to inform and improve simple classifiers. This saves a lot of time as the pre-trained models are generally trained on large corpus of wikipedia/web pages and will provide a decent knowledge base to transfer to a simple classifier model which then can be trained much easily in a short span of time. Topic Modeling on Faculty bios  Used gensim library to generate the topic model based on the compiled bios  Running information can be found in 'Run Help' section NER Model  Used spacy library to extract named entities, in particular faculty names and different organizations that are mentioned on their page. Both these entities will help in improving the search index. Run help: Python setup (python >= 3.5, only web scraping needs python 3.5, other modules work with newer versions of python also ): Please run : pip install -r source_code/requirements.txt You can also use the venv by using following commands: * cd source_code * python -m venv <venv-name> * source <venv-name>/bin/activate * python3 -m pip install -r requirements.txt Web Scrapping: 1. Scrapy (Use Python 3.5 Version): a. Script created using Scrapy framework will extract links for the web-page b. Create a scrapy project using i. ""Scrapy startproject link_extractor c. Configure items.py to create class ""LinkExtactorItem"" class and define below fields for the item i. url_from ii. urk_to d. Create a new spider using below command i. Scrapy genspider uiuc e. Crawl links of a web-page using below command i. scrapy crawl uiuc -o links.csv -t csv It will extract all the links associated with web page to csv file. 2. Split (Use Python 3.5 Version) a. Split the input file into multiple files using script ""split.py"" 3. Extract web contents (Use Python 3.5 Version) a. Extract web contents using script ""python extract.py uiuc1.csv uiuc1.txt"". This script uses Beautiful soup as the toolkit 4. Merge (Use Python 3.5 Version) a. Merge all output files using script ""merge.py"" Web Page Classification Logistic Regression Model (python >= 3.7):  Training: * Navigate to source_code/logistic_regression * Run python train.py * The model file will be saved at source_code/logistic_regression/logit.model  Inference: * There are two ways to run inference once: * By providing a file with all the webpage (one str per web page on one line): * From root directory (TextInformationSystem-CourseProject) run following command: python -m source_code.logistic_regression.inference <webpage data file path> * For file format look at: TextInformationSystem-CourseProject/source _code/Crawl-n-Extract/Merge/UIUC.txt * By FacultyClassifier module (which is an entry point for all of our classification models): * Create an object of FacultyClassifier class with classifier_type = 'logit' and then run predict method on list of web pages text * A sample run is defined in the ""__main__"" block of that module. XGBoost Model (python >= 3.7):  Training: * Navigate to source_code/XGboost * Run python train.py * The model file will be saved at source_code/XGboost/xgb.model  Inference: * There are two ways to run inference once: * By providing a file with all the webpage (one str per web page on one line): * From root directory (TextInformationSystem-CourseProject) run following command: python -m source_code.XGboost.inference <webpage data file path> * For file format look at: TextInformationSystem-CourseProject/source _code/Crawl-n-Extract/Merge/UIUC.txt * By FacultyClassifier module (which is an entry point for all of our classification models): * Create an object of FacultyClassifier class with classifier_type = 'xgb' and then run predict method on list of web pages text * A sample run is defined in the ""__main__"" block of that module. Neural Network Model (python >= 3.7):  Predict Faculty Pages (Inference): Trained neural network model is of size 187MB on disk. Git has an upper cap of 100MB. For this reason, the trained model has been uploaded to box. Please download the trained neural network model from the box link below Step 1: Download the folder 'neural_network_model_v2' from the link below. https://uofi.box.com/v/es-neural-network-model-v2 Step 2: Ensure that directories 'model', 'vectorizer' are placed directly under 'fully_trained_model'. Step 3: Ensure below directories are safely extracted. 'fully_trained_model/model' 'fully_trained_model/vectorizer' Step 4: Point 'crawled_data_path' in the script 'classify/infer_crawled_data.py' point to your webpage dump. Note: The crawled data should have one doc per line. Separated by '#####' (Eg: ~/source_code/Crawl-n-Extract/Merge/UIUC.txt) Step 5: Run the script 'infer_crawled_data' and observe the faculty links printed in the console. Note: You can also use source_code/faculty_page_classifier/faculty_page_classifier/FacultyClassifier to programmatically use this model to run predictions. You need to initialize the FacultyClassifier with classifier_type='nn'. But this also requires copying over the model file from the location provided in Step 1 and saving it to the appropriate folder.  Training the model: We suggest that you use the model we have already trained. Please follow the steps below if you intend to train a new model using the source code. * Use the script ""model_driver.py"" under the package below to train the model ""source_code.neural_network_ml_classifier.train"" * Initialize the path to training data and target location a. data_base_dir - base directory to training data. This data must have been preprocessed using PreProcessor.py b. The script then splits the data into 70% train and 30% validation dataset c. This data used then used to train the four layered Neural network model Topic Modeling on Faculty bios  Used gensim library to generate the topic model  Topic model can be run using the following notebook (this notebook contains self explanatory documentation to run the notebook): * source_code/topic_modeller/TopicModelling.ipynb NER Model  Used spacy library to extract named entities, in particular faculty names and different organizations that are mentioned on their page. Both these entities will help in improving the search index.  NER Model can run using following notebook (this notebook contains self explanatory documentation to run the notebook): * source_code/ner_model/spacy_ner.ipynb  Also, extracted faculty names as well as different organizations mentioned on the faculty pages. These two data points will help in indexing the faculty bios for searching. * Commands to run this step: * cd source_code/ner_model * python extract_names_and_orgs.py <bios_dir> <destination_dir_for_names> <destination_dir_for_orgs> * E.g. bios_dir = source_code/data/compiled_bios, destination_dir_for_names = source_code/data/compiled_bios_names, destination_dir_for_orgs = source_code/data/compiled_bios_orgs  Names are extracted and saved in source_code/data/compiled_bios_names/  Organizations are extracted and saved in source_code/data/compiled_bios_orgs  We have used the same filenames as faculty bios for easier retrieval when this information is needed later References:  Scrapy - https://docs.scrapy.org/en/latest/  Beautiful Soup - https://pypi.org/project/beautifulsoup4/  Beautiful Soup - https://www.kite.com/python/docs/bs4.BeautifulSoup  Scikit Learn - https://scikit-learn.org * Logistic Regression - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Logistic Regression.html * TFIDf vectorizer- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.te xt.TfidfVectorizer.html  Gensim - https://pypi.org/project/gensim/ * https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/  XGBoost - https://xgboost.readthedocs.io/en/latest/python/python_api.html * https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit -learn/  Spacy - https://spacy.io/ , https://spacy.io/models/en#en_core_web_md  Tensorflow - https://www.tensorflow.org/api_docs/python/tf/all_symbols * https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	README.md	CourseProject Project Presentation Document: Final Presentation Document Project Documentation and run help: Documentation Video Presentation: Presentation If you face any issue viewing the video at the above link you can also download the video from here Project Source Code: SourceCode Team Members' Email Ids (Please feel free to reach out for any questions or clarifications): Pushpit Saxena (pushpit2@illinois.edu) Govindan Menon (gvmenon2@illinois.edu) Harikrishna Bojja (hbojja2@illinois.edu)
https://github.com/pushpit-UIUC-courses/TextInformationSystem-CourseProject	Text Information System - Project Proposal.pdf	"CS-410 Text Information Systems: Final Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name Net-Id Govindan Kutty Menon gvmenon2 Harikrishna Bojja hbojja2 Pushpit Saxena pushpit2 Team Name: BayToBay Team Captain: Pushpit Saxena (netid: pushpit2) 2. What system have you chosen? Which subtopic(s) under the system? System ExpertSearch System Subtopics 1. Automatically crawling faculty webpages 2. Extracting relevant information from faculty bios 3. Stretch Goals: Topic Modeling on faculty bios. 3. Briefly describe the datasets, algorithms or techniques you plan to use Techniques for Subtopic: Automatically crawling faculty: 1) Users provide University URLs as input. 2) The websites (URLs) will be leveraged to crawl and collect the dataset for classifying ""faculty directory pages"" v/s ""non-faculty"". 3) To collect the dataset for classifying ""faculty webpage"" v/s ""non-faculty webpage"", the faculty directory pages will be crawled. Classification task: 1) Classify ""faculty directory pages"" v/s ""non-faculty"" 2) Classify ""faculty webpage"" v/s ""non-faculty webpage"" 3) We will try to build different classification models. Some of the models that we are planning to try and evaluate are SVM, XGBoost, DL (hugging face transformers) etc. We will leverage the URLs as well as text on the web-pages to extract features (vectorize text) to train the classification models. Extracting relevant information from faculty bios: 1) Enhance Regular-expression to extract email-id from the bios 2) Enhance Named Entity Recognition (NER) to identify/ extract faculty name from bios. 3) Topic mining & keyword extraction on faculty bios information (stretch goal, if time permits). 4) We are planning to use Spacy, Gensim as well as Flair (BERT) for both NER models as well as topic modelling. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Automatic Crawler: 1) We will provide an API/Console utility which can take an university home URL and then crawl the web-pages from that URL and return the list of faculty web-pages. This will demonstrate that the crawler and classifier that we have built are working. Web-pages classification tasks: 1) We will use some of the data from MP2.2 and MP2.3 assignment as suggested in the project topics document and demonstrate the performance of our models (F1 metric). Extracting relevant information from bios pages: 1) As we are planning to use some recent and advanced NER models, we will show the difference in performance between regex based email/name extraction vs our NER model and also we will clearly state whether we are able to improve performance from a simple regex based approach or not. Topic modelling: 1) We will demonstrate the top topics we can identify from the faculty bios dataset. Also, we will try to calculate topic coherence metric. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 1) We will build our own system and will try to use the datasets provided by the existing ExpertSearch system (e.g. faculty bios, names, emails etc.) for training and evaluation. 2) We will also use the regex based NER model as a baseline to compare some of the more advanced NER models that we will try to train. 6. Which programming language do you plan to use? Programming Language Python, Javascript, HTML 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Number Main tasks Hours 1 Evaluation/ Analysis of the current system/ process and any baseline models already implemented in the system. 12 hrs. 1.1 Any required training data annotation 6 hrs. 2 Researching algorithms and overall system design 10 hrs. 3 Development of functionalities envisioned 3.1 Recursive Crawler Implementation 10 hrs 3.2 Web Page Classification Model Implementation 10 hrs 3.3 Information Extraction 8 hrs 3.4 Topic Modeling 8 hrs 4 Self-evaluation and modification 24 hrs 5 Demo Preparation and Documentation 6 hrs. 6 Collaboration 3 hrs. Total 96 hrs."
https://github.com/PSUlion16/CourseProject	CS 410 Project Topics.pdf	"CS 410 Project Topics Overview We will use Microsoft CMT to manage course project grading. Each student should please create an account there using their illinois email ids. After deciding your topics, each student should please enter their details in this sign-up sheet. Carefully enter the information used while registering in CMT into the first few columns (this information will be used for grading, so please be careful! ). Then, enter your group name (could be anything) and project topic. Only the group leader needs to enter the project topic. However, every student needs to enter all other details. This needs to be completed before the proposal submission, i.e. before Oct, 25 to facilitate grading. Multiple groups can choose the same topic. Feel free to coordinate with other groups working on the same topic. For example, different groups can work on separate sub-tasks to increase the project-scope and overall contribution. For the course project topics, we provide five broad categories of options for you: 1. You can choose to reproduce the model and results in a published paper. We provide some papers below. If you choose one of those papers, your project proposal is almost certain to get ""approved""*. 2. You can choose to improve over a current system by adding a function that is relevant to this course. We provide some systems and candidate functions to add below. If you choose among those systems and functions, your project proposal is almost certain to get ""approved""*. 3. It is possible that you work on other papers or systems that are not listed by us. 4. You can choose to join a text classification competition, or an information retrieval competition. If you choose this option, your project proposal is almost certain to get ""approved""*. 5. You can freely propose a topic relevant to this course. * For all the categories, the instructors will carefully review your project proposals and provide feedback. If we find your project topic/plan has some limitations, we will provide suggestions to improve it or suggest you to pick one of the sample topics. You're allowed to change topics after the proposal stage based on our feedback. More detailed information about each option is given below. Option 1: Reproducing A Paper You can choose to reproduce one of the following papers from one of the following subtopics: * Subtopic: Latent aspect rating analysis * Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 * Subtopic: Pattern annotation * Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 * Subtopic: Contextual text mining * ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 * Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture model for contextual text mining. In Proceedings of the 12th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 649-655. DOI=10.1145/1150402.1150482 * Qiaozhu Mei, Chao Liu, Hang Su, and ChengXiang Zhai. 2006. A probabilistic approach to spatiotemporal theme pattern mining on weblogs. In Proceedings of the 15th international conference on World Wide Web (WWW 2006). ACM, New York, NY, USA, 533-542. DOI=10.1145/1135777.1135857 * Subtopic: Causal topic modeling * Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 All these papers are discussed in the lectures of Week 12. Once you have chosen one of these papers, please provide clear answers to the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. Which paper have you chosen? 3. Which programming language do you plan to use? 4. Can you obtain the datasets used in the paper for evaluation? 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? At the final stage of your project, you need to deliver the following: * Your documented source code and main results. * A demo that shows your code can actually run on the test dataset and generate the desired results. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. * Discuss how your results match or mismatch those reported in the original paper. Your results should cover all the main aspects and datasets discussed in the paper. * If some of your results do not match the paper, discuss possible reasons and solutions. Option 2: Improving A System You may choose to improve a system or service. We provide some candidates below. Depending on your group size and the complexity of the techniques you develop, you may choose to work on one or several sub-topics provided per system. Again, it should take you at least 20*N hours, where N is the total number of students in your team. 2.1 MeTA Toolkit By now, you should all be familiar with the MeTA toolkit and its Python library Metapy. You can also refer to the publication. Choose this option if you wish to contribute to it as stated below. * Enhance MeTA and Metapy usability Many of you have experienced difficulties while using Metapy in assignments this semester. You now have a chance to improve it so that future students and researchers can use this useful resource easily! Some ideas for improvement are given below: * Make Metapy compatible with the latest Python versions and different OS systems * Integrate it with existing popular toolkits e.g. NLTK, gensim * Enhance available tutorials for installing and using the tool on different platforms * Add text mining functions to the MeTA toolkit The aim of this subtopic is to add some existing text mining algorithms to MeTA. Note that the papers mentioned below are all discussed in the lectures of Week 12 but cannot go to Option 1 because there are online GitHub repositories that implement them. If you want to borrow some code snippets from some repositories, make sure the licenses of those repositories allow you to do so, and you should follow the instructions in the licenses. If a repository does not have a license file, according to GitHub, the default copyright laws apply, meaning that the authors retain all rights to their source code and no one may reproduce, distribute, or create derivative works from their work. That means you CANNOT use codes from GitHub repositories without a proper license unless you obtain explicit written permission from the authors. * Latent aspect rating analysis, given by the following paper # Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903 * Topic modeling with network regularization, given by the following paper # Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengXiang Zhai. 2008. Topic modeling with network regularization. In Proceedings of the 17th international conference on World Wide Web (WWW 2008). ACM, New York, NY, USA, 101-110. DOI=10.1145/1367497.1367512 2.2 ExpertSearch System The ExpertSearch system (http://timan102.cs.illinois.edu/expertsearch//) was developed by some previous CS410 students as part of their course project! The system aims to find faculty specializing in the given research areas. The underlying data and ranker currently comes from the MP2 submissions of the previous course offering. You can read more about it here (Sections 3.6 and 4: Project are especially relevant). The code is available here. Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms from scratch. * Automatically crawling faculty webpages Recall that you developed scrapers for faculty web-pages in MP2.1, which, in general, can be a time-consuming task. So, the question is can we automate this process? Some challenges include: * Identifying faculty directory pages: First, we need to identify the pages from where faculty web-pages can be mined. In MP2.1, we used faculty directory pages as the starting point to find faculty webpages. So, given a university website, can we automatically identify the directory pages? This can be posed as a classification task, i.e. classify a URL into a directory page vs. non-directory page. We have a huge resource of directory page URLs available in the sign-up sheet. These can be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs(e.g. other URLs on the university websites, product websites, news sites,etc.). These would be the ""negative"" examples. * Identifying faculty webpage URLs: Next, we need to extract the faculty webpages from the directory pages. This can again be posed as a classification task. Given a URL, can we identify whether it is a faculty webpage or not? We have a huge resource of faculty webpage URLs (available under MP2.3 on Coursera). These would be the ""positive"" examples. You can get a list of some random URLs online or crawl some other pages to get URLs (e.g. other URLs on the university websites, product websites, news sites, etc.) to get the ""negative"" labels. * Extracting relevant information from faculty bios: The problem here is to convert the unstructured text in faculty webpages into more structured text. Such structured information would enhance the utility of the system. For example, in the ExpertSearch system, emails and faculty names extracted from bios are shown in the search results. Users can click on the ""mail"" button to directly mail the faculty. Extraction is done using regex-based techniques and Named Entity Recognition (NER) that don't always work well. Can you improve those existing techniques? You can also develop techniques for extracting other information, e.g. faculty research interests. For example, you may perform topic mining  on the bios available under MP2.3 on Coursera. The top-keywords per topic could be the common research areas. You might also perform keyword extraction  from faculty bios, research papers, etc. 2.3 EducationalWeb System The EducationalWeb system (http://timan102.cs.illinois.edu/explanation//slide/cs-410/0) is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and recommend relevant slides for each slide. You can read more about this in the following papers Web of Slides, WOS Demo.; 2) Find an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""cap/scholar"" button on the top-right of a slide. It will try to retrieve a relevant section from the Professor's textbook that contains an explanation of the selected phrase. You can read more about the underlying algorithm here. The code for the system is available here. Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms. * Improving the usability and reach of the existing system Some of you might have used the system and identified potential areas of improvement. The aim of this subtopic is to refine the current version of EducationWeb. Some specific ideas include (many are borrowed from this Piazza post): 1. Scale up the current system. Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. Again, it might be useful to think about automatic crawling similar to the subtopic in 2.2 above. It would be very interesting to see the interaction between slides/textbooks at a large scale!! 2. Improve the performance of the system. Currently, loading each slide takes time. 3. Allow downloading slides in bulk. Currently, we can only download one slide at a time. 4. Add more context to the explanations (e.g. link to the specific page in the textbook) 5. Allow adding additional courses/lectures directly from the web interface. This would also involve dynamically identifying the recommended/relevant slides for a new slide. Currently, a static file is used which contains pre-computed recommendations for each slide. 6. Integrate the tool with Piazza/Coursera, i.e. maybe link Piazza/Coursera to the tool or vice-versa. Alternatively, add discussion forum and video capabilities to the tool so that it serves as a one-stop-shop for all users' educational needs. 7. Link to latest related research articles: In this way, the lecture content can be automatically updated 8. You could also work on improving the current recommendation, search and explanation mining algorithms (described in the papers at the beginning of this section 2.3) * Automatically creating teaching material for in-demand skills This subtopic is an extended version of the existing EducationWeb system. There is an increasing demand for skilled workers in the industry. Quality education is not easily accessible to everyone due to barriers such as high cost, geographical and language barriers, etc. Also, instructors cannot be available 24*7 to provide personalized support to all learners. In this subtopic, the overarching aim is to tackle some of these issues. In particular, the following tasks might be good starting points. * Identifying in-demand skills: You can crawl and analyze relevant sections of job boards, news articles, scientific articles, social media, etc. to automatically identify the emerging keywords /topics. For this, you may refer to some papers on contextual text mining (mentioned in Option 1 of this document). * Creating lectures and tutorials for those skills: For this, you may consider lecture slides (e.g. from Coursera courses) as the basic units of knowledge. Then, the task could be to find the most relevant slides or clusters of slides (could be across multiple courses/lectures) for a given skill (topic). You may borrow some ideas from the EducationWeb system for this. You may also use the slides in existing lectures on some topics as the ""relevant slides"" for those topics. In this way, you can automatically generate training data for supervised learning. You could also combine knowledge from multiple sources (e.g. textbook sections, slides, videos, blogs, codebases) for creating more comprehensive tutorials. A more challenging task would be to automatically generate  the lectures/tutorials using techniques from natural language generation and abstractive text summarization. Another interesting idea is to automatically generate agents , e.g. using Virtual Agent Interaction Framework (VAIF). This goes beyond the material covered in class but could lead to some highly innovative and state-of-the-art projects! If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What system have you chosen? Which subtopic(s) under the system? 3. Briefly describe the datasets, algorithms or techniques you plan to use 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 6. Which programming language do you plan to use? 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Option 3: Working on Other Papers or Systems It is also possible that you work on other papers or systems that are not listed by us. Because we do not list them, we will need more information from your proposal to decide whether you have a good topic. You may find more guidelines below. 3.1 Reproducing an unlisted paper If you choose to reproduce a paper not listed under Option 1, please make sure that your chosen paper satisfies the following criteria: 1. The paper should solve one of the research challenges introduced in lectures. 2. The paper should be published at a trustable venue (i.e. conference or journal). Some examples include ACM SIGIR, KDD, EMNLP, ACL, Learning @ Scale, EDM, etc. If you can find Cheng's paper(s) at some venue, then that venue is likely to be trustable. 3. There should be NO publicly available implementation for this paper. For example, if the main method in a paper is already released on GitHub or built into a library, you cannot choose that paper. 4. The main method in the paper should be advanced enough so that the work of reproducing it likely takes at least 20*N hours, where N is the total number of students in your team. You may justify this by listing the main tasks to be completed and the estimated time cost for each task. In your proposal, please explain how your chosen paper satisfies the above criteria, and also answer the questions listed under Option 1. Other requirements are the same as Option 1. 3.2 Improving over a paper You can choose to improve over a paper that is relevant to one of the tasks introduced in the lectures. If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What paper have you chosen? 3. What is your idea for improving the paper/system? Why do you think your idea will hopefully work better? 4. How are you going to evaluate your idea? What are the datasets and baseline methods? 5. Which programming language do you plan to use? 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code and main results. * A demo that shows your code can actually run on the test dataset and generate the desired results. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. * If your idea works, discuss what advantages it has over the original paper, and what possible limitations are still there in your method. If your idea does not work, don't worry - just discuss possible causes and potential solutions, and you will get your credits as long as your study is solid and your discussion is thorough. 3.3 Adding an unlisted function to a listed system You can choose to improve one of the systems listed under Option 2 by adding a function that is not listed there but relevant to the course content. If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What system have you chosen? What function are you adding? How will the new function benefit the users? 3. How will you demonstrate that the new function works as expected? 4. How will your code communicate with or utilize the system? 5. Which programming language do you plan to use? 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code. Explain how your code communicates with or utilize the system. * A demo that shows your implementation actually works. 3.4 Improving over an unlisted system You can choose to improve a system or service that is relevant to the course content but not listed under Option 2. You may either add a new function to the system or improve the performance of an existing function. If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What system have you chosen? Are you adding a function or improving a function? What function? 3. If you are adding a function, why is the new function important or interesting? How will it benefit the users? If you are improving a function, what are the main limitations of the current function? How are you going to improve it? How will your improvements benefit the users? 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5. How will your code communicate with or utilize the system? 6. Which programming language do you plan to use? 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code. Explain how your code communicates with or utilize the system. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why. Option 4: Competitions This option may fit you well if you would like to gain some experience in state-of-the-art text classification or information retrieval practices. You will need to research by yourselves some cutting-edge models that are more recent than those introduced in the lectures. Of course your TAs will be there for you when you need help. Performance is the most important factor. Once you achieve the state-of-the-art performance, we give you a bonus (10% extra credit) that can be used to cover any loss of points in the project caused by small mistakes. We hope you can have fun in learning and trying recent methods in this option. We are thinking of hosting an information retrieval (IR) competition and a text classification competition. You will have access to the text and labels of a training dataset and the text of the test dataset, but you cannot see the labels of the test set. You will then develop a text classifier or a document ranker and submit your test set predictions. We will automatically evaluate your results on the test dataset and release the test set performance of all participating teams on a leaderboard (the setup would be similar to MP2.4). The text classification competition is available here. It is also available on LiveDataLab along with the baseline scores. The IR competition is available here. It is also available on LiveDataLab along with the baseline scores. You're allowed to use pre-built machine learning packages. However, if you find someone's solutions (source code) to similar competitions online, you may not use them directly. You're free to borrow some of their ideas with proper citations and credit attribution. You can also use publicly available external datasets but please make sure they don't overlap with the test sets. In each competition, you will compete with a competitive baseline and your classmates. Your grade will largely depend on your test set performance. Recall that in your project grading, there is 45% on ""source code submission"". (You can find the project grade composition in Week 1.) Assuming that there is no issue with your submitted source code, that 45% will be graded with the following criteria based on your test set performance on the leaderboard: * If you outperform the baseline, you get all 45%, plus 10% extra credit to make it up for you if you lose points in other parts of the project. The extra credit will not make you earn more than 100% for your project, and cannot be applied to other parts of this course. * If you do not outperform the baseline but make a valid submission, you get 15% + 30% * (1 - (r-1)/N), where r is your rank on the leaderboard and N is the total number of teams that have chosen this option. If there is an issue with your source code (e.g. using others' code without properly handled copyright, obvious bugs, etc.), it is possible that you get a lower score than those listed above. In your project proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. Which competition do you plan to join? 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods 4. Which programming language do you plan to use? At the final stage of your project, you need to deliver the following: * Your documented source code and test set predictions. * Explain your model, and how you perform the training. Describe your experiments with other methods that you may have tried and any hyperparameter tuning. * A demo that shows your code can actually run on the test set and generate your submitted predictions. You don't need to run the training process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand. Option 5: Free Topics You may freely propose a topic that is not listed in this document but relevant to this course. In your proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? 3. Which programming language do you plan to use? 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code and main results. * Self-evaluation. Have you completed what you have planned? Have you got the expected outcome? If not, discuss why. * A demo that shows your code can actually run and generate the desired results. If there is a training process involved, you don't need to show that process during the demo. If your code takes too long to run, try to optimize it, or write some intermediate results (e.g. inverted index, trained model parameters, etc.) to disk beforehand."
https://github.com/PSUlion16/CourseProject	project_proposal_ctoombs2.pdf	CS410 Project Proposal Name: Chris Toombs NetID: ctoombs2@illinois.edu Names and NetIDs of Members: For the scope of this project, I will be completing this text classification competition individually. My name and NetID is listed at the top of this page for reference. Intended Competition: I intend to join the text classification competition, which is based around a sarcasm detector based on twitter posts. Details of project: For the scope of this competition, I intend to complete my code using Python, as I mainly focus on Python at my current workplace and due purely to the ease of coding with Python. I am very interested in this competition as there are direct parallels with my job at General Motors in the field of text classification. In my current role, we have a need for classification of Vehicle Repair Verbatems, so that we can more easily determine root cause in an automated fashion. This will be a related project which I can use in my day-to-day job. I am excited to use Neural Network classification techniques, and will be focusing mainly on Word2vec and Glove (although I am not sure which I will use yet). A few of my coworkers at work have used these, and python has a few libraries created to leverage, such as gensim. I will make it clear which approach I will take in my project documentation. I do not have prior experience with these frameworks however there is a robust set of documentation online which I can leverage.
https://github.com/PSUlion16/CourseProject	project_status_report_chris_toombs.pdf	CS410 Project Status Report Name: Chris Toombs NetID: ctoombs2@illinois.edu Current Progress: For this project, I am working on the twitter sarcasm detector as part of the text classification project. I have successfully created a python project with git integration to my project repository. I have verified that the repository connects successfully to Live Data Lab. In terms of implementation of the project, I was going to use word2vec, but I am looking at utilizing gensim's doc2vec along with scikit-learn for classification. The reason for this is that doc2vec will allow me to retain semantics, whereas word2vec does not. I have successfully loaded in both the train and test data sets in my programs and have preprocessed the words using the NLTK package and it's associated stop words. I am not going to perform stemming at this time, but I am adding some custom stop words (such as @USER) to clean up the documents (i.e. tweets) Remaining Tasks: I will need to make documentation on how to run the program, as well as fully comment my code. To be completed, I need to instantiate my documents with word2vec and implement the logistic classifier. I also want to clean up my pre-processing step, so I will be working on that in the next couple of days. Challenges: I've never used Gensim before, so I will need to read up on that, but there seems to be a lot of content online regarding. I will be getting married this week, so I will not be completing this project (most likely) until next week, so I am going to try and get as much work done as possible from 11/29-12/1. So far, nothing blocking me from finishing this on time.
https://github.com/PSUlion16/CourseProject	README.md	CTOOMBS CS410 Course Project Documentation Description of this repository: answer.txt - Classified answers based off the test.jsonl document provided to team classify.py - final code for the Doc2Vec Twitter Sarcasm model CS410_Classification_Demo LINK!!! - 20 Minute Demonstration of project, code, issues - https://mediaspace.illinois.edu/media/t/1_fltka8gr project_proposal_ctoombs2.pdf - proposal for project project_status_report_chris_toombs.pdf - Status report from November in regards to project progress README.md - Full description and documentation for code TwitterSarcasmModel.d2v - Doc2Vec model from successful submission of project To Run the classifier: FROM PYCHARM: - You can load in the Project Repo AS-IS and run the code directly without modification - Most environments will already have NLTK and GENSIM included. If you get an error, just import those libraries to your environment FROM CLI: - Navigate to local folder for course project repo - type classifier.py NOTE: THIS ASSUMES YOU HAVE PYTHON INSTALLED ON YOUR COMPUTER Description of Code My classify utilizes both GENSIM and NLTK to classify the test data into SARCASTIC or NON_SARCASTIC tweets. I was able to use the TwitterTokenizer to parse the REPONSE fields of the data into Tokenized lists, with Handles removed, words moved to lowercase, and repeated characters shortened to one. A brief description of the code methods: read_inputs() -- Reads in both TRAIN and TEST Files and tokenizes into lists initialize_doc2vec() -- Initializes the DOC2VEC Model based off the TaggedDocument objects in the TRAIN list get_labels() -- Determines the labels for the TEST dataset based off the DOC2VEC model saved in initialize_doc2vec() This method uses the INFER_VECTOR method of doc2vec to vectorize the test tweets output_results() -- Outputs the data to answer.txt Difficulties The most difficult part of this project was the hyperparameter tuning. Much of the parameters had vague documentation and it seemed like use cases for parameter values varied greatlywith different users online. I probably spent 10-12 hours or so toying around with the parameters adding / removing items and observing behavior before landing on the set that worked for me. In the future, it may be more useful to find a more recent model with a larger user base, as it seems doc2vec is not as widely used (the git repo has not had a commit since 2018). I was confortable with the implementation of this as I have used word2vec in previous projects. References I saw a ton of forums for where people were discussing use cases, but the main sources I pulled from were the following: stackoverflow.com https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/doc2vec.py https://www.nltk.org/ https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb
https://github.com/anilkpalli/CourseProject	Project_progress.pdf	Project: Text classification competition Progress: I was able to achieve an accuracy of 0.77 with my initial implementation which includes pre- trained language model along with pytorch neural network framework for fine-tuning the model to fit to the sarcastic dataset. However I forgot to set the seed values due to which I couldn't reproduce the results although the output predictions were saved. With a specific seed set, I am able to reproduce the output predictions with accuracy ~0.765, almost close to my max. I have used Google-Colab free version for training the model so it comes with its own limitations. I am kind of struck on achieving maximum accuracy of 0.77 (currently (as on 25- Nov) best on the leaderboard) on the hidden dataset, although the models performance is good enough (~0.86) on the hold-out validation set created from the overall training dataset. I will try to use few more derived heuristic variables along with pre-trained models to see if that helps in improving the performance a bit more. Remaining tasks:  Need to do a thorough documentation of the code  Create tutorial presentation as part of project deliverables Challenges/Issues:  Limited GPU memory is allocated in Google-Colab free version. Layering the pre-trained models with any more slightly complex or deep neural layers is resulting in out of memory issues. Had to settle with shallow layers.  Struck within a performance range of 0.7 to 0.77 with the approach listed above. Need to come up with a different approach to improve the performance while working with limited memory
https://github.com/anilkpalli/CourseProject	Project_proposal.docx	Name: ANIL KUMAR PALLI Net ID: anilkp2 Email: anilkp2@illinois.edu Project Topic: Text Classification Competition Programming Language: Python Prior Experience: Most of my real-time work experience has been on classification problems but never got to work with text classification. This would be a great opportunity and learning experience while researching and participating in the competition. In this process I would to explore more on the neural network classifiers based on Convolution and Recurrent Neural Networks preferably using Keras framework. Although my main interest lies in exploring the latest and leading language models like BERT, Open AI's GPT etc... that are being discussed a lot in the NLP community recently.
https://github.com/anilkpalli/CourseProject	README.md	Objective: The objective of this task is to detect sarcasm in tweets. Datasets: Train: 5000 records (Columns: response, context, and label) Test: 1000 records (Columns: response, context) Tools Used: Google colab notebook (free version) Language: Python Libraries: torch, sklearn, transformers (ver=3.0.0), numpy, pandas, json, time Approach: RoBERTa (developed as an extension of BERT) pre-trained model is used as the starting point. Further the model is fine-tuned using Sarcasm train dataset and the final optimal model is used for predicting the labels on Sarcasm test dataset. The code is split into 6 sections. In order to get the final predictions the code needs to be just executed in sequence as described below. Section: 1 - Read input data The train and test json files are stored in google drive and are directly imported into google colab by providing appropriate credentials. Refer to this short tutorial for importing data into colab Section: 2 - Preprocess data The input data (train) consists of 3 columns: response, context and label. In this step the response and context are combined into one single sentence in the order of last to first conversation sequence. Further analysis is done using this single sentence approach which captures both response and context as one complete conversation. Note: * Tried out using response alone without context and it didn't perform better than using response and context together * Tried out data cleaning steps like removing stop words, special characters, urls etc.. but they didn't prove to be any useful in improving the accuracy while using this approach so removed those steps from final code Section: 3 - Prepare data for modeling Split the data into training and validation: 80% of the data from train set is used for training the model and 20% of train dataset is hold-out for validation purpose. Tokenize and encode sequences of both training and validation sets: The conversations are of varying lengths and therefore have to be truncated and padded to equal lengths. Based on the distribution of lengths, max length is selected as 200 since most of the conversations are covered within this range. Any higher number could possibly cause model to train slower or run out of memory due to colab limitations in its free version Create train and validation tensor datasets: Tensor datasets are created to work efficiently with torch framework while building the model Section: 4 - Define model build functions Define functions to initialize the pre-trained roberta-base model and fine-tune the parameters as needed to fit the data in hand. In-line documentation of these functions is available in the code. Note: * For the per-trained model, tried out bert-base-uncased, bert-large-uncased, roberta-base, roberta-large. Among these the large variants ran out of memory quickly and among the base variants roberta-base performed better, so retained it in the final submission * Make sure to set appropriate seed values to reproduce the results Section: 5 - Build the model Define model parameters: Includes defining optimizer, loss functions, number of epochs and batch size. Used Adam optimizer and a suitable learning rate to tune roberta-base for 10 epochs. Negative Log-likelihood loss (alternatively cross-entropy loss) is used as the loss function. Run the model and store the best model: The model is iterated for each epoch while optimizing the parameters. During training, the model parameters are evaluated against the validation set. Saved the model each time the validation accuracy increases so that the model with the highest validation accuracy is identified and stored. The train and validation metrics (loss and accuracy) are captured and stored for all the epochs. Section: 6 - Test predictions Load the best model - Load the model having the highest validation accuracy that is stored in prior step for predicting the test set Prepare and manipulate test data - The test set should undergo same data preprocessing and preparation steps as the training which includes: combining response and context into one sentence, tokenize and encode test conversations Split the test dataset into 2 parts to overcome limited space issue in colab - There are 1800 records in test set. Due to the space limitations in colab where majority was already utilized for loading and tuning pre-trained RoBERTa, the test set was split into 2 sets of 1000 and 800 records respectively. Get the predictions for the 2 test sets and combine into one final dataframe - The test sets are scored using the best model and the predictions are combined into one single dataframe and then to csv to submit the result in appropriate format
https://github.com/acscharf/CourseProject	Progress report.pdf	"Alexander Scharf CS410 (Fall 2020) Course Project Background: After learners complete an online business course, they are prompted to enter a ""reflection"" on how they can apply the knowledge from the course to their job or daily life. These reflections are shared with other learners so they can deepen their understanding, learning how others applied their learning. This project aims to: 1.) to analyze ""useful"" and ""not useful"" reflections, finding syntactic elements that make up each 2.) gather user input for a user reflection and predict whether that reflection is ""useful"" or ""not useful"" via a web application The project uses a real data set from an online learning service with reflections in both English and Japanese language. 1.) Progress made thus far - The data set has been extracted for both English and Japanese text - The labeling of the data set (""useful"" or ""not useful"" is being done manually and took longer than expected just for English, but is nearly complete - Decided to use spAcy as framework and researched its usage as part of tech review - Ran preliminary analysis on data set (number of words, parts of speech, common words) for both ""useful"" and ""not useful"" reflections - Coding for training model complete by referencing spAcy sample code - Preliminary web app created and hosted on personal server (take a look at http://alexscharf.com/) 2.) Remaining tasks - Complete labeling the English data set - Begin labeling the Japanese data set - Make adjustments to the code as needed for the Japanese data set - Make the web app slightly more user friendly - Conduct user tests - Update documentation and clean up code to remove debugging 3.) Any challenges/issues being faced - I thought I could outsource labeling the training data with Amazon Mechanical Turk, but labeling it required too much domain knowledge - taking more time than I thought - Unrelated to the direct goal of this project, but fiddling with the public facing web server took longer than expected since I don't have experience in this area. Ended up switching from Apache to Nginx - Labeling the Japanese data set will take more time than expected. I plan labeling a train data size of 1000 reflections for English, I may do half of that for Japanese - Still a bit unknown how much rewriting the original application will be necessary for Japanese or any other strange bugs like character encoding, especially with web app"
https://github.com/acscharf/CourseProject	Proposal.pdf	"Alex Scharf Project Proposal, CS410 Free Topic - English and Japanese Course Reflection Analysis 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Individual project, Alex Scharf (acscharf2) 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? I am the product owner of GLOBIS Unlimited (GLOBIS Manabihodai) in Japanese, an online course platform focused on business content with currently over 200,000 learners. After learners complete a course on the platform, they are prompted to enter an optional reflection about how they can apply the learning from the course to their job or life: Some of the responses are quite good and well-thought out (""I am an engineer, and I can use logic trees to help me create test cases"") and others are not as insightful (""Logic trees are good""). After learners post their reflections, they can view the reflections of other learners and ""Like"" ones they found useful. As learners see the reflections of others, it is important that the reflections be of high quality, and hence useful to a wider audience. Because the service is in English and Japanese, data exists for both Japanese and English reflections. I would like to do the following: A.) Identify language and parts of speech that are common in reflections for both English and Japanese B.) Create an application that predicts whether an input reflection is ""high quality"" or ""low quality"" for English only C.) Host that application on a public web server This task is important and interesting for the following reasons: - Real-world challenge based upon actual data set - Creates a proof of concept for a feature that would give learners feedback about the quality of their course reflection, leading to great personalization in online learning - Creating a user-facing application that potential users could interact with - Working across two language (Japanese and English) that are very different My current plan is as follows: 1.) Extract data set for both English and Japanese 2.) Choose the most appropriate way to score the data set (manual approach or use ""likes"") 3.) Find tool that can analyze and make predictions for both English and Japanese 4.) Run analysis of common words in reflections to satisfy point A 5.) Train predictive model on current data set 6.) Create application that allows user input and compares against predictive model 7.) Host application on a publicly accessible web server 8.) Run user test with 4 people to see if their experience is in line with expectations I expect that the application should be able to identify ""good"" and ""bad"" reflections to a certain degree. I plan on evaluating my work if the test users enter a ""good"" reflection and a ""bad"" reflection and the application is in line with their expectations. Here is what I imagine the application flow to look like: 1.) User is prompted for reflection input and submits 2.) System checks user input against trained data and decides whether it is ""high quality"" or ""low quality"" 3.) Feedback is sent back to the user 3. Which programming language do you plan to use? I plan on using Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. - Research tool that is appropriate for both Japanese and English (2 hours) - Extract data set and prepare in proper format (3 hours) - Score the data set in appropriate format (3 hours) - Create application to run semantic analysis of reflections (5 hours) - Create application to train predictive model (5 hours) - Create front-end application that allows users to input data against predictive model (5 hours) - Host front-end application on publicly accessible web server (2 hours) - Prepare for user test (1 hour) - 4 user tests x 30 minutes (2 hours) - Create full documentation (2 hours) - Summarize findings (2 hours) Total: 32 hours"
https://github.com/acscharf/CourseProject	README.md	"English and Japanese Course Reflection Analysis and Prediction About After learners complete a video-based online business course, they are prompted to enter a ""reflection"" on how they can apply the knowledge from the course to their job or daily life. These reflections are shared with other learners so they can deepen their understanding, learning how others applied their learning. This project aims to: 1.) to analyze ""useful"" and ""not useful"" reflections, finding syntactic elements that make up each, and create a trained model (train.py) 2.) gather user input for a user reflection and predict whether that reflection is ""useful"" or ""not useful"" based upon trained model (webapp.py) Both 1.) and 2.) are done for both English and Japanese language. Project Presentation https://www.youtube.com/watch?v=JN-Gm5Pj-hs Try It A live version of the software is hosted below, complete with sample videos. Try a ""useful"" and ""not useful"" reflection and see if it matches your expectations. The sample reflections will give you an idea of what might be considered ""useful"" and ""not useful."" English http://alexscharf.com/ Japanese http://alexscharf.com/ja train.py (Training Application) Overview Reads labeled CSV for reflection data, analyzes reflections, trains a model, and saves that model to disk. Analysis looks at parts of speech (by percentage), common words, and average word counts for both ""useful"" and ""not useful"" reflections. Implementation The application has three key functions, explained below: read_csv(filename, rows) Opens a csv with the name of ""filename"" and reads the first number rows specified by ""rows."" The CSV should have two columns, the first with a label of '1' if the reflection is 'useful' or '0' if it is ""not useful."" Strips whitespace and returns a pandas DataFrame. analyze_reflections(reflections, nlp, language) Analyze reflections when provided with a pandas DataFrame, spaCy NLP object, a string to display for output. Iterates over each unigram for both useful and not useful reflections, counting parts of speech, common words, and average length while ignoring whitespace. Outputs the result using the print function. Example output: https://github.com/acscharf/CourseProject/blob/main/example_output.txt train_reflections(reflections, nlp, n_iter, n_texts): Trains model based upon label reflections data with a pandas DataFrame, spaCy NLP object, and the number of iterations and items in the reflection data. Holds 20% of the labeled data for evaluation, training off of the remaining 80%. Prints loss, recall, precision, and f-score for each training iteration. Currently build using the ""simple_cnn"" architecture provided by spaCy. Usage The application requires the spaCy and pandas libaries as well as the ""en_core_web_sm"" and ""ja_core_news_sm"" spaCy models. Additionally, the software needs the english.csv and japanese.csv labeled reflection datasets in a ""data"" subfolder. These data sets were labeled ""useful"" or ""not useful"" by me and reflect actual user output. The data set for this project can be found in the below repository: English https://github.com/acscharf/CourseProject/blob/main/data/english.csv Japanese https://github.com/acscharf/CourseProject/blob/main/data/japanese.csv After completion, the program saves a model to disk in the ""english_model"" and ""japanese_model"" subfolders. Assuming the provided csv files are included, the program can be run as-is with no additional parameters. webapp.py (User-facing Web Application) Overview Flask-based web application that loads training model and gathers uset input to predict usefulness of reflection. English version can be accessed at the main directory (/), while Japanese version can be accessed via a subdirectory (ja). Implementation The program is implemented with Flask, mixing Python and HTML. There are two pages, and submission page and a results page, both in English and Japanese. The submission page is pure HTML and Javascript. The results page takes the submission from the previous page a parameters, loads a spaCy model created by the train.py application, and runs the user submission against the trained text classifier to guess whether the submission is ""useful"" or ""not useful."" This output is displayed to the user, along with some generic hints for a useful reflection inferreed from analyzing reflections through the training application. Usage The application requires the spaCy and flask libraries as well as a training modeled generated by train.py. The app can be launched with the following commands: export FLASK_APP=webapp.py flask run This will start a development server on http://127.0.0.1:5000/. A working version can be found at http://alexscharf.com/ Other files Proposal.pdf Project proposal Progress report.pdf Mid-term progress report example_output.txt Example output of train.py analysis waitress_server.py Configuration file for production web server Self-evaluation Have you completed what you have planned? I was able to complete complete all the planned outcomes as mentioned in the original project proposal. In fact, I went beyond the project proposal by including generating a training model and a web front-end for Japanese as well as English. I initially did not include this in the original proposal because I was not sure of my ability to correctly label the Japanese data, but I found a subset of the data (for an accounting course) that allowed me to do so, and hence exceeded the original project proposal. Have you got the expected outcome? The outcome for the reflection predicter is as expected. As originally proposed, I conducted user tests to see if the program functions to their expectations. Their feedback was as follows: - The application is very good at filtering out obviously bad reflections (""The course was interesting""). This is very useful, as these low-quality responses have the largest user impact - The application can still be ""tricked"" by writing grammatically correct and keyword packed sentences that ultimately have little meaning (""I love studying business and applying business for my presentations. It helps me succeed at work with my boss and also with my coworkers."" gets a perfect score). This is not intended to be a grading mechanism, however, so tackling these it outside the scope of the project. The outcome for the reflection analyzer was also insightful, but not as much as expected. As expected, better reflections tend to have more words (around 22 on average, compared with 9 for not useful ones). However, the parts of speech and common words were quite similar for ""useful"" and ""not useful"" reflections, suggesting that to do a heuristic analysis of reflections, much deeper insight is needed and training a model is a much more effective approach, justifying the original project."
https://github.com/vkreiden/CourseProject	ProgressReport.pdf	Progress Report: Applying ML to log analysis for anomalies detection Progress made so far: * Deepening knowledge in the area. The technology review in the related area as well as some video-courses on NLP and Leveraging NLP and Word Embeddings in ML. * Dependencies for the project. Python libraries: spaCy (text parsing), pandas, numpy (data manipulation), gensim (word2vec embedding), sklearn (ML models, validations, ...) * Installing and configuring dev env. * Breaking down the tasks and creating the backlog. Backlog: * (in background) Data acquisition. Work to obtain real log files * Skeleton. Build a skeleton of the steps w/stubs (+ some tests) * 1st implementation. Implement word2vec embedding w/small synthetic log files; kFold of the first ML algorithm (TBD); implement perf. reporting (e.g. accuracy, runtime) * Refactor to OOP. Should enable an easy extension at least with a different ML algorithm; (optionally embeddings as well) * Extend the implementation with at least one more model. Ideally should be able to run a comparison of models (details TBD) * Run the tool on the real log files. Run and compare models performance * Documentation. [Optional] * Dockerization. For more convenient tool usage * Non-functional requirements. Look at the performance optimization * Hyperparameters tuning. Challenges so far: * Real data acquisition. If no real data achieved the fallback would be to use a synthetically generated dataset * Env tech issues. Some issue w/scapy corpus load - troubleshooting in progress...
https://github.com/vkreiden/CourseProject	proposal.pdf	Tool: Applying ML to log analysis for anomalies detection Team (C) valeryk2 (Valery Kreidenko) What is the function of the tool The tool is a thematic continuation of the technology review. The tool will allow to train and evaluate performance of different machine learning models on the datasets of the labeled log files. Who will benefit from such a tool SREs, data scientists and researchers who would like to experiment and test different ML models for anomalies detection from the log files. Does this kind of tools already exist Some work has been already done in this area to support the research brought in the technology review though not sure what exactly since the code isn't publicly available. What existing resources can be used We'll use the approach which was researched and suggested by the authors of https://hal.laas.fr/hal-01576291/document. We'll try to acquire some real existing datasets from the real system. What techniques/ algorithms will be used The current plan is: - to use word2vec or other NLP technique(s) to map text data into the ML models consumable data - we'll implement at least 2 supervised ML methods - per performance evaluation o for effectiveness we'll use F-measure and maybe others o for efficiency we'll compute model training and classification times for models comparison - for validation we plan to implement 10-fold or other method - finally, we'll try to apply OOP design patterns to make this tool extensible for easy plugging of more ML models, etc. - implementation language: Python + its ML libraries How the usefulness of the tool will be demonstrated We'll run at least 2 models of the tool on the same data and we'll evaluate and compare the relative performance in terms of the effectiveness and efficiency. We hope to demo this tool on the real dataset, otherwise we'll synthesize some. Timeline This is estimated as a ~3 weeks project.
https://github.com/vkreiden/CourseProject	README.md	"CourseProject This is an #UIUC #MCS_DS Text Information Systems course final project. The developed tool is based on the technology review. Table of contents Functionality Implementation details Usage Usage presentation Functionality This tool provides a functionality of assessing performance as well as performing comparison of supervised learning classifiers when applied to the log files or chunks of log files for anomalies detection. It is mostly based on the approach described in this paper. At high level, the tool receives 2 lists of text files as input - one dataset corresponds to the system in error state (e.g. under stress, performance issues, security incidents, etc.) and the second set corresponds to a benign work of the system. Each file represents one data item (document) in one of the two lists thus the dataset items are prelabeled with either ""fail"" or ""normal"" labels. After the data is read and preprocessed, an NLP method is applied to transform each data item (log file) into a vector space. The received matrix along with labels is fed into each of of the supervised machine learning models to evaluate the performance. The output of the tool run is a scatter chart displaying each tested classifier over 2 dimensions - classification accuracy vs. runtime. The tool is implemented in python (see Implementation details) and runs using the configuration provided in the ini format config file (see Usage). Implementation details General The tool is implemented in python and uses a number of standard python modules for data consumption and processing as well as specialized modules for applying NLP, ML methods and plotting the results as detailed below. Reading the configuration The standard configparser is used to read the following configuration items form the configuration file file: locations of the two sets of files, preprocessing directives on how to optionally drop the ""standard"" prefix, a list of classifiers to assess along with the K-fold test set size parameter. There's also a file format parameter which used when plotted into a file. Datasets and preprocessing The expected datasets are two sets of log files (one corresponding to a normal state of the system and the other one during abnormal work). Each file contains a number of log messages. There are two preprocessing steps which are performed on the input data using standard string manipulation functions: 1 drop a standard prefix of a log message using delimeters 2 remove non-alphanumerical characters NLP To prepare the data for ML algorithms we start with applying the word2vec word embedding to vectorize the words. We train the word2vec model on the vocabulary generated from all the words appearing in the log messages. We use gensim library to build the word2vec model over default number of dimensions (100) of the vector space. To calculate the positions of the files in the vector space we use a centroid approach twice: we first calculate an average postion vector per log message over all the mesage words vectors. Then we find a position of each file in the vector space by calculating the average of all the file's log messages vectors. ML estimators assessment and comparison We use scikit-learn library to build and evaluate ML models. Specifically, we use K-fold validation. When enumerating a list of the fully-qualified class names of the estimators the reflection is used to instantiate an instance of each. We capture accuracy and a model evaluation time measurement for plotting the results at the next step. Results presentation We use a matplotlib module to plot a scatter chart presenting the results of the run. X axe shows the classification accuracy vs. model evaluation runtime on the Y axe. Usage describes two ways of using the tools. When the tool is run on the host OS the chart will be displayed onto the display. This may not be easily achieved if the tool is run in the container. In this case the tool will save the chart into figures folder in the format specified by user in the config. Testing datasets We provide two sample datasets ""perfect split"" and ""mix"". These datasets are mostly synthetic. They were generated from the real httpd error log files in the following way: the notice level messages are separated into the ""notice"" files while error level messages are separated into the ""error"" files. ""Perfect split"" dataset includes strictly separated notice and error files while the ""mix"" contains a ~70%-30% mixes of two kinds. This allows to see the implemented approach works from the perspective of expected accuracy: with the perfect split all the classifiers are 100% accurate while when we use a mixed dataset the accuracy drops below 100%. Perfect split dataset Mix dataset Usage We prepared and tested two running configurations below. Clone this project and cd into it. Docker (recommended) Build docker container: docker build -t ad_app . (On Linux/ Mac) Run: docker run -d --mount type=bind,src=""$(pwd)""/config,target=/config \ --mount type=bind,src=""$(pwd)""/datasets,target=/datasets \ --mount type=bind,src=""$(pwd)""/figures,target=/figures ad_app (On Windows) substitute ""$(pwd)"" in the above command with the current directory path Using pip package manager in virtual environment python -m ensurepip --default-pip python -m venv test_ad_app (On Linux/ Mac): source test_ad_app/bin/activate (On Windows): test_ad_app\Scripts\activate pip install -r requirements.txt Using conda package manager Configure: conda update conda conda install -c anaconda gensim scikit-learn conda install -c conda-forge matplotlib conda update -y smart_open Run: python anomaly_detector.py Usage presentation recording"
https://github.com/sonalsharma5990/CourseProject	CS410 Project Proposal Text Summarization.docx	"PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Sonal Sharma sonals3 (Captain) Maneesh Kumar Singh mksingh4 Kamlesh Chegondi Kamlesh2 2. Which paper have you chosen? Subtopic: Causal topic modeling Paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? We have registered on the LDC site and are awaiting access. TAs are notified on same via Piazza@770 post 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?"
https://github.com/sonalsharma5990/CourseProject	detailed.md	Topic-level Causality Analysis From topic modeling results -> generate topic curve over time topic's coverage on each time unit (e.g. one day) Consider a weighted coverage count. Specifically compute the coverage of a topic in each document p(topic_j|document_d) => theta_j Estimate the coverage of topic j at t_i t_c_i_j as sum of thera_j over all documents with t_i timestamp. t_c_i_j = Sum(theta_j) TS_j => list of t_c_j for all the timestamps creates topic stream time series that, combined with non-textual time series data lends itself to standard time series causality measures C and testing. Select lag value is important? How => chose lag with highest significance Word-level causality and Prior Generation Chose topics with highest causality scores and further analyze the words withing each topic to generate topic proors For each word, generate a word count stream W S_w by counting frequencies in the input document collection for each day: w_c_i = Sum c(w,d) Measure correlation and significance between word streams and external non-textual time series. Then wemeasure correlations and significance between word streams andthe external non-textdual time series. This identifys words that aresignificantly correlated and their impact values.
https://github.com/sonalsharma5990/CourseProject	Final_Project_Proposal.pdf	"PROJECT PROPOSAL 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Sonal Sharma sonals3 (Captain) Maneesh Kumar Singh mksingh4 Kamlesh Chegondi Kamlesh2 2. Which paper have you chosen? Subtopic: Causal topic modeling Paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? We have registered on the LDC site and are awaiting access. TAs are notified on same via Piazza@770 post 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?"
https://github.com/sonalsharma5990/CourseProject	Project Progress Report.pdf	Project Progress Report Team: Best Bots S.No. Task Current Status Remarks/Impediments 1 Access to DataSet by all members Completed 2 Understanding the dataset Completed How to deal with gaps in dataset. Raised @1273 on Piazza 3 Find parallel time series dataset Completed 4 Finding the libraries required for implementation Completed 5 Perform Preprocessing of data 90% Completed 6 Perform LDA on news data set 10% Completed Currently performing LDA on May data 7 Apply Granger Test to determine causality relationship. 75% completed Matrix multiplication not providing expected results. Trying to do this using for loops 9 For each candidate topic apply Causality measure to find most significant causal words among top words in each Topic. 75% completed Matrix multiplication not providing expected results. Trying to do this using for loops 10 Record the impact values of these significance words using Pearson correlations Not Started Currently being done in Granger test. Yet to implement Pearson coefficients. We may not need it. 11 Separate positive impact terms and negative impact terms 75% completed Implementation is done. Testing in progress with actual data. 12 If orientation of words in prior step is very weak, ignore minor group 75% completed Implementation is done. Testing in progress with actual data. 13 Assign prior probabilities proportions according to significance levels 75% completed Implementation is done. Testing in progress. 14 Apply LDA to Documents using prior obtained 10% Completed Research on parameters for gensim LDA. 15 Repeat until satisfying stopping criteria (e.g. reach topic quality at some point, no more significant topic change). Completed In our experiment iteration count is 5 16. Make final project documentation report. Not Started 17. Software usage tutorial presentation Not Started
https://github.com/sonalsharma5990/CourseProject	README.md	"Iterative Topic Modeling Framework with Time Series Feedback Abstract As part of our final project for CS410 Text Information Systems, we are reproducing paper ""Topic Modeling Framework with Time Series Feedback"". We chose to reproduce experiment of 2000 U.S. Presidential election campaign due to its low data size. We followed the steps and algorithm as mentioned in the paper and were able to get results which are very similar to results provided in the paper. With this project, we learnt about Topic Modeling and how topic modeling combined with TimeSeries feedback can be used to explain the relation between text and non-text time series. Authors Team BestBots | Name | NetId | | ------------------- | --------------------- | | Maneesh Kumar Singh | mksingh4@illinois.edu | | Sonal Sharma | sonals3@illinois.edu | | Kamlesh Chegondi | kamlesh2@illinois.edu | Table of Contents Iterative Topic Modeling Framework with Time Series Feedback Abstract Authors Team BestBots Table of Contents Project Video Algorithm Parameters Time series data Collection of documents with ts from same period Topic modeling method M Causality measure C tn mu m Gamma  Delta d Output Steps How to run? Initial Setup Run program DataSet NYT Corpus data IEM 2000 Winner takes all data Data preprocessing NY Times Corpus IEM Winner takes all data Handling of missing data Stop words removal Implementation Hurdles and Ladders Final Results Significant Topics 2000 Presidential Election Quantitative Evaluation Results Conclusion Acknowledgments References Appendix Evaluation System Software/Tools used Project Video https://youtu.be/bP7eKOCasVU Algorithm Parameters Time series data X = x_1, ... , x_n with timestamp (t_1, t_2, ..., t_n) Collection of documents with ts from same period D = {(d1,td1),..,(dm,tdm)} Topic modeling method M Identifies topics Causality measure C Significance measures (e.g. p-value) and impact orientation tn How many topics to model mu m strength of the prior Gamma  Significance Threshold Delta d Impact Threshold Output k potentially causal topics (k<=tn): (T1,L1),... (Tk, Lk) Steps Apply M to D to generate tn topics T1,..,TN Use C to find topics with significance value sig(C,X,T) > gamma(95%) CT: set of candidates causal topics with lags {(tc1, L1),..,(tck,Lk)}. For each candidate topic CT, apply C to find most significant causal words among top words w subset of T. Record the impact values of these significance words (e.g. word-leave Pearson correlations with time series variable) Define a prior on the topic model parameters using significant terms and impact values Separate positive impact terms and negative impact terms If orientation is very weak ( delta < 10%) ignore minor group Assign prior probabilities proportions according to significance levels Apply M to D using prior obtained in step 4. Repeat 2-5 until satisfying stopping criteria (e.g. reach topic quality at some point, no more significant topic change). When the process stops, CT is the output causal topic list. How to run? This program has been tested for python 3.8.5. It may work for older version of python (>=3.6), but was not tested. Here are the instructions to run this program. Initial Setup This program can be run with or without virtual environment setup. However virtual environment is highly recommended. Below are the steps required for Ubuntu 20.04.1 LTS. ```bash install virtualenvwrapper pip3 install virtualenvwrapper --user Setup virtualenvwrapper path if not already done export PATH=""$HOME/.local/bin:$PATH"" If python3 is aliased on your system, you need to setup environment variable for virtualenvwrapper to know where to pick python export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 Source virtualenvwrapper.sh so that all helper commands are added to path source ~/.local/bin/virtualenvwrapper.sh Make virtualenv. demo can be any name. mkvirtualenv demo Clone the repository git clone https://github.com/sonalsharma5990/CourseProject.git install dependencies cd CourseProject/src pip install -r requirements.txt ``` Run program There are three options to run the program. Below command uses the saved model to find significant topics and words. As it does not need to train the model, it is the fastest option and finishes in less than 5 minutes. bash python main.py You can also train the model again. Below command trains the model using the prior strength mu and eta (topic_word_prob) for 5 iterations. Please note currently it is not possible to set the mu, eta or number of iteration options from command line. These must be changed in code. As this option trains the model five times (five iterations). It takes 12 to 15 minutes to run the program. bash python main.py retrain You can run topic causal modeling for various prior strength (mu) and number of clusters (tn), as mentioned in the section 5.2.2 Quantitative evaluation results. This generates graphs between various mu, tn and average causal significance and purity. As this program runs model training for each mu/tn combination five times. This takes 40 minutes to run on a large AWS EC2 instance. ```bash python main.py graph ``` DataSet NYT Corpus data New York Times corpus was provided by TA's based on request by each team member. The dataset due to its huge size and access restriction is not included in this repository. IEM 2000 Winner takes all data The data from May-2000 to Nov-2000 was manually selected from IEM Website IEM 2000 U.S. Presidential Election: Winner-Takes-All Market The data for each month was selected using dropdown and copied to a spreadsheet. After data for all months have been collected, the spreadsheet is saved as an CSV file. Data preprocessing 2000 presidential election campaign experiment required NY Times corpus and IEM Winner Takes all data from May-2000 to Oct-2000. In additional NY Times corpus data is in XML so it was required to parse and extract required component. We wrote pre_processing.py module to take care for both tasks. It performs following tasks NY Times Corpus Using lxml library, extract body of NY corpus dataset between 01-May-2000 to 31-Oct-2000. Check if body contains Gore or Bush, If yes extract only the paragraph containing Bush and Gore. Combine all paragraphs for single article and write it as a single line NY Times output is gzipped to save space. It is stored as data.txt.gz in data/experiment_1 folder. NY Times output file contains each document as line, so line index is treated as document ID Using line index (starts from 0), a document and date mapping file is created. This file is required in order to count documents/words for causal processing. IEM Winner takes all data Using pandas, the IEM Winner takes CSV is read Filter dates between 01-May-2000 to 31-Oct-2000. Using Gore as baseline, the LastPrice column is normalized GorePrice = GorePrice/(GorePrice + BushPrice) Only date and LastPrice column is kept. As this file is small it is kept in memory for whole run. Handling of missing data On analysis of IEM data with NY Times corpus data, we found that IEM data is missing values for two dates 07-Jun-2000, 08-Jun-2000. These dates are filled with values from next (future) available last price which was available for 09-Jun-2000. Stop words removal On running the topic modeling several times we realized the initial and significant topics are dominated by most occurred terms in the corpus. E.g. almost every topic had Bush said with highest probability. We had to either implement some kind of TF-IDF for topic modeling or ignore the common words altogether. After some review we decided to go with removal of these words from the corpus as stop words. names of candidates as they are frequently used e.g. Bush, Gore common political words e.g. president, presidential, campaign parties e.g. republican, democratic states e.g. New York, Florida. These states were home states for the candidates. common verbs and words e.g. said, asked, told, went time words e.g today, yesterday, wednesday Implementation Our goal was to reproduce experiment-1 involving 2000 U.S. Presidential election campaign. We took the various parameter values as mentioned in the paper to find significant topics in causal analysis. The parameters used are Number of topics (Tn) = 30 Prior Strength (m) = 50 Significance Threshold () = 95% (0.95) Delta Threshold (To ignore topic based on impact) (d) = 10% (0.10) Number of iterations = 5 We also tried to analyse our topic modeling results quantitatively. For this we ran our experiment with different values for prior strength m and number of topics Tn. We fixed Tn = 5 during run for testing different m. m values tested: 10, 50, 100, 500, 1000 We fixed prior strength m = 50 to test different values for Tn. Tn values tested: 10, 20, 30, 40 1) Gensim LDA is used for topic modeling. That represents M in the algorithm. eta parameter was used for priors and delay is used for prior. Delay may not be correct parameter for prior strength, but after weighing other options and discussions with students, we decided to use it. @1378 on Piazza. 2) Statsmodel library granger test function. This function allowed us to test correlation with different lag. We tried granger test with up to 5 day delay and took the best lag for both topic and word significance. 3) For impact value, we calculated the average of lagged x coefficients as mentioned in the paper. If the impact was positive, we interpreted as +1 and negative value as -1. As we were only interested in most significant topic and words we didn't get a result with 0 impact. We used 1 - p value as significance score. 4) In order to select Top words for causal analysis, the paper discusses using Mass probability cutoff (ProbM). Its value was not provided. After trying with various values for ProbM we settled on 0.40. This cutoff allowed us to select most important words for causal analysis. Increasing this from 0.40 doesn't give any better results. Hurdles and Ladders 1) Algorithm to implement Topic Modelling: We had a tough call between PLSA and LDA here. MP3 PLSA is heavily un-optimized. It even fails with memory error with experiment-1 document data. (Presidential campaign vs IOWA market), whereas LDA using gensim library uses a lot of inner dependencies and the m step is not as clear(as in lectures) to incorporate Mu.(question @1378 on Piazza) Post feedback and discussion with Professor and Students we used the decay parameter as Mu to implement the paper. 2) Missing data for some dates in Non-text series We have used future value in this case after research.To justify the same, in case of stock data in week 9/11, we would miss the impact in stock if using previous values. 3) Granger Test to determine causality relationship We used 1- p value for score which amounts to almost 0 values getting 100% score. 4) Add customized stop words in data preprocessing We removed words that were not adding any value to topics found. names of candidates as they are frequently used e.g. Bush, Gore political words e.g. president, presidential parties e.g. republican, democratic states e.g. New York, Florida common verbs and words e.g. said, asked, told, went time words e.g today, yesterday, wednesday Final Results Significant Topics 2000 Presidential Election After training the LDA model for several times, we were able to match few important topics of 2000 Presidential election campaign. Tax cut, Healthcare, abortion played a major role during the campaign and we can see our method was able to find some of these major topics. | TOP 3 WORDS IN SIGNIFICANT TOPICS | | --------------------------------- | | tax lazio house | | national know administration | | tax aides advisers | | security social american | | officials aides american | | medicare tax security | | tax federal polls | | national house tax | | abortion american national | | drug american tax | Quantitative Evaluation Results As discussed in the paper, we also tried validating our results with various values of prior strength mu and number of topics (tn) with average causality and purity. In the paper, increasing values of mu and tn causes a mostly upward trend in average causality and and purity. Our reproduction however could not see the same relation. We got the highest causality and purity with the initial values of mu and tn and it mostly remains constant after that. This is something that we would like to further research. Conclusion Using Iterative Topic Modeling with Time Series Feedback (ITMTF) for causal topic mining, we were able to reproduce the paper. We were able to successfully get topics which were prominent in 2000 Presidential Election. We also tried to quantitative evaluate the topic mining results and unlike paper, our results saw little gain in confidence and purity with increasing values of prior strength (mu) and number of topics(tn). As a future task, the quantitative results need to be further researched and the findings can be utilized to improve our algorithm. Acknowledgments We would like to thank Professor ChengXiang Zhai for a great course and guidance to complete this project. We would also like to thank our TAs for always being available for any questions and issues. We are also grateful for fellow students on Piazza and Slack for motivation and fruitful discussion in completing this project. References [Kim et al. 13] Hyun DukKim, MaluCastellanos, MeichunHsu, ChengXiangZhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management(CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 Appendix Evaluation System The algorithm was evaluated on AWS EC2 c5.12xLarge Ubuntu instance. Software/Tools used | Tool | Usage | Version | Link | | -------------- | ------------------------- | ------- | ----------------------------------------------- | | Python | Programming language | 3.8.5 | https://www.python.org/ | | statsmodel | Granger Test | 0.12.1 | https://www.statsmodels.org/stable/index.html | | gensim library | LDA topic modeling | 3.8.3 | https://radimrehurek.com/gensim/ | | Pandas | CSV and data processing | 1.1.5 | https://pandas.pydata.org/ | | NumPy | Array/Matrix Manipulation | 1.19.4 | https://numpy.org/ | | Tabulate | Printing results in table | 0.8.7 | https://github.com/astanin/python-tabulate | | lxml | Parse NY Times xml corpus | 4.6.2 | https://pypi.org/project/lxml/ | | Matplotlib | Draw evaluation plots | 3.3.3 | https://pypi.org/project/matplotlib/ |"
https://github.com/j5un/CourseProject	Documentation.pdf	"Documentation: BERT fine-tuning for Twitter sarcasm detection Name: Junzhe Sun NetID: junzhes2 Team: Junzhe Sun (individual) Competition: text classification competition Overview This project aims to detect sarcasm from Twitter posts. This is a text classification task related to sentiment analysis. The training data set includes text-label pairs, therefore constituting a supervised machine learning problem. I experimented with two different methods, including FastText and BERT. I found BERT with fine-tuning to have the better performance, which achieved a higher f1 score than the baseline. The code is implemented in Python under the Google Colab environment. Two Jupyter notebooks are available, one using fastText and the other using BERT in PyTorch. Both notebooks are self-contained, in that data loading, data cleaning/processing, training and prediction are available in each notebook. The input to both notebooks are a training data set with text-label pairs, and a test data set without labels for prediction. The notebook outputs predicted labels after training the respective neural network using the input data. They can be used for general text classification tasks beyond the binary classification task in this project. Implementation details Model Two models were tested for the text classification competition. I first tried fastText, a shallow-learning library that can be used for both word embedding and text classification. FastText is a close sibling to Word2Vec, and was introduced by Bojanowski et al. (2017) from Facebook AI Research. FastText is a method that aims to extend word vectors to capture subword information using character n-grams, instead of representing each word in the vocabulary as distinct vectors. This concept can be extended to the bag of word (BoW) representation of texts using latent vectors of words and word n-grams, which leads to a simple yet efficient baseline method for text classification (Joulin et al., 2016). The second model is a transformer network known as BERT, which stands for Bidirectional Encoder Representations from Transformers and is introduced by Devlin et al. (2018) from Google AI Language. The transformers is a type of deep neural network that's based on attention mechanism (Vaswani et al., 2017), and has recently gained a lot of popularity in the NLP community. The BERT model is designed to pre-train deep bidirectional representations of language models using unlabeled text. Bidirectional training means that representations are trained by jointly conditioning on both left and right context in all layers of the deep network. Consequently, a pre-trained BERT model, available from a variety of sources, can be very conveniently obtained and fine-tuned with just one additional output layer for a wide range of tasks, such as text classification. The Transformers library from Hugging Face provides a wide selection of pre-trained, general-purpose architectures for NLP tasks, including BERT. In particular, they provide the BertForSequenceClassificaiton model which attaches a pooling layer and a linear layer after the BERT network. It can be used for the Twitter sarcasm detection task. Training Data loading and data cleaning are performed using Pandas. Consecutive spaces were replaced by a single whitespace, and leading and trailing spaces were removed. Some special characters, such as ""@USER"", were removed, while Emoji's were kept as they capture emotions. Emoji needs to be converted to known tokens for the network, and the emoji library provides a convenient function to do so. For example, emoji.demojize('')returns ':thinking_face:'. Cases were also kept because in my tests they tend to improve the f1 score (for both models). Punctuations are treated differently depending on the model. They were removed in the case of fastText, while for BERT, they were kept since BERT has embedding for most of the punctuations. The data cleaning steps are the same for both the training and test data sets. The training data (after data cleaning) is first shuffled randomly in order to mix SARCASM and NOT_SARCASM entries. Then a total of 5000 examples in the training data are split into a training set of 4500 examples and a validation set of 500 examples (10% split). The validation set is used for periodically evaluating the current model for tuning the model's hyperparameters and preventing overfitting. A minor challenge is that, since the training data set is not very big, splitting it into training and validation sets can be tricky. My solution is to first use the validation to find the optimal hyper parameters, and then retrain the model using the entire training set. For fastText, the most important hyperparameters include learning rate, number of epochs and wordNgrams. For BERT, some of the hyperparameters include learning rate, number of epochs, drop out rate and maximum length of text that can be handled by the tokenizer. Result and Deliverables For fastText using the tri-gram model, it is unable to beat the baseline, having an f1 score of 0.6642. BERT with fine tuning is able to reach an f1 score of 0.7362 within a few rounds to hyperparameter tuning. All model parameters allowed to be updated during the training process. I found four epochs is enough for the fine turning, while running more epochs tends to overfit the training data set and leads to a lower f1 score on the test data set. The code/deliverables include two Jupyter notebooks. More detailed comments are available in the notebooks about the specific functions of each cell. * 01_fasttext4Twitter.ipynb: This notebook contains implementation using the fastText library. The data loading and data cleaning steps use Pandas. The preprocessed training data set is written to disk in a format that's compatible with fastText. Training and prediction follows the steps of the text classification example in fastText's documentation ([5]), and only involves a few lines of code. To obtain additional metrics, the scikit-learn library is used to compute precision, recall and f1 score. Finally, both the trained model and the predicted labels are written to disk. * 02_bert4Twitter_pytorch.ipynb: This notebook contains implementation using PyTorch and the transformers library from Hugging Face. Pre-trained BertForSequenceClassification and BertTokenizer is downloaded from Hugging Face, and additional fine tuning of Bert network weights is performed using the training data. Similar to fastText, data is preprocessed using Pandas. To convert from Pandas data frame to PyTorch compatible data format, the datasets library from Hugging Face is used. To obtain additional metrics, the scikit-learn library is used to compute precision, recall and f1 score. Hugging Face also provides a simple but feature-complete Trainer class for training/fine-tuning the network ([6]). Trained model is then set to evaluation mode, and test data is transferred to GPU for prediction. Finally, both the trained model and the predicted labels are written to disk. Usage of software This project is implemented using Google Colab. Therefore, the most convenient way to reproduce and verify the results is through Google Colab and using a GPU runtime environment. Google Colab provides a free Nvidia Tesla T4 GPU for 12 hours for continuous usage for free. Open the Jupyter notebooks in Google Colab, map the correct Google Drive that contains the data and trained model, then modify the ""filepath"" variable to reflect the correct path. The notebook can be executed block by block, or select run all under Runtime. For 01_fasttext4Twitter.ipynb, since the training is so efficient, there is no need to load the trained model. On the other hand, for 02_bert4Twitter_pytorch.ipynb, the deep neural network can take a while to train. If one would like to avoid re-training the model, skip the training block and execute the Demo part instead (but don't skip the data loading blocks). Team Contribution This is a one-person team. Tutorial A software tutorial presentation is uploaded to a Box drive accessible with the link: https://uofi.box.com/s/wedyjqpel0rw2uxprax10mh2rnwxhu7t Note that although the tutorial is 15 mins long, the review can feel free to watch only the first 10 mins, which is about the BERT approach. The last 5 mins talks about fastText, which is optional to watch. Finally, to avoid re-training the BERT model, a trained model is available with the link: https://uofi.box.com/s/t64lnt83ck2m3khtrk4s3r46fjhd5gsk References [1] Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5, 135-146. [2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [3] Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. [4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30, 5998-6008. [5] https://fasttext.cc/docs/en/supervised-tutorial.html [6] https://huggingface.co/transformers/training.html [7] https://medium.com/atheros/text-classification-with-transformers-in-tensorflow-2-bert-2f4f16eff5ad [8] https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"
https://github.com/j5un/CourseProject	ProgressReport.pdf	Final Project Progress Report - Text Classification Competition Name: Junzhe Sun NetID: junzhes2 Captain: Junzhe Sun (individual) Competition: text classification competition Progress: For the text classification competition, I looked at a few different options and decided to use a pre-trained BERT model with fine tuning. 1) Which tasks have been completed? A: I looked at different shallow/deep learning methods, including FastText, LSTM and Transformer neural networks. I first trained a FastText model, a shallow neural network, using the training data set, but wasn't able to beat the baseline. Then I used a pre-trained BERT model from huggingface using PyTorch and fine tuned it using the training data set. The prediction using the BERT model was able to beat the baseline. 2) Which tasks are pending? A: (1) Further fine-tuning the BERT model to improve F1 score. (2) Source code documentation. (3) Software code submission with documentation/report. (4) Record software usage tutorial presentation. 3) Are you facing any challenges? A: No big challenge remains. A minor challenge is that, since the training data set is not very big, splitting it into training and validation sets can be tricky. In addition, I will probably need a more automated and systematic way for hyperparameter tuning (train-test split, number of epochs, rate for drop-out layer, etc.).
https://github.com/j5un/CourseProject	Proposal.pdf	Final Project Proposal - Text Classification Competition Name: Junzhe Sun NetID: junzhes2 Captain: Junzhe Sun (individual) Competition: text classification competition Method: I have learned about deep neural networks on the coursera platform before (Deep Learning Specialization). I understand the theory behind CNNs and RNNs (e.g. LSTM and GRU). I am interested in learning more about transformer networks such as BERT as well as CNNs for natural language processing (NLP). I have used Tensorflow before in course projects, and I know about Pytorch from reading publications. Programming Language: Python. In particular, I will be using Pandas and Tensorflow 2.
https://github.com/j5un/CourseProject	README.md	CS410 Text Information Systems Course Project Author: Name: Junzhe Sun Team: Individual NetID: junzhes2 Content: ./Documentation.pdf: software documentation ./notebooks: Jupyter notebooks for Twitter Sarcasm Detection ./data: training and test data ./Proposal.pdf: project proposal ./ProgressReport: project progress report Tutorial: Link to software tutorial Note that although the tutorial is 15 mins long, the review can feel free to watch only the first 10 mins, which is about the BERT approach. The last 5 mins talks about fastText, which is optional to watch.
https://github.com/milan-saroj/CourseProject	documentation on Twitter Sarcasm Detection using BERT.pdf	"Documentation for Twitter Sarcasm Detection using BERT 1 A. Introduction: As a part of final project of CS 410: Text Information Systems of Fall 2020 from University of Illinois- Urbana Champaign, we formed a team of three members. We decided to go for the Classification competition (https://github.com/CS410Fall2020/ClassificationCompetition) where we have to create a model that will classify the given tweets as ""Sarcasm"" or ""Not Sarcasm"" and predict the class of 1800 tweets of test set. After preprocessing, we implemented different methods such as Naive bayes, LSTM (Long Short- term Memory), BiLSTM (Bidirectional LSTM), and BERT (Bidirectional Encoder Representations from Transformers). Among all, the encoding and the model using BERT gave us the F1 score that was good enough to beat the baseline as required for successful completion of the project. We ran this model over Google Colab and Jupyter Notebook. However, we got the best training time using AWS Sagemaker Notebook instance. The detailed implementation is provided in the presentation document. B. Overview of the Code: The goal of the code is to detect the sarcasm of tweets. The source code for this project performs following task in a sequential order: a. Import the training and the test set b. Text preprocessing c. BERT encoding d. Training the model with BERT layer (deep learning) e. Use trained model to make prediction on the test set with 1800 tweets (both tweet response and tweet context). The training set includes 5000 tweets with ""Response"" and ""Context"" along with the label as ""Sarcasm"" or ""Not Sarcasm"". It also makes a prediction on the imported twitter test set with output as a text file ""answer.txt"" that has columns Twitter Id and class label as ""Sarcasm"" and ""Not Sarcasm"". In a nutshell, this code trains on the twitter dataset, classifies whether new tweets are sarcasm or not and it achieves an F1 score over 0.74. This code can be used for almost any text classification task with some modifications. Even though it only reads JSON file format for now, it can read any kind of acceptable file formats such as pd.read_csv and etc. with simple changes. The code calls specific column names such as Response and Context and they need to be modified as per training set. Even though this code uses two different models, each for Response and Context, one can choose to use single column data with some modifications on the model structure. Using an activation function such as Softmax instead of Sigmoid can be used to perform a multi- classification instead of a binary classification. 2 One can modify max_length values as needed based on the text length. The lower the value is, the quicker the training process will be, but it is important that this length should be large enough to cover the size of the text (each row) for the optimal result. C. Source Code Implementation: How the source code is implemented? How source code is working together to give the prediction file? The source code can be split into groups based on the functionality as described below along with the screenshots of source code: 1. Installing required libraries and importing preinstalled libraries This source code requires some libraries other than that comes with the kernel. Here sentencepiece library later helps to receive any iterable object to feed training sentences. Wget helps to download tokenization.py file that will be later used during a tokenization. Tensorflow-hub and tensorflow-addons will be required to run some functions. See below to check the implementation and the output of installing the library. The popular ""pip install"" is used to download these libraries. 3 The picture below includes the list of other libraries that are imported in the notebook. Nltk.punkt and other tokenization tools are used for the purpose of dividing a string into substrings by splitting on the specified string. 2. Data Preprocessing: We now perform a data preprocessing. This code has several functions that will work together to preprocess the data. Once the json training and test files are read using Pandas library, the code performs a preprocessing. Some of the notable preprocessing this code performs are: a. Remove punctuation b. Remove @User c. Change abbreviations to normal meaningful strings d. Remove emojis e. Remove links and non-ASCII characters, if any f. Combine separate sentences of context of same tweet into single documents g. Change the class label string to numerical forms using labelencoder. Listed below are some preprocessing functions that we created: 4 The function preprocessing calls other helper functions such as remove_emoji, remove_punctuations, etc. and applies it to each row of the data using an efficient pandas library 5 function called ""apply"". This is an example of a vectorizing function that works on all rows at the same time and is efficient and faster. The class labels of the training dataset are in string forms as they are named ""Sarcasm"" and ""Not Sarcasm"". They are converted into numerical forms using LabelEncoder function for the training purpose and saved in as a series with a variable name ""train_label"" as shown below: We then pass the train and test data frames to this function to get the preprocessed test and train sets as shown below: 3. BERT embedding, Training and Fitting: After the preprocessing, we now move on to BERT embedding, training, and model fitting. In this step, we conducted the BERT embedding, training and model fitting. First, we downloaded bert_layer, which was a pre-trained neural network with the Transformer architecture. We chose l=24 as hidden layers. Then, we encoded texts to ids to generate the encoded tokens, masks and segments using the pre-trained bert layers. One thing to be noticed, we encoded response and context separately and combined them afterwards. Finally, we fit the Bert encoded matrices into a model with epochs size of three and batch size of six. We use 90% of the dataset as a training set and 10% as a validation set. As a result, we achieved the F1 score of 0.74 which is about 3% above the baseline. This is where the initially installed tensorflow_hub comes into play. This model has been pre- trained for English on the Wikipedia and BooksCorpus using the code published on GitHub. Inputs 6 have been ""uncased"", meaning that the text has been lower-cased before tokenization into word pieces, and any accent markers have been stripped. The following function converts the token into an encoding that is later used as an input to a Bert layer (neural network). The following set of code tokenizes the sentences and embeds them using BERT. Here we chose max_len of 256. Choosing the right number was part of tuning the model since the smaller number will speed up the training process in expense of the performance of the model. This number of 256 works well for us and we got the best speed and performance with it. The following code creates a neural network model with the context and the response as inputs. However, we encoded the response and the context separately, and created separate neural network models and later concatenated them together right before they went to the output layer. 7 Lists test_generate and train_generate are the output of BERT encoding and they are required to feed into a bert layer of the neural network. We use the sigmoid function as we have a binary output. We set the learning rate to 1e-6 after a few optimizations runs and use Adam as an optimizer since it is one of the most popular optimizers in the industry nowadays. Adam moves faster at first and then slows down once it starts to get closer to the local/global minima while training. 4. Making prediction based on the trained model Once we create a model, we train it with our training set. We have used 90% of training set as the training data and 10% as the validation dataset. The values for the parameters Training set percentage, batch size and epoch were empirically determined by us to get the best f1 score and the shown values gave us the best result. Note that each epoch took about one and a half hours even after running with Sagemaker which has the larger RAM than the other options. Running them on a local computer would take almost a day to run 2-3 epoch and it is one of the reasons why we chose Sagemaker as a platform to train our model. Once the training is completed, we use our test set to make a prediction. Once the prediction is made, it is converted back to non-numerical form of class labels as ""Sarcasm"", ""Non-Sarcasm"" using LabelEncoder and inverse_transform. The prediction is later to be converted into Dataframe, and concatenated with the twitter ids from the test set so we can have dataframe with twitter ids and a prediction class. The dataframe is later saved as a text file ""answer.txt"" which is later uploaded on github. 8 Since github is already set with the webhook, once we commit and push, the result of our prediction will show on livelab. At the time of writing this documentation we got the F1 score of 0.742 and we were ranked at 30 in the leaderboard. D. How to run the Source Code? The code will install all the necessary libraries, import all required libraries, therefore there is no need for any additional installation to run this code provided that this code is ran under specific kernel of ""conda_amazonei_tensorflow2_p36"" which is available in Notebook instance of AWS Sagemaker. Here are the stepwise details on how to run the source code (notebook) on AWS Sagemaker. 1. Go to https://aws.amazon.com/ and create an account if you do not have one. 2. Once you are logged in, type SageMaker on the search tab. 3. Go on Notebook Instances as shown below. 9 4. Click on Create Notebook Instance 5. Type any name for Notebook Instance name and select Instance type. The free tier AWS version only allows certain maximum size. We use ml.c4.8xlarge to train our model which is available for a free tier account. 10 6. Once you create a notebook instance, it will take 1-2 minutes to be activated (inService). Once you see an inService sign, you can click ""Open Jupyter"" as below. It will open a notebook on your default browser. 7. Once your AWS notebook opens, upload the source code (notebook file), train.jsonl, and test.josnl file on the notebook. 11 8. Once your upload is finished, click on notebook (""Source code""). 9. Change the kernel to conda_amazonei_tensorflow2_p36 by selecting the kernel tab from the navigation menubar and select ""change kernel"" tab to choose conda_amazonei_tensorflow2_p36 kernel. 10. Once the notebook is open, go to ""Cell"" and click ""Run All"". 12 All the code cell will run and the prediction on test set will be exported as ""answer.txt"" after a few hours on the root directory that can be accessed by going File and click ""Open"". This txt file will have the prediction as per the project requirement. How did we get here? What are our experiments with other methods and hyperparameter tuning? Once we did the preprocessing, we used different machine learning algorithm to make predictions. While other ML algorithm was not giving us a good result, BiLSTM was giving us some encouraging result. Below are the screenshots of one of the models we used. This one gave us the F1 score of about 0.65 which was not good enough. After some suggestion from the discussion board, we came to realize that BERT with its attention layer will definitely help us to achieve the prediction on test set that could beat the baseline. All three of us started our own research and share ideas and references with each other. We came up with one model that started to look promising with the F1 score of 0.69. That is the time when we realized that all we need was a good hyperparameter tuning. Here are the lists of parameters that we tuned in order to get the result to beat the baseline. a. Epoch b. Learning batch size c. Learning rate d. Max Length to feed into Bert Layer (max_length) e. Test/Validation split percentage Since the training took us over 4 hours for a single epoch with 15 batch size, we needed to find a way to speed up the process. That is when we decided to go with Sagemaker where we could leverage larger RAM size (60 gb RAM with 36vCPU). After reducing the training time from 4 hours to 40 mins using Sagemaker, we were able to run the model with different 13 hyperparameters more easily. Our max_length at first was 150 but later on we changed it to 256 in order to get the better result. We were able to get closer to baseline after a few trials, but we still could not beat the baseline by a small margin. We then went back and started making changes on the preprocessing. Realizing the feature engineering was a very important aspect of the training, we made some changes such as converting abbreviated words to natural words and implementing the better handling of punctuations by keeping some punctuations such as (...). With these changes and a few more trials with the different hyperparameters, we managed to beat the baseline. How was our team effort? Who did what? After our initial meetings, we decided that we all should be working separately during the preprocessing. There were two main reason for this decision: a. It was necessary to come up with the preprocessed data since we cannot move forward without it. Therefore, coming up with a near-perfect preprocessing was important. b. Working independently would bring more creativity while preprocessing. Once we came up with our own set of the preprocessing, we had a meeting and come up with the best preprocessing code which included best parts of all three different preprocessing code. We then decided to work on different models and compare results. Saroj worked on BiLSTM and k_train library, Jiayi worked on BERT encoding and layer, while Kevin worked on Naive Bayes, and an additional feature engineering that could be possible to implement. While BiLSTM showed some good signs, BERT started to get closer to the baseline. At that point, we all started working on BERT and tried different hyperparameters to train the models. We have each tried over 15 different trainings with different hyperparameters and it took about three hours per training on average. Sometimes we ran two different models simultaneously leveraging AWS Sagemaker. Once we beat the baseline, Jiayi worked on cleaning and organizing the source code, Kevin worked on the presentation and Saroj worked on the first draft of the documentation. Later, everybody came together to finalize the source code, presentation, and demo video. It was a good team effort, and we all contributed almost the same amount of time. With the training time accounted, we worked over 35 hours each for this project. References: * https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1 * https://www.kaggle.com/rftexas/text-only-kfold-bert * https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for- nlp-f8b21a9b6270 * https://huggingface.co/transformers/model_doc/bert.html * https://www.kaggle.com/funxexcel/keras-bert-using-tfhub-trial"
https://github.com/milan-saroj/CourseProject	Progress Report.pdf	"Progress Report Three of us had a kick off meeting a few weeks ago, and decided to work separately on data cleaning and preprocessing. This decision was made because we won't be able to work on ML/DL implementation without preprocessing. We share our preprocessing code with each other, and make progressive changes on our individual works. Since there are many directions we can go with feature engineering such as whether to use just the response or use both response and context, keep the emojis or leave it alone, etc., we have kept all different versions of preprocessing and feature engineering codes to implement ML/DL models later on. Current progress We have put together a few different preprocessing codes, and have created ML models (like Multinomial Naive Bayes), LSTM, BiLSTM and BERT models. We have completed a whole cycle of the project such as importing the train, test file, preprocess and data cleaning, implementing DL models, and have created an ""answer.txt"" file as per the project requirement. We have individually submitted the code (pushed) through livelab, but failed to beat the baseline. Remaining Task Since we completed the whole cycle of project submission without beating the baseline, our task is to improve our models. We need to improve on feature engineering by trying a few different methods. Some of them we are thinking that could help are: 1. Leaving few punctuation marks such as ""!"" which could improve the model, 2. Run models without removing emojis which might help. 3. Need to figure out a perfect way to combine response and context. Such as do we use all the context or just the last one followed by the response, 4. Try pre-trained models like K-train 5. Fine tune the models (using different epoch, validation split allocation, etc) Challenges I.Although we achieved 0.694 of f1, we still need to improve the BERT model to beat the baseline (which is 0.723 of f1) or learn about K-train as well. II.To find a perfect way to use response and context is one of the challenges. III.Find the perfect feature engineering such as whether we need to remove all punctuation, emoji, stopwords, etc."
https://github.com/milan-saroj/CourseProject	project_proposal.pdf	Project Proposal: Text Classification Competition: Twitter Sarcasm Detection 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name NetID Saroj Khanal khanal2 Captain Kevin Choi gchoi17 Jiayi Chen jiayic15 2. Which competition do you plan to join? Text Classification Competition: Twitter Sarcasm Detection 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Some of the neural classifiers are : ANN (Artificial Neural Network), RNN(Recurrent Neural Network), LSTM (Long Short Term Memory). ANN is a feed forward neural network where data pass forward from input to output. On the other hand, RNN has feedback loops in the recurrent layer which enables maintaining information in 'memory' over time. However, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). To look into this issue further, LSTM networks are a type of RNN that uses special units in addition to standard units.LSTM units include a 'memory cell' that can maintain information in memory for long periods of time.There are three gates for LSTM: an input gate, an output gate and a forget gate. Gates are used to control when information enters the memory, when it's output, and when it's forgotten.This architecture lets them learn longer-term dependencies. LSTM assigned relatively more important weights on units with longer term to improve general accuracy. Two of our team members have experiences with Neural Networks and have worked on personal projects to some extent. However, for this project, our team needs to do some research, learn more about the above mentioned classifiers to properly implement to increase the accuracy of the model. However, before feeding the data into Neural Network, we will need to perform several text processing that we learn on the earlier part of the class such as tokenization, change words to lowercase, remove numerical data, remove stopwords, stemming, lemmatization, vectorization (Bag of Words), TF-IDF, use Word2Vec and etc. 4. Which programming language do you plan to use? Python (Tensorflow library, Keras API, Gensim, NLTK library, TextBlob, spaCy).
https://github.com/milan-saroj/CourseProject	README.md	CourseProject Guidelines for Reviewer Source code, train and test json files are here in the github. Please follow the link below for Demonstration video: https://mediaspace.illinois.edu/media/t/1_adl8xwo0
https://github.com/houyuan2/CS410CourseProject	CS410 proposal.pdf	If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Yuechen Liu yuechen7 Houyuan Sha houyuan2 Ruobin Wang ruobinw2 Tianren Zhou tianren2 captain 2. What system have you chosen? Which subtopic(s) under the system? Improving A System: Expert search system 3. Briefly describe the datasets, algorithms or techniques you plan to use For the dataset, we can obtain the current collection of faculty pages dataset from the course TA. We plan to use ElasticSearch to implement our search engine. https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html We will import the dataset to ElasticSearch and implement corresponding unstructured queries to obtain the relevant result. We will implement a python backend and html frontend, which should provide an accessible interface for the user. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We attempt to provide an alternative method to query the existing expert dataset using elastic search. In doing so, we attempt to achieve better accuracy and speed. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We plan to implement APIs in the python server. The frontend will communicate with backend using the API. 6. Which programming language do you plan to use? Python, reactjs, js 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Total 20*4 = 80 hours * Frontend (30 hours) * Backend - api (20 hours) * Backend - db, search engine (30 hours) At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why.
https://github.com/houyuan2/CS410CourseProject	CS410_Progress_Report_1129.pdf	Progress thus far: 1. Deployed elastic search API and the visualization tool Kibana in Docker containers 2. Extracted email address from bios using regex 3. Populated the elastic search database with url, bio, and email data 4. Built the basic front end that is able for API and backend to test queries including search bars and outputs Remaining Tasks: 1. Build a clear front end that is enough to show results properly 2. Extend more sophisticated search using ElasticSearch 3. Filter unrelated information given certain keywords 4. Enhance performance of original ranking system of ExpertSearch Challenges: 1. Extracting faculty names from bios is challenging. Plan to resolve this via Python Spacy library 2. Query in ElasticSearch is new to us. We need to learn more about how to write queries in this engine.
https://github.com/houyuan2/CS410CourseProject	README.md	"Video Presentation https://www.youtube.com/watch?v=TLylfLThiGk Setup install Docker (https://www.docker.com/get-started, install Docker Desktop) install python3 (at least 3.7) in project folder, run the following in terminal: pip3 install flask\ pip3 install regex\ pip3 install elasticsearch\ pip3 install spacy\ python3 -m spacy download en\ docker-compose up -d wait 3 minutes for docker containers to be set up in terminal: python3 setup.py wait for all index to be created, this could take up to 3 hours, due to NLP and indexing Server in terminal: python3 server.py navigate to http://127.0.0.1:5000/ Goal Our project aims to develop a search tool for experts in different fields. An user can enter a search phrase(e.g Data Mining) in the search bar and our application will return the likely results according to similarity between the bios of the expert and the search phrase. The user can inspect the validity of the result by accessing the homepage of the expert using returned URL. Furthermore, the user can contact the expert via the returned email. Overview Our project is based on elastic search, a document-orientated database that provides unstructured search functionality. The code base is divided to 3 parts. In ""setup.py"", the application would read the input text files to obtain faculty bios and the corresponding homepage URL. The application would then attempt to extract the faculty name and email from the bios using spacy, a NLP package. These information would be stored into the elastic search database. In ""server.py"", the application would query the database with the input typed in search bar, and return the corresponding results. Comparison Original Project: https://github.com/CS410Fall2020/ExpertSearch/ \ Compared to the original project, which used the Stanford model, our project took a different route. We used elastic search as text tokenization and ranking tool. We also used python3 spacy to extract email and name from faculty bios. We attempt to extract more names and emails from the input bios. We had more success with email, but name extact is less successful due to the limitation of NLP. Another difference is our search engine is key word based, which is more relaxed compared to the original project. Moreover, our project allows user to specify how many results to be returned. Finally, our front end is more fine-tuned. Contributions Houyuan Sha: Set up the docker container for elastic search; Extract name and email from faculty bios; Store faculty information to elastic search database Yuechen Liu: Set up Flask; Write api connecting database and frontend Tianren Zhou: Write query and comments for future extension Ruobin Wang: Designed user interfaces; Set up front-end"
https://github.com/rnowak6/CourseProject	Final Project CS410.pdf	Team Captain : Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 Final Project Introduction The project we decided to focus on creating a tool to help students navigate through multiple slide decks and be able to easily find similar slides to topics that students are attempting to search for. Our team focused on expanding the EducationalWeb System by adding functionalities to improve navigation between slides and uploading/downloading. Our datasets included the entirety of the CS 410 slides from coursera along with other courses. We decided to use similar techniques to the current Web of Slides and connect new slides to the web framework by using a combination of TF-IDF similarity and word embedding based similarity. Features Our project allows for downloading multiple slides at a time along with the option where you can add any slides you like, and then download them at the end of your session. For the main layout of the application, we created more than one page for this application so that it can easily be expanded on in the future. With this goal in mind, we decided to create a homepage where you can see all of the current classes listed that have slides, and then once selecting, it will bring you to the slide page that is currently in production now. Additionally, we have made improvements on the current user interface of the slides page. We have changed the way the image for the slide is generated, and convert it to a jpeg to help with seamless loading and to give the page a more holistic feel. Challenges Our team faced multiple challenges with this project. It was challenging to set up the Education web system on our own devices however, luckily, we were all able to have it running on our computers. Another challenge we faced was being able to upload our source code to github since the files were so large and github struggled to upload them all however this issue was fixed by omitting the large files. Lastly, our biggest challenge was being able to have our features fully functioning. We struggled to create functioning features due to having to also learn the given code and be able to make it work with our vision.
https://github.com/rnowak6/CourseProject	Progress Report.pdf	"Team Captain: Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 Progress Report Team Bogus Progress made thus far: 1. All team members were able to get the code downloaded and working on their own computer 2. 5 different UIUC course slides were downloaded to be added to the website 3. Updating the user interface for the Educational Tool a. We have decided to create more than one page for this application so that it can easily be expanded on in the future. With this goal in mind, we decided to create a homepage where you can see all of the current classes listed that have slides, and then once selecting, it will bring you to the slide page that is currently in production now. b. We have made improvements on the current user interface of the slides page. We have changed the way the image for the slide is generated, and convert it to a jpeg to help with seamless loading and to give the page a more holistic feel Here is the current new front page. Remaining tasks: 1. Make final adjustments to front end User Interface a. We will create a ""Upload your own"" slides button on our new front page to allow students to upload their own slides if our algorithms permit for analysis on new slides b. Add functionality and views for notebook feature outlined below 2. Create a notebook feature on the side of the screen where a user can save specific slides to create their own study tool 3. Creating a toolbar where we can highlight and take notes on our notebook feature Any challenges/issues being faced: * Ran into numerous errors when trying to get the code working. Took much longer than expected. * Have not been able to get the code uploaded to our own team's github repository. Numerous errors in uploading."
https://github.com/rnowak6/CourseProject	Project Proposal 410.pdf	"Team Captain : Rose Nowak - rnowak6 Calina Shaw - ceshaw2 Ayline Villegas - aylinev2 Sarah Menza - semenza2 Nicole Kolbasov - nsk3 1. What system have you chosen? Which subtopic(s) under the system? Our group has chosen to expand the EducationalWeb System by adding functionalities to improve navigation between slides and uploading/downloading. 2. Briefly describe the datasets, algorithms or techniques you plan to use Our datasets will include the entirety of the CS 410 slides from coursera. If time permits we may expand the dataset by adding slides from other UIUC courses such as HORT 106, but this is not the main priority of our project. We will use similar techniques to the current Web of Slides. To connect new slides to the web framework we will use a combination of TF-IDF similarity and word embedding based similarity. We will also introduce some kind of data structure (probably a doubly linked list) to keep track of the student's path throughout the web so they can return to a previous slide. 3. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? We would like to add a popup or sidebar that displays the most relevant similar documents as you are viewing one set of slides. The relevant documents would be calculated using one of the ranking algorithms. We would like to add functionality to go ""back"" to your original slide after clicking related slides on the right hand side. This will be a good feature because that way you can go back to your original lecture after addressing anything you were confused about. We will demonstrate that this works by recording a short demo where we navigate away from the start slide and then hit the back button to return to it. We would also like to add downloading multiple slides at a time. We will be able to add a ""notebook"" option where you can add any slides you like, and then download them at the end of your session. We will demonstrate that this functionality works by testing it locally by downloading a group of files. Students will be able to upload their own pdf slides to perform analysis for them (if runtime permits, we will know more if this is possible after beginning). This could also be checked with local testing by uploading a pdf file that isn't included in the web yet. 4. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Our code will extract all of the Coursera slides and determine which slides are similar to the one we are currently viewing. It will then display the related slides on the side in a pop up or sidebar so you can easily access them. This popup/sidebar will then have an option to be closed or to help you navigate back to the original slides you were viewing. 5. Which programming language do you plan to use? Python 6. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Main Tasks: * Project Topic Brainstorming Session - 1 hour * Project Proposal and Team Formation Submission - 3 hours * Project Setup - 1 hour * Add algorithm to help find most related documents - 1 hour * Add pop up or sidebar that displays the results of the algorithm run - 5 hours * Add functionality to go ""back"" to your original slide after clicking related slides on the right hand side - 20 hours * Add functionality to download multiple slides at a time - 20 hours * Team Check-In Meetings (over Zoom) - 5 hours * Progress Report - 3 hours * Add functionality to allow students to upload their own slides - 20 hours * Documentation - 15 hours * Improve front end functionality- 8 hours Total Hours: 102"
https://github.com/rnowak6/CourseProject	Project Resources.pdf	"2.3 EducationalWeb System The EducationalWeb system (http://timan102.cs.illinois.edu/explanation//slide/cs-410/0) is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and recommend relevant slides for each slide. You can read more about this in the following papers Web of Slides, WOS Demo.; 2) Find an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""cap/scholar"" button on the top-right of a slide. It will try to retrieve a relevant section from the Professor's textbook that contains an explanation of the selected phrase. You can read more about the underlying algorithm here. The code for the system is available here. Below are some ideas to improve and expand this system. You may choose to integrate your code with the existing system, or borrow some ideas from it, or build your own systems/algorithms. * Improving the usability and reach of the existing system Some of you might have used the system and identified potential areas of improvement. The aim of this subtopic is to refine the current version of EducationWeb. Some specific ideas include (many are borrowed from this Piazza post): 1. Scale up the current system. Add more slides and courses from multiple sources e.g. Coursera, UIUC courses, etc. and run the existing algorithms on them. Again, it might be useful to think about automatic crawling similar to the subtopic in 2.2 above. It would be very interesting to see the interaction between slides/textbooks at a large scale!! 2. Improve the performance of the system. Currently, loading each slide takes time. 3. Allow downloading slides in bulk. Currently, we can only download one slide at a time. 4. Add more context to the explanations (e.g. link to the specific page in the textbook) 5. Allow adding additional courses/lectures directly from the web interface. This would also involve dynamically identifying the recommended/relevant slides for a new slide. Currently, a static file is used which contains pre-computed recommendations for each slide. 6. Integrate the tool with Piazza/Coursera, i.e. maybe link Piazza/Coursera to the tool or vice-versa. Alternatively, add discussion forum and video capabilities to the tool so that it serves as a one-stop-shop for all users' educational needs. 7. Link to latest related research articles: In this way, the lecture content can be automatically updated 8. You could also work on improving the current recommendation, search and explanation mining algorithms (described in the papers at the beginning of this section 2.3) * Automatically creating teaching material for in-demand skills This subtopic is an extended version of the existing EducationWeb system. There is an increasing demand for skilled workers in the industry. Quality education is not easily accessible to everyone due to barriers such as high cost, geographical and language barriers, etc. Also, instructors cannot be available 24*7 to provide personalized support to all learners. In this subtopic, the overarching aim is to tackle some of these issues. In particular, the following tasks might be good starting points. * Identifying in-demand skills: You can crawl and analyze relevant sections of job boards, news articles, scientific articles, social media, etc. to automatically identify the emerging keywords /topics. For this, you may refer to some papers on contextual text mining (mentioned in Option 1 of this document). * Creating lectures and tutorials for those skills: For this, you may consider lecture slides (e.g. from Coursera courses) as the basic units of knowledge. Then, the task could be to find the most relevant slides or clusters of slides (could be across multiple courses/lectures) for a given skill (topic). You may borrow some ideas from the EducationWeb system for this. You may also use the slides in existing lectures on some topics as the ""relevant slides"" for those topics. In this way, you can automatically generate training data for supervised learning. You could also combine knowledge from multiple sources (e.g. textbook sections, slides, videos, blogs, codebases) for creating more comprehensive tutorials. A more challenging task would be to automatically generate  the lectures/tutorials using techniques from natural language generation and abstractive text summarization. Another interesting idea is to automatically generate agents , e.g. using Virtual Agent Interaction Framework (VAIF). This goes beyond the material covered in class but could lead to some highly innovative and state-of-the-art projects! If you choose this option, please answer the following questions in your proposal: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. 2. What system have you chosen? Which subtopic(s) under the system? 3. Briefly describe the datasets, algorithms or techniques you plan to use 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly 6. Which programming language do you plan to use? 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. At the final stage of your project, you need to deliver the following: * Your documented source code. * A demo that shows your implementation actually works. If you are improving a function, compare your results to the previously available function. If your implementation works better, show it off. If not, discuss why."
https://github.com/rnowak6/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/oransum/CourseProject	CS410_ProjectProposal.pdf	CS410 Text Information Systems(Fall 2020) Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Edward Ma - kcma2(Captain) Oran Chan - wlchan2 2. Which competition do you plan to join? We are planning to work on the competition of Text Classification 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We are interested in SVM, XGBoost and especially the BERT model which has proven performance on downstream tasks. Hence, we decided to use PyTorch to fine-tune this state-of-the-art method to tackle various classification tasks. Toxic comment classification is one of my experiences working on BERT. 4. Which programming language do you plan to use? Python
https://github.com/oransum/CourseProject	Documentation.pdf	"Text Classification Competition: Twitter Sarcasm Detection Oran Chan (wlchan2) and Edward Ma (kcma2) ABSTRACT Sarcasm detection is a specific case of sentiment analysis. Instead of classifying all kinds of motion, this task only focuses on sarcasm. Given a text as input, the detection model outputs whether it is sarcastic or not. The most challenging part is that judgement of sarcasm detection is not very clearly defined or it is subjective. In this classification competition, we suggest using less human and computer resources to achieve a better result. We demonstrate how synthetic data helps to boost up model performance with manual effort. 1. INTRODUCTION In this classification competition, the training data size is 5000 with equal distribution. This size is relatively small in natural language processing (NLP). Therefore, we proposed to use transfer learning and data augmentation strategy to tackle this problem. For the transfer learning part, we will train our model based on a pre-trained model which was trained on a very large corpus to solve some basic NLP tasks. It is a promised way to start with them instead of training from scratch. It does not only provide a converge word embeddings but also shortens training time. For the data augmentation part, we leverage contextual word embeddings training methods to generate synthetic data based on limited training data. 2. DATA PROCESSING Given 5000 twitter text, we split it into training dataset and evaluation dataset with 9:1 ratio. In other words, the training dataset includes 4500 records while evaluation dataset includes 500 records. 2.1 Preprocessing As mentioned before, we adopted a pre-trained model which can take care of lots of data processing. We do not need to do lots of feature engineering based on pure text but tokenizing text into subwords. Instead of using words as a feature, we decode to use subwords. For instance, ""language"" can be represented by ""lang"", ""uage"" tokens. One of the major benefits is that it can handle out-of-vocabulary (OOV) problems. Giving that we use 26 (or 52 if case sensitive) characters, we can represent all English words. Another advantage is that it can converge rare word's embeddings as we may break down a single rare word into multiple tokens. Also, subword algorithms leverage an affix behavior to further coverage subword embeddings. In English linguistics, an affix is a morpheme that is attached to a word stem to form a new word or word form. For example, a ""dis"" prefix means opposite while a ""less"" suffix means no. The following part covers Byte pair encoding (BPE) [1] and WordPiece [2] subword algorithm. Example of Word Tokenization 2.2 BPE BPE is proposed by Sennrich et al. (2016) and the general idea is counting the frequency of subwords up to a predefined maximum number of subwords. BPE is adopted by RoBERTa [5]. The algorithm is: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters and append the suffix ""</w>"" to the end of word with word frequency. So the basic unit is character in this stage. For example, the frequency of ""low"" is 5, then we rephrase it to ""l o w </w>"": 5 4. Generating a new subword according to the high frequency occurrence. 5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1. 2.3 WordPiece WordPiece is proposed by Schuster and Nakajima (2012). The idea is the same as BPE except the criteria of forming new subwords. New subwords will be formed based on likelihood but not the next highest frequency pair. WordPiece is adopted by BERT [6] The algorithm is: 1. Prepare a large enough training data (i.e. corpus) 2. Define a desired subword vocabulary size 3. Split word to sequence of characters 4. Build a languages model based on step 3 data 5. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model. 6. Repeating step 5 until reaching subword vocabulary size which is defined in step 2 or the likelihood increases falls below a certain threshold. 2.4 Data Augmentation There are lots of different ways to generate synthetic data. One of the typical ways is replacing words by synonyms [3] over NLTK [4] library. The limitation of using NLTK's synonyms is that it does not consider context. Considered the nature of sarcasm detection, we decided to levearge neural network models to find similar meaning words when considering context. The mechanism is picking a word randomly and using a neural work model to predict the possible word to replace it by providing whole content to the neural network models. We adopted BERT and RoBERTa neural network models to perform this data augmentation and the ration is 1:1. Here is the mechanism: 1. Pick a word from input 2. Replace the picked word by reserved token (e.g. [MASK]) 3. Tokenize input with masked token 4. Feeding tokenize tokens to masked language model 5. Replace the picked word by language model prediction. Flow of Augmentation As nlpaug [7] implemented over 10 different data augmentation, we decided to leverage this library for data augmentation. In the evaluation, we tried different sizes of synthetic data to see how synthetic data affect the model performance. Also, we adopted two neural network models for comparison. Example of augmented data Type Content Original @USER @USER Stephen Jones finally losing it on Twitter by claiming the Liberty is a great stadium . Never mind his thoughts on Sarries and the salary cap breach - this is a new low even for him . <URL> Augmented Data #1 @USER @USER Stephen Jones finally losing it at Twitter suddenly claiming Kings Liberty is a great stadium . Never mind after thoughts over Sarries and the transfer tax breach - this is a historic low from meeting him . <URL> Augmented Data #2 @user @user stephen jones finally losing it on twitter by claiming the series is its great success . never mind his thoughts on soccer but high salary cap pro - football is a serious low even for david . < url > 3. MODEL ARCHITECTURE In this text classification, we evaluated both BERT and RoBERTa model and we finally picked RoBERTA as it outperforms BERT model based on our evaluation dataset. 3.1 BERT BERT (Devlin et al., 2018) is a method of pre-training language representations, meaning that it was trained as a general-purpose ""language understanding"" model on a large text corpus (like Wikipedia), and adopting it for downstream NLP tasks that we care about. BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP. BERT uses three embeddings to compute the input representations. They are token embeddings, segment embeddings and position embeddings. ""CLS"" is the reserved token to represent the start of a sequence while ""SEP"" is a separate segment (or sentence). Token embeddings are subword embeddings which represent the subword itself. Segment embeddings only include two embeddings which represent the first segment of input and second segment of input. Position embeddings refers to the position of the subword in the input. Segment embeddings help to distinguish two segments in some NLP downstream tasks such as question and answering tasks. For classification tasks, we only use a single segment. Position embeddings reflect the location as the same word may have different meanings in different positions of text. For BERT's training setup, it uses the Masked Language Model (MLM) and Next Sentence Prediction mechanism. By masking some tokens randomly, using other tokens to predict those masked tokens to learn the representations. For example, the original sentence is ""I am learning NLP"". Assuming ""NLP"" is a selected token for masking. Then 80% of time, it will show as ""I am learning [MASK]. For the Next Sentence Prediction approach, it targets to learn the relationship between sentences. The objective is classifying whether the second sentence is the next sentence or not. 3.2 RoBERTa Liu et al. (2019) studied the impact of many key hyper-parameters and training data size of BERT. They found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. RoBERTa (Robustly optimized BERT approach) is introduced and performance is either matching or exceeding original BERT. RoBERTa is developed based on BERT. By applying some modifications, it outperformed BERT model performance according to Liu et al (2019) experiments. First of all, it uses a larger training data. On the other hand, RoBERTA uses dynamic masking instead of static masking. Dynamic masking means that the masked token will be different every time. 4. EXPERIMENTS In the experiments, we mainly compare two BERT and RoBERTa with different numbers of augmentation data. The size of original training record and evaluation record are 4500 and 500 respectively while size of augmentation data are various. Taking experiment #2 as an example, it used 9000 augmentation data and 4500 original data for training. Instead of using a number of epochs for comparison, we use global steps. An epoch is one full pass through the training set, so that every sample gets the chance to be seen by model. Global steps refer to the number of batches seen by the model. For example, 1 epoch includes 4500 training records when it does not include any augmentation data while there are 13500 training records in 1 epoch when introducing 9000 augmentation data. Therefore, using global steps for comparison is a better approach. 4.1 BERT vs RoBERTa This experiment aims at demonstrating the model performance between BERT and RoBERTa. RoBERTa shows it converges to minima faster than BERT. RoBERTa reaches its best F1 score at 6756 global steps while BERT reaches at 14000 global steps. Although BERT is sligher better than RoBERTa 0.004, we decided to adopt RoBERTa after balancing between computation and model performance. Experiment Result #1 The following experiment is comparing model performances between BERT and RoBERTa when there are augmentation data. We noticed that RoBERTa outperforms BERT in 3 check global step checkpoints. Global Step BERT RoBERTa Precision Recall F1 Precision Recall F1 1126 0.797 0.768 0.782 0.649 0.960 0.774 2815 0.741 0.904 0.814 0.776 0.764 0.770 6756 0.749 0.872 0.806 0.767 0.880 0.819 7319 0.796 0.748 0.771 0.822 0.756 0.787 11823 0.816 0.708 0.758 0.805 0.76 0.782 14000 0.777 0.876 0.823 0.771 0.860 0.813 Global Step BERT RoBERTa Precision Recall F1 Precision Recall F1 Experiment Result #2 4.2 Data Augmentation Besides model architecture, we want to see whether synthetic data helps to improve performance. Different sizes of augmentation data are adopted for comparisons. It includes 2250 (0.5 times of original dataset), 4500 (same size of original dataset), 9000 (2 times of original dataset) and 45000 (10 times of original dataset). In this experiment, we want to evaluate whether augmentation data can boost up model performance. Due to the setup, experiment #1 does not include six thousandths global step so we use the nearest one which is 6756 global setup as reference. From the below figure, we can see that significant improvement when the size of augmentation data increased to 45000. Experiment Result #3 4.3 Final Submission After conducting previous experiments, we decided to further train a RoBERTa with 45000 augmentation data for final submission. Between 0 and 10000 global steps, F1 score improved quickly. After that it converged slowly until 18000 global steps. After 18000 global steps, F1 score drops until 32000 global steps. Finally, the model only predicts either SARCASM or NOT_SARCASM for the whole validation dataset. Therefore, we picked the 18000 global step checkpoint as our final submission. Finally, our precision, recall and F1 are 0.687, 0.816 and 0.746 respectively. 2000 0.711 0.876 0.785 0.736 0.880 0.801 4000 0.751 0.748 0.750 0.779 0.784 0.781 6000 0.803 0.768 0.785 0.762 0.884 0.819 # Augmentation Size Global Step Precision Recall F1 1 0 6756 0.767 0.880 0.819 2 2250 6000 0.805 0.840 0.822 3 4500 6000 0.782 0.804 0.793 4 9000 6000 0.762 0.884 0.819 5 45000 6000 0.926 0.848 0.885 # Global Step Precision Recall F1 1 2000 0.710 0.840 0.769 2 4000 0.801 0.852 0.826 Experiment Result #4 4.4 Execution We prepared a specific python file for each experiment. All of them will invoke the same python class with different parameters. The following table shows the mapping between python execution and experiment. To generate an answer for final submission, we use the following script. 3 6000 0.926 0.848 0.885 4 8000 0.924 0.976 0.949 5 10000 0.980 0.976 0.978 6 14000 0.992 0.968 0.980 7 16000 0.980 0.976 0.978 8 18000 0.980 0.988 0.984 9 20000 0.980 0.676 0.980 10 24000 0.984 0.976 0.980 11 30000 0.988 0.976 0.982 12 32000 0.98 0.980 0.980 13 34000 0 0 N/A 14 36000 0 0 N/A 15 136000 0 0 N/A Python File Model Augmentation Size run_bert_without_aug_epoch.py BERT 0 run_bert_with_aug_9000.py BERT 9000 run_roberta_without_aug_epoch.py RoBERTa 0 run_roberta_with_aug_2250.py RoBERTa 2250 run_roberta_with_aug_4500.py RoBERTa 4500 run_roberta_with_aug_9000.py RoBERTa 9000 run_roberta_with_aug_45000.py RoBERTa 45000 4.5 Presentation You may also check out this link (https://mediaspace.illinois.edu/media/t/1_fpgtkn68) for our video presentation. 5. FUTURE WORK Besides data augmentation, we also brainstormed other ideas which include gathering more twitter data, evaluating other state-of-the-art models such as ELECTRA [8] and tuning hyperparameters. 6. REFERENCES 1. R. Sennrich, B. Haddow and A. Birch. Neural Machine Translation of Rare Words with Subword Units. 2015 2. M. Schuster and K. Nakajima. Japanese and Korea Voice Search. 2012 3. X. Zhang, J. Zhao and Y. LeCun. Character-level Convolutional Networks for Text Classification. 2015 4. E. Loper and S. Bird. NLTK: The Natural Language Toolkit. 2002 5. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. 2019 6. J. Devlin, M. Chang, K. Lee and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018 7. E. Ma. NLP Augmentation. https://github.com/makcedward/nlpaug. 2019 8. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M.Matena, Y. Zhou, W. Li and J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. 2019 Python File prediction.py"
https://github.com/oransum/CourseProject	progress_report.pdf	Progress Report for Text Classification Competition: Twitter Sarcasm Detection Oran Chan (wlchan2) and Edward Ma (kcma2) Progress Made - Completed data exploration and exploitation. Having 5000 equal distributed label training data. Split data to training set and evaluation set and sizes are 4500 and 500 respectively. - Possible to find external data to enrich the dataset but considering the efforting of searching and data processing. - We proposed to generate synthetic data instead of looking for external data as it involves lower effort. Generated different sizes of synthetic data for evaluation. From 0.5 times to 10 times. - Evaluated deep neural network model architecture for building classification model - Trained model based on the pre-trained neural network model (BERT and RoBERTa) and achieved a good result which exceeds the baseline. Remaining Tasks - Refractor coding for easier understanding - Summarize the effectiveness of synthetic data. It includes the comparison among different sizes of synthetic data and models. - Prepare the documentation. The focal point is how we can leverage synthetic data to boost up model performance with minimum human effort. - Prepare the presentation material about what we did and how it works Challenges - Spends time on understanding the relationship between response and main thread content. - Learn subword algorithms such as WordPiece (adopted by BERT) and Byte Pair Encoding (adopted by RoBERTa) - Learn transformer architecture (i.e. the base architecture of BERT and RoBERTa models) - As using transformer models, computation resource requirement is high. It takes several days to complete several epochs.
https://github.com/oransum/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/n3a9/CourseProject	CS410 Documentation.pdf	"Text Classification Competition - Twitter Sarcasm Detection CS410 Fall 2020 Neeraj Aggarwal (noa2@illinois.edu), Samarth Keshari (keshari2@illinois.edu), Rishi Wadhwa (rishiw2@illinois.edu) Background The goal of this project is to build a model that can perform Sarcasm Detection on twitter data. The trained model should have a F1-score above 0.723 on test data. Solution Approaches Machine Learning Standard machine learning requires us to manually select a set of relevant features and then train a machine learning model based on the features. As part of this project we trained multiple classifiers including a Random Forest Classifier using Bag-of-Words of bigrams as the feature representation. The random forest classifier performed best in this category, therefore, it was trained on many different hyperparameters. We eventually settled on 1000 trees, and a 46% confidence threshold to define as sarcasm. Deep Learning In contrast to the standard machine learning approach in deep learning we skip the manual step of feature extraction and directly feed the data to the deep learning algorithm which automatically learns features. The trained deep learning model is then used to perform the prediction task. In this project we used Convolutional Neural Network(CNN) based architecture along with a pre-trained BERT Tokenizer to generate the token ids. Following are the different elements of the software built as part of this approach Model Architecture The figure below shows the architecture of the deep neural network model implemented. The input text to the neural network is first tokenized using a pre-trained BERT tokenizer. The tokenized text is then fed to an embedding layer, followed by three parallel convolutional layers. Finally, the outputs from the convolutional layer become input to a fully connected layer followed by the softmax output layer. Implementation Comparison Below table compares the F1-scores of the tuned models based on both machine learning and deep learning approaches Both the tuned models are able to achieve the goal to get F1 scores above the baseline of 0.723. Software Implementation Machine Learning As stated above, we employ pre-processing on the text data and then use the TF-IDF with n_gram = (1,2) to represent text. The TF-IDF features are then used for training the model. The trained model is finally used to perform the inference. Approach Algorithm Precision Recall F1 Deep Learning Convolutional Neural Networks 0.6227867590 0.89888888 0.7357889949977261 Machine Learning Random Forest Classifier 0.6824825174825 175 0.7265952491 849093 0.7265952491849093 Deep Learning - CNN Model Training Flow The figure below shows the flow of the training process that is followed to build the deep learning model from data - Response + Context. The training process consists of two components - 1) Data Processor 2) Model Builder. The Data Processor takes the context and response text from training data as input and generates required features as output. The Model Builder then takes the generated features as input and performs a sequence of steps to train and save the trained model. Both the components can be controlled based on the input parameters. Model Inference Flow The figure below shows the flow of the inference engine that is used to predict the results of test data The inference process makes use of the same Data Processor from training to generate the features from input test data - context + response text. The generated features are then fed to the inference engine which loads the trained model and performs the prediction and saves the results. Software Usage Installation Requirements The software is built with Python3.7.7 and uses the following packages. emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 You can automatically install all of these packages and download the source code by first cloning the repo https://github.com/n3a9/CourseProject.git. Then navigate into the project directory and run pip install -r requirements.txt Machine Learning There are 4 machine learning models that are available for usage: - Random Forest Classifier `random_forest.py` - MLP Classifier `mlp_classifier.py` - SGD Classifier `sgd_classifier.py` - Logistic Regression `logistic_regression.py` To run the machine learning models, `python [file.py]`. It will generate an `answer.txt` in `./src/machinelearning`. Deep learning - CNN APIs As part of this project, the user can call two apis * Model Training * In order to call this api, in the command terminal the user need to # Navigate into the cloned repository to the directory ../src/deeplearning # If required, change the parameters file 'params_config.json ' at ../src/deeplearning/parameters . Refer to the Parameters section  below for details about the different parameters used during the model training # Run command - python modelTrain.py # The trained model weights will be saved at ../src/deeplearning/trained-models in 'cnn_model_weight.h5 ' file Note: For the project verification purpose, model training can be performed by changing different parameters(refer to Parameters section below). Currently by default any new trained model weights will not be saved, however, caution should be taken that any new trained model weights if saved can vary the final results. * Model Inference * In order to call this api, in the command terminal the user need to # Navigate into the cloned repository to the directory ../src/deeplearning # If required, change the parameters file 'params_config.json ' at ../src/deeplearning/parameters . Refer to the Parameters section  below for details about the different parameters used during the model inference. # Run command - python modelInference.py # The final predictions will be saved at ../src  in 'answer.txt ' file Note: For project verification purpose run only the Model Inference Parameters Below is the list of parameters that are used during the model training and inference process. Refer to 'params_config.json ' file in the cloned repository at twitter-sarcasm-detection/src/deeplearning/parameters Name Description Used In n_last_context Number of last entries from in the context list Training + Inference data-path Path to folder storing the train and test data files Training + Inference train-data-filename Name of the train file in .jsonl format Training test-data-filename Name of the test file in .jsonl format Inference processed-data-path Path to folder storing the processed train and test data files Training + Inference processed-train-data-filename Name of the processed train file in .csv format Training processed-test-data-filename Name of the processed test file in .csv format Inference features-path Path to folder storing the train and test features files Training + Inference features-train-filename Name of the train features file in .json format Training features-test-filename Name of the test features file in .json format Inference trained-model-save Flag to indicate that the weights of the trained model should be saved. By default the model will not be saved. If required, set the flag to ""X"". Training trained-model-path Path to the folder storing the trained model weights Training trained-model-weight-filename Name of the file storing the trained model weights in .h5 format Training train_test_split % of records that are needed for validation during model training. The value of this parameter should be between (0,1) Training embedding_dimensions Number of dimensions in the embedding layer of the model Training cnn_filters Number of CNN filters in the CNN layers of the model Training dnn_units Number of neurons in the fully connected layer of the model Training dropout_rate Dropout rate for the fully connected layer of the model Training verbose Training References * https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNe PM * https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-kera s n_epochs Number of epochs for model training Training batch_size Batch size for model training Training prediction-threshold Model predictions for test data are in terms of probabilities. For a particular test sample, if the prediction probability is above this threshold value, then the test sample is flagged as SARCASM otherwise NON-SARCASM Inference answer-file Path + filename of the final results file in .txt format Inference"
https://github.com/n3a9/CourseProject	CS410 Project Presentation.pptx	Team Simple Neeraj Aggarwal, Samarth Keshari, Rishi Wadhwa Our Project: Text Classification Competition Goal = to build a model that can perform Sarcasm Detection on twitter data Evaluation Target = the model should have F1 score greater than 0.723 Our Approach Approach-1 = Traditional Machine Learning Input Text Representation = Bag-of-Words (bi-gram) Algorithms = Random Forest, Logistic Regression, SGD(SVM) & MLP Approach-2 = Deep Learning Input Text Representation = Pre-trained BERT tokenizer Architecture = Convolutional Neural Networks Results Installation Python(version 3.7 - 3.8) Visit https://github.com/n3a9/CourseProject Download or Clone the Code Required Packages(pip install) emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 * You can automatically install all of these packages by navigating into the project directory './src' and then running command pip install -r requirements.txt Code Repository Structure Data in 'data' directory where we can find train and test files Source Code in 'src' directory Machine Learning code in 'machinelearning' directory - './src/machinelearning' Deep Learning in 'deeplearning' repository - './src/deeplearning' Usage - Machine Learning There are 4 machine learning models that are available for usage: - Random Forest Classifier `random_forest.py` (best result classifier) - MLP Classifier `mlp_classifier.py` - SGD(SVM) Classifier `sgd_classifier.py` - Logistic Regression `logistic_regression.py` To run the machine learning models, navigate to the machine learning directory - `./src/machinelearning` - and execute command`python [file.py]`. For example executing command 'python random_forest.py' in the terminal will run Random Forest Classifier. It will generate an `answer.txt` in `./src/machinelearning` Usage - Deep Learning There are 2 programs that are available for usage: - Model Training `modelTrain.py` - Model Inference `modelInference.py` There is a list of parameters that are used to control the training and inference programs. These parameters can be adjusted to influence the behavior of the 2 programs. *Refer to documentation to learn more about these parameters Usage - Deep Learning(Training) To run the model training, navigate to the deep learning directory - `./src/deeplearning` - and execute command`python modelTrain.py` This will save the trained model weights as `cnn_model_weight.h5` in `./src/machinelearning/trained-models` * Currently, by default any new trained model weights will not be saved. Also, caution should be taken that any new trained model weights, if saved can vary the final results Usage - Deep Learning(Inference) To run the model inference, navigate to the deep learning directory - `./src/deeplearning` - and execute command`python modelInference.py` It will generate an `answer.txt` in `./src/deeplearning` Thank You!!! For any questions please contact Neeraj Aggarwal (noa2@illinois.edu) Samarth Keshari (keshari2@illinois.edu) Rishi Wadhwa (rishiw2@illinois.edu)
https://github.com/n3a9/CourseProject	CS410 Project Proposal.docx	CS410 Project Proposal 1. Team Captain: Neeraj Aggarwal, noa2 Rishi Wadhwa, rishiw2 Samarth Keshari, keshari2 2. Competition: text classification competition 3. Yes, we are prepared to learn state-of-the-art neural network classifiers. We've heard of Tensorflow, PyTorch, and LSTM. We have relevant experience with text classification, neural networks, relisiency. We aren't experts, but we have dabbled in some of these methods in our pasts. 4. Programming Language: Python
https://github.com/n3a9/CourseProject	cs410_progress_report.pdf	Sarcasm Detection (Project Progress) Neeraj Aggarwal(noa2@illinois.edu), Samarth Keshari(keshari2@illinois.edu), Rishi Wadhwa (rishiw2@illinois.edu) Progress As part of the project work we have used both Machine Learning and Deep Learning approaches to solve the problem. Based on our analysis we found that among different machine learning algorithms, Random Forest Classifier performed best after some hyperparameter tuning. Below are metrics obtained after running different algorithms. After this, we decided to change the threshold in which we determined whether the tweet was sarcastic or not, by lowering the confidence level necessary. Approach Algorithm Precision Recall F1 Machine Learning LogisticRegression 0.672395273899 0333 0.695555555 5555556 0.6837793555434188 Machine Learning SGDClassifier 0.676282051282 0513 0.703333333 3333334 0.6895424836601308 Machine Learning LinearSVC 0.665263157894 7369 0.702222222 2222222 0.6832432432432433 Machine Learning MLPClassifier TBD TBD TBD Machine Learning RandomForest Classifier 0.642482517482 5175 0.816666666 6666667 0.7191780821917808 From this, we found the RandomForestClassifier with 1000 trees and 0.48 threshold. Eventually, the Deep Learning based approach gave the best performance results. Both the tuned Random Forest(Machine Learning) and Convolutional Neural Network(Deep Learning) are able to get F1 scores above the baseline of 0.723. Remaining Tasks We already crossed the baseline, but if we have time we could explore approaches in Deep Learning and improve CNN by tuning the hyperparameters. We could also explore different tokenization techniques and draw different insights from the tweets. For example, tokening and separating emojis and hashtags may allow us to bring significant improvements to sarcasm detection. Threshold F1 0.5 0.7191780821917808 0.4 0.7126436781609196 0.48 0.7265952491849093 0.46 0.7231386535889435 0.44 0.7187904967602592 Approach Algorithm Precision Recall F1 Deep Learning Convolutional Neural Networks 0.6227867590 0.89888888 0.7357889949977261 Challenges and Issues None.
https://github.com/n3a9/CourseProject	README.md	"CourseProject Twitter sarcasm detection by Samarth Keshari, Rishi Wadhwa, Neeraj Aggarwal. Installation The software is built with Python3.7.7 and uses the following packages. emoji==0.6.0 pandas==1.1.3 nltk==3.5 tensorflow==2.3.1 numpy==1.18.5 Keras==2.4.3 scipy==1.5.2 demoji==0.3.0 bert-for-tf2==0.14.7 scikit_learn==0.23.2 You can automatically install all of these packages by first cloning this repo. Then navigate into the project directory and run pip install -r requirements.txt. Usage Machine Learning There are 4 machine learning models that are available for usage: Random Forest Classifier random_forest.py MLP Classifier mlp_classifier.py SGD Classifier sgd_classifier.py Logistic Regression logistic_regression.py To run the machine learning models, python [file.py]. It will generate an answer.txt in ./src/machinelearning. Deep Learning There are two APIs that you can use. Model Training cd src/deeplearning. If required, change the parameters file params_config.json at in /parameters. Refer to the parameters section below for details about the different parameters used during the model training. To run, python modelTrain.py. The trained model weights will be saved at ./trained-models in cnn_model_weight.h5 file. Note: For the project verification purpose, model training can be performed by changing different parameters(refer to Parameters section below). Currently by default any new trained model weights will not be saved, however, caution should be taken that any new trained model weights if saved can vary the final results. Model Inference cd src/deeplearning. If required, change the parameters file params_config.json at in /parameters. Refer to the parameters section below for details about the different parameters used during the modelinference. To run, python modelInference.py. The final predictions will be saved at ./src in answer.txt file. Note: For project verification purpose run only the Model Inference. Parameters Below is the list of parameters that are used during the model training and inference process. Refer to params_config.json file in the cloned repository at ./parameters | Name | Description | Used In | | ----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------- | | n_last_context | Number of last entries from in the context list | Training + Inference | | data-path | Path to folder storing the train and test data files | Training + Inference | | train-data-filename | Name of the train file in .jsonl format | Training | | test-data-filename | Name of the test file in .jsonl format | Inference | | processed-data-path | Path to folder storing the processed train and test data files | Training + Inference | | processed-train-data-filename | Name of the processed train file in .csv format | Training | | processed-test-data-filename | Name of the processed test file in .csv format | Inference | | features-path | Path to folder storing the train and test features files | Training + Inference | | features-train-filename | Name of the train features file in .json format | Training | | features-test-filename | Name of the test features file in .json format | Inference | | trained-model-save | Flag to indicate that the weights of the trained model should be saved. By default the model will not be saved. If required, set the flag to ""X"". | Training | | trained-model-path | Path to the folder storing the trained model weights | Training | | trained-model-weight-filename | Name of the file storing the trained model weights in .h5 format | Training | | train_test_split | % of records that are needed for validation during model training. The value of this parameter should be between (0,1) | Training | | embedding_dimensions | Number of dimensions in the embedding layer of the model | Training | | cnn_filters | Number of CNN filters in the CNN layers of the model | Training | | dnn_units | Number of neurons in the fully connected layer of the model | Training | | dropout_rate | Dropout rate for the fully connected layer of the model | Training | | verbose | | Training | | n_epochs | Number of epochs for model training | Training | | batch_size | Batch size for model training | Training | | prediction-threshold | Model predictions for test data are in terms of probabilities. For a particular test sample, if the prediction probability is above this threshold value, then the test sample is flagged as SARCASM otherwise NON-SARCASM | Inference | | answer-file | Path + filename of the final results file in .txt format | Inference | References https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras"
https://github.com/jessehenn/CourseProject	README.md	CourseProject Overview Team: PiazzaEd Topic: Integrating Piazza with Educational Web Project Proposal Project Proposal Project Progress Report Project Progress Report Final Artifacts Written Documentation Source Code Repository Demo Video* Implementation Details Overview Video* Survey Overview Video Survey Results in CSV *NOTE:The OneDrive movie viewer may show the video as blurry. Download it and view it locally for best results.
https://github.com/enaena633/CourseProject	Mining Causal Topis-paper-final-kim.pdf	"Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Hyun Duk Kim Dept. of Computer Science University of Illinois at Urbana-Champaign hkim277@illinois.edu Malu Castellanos Information Analytics Lab HP Laboratories malu.castellanos@hp.com Meichun Hsu Information Analytics Lab HP Laboratories meichun.hsu@hp.com ChengXiang Zhai Dept. of Computer Science University of Illinois at Urbana-Champaign czhai@illinois.edu Thomas Rietz Dept. of Finance The University of Iowa thomas-rietz@uiowa.edu Daniel Diermeier Kellogg School of Management Northwestern University d-diermeier @kellogg.northwestern.edu ABSTRACT Many applications require analyzing textual topics in conjunction with external time series variables such as stock prices. We de- velop a novel general text mining framework for discovering such causal topics from text. Our framework naturally combines any given probabilistic topic model with time-series causal analysis to discover topics that are both coherent semantically and correlated with time series data. We iteratively refine topics, increasing the correlation of discovered topics with the time series. Time series data provides feedback at each iteration by imposing prior distribu- tions on parameters. Experimental results show that the proposed framework is effective. Categories and Subject Descriptors I.2.7 [Artificial Intelligence]: Natural Language Processing--text analysis; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--Linguistic processing; H.3 [Information Storage and Retrieval]: Information Search and Retrieval Keywords Causal Topic Mining, Iterative Topic Mining, Time Series 1. INTRODUCTION Analyzing and understanding text data can provide useful infor- mation. Probabilistic topic models [4, 8] have proven very useful for mining text data in a range of areas including opinion analysis [11, 17], text information retrieval [19], image retrieval [9], natural language processing [6], and social network analysis [12]. Most existing topic modeling techniques focus on text alone. However, text topics often occur in conjunction with other vari- ables through time. Such data calls for integrated analysis of text and non-text time series data. The causal relationships between the two may be of particular interest. For example, news about Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM'13, Oct. 27-Nov. 1, 2013, San Francisco, CA, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2263-8/13/10 ...$15.00. http://dx.doi.org/10.1145/2505515.2505612. companies can affect stock prices. How do particular topics lead to increasing or decreasing prices? Can topics help forecast future price changes? Similar examples occur in many domains. How do a company's product sales rise and fall in response to text in adver- tising or reviews? Understanding the causal relationships can im- prove future sales strategies. In election campaigns, news analysis news may explain why a candidate's support changes significantly in the polls. Understanding the causal relationships can improve future campaign strategies. While there are many variants of topic models [2, 3, 18], no existing model incorporates jointly text and associated ""external"" time series variables to find causal topics. A precise, conceptual definition of ""causal topics"" is beyond our scope here. We make a computational definition: a causal topic is a semantically coherent topic from text data that has strong, possibly lagged, associations with a non-textual time series variable. This allows for two-way relationships: topics may affect the time series and/or vice versa. Thus, we identify potential causal relationship. Interpretation will vary with the exact application and modeling or knowledge specific to the application. Our method can help an analyst quickly identify a small set of possibly causal topics for further analysis.1 A basic approach to identifying causal topics is to: (1) find top- ics with topic modeling techniques then (2) identify causal topics using correlations and causality tests. However, because the topic modeling step ignores the time series data, it does not focus ex ante on topics that are particularly closely related to it. Even if two time series variables are completely different (e.g., stock prices for an airline vs. national election polls), the candidate topic set would not change. Some topics from this single set may happen to be cor- related with any given time series, but the modeling stage does not seek particularly relavent information in any way. We propose a novel general text mining framework: Iterative Topic Modeling with Time Series Feedback (ITMTF), for discover- ing causal topics from text. ITMTF naturally combines probabilis- tic topic modeling with time series causal analysis to uncover topics that are both coherent semantically and correlated with time series data. ITMTF can accommodate any topic modeling technique and any causality measure. This generality is a significant advantage and which enables users to easily adapt different topic models and causality measures as needed for specific applications. Thus, the framework can support many different applications. ITMTF iter- atively refines a topic model, gradually increasing the correlation of discovered topics with the time series data through a feedback 1Our definition allows using ""correlation"" and ""cause"" inter- changeably as convenient. mechanism. In each iteration, the time series data informs a prior distribution of parameters that feeds back into the topic model. Thus, the discovered topics are dynamically adapted to fit the pat- terns of different time series data. We evaluate ITMTF on a news data set with multiple stock price time series, including stock prices from the Iowa Electronic Mar- kets and those of two large US companies (American Airlines and Apple). The results show that ITMTF can effectively discover causal topics from text data and the iterative process improves the quality of the discovered causal topics. The main contributions of this paper include: 1) We introduce a novel problem setup for discovering causal topics from text data with supervision by time series data, which has many applications in multiple domains. 2) Our general framework for solving this new problem naturally combines any probabilistic topic model with any correlation or causal analysis method for time series data. This novel iterative feedback algorithm uses time series data to influence topic discovery from text by iteratively imposing a time-series- based prior on topic model parameters. 3) We evaluate the pro- posed framework and algorithms on real application data sets and show that the proposed framework can effectively exploit time se- ries data to supervise and improve topic discovery from text data. 2. RELATED WORK There are two basic topic models: Probabilistic Latent Seman- tic Analysis (PLSA) [8] and Latent Dirichlet Analysis (LDA) [4]. Both focus on word co-occurrences. Recent advanced techniques analyze the dynamics of topics on a time line [2, 18]. However, they do not conduct integrated analyses of topics and external vari- ables; the topic analysis is separate from the external time series. There is also some effort to incorporate external knowledge in modeling: Supervised topic models [3] and Labeled LDA [16]. The former uses supervised LDA which incorporates a reference value (e.g. movie review article with movie rating) in the modeling pro- cess. The modeled topic shows better prediction power on the ref- erence value than simple LDA. The latter extends supervised LDA associate categorical labels and even text labels for topics. Another way of incorporating external knowledge is to use conjugate prior probabilities in the topic modeling process [13]. Topic Sentiment Mixture (TSM) modeles positive and negative sentiment topics us- ing seed sentiment words such as ""good"" or ""bad."" While these methods show that topic mining can be guided by external vari- ables, none achive our objective of capturing the correlation struc- ture between text topics and external time series variables. More- over, while these models are specialized for supervision with spe- cific external data, our general approach can flexibly combine any reasonable topic model with any causal analysis method. Research on stock prediction using financial news content also relates to our work [14]. Such research typically identifies the most predictive words and labels news according its effect on stock prices on a specific day using a supervised regression or a classi- fication problem setup. In contrast, we search for general causal topics with unsupervised methods. Granger testing [7] is popular for testing causality in economics using lead/lag relationships across multiple time series. Recent ev- idence shows that Granger tests can be used in an opinion mining context: predicting stock price movements with a sentiment curve [5]. However, Granger testing has not been used directly in text mining and topic analysis. [10] provides a very brief description of a demo system based on our framework. This focuses on describing the system components and sample results. Here, we describe the general problem and framework in detail and evaluate the algorithms rigorously. 3. MINING CAUSAL TOPICS IN TEXT WITH SUPERVISION OF TIME SERIES DATA We formulate a new research problem: causal topic mining. Con- sider time series data x1, ..., xn, with time stamps t1, ..., tn, and a collection of time stamped documents from the same period, D = {(d1,td1), ..., (dm, tdm)}. The goal is to discover a set of causal topics T1, ..., Tk with associated time lags L1, ..., Lk. A causal topic Ti with time lag Li is a topic that is semantically coherent and has a strong correlation with the time series data with time lag Li. Note that Li can be positive or negative, corresponding to top- ics that might cause, or might be caused by, time series data. A natural first step is to use existing work on topic modeling to identify generally coherent topics and then find topics that correlate highly with the external time series variables. However, this has a drawback: topic formation is completely independent of the time series data. Regardless of how much two time series differ, our candidate set of topics to choose from remains exactly the same. This is clearly non-optimal and leads us to our iterative solution. 4. ITERATIVE TOPIC MODELING WITH TIME SERIES FEEDBACK We have two criteria to optimize: topic coherence and topic cor- relation. We want to retain the generality of the topic modeling framework while extending it allow the time series variable to influ- ence topic formation so we can optimize both criteria over a more flexible topic space. 4.1 Causal analysis with time series data Potential ""causal"" relationships between times series are identi- fied through contemporaneous and/or lagged correlation measures (e.g., Granger tests). The correlation lag structure suggests direc- tional causality. If current observations in time series A correlated with later observations in B, A is said to ""cause"" B. A simple and very common measure uses Pearson correlations, contemporaneously or with leads and lags. Correlations range from -1 to +1 with the sign indicating the direction of correlation and can be used as ""impact"" measures here. A correlation significance depends on its value and the number of observations. Granger tests are more structured measures of causality, measur- ing statistical significance at different time lags using auto regres- sion to identify causal relationships. Let yt and xt be two time series. To see if xt ""Granger causes"" yt with maximum p time lag, run the following regression: yt = a0 + a1yt-1 + ... + apyt-p + b1xt-1 + ... + bpxt-p. Then, use F-tests to evaluate the significance of the lagged x terms. The coefficients of lagged x terms estimate the impact of x to y. We average the x term coefficients, p i=1 bi |p| , as an impact value. 4.2 An Iterative Topic Modeling Framework with Time Series Feedback Input: time series data X = x1, ..., xn with time stamp t1, ..., tn, and a a collection of text documents with time stamps from the same period, D = {(d1,td1), ..., (dm, tdm)}, Topic modeling method M, Causality measure C, a parameter tn (how many topics to model) Output: k potentially causal topics (k<=tn): (T1, L1), ..., (Tk, Lk) Topic modeling method M identifies topics. The causality mea- sure C gives significance measures (e.g. p-value) and impact ori- entation. Figure 1 illustrates our iterative algorithm. It works as follows: 1. Apply M to D to generate tn topics T1, .., Ttn 2. Use C to find topics with significance values sig(C, X, T) > g (e.g. 95%). Let CT be the set of candidate causal topics with lags: CT = {(Tc1, L1), ..., (Tck, Lk)}. 3. For each candidate topic in CT, apply C to find most signif- icant causal words among top words w  T. Record the im- pact values of these significant words (e.g., word-level Pear- son correlations with time series variable). 4. Define a prior on the topic model parameters using signifi- cant terms and their impact values. (a) Separate positive impact terms and negative impact terms. If one orientation is very weak (< d%, e.g. 10%), ignore the minor group. (b) Assign prior probability proportions according to sig- nificance levels. 5. Apply M to D using the prior obtained in step 4 (this injects feedback signals and guides the topic model to form topics that are more likely correlated with the time series) 6. Repeat 2-5 until satisfying stopping criteria (e.g. reach topic quality at some point, no more significant topic change). When the process stops, CT is the output causal topic list. Figure 1: Overview of iterative topic modeling algorithm ITMTF considers the non-textual time series data in the text min- ing process to find topics that are more highly correlated to non- textual data than general modeling systems. Moreover, ITMTF iteratively improves modeling results by considering interactions between the text and time series data at both topic and word levels. After identifying causal topics, ITMTF shifts to word level corre- lations between the text and external time series data. It also im- proves the topic modeling process by splitting positively and neg- atively impacting terms into different topics. Because generating and testing all the word time series is inefficient, ITMTF focuses only on the words with the highest frequencies in the most highly correlated topics discovered in each iteration. The ideal set of causal topics should have tight relationships with the external time series and high topic quality. Traditional topic modeling algorithms form topics based on word coherences in the text data, while causality tests can filter out non-causal topics. Fo- cusing exclusively on one criterion sacrifices the other. Our itera- tive process is a greedy approximate solution to the two-item max- imization problem. It takes turns optimizing each criterion. The prior formation based on causality attempts to optimize causality while an topic modeling optimizes coherence of topics. 4.2.1 Topic-level Causality Analysis From topic modeling results, we generate a topic curve over time by computing each topic's coverage on each time unit (e.g., one day). Consider a weighted coverage count. Specifically, compute the coverage of a topic in each document, p(topicj|Documentd). This is simply the parameter th(d) j estimated in the modeling pro- cess. Estimate the coverage of topic j at ti, tcj i as the sum of th(d) j over all the documents with ti time stamp. Call the list of tcj for all the time stamps the topic stream TSj: tcj i =  d with ti th(d) j , TSj = tcj 1, tcj 2, ..., tcj n This creates a topic stream time series that, combined with the non- textual time series data, lends itself to standard time series causality measures C and testing. Selecting lag values is important. One possibility is to use the most significant lag within a given maximum. For example, if we want to find causal topics within 5 days, we can choose the lag within 5 days with the highest significance. If we want to focus on yesterday's effect, we can choose a fixed lag of 1. The specific choice depends on the specific aims of an application. 4.2.2 Word-level Causality and Prior Generation Based on topic causality scores, we choose a subset of promis- ing topics with the highest causality scores and further analyze the words within each topic to provide feedback for the next iteration by generating topic priors. Specifically, for each significant topic, we check whether the top words in the topic are also significantly correlated with the external time series. For each word, we gener- ate a word count stream WSw by counting frequencies in the input document collection for each day: wci =  d with ti c(w, d) , WSw = wc1, wc2, ..., wcn where c(w, d) is the count of word w in document d. Then we measure correlations and significance between word streams and the external non-textdual time series. This identifys words that are significantly correlated and their impact values. Intuitively, we want to emphasize significant topics and words in our next topic modeling iteration to focus in more promising topic space. To do this, we generate topic priors for significant words in significant topics. A topic prior is a Dirichlet distribution that fa- vors topics assigning high probabilities to the identified significant words in significant topics. We assign prior word probabilities in proportion to the significance value of the words. This prior helps ""steer"" the topic modeling process to form/discover topics similar to the prior topics [13]. In the next topic modeling iteration, the dis- covered topics are likely to be close to the prior, which is based on the feedback from the time series variable through causality scores. In addition to keeping significant topics and words, we also want to improve topic quality. A ""good"" correlated topic has a consistent impact relative to the external time series. We want relatively con- sistent topics, those containing words that have mostly ""positive"" or mostly ""negative"" impacts on the external time series. Therefore, if one topic has both positive and negative impact words, we separate the positive and negative impact words into two topics in the prior for the next topic modeling iteration. If one of the word impact groups is much smaller than the other (e.g. the number of positive impact words < 0.1 * the number of negative impact words), we keep only one topic and set the probability of words in the smaller group zero. This ""tells"" the topic model not to include such words in the refinement of the topic. Table 1: Example of topic and word correlation analysis re- sult (left) and prior generated (right) (Sig: significance, Prob: probability WORD IMPACT SIG (%) WORD PROB social + 99 social 0.8 security + 96 security 0.2 gun - 99 gun 0.75 control - 97 control 0.25 september - 99 = september 0.1 terrorism - 97 terrorism 0.075 ... ... attack - 96 attack 0.05 good + 96 good 0.0 Suppose, among N total topics, we identify 2 topics as signifi- cantly correlated with the external time series (left of Table 1). We check correlations of the top words in these two topics. Suppose 4 and 10 words were significant for the two topics respectively. Be- cause there are both positive and negative word groups with similar sizes, we would split the first topic into two topics and assign word probabilities based on significance values. For the second topic, only one word has a different impact orientation from the others making the negative group much smaller than the positive group. Therefore, instead of making a separate negative word group topic, we exclude it from the positive word group topic by assigning it zero weight. Right side of Table 1 shows the example prior gener- ated. Another challenging is selecting a cutoff for ""top"" words in the correlation list. The simplest solution is to set a fixed cutoff, say k, and use the top k words. However, rank alone does not determine the importance of words. Importance is determined by word prob- abilities as well. For example, suppose the top three words in Topic 1, A, B, and C have probabilities 0.3, 0.25 and 0.2, respectively, and the top three words in Topic 2, D, E, and F have probabilities 0.002, 0.001 and 0.0001. In this case, A, B and C are very impor- tant in Topic 1. However, D, E, and F combined only represent a small part of Topic 2. Hence, Topic 2 may require more words to be considered. We address this by using a cumulative probability mass cutoff, probM. We use all words whose accumulated probabilities are within a cutoff. Formally, for each topic Tj = (w1, ph(j) w1), ..., (w|V |, ph(j) w|V |) (|V | is the number of words in the input data set vocabulary), when items are sorted by ph(j) wi , we can add the top ranked word to the top word list TW without violating the constraint  wT W ph(j) w <= probM where TW = (w1, ..., wm). That is,  wT W ph(j) w + ph(j) wm+1 > probM. With top word TW = (w1, ..., wm) and significance value of each word sig(C, X, w) , the topic prior ph' w can be com- puted by the following formula: ph' w = sig(C, X, w) - g m j=1(sig(C, X, wj) - g) where g is a significance cutoff (e.g. 95%). 4.2.3 Iterative Modeling with Feedback Using the new prior, we remodel topics. New topics will be guided by priors, which depend on correlations with the external data. High probability words in the prior have a greater impact in the modeling results and words with zero probability are not in- cluded in the topic. By repeating the process of topic modeling, correlation analysis, and prior generation, the resulting topics are likely more highly correlated with the external time-series. The strength of the prior in each iteration is set by a parameter u in the modeling process [13]. With u = 0, modeling would not consider the prior information at all (making it the same as independent modeling). With a very high u, words in the prior are very likely to appear in the topic modeling results. (We study this parameter's influence in our experiments.) While we observe correlations between non-textual series and both word streams and topic streams, we do not compute correla- tions for all word streams. Word level analysis would give us finer grain signals. However, generating all the word frequency time series and testing correlations would be very inefficient. By nar- rowing down to significant topics first, we can prune the number of words to test. This increases efficiency and effectiveness. 5. EVALUATION 5.1 Experiment Design We evaluate the proposed algorithms on the New York Times data set 2 with multiple stock time series data. In one experiment, we examine the 2000 U.S. Presidential elec- tion campaign. The input text data comes from New York Times 2http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2008T19 articles from May through October of 2000. We filter them for key words ""Bush"" and ""Gore,"" and use paragraphs mentioning one or both words. The idea is to find specific topics which caused sup- port for Bush or Gore to change and not simply election related topics. As a non-textual time series input, we use prices from the Iowa Electronic Markets (IEM)3 2000 Presidential Winner-Takes- All Market [1]. In this on-line futures market, prices forecast the probabilities of candidates winning4 the election. We follow stan- dard practice in the field and use the ""normalized"" price of one candidate as a forecast probability of the election outcome: (Gore price)/(Gore price + Bush price). In another experiment, we use stock prices of American Airlines and Apple and the same New York Times text data set with longer time period, but without keyword filtering, to examine the influence of having different time series variables for supervision. While the framework is general, comparing different topic mod- els is not the focus of our paper. So, we only used one topic model: PLSA implemented based on the Lemur information re- trieval toolkit.5 For correlation measures, we use both contem- poraneous Pearson correlation coefficients and Granger tests. For Granger tests, we use the R statistical package6 implementation. Granger tests require stationary time series as inputs. To make the input series stationary, we smooth with a moving average filter with window size 3 (average with adjacent values) and use first differ- ences (xt) - (xt-1) of each series. We test causality with up to 5 day lags and pick the lag which shows highest significance. 5.1.1 Measures of Quality We report two measure the quality for mined topics: causality confidence and topic purity. For causality confidence, we use the significance value (i.e. p-value) of the Granger test between the text stream and external variable. For topic purity, we use the impact orientation distribution of significant words. If all the significant words in one topic have the same orientation, it has 100% purity. If significant words are evenly split by positive and negative impact, it has 0% purity. We calculate the entropy of significant word dis- tributions and normalize it to the [0, 100] range. Thus, the Purity of topic T is defined as: pProb = the number of positive impact words the number of significant words nProb = the number of negative impact words the number of significant words Entropy(T) = pProb * log(pProb) + nProb * log(nProb) Purity(T) = 100 + 100 * Entropy(T). We report average causality confidence and purity for topics with more than 95% significance. Thus, when there are more significant topics, this measure may be penalized. However, because measur- ing general utility of significant topics is meaningful from a user perspective, we do not adjust this measure. 5.1.2 Baseline The first iteration of our method is based on topic modeling with- out guidance from the time series and, thus, is a natural baseline. Comparing iterations and final results to this shows the benefit of iterative topic mining. 5.1.3 Parameter Tests We test two parameters for effects on performance. The first is the number of topics modeled (tn). A large number of topics may 3http://tippie.uiowa.edu/iem/ 4""Winning"" as defined by the IEM is taking the majority of the two-party popular vote 5http://www.lemurproject.org/ 6http://www.r-project.org/ help identify more specific topics and more coherent (higher purity) topics. However, topics that are too specific result in data sparse- ness that reduces the power of significance testing. A small number of topics gives the opposite effects: topics are likely to have higher statistical significance, but would have lower purity. Also, because many meaningful topics may be merged into a single topic, topics may be too coarse to interpret easily. The second parameter is the strength of the prior (u). A stronger topic prior would guarantee prior information is reflected in the next topic modeling iteration. However, if the initial topic modeling (which uses random initiation without a prior) ends up at a poor local optimum, a strong prior may keep the process there, resulting in poor topics. Strong priors may also exacerbate spurious correlation resulting from noise in the first round. In contrast, weaker priors allow a less restricted iteration of topic modeling, reducing these negative effects. However, positive signals would also have weak impact. Because prior research gives no guidance for selecting these parameters, we examine how they affect the performance of our algorithm. 5.2 Experiment Results 5.2.1 Sample Results 2000 Presidential Election: Table 2 shows sample results from the 2000 U.S. Presidential election. It shows the top three words of sig- nificant causal topics mined (Pearson correlation, tn=30, u=50, 5th iteration). The result reveals several important issues from the cam- paigns, e.g. tax cuts, abortion, gun control and energy. Such topics are also cited in political science literature [15] and Wikipedia7 as important election issues. This shows that our iterative topic min- ing process can converge to issues expected to affect the election. Table 2: Significant topic list of 2000 Presidential Election (Each line is a topic. Top three probability words are dis- played.) TOP THREE WORDS IN SIGNIFICAN TOPICS tax cut 1 screen pataki giuliani enthusiasm door symbolic oil energy prices pres al vice love tucker presented partial abortion privatization court supreme abortion gun control nra news w top Stock Time Series, AAMRQ vs. AAPL: To study how different time series affect the topics discovered, we compare the topics dis- covered from the same text data set using two different time series. We use New York Times articles from July 2000 through Decem- ber 2001 as the text input. We use American Airlines (AAMRQ) and Apple (APPL) stock prices as external time series. American Airlines' stock (travel industry) dropped significantly in September 2001 because of the 9/11 terrorist attack, while Apple stock (IT in- dustry) was less affected. We start with the same modeled topic list at the first iteration. Thus, any differences in modeled topics are from feedback of the external time series. Table 3 shows the top three words of significant topics mined us- ing the two different external time series after three rounds (tn=30 and u=1000). Topics associated with American airlines include clearly relevant words such as ""airlines airport air"" and ""united trade terrorism."" One topic is clearly about the terrorist attack. Topics associated with Apple differ dramatically. Revelant refer- ence such as ""computer technology software"" and ""internet com web"" reference Apple's IT industry. 7http://en.wikipedia.org/wiki/United_ States_presidential_election,_2000#General_ election_campaign Table 3: Significant topic list of two different external time se- ries: AAMRQ and AAPL (Each line is a topic. Top three prob- ability words are displayed.) AAMRQ AAPL russia russian putin paid notice st europe european germany russia russian europe bush gore presidential olympic games olympics police court judge she her ms airlines airport air oil ford prices united trade terrorism black fashion blacks food foods cheese computer technology software nets scott basketball internet com web tennis williams open football giants jets awards gay boy japan japanese plane moss minnesota chechnya ... This example also shows a limitation of our algorithm. In ad- dition to clearly relevant topics, there appear other general topics (e.g., sports). This task presents more challenges than the 2000 U.S. Presidential election example because of the diversity in text data and long time period. While we use text articles which are related to candidates for the Presidential election case, we use all articles in the time period for this example. Greater topic diversity can lead to more spurious correlation independent of real causality. Moreover, our analysis is over 18 months and the algorithm mea- sures correlation over the entire time period. Therefore, if an event is only locally correlated, it may not be selected in the final output topic list. How to measure and deliver local correlation feedback remains for future work. Despite these difficulties, our algorithm shows how different time series inputs select different topics relevant to themselves using the same text data and initially modeled topics. Thus, our algorithm can effectively guide topic modeling. Pre-filtering relevant articles and shorter time periods may yield better results. Moreover, while some topics seem seem unrelated to stock prices at first blush, they may reveal unexpected, but meaningful, relationships. 5.2.2 Quantative evaluation results We ask two questions: 1) Is our joint mining algorithm more effective than simply combining an existing topic model with a time series analysis method in a straightforward way? 2) Is the feedback mechanism in the proposed framework beneficial? To answer both, we study how results change between the baseline method (with no feedback) through multiple iterations. Figure 2a shows performance evaluation results with different us using Granger tests. Average causality confidence increases over iterations regardless of the strength of feedback, u. The perfor- mance improvement is particularly notable between the first itera- tion (baseline with no feedback) and the second iteration (with one feedback round). Clearly, feedback is beneficial. After the second iteration, performance shows slower improvement. Later rounds appear to fine tune topics resulting from the first feedback round. The average purity graph shows mixed results. For small u values (u=10, 50, 100), iterations do not always improve purity. With higher u values (u=500, 1000), average purity improved from the first iteration to the second. Furthermore, for the highest u value (u=1000), it showed a steady increase. Weak us would al- low topic modeling more room to explore variations regardless of prior. Therefore, the purity improvement may not be guaranteed in each iteration. Thus, high u values lead to higher increases in pu- rity. Further, reported purity is the averaged purity value of all the significant topics found. The number of significant topics increases dramatically between the first and second iteration. Thus, the drop in average purity may not be a negative sign. Still, higher us ensure purity improvement. Figure 2b shows performance evaluation results with different topic numbers (tn) using Granger tests. Initially, small topic num- bers (tn=10, 20) had higher confidence levels than large topic num- bers (tn=30, 40). Intuitively, the statistical signal is stronger with (a) With different u (b) With different tn Figure 2: Causality confidence (left) and purity (right) with dif- ferent parameter settings over iteration (Presidential election data, Granger tests) small topic numbers, while sparseness associated with large topic numbers reduces statistical strength. However, with more itera- tions, relative order changes. Thus, the feedback process helps overcome sparseness problem. Significantly correlated topics and words are kept by iterations of priors and topic modeling iterations add coherence to them. Thus, in the end, the number of topics has little effect on average confidence. In general, modeled topics with larger tn show higher purity than with small tn. As expected, each topic would is specific and the chance of combining multiple real topics in one is smaller. There- fore, topics likely have better purity.8 To show that the improvement not an artifact of noise in topic modeling, we test significance of the performance improvement between the first and second iteration. We execute 20 separate tri- als with random initiation and applied ITMTF (tn=30, u=1000). Paired t-test between the first and second iterations showed >99% significance for each measure (t-value: 3.87 for average confidence, 14.34 for average purity). Thus, feedback significantly improves average correlation and topic quality over simple topic modeling. Beyond the first feedback round, causality improvements are rela- tively small. Thus, in practice, one feedback round may be suffi- cient. Overall high u values clearly improve topic quality. Results on tn values are less clear. Next, we describe an approach and experi- ment results for finding the optimal tn. 5.2.3 Optimal Number of Topics In practice, selecting the appropriate number of topics (tn) presents challenges. We propose a ""variable topic number"" approach. Our algorithm naturally splits topics by word impacts in each iteration. Therefore, we can start with an small tn, let the algorithm split top- ics in each prior and use the increased number of topics in the next iteration. We can also add some buffer topics in some or all rounds to give topic modeling room to explore more topics. For example, 7 out of 10 initial topics may be deemed causal in a round and 5 out 8We performed the same series of tests using Pearson correlation coefficients. In general, the results are similar, but not reported because of space limitations. of 7 may be split. The next prior will include 12 topics. Adding 5 more buffer would result in 17 topics for the next iteration. Topics tend to have high causality with small tn. Likely, many will be retained as causal from the beginning. With iteration, top- ics are split. While the number of topics rises, the proportion of causal topic will likely fall. We suggest stopping when the number of causal topics starts fall relative to the previous iteration, which means topic splitting hurts more than iterative modeling improves causal topic identification. When we actually apply this starting with tn = 10, and the number of significant causal topics starts to decrease after 30 total topics, so we set tn=30. We test this variable topic number algorithm against fixed topic numbers in our topic number analysis. TNVar in Figure 2(b) shows average confidence and purity compared to the fixed tn methods. In both average confidence and purity, the variable topic number approach performs well, proving its efficacy. 6. CONCLUSIONS Here, we present a novel general text mining framework: It- erative Topic Modeling with Time Series Feedback (ITMTF) for causal topic mining. ITMTF uses text data and a non-text exter- nal time series as inputs and iteratively models topics related to changes in the external time series. Experimental results show that ITMTF finds topics that are both more pure and more highly cor- related with the external time series than typical topic modeling, especially with a strong feedback loop. The general problem of mining causal topics opens new direc- tions for future research, both theoretical and applied. ITMTF can be generalized using any topic modeling techniques and causal- ity/correlation measures desired. Our examples illustrate one of many ways to implement the framework, which can certainly be implemented in other ways. In future work, we hope to extend ITMTF's ability to identify locally correlated events. In addition, the proposed alternating optimization strategy is only a heuristic without theoretical guarantees of convergence. While, empirically, this strategy works well on tested data sets, an obvious and interest- ing extension would be to integrate topic models with time series data more tightly using a single unified objective function for opti- mization. Acknowledgments This material is based upon work supported in part by the National Science Foundation under Grant Number CNS-1027965 and by an HP Innovation Research Award. 7. REFERENCES [1] J. Berg, R. Forsythe, F. Nelson, and T. Rietz. Results from a Dozen Years of Election Futures Markets Research, volume 1 of Handbook of Experimental Economics Results, chapter 80, pages 742-751. Elsevier, 2008. [2] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning, pages 113-120, New York, NY, USA, 2006. ACM. [3] D. M. Blei and J. D. Mcauliffe. Supervised topic models. 2007. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [5] J. Bollen, H. Mao, and X.-J. Zeng. Twitter mood predicts the stock market. CoRR, abs/1010.3003, 2010. [6] J. Boyd-Graber, D. M. Blei, and X. Zhu. A topic model for word sense disambiguation. In EMNLP '07: Proceedings of the 2007 conference on Empirical Methods in Natural Language Processing, 2007. [7] C. W. J. Granger. Essays in econometrics. chapter Investigating causal relations by econometric models and cross-spectral methods, pages 31-47. Harvard University Press, Cambridge, MA, USA, 2001. [8] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR '99: Proceedings of the 1999 international ACM SIGIR conference on research and development in Information Retrieval, pages 50-57, New York, NY, USA, 1999. ACM. [9] E. Horster, R. Lienhart, and M. Slaney. Image retrieval on large-scale image databases. In CIVR '07: Proceedings of the 2007 ACM international conference on Image and video retrieval, pages 17-24, New York, NY, USA, 2007. ACM. [10] H. D. Kim, C. Zhai, T. A. Rietz, D. Diermeier, M. Hsu, M. Castellanos, and C. Ceja. Incatomi: Integrative causal topic miner between textual and non-textual time series data. In CIKM '12: Proceedings of the 2012 ACM international Conference on Information and Knowledge Management, pages 2689-2691, New York, NY, USA, 2012. ACM. [11] C. Lin and Y. He. Joint sentiment/topic model for sentiment analysis. In CIKM '09: Proceedings of the 2009 ACM international Conference on Information and Knowledge Management, pages 375-384, New York, NY, USA, 2009. ACM. [12] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link lda: joint models of topic and author community. In ICML '09: Proceedings of the 2009 annual International Conference on Machine Learning, pages 665-672, New York, NY, USA, 2009. ACM. [13] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th international conference on World Wide Web, pages 171-180, New York, NY, USA, 2007. ACM. [14] G. Mitra and L. Mitra. The handbook of news analytics in finance /. Wiley ;, Hoboken, N.J. :, 2011. [15] G. Pomper. The election of 2000: reports and interpretations. ELECTION OF. Chatham House Publishers, 2001. [16] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. Labeled lda: a supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP '09, pages 248-256, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics. [17] I. Titov and R. McDonald. A joint model of text and aspect ratings for sentiment summarization. In ACL '08: Proceedings of the 2008 annual meeting on Association for Computational Linguistics, pages 308-316, Columbus, Ohio, 2008. Association for Computational Linguistics. [18] X. Wang and A. McCallum. Topics over time: a non-markov continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD international conference, pages 424-433, New York, NY, USA, 2006. ACM. [19] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In SIGIR '06: Proceedings of the 2006 international ACM SIGIR conference on research and development in Information Retrieval, pages 178-185, New York, NY, USA, 2006. ACM."
https://github.com/enaena633/CourseProject	README.md	CS410 Course Project Team JEM: Reproducing Paper on Casual Topic Mining Zhangzhou Yu (leader) Matthew McCarty Jack Ma Presentation/Demo A presentation and demonstration of installing and running the application is available at https://mediaspace.illinois.edu/media/t/1_yra0qvjp . Overview This repository contains code to replicate an experiment done in a paper regarding causal topic mining with time series feedback: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The intent of this paper is to develop a method to consider additional contextual data (specifically, in the form of a time series) to supplement topic mining. The paper discusses two scenarios (presidential elections and stock prices); we chose to replicate the former. The specific experiment that was replicated involves determining topics from New York Times (NYT) articles from May-October 2000, with the additional context of betting odds for Bush and Gore winning the 2000 Presidential election. There are two files which are used as input for the Python code. One is the time series data for the betting odds, which is located in time_series.csv (Iowa2000PresidentOdds.csv is the raw data). The second input file, consolidated_nyt.tsv, is a list of NYT articles between May and October 2000. The NYT articles were filtered by 'Bush' and 'Gore' keywords to ensure that non-relevant documents were not considered for topic generation. The article date is also included with the article content, so that the time context of the article's publication can be considered with the presidential odds time series. The output of the program will be a list of topics, and the top three words within each topics. Unlike the plain vanilla PSLA algorithm, these topics highlight words that are highly correlated with the change of betting odds for Bush or Gore winning the election. The number of topics is determined by a parameter tn, and the paper discusses the performance of the algorithm with varying values of tn. For the purposes of our experiment reproduction, we chose tn=10. Software Implementation The experiment was reproduced in Python (version 3.8.6) with the help of several libraries, which are listed below: numpy - for general linear algebra operations gensim - for generating a mapping between a token id and the normalized words which they represent statsmodels - for the time-series causality test The algorithm itself is a modified version of the PLSA algorithm, which was initially implemented for a homework assignment (MP3) in CS410 at UIUC. The plsawithprior.py file contains a plsa class which contains many variables of use, some of which are highlighted below: term_doc_matrix - word count of terms in a given document document_topic_prob - the probability of p(z | d) where z represents a specific topic and d represents a specific document mu - the strength of the prior probability (when mu=0, the result would match PLSA with no prior) prior - the prior probability of p(w | z) where w represents a word and z represents a specific topic topic_word_prob = the posterior probability of p(w | z) Additional descriptions of software programs are illustrated below: * Granger_Casuality_Test.py- first clean the presidential betting odds time series raw data Iowa2000PresidentOdds.csv and output the cleaned data time_series.csv; implemented the granger casuality test function ready for use in main.py * calc_prior.py - calculate the prior of significant words within significant topics from the granger casuality test significance level output * sanitize_nyt.py - text data extraction, filtering and cleaning on the NYT articles from May-October 2000; output consolidated_nyt.tsv where each line contains documents with 'Bush' and 'Gore' keywords from one day (the number of lines in this clean text data matches with the number of rows in the clean time series data; readily available for use in the granger casuality test) * main.py - include all the functions discussed above; consolidated main program Future modifications could be made to the algorithm to change how the prior is generated (based on other time-series/non-document data source). Software Usage Run git clone https://github.com/enaena633/CourseProject.git to clone the code repository. Install Python 3. Install the following python libraries (via pip, etc.): numpy gensim statsmodels Run python main.py in the repository directory. Results The following list is the top 3 words in the ten topics that were mined from the New York Times documents: ad win ralli night lehrer trillion econom recent try support governor alaska state governor alaska governor clarenc right night win tuesdai wetston abm recent offic men try win ralli church These results are different from the paper's results, which are included below: tax cut 1 screen pataki giuliani enthusiasm door symbolic oil energy prices pres al vice love tucker presented partial abortion privatization court supreme abortion gun control nra news w top This can be explained by the following: The implementation of several elements of the algorithm (Granger causality test, PLSA, etc.) were implemented in Python, whereas the paper used R. We used the gensim package to perform stemming of words (which would cause words like econom to appear instead of economy or economic). gensim was also used to remove stop words. The paper does not specify whether a background language model was used in its implementation of PLSA or if any stop word removal was done. The EM algorithm is guaranteed to converge to a local (but not necessarily global) maximum, which causes output to be different even with the same implementation when different random starting values are used. Certain parameters in the paper are not specified (e.g., the threshold value gamma for the significance cutoff for words at the topic level, we used 90%). Team Member Contributions All team members were engaged and involved in reproducing the experiment from the paper sourced above. In addition to weekly meetings where everyone contributed, individual team members were responsible for the following: Zhangzhou Yu (leader) - Time series data retrieval/cleaning, Granger causality test, administrative/organizational tasks Matthew McCarty - Text data retrieval/cleaning, library research, documentation, presentation/demo recording Jack Ma - PLSA augmentation to include use of contextual time series data, prior implementation, consolidate/structure software programs
https://github.com/enaena633/CourseProject	Team JEM - Progress Report.pdf	"1) Progress made thus far * We have obtained the data, cleaned and filtered only articles containing ""Gore"" or ""Bush"" to start with * We have mostly understood the whole paper and identified the approach to implement the PLSA with Dirichlet prior * We have split up the remaining work 2) Remaining tasks * Further clean the data into only paragraphs containing ""Gore"" and ""Bush"" and remove all non- alphanumerical characters * Work on the granger causality test combining the time series data with the word counts of document data * Calculated the prior according the formula specified in the paper and update the maximization step of PLSA with the given mu and prior previously calculated 3) Any challenges/issues being faced * For now we are not running into any execution issues yet"
https://github.com/enaena633/CourseProject	Team JEM - Project Proposal.pdf	"Project Proposal for Team JEM 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Team JEM is made up of the following three members. The team coordinator/leader is Zhangzhou Yu (zy37@illinois.edu). o Zhangzhou Yu (zy37@illinois.edu), o Matthew McCarty (mdm12@illinois.edu), o Jack Ma (jma46@illinois.edu) 2. Which paper have you chosen? Causal topic modeling o Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python (as we found a package to do granger causality test). 4. Can you obtain the datasets used in the paper for evaluation? We are in the process of gaining access to the text datasets on Linguistic Data Consortium for the New York Times corpus from May through October 2000. The access is pending upon approval by the UIUC admin access, where the professor is actively engaged on this issue. Currently, we are able to access prices from the Iowa Electronic Markets (IEM) 2000 Presidential Winner-Takes-All Market as well as the stock prices of Apple and American Airlines during that time period. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? If we are unable to access the text data on the Linguistic Data Consortium for the New York Times Corpus for the 2000 presidential election, we will investigate using the online New York Times archives (or some other accessible newspaper archive) for the 2016 presidential election articles along with stock prices of American Airlines and Apple of the same time period. 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? To demonstrate that we have successfully reproduced the method introduced in the paper, we will output significant topic lists that affected the 2016 presidential election and stock prices of American Airlines and Apple in 2016 respectively. We will check the results of the top topic lists against what truly happened in that time frame."
https://github.com/raman162/UofICS410FinalProject	progress_report.md	CS 410 Final Project Progress Report: Topic Mining Healthcare Data Completed Tasks [x] Curate Dataset [x] Exported 20,000 labelled (positive/not positive) notes summarizing telehealth care encounters [x] Automated de-identification of PHI using DEID Software Package [x] Exported Sample Of De-Identification to work on Topic Mining Pending Tasks [ ] Topic Mining [ ] Prep De-Identification notes for Topic Mining tool [ ] Run a series of topic mining trails changing the number of topics trying to be mined [ ] Perform analysis of topic coverage for positive notes and non-positive notes [ ] Classifier [ ] Create a classifier that can determine if a telehealth note summary had a positive outcome for the patient [ ] Train on half of the de-identified dataset [ ] Run on other half of the de-identified dataset and compare the results Current Challenges/Issues Faced Due to the export of the providers from medical systems containing a lot of invalid data such as names of lab tests, diseases and specialists, the list had to be manually scrubbed to avoid the DEID tool from falsely redacting it thinking they were doctor names. This was a manual tedious process that required review of a few thousand records. Detailed Progress Updates Curate Dataset 1. Exporting Labelled Dataset Two CSV files, each containing 10,000 records was exported from the TimeDoc system. One file named positive_encounters.csv contained only notes that were labelled as a positive outcome due to the telehealth services while another file named no_positive_encounters.csv only contained notes that weren't labelled as a positive outcome for the patient. The format of the exported CSV files are as follows: <note_id>,<patient_id>,<purpose>,<duration>,<note> The <purpose> is an array of attributes of the telehealth encounter, it is selected from a pre-defined list and can provide insights to the actions of the telehealth encounter. The <duration> is the total amount of time the telehealth encounter took, and <note> is the free-text nursing note summarizing the encounter that we will be performing topic mining on. 2. Automated De-Identification of Protected Health Information (PHI) To ensure we're adhering to HIPPA ^[HIPPA Privacy Guidelines, https://www.physionet.org/content/deid/1.1/] we have to redact Protected Health Information (PHI). This was redacted using the De-Identification (DEID) Software Package ^[De-Identification Software Package, https://www.physionet.org/content/deid/1.1/]. For the DEID to be effective, it required creating separate files of patient names and identifiers pid_patientname.txt, doctor first names doctor_first_names.txt, doctor last names doctor_last_names.txt, locations local_places.txt, and company names company_names.txt. The pid_patientname.txt was created by referencing all the patients from the two exported CSV lists and curating a file formatted with each line as <PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME>. The doctor_first_names.txt and the doctor_last_names.txt files were created by referencing exporting each care team member such as their Primary Care Physician (PCP), Radiologist, etc. and writing each name to a new line. Both files were scrubbed for duplicates and invalid data. The local_places.txt was created by taking each address related for the patient and writing the city to a town to each line. The company_names.txt file was created by listing out the pharmacies and local healthcare organizations that the patient utilizes and writing each to a new line. For the DEID to perform the redaction of PHI, it required to be fed the notes in a particular format. So the exported CSV file had to be transformed to the following format: START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>|||| <DOCUMENT_CONTENT> ||||END_OF_RECORD We accomplished this transformation for both of the CSV exported files using a ruby script located at deid/convert_csv_to_text.rb and ran the following commands: ``` convert csv files to deid text format ruby deid/convert_csv_to_text.rb positive_encounters.csv ruby deid/convert_csv_to_text.rb no_positive_encounters.csv ``` The output produced two files named positive_encounters.text and no_positive_encounters.text respectively. Afterwards the new text files were copied into the DEID directory and we ran the DEID perl script to remove the PHI using the following commands: ``` redact PHI from text files perl deid.pl positive_encounters deid-output.config perl deid.pl no_positive_encounters deid-output.config ``` The output produced two PHI redacted files named positive_encounters.res and no_positive_encounters.res. To convert the files back into the CSV format, we used the following script located at deid/convert_res_to_csv.rb and ran the following commands: ``` convert redacted res files to csv ruby deid/convert_res_to_csv.rb \ positive_encounters.res \ positive_encounters.csv ruby deid/convert_res_to_csv.rb \ no_positive_encounters.res \ no_positive_encounters.csv ``` The output produced two files named positive_encounters.res.csv and no_positive_encounters.res.csv. 3. Sample De-Identified Notes Data Since the DEID is an automated tool, we had to account for the possibility on not redacting all PHI data. To minimize actual PHI distributed, 50 samples were taken from both the positive_encounters.res and no_positive_encounters.res file and manually verified to not contain PHI. After the verification, the sampled data was shared with the rest of the team so to create scripts that will perform the topic mining and analysis. This was accomplished by utilizing the deid/sample_res.rb and running the following commands: ``` sample res files per manual review ruby deid/sample_res.rb positive_encounters.res 50 ruby deid/sample_res.rb no_positive_encounters.res 50 The output produced two files only containing 50 redacted PHI documents named positive_encounters.res-sample-50.res no_positive_encounters.res-sample-50.res ``` After manual verification that all PHI was redacted, the sampled files were transformed to the original CSV format by running the following commands: ``` convert sampled res files to CSV format ruby deid/convert_res_to_csv.rb \ positive_encounters.res-sample-50.res \ positive_encounters.csv ruby deid/convert_res_to_csv.rb \ no_positive_encounters.res-sample-50.res \ no_positive_encounters.csv ``` The output produced two files named positive_encounters.res-sample-50.res.csv and no_positive_encounters.res-sample-50.res.csv. The four sampled redacted PHI documents can be provided at request by emailing rsw2@illinois.edu
https://github.com/raman162/UofICS410FinalProject	progress_report.pdf	CS 410 Final Project Progress Report: Topic Mining Health- care Data Completed Tasks  Curate Dataset  Exported 20,000 labelled (positive/not positive) notes summarizing telehealth care en- counters  Automated de-identifcation of PHI using DEID Software Package  Exported Sample Of De-Identifcation to work on Topic Mining Pending Tasks # Topic Mining # Prep De-Identifcation notes for Topic Mining tool # Run a series of topic mining trails changing the number of topics trying to be mined # Perform analysis of topic coverage for positive notes and non-positive notes # Classifer # Create a classifer that can determine if a telehealth note summary had a positive outcome for the patient # Train on half of the de-identifed dataset # Run on other half of the de-identifed dataset and compare the results Current Challenges/Issues Faced Due to the export of the providers from medical systems containing a lot of invalid data such as names of lab tests, diseases and specialists, the list had to be manually scrubbed to avoid the DEID tool from falsely redacting it thinking they were doctor names. This was a manual tedious process that required review of a few thousand records. Detailed Progress Updates Curate Dataset 1. Exporting Labelled Dataset Two CSV fles, each containing 10,000 records was exported from the TimeDoc system. One fle named positive_encounters.csv contained only notes that were labelled as a positive outcome due to the telehealth services while another fle named no_positive_encounters.csv only contained notes that weren't labelled as a positive outcome for the patient. The format of the exported CSV fles are as follows: <note_id>,<patient_id>,<purpose>,<duration>,<note> The <purpose> is an array of attributes of the telehealth encounter, it is selected from a pre-defned list and can provide insights to the actions of the telehealth encounter. The <duration> is the total amount of time the telehealth encounter took, and <note> is the free-text nursing note summarizing the encounter that we will be performing topic mining on. 2. Automated De-Identifcation of Protected Health Information (PHI) To ensure we're adhering to HIPPA 1 we have to redact Protected Health Information (PHI). This was redacted using the De-Identifcation (DEID) Software Package 2. For the DEID to be efec- tive, it required creating separate fles of patient names and identifers pid_patientname.txt, doc- 1HIPPA Privacy Guidelines, https://www.physionet.org/content/deid/1.1/ 2De-Identifcation Software Package, https://www.physionet.org/content/deid/1.1/ 1 tor frst names doctor_first_names.txt, doctor last names doctor_last_names.txt, locations local_places.txt, and company names company_names.txt. The pid_patientname.txt was created by referencing all the patients from the two exported CSV lists and curating a fle format- ted with each line as <PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME>. The doctor_first_names.txt and the doctor_last_names.txt fles were created by referencing ex- porting each care team member such as their Primary Care Physician (PCP), Radiologist, etc. and writing each name to a new line. Both fles were scrubbed for duplicates and invalid data. The local_places.txt was created by taking each address related for the patient and writing the city to a town to each line. The company_names.txt fle was created by listing out the pharmacies and local healthcare organizations that the patient utilizes and writing each to a new line. For the DEID to perform the redaction of PHI, it required to be fed the notes in a particular format. So the exported CSV fle had to be transformed to the following format: START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>|||| <DOCUMENT_CONTENT> ||||END_OF_RECORD We accomplished this transformation for both of the CSV exported fles using a ruby script located at deid_support/convert_csv_to_text.rb and ran the following commands: # convert csv files to deid text format ruby deid_support/convert_csv_to_text.rb positive_encounters.csv ruby deid_support/convert_csv_to_text.rb no_positive_encounters.csv The output produced two fles named positive_encounters.text and no_positive_encounters.text respectively. Afterwards the new text fles were copied into the DEID directory and we ran the DEID perl script to remove the PHI using the following commands: # redact PHI from text files perl deid.pl positive_encounters deid-output.config perl deid.pl no_positive_encounters deid-output.config The output produced two PHI redacted fles named positive_encounters.res and no_positive_encounters.res. To convert the fles back into the CSV format, we used the following script located at deid_support/convert_res_to_csv.rb and ran the following commands: # convert redacted res files to csv ruby deid_support/convert_res_to_csv.rb \ positive_encounters.res \ positive_encounters.csv ruby deid_support/convert_res_to_csv.rb \ no_positive_encounters.res \ no_positive_encounters.csv The output produced two fles named positive_encounters.res.csv and no_positive_encounters.res.csv. 3. Sample De-Identifed Notes Data Since the DEID is an automated tool, we had to account for the possibility on not redacting all PHI data. To minimize actual PHI distributed, 50 samples were taken from both the positive_encounters.res and no_positive_encounters.res fle and manually verifed to not contain PHI. After the verifcation, the sampled data was shared with the rest of the team so to create scripts that will perform the topic mining and analysis. This was accomplished by utilizing the deid_support/sample_res.rb and running the following commands: # sample res files per manual review 2 ruby deid_support/sample_res.rb positive_encounters.res 50 ruby deid_support/sample_res.rb no_positive_encounters.res 50 The output produced two fles only containing 50 redacted PHI documents named positive_encounters.res-sample-50.res no_positive_encounters.res-sample-50.res After manual verifcation that all PHI was redacted, the sampled fles were transformed to the original CSV format by running the following commands: # convert sampled res files to CSV format ruby deid_support/convert_res_to_csv.rb \ positive_encounters.res-sample-50.res \ positive_encounters.csv ruby deid_support/convert_res_to_csv.rb \ no_positive_encounters.res-sample-50.res \ no_positive_encounters.csv The output produced two fles named positive_encounters.res-sample-50.res.csv and no_positive_encounters.res-sample-50.res.csv. The four sampled redacted PHI documents can be provided at request by emailing rsw2@illinois.edu 3
https://github.com/raman162/UofICS410FinalProject	proposal.md	CS 410 Final Project Proposal: Topic Mining Healthcare Data Team Members Satish Reddy Asi- sasi2@illinois.edu Srikanth Bharadwaz Samudrala - sbs7@illinois.edu Raman Walwyn-Venugopal (Project Coordinator/Team Leader) - rsw2@illinois.edu Motivation TimeDoc is a telemedicine company that focuses on ensuring patients receive proactive healthcare to improve the treatment of their chronic diseases. Since 2015, TimeDoc has accumulated roughly 1.8 million unstructured text documents created by licensed healthcare professionals that summarize telemedicine encounters with patients. Out of that total dataset there are 13,496 unique documents that have been labelled as a positive outcome for the patient by their author. A positive outcome is very important for a patient as it indicates that their health was improved which also translates into them valuing the telemedicine service. Utilizing TimeDoc's data, our primary goal is to identify more patients that had these positive outcomes and identify patients that are more likely to have a positive outcome. To accomplish this our team plans to primarily use python and open source tools as described in our solution below. Solution Our assumption is that there is a relation between a patient's profile and their likelihood of having a positive outcome. If this assumption is correct we can begin recommending patients with certain profiles for healthcare professionals to focus on. Curate Dataset Expected Duration: 20 hours Expected Completion Date: November 15th To ensure we're adhering to HIPPA ^[HIPPA Privacy Guidelines, https://www.hhs.gov/hipaa/for-professionals/privacy/index.html] guidelines. The telemedicine encounter document data and the patient profiles need go through a Patient Health Information ^[Patient Health Information, https://www.hipaajournal.com/what-is-protected-health-information/] (PHI) de-identification process. We will automate the PHI de-identification for free text fields on the patient profile and the telemedicine documents utilizing a De-Identification Software Package ^[De-Identification Software Package, https://www.physionet.org/content/deid/1.1/]. All structured fields of a patient profile known to contain PHI will be replaced as well. Topic Mining and Analysis on Dataset Expected Duration: 20 hours Expected Completion Date: November 24th We plan on mining the topic models using Latent Dirichlet Allocation ^[Latent Dirichlet Allocation (LDA), https://arxiv.org/pdf/1711.04305.pdf] for both the de-identified encounter telemedicine and patient profiles from the curated dataset. We plan on using gensim ^[gensim: python library for topic Modeling, https://pypi.org/project/gensim/] and nltk ^[nltk: Natural Language Toolkit python package, https://pypi.org/project/nltk/] in python to accomplish this task. After completing the topic modeling we will investigate if there is a significant difference between telemedicine encounters that were labelled as a success story versus ones that were not. We will also be investigating if there is a significant difference between the patient profiles that had success stories versus the patient profiles that do not have any success stories. Recommendation Expected Duration: 20 hours Expected Completion Date: December 5th After we have identified the topics of telemedicine documents and patients that had positive outcomes, we would like to identify other patients that may fit this criteria. We can accomplish this by indexing all the patient profiles by their mined topic data, the likelihood of the topic will be used as the score for the vector. We will then recommend patients that have topic profiles similar to the topics of patient profiles that had positive outcomes. The similarity score will be calculated using one of the newer variations of Okapi BM25. In addition to this, we can also design a similar recommendation system to attempt to identify telemedicine documents that may contain a positive outcome for out patient. The utility for both of these systems will be evaluated by licensed healthcare professionals at TimeDoc. Miscellaneous Other libraries that may be used but not discussed are listed but not limited to; pyspark ^[pyspark, https://www.gangboard.com/blog/what-is-pyspark], scipy ^[scipy, https://www.scipy.org/scipylib/index.html], metapy ^[metapy, https://github.com/meta-toolkit/metapy], pandas, ^[pandas, https://pandas.pydata.org/], numpy, ^[numpy, https://numpy.org/] and whoosh ^[whoosh, https://whoosh.readthedocs.io/en/latest/intro.html].
https://github.com/raman162/UofICS410FinalProject	proposal.pdf	CS 410 Final Project Proposal: Topic Mining Healthcare Data Team Members * Satish Reddy Asi- sasi2@illinois.edu * Srikanth Bharadwaz Samudrala - sbs7@illinois.edu * Raman Walwyn-Venugopal (Project Coordinator/Team Leader) - rsw2@illinois.edu Motivation TimeDoc is a telemedicine company that focuses on ensuring patients receive proactive healthcare to improve the treatment of their chronic diseases. Since 2015, TimeDoc has accumulated roughly 1.8 million unstructured text documents created by licensed healthcare professionals that summarize telemedicine encounters with patients. Out of that total dataset there are 13,496 unique documents that have been labelled as a positive outcome for the patient by their author. A positive outcome is very important for a patient as it indicates that their health was improved which also translates into them valuing the telemedicine service. Utilizing TimeDoc's data, our primary goal is to identify more patients that had these positive outcomes and identify patients that are more likely to have a positive outcome. To accomplish this our team plans to primarily use python and open source tools as described in our solution below. Solution Our assumption is that there is a relation between a patient's profle and their likelihood of having a positive outcome. If this assumption is correct we can begin recommending patients with certain profles for healthcare professionals to focus on. Curate Dataset Expected Duration: 20 hours Expected Completion Date: November 15th To ensure we're adhering to HIPPA 1 guidelines. The telemedicine encounter document data and the patient profles need go through a Patient Health Information 2 (PHI) de-identifcation process. We will automate the PHI de-identifcation for free text felds on the patient profle and the telemedicine documents utilizing a De-Identifcation Software Package 3. All structured felds of a patient profle known to contain PHI will be replaced as well. Topic Mining and Analysis on Dataset Expected Duration: 20 hours Expected Completion Date: November 24nd We plan on mining the topic models using Latent Dirichlet Allocation 4 for both the de-identifed encounter telemedicine and patient profles from the curated dataset. We plan on using gensim 5 and nltk 6 in python to accomplish this task. 1HIPPA Privacy Guidelines, https://www.hhs.gov/hipaa/for-professionals/privacy/index.html 2Patient Health Information, https://www.hipaajournal.com/what-is-protected-health-information/ 3De-Identifcation Software Package, https://www.physionet.org/content/deid/1.1/ 4Latent Dirichlet Allocation (LDA), https://arxiv.org/pdf/1711.04305.pdf 5gensim: python library for topic Modeling, https://pypi.org/project/gensim/ 6nltk: Natural Language Toolkit python package, https://pypi.org/project/nltk/ 1 After completing the topic modeling we will investigate if there is a signifcant diference between telemedicine encounters that were labelled as a success story versus ones that were not. We will also be investigating if there is a signifcant diference between the patient profles that had success stories versus the patient profles that do not have any success stories. Recommendation Expected Duration: 20 hours Expected Completion Date: December 5th After we have identifed the topics of telemedicine documents and patients that had positive out- comes, we would like to identify other patients that may ft this criteria. We can accomplish this by indexing all the patient profles by their mined topic data, the likelihood of the topic will be used as the score for the vector. We will then recommend patients that have topic profles similar to the topics of patient profles that had positive outcomes. The similarity score will be calculated using one of the newer variations of Okapi BM25. In addition to this, we can also design a similar recommendation system to attempt to identify telemedicine documents that may contain a positive outcome for out patient. The utility for both of these systems will be evaluated by licensed healthcare professionals at Time- Doc. Miscellaneous Other libraries that may be used but not discussed are listed but not limited to; pyspark 7, scipy 8, metapy 9, pandas, 10, numpy, 11 and whoosh 12. 7pyspark, https://www.gangboard.com/blog/what-is-pyspark 8scipy, https://www.scipy.org/scipylib/index.html 9metapy, https://github.com/meta-toolkit/metapy 10pandas, https://pandas.pydata.org/ 11numpy, https://numpy.org/ 12whoosh, https://whoosh.readthedocs.io/en/latest/intro.html 2
https://github.com/raman162/UofICS410FinalProject	README.md	"CS 410 - Final Project: Topic Mining Healthcare Data & Classification Team Members Raman Walwyn-Venugopal - rsw2@illinois.edu Srikanth Bharadwaz Samudrala - sbs7@illinois.edu Satish Reddy Asi - sasi2@illinois.edu Quick Links Proposal PDF Progress Report PDF Video Demo (YOUTUBE) Overview The goal of this project is to perform topic mining and classification on telehealth encounter nursing notes for notes that documented a positive outcome for the patient form the telehealth services. To accomplish this, we divided the project into four steps; 1. curating the dataset 2. build topic miner and mine topics from dataset 3. perform analysis on topics 4. build binary classifier that attempts to predict if a document is a positive outcome for the patient Curating Dataset Requirements: - ruby 2.X - perl 5.X Note: This code was ran on ubuntu 18.04 and ubuntu 20.04 Exporting Raw Dataset The source of the data is from TimeDocHealth that has a care team that focuses on providing telehealth services to patients with multiple chronic diseases. Two CSV files, each containing 10,000 records were exported from the TimeDoc system. One file named positive_encounters.csv contained only notes that were labelled as a positive outcome due to the telehealth services while another file named no_positive_encounters.csv only contained notes that weren't labelled as a positive outcome for the patient. The format of the exported CSV files are as follows: <note_id>,<patient_id>,<purpose>,<duration>,<note>. <purpose> is an array of attributes of the telehealth encounter, it is selected from a pre-defined list and can provide insights to the actions of the telehealth encounter. <duration> is the total amount of time the telehealth encounter took <note> is the free-text nursing note summarizing the encounter. This data is what the topic mining and classification will be performed Automating De-Identification of Protected Health Information (PHI) To ensure we're adhering to HIPPA Privacy Guidelines, Protected Health Information (PHI) was redacted using De-Identification (DEID) Software Package. For the DEID to be effective, it had to be configured with the following lists: - pid_patientname.txt - patient names and identifiers. Was created by referencing all the patients from the two exported CSV lists and curating a file formatted with each line as <PATIENT_ID>||||<PATIENT_FIRST_NAME>||||<PATIENT_LAST_NAME> - doctor_first_names.txt - doctor first names. Created by exporting each care team member for the patient such as their Primary Care Provider, Radiologist, etc. - doctor_last_names.txt - doctor last names. Created using same strategy as doctor first names. - unambig_local_places.txt - locations near the patient. Created using the cities, towns of addresses for patients and businesses near them. - company_names.txt - company names. Created by listing out local healthcare organizations surrounding the patient. For the DEID to perform the redaction of PHI, it required to be fed the notes in a particular format. So the exported CSV file had to be transformed to the following format: START_OF_RECORD=<PATIENT_ID>||||<DOCUMENT_ID>|||| <DOCUMENT_CONTENT> ||||END_OF_RECORD We accomplished this transformation for both of the CSV exported files using a ruby script located at deid/convert_csv_to_text.rb and ran the following commands: ``` convert csv files to deid text format ruby deid/convert_csv_to_text.rb demo_data/positive_encounters.csv ruby deid/convert_csv_to_text.rb demo_data/no_positive_encounters.csv ``` The output produced two files named positive_encounters.text and no_positive_encounters.text respectively. Afterwards we ran the DEID perl script to remove the PHI using the following commands: ``` enter deid directory cd deid redact PHI from text files perl deid.pl ../demo_data/positive_encounters deid-output.config perl deid.pl ../demo_data/no_positive_encounters deid-output.config ``` The output produced two PHI redacted files named positive_encounters.res and no_positive_encounters.res. To convert the files back into the CSV format, we used the following script located at deid/convert_res_to_csv.rb and ran the following commands: ``` convert redacted res files to csv ruby deid/convert_res_to_csv.rb \ demo_data/positive_encounters.res \ demo_data/positive_encounters.csv ruby deid/convert_res_to_csv.rb \ demo_data/no_positive_encounters.res \ demo_data/no_positive_encounters.csv ``` The output produced two files named positive_encounters.res.csv and no_positive_encounters.res.csv. Note: Since the DEID is an automated too, we have to account for the possibility of not redacting all PHI data. To minimize actual PHI distributed 50 samples were taken form both the positive_encounters.res and no_positive_encounters.res file and manually verified to not contain PHI. This sampled may be provided upon request by emailing rsw2@illinois.edu Topic Mining Requirements: - Python 3.X Python Libraries Used: - nltk - pandas - numpy - matplotlib/pylab - regex Extracting Documents The source to extract documents from is the notes. The Telemedicine responses are saved as CSV files with multiple fields. ""notes"" from the response file is fed as Document input to our PLSA implementation. The input responses file is in CSV file and the data is delimited by "","" character. Generating stop words Stop words are generated using standard python nltk libraries. The stopwords are saved as file and is used as input for topic_miner program. stop words can be manually edited adding any tele-medicine specific words such as patient, call, treatment, phone etc.. since these are repeated frequently in every note. stop words program is run separately and the file is saved under ""patient_data"" folder where the input files are placed under. Mining Topics from Documents The topic_miner is run with data-file (in CSV format), stop-words file as input. The additional arguments to the program include number of topics, Max Iterations, Threshold, Number of Topic words. The arguments also include the path to output files: - Document Topic Coverage - Topic Word Coverage - Vocabulary - Topic words More details about the module are available at: topic miner Note: Due to the slow performance of our manually written PLSA topic miner, we created topic miner v2 that uses an open source python PLSA package and produces the same documents as our home-crafted PLSA topic miner. Setup change to directory of topic miner cd topic_miner_v2 Create new virtual environment python -m venv venv Activate virtual environment source venv/bin/activate Install required packages pip install -r requirements.txt Run Topic Miner ``` python topic_miner.py python topic_miner.py ../demo_data/all_encounters.res.csv 10 ``` Output would be: ``` topic coverage of topic probability per document in corpus all_encounters.res.csv.10-doc-topic-cov.txt grouping of words and probabilities of topic per line all_encounters.res.csv.10-topic-word-probs-grouped.txt all the probabilities for each topic per line all_encounters.res.csv.10-topic-word-probs.txt all the words for each topic per line all_encounters.res.csv.10-topics.txt vocabulary of corpus all_encounters.res.csv.vocab ``` Topic Analysis Requirements: - Python 3.X This topic analysis script performs analysis on the results of the topic miner when both the positive and non-positive encounters are included in the whole corpus. It attempts to: 1. Identify which topics are related to positive outcomes and which topics are related to non-positive outcomes 2. Pull the top words from the positive outcome topics and non-positive outcome topics 3. Highlight which top words from positive and non-positive overlap with each other versus which words are unique to their own topics 4. generates 3 files: pos-non-pos-topics.txt, top-pos-words.txt and top-non-pos-words.txt Usage python topic_analysis/topic_analysis.py \ demo_data/all_encounters.res.csv.10-doc-topic-cov.txt \ demo_data/all_encounters.res.csv.10-topics.txt Classifier Requirements: - Python 3.X - Python Virtual Environment Package (Included in Python standard library) Overview of Functionality The text classifier is responsible for reviewing the notes of the telehealth encounters and classifying the note as positive outcome versus non-positive outcome. The classifier module has the following features: - Load positive and non-positive CSV files generated from the PHI De-identification process - Clean data by removing PHI redaction sections, non-alphanumeric characters, extra white space, lemmatization, and stop words - Generate a classifier using the RandomForestClassifier from sklearn - Evaluate classifier by collecting Recall, Precision, F1 Score, micro averages per category, and the overall classification accuracy - Store classifier to a file - Load classifier from a file - Score optimizer that steps through a combination of number of features and estimators for the classifier model and returns the optimal inputs and score The process of generating the classifier requires the docs to be cleaned and vectorized into TF-IDF weights. The vectorized version of the corpus was then split into two sets; 20% for training and 80% for testing. The model used for training is the RandomForestClassifier from sklearn which is based on a Random Forest Algorithm that uses a 'random forest' of numerous decision trees. The core of the algorithm follows the steps below: - Pick N random records from the dataset - Build a decision tree on the randomly selected N records - Choose the number of trees used in the algorithm and repeat steps 1 and 2 The algorithm is ideal for classification because it is known to reduce biases with the use of multiple randomly formed decision trees and it performs well when unknown data points are introduced. Disadvantages of the algorithm is that the complexity causes it to take longer to train and process due to the amount of decision trees. Setup ``` change directory to classifier cd classifier initlize python virtual evnrionment python -m venv venv source venv/bin/activate install dependencies pip install -r requirements.txt ``` Usage Be sure to update the following constants POSITIVE_CSV_FILE and NO_POSITIVE_CSV_FILE to the true file paths of the redacted data produced from the De-Identification process. Also update the CLASSIFIER_FILE for where you want to store the classifier. The classifier module can be run as a script to quickly generate a classifier with the pre-optimized defaults determined from testing. python classifier.py This will load the data, clean the data, generate a classifier, print out the evaluation metrics and store it to the path defined in the CLASSIFIER_FILE constant. An example of the classifier evaluation is shown below. ``` precision recall f1-score support non-positive 0.88 0.95 0.91 2014 positive 0.94 0.85 0.90 1842 accuracy 0.91 3856 macro avg 0.91 0.90 0.90 3856 weighted avg 0.91 0.91 0.90 3856 Accuracy: 0.9050829875518672 ``` The classifier can be loaded and used on new documents. Enter the python console and run the following commands ``` import classifier.py text_classifier = classifier.load(classifier.CLASSIFIER_FILE) docs = [ 'Scheduled transportation for patient appointment on Thursday', 'discussed personal goals with patient for patient to work on quitting smoking' ] predictions = classifier.predict(text_classifier, docs) print(predictions) ``` Optimizations The classification accuracy score was optimized by varying the number of features and estimators (decision trees) used in the algorithm. This was a simple iterative algorithm that calculated the accuracy for each feature/estimator combination and then returned the optimal score and the combination used to accomplish. The classifier module has an optimize_score function that accepts the following arguments: docs (default: to cleaned version of dataset) - complete corpus of documents labels (default to dataset defined) - labels each document min_features (default: 1000)- start number of features to use max_features (default: 5000)- max number of features to use feature_step (default: 250) - amount to increase number of features by min_df (default: 10) - minimum document frequency for a feature to be selected max_df (default: 0.8) - maximum document frequency for a feature to be selected min_estimators (default: 750) - start number of estimators to use max_estimators (default: 2500) - max number of estimators to use estimator_step (default: 250) - amount to increase number of estimators by It outputs a dictionary that contains the following keys: feature_steps - varying features used estimator_steps - varying estimators used scores - 2-dimension numpy array containing all scores generated. Shape is feature stpes length x estimator steps length optimal_score - The highest accuracy result from the iterations optimal_num_features - The number of features used to generate optimal score optimal_num_estimators - The number of estimators used to generate optimal score The optimal number of features used was determined to be 1500 while the optimal number of estimators was determined to be 750. BONUS: Classifying the top positive and top non-positive topic words As a bonus test, we tested the classifier predictions on the top positive and non-positive words generated from the topic analysis step. ``` enter python console python import classifier module import classifier load stored classifier text_classifier = classifier.load(classifier.CLASSIFIER_FILE) ``` Classify top positive words f = open('../demo_data/all_encounters.res.csv.2-topics.txt.top-pos-words.txt', 'r') pos_docs = [f.read()] f.close() print('top pos words: ', pos_docs[0]) print('top pos words classifier predictions: ', classifier.predict(text_classifier, pos_docs)[0]) Classify top non-positive words f = open('../demo_data/all_encounters.res.csv.2-topics.txt.top-non-pos-words.txt', 'r') non_pos_docs = [f.read()] f.close() print('top non pos words: ', non_pos_docs[0]) print('top non pos words classifier predictions: ', classifier.predict(text_classifier, non_pos_docs)[0]) Output is ``` top pos words: pharmacy appointment medication service information poa cuff call care sugar pressure concern blood morning meal state report insulin transportation time top pos words classifier predictions: positive top non pos words: pharmacy medication education exercise today appt goal inhaler level weight plan pressure knee minute state phone transportation day time ncm top non pos words classifier predictions: non-positive ``` Conclusion Automating the redaction of PHI data is plausible and should be used by data scientists trying to perform analysis on free text health data to respect patient privacy and adhere to HIPPA rules. One thing to note is that the redaction process is slow on large datasets. Redacting PHI on the 20,000 document dataset took nearly an hour running on an Intel i7 10th gen processor. To avoid this issue in a production workflow with much larger datasets, an automated redacted pipeline should be considered where as soon as a note is created, a redaction process is triggered and stored in a separate bucket. When performing topic mining with our home-crafted PLSA topic miner, we noted that performance was poor on large datasets when compared to an open-source PLSA python package. This was likely due to unoptimized implementation of the EM-algorithm when handling large matrices. While performing topic analysis, we noticed that the fewer number of topics generated made it easier to relate topics to positive outcomes and other topics to non-positive outcomes. As we increased the topic count when performing PLSA, this was no longer the case and the distributions of topics among positive corpus and non-positive corpus were similar. From this behaviour, we can infer that there is definitely a difference of themes discussed in positive outcomes but that there is definitely overlapping themes. The classifier we created performs only well on large datasets. On the sampled and demo datasets of only 100 records, the maximum classification accuracy that was achieved was 85%. When training the classifier on the complete corpus of 20,000 documents, the classification accuracy jumped to 90%. These maximum scores were calculated by using an iterative algorithm that varied the number of features and the number of estimators used by the classifier. The classifier consistently had better precision at 94% when labelling positive documents versus non-positive but had worse recall at 85% for all positive documents. This means that a user can trust the result of a classification of a positive document but cannot guarantee all to be retrieved. This would be preferred for a recommendation engine. BONUS: The classifier was also tested on the documents containing the top words from positive and non-positive topics generated from the topic analysis step. The classifier correctly classified the doc containing words from positive topics as 'positive' and the doc containing words from non-positive topics as 'non-positive'."
https://github.com/gnsandeep/CourseProject	README.md	CourseProject CS410Fall2020 Course Project : Text Classification Team members: 1. Sandeep Nanjegowda ( sgn3@illinois.edu) 2. Sunitha Vijayanarayan 3. Valentina Mondal Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7-b1b43508023f Live Lab Data Linked Git Hub Account : gnsandeep
https://github.com/gnsandeep/CourseProject	Team_Dream_Text_Project_Documentation.docx	"Public Domain Public Domain CS-410 Text Information Systems PROJECT DOCUMENTATION Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 INDEX An overview of the function of the code ---------------------------------------------------------------------------3 Implementation of models for Tweet Classification -------------------------------------------------------------4 2.1 BERT ----------------------------------------------------------------------------------------------------------------------4 2.2 SVM -----------------------------------------------------------------------------------------------------------------------5 2.3 CNN -----------------------------------------------------------------------------------------------------------------------6 2.4 LSTM ----------------------------------------------------------------------------------------------------------------------8 2.5 GRU -----------------------------------------------------------------------------------------------------------------------9 2.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------10 2.7 Bi-directional Models (GRU & LSTM) ----------------------------------------------------------------------------11 Tweet Classification Model Usage ------------------------------------------------------------------------------------13 3.1 BERT ---------------------------------------------------------------------------------------------------------------------13 3.2 SVM ---------------------------------------------------------------------------------------------------------------------14 3.3 CNN ----------------------------------------------------------------------------------------------------------------------15 3.4 LSTM --------------------------------------------------------------------------------------------------------------------15 3.5 GRU ----------------------------------------------------------------------------------------------------------------------16 3.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------17 3.7 Bi-directional LSTM and GRU -------------------------------------------------------------------------------------18 Results -----------------------------------------------------------------------------------------------------------------------19 Collaboration ---------------------------------------------------------------------------------------------------------------20 Presentation ----------------------------------------------------------------------------------------------------------------21 REFERENCES ------------------------------------------------------------------------------------------------------------------21 Overview Our goal is to classify the given test set data of twitter responses as ""SARCASM"" or ""NOT SARCASM"" by using various classification methods. Have classified the test data using Linear Regression, Naive Bayes, GRU, CNN, LSTM, SVM, Bidirectional and BERT models. While predicting the label of the ""response"" based on the ""context"". Context which is an ordered list of dialogue for which response is a reply to the last dialogue in the context. Test data has unique ids along with tweet ""responses"" to be classified. We build, train several different models using given trained data and predict the test data using the trained model and generate an ""answer.txt"" file that has unique ids along with the predicted label. Since there are 1800 ids in test dataset, our generated ""answer.txt"" file has exactly 1800 rows. At a high level, we perform the below steps: Preprocess both the training and test data by removing the html tags, converting the tweets to lower case, removing punctuations and numbers, removing stop words, converting emojis and emoticons, removing single character and multiple spaces, removing left over special characters. Split the test dataset into train and test data. Create the embedding matrix using glove or word2vec embeddings. Create the model using embedding layer, adding layers such as LSTM, CNN etc. Fit the model on train data Predict the model on test data and generating predicted labels Once the answer.txt file is generated and uploaded to git hub, predicted label values are compared with the actual result set to obtain the precision, recall and F-score value. Precision indicates fraction of relevant instances among the retrieved instances. Precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly. Recall indicates fraction of the total amount of relevant instances that were retrieved. Recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. F-score is the harmonic mean of the precision and recall. The highest possible value of an F-score is 1. Our baseline to be achieved is 0.723 We were successfully able to beat the baseline using Logistic Regression and BERT model. We will be walking through further in below points regarding implementation and usage for all the models we tried. The models that we tried can be used to classify any tweets that has response and context attributes. The models can be leveraged for other applications with appropriate changes such as classification of tags in news headline, sentiment analysis for movie reviews etc. Implementation of Models for Tweet Classification This section describes implementation of different Tweet Classification Models 2.1 BERT We have used BERT (Bidirectional Encoder Representations from Transformers) model BertForSequenceClassification for Tweet classification. Model is built using Jupyter Notebook. Jupyter Notebook has following Sections in same order. Chris McCormick Blogs and code about BERT were very helpful for using BERT for classification. We have used certain sections of this code from (McCormick) in our Model. 2.1.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. URLs, @, non-ASCII characters, extra space, &, <, > are removed, space is inserted between punctuation marks. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.1.2 Model We have used transformers package from Hugging Face which will give us a PyTorch interface for working with BERT. We have installed transformers package. 2.1.3 Tokenizing Input and creating DataLoaders To feed our text to BERT, it must be split into tokens, we need to add special tokens to start and end, Pad and Truncate all sentences to a single constant length (we have used max length as 256), differentiate real tokens from padding tokens with attention mask. and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. We have used ""uncased"" version - ""bert-base-uncased"". Training dataset is split for training and validation. We will also create an iterator for our training, validation and test dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. Note: We have used SequentialSampler for validation and test datasets. 2.1.4 Pre-Trained BERT Model After this we have loaded Pre-Trained BERT model - BertForSequenceClassification with a single linear classification layer on top. 2.1.5 Model Parameters and Learning Rate Scheduler We have used AdamW Optimizer with learning rate of 2e-5 and eps of 1e-8 and get_linear_schedule_with_warmup is created out of the Optimizer. 2.1.6 Utility Functions Utility function format_time is created for formatting time and utility function flat_accuracy is created for calculating accuracy. 2.1.7 Training Loop Each pass in our loop we have a training phase and a validation phase Training: Unpack our data inputs and labels Load data onto the GPU for acceleration Clear out the gradients calculated in the previous pass. In PyTorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. Forward pass (feed input data through the network) Backward pass (backpropagation) Tell the network to update parameters with optimizer.step() Track variables for monitoring progress Evalution: Unpack our data inputs and labels Load data onto the GPU for acceleration Forward pass (feed input data through the network) Compute loss on our validation data and track variables for monitoring progress 2.1.8 Evaluating Test Data Prediction is done on test dataset. Test dataset is read in batches and prediction is appended to list, which is then added to test panda's data frame and answer_BERT.txt is created. 2.2 SVM We have used Support Vector Machines model for Tweet classification. Code is written in python language. We have used certain sections of code from (Bronchal) in our model: 2.2.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. HTML tags, punctuation, numbers, single character, multiple spaces, stop words, left over special characters are removed. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.2.2 Tokenizing Input Twitter-aware tokenizer designed to be flexible and easy to adapt to new domains and tasks. Tuple regex_strings define a list of regular expression strings. regex_strings strings are put, in order, into a compiled regular expression object called word_re. The tokenization is done by word_re.findall(s), where s is the user-supplied string, inside the tokenize() method of the class Tokenizer. When instantiating Tokenizer objects, there is a single option: preserve_case. By default, it is set to True. If it is set to False, then the tokenizer will downcase everything except for emoticons. 2.2.3 Vectorize CountVectorizer converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. The following parameters are used: Analyzer: feature is made of word n-grams. Tokenizer: override the string tokenization step while preserving the preprocessing and n-grams generation steps Lowercase: convert all characters to lowercase before tokenizing. Ngram_range: The lower and upper boundary of the range of n-values for different word n-grams to be extracted i.e. (1,1) Stop_words: a built-in stop word list for English is used 2.2.4 Cross validation and grid search We use cross validation and grid search to find good hyperparameters for our SVM model. We build a pipeline to get features from the validation folds when building each training model. GridSearchCV implements a ""fit"" and ""predict"" method. Snapshot of code below for reference. It exhaustively searches over specified parameter values for an estimator. param_grid enables searching over any sequence of parameter settings. cv determines the cross-validation splitting strategy. n_jobs as -1 means to use all processors in parallel. Verbose controls the verbosity of messages. Scoring is to evaluate the predictions on the test set. 2.2.5 Evaluating Test Data Model with the best hyperparameters works on test data and basis the prediction, labels generated with value greater than 0.5 as ""SARCASM"" else ""NOT_SARCASM"" is appended to test panda's data frame and answer_SVM.txt is created. 2.3 CNN We have used Convolutional neural network model for Tweet classification. Code is written in python language. We have used certain sections of this code from (Celeni) in our model: 2.3.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for CNN model. 2.3.2 Word embedding Word embedding is vector representation of a particular word. Weight matrix is created from word2vec gensim model. And then embedding vectors are obtained from word2vec and using it as weights of non-trainable keras embedding layer. Corpus for twitter data for word2vec named as ""3000tweets_notbinary"" was referred from (Celeni) We also build the embeddings using Global Vectors for Word Representation. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Corpus can be downloaded from the link (Paletto) that we used as embedding file in our code to build the weight matrix. 2.3.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of CNN layer that creates a convolution kernel that is convoluted with the layer input over a single spatial dimension to produce a tensor of outputs. The action ReLU is applied to outputs as well. We set the number of filters to the dimensionality of the output space. Kernel_size is to specify the length of the 1D convolution window. Further we maxpool that summarize the most activated presence of a feature. Pooling is required to down sample the detection of features in feature maps. We also use global pooling that that down sample the entire feature map to a single value. This is same as setting the pool_size to the size of the input feature map. We then use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. Code snippet below: 2.3.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.3.5 Evaluating Test Data Once the model is created, model.predict() is run to predict for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_CNN.txt is created. 2.4 LSTM We have used Long Short-Term Memory network model for Tweet classification. Code is written in python language. We have used certain sections of this code in our model from (nana). 2.4.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for LSTM model. 2.4.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for LSTM model. 2.4.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of LSTM layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.4.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.4.5 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_LSTM.txt is created. 2.5 GRU We have tried GRU (Gated Recurrent Unit) model for Tweet classification. Code is written in python language. GRU is a special type of Recurrent Neural network. This type of sequence model can retain information from long ago, without washing it through time or remove information which is irrelevant to the prediction. Some of the code & hyper-parameter values specifically those related to ReduceLROnPlateau were decided based on the blog by (Kohli) 2.5.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for GRU model. 2.5.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for GRU model. 2.5.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.5.4 ReduceLROnPlateau This option was used so that the learning rate to be reduced when training is not progressing. 2.5.5 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.5.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.5.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_GRU.txt is created. 2.6 Naive Bayes and Linear Regression Naive Bayes and Logistic Regression Models are built using Jupyter Notebook for Tweet Classification. Jupyter Notebook has following Sections in same order. Bert Carremans Blog and code about Naive Bayes and Linear Regression were very helpful. We have used certain sections of code in our Model from the reference (Carremans) 2.6.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. We have Python class TextCounts , it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class extracts additional features like word counts, hash tags, mentions, capital words, question marks, urls and emojis. We have Python class CleanText it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class removes mentions, urls, oneword, punctuations, digits, Stopwords , performs stemming and converts to lower case. Test and Training data is passed through this classes fit methods. Extra features extracted for training data is combined with Cleaned data for both Training and Test data. Steps for Test data is done after training. We have Python class ColumnExtractor it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class is used for selecting columns in test and training dataset. 2.6.2 Hyperparameter tuning and cross-validation We first declare parameters for grid search. We have set parameters_vect , parameters_mnb and parameters_logreg parameters. We have parameters for TF-IDF like max_df , min_df , ngram_range , parameters for Linear Regression like clf_c and clf_penality , parameters for Naiive bayes like alpha. we have function grid_vect1 which does grid search. This function uses skilearn pipeline and it is based on below code from the reference (Scikit Learn) . 2.6.3 Model Training and Prediction Naiive Bayes and Logistic Regression models are created and passed to grid_vect1 which vectorizes the data using TF-IDF in ski learn pipeline and does grid search for best Hyper parameters. Once we find the best hyperparameters, we fit both Naive bayes and Logistic regression models with best parameters and perform prediction on test data. Predicted data is added to pandas data frame as new column and answer.txt is created for both models. 2.7 Bi-directional Models (GRU & LSTM) We have tried bi-directional versions of both GRU (Gated Recurrent Unit) model & LSTM model for Tweet classification. Code is written in python language. Bidirectional models are an extension of traditional LSTM & GRU models that can improve model performance on sequence classification problems. In problems where all timesteps of the input sequence are available, Bidirectional models train two instead of one LSTMs on the input sequence. Some of the code was referenced from (LillySimeonova) 2.7.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for these models as well. In addition, we also combined the additional text counts features calculated in the LogisticRegression model and combined them with the text features in a bid to improve model performance on the test set. 2.7.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for Bidirectional LSTM model & Bidirectional GRU. 2.7.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU/LSTM layers with units specifying the dimensionality of the output space. We are then combining text features with count features using multiple input models. All the other layers are added exactly like GRU/LSTM models already discussed in section 2.5 & 2.4. Combining of text features with non-text was coded by referring (Freischlag) We compile the model using the binary cross-entropy loss function since it predicts a binary value and Adam optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.7.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.7.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.7.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_Bidirectional.txt is created. Tweet Classification Models Usage This Section describes steps for running different Tweet Classification Models which are described in Section 2. 3.1 BERT Google Colab is preferred for running BERT model. We have used Colab for training and evaluating test dataset. 3.1.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose BERTSeq.ipynb 3.1.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files. 3.1.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.1.4 Result Predicted values are stored in answer_BERTSeq.txt, we can see the file in Files section Colab. 3.2 SVM SVM model can be run locally. We have used python language. 3.2.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/CS410Fall2020/ClassificationCompetition/tree/main/data to your local path under data folder. 3.2.2 SVM code Load the SVM.py code in your local path 3.2.3 Prerequisites Ensure to download and install the below libraries and modules to run the code to not throw any errors 3.2.4 Run Run SVM.py code in your local. 3.2.5 Result Predicted values are stored in your local path as 'answer_SVM.txt' 3.3 CNN CNN model can be run locally. We have python language. 3.3.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.3.2 CNN code Load the CNN.py code in your local path 3.3.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.3.4 Run Run CNN.py code in your local. 3.3.5 Result Predicted values are stored in your local path as 'answer_CNN.txt' 3.4 LSTM LSTM model can be run locally. We have used python language. 3.4.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.4.2 LSTM code Jupyter notebook LSTM.ipynb . 3.4.3 Prerequisites Please download Glove embedding file glove.6B.100d.txt ( it is available under glove.6B.zip) and place it in data folder. We can download Glove embedding file from https://nlp.stanford.edu/projects/glove/ Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip 3.4.4 Run Open the Jupyter notebook LSTM.ipynb and click Kernel Restart and Run All. 3.4.5 Result Predicted values are stored in your local path as 'answer_LSTM.txt' 3.5 GRU GRU model can be run locally. We have used python language. 3.5.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.5.2 GRU code Load the GRU.py code in your local path 3.5.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.5.4 Run Run GRU.py code in your local. 3.5.5 Result Predicted values are stored in your local path as 'answer_GRU.txt' 3.6 Naive Bayes and Logistic Regression Google Colab is preferred for running Naive Bayes and Logistic Regression model. We have used Colab for training and evaluating test dataset. 3.6.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose LGNB_CV.ipynb 3.6.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files from https://github.com/gnsandeep/CourseProject/tree/main/data. 3.6.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.6.4 Result Predicted values are stored in answerNB.txt and answerCVLG.txt, we can see the file in Files section Colab. 3.7 Bi-directional Models (GRU & LSTM) Bi-directional models can be run locally. We have used python language. 3.7.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.7.2 Bi-directional model code Load the BidirectionalModels.py code in your local path 3.7.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.7.4 Run Run BidirectionalModels.py code in your local. 3.7.5 Result Predicted values are stored in your local path as 'answer_blstm.txt' & answer_bgru.txt Results Below are some of the results that were captured in leaderboard live data lab for individual model: Model Precision Recall F1 Base Line 0.723 0.723 0.723 BERT 0.7181913774973712 0.7588888888888888 0.7379794705564559 Logistic Regression 0.6593406593406593 0.8 0.7228915662650602 Naive Bayes 0.6177215189873417 0.8133333333333334 0.702158273381295 SVM 0.6225716928769658 0.7477777777777778 0.6794548207975769 CNN 0.6376811594202898 0.6844444444444444 0.660235798499464 LSTM 0.620242214532872 0.7966666666666666 0.6974708171206226 GRU 0.5968109339407744 0.8733333333333333 0.7090663058186738 Bidirectional- GRU 0.6535819430814525 0.74 0.69411151641479 Bidirectional -LSTM 0.639927073837739 0.78 0.703054581872809 Screen shot for BERT Collaboration Brief description of contribution of each team member in case of a multi-person team. We had frequent meetings, we discussed about the models we had learnt, built, tested and results. We also did code reviews, incorporated suggestions. Sandeep: Build and test of LSTM on word embeddings using glove. Build and test of Naive Bayes and Logistic Regression. Build and test of BERT. Documentation for BERT, Naive Bayes and Logistic Regression models. Bidirectional LSTM along with combining Text & Non-Text features. Voice over project presentation and demo. Investigation & Test of Bi-directional LSTM model with multiple input features. Sunitha: Build and test of GRU on word embeddings using word2vec and glove. Investigation & Test of Bi-directional GRU model with multiple input features. Implementation for additional preprocessing steps to convert emojis and emoticons to text. Implementation of ModelCheckpoint ,ReduceLROnPlateau. Test effects of additional emoji pre-processing, EarlyStopping, ModelCheckpointing & ReduceLROnPlateau on GRU, LSTM & CNN models. Documentation for GRU & Bi-directional Models. Valentina: Creating word vectors by word2vec method, create weight matrix from word2vec gensim model, getting embedding vectors from word2vec and using it as weights of non-trainable keras embedding layer. Build and test of SVM model using cross validation and grid search to find good hyperparameters by building a pipeline. Implementation of early stopping - Build network and train it until validation loss reduces. Build and test of CNN on word embeddings using word2vec and glove. Convert test and train features to InputFeatures that BERT understands, create model using pooled output, layer for tuning, dropout, labels conversion to one hot encoding; and get predictions. Investigation & Test of LSTM model with word2vec, glove and different parameters. Documentation for project report, documentation overview, SVM, CNN and LSTM models. Presentation Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7-b1b43508023f References [Online] / auth. Scikit Learn. - http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html. https://mccormickml.com/2019/07/22/BERT-fine-tuning/ [Online] / auth. McCormick. Combining numerical and text features in deep neural networks [Online] / auth. Freischlag Christian. - https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks-e91f0237eea4. https://github.com/ibrahimcelenli/cnn-word2vec-tweets-classification / auth. Celeni Ibrahim. https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation [Online] / auth. Paletto J.D.. https://www.kaggle.com/lbronchal/sentiment-analysis-with-svm [Online] / auth. Bronchal Luis. Sarcasm Detection using LSTM, GRU, (85% Accuracy) [Online] / auth. Kohli Nikhil. - https://www.kaggle.com/nikhilkohli/sarcasm-detection-using-lstm-gru-85-accuracy. Sentiment Analysis in Python with keras and LSTM [Online] / auth. nana roblex. - https://www.kaggle.com/roblexnana/sentiment-analysis-with-keras-and-lstm. Sentiment Analysis with Bidirectional LSTM [Online] / auth. LillySimeonova. - https://www.kaggle.com/liliasimeonova/sentiment-analysis-with-bidirectional-lstm. Sentiment Analysis with Text Mining [Online] / auth. Carremans Bert. - https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27."
https://github.com/gnsandeep/CourseProject	Team_Dream_Text_Project_Documentation.pdf	"Public Domain CS-410 Text Information Systems PROJECT DOCUMENTATION Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 Public Domain INDEX 1. An overview of the function of the code ---------------------------------------------------------------------------3 2. Implementation of models for Tweet Classification -------------------------------------------------------------4 2.1 BERT ----------------------------------------------------------------------------------------------------------------------4 2.2 SVM -----------------------------------------------------------------------------------------------------------------------5 2.3 CNN -----------------------------------------------------------------------------------------------------------------------6 2.4 LSTM ----------------------------------------------------------------------------------------------------------------------8 2.5 GRU -----------------------------------------------------------------------------------------------------------------------9 2.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------10 2.7 Bi-directional Models (GRU & LSTM) ----------------------------------------------------------------------------11 3. Tweet Classification Model Usage ------------------------------------------------------------------------------------13 3.1 BERT ---------------------------------------------------------------------------------------------------------------------13 3.2 SVM ---------------------------------------------------------------------------------------------------------------------14 3.3 CNN ----------------------------------------------------------------------------------------------------------------------15 3.4 LSTM --------------------------------------------------------------------------------------------------------------------15 3.5 GRU ----------------------------------------------------------------------------------------------------------------------16 3.6 Naive Bayes and Linear Regression ------------------------------------------------------------------------------17 3.7 Bi-directional LSTM and GRU -------------------------------------------------------------------------------------18 4. Results -----------------------------------------------------------------------------------------------------------------------19 5. Collaboration ---------------------------------------------------------------------------------------------------------------20 6. Presentation ----------------------------------------------------------------------------------------------------------------21 REFERENCES ------------------------------------------------------------------------------------------------------------------21 Public Domain 1. Overview Our goal is to classify the given test set data of twitter responses as ""SARCASM"" or ""NOT SARCASM"" by using various classification methods. Have classified the test data using Linear Regression, Naive Bayes, GRU, CNN, LSTM, SVM, Bidirectional and BERT models. While predicting the label of the ""response"" based on the ""context"". Context which is an ordered list of dialogue for which response is a reply to the last dialogue in the context. Test data has unique ids along with tweet ""responses"" to be classified. We build, train several different models using given trained data and predict the test data using the trained model and generate an ""answer.txt"" file that has unique ids along with the predicted label. Since there are 1800 ids in test dataset, our generated ""answer.txt"" file has exactly 1800 rows. At a high level, we perform the below steps: * Preprocess both the training and test data by removing the html tags, converting the tweets to lower case, removing punctuations and numbers, removing stop words, converting emojis and emoticons, removing single character and multiple spaces, removing left over special characters. * Split the test dataset into train and test data. * Create the embedding matrix using glove or word2vec embeddings. * Create the model using embedding layer, adding layers such as LSTM, CNN etc. * Fit the model on train data * Predict the model on test data and generating predicted labels Once the answer.txt file is generated and uploaded to git hub, predicted label values are compared with the actual result set to obtain the precision, recall and F-score value. Precision indicates fraction of relevant instances among the retrieved instances. Precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly. Recall indicates fraction of the total amount of relevant instances that were retrieved. Recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive. F-score is the harmonic mean of the precision and recall. The highest possible value of an F-score is 1. Our baseline to be achieved is 0.723 We were successfully able to beat the baseline using Logistic Regression and BERT model. We will be walking through further in below points regarding implementation and usage for all the models we tried. The models that we tried can be used to classify any tweets that has response and context attributes. The models can be leveraged for other applications with appropriate changes such as classification of tags in news headline, sentiment analysis for movie reviews etc. Public Domain 2. Implementation of Models for Tweet Classification This section describes implementation of different Tweet Classification Models 2.1 BERT We have used BERT (Bidirectional Encoder Representations from Transformers) model BertForSequenceClassification for Tweet classification. Model is built using Jupyter Notebook. Jupyter Notebook has following Sections in same order. Chris McCormick Blogs and code about BERT were very helpful for using BERT for classification. We have used certain sections of this code from (McCormick) in our Model. 2.1.1 Preprocessing of Data Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. URLs, @, non-ASCII characters, extra space, &, <, > are removed, space is inserted between punctuation marks. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.1.2 Model We have used transformers package from Hugging Face which will give us a PyTorch interface for working with BERT. We have installed transformers package. 2.1.3 Tokenizing Input and creating DataLoaders To feed our text to BERT, it must be split into tokens, we need to add special tokens to start and end, Pad and Truncate all sentences to a single constant length (we have used max length as 256), differentiate real tokens from padding tokens with attention mask. and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. We have used ""uncased"" version - ""bert-base-uncased"". Training dataset is split for training and validation. We will also create an iterator for our training, validation and test dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. Note: We have used SequentialSampler for validation and test datasets. 2.1.4 Pre-Trained BERT Model After this we have loaded Pre-Trained BERT model - BertForSequenceClassification with a single linear classification layer on top. 2.1.5 Model Parameters and Learning Rate Scheduler Public Domain We have used AdamW Optimizer with learning rate of 2e-5 and eps of 1e-8 and get_linear_schedule_with_warmup is created out of the Optimizer. 2.1.6 Utility Functions Utility function format_time is created for formatting time and utility function flat_accuracy is created for calculating accuracy. 2.1.7 Training Loop Each pass in our loop we have a training phase and a validation phase Training: * Unpack our data inputs and labels * Load data onto the GPU for acceleration * Clear out the gradients calculated in the previous pass. o In PyTorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out. * Forward pass (feed input data through the network) * Backward pass (backpropagation) * Tell the network to update parameters with optimizer.step() * Track variables for monitoring progress Evalution: * Unpack our data inputs and labels * Load data onto the GPU for acceleration * Forward pass (feed input data through the network) * Compute loss on our validation data and track variables for monitoring progress 2.1.8 Evaluating Test Data Prediction is done on test dataset. Test dataset is read in batches and prediction is appended to list, which is then added to test panda's data frame and answer_BERT.txt is created. 2.2 SVM We have used Support Vector Machines model for Tweet classification. Code is written in python language. We have used certain sections of code from (Bronchal) in our model: 2.2.1 Preprocessing of Data Public Domain Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. HTML tags, punctuation, numbers, single character, multiple spaces, stop words, left over special characters are removed. All characters are converted to lower case for both training and test dataset. Labels in training are converted to 1 and 0 (1 for Sarcasm and 0 for Not Sarcasm). 2.2.2 Tokenizing Input Twitter-aware tokenizer designed to be flexible and easy to adapt to new domains and tasks. Tuple regex_strings define a list of regular expression strings. regex_strings strings are put, in order, into a compiled regular expression object called word_re. The tokenization is done by word_re.findall(s), where s is the user-supplied string, inside the tokenize() method of the class Tokenizer. When instantiating Tokenizer objects, there is a single option: preserve_case. By default, it is set to True. If it is set to False, then the tokenizer will downcase everything except for emoticons. 2.2.3 Vectorize CountVectorizer converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. The following parameters are used: Analyzer: feature is made of word n-grams. Tokenizer: override the string tokenization step while preserving the preprocessing and n-grams generation steps Lowercase: convert all characters to lowercase before tokenizing. Ngram_range: The lower and upper boundary of the range of n-values for different word n-grams to be extracted i.e. (1,1) Stop_words: a built-in stop word list for English is used 2.2.4 Cross validation and grid search We use cross validation and grid search to find good hyperparameters for our SVM model. We build a pipeline to get features from the validation folds when building each training model. GridSearchCV implements a ""fit"" and ""predict"" method. Snapshot of code below for reference. It exhaustively searches over specified parameter values for an estimator. param_grid enables searching over any sequence of parameter settings. cv determines the cross-validation splitting strategy. n_jobs as -1 means to use all processors in parallel. Verbose controls the verbosity of messages. Scoring is to evaluate the predictions on the test set. Public Domain 2.2.5 Evaluating Test Data Model with the best hyperparameters works on test data and basis the prediction, labels generated with value greater than 0.5 as ""SARCASM"" else ""NOT_SARCASM"" is appended to test panda's data frame and answer_SVM.txt is created. 2.3 CNN We have used Convolutional neural network model for Tweet classification. Code is written in python language. We have used certain sections of this code from (Celeni) in our model: 2.3.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for CNN model. 2.3.2 Word embedding Word embedding is vector representation of a particular word. Weight matrix is created from word2vec gensim model. And then embedding vectors are obtained from word2vec and using it as weights of non- trainable keras embedding layer. Corpus for twitter data for word2vec named as ""3000tweets_notbinary"" was referred from (Celeni) We also build the embeddings using Global Vectors for Word Representation. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. Corpus can be downloaded from the link (Paletto) that we used as embedding file in our code to build the weight matrix. 2.3.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of CNN layer that creates a convolution kernel that is convoluted with the layer input over a single spatial dimension to produce a tensor of outputs. The action ReLU is applied to outputs as well. We set the number of filters to the dimensionality of the output space. Kernel_size is to specify the length of the 1D convolution window. Further we maxpool that summarize the most activated presence of a feature. Pooling is required to down sample the detection of features in feature maps. We also use global pooling that that down sample the entire feature map to a single value. This is same as setting the pool_size to the size of the input feature map. We then use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. Public Domain Code snippet below: 2.3.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.3.5 Evaluating Test Data Once the model is created, model.predict() is run to predict for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_CNN.txt is created. 2.4 LSTM We have used Long Short-Term Memory network model for Tweet classification. Code is written in python language. We have used certain sections of this code in our model from (nana). 2.4.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for LSTM model. 2.4.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for LSTM model. 2.4.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding non trainable embedding layer followed by the addition of LSTM layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Public Domain Code snippet below: 2.4.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.4.5 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_LSTM.txt is created. 2.5 GRU We have tried GRU (Gated Recurrent Unit) model for Tweet classification. Code is written in python language. GRU is a special type of Recurrent Neural network. This type of sequence model can retain information from long ago, without washing it through time or remove information which is irrelevant to the prediction. Some of the code & hyper-parameter values specifically those related to ReduceLROnPlateau were decided based on the blog by (Kohli) 2.5.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for GRU model. 2.5.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for GRU model. 2.5.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU layers with units specifying the dimensionality of the output space. We use regularization method ""dropout"" that approximates training many neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or ""dropped out."" This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. Also, we then add dense layer to perform Public Domain a linear operation on the layer's input vector. We compile the model using the binary cross-entropy loss function since it predicts a binary value and opt optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.5.4 ReduceLROnPlateau This option was used so that the learning rate to be reduced when training is not progressing. 2.5.5 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.5.5 Model Checkpointing Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.5.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_GRU.txt is created. 2.6 Naive Bayes and Linear Regression Naive Bayes and Logistic Regression Models are built using Jupyter Notebook for Tweet Classification. Jupyter Notebook has following Sections in same order. Bert Carremans Blog and code about Naive Bayes and Linear Regression were very helpful. We have used certain sections of code in our Model from the reference (Carremans) 2.6.1 Preprocessing of Data Public Domain Two data files were provided - train.jsonl and test.jsonl . Training and test data are read into Pandas data frame. Python functions load_training_data_to_pandas and load_test_data_to_pandas load training and test data to panda's data frame. We have Python class TextCounts , it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class extracts additional features like word counts, hash tags, mentions, capital words, question marks, urls and emojis. We have Python class CleanText it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class removes mentions, urls, oneword, punctuations, digits, Stopwords , performs stemming and converts to lower case. Test and Training data is passed through this classes fit methods. Extra features extracted for training data is combined with Cleaned data for both Training and Test data. Steps for Test data is done after training. We have Python class ColumnExtractor it extends sklearn.base.BaseEstimator and sklearn.base.TransformerMixin , this class is used for selecting columns in test and training dataset. 2.6.2 Hyperparameter tuning and cross-validation We first declare parameters for grid search. We have set parameters_vect , parameters_mnb and parameters_logreg parameters. We have parameters for TF-IDF like max_df , min_df , ngram_range , parameters for Linear Regression like clf_c and clf_penality , parameters for Naiive bayes like alpha. we have function grid_vect1 which does grid search. This function uses skilearn pipeline and it is based on below code from the reference (Scikit Learn) . 2.6.3 Model Training and Prediction Naiive Bayes and Logistic Regression models are created and passed to grid_vect1 which vectorizes the data using TF-IDF in ski learn pipeline and does grid search for best Hyper parameters. Once we find the best hyperparameters, we fit both Naive bayes and Logistic regression models with best parameters and perform prediction on test data. Predicted data is added to pandas data frame as new column and answer.txt is created for both models. 2.7 Bi-directional Models (GRU & LSTM) We have tried bi-directional versions of both GRU (Gated Recurrent Unit) model & LSTM model for Tweet classification. Code is written in python language. Bidirectional models are an extension of traditional LSTM & GRU models that can improve model performance on sequence classification problems. In problems where all timesteps of the input sequence are available, Bidirectional models train two instead of one LSTMs on the input sequence. Some of the code was referenced from (LillySimeonova) Public Domain 2.7.1 Preprocessing of Data For preprocessing of twitter test and train data, same function that was used in BERT model as described in section 2.2.1 has been also used for these models as well. In addition, we also combined the additional text counts features calculated in the LogisticRegression model and combined them with the text features in a bid to improve model performance on the test set. 2.7.2 Word embedding Same functional code as discussed in Section 2.3.2 is used for Bidirectional LSTM model & Bidirectional GRU. 2.7.3 Train Embedding Model Depending upon whichever embedding - word2vec or glove, we are using we train the model by adding a trainable embedding layer followed by the addition of GRU/LSTM layers with units specifying the dimensionality of the output space. We are then combining text features with count features using multiple input models. All the other layers are added exactly like GRU/LSTM models already discussed in section 2.5 & 2.4. Combining of text features with non-text was coded by referring (Freischlag) We compile the model using the binary cross-entropy loss function since it predicts a binary value and Adam optimizer. The hyperparameters were tuned experimentally over several runs. Code snippet below: 2.7.4 Early stopping Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold-out validation dataset. 2.7.5 Model Checkpointing Public Domain Model checkpointing was used so that the best training model based on hold-out set validation would be saved and the saved model used to evaluate test data. 2.7.6 Evaluating Test Data Once the model is created, model.predict() is done to do the prediction for test data where prediction with value greater than 0.5 is labelled as ""SARCASM"" else ""NOT_SARCASM"". Label is appended to the test panda's data frame and answer_Bidirectional.txt is created. 3. Tweet Classification Models Usage This Section describes steps for running different Tweet Classification Models which are described in Section 2. 3.1 BERT Google Colab is preferred for running BERT model. We have used Colab for training and evaluating test dataset. 3.1.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose BERTSeq.ipynb 3.1.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files. 3.1.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.1.4 Result Predicted values are stored in answer_BERTSeq.txt, we can see the file in Files section Colab. Public Domain 3.2 SVM SVM model can be run locally. We have used python language. 3.2.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/CS410Fall2020/ClassificationCompetition/tree/main/data to your local path under data folder. 3.2.2 SVM code Load the SVM.py code in your local path 3.2.3 Prerequisites Ensure to download and install the below libraries and modules to run the code to not throw any errors 3.2.4 Run Run SVM.py code in your local. 3.2.5 Result Predicted values are stored in your local path as 'answer_SVM.txt' 3.3 CNN CNN model can be run locally. We have python language. 3.3.1 Load Training and Test data Public Domain First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.3.2 CNN code Load the CNN.py code in your local path 3.3.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors 3.3.4 Run Run CNN.py code in your local. 3.3.5 Result Predicted values are stored in your local path as 'answer_CNN.txt' 3.4 LSTM LSTM model can be run locally. We have used python language. 3.4.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.4.2 LSTM code Jupyter notebook LSTM.ipynb . 3.4.3 Prerequisites Public Domain Please download Glove embedding file glove.6B.100d.txt ( it is available under glove.6B.zip) and place it in data folder. We can download Glove embedding file from https://nlp.stanford.edu/projects/glove/ Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip 3.4.4 Run Open the Jupyter notebook LSTM.ipynb and click Kernel Restart and Run All. 3.4.5 Result Predicted values are stored in your local path as 'answer_LSTM.txt' 3.5 GRU GRU model can be run locally. We have used python language. 3.5.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.5.2 GRU code Load the GRU.py code in your local path 3.5.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors Public Domain 3.5.4 Run Run GRU.py code in your local. 3.5.5 Result Predicted values are stored in your local path as 'answer_GRU.txt' 3.6 Naive Bayes and Logistic Regression Google Colab is preferred for running Naive Bayes and Logistic Regression model. We have used Colab for training and evaluating test dataset. 3.6.1 Google Colab Open Colab in browser (Mozilla, Chrome) https://colab.research.google.com , choose GitHub and use our Project GitHub URL and chose LGNB_CV.ipynb 3.6.2 Load Training and Test data to google Colab Click on Files and Click on upload button and upload both test and train jsonl files from https://github.com/gnsandeep/CourseProject/tree/main/data. Public Domain 3.6.3 Run Click on Runtime and click on Run All to Run all the Cells. 3.6.4 Result Predicted values are stored in answerNB.txt and answerCVLG.txt, we can see the file in Files section Colab. 3.7 Bi-directional Models (GRU & LSTM) Bi-directional models can be run locally. We have used python language. 3.7.1 Load Training and Test data First place the test.json and train.json data files as provided in github link https://github.com/gnsandeep/CourseProject/tree/main/data to your local path under data folder. 3.7.2 Bi-directional model code Load the BidirectionalModels.py code in your local path 3.7.3 Prerequisites Ensure to download and install the below libraries and modules to run the code without throwing any errors Public Domain 3.7.4 Run Run BidirectionalModels.py code in your local. 3.7.5 Result Predicted values are stored in your local path as 'answer_blstm.txt' & answer_bgru.txt 4. Results Below are some of the results that were captured in leaderboard live data lab for individual model: Public Domain Model Precision Recall F1 Base Line 0.723 0.723 0.723 BERT 0.7181913774973712 0.7588888888888888 0.7379794705564559 Logistic Regression 0.6593406593406593 0.8 0.7228915662650602 Naive Bayes 0.6177215189873417 0.8133333333333334 0.702158273381295 SVM 0.6225716928769658 0.7477777777777778 0.6794548207975769 CNN 0.6376811594202898 0.6844444444444444 0.660235798499464 LSTM 0.620242214532872 0.7966666666666666 0.6974708171206226 GRU 0.5968109339407744 0.8733333333333333 0.7090663058186738 Bidirectional- GRU 0.6535819430814525 0.74 0.69411151641479 Bidirectional -LSTM 0.639927073837739 0.78 0.703054581872809 Screen shot for BERT 5. Collaboration Brief description of contribution of each team member in case of a multi-person team. We had frequent meetings, we discussed about the models we had learnt, built, tested and results. We also did code reviews, incorporated suggestions. Sandeep: 1. Build and test of LSTM on word embeddings using glove. 2. Build and test of Naive Bayes and Logistic Regression. 3. Build and test of BERT. 4. Documentation for BERT, Naive Bayes and Logistic Regression models. 5. Bidirectional LSTM along with combining Text & Non-Text features. 6. Voice over project presentation and demo. 7. Investigation & Test of Bi-directional LSTM model with multiple input features. Sunitha: 1. Build and test of GRU on word embeddings using word2vec and glove. 2. Investigation & Test of Bi-directional GRU model with multiple input features. 3. Implementation for additional preprocessing steps to convert emojis and emoticons to text. 4. Implementation of ModelCheckpoint ,ReduceLROnPlateau. Public Domain 5. Test effects of additional emoji pre-processing, EarlyStopping, ModelCheckpointing & ReduceLROnPlateau on GRU, LSTM & CNN models. 6. Documentation for GRU & Bi-directional Models. Valentina: 1. Creating word vectors by word2vec method, create weight matrix from word2vec gensim model, getting embedding vectors from word2vec and using it as weights of non-trainable keras embedding layer. 2. Build and test of SVM model using cross validation and grid search to find good hyperparameters by building a pipeline. 3. Implementation of early stopping - Build network and train it until validation loss reduces. 4. Build and test of CNN on word embeddings using word2vec and glove. 5. Convert test and train features to InputFeatures that BERT understands, create model using pooled output, layer for tuning, dropout, labels conversion to one hot encoding; and get predictions. 6. Investigation & Test of LSTM model with word2vec, glove and different parameters. 7. Documentation for project report, documentation overview, SVM, CNN and LSTM models. 6. Presentation Video Presentation : https://web.microsoftstream.com/video/4436fc73-53d1-4dc4-a386-29e7c8e20eca Leaderboard Results : https://web.microsoftstream.com/video/7ae237ae-9c20-4a19-b4b7- b1b43508023f References [Online] / auth. Scikit Learn. - http://scikit- learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html. https://mccormickml.com/2019/07/22/BERT-fine-tuning/ [Online] / auth. McCormick. Combining numerical and text features in deep neural networks [Online] / auth. Freischlag Christian. - https://towardsdatascience.com/combining-numerical-and-text-features-in-deep-neural-networks- e91f0237eea4. https://github.com/ibrahimcelenli/cnn-word2vec-tweets-classification / auth. Celeni Ibrahim. https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation [Online] / auth. Paletto J.D.. https://www.kaggle.com/lbronchal/sentiment-analysis-with-svm [Online] / auth. Bronchal Luis. Public Domain Sarcasm Detection using LSTM, GRU, (85% Accuracy) [Online] / auth. Kohli Nikhil. - https://www.kaggle.com/nikhilkohli/sarcasm-detection-using-lstm-gru-85-accuracy. Sentiment Analysis in Python with keras and LSTM [Online] / auth. nana roblex. - https://www.kaggle.com/roblexnana/sentiment-analysis-with-keras-and-lstm. Sentiment Analysis with Bidirectional LSTM [Online] / auth. LillySimeonova. - https://www.kaggle.com/liliasimeonova/sentiment-analysis-with-bidirectional-lstm. Sentiment Analysis with Text Mining [Online] / auth. Carremans Bert. - https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27."
https://github.com/gnsandeep/CourseProject	Team_Dream_Text_Project_Progress_Report.pdf	PROJECT PROGRESS REPORT Sandeep Nanjegowda - sgn3 (Captain) Sunitha Vijayanarayan - sunitha3 Valentina Mondal - vmondal2 1. Progress made so far Models Tried: * Logistic Regression * Naive Bayes * LSTM (Long-short term memory) * GRU (Gated recurring units) * CNN (Convolutional Neural Network) * SVM (Support Vector Machine) Pre-processing & Feature extraction: * Have tried word embeddings like glove and word2vector on the models: LSTM, GRU and CNN. * New features like word counts, emojis, hash tags were extracted and using for Logistic Regression and Naive bayes. * Both test and training set data was initially processed to remove html tags, punctuations, numbers, single characters, multiple spaces. * Models were trained and fitted using the given twitter trained set data by splitting it into training and test set. Further the model was applied on the test data set to find out the precision, recall and F-Score. More Details about models tried: * Logistic Regression: Logistic Regression model performed well on the test set data and gave precision of 0.658, recall of 0.808 and F-score of 0.725. Were able to beat the baseline using the LR model. * Naive Bayes: Naive Bayes did well on test data and gave F-Score of 0.702 * Neural Network Models - LSTM, GRU & CNN: We tried 3 neural network models LSTM, GRU, CNN with different combinations of hyper parameters. The maximum F- score obtained in all these neural network methods with different combinations of hyper parameters was 0.69. We tried word embedding with glove and word2vec but got almost the same results and not able to beat the baseline. Also tried without Glove and word2vec by training on all the words, but that model performed well only in local tests and did poorly in live data lab. * SVM: For SVM, after using cross validation and grid search to find the good hyperparameter for SVM model, still the model could not beat the baseline with the test set data. Got an F-score of around 0.68. 2. Remaining tasks * We have extracted additional features such as word counts, number of has tags, number of emojis in each of the tweet and used them in Logistic Regression. We are planning to use these features in Neural Network Models. * Project documentation and presentation. * We are looking at adding additional pre-processing steps like converting emojis and emoticons to text. 3. Any challenges/issues being faced * Unable to beat the baseline (i.e., F-score of 0.723) in live lab leaderboard using LSTM, GRU and CNN. * For neural network models like LSTM and GRU, overfitting is a problem as we get very good results on the graded test set in some cases but are not able to replicate the results on the leaderboard. * Some models took very long time to train based on the hyper-parameters chosen.
https://github.com/Wenfan1993/CourseProject	CS410 Project Proposal (Submission).docx	CS410 Text Information Systems - Team: Fan & Jain Rhymes Project Proposal Team Members: Member Name NetID Email Wenxi Fan (Captain) wenxif2 wenxif2@illinois.edu Abhishek Jain aj26 aj26@illinois.edu Competition to join: Text Classification Language to use: Python Our team's key motivation to join the Text Classification Competition is to go beyond what has been taught in the course, study state-of-the-art NLP tools and apply it on a real-world problem. This competition gives us such an opportunity where we cannot just learn the latest tools by solving a real-world problem but also it challenges us to raise our bars to implement optimal solutions to meet the baseline scores. We are extremely excited to learn, participate and give our best performance. In our current professional engagements, both of us are part of AI teams respectively where machine learning tools & techniques are used for problem solving. Neural networks being the most advanced & extensively used methodologies in our respective enterprises, we both have got a chance to work and closely observe best practices & implementation styles using it. We have high level understanding and hands on experience of ANN, RNN, CNN & LSTM architecture styles and its realization using TensorFlow Keras & PyTorch based implementation. For text related problems we have awareness on usage of word embeddings viz. Word2Vec & GloVe. With the latest developments in NLP, we are aware of the release of newer advanced tools like BERT, OpenAI GPT1/2/3, HuggingFace Transformers which we have not yet explored. These tools are also in our consideration list for approaching the problem solution. Wenxi was engaged in building RNN models for contract terms cleaning and classification, and in building the Seq2Seq model for document cleaning and document state mapping. Abhishek has experience of working closely with data science teams which utilizes neural networks-based framework implementation to solve several problems like task sequence prediction, call conversation compliance, sales forecasting, customer look-a-likes and segmentation in customer relationship management domain. For this project, we will explore the sequential text features, and learn the associations between words in the short text. We will plan to explore the models that combine the text ranking algorithms and machine learning models (e.g. TF-IDF and SVM). Then, we will plan to build models with Recurrent Neural Networks, or Convolutional Neural Networks or a combination of both and learn to improve the model performance with different architectures. We also plan to explore attention-based models and leverage attention models/hierarchical attention models to explore the performance impact. We will compare the different models and select the model with the best performance.
https://github.com/Wenfan1993/CourseProject	Progress Report (Submission).pdf	CS410 Text Information Systems - Team: Fan & Jain Rhymes Progress report Task completed: Below are the tasks we have completed so far: 1. Solution Architecture Design & Approach Finalization: Team planned to use several state-of-art architectural styles to experiment & validate the results & compare different approaches results. Approach is to start from conventional styles and gradually progress to use advance methodologies. a. Conventional ML methodologies b. Deep Learning based implementations c. Attention & Transformers based implementations. 2. Environment setup a. Analyzed different environment viz. local desktop, Google Colab and Cloud to setup experimentation playground. Cost effectiveness & high processing needs were the key parameters considered. b. Colab Pro environment was preferred over others which enabled us using High Memory & GPU/TPU based processing for Deep Learning based implementations. 3. ML Pipeline setup: a. Data Import: Training & Test data was imported to google drive & authentication setup was done to access it. b. Data Preprocessing: Data clean step is performed to address words spelling error, repetition, signs & emoji. c. Feature Engineering: Several features were constructed to support solution approach as multi sentence sequence & single document classification problem. d. Model Training/Fine Tuning: Select, train, and evaluate the model. For pre-trained models fine tuning step was performed considering different solution classification styles. e. Prediction: Output the predictions. 4. ML Models: Below are several modelling strategies which team has evaluated for given Classification Problem. a. TF-IDF + dimensionality reduction (via SVD) + Tree (Random forest). b. General LSTM model (with one or multiple LSTM layers followed by fully connected layers). c. Transformer (attention-based) models leveraging hugging face pre-trained models: Roberta, BERT, XLM, based on which we fine-tune for our task. 5. Observations: We have compared all the approaches & results are per our expectation as below: - Conventional ML (TFIDF/SVD/Random Forest) based implementation was not able to beat the baseline score. - Deep Learning (LSTM based) implementation - 2-3 LSTM layers followed by 2-5 fully connected layers, which implementation was not able to beat the baseline score. - Transformer based approach: CS410 Text Information Systems - Team: Fan & Jain Rhymes o Bert base: Just at par with Baseline results but ranked intermediate on leader board. o Roberta base: Performed very well with highest accuracy, precision & F1 values (achieved 72.3% accuracy, 80.4% recall, 76.2% F1) Task pending: Below are the tasks we plan to complete before the final submission: 1. We are planning to further explore other models for our task, that includes: GPT-2, GPT-3. 2. We are planning to complete the documentation of our pipeline to be prepared for the final submission. Challenges faced: Below are the challenges we faced: 1. Noises in the data: The input data appears to have a lot of noises - words spelling error, repetition, words/signs/emoji that appear to occur very few times. In addition, there are many words/signs in test sets that are not included in the training set. These create challenges in extracting useful information out from the inputs and challenges in generalizing the model to the data not included in the training process. 2. The way to leverage context/response to engineer effective input features: Currently we use the context/response by inputting them as two separate features or concatenating as one (and/or reversing orders). Although we leveraged the transformer models that generate attention mechanism(both on words and position of words), we have not yet been able to explore a way ourselves to engineer features that might be more effective as inputs than pure words and sentences. The example of such inputs could be sentiment of sentences, the hierarchical attention from character-level to sentence level. 3. Hyper-parameter tuning We found challenges in hyper-parameter tuning, especially when the parameter space is large. We used 'Trainer' (utility from transformers) that helped automate the parameter search, but still the parameter space is large, and getting the best model may take time. 4. Large Model We are unable to tune large transformer models like ROBERTA_LARGE & BERT_LARGE models due to GPU memory issues. 5. Select the performance criteria We found challenges in selecting models that generates well on test data - although we used validation set in addition to test set (which sets are separated out from training data), the model that test well (good F1) on validation and test set may not generalize well to the other data not seen in the training process. Instead of just using F1 score, the one datapoint, as the criteria to judge the performance, the below steps were performed that achieved better performance criteria (which gives better picture on the performance of model): CS410 Text Information Systems - Team: Fan & Jain Rhymes a. Evaluate the distribution of scores on multiple batches of the validation set, to analyze the level and consistency of the performance. b. In addition to evaluate precision/recall/f1 the level of performance, we could also evaluate from a different perspective, such as analyzing the correlation between prediction and ground truth.
https://github.com/Wenfan1993/CourseProject	readme.md	Colab Notebook Final version of our project code is available as Google Colab Notebook which is shared and available at SequenceClassification. Through this code, we trained and fine-tuned the RoBERTa based transformer model for the text classification competition (identifying if the tweet is SARCASM or NOT_SARCASM), and we are able to achive f1 score higher than the base-line. Below are the several steps detailed to be execute to run the code end to end. Since modelling methodology is Transformer based it would be recommended to use GPU for processing. Google Colab is the preferred environment to run the end to end process and generate the results. Use of our model We have created the checkpoint our model, available to download from the below path. For use of our model, user can just just load the checkpoint to the API 'AutoModelForSequenceClassification'. (See https://huggingface.co/transformers/model_doc/auto.html for more details) For the use case of our text competition (and as an example of using our model), please refer to colab notebook RoBERTa_Model_Test fpr using the checkpoint of our model and reproducing answer.txt. Tutorial: Reproduce_Answers_with_Checkpoint Checkpoint available to Download from: Checkpoint See slide 11 for detailed step-by-step guidance Presentation Presentation and Model Testing Please see Presentation for the slides we created for the presentation. Please see video Tutorial: Reproduce_Answers_with_Checkpoint for step by step guidance how to re-run and test our model Contribution of Team Members Each of us collaborated very closely in each step of reseach, experiment, and improvise. We have touch point scheduled on a weekly basis, where we shared the learning and resources, discuss our approach, and walk-through our codes. With that, each of us contributed 100% effort in each step of the project process. Our team members are: Wenxi Fan (NetID: wenxif2; Email:wenxif2@illinois.edu) Abhishek Jain (NetID: aj26; Email:aj26@illinois.edu) Below are the Steps in our Model Training and Output Generation (as in SequenceClassification.) Environment Setup First setup the environment, we will do the following steps here. - Transformers Model Installation - Hyper Parameter Tuning Library Installation - Colab Setup You will be required to authorize the code using your google account. Copy the authorization code generated and pass it in the notebook in the input box provided when you run mount drive code. Below is the reference: ```py Colab setup from google.colab import drive drive.mount('/content/drive', force_remount=True) ``` Also please copy the train & test JSONL files provided in your google drive required for training and testing the models further. Tutorial: Environment Setup Data Load Next step is to load the Training & Test Datasets as Pandas dataframe. Please update correct data path where training and test dataset is copied in your google drive. py datapath = r'/content/drive/My Drive/mcsds/cs-410-text-mining/project/ClassificationCompetition/data' train_pddf = pd.read_json(datapath+'/train.jsonl', lines=True) test_pddf = pd.read_json(datapath+'/test.jsonl', lines=True) Above example suggests my train & test jsonl files are copied in my drive at '/mcsds/cs-410-text-mining/project/ClassificationCompetition/data' location. Further run the data load section. Reference: Data Load & Preprocessing Data Preprocessing Next step is to run the data preprocessing steps. Below are the different components of it: Feature Engineering Create new features: * Last Response: Extract the last response from the context since the current response was generated on Last this can be separately treated. * Context Reversed: Reverse the context before feeding to transformers so that latest tweets are given more attention and incase if context is too big latest shall be considered. * Combine all into a single * Combine Current & Last Response into Single Sequence Structuring Define how do we want to structure the different tweets, basically two approaches are followed: * Combine into single: Last response only, Combine all tweets togeather or current and last. * Two Sentence: (Current, Last Response) or (Current, Context Reversed). Transform to Datasets Translate preprocessed dataframes to Transformer Datasets. This step is required to make our dataset translated into Transformer datasets construct. Reference: Data Load & Preprocessing Model Configurtion Configure which model strategy to select, train test valid splits, performance metrics, training batch sizes etc. Below are the details: 1. model_checkpoint: which model to use for text sequence classification. Roberta models are observed to give the maximum performance. 2. task: specify how to structure the sequences as described in sequence structuring step. We have observed the maximum performance with 'response_context_rev_sep' structure. This format structures input as two sequence where response is last tweet to be classified, and context tweets are the previous tweets in an reversed order of occurance. 3. metric_name: metric to be optimized while training. We have configured it to accuracy. 4. num_labels: 2, number of classes Sarcasm, Not Sarcasm 5. batch_size: 16 for roberta, 64 for bert otherwise we face out of memory issues. 6. train_test_split: to divide training data into train and test datasets. 7. test_valid_split: to divide test dataset into test and validation set. 8. epoch: number of epochs to train model on. 9. weight_decay: determines how much an updating step influences the current value of the weights 10. learning_rate: weight update rule that causes the weights to exponentially decay to zero Reference: Model Config Tokenization This step translates words to context tokens. Transformers Tokenizer tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires. Reference: Tokenization & Single Model Fine Tuning Single Model Fine Tuning Download the pretrained model and fine tune the selected model with arguments configured in the previous step. Reference: Tokenization & Single Model Fine Tuning Reference: Training Results Test Validation Validate the results on test data and compute the metrics. Reference: Validation Results Hyper Parameter Tuning Could be only run with HIGH GPU environment Using Transformer Trainer utility which supports hyperparameter search using optuna or Ray Tune libraries which we have installed in our previous step. During hyperparameter tuning step, the Trainer will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. The hyperparameter_search method returns a BestRun objects, which contains the value of the objective maximized and the hyperparameters it used for that run. Reference: Hyperparameter Tuning Best Run Selection & Training Could be only run with HIGH GPU environment To reproduce the best training run from our previous hyper parameter train setp we will set the best hyperparameters TrainingArgument before training the model again. Reference: Hyperparameter Tuning
https://github.com/Wenfan1993/CourseProject	Text Classification Competition – Attention-based Transformers.pptx	Text Classification Competition - Attention-based Transformers CS410: Text Information System Team: Fan & Jain Rhymes - Wenxi Fan (NetID: wenxif2; Email:wenxif2@illinois.edu) Abhishek Jain (NetID: aj26; Email:aj26@illinois.edu) Agenda: Project Goal and Our Approach Code Walkthrough Run the Code Test Our Results Project Goal Project Goal: Implement text classifier to be able to identify if the twit is sarcasm or not. Inputs: text (tweets) - Response : the tweet response to be classified Context : the conversation context of the response Labels: SARCASM or NOT_SARCASM (two classes) Goal: text classifier implemented to achieve f1 score >70% Motivation: Research, learn and implement best-in-class next generation techniques to approach text-based classification problems. Expand our knowledge & understanding in NLP space beyond what is taught in the course. Approach - Overview Research Goal Research available best-in-class NLP techniques Learn & understand the application of each techniques Outcome Attention-based Approach to Text Classification https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/AttentionBased%20Approach%20to%20Text%20Classification.pdf The Application of Attention Models in Text Classification https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/Application%20of%20Attention%20to%20Classification.pdf Experiment Goal Evaluate different NLP techniques & short list for final implementation Outcome Initial experiments conducted for baseline results TFIDF/SVM Bi-directional LSTM with embedding Transformer based Pre-Trained Model) Improvise Goal Optimize the best technique researched & tune it for result generation. Outcome Transformer based implementation were finalized Tuned with different input structures Used different pre-trained models like Bert, Roberta, XLM etc. Approach - Research Research Focus: Attention Mechanism, a breakthrough principle which has revolutionized building state-of-the-art NLP solutions. Detailed overview on different solution architectures styles which can be used to solve text classification problem. See below reference to our research documents: https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/AttentionBased%20Approach%20to%20Text%20Classification.pdf https://github.com/Wenfan1993/CourseProject/blob/main/Research_Summary/Application%20of%20Attention%20to%20Classification.pdf Approach - Experiment Approach - Improvise (1) Feature Engineering & Problem Structuring Input sequence structured as: Single Document Classification All tweets combined as single document for classification Response and Last tweet combined as single document for classification Sequence Classification Tweets modeled as two text sequence for classification Response, Last Tweet Response, Context Response, Context reversed Best results observed when problem modelled as Sequence Classification setup with text sequence as Response and Context reversed. Approach - Improvise (2) BERT and RoBERTa BERT Model - Bidirectional Encoder Representations from Transformer Designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers; Has certain limitations, e.g. BERT is limited to a particular input length for long document is split into segments of a fixed size with overlap. RoBERTa - Robustly optimized BERT approach RoBERTa is the enhanced transformer based on BERT approach RoBERTa is trained with dynamic masking, FULL-SENTENCES without NSPloss, large mini-batches, and a larger byte-level BPE. (2) RoBERTa overcomes several limitations of BERT including the one mentioned above. Approach - Improvise (3) Model Overview Roberta Embeddings: Word embeddings Position embeddings Token-type embeddings Roberta Encoder (12 Roberta Layers) Each layer includes: Roberta Self-attention based on query, key, value Roberta Self-output with dense layer, layer norm and dropout Roberta Intermediate with dense/linear layer Roberta Classifier: with dense/liner layers For more information, please refer to reference (1) Code Walk-through Steps: Environment Setup (Transformers model installation, Hyper-parameter tuning Library installation, Colab set-up) Data load and preprocessing Features: we explored using the response, latest context, response and latest context concatenated, context reversed, and the combination of such, and turns out using the below features gave the best performance Response Context reversed (starting from latest dialog) Label: 1 for SARCASM; 0 for Non-SARCASM Model Configuration Choose roberta-base model Set up the evaluation metrics, batch-size, train-test split ratio, number of epochs Tokenization Use API AutoTokenizer Single model finetuning Set AutoModelForSequenceClassification, TrainingArguments wrapped in Trainer object Hyperparameter tuning Use hyperparameter_search method of the trainer object Selected the best run Select the best hyper-parameter that maximize the metrics (accuracy) Test and compute the accuracy, precision, recall and f1 of the best model Generate the outputs for submission See Colab file 'SequenceClassifier.ipynb' for the script that includes these steps https://colab.research.google.com/drive/1nhsCc1krBzPR6LKg3Qfwq_cxHv4sr_Ib?usp=sharing Run the Code Test Steps: Tutorial available at https://drive.google.com/file/d/1aa75KqPg4qnEJ5HYpZrW51ETR_UA0IDN/view?usp=sharing Open Colab notebook 'RoBERTa_Model_Test' at the below link: https://colab.research.google.com/drive/1S9g8dD7JmuT6JsJo1ysAa4e3nTCNakxk?usp=sharing Also uploaded to GitHub Repo GitHub Repo path: https://github.com/Wenfan1993/CourseProject Load the below items from the GitHub Repo to Colab environment at '\Content\' test.jsonl (Uploaded to GitHub Repo) https://github.com/Wenfan1993/CourseProject/tree/main/Test_Source_Data check_point.pth (the checkpoint where we stored our trained model) https://drive.google.com/file/d/1z1IIeU1e7DgqtAyyPWE66QyAG7h1D-sT/view?usp=sharing Run the Colab notebook The output 'Answer.txt' will be output and stored at Collab '\Content\' Our Results We passed the base-line! As of 12/11, 11:00PM CST, we ranked No.5 at the leaderboard, We achieved precision 72.3%, recall 80.4% and f1 76.2%. We are here Appendix References: (1) https://huggingface.co/roberta-base (2) RoBERTa: A Robustly Optimized BERT Pretraining Approach, by Linhan Liu et. al. https://arxiv.org/abs/1907.11692 GitHub Repo path: https://github.com/Wenfan1993/CourseProject Thank You! For any question, please feel free to contact our team members: Wenxi Fan Email: wenfan1993@gmail.com/Wenxif2@illinois.edu Abhishek Jain Email:aj26@illinois.edu
https://github.com/theRocket/CourseProject	BagofTricks_1607.01759.pdf	"arXiv:1607.01759v3 [cs.CL] 9 Aug 2016 Bag of Tricks for Efficient Text Classification Armand Joulin Edouard Grave Piotr Bojanowski Tomas Mikolov Facebook AI Research {ajoulin,egrave,bojanowski,tmikolov}@fb.com Abstract This paper explores a simple and efficient baseline for text classification. Our ex- periments show that our fast text classi- fier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute. 1 Introduction Text classification is an important task in Natural Language Processing with many applications, such as web search, information retrieval, ranking and document classification (Deerwester et al., 1990; Pang and Lee, 2008). Recently, models based on neural networks have become increasingly popular (Kim, 2014; Zhang and LeCun, 2015; Conneau et al., 2016). While these models achieve very good performance in practice, they tend to be relatively slow both at train and test time, limiting their use on very large datasets. Meanwhile, linear classifiers are of- ten considered as strong baselines for text classification problems (Joachims, 1998; McCallum and Nigam, 1998; Fan et al., 2008). Despite their simplicity, they often obtain state- of-the-art performances if the right features are used (Wang and Manning, 2012). They also have the potential to scale to very large cor- pus (Agarwal et al., 2014). In this work, we explore ways to scale these baselines to very large corpus with a large output space, in the context of text classification. Inspired by the recent work in efficient word representation learning (Mikolov et al., 2013; Levy et al., 2015), we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes, while achieving perfor- mance on par with the state-of-the-art. We evalu- ate the quality of our approach fastText1 on two different tasks, namely tag prediction and sentiment analysis. 2 Model architecture A simple and efficient baseline for sentence classification is to represent sentences as bag of words (BoW) and train a linear classifier, e.g., a logistic regression or an SVM (Joachims, 1998; Fan et al., 2008). However, linear classifiers do not share parameters among features and classes. This possibly limits their generalization in the context of large output space where some classes have very few examples. Common solutions to this problem are to factorize the linear clas- sifier into low rank matrices (Schutze, 1992; Mikolov et al., 2013) or to use multilayer neural networks (Collobert and Weston, 2008; Zhang et al., 2015). Figure 1 shows a simple linear model with rank constraint. The first weight matrix A is a look-up table over the words. The word representations are then averaged into a text representation, which is in turn fed to a linear classifier. The text representa- 1https://github.com/facebookresearch/fastText x1 x2 . . . xN-1 xN hidden output Figure 1: Model architecture of fastText for a sentence with N ngram features x1, . . . , xN. The features are embedded and averaged to form the hidden variable. tion is an hidden variable which can be potentially be reused. This architecture is similar to the cbow model of Mikolov et al. (2013), where the middle word is replaced by a label. We use the softmax function f to compute the probability distribution over the predefined classes. For a set of N doc- uments, this leads to minimizing the negative log- likelihood over the classes: - 1 N N  n=1 yn log(f(BAxn)), where xn is the normalized bag of features of the n- th document, yn the label, A and B the weight matri- ces. This model is trained asynchronously on mul- tiple CPUs using stochastic gradient descent and a linearly decaying learning rate. 2.1 Hierarchical softmax When the number of classes is large, computing the linear classifier is computationally expensive. More precisely, the computational complexity is O(kh) where k is the number of classes and h the di- mension of the text representation. In order to im- prove our running time, we use a hierarchical soft- max (Goodman, 2001) based on the Huffman cod- ing tree (Mikolov et al., 2013). During training, the computational complexity drops to O(h log2(k)). The hierarchical softmax is also advantageous at test time when searching for the most likely class. Each node is associated with a probability that is the probability of the path from the root to that node. If the node is at depth l + 1 with parents n1, . . . , nl, its probability is P(nl+1) = l i=1 P(ni). This means that the probability of a node is always lower than the one of its parent. Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability. In practice, we observe a reduction of the complexity to O(h log2(k)) at test time. This approach is fur- ther extended to compute the T-top targets at the cost of O(log(T)), using a binary heap. 2.2 N-gram features Bag of words is invariant to word order but taking explicitly this order into account is often computa- tionally very expensive. Instead, we use a bag of n-grams as additional features to capture some par- tial information about the local word order. This is very efficient in practice while achieving compa- rable results to methods that explicitly use the or- der (Wang and Manning, 2012). We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick (Weinberger et al., 2009) with the same hash- ing function as in Mikolov et al. (2011) and 10M bins if we only used bigrams, and 100M otherwise. 3 Experiments We evaluate fastText on two different tasks. First, we compare it to existing text classifers on the problem of sentiment analysis. Then, we evaluate its capacity to scale to large output space on a tag prediction dataset. Note that our model could be im- plemented with the Vowpal Wabbit library,2 but we observe in practice, that our tailored implementation is at least 2-5x faster. 3.1 Sentiment analysis Datasets and baselines. We employ the same 8 datasets and evaluation protocol of Zhang et al. (2015). We report the n-grams and TFIDF baselines from Zhang et al. (2015), as well as the character level convolutional model (char-CNN) of Zhang and LeCun (2015), the character based convolution recurrent net- work (char-CRNN) of (Xiao and Cho, 2016) and the very deep convolutional network (VDCNN) of Conneau et al. (2016). We also compare 2Using the options --nn, --ngrams and --log multi Model AG Sogou DBP Yelp P. Yelp F. Yah. A. Amz. F. Amz. P. BoW (Zhang et al., 2015) 88.8 92.9 96.6 92.2 58.0 68.9 54.6 90.4 ngrams (Zhang et al., 2015) 92.0 97.1 98.6 95.6 56.3 68.5 54.3 92.0 ngrams TFIDF (Zhang et al., 2015) 92.4 97.2 98.7 95.4 54.8 68.5 52.4 91.5 char-CNN (Zhang and LeCun, 2015) 87.2 95.1 98.3 94.7 62.0 71.2 59.5 94.5 char-CRNN (Xiao and Cho, 2016) 91.4 95.2 98.6 94.5 61.8 71.7 59.2 94.1 VDCNN (Conneau et al., 2016) 91.3 96.8 98.7 95.7 64.7 73.4 63.0 95.7 fastText, h = 10 91.5 93.9 98.1 93.8 60.4 72.0 55.8 91.2 fastText, h = 10, bigram 92.5 96.8 98.6 95.7 63.9 72.3 60.2 94.6 Table 1: Test accuracy [%] on sentiment datasets. FastText has been run with the same parameters for all the datasets. It has 10 hidden units and we evaluate it with and without bigrams. For char-CNN, we show the best reported numbers without data augmentation. Zhang and LeCun (2015) Conneau et al. (2016) fastText small char-CNN big char-CNN depth=9 depth=17 depth=29 h = 10, bigram AG 1h 3h 24m 37m 51m 1s Sogou - - 25m 41m 56m 7s DBpedia 2h 5h 27m 44m 1h 2s Yelp P. - - 28m 43m 1h09 3s Yelp F. - - 29m 45m 1h12 4s Yah. A. 8h 1d 1h 1h33 2h 5s Amz. F. 2d 5d 2h45 4h20 7h 9s Amz. P. 2d 5d 2h45 4h25 7h 10s Table 2: Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN. to Tang et al. (2015) following their evaluation protocol. We report their main baselines as well as their two approaches based on recurrent networks (Conv-GRNN and LSTM-GRNN). Results. We present the results in Figure 1. We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a valida- tion set from {0.05, 0.1, 0.25, 0.5}. On this task, adding bigram information improves the perfor- mance by 1-4%. Overall our accuracy is slightly better than char-CNN and char-CRNN and, a bit worse than VDCNN. Note that we can increase the accuracy slightly by using more n-grams, for example with trigrams, the performance on Sogou goes up to 97.1%. Finally, Figure 3 shows that our method is competitive with the methods pre- sented in Tang et al. (2015). We tune the hyper- parameters on the validation set and observe that using n-grams up to 5 leads to the best perfor- mance. Unlike Tang et al. (2015), fastText does not use pre-trained word embeddings, which can be explained the 1% difference in accuracy. Model Yelp'13 Yelp'14 Yelp'15 IMDB SVM+TF 59.8 61.8 62.4 40.5 CNN 59.7 61.0 61.5 37.5 Conv-GRNN 63.7 65.5 66.0 42.5 LSTM-GRNN 65.1 67.1 67.6 45.3 fastText 64.2 66.2 66.6 45.2 Table 3: Comparision with Tang et al. (2015). The hyper- parameters are chosen on the validation set. We report the test accuracy. Training time. Both char-CNN and VDCNN are trained on a NVIDIA Tesla K40 GPU, while our models are trained on a CPU using 20 threads. Ta- ble 2 shows that methods using convolutions are sev- eral orders of magnitude slower than fastText. While it is possible to have a 10x speed up for char-CNN by using more recent CUDA implemen- tations of convolutions, fastText takes less than a minute to train on these datasets. The GRNNs method of Tang et al. (2015) takes around 12 hours per epoch on CPU with a single thread. Our speed- Input Prediction Tags taiyoucon 2011 digitals: individuals digital pho- tos from the anime convention taiyoucon 2011 in mesa, arizona. if you know the model and/or the character, please comment. #cosplay #24mm #anime #animeconvention #arizona #canon #con #convention #cos #cosplay #costume #mesa #play #taiyou #taiyoucon 2012 twin cities pride 2012 twin cities pride pa- rade #minneapolis #2012twincitiesprideparade #min- neapolis #mn #usa beagle enjoys the snowfall #snow #2007 #beagle #hillsboro #january #maddison #maddy #oregon #snow christmas #christmas #cameraphone #mobile euclid avenue #newyorkcity #cleveland #euclidavenue Table 4: Examples from the validation set of YFCC100M dataset obtained with fastText with 200 hidden units and bigrams. We show a few correct and incorrect tag predictions. up compared to neural network based methods in- creases with the size of the dataset, going up to at least a 15,000x speed-up. 3.2 Tag prediction Dataset and baselines. To test scalability of our approach, further evaluation is carried on the YFCC100M dataset (Thomee et al., 2016) which consists of almost 100M images with cap- tions, titles and tags. We focus on predicting the tags according to the title and caption (we do not use the images). We remove the words and tags occurring less than 100 times and split the data into a train, validation and test set. The train set contains 91,188,648 examples (1.5B tokens). The validation has 930,497 examples and the test set 543,424. The vocabulary size is 297,141 and there are 312,116 unique tags. We will release a script that recreates this dataset so that our numbers could be reproduced. We report precision at 1. We consider a frequency-based baseline which predicts the most frequent tag. We also com- pare with Tagspace (Weston et al., 2014), which is a tag prediction model similar to ours, but based on the Wsabie model of Weston et al. (2011). While the Tagspace model is described using convolutions, we consider the linear version, which achieves com- parable performance but is much faster. Results and training time. Table 5 presents a comparison of fastText and the baselines. We run fastText for 5 epochs and compare it to Tagspace for two sizes of the hidden layer, i.e., 50 Model prec@1 Running time Train Test Freq. baseline 2.2 - - Tagspace, h = 50 30.1 3h8 6h Tagspace, h = 200 35.6 5h32 15h fastText, h = 50 31.2 6m40 48s fastText, h = 50, bigram 36.7 7m47 50s fastText, h = 200 41.1 10m34 1m29 fastText, h = 200, bigram 46.1 13m38 1m37 Table 5: Prec@1 on the test set for tag prediction on YFCC100M. We also report the training time and test time. Test time is reported for a single thread, while training uses 20 threads for both models. and 200. Both models achieve a similar perfor- mance with a small hidden layer, but adding bi- grams gives us a significant boost in accuracy. At test time, Tagspace needs to compute the scores for all the classes which makes it relatively slow, while our fast inference gives a significant speed-up when the number of classes is large (more than 300K here). Overall, we are more than an order of mag- nitude faster to obtain model with a better quality. The speedup of the test phase is even more signifi- cant (a 600x speedup). Table 4 shows some quali- tative examples. 4 Discussion and conclusion In this work, we propose a simple baseline method for text classification. Unlike unsupervisedly trained word vectors from word2vec, our word features can be averaged together to form good sentence repre- sentations. In several tasks, fastText obtains per- formance on par with recently proposed methods in- spired by deep learning, while being much faster. Although deep neural networks have in theory much higher representational power than shallow models, it is not clear if simple text classification problems such as sentiment analysis are the right ones to eval- uate them. We will publish our code so that the research community can easily build on top of our work. Acknowledgement. We thank Gabriel Synnaeve, Herv'e G'egou, Jason Weston and L'eon Bottou for their help and comments. We also thank Alexis Con- neau, Duyu Tang and Zichao Zhang for providing us with information about their methods. References [Agarwal et al.2014] Alekh Agarwal, Olivier Chapelle, Miroslav Dud'ik, and John Langford. 2014. A reliable effective terascale linear learning system. JMLR. [Collobert and Weston2008] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan- guage processing: Deep neural networks with multi- task learning. In ICML. [Conneau et al.2016] Alexis Conneau, Holger Schwenk, Lo""ic Barrault, and Yann Lecun. 2016. Very deep con- volutional networks for natural language processing. arXiv preprint arXiv:1606.01781. [Deerwester et al.1990] Scott Deerwester, Susan T Du- mais, George W Furnas, Thomas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for informa- tion science. [Fan et al.2008] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li- blinear: A library for large linear classification. JMLR. [Goodman2001] Joshua Goodman. 2001. Classes for fast maximum entropy training. In ICASSP. [Joachims1998] Thorsten Joachims. 1998. Text catego- rization with support vector machines: Learning with many relevant features. Springer. [Kim2014] Yoon Kim. 2014. Convolutional neural net- works for sentence classification. In EMNLP. [Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. TACL. [McCallum and Nigam1998] Andrew McCallum and Ka- mal Nigam. 1998. A comparison of event models for naive bayes text classification. In AAAI workshop on learning for text categorization. [Mikolov et al.2011] Tom'aVs Mikolov, Anoop Deoras, Daniel Povey, Luk'aVs Burget, and Jan VCernock`y. 2011. Strategies for training large scale neural network lan- guage models. In Workshop on Automatic Speech Recognition and Understanding. IEEE. [Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. [Pang and Lee2008] Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval. [Schutze1992] Hinrich Schutze. 1992. Dimensions of meaning. In Supercomputing. [Tang et al.2015] Duyu Tang, Bing Qin, and Ting Liu. 2015. Document modeling with gated recurrent neural network for sentiment classification. In EMNLP. [Thomee et al.2016] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Dou- glas Poland, Damian Borth, and Li-Jia Li. 2016. Yfcc100m: The new data in multimedia research. vol- ume 59, pages 64-73. ACM. [Wang and Manning2012] Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL. [Weinberger et al.2009] Kilian Weinberger, Anirban Das- gupta, John Langford, Alex Smola, and Josh Atten- berg. 2009. Feature hashing for large scale multitask learning. In ICML. [Weston et al.2011] Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI. [Weston et al.2014] Jason Weston, Sumit Chopra, and Keith Adams. 2014. #tagspace: Semantic embed- dings from hashtags. In EMNLP. [Xiao and Cho2016] Yijun Xiao and Kyunghyun Cho. 2016. Efficient character-level document classification by combining convolution and recurrent layers. arXiv preprint arXiv:1602.00367. [Zhang and LeCun2015] Xiang Zhang and Yann LeCun. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710. [Zhang et al.2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In NIPS."
https://github.com/theRocket/CourseProject	FinalProjectReport.pdf	"CS 410 Final Project Gluon NLP with MXNet on AWS Sagemaker Training for Yelp Sentiment Analysis Text Information Systems Fall 2020 Ryan Rickerts (ryanjr3) https://github.com/theRocket/CourseProject CS 410 FINAL PROJECT UIUC MCS-DS 1 CS 410 Final Project Gluon NLP with MXNet on AWS Sagemaker Yelp Sentiment Analysis Motivation State-of-the-art deep learning models in natural language processing (NLP) are fascinating to read about and try to understand for a graduate-level computer science student. It is also possible for a hobbyist or student to obtain pre-trained models from these research breakthroughs and perform inference using the meager hardware at their disposal while working and studying from home (i.e. no access to lab clusters at an institution). Perhaps a deeper and more satisfying approach for investigating this multi-layered architecture with broad areas of application is to train a dataset by one's self, much like raising your own puppy to learn preferred tricks. However, time and compute resources can be an intimidating constraint in these compute-intensive algorithms. For this project, I aimed to find a relatively cutting edge approach in NLP where the training can be replicated by a hobbyist developer. The Gluon Project aims to meet this need. According to this blog post:  Symptom: Natural language processing papers are difficult to reproduce. The quality of open source implementations available on Github varies a lot, and maintainers can stop supporting the projects.  GluonNLP prescription: Reproduction of latest research results. Frequent updates of the reproduction code, which comes with training scripts, hyper-parameters, runtime logs etc. 1 A broad description from the GluonNLP website is as follows: https://medium.com/apache-mxnet/gluonnlp-deep-learning-toolkit-for-natural-language- 1 processing-98e684131c8a CS 410 FINAL PROJECT UIUC MCS-DS 2 GluonNLP provides implementations of the state-of-the-art (SOTA) deep learning models in NLP, and build blocks for text data pipelines and models. It is designed for engineers, researchers, and students to fast prototype research ideas and products based on these models. 2 From their model zoo, we selected the Text Classification example, which trains the FastText classification model on the Yelp review dataset. This is a binary classification dataset (positive vs. negative sentiment), and we aim to replicate their validation accuracy score of 94% in a reasonable time frame and cost. For optimum training times, we turned to NVIDIA Tesla V100 GPUs available on Amazon Web Services (or AWS) Sagemaker instances. These are available on- demand and can be powered up just for training times to keep costs at a minimum. We investigate adapting the FastText scripts provided by the Gluon project to the AWS environment, such as loading required Jupyter kernels and dependent libraries for this algorithm, reading and writing data from S3 buckets, and of course keeping compute regions and permissions (or roles) in good order. https://nlp.gluon.ai/#about-gluonnlp 2 CS 410 FINAL PROJECT UIUC MCS-DS 3 Fast-text Word N-gram This model is a slight variation of the one published by the Facebook AI Research team in 2016 in a paper called ""Bag of tricks for efficient text classification"" by Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. The aim of this approach, which they call fastText or a ""library for efficient text classification and representation learning,"" is also to increase training speed over the dominant deep learning algorithms of the day. The abstract states: Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among ~312K classes in less than a minute. 3 The Gluon-provided code used in this project, found at text_classification/ fasttext_word_ngram.py in the repo, does not reference any code from the fastText project. It uses only gluonnlp and mxnet python libraries, so it appears to be a complete rewrite of the approach. It is modified here to meet the requirements of running on AWS infrastructure, which we cover in the next section, and becomes text_classification/fasttext_word_ngram_aws.py in the Github repo. Apache MXNet is an open-source deep learning framework that is well supported on AWS and is rivaled by TensorFlow and PyTorch in popularity and performance on NLP. The advantage of MXNet (short for mix net) is the broad language support of Python, R, C++, Julia, and Scala. It also scales well: ""Another advantage is that the models built using MXNet are portable such that they can fit in small amounts of memory. So, once your model is trained and tested, it can be easily deployed to mobile devices or connected systems. MXNets are scalable to be https://arxiv.org/abs/1607.01759 3 CS 410 FINAL PROJECT UIUC MCS-DS 4 used on multiple machines and GPU simultaneously. This is why Amazon has chosen this framework for its deep learning web services."" 4 In this code, the Gluon authors create two classes: * a MeanPoolingLayer -- a block for mean pooling of encoder features -- which takes a gluon.HybridBlock as a parameter. * a FastTextClassificationModel--the trained embeddings layer--which takes a HybridBlock as a parameter. The rest of the logic centers around argument parsing to the main function, the creation of n-grams (defaults to 1 and recommended <=2 for large datasets), data loading, preprocessing, labeling, and evaluating accuracy of the training against a test dataset. Scripts to fetch the the training and test datasets from Google Drive are hosted at fastText's Github repo , but these proved problematic given the response time of 5 the downloads. I manually fetched the Yelp Review Polarity set only, then modified the code to normalize the text. This modified bash script is at text_classification/ data_fetch.sh in the project repo, and the CSV files are placed in the data/ sub- directory. Once this was prepared, we can start running the python script locally with the recommended parameters as follows: python fasttext_word_ngram.py --input data/yelp_review_polarity.train \ --output data/yelp_review_polarity.gluon \ --validation data/yelp_review_polarity.test \ --ngrams 1 --epochs 10 --lr 0.1 --emsize 100 This set of parameters is notably missing the '--gpu=0' flag for the local run only. This is because my laptop, a 2019 MacBook Pro with a 6-Core Intel i7 CPU, has a AMD Radeon Pro 5300M graphics card that does not implement CUDA architecture. This job will run on the CPU instead, and while threading does not appear to be implemented, according to MXNet documentation: https://analyticsindiamag.com/mxnet-tutorial-complete-guide-with-hands-on-implementation-of-deep-learning- 4 framework/ https://github.com/facebookresearch/fastText/blob/master/classification-example.sh 5 CS 410 FINAL PROJECT UIUC MCS-DS 5 ""For a CPU, MXNet will allocate data on main memory, and try to use all CPU cores as possible, even if there is more than one CPU socket."" 6 The Activity Monitor on the laptop shows the CPU being pegged at 100% and no use of the GPU: With 35000 batches configured to run on 464402 vocabulary words and 560000 sentences, the first attempt never even completed a single epoch. I quit the job after 8 hours of processing time and only 17000 batches completed. So now we move this workload to the GPUs hosted on the cloud! AWS Sagemaker and NVIDIA Tesla V100 GPUs I implement this training job using high-end P3 AWS Sagemaker instances. 7 According to the table of instance sizes listed at the bottom of the above linked page, the cheapest instance offered - p3.2xlarge - provides one Tesla V100 GPU with 16GB of GPU memory for $3.07/hr on demand. A helpful reference were these published benchmarks of NVIDIA GPUs provided in TensorFLOPs, which are units of floating-point arithmetic performance aimed at NVIDIA GPU hardware called Tensor Cores: ""A new, specialized Tensor Core unit was introduced with ""Volta"" generation GPUs. It combines a multiply of two FP16 units (into a full precision product) with a FP32 accumulate operation--the exact operations used in Deep Learning Training https://mxnet.apache.org/versions/1.7.0/api/python/docs/tutorials/getting-started/crash-course/6- 6 use_gpus.html https://aws.amazon.com/ec2/instance-types/p3/ 7 CS 410 FINAL PROJECT UIUC MCS-DS 6 computation. NVIDIA is now measuring GPUs with Tensor Cores by a new deep learning performance metric: a new unit called TensorTFLOPS."" 8 According to that metric, the Tesla V100 GPU rates around 112-125 TensorTFLOPS (exact figure depending on the use of PCI-Express or SXM2 SKU interfaces). For comparison, the maximum known deep learning performance at any precision of the Tesla K80 is 5.6 TFLOPS for FP32. This GPU is provided on the P2 Sagemaker instances, and for 1 GPU on the p2.xlarge instance size, the cost is $0.90/hr. If we can attain a 20x performance increase on our training job for approx. 4x compute resource cost, that seems like a great win! The clock time used by the p3.2xlarge instance and costs incurred for several training runs is discussed in the Results sub-section below. Data Workflow The first step on AWS Services is to select a region for all our work, in this case US East (Ohio) or us-east-2. This is important for several reasons: one, the data transferred between Amazon S3 buckets and Amazon EC2 instances in the same Region and account is relatively straightforward and free, and our training data sizes are significant (where I live in a remote area, it took 30 minutes to upload due to low bandwidth). Two, the cost of the P3 instances (also significant) are determined by region, and we want to accurately estimate our costs for this workload to avoid a surprise bill. Furthermore, I had to request access to these highly specialized machines on my account via a support ticket, and their staff response time was not immediate. On the first attempt, they enabled a different region than I had uploaded the training data, so I had to sync it across to another S3 bucket, now called sagemaker-cs410-finalproj and pictured below. Sagemaker has an S3 Uploader library available for outputting the model parameters (a local file save is first required first) and returning the training results --net.params-- to the S3 bucket under a yelp_sentiment_polarity.gluon directory, as pictured below. These parameters will be needed for later inference use cases outside this JupyterNotebook instance, so they must offloaded for access. https://www.microway.com/knowledge-center-articles/comparison-of-nvidia-geforce-gpus-and-nvidia-tesla-gpus/ 8 CS 410 FINAL PROJECT UIUC MCS-DS 7 def save_model(net, output_file): file_name = ""net.params"" # local version net.save_parameters(file_name) S3Uploader.upload(file_name,output_file) Since we established this bucket in the same region, the S3 Uploader utility only requires the bucket name to perform this work. That variable was configured in the first JupyterNotebook cell, with: bucket = 'sagemaker-cs410-finalproj' Although you can also use a default bucket for the session, which is initialized like this: bucket = Session().default_bucket() For streaming lines from the data in S3 to the data parsing method, we used a library called smart_open which simulates reading in data from a local file in an 9 iterable function such as a for-in loop, like: for line in open(filename): tokens = line.split(',', 1) labels.append(tokens[0].strip()) data.append(tokens[1].strip()) return labels, data https://pypi.org/project/smart-open/ 9 CS 410 FINAL PROJECT UIUC MCS-DS 8 This proved very effective and the data was parsed quickly into labels and normalized text data for the training run. Training Results The one Tesla V100 GPU performing text classification on our p3.2xlarge instance exceeded expectations and churned out training and test results for 10 Epochs in under 20 mins. Furthermore, the same accuracy of 94% was achieved as mentioned on the Gluon page where we sourced this code. SGD performed slightly better than Adam as an optimizer, and this is discussed in the Further Work section below. Jupyter notebook run 1 with `adam` as optimizer, 10 Epochs * Highest Test Accuracy: 0.9401578947368421 (Epoch 8) * Final Test Accuracy: 0.939921052631579, Test Loss: 0.17803387705344548 (Epoch 10) Jupyter notebook run 2 with `sgd` as optimizer, 10 Epochs * Highest Test Accuracy: 0.9403157894736842 (Epoch 8) * Final Test Accuracy: 0.9400526315789474, Test Loss: 0.17815197125596924 (Epoch 10) Jupyter notebook run 3 with `sgd` as optimizer, 25 Epochs * Highest Test Accuracy: 0.9403157894736842 (Epoch 8) * Final Test Accuracy: 0.9397105263157894, Test Loss: 0.17758843273002864 (Epoch 25) * Note, the accuracy above was reached at Epoch 18 and remained stable. CS 410 FINAL PROJECT UIUC MCS-DS 9 To reiterate, my MacBook Pro CPU (ctx=cpu(0)) never completed a single epoch after 8 hours of run time, and the V100 GPU (ctx=gpu(0)) finished one epoch in less than 2 minutes! The entire job run of 10 epochs required only 17 minutes total, which is very satisfying. The costs incurred for several hours of experimenting with the AWS SageMaker JupyterNotebook environment, plus the three training runs above (and a few failed ones when parameter output to file was not configured correctly) remained under $10. So we were able to capture reliable and conclusive training work at the expected cost of around $3/hr! The Cost Explorer returns this data analysis by service typically the next day. Importantly, I shut off my P3 EC2 instance whenever I was not using it, and I deleted other experimental instances, such as P2, when I moved onto another solution. Not performing this cleanup task is a painful way to discover an exorbitant bill from AWS at the end of the month! CS 410 FINAL PROJECT UIUC MCS-DS 10 Further Work 1) The Gluon team notes that they simplified the model somewhat for training speed as follows: ""We have added dropout to the final layer, and the optimizer is changed from 'sgd' to 'adam' These are made for demo purposes and we can get very good numbers with original settings too, but a complete async sgd with batch size = 1, might be very slow for training using a GPU."" 10 I was able to pass 'sgd' (or Stochastic Gradient Descent) as an optimizer parameter and get slightly better testing accuracy over 'adam' (0.94032 vs. .94016) with no noticeable decrease in training speed. I did not remove the dropout, increase the size of n-grams, nor change the batch size, so these could be restored to the original fastText algorithm for more benchmarking on this particular GPU instance. 2) Rather than firing up JupyterLab in our P3 instance and copy/pasting in the custom algorithm for our training job, Amazon SageMaker can interact with a Docker container based on a pre-built image. This would be a better practice for continued use of this approach, since it can easily be deployed into different regions and scaled up or down on different EC2 instances for more or less GPU power. 11 This workflow is covered in more detail here, which was also an early inspiration for this project (although they use GluonCV): Deploying custom models built with Gluon and Apache MXNet on Amazon SageMaker ""When you build models with the Apache MXNet deep learning framework, you can take advantage of the expansive model zoo provided by GluonCV to quickly train state-of-the-art computer vision algorithms for image and video processing. A typical development environment for training consists of a Jupyter notebook hosted on a https://nlp.gluon.ai/model_zoo/text_classification/index.html 10 https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html 11 CS 410 FINAL PROJECT UIUC MCS-DS 11 compute instance configured by the operating data scientist. To make sure this environment is replicated during use in production, the environment is wrapped inside a Docker container, which is launched and scaled according to the expected load."" 12 3) Elastic Inference MXNet on Amazon SageMaker has support for Elastic Inference, which allows for inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance. In order to attach an Elastic Inference accelerator to your endpoint provide the accelerator type to accelerator_type to your deploy call. predictor = mxnet_estimator.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, accelerator_type='ml.eia1.medium') I felt great enjoyment in this implementing this project and learned a lot about NLP using AWS Sagemaker GPUs. I look forward to experimenting more with Gluon and MXNet to obtain rapid training times on state-of-the-art deep learning algorithms. https://aws.amazon.com/blogs/machine-learning/deploying-custom-models-built-with-gluon-and-apache- 12 mxnet-on-amazon-sagemaker/ CS 410 FINAL PROJECT UIUC MCS-DS 12"
https://github.com/theRocket/CourseProject	README.md	"Training FastText ""Bag of Tricks"" using Gluon & MXNet on AWS GPUs (ryanjr3) CS 410 - Text Information Systems (MCS-DS at UIUC) Final Project Report: Gluon NLP with MXNet on AWS Sagemaker (PDF) AWS Services presentation (YouTube) The project formerly known as BERT & ERNIE (benchmarking on Google Cloud TPUs): Please see the October README commit for the original project proposal. Training times and cloud compute costs for this model were discovered to be too prohibitive for our project timeline and resources. Gluon for NLP and MXNet The aim of my project is to become better acquainted with the Gluon API for Natural Language Processing (NLP). GluonNLP provides implementations of the state-of-the-art (SOTA) deep learning models in NLP, and build blocks for text data pipelines and models. It is designed for engineers, researchers, and students to fast prototype research ideas and products based on these models. For example, it can easily provide the cosine similarity of two word vectors with the following simple python function, cos_similarity(): ``` import mxnet as mx import gluonnlp as nlp glove = nlp.embedding.create('glove', source='glove.6B.50d') def cos_similarity(embedding, word1, word2): vec1, vec2 = embedding[word1], embedding[word2] return mx.nd.dot(vec1, vec2) / (vec1.norm() * vec2.norm()) print('Similarity between ""baby"" and ""infant"": ', cos_similarity(glove, 'baby', 'infant').asnumpy()[0]) ``` The Gluon API provides user-friendly access to the Apache MXNet library for Deep Learning, which advertises itself as being a ""truly open source deep learning framework suited for flexible research prototyping and production."" The Amazon Web Services (AWS) Sagemaker instances support MXNet running on Python 3.6 with the conda_mxnet_p36 kernel selected for the Jupyter Notebook. Gluon NLP dependencies are easily added to the notebook by running a cell with: !pip install gluonnlp AWS Sagemaker DL performance on NVIDIA GPUs I plan to implement the training job using high-end P3 AWS Sagemaker instances to benchmark rapid training of models using python v3.6. According to the table of instance sizes listed at the bottom of the above linked page, the cheapest instance offered - p3.2xlarge - provides 1 Tesla V100 GPU with 16GB of GPU memory for $3.07/hr on demand. We have published benchmarks of NVIDIA GPUs provided in TensorFLOPs, which are units of floating-point arithmetic performance aimed at NVIDIA GPU hardware called Tensor Cores: A new, specialized Tensor Core unit was introduced with ""Volta"" generation GPUs. It combines a multiply of two FP16 units (into a full precision product) with a FP32 accumulate operation--the exact operations used in Deep Learning Training computation. NVIDIA is now measuring GPUs with Tensor Cores by a new deep learning performance metric: a new unit called TensorTFLOPS. According to that metric, the Tesla V100 GPU rates around 112-125 TensorTFLOPS (exact figure depending on the use of PCI-Express or SXM2 SKU interfaces). For comparison, the maximum known deep learning performance at any precision of the Tesla K80 is 5.6 TFLOPS for FP32. This GPU is provided on the P2 Sagemaker instances, and for 1 GPU on the p2.xlarge instance size, the cost is $0.90/hr. If we can attain a 20x performance increase on our training job for approx. 4x compute resource cost, that seems like a great win! I also compare running this training job on my Macbook Pro CPU - a 2.6 GHz 6-Core Intel Core i7. Since the included AMD Radeon Pro 5300M graphics card does not implement CUDA architecture, we must deleted the following option when running the training job: --gpu=0. Otherwise, this signals the index number of the GPU to use. Since we selected AWS instances with only one GPU, this flag will always be zero. FastText ""Bag of Tricks"" and Yelp Sentiment Classification The primary influence for this project was an entry hosted at nlp.gluon.ai for Text Classification called Fast-text Word N-gram. It leverages the fastText python library used for ""efficient text classification and representation learning"" and developed at Facebook research. The paper was also published by the Facebook AI Research team in 2016 and is called Bag of Tricks for Efficient Text Classification (the full PDF is included in this repo). They claim in the abstract: Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. In this project, I begin by training the model on AWS using the Yelp Sentiment (binary classification) data to establish a workflow. Once the architecture is in place and proven to achieve timely results, we can expand into the other datasets. Each are manually uploaded to S3 buckets to make them accessible to our Sagemaker instance, rather than using the script provided by fastText (although we do use their text normalization function). Output from AWS Sagemaker: Jupyter notebook run 1 with adam as optimizer, 10 Epochs - Highest Test Accuracy: 0.9401578947368421 (Epoch 8) - Final Test Accuracy: 0.939921052631579, Test Loss: 0.17803387705344548 (Epoch 10) Jupyter notebook run 2 with sgd as optimizer, 10 Epochs - Highest Test Accuracy: 0.9403157894736842 (Epoch 8) - Final Test Accuracy: 0.9400526315789474, Test Loss: 0.17815197125596924 (Epoch 10) Jupyter notebook run 3 with sgd as optimizer, 25 Epochs - Highest Test Accuracy: 0.9403157894736842 (Epoch 8) - Final Test Accuracy: 0.9397105263157894, Test Loss: 0.17758843273002864 (Epoch 25) - Note, the accuracy above was reached at Epoch 18 and remained stable. Other Resources: Deploying custom models built with Gluon and Apache MXNet on Amazon SageMaker Use MXNet with the SageMaker Python SDK SageMaker MXNet Inference Toolkit Another possible compute resource are the AWS Deep Learning containers. Amazon claims that: AWS DL Containers include AWSoptimizations and improvements to the latest versions of popular frameworks, like TensorFlow, PyTorch, and Apache MXNet, and libraries to deliver the highest performance for training and inference in the cloud. For example, AWS TensorFlow optimizations allow models to train up to twice as fast through significantly improved GPU scaling."
https://github.com/pebblespot/CourseProject	CS410__Reading-Assistant__Project_Progress_Report.pdf	Reading Assistant: Project Progress Report Christopher Rock (cmrock2) (Captain) Zichun Xu (zichunx2) Kevin Ros (kjros2) Due 29 November 2020 1 Progress We began by planning a road-map for our project, which consisted of a set of goals and group meeting checkpoints to evaluate progress. Additionally, we created a GitHub repository which holds our project code. Following this, we implemented the basic structure for our reading assistant. On start-up, text documents (directory path provided by user) are loaded by the assistant. During loading, the documents are processed and added to an inverted index. These documents are considered to be the previously-read documents by the user. Once the loading is complete, the assistant waits for a path to a text file (unseen document). Given this path, the assistant ranks the previously-read documents using the unseen document and returns the most similar read document names to the user. We also provided methods for a user to add and remove previously-read documents. Currently, the assistant calculates similarity using the Okapi BM25 ranking function along with various optimization techniques, including an inverted index. To heuristically gauge the effectiveness of this method, each team member collected approximately 8-10 documents. These documents were loaded as the previously-read documents, and additional documents were provided as the unseen documents. From the preliminary examination, the results seem promising. The code is written in a modular fashion, so that we can easily extend the assistant to use different similarity/difference measures and methods. In addition to document-level BM-25, we have implemented paragraph-level BM25 which allows more detailed evaluation of unseen documents compared to seen documents. We have also used the external library gensim to include Latent Semantic Indexing at a document level, with document- level similarity. This is currently a separate script and will be integrated for the final project. Our planned extensions are discussed in the following section. 2 Remaining Tasks Our first remaining task is to add more fine-grained similarity and difference measurement tech- niques. Regarding the ranking function itself, we are considering adding pre-trained word embed- dings and cosine similarity to effectively assess similarity and differences on a word and sentence level. We will combine this with our Okapi BM25 calculations to compare seen and unseen docu- ments on a paragraph granularity. Our second remaining task is to create a user-friendly command line interface. This will allow the user to easily add and remove documents, and view the similarities and differences between 1 the seen and unseen documents. Ideally, we plan to output a detailed summary that describes the relationship between the documents. 3 Challenges and Issues One particular challenge that we've encountered is the evaluation of our reading assistant's effec- tiveness. Because we haven't encountered a data set that exactly fits our needs, we plan to address this issue by incorporating a feedback mechanism in the terminal. This way, users can provide real-time feedback that we can dynamically incorporate into the reading assistant. 2
https://github.com/pebblespot/CourseProject	CS410__Reading-Assistant__Project_Propsal.pdf	"Free Topic: Reading Assistant Christopher Rock (cmrock2) (Captain) Zichun Xu (zichunx2) Kevin Ros (kjros2) 25 October 2020 1 Project Proposal 1.1 Overview In the early months of 2020, the now named SARS-CoV-2 virus was rapidly spreading across the countries of the Pacific and jumping to new locations in every corner of the globe. The flu pandemic that had long been predicted was happening - except it wasn't the flu. First in China, and soon throughout the world there was talking, writing, researching, and publishing about the virus. This was the first pandemic in the smart-phone era, and the only thing that seemed to spread faster than the virus was information. Governments, medical organizations, companies, and every institution imaginable began pushing out not just information, but also guidelines, rules, and policies. ""Information overload"" is something people in today's society are accustomed. Most people develop methods of coping with the huge amount of information available. We filter things through trusted sources, prioritize information that is actionable, and change our mental model of the situation as necessary. However in the face of a new dangerous situation every piece of information becomes potentially critical. Dynamic situations such as a pandemic require quick reactions to new knowledge. It is our experience that those in leadership and decision-making roles are pushed a large number of documents and expected to be up to date on this rapidly expanding corpus of information. New situations create organizational chaos, and the individual's strategies to limit information overload break down. Trusted sources are more difficult to identify when information comes from many sources simultaneously. And because all information is potentially actionable, all information must seemingly be reviewed. Initially the challenge is simply to read and understand the documents sent. However the difficulty quickly becomes identifying what is new knowledge. New documents may have significant overlap with prior knowledge. Differences between documents must be reviewed, and often the progression of changes is important. Information retrieval, text mining, and recommender systems have developed algorithmic strate- gies to identify and extract useful information from text-based knowledge. The focus of these tools has generally been to pull relevant documents via (search), or push potentially interesting docu- ments (recommender). These techniques can be modified to assist a reader in identifying new useful information. Our goal is to create a reading assistant tool that allows a user to maintain a collection of ""seen"" or ""read"" documents (reflecting the current knowledge of the user) and provides novelty scores based on new documents introduced to the collection. Given a new document, the reading assistant tool will compare the document to all ""seen"" documents, and provide the user with measures indicating 1 how the new document differs from the document collection. In this way, potentially useful new documents can be efficiently prioritized by the user. 1.2 Project Description The task of our free topic is to design and implement a reading assistant software tool that helps users determine the novelty of never-before-seen documents based on previously-seen documents. Each user will have a collection of read documents, known to the reading assistant. When the user is provided a new document, the reading assistant will quickly scan the user's read document collection and score the new document (or sections of the new document). This score will reflect how novel the new document is relative to the previously-read documents. Ideally, this will provide the user with a high-level understanding of the importance of the document, allowing the user to better optimize their time. There are many users who would benefit from such a tool. As we discussed in Section 1.1, medical researchers and doctors could use a tool to help sift through and sort the vast amount of information provided during events such as a global pandemic. In academia, researchers could leverage this tool to filter research papers for information relative or novel to their current work. Outside of academia, the general public could use this tool to augment online browsing, as such a tool would allow them to quickly look up previously-read documents and news articles, and interpret new articles in the context of what they have already read. To our knowledge, no such tool currently exits. We will use the MetaPy toolkit1 to provide a suite of ranking and evaluation methods for our tool, along with the publically-available CORD-19 Coronavirus document data set2 as our training and testing data. Aditionally, we will use the Python programming language. To create the tool, we will begin by leveraging our understanding of the BM25 ranking algorithm (which measures document similarity) to construct an ""inverted BM25"" distance function (which measures document difference). In order to demonstrate the usefulness of our tool, we will manually score a subset of documents in terms of similarity to a collection of seen documents. In some cases the seen documents will be randomly selected, and in other cases they will all be of a certain topic. Then, we will pass the scored documents to our tool and see if it categorizes the documents in line with our manual scoring. We discuss a rough timeline in Section 1.3 1.3 Workload We will spend the first 20 hours defining and understanding the project scope. Here, we will begin by defining what it means for two documents to be distinct (or similar). We will also attempt to quantitatively define a distance measure between documents or paragraphs. Additionally, we will define the scope of ""seen"" and ""unseen"" documents. That is, we might need to assume that the reader has read many documents for recommendation to be effective (otherwise many documents will be considered novel). Following this, we will spend the next 20 hours implementing our distance measure using Python and MetaPy. We will likely begin with an inverted version of BM25, but it is difficult to know how well it will work for measuring document difference. Thus, we expect that a significant portion of the 20 hours will be testing out and debugging various implementations, fine-tuning parameters, curating the training and testing documents, and adjusting any initial assumptions in light of new 1https://github.com/meta-toolkit/metapy 2https://www.semanticscholar.org/cord19 2 evidence. Once we decide on a specific implementation, we will define various evaluation measures in order to determine the effectiveness of our tool. The remaining 20 hours will be spent evaluating the tool and tuning any parameters. Given the subjectivity of relevance scores, we will likely need to manually judge documents. For example, this could include randomly choosing a set""already seen"" documents, and hand-labeling additional documents as ""very similar"", ""somewhat similar"", or ""not similar"" to ""already seen"" documents. Then, we would see if the tool's scores corresponded to our similarity classifications. In the case that we overestimated the time it takes to complete the aforementioned tasks, we will fill the remaining time by making our tool more robust, more user-friendly, or more expansive. This will be accomplished by introducing various similarity score measures (such as word embeddings), a command-line interface, and considering additional data sets, respectively. 3"
https://github.com/pebblespot/CourseProject	README.md	"Reading Assistant CS410, Fall 2020 Christopher Rock (cmrock2) Zichun Xu (zichunx2) Kevin Ros (kjros2) Video presentation: https://youtu.be/RO351eoZ1ZU ### Documentation Guidelines The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Overview Problem ""Information overload"" is something people in today's society are accustomed. Most people develop methods of coping with the huge amount of information available. We filter things through trusted sources, prioritize information that is actionable, and change our mental model of the situation as necessary. Earlier this year, the 2019 novel Coronavirus epidemic turned the world on its collective head and created a flurry of information. Large volumes of text data are expected to be consumed and acted upon in a short period of time. For humans, initially the challenge is simply to read and understand the documents. However the difficulty quickly becomes identifying what is new knowledge, and remembering the source of previously seen similar knowledge. New documents may have significant overlap with prior knowledge. Differences between documents must be reviewed, and often the progression of changes is important. Information retrieval, text mining, and recommender systems have developed algorithmic strategies to identify and extract useful information from text-based knowledge. The focus of these tools has generally been to pull relevant documents via (search), or push potentially interesting documents ( recommender). These techniques can be modified to assist a reader in identifying new useful information. What is our tool? In line with our project proposal, our overall goal was to create a reading assistant tool that allows a user to maintain a collection of read documents ( reflecting the current knowledge of the user) and then provides insight about a new unread document when compared to the all read documents. The output includes: A ranked list of documents that are most similar to the unread document based on BM25 scores and LSI similarity scores, respectively. A ranked list of paragraphs that are most similar to each paragraph of the unread document based on BM25 scores and LSI similarity scores, respectively. Our initial idea from the project proposal was to focus on the differences between documents, however the reality is that there were so many ways documents could be different that this was not particularly helpful. In this final version we instead focus on the areas of similarity betwen the unread document from the corpus of read. This allows the user to then quickly hone in on areas where the new document reinforce or subtly change what they had previously discovered from the read docuements. What can it be used for? Although this is created as a command line tool, as described above the initial inspiration for this idea was the surplus of information that was being pushed out (in our case via email) during the first months of the COVID-19 pandemic. One way to use this tool would be to integrate it with a mail server, so that you could forward an email or attachment and indicate that you had or had not read the document - then the server could spit back a new email with some analysis of the document including a list (and linke) to the other read similar documents, as well as highlight passages (paragraphs in our case) of particular interest. Implementation On start-up, text documents (directory path provided by user) are loaded by the assistant. During loading, the documents are processed (remove non-ascii characters, blank lines, etc) and added to an inverted Index. These documents are considered to be the previously-read documents by the user. Once the loading is complete, the assistant waits for a path to a text file (unseen document). Given this path, the assistant ranks the previously-read documents using the unseen document and returns the most similar read document and paragraph names to the user. We also provided methods for a user to view, add and remove previously-read documents. Currently, the assistant calculates similarity scores using two approaches: Okapi BM25 and Latent Semantic Indexing (LSI) similarity. To implement the Okapi BM25 ranking function, first an inverted index is built based on the previously-read documents. From there, it calculates the term frequency, inverse document frequency, and document length normalization. Finally, the similarity score for each document is calculated and scores are sorted in descending order. In addition to document-level BM-25, we also implemented paragraph-level BM25, which follows a similar approach but considers each paragraph of the document as an individual ""document"". This allows more detailed evaluation of unseen documents. To implement LSI similarity ranking function, we utilized the external gensim library. This is currently in a separate script (gensimlsi.py) and is integrated to the reading assistant. During document pre-processing, it removes stop words, blank lines, and words that only appeared once in the document to achieve better topic discovery. It first transforms the previously-read documents to Tf-Idf vectors. It then builds an LSI model with 200 (default) topics. Note that if the number of read documents is less than 200 then the number of read documents will be used. This LSI model will discover topics based on all the previously-read documents, and map the document vectors to LSI space, i.e. describe how strongly each document is related to each topic. Upon receiving the unseen document specified by the user, it will transform the document into LSI space and compute the cosine similarity. The scores will then be sorted in descending order. Similarly, we also implemented paragraph level analysis for LSI similarity. The code is written in a modular fashion, so that we can easily extend the assistant to use different similarity/difference measures and methods Usage View our usage video here: We recommend using python 3.6 or 3.7, with gensim (and its dependencies), as well as the smart_open package. To start the reading assistant, you must first have two directories of text files. One directory should be ""read"" documents, and the other is ""unread"" documents. Start the program by running the script like so: python reading_assistant.py read_docs_path unread_docs_path [k1] [b] read_docs_path : path containing text files that have been read by the user unread_docs_path : path containing text files that have not been read by the user [k1] : value for BM25. Default: 1.2 (optional) [b] : the b value for BM25. Default: 0.75 The script will load the read documents into an inverted index, and then go into the Read-Eval-Print Loop (REPL). Once the REPL is running, you will be presented with a list of Read documents and Un-Read documents. For example, you may see the following: ``` -= READ FILES: =- 0 : covid-bhc-contact-sop-1.txt 1 : covid-isos-brief.txt 2 : covid-update-4.txt 3 : covid-dod-mgmt-guide.txt 4 : covid-update-1.txt 5 : covid-update-3.txt 6 : covid-bhc-pt.txt 7 : covid-update-2.txt 8 : covid-yoko-sop.txt 9 : covid-fragord.txt 10 : covid-bhc-extended-use.txt =- UN-READ FILES: -= 0 : covid-bhc-contact-sop-2.txt 1 : covid-annex-1.txt Please use one of the following commands: rank [unread_file_#] --> Compares new document to previously-read documents read [unread_file_#] --> add the document from the unread list to the read list forget [read_file_#] --> remove a document from the read list view document [document name] --> prints the document view paragraph [paragraph name] --> prints the paragraph set scope [integer] --> only documents above this number of standard deviations above mean ranking score are returned exit --> Exits the program ``` To see the rank of the unread document covid-annex-1.txt you would enter rank 1 at the prompt. An ""output.html"" will also be generated in the directory where the command is issued. It contains the same info as the console output and provides a better visual representation. To move a document covid-bhc-contact-sop-2.txt from the unread into the * read* grouping, type read 0. Or, to move document covid-fragord.txt from read to unread, type forget 9. To view the text of document covid-yoko-sop.txt, type 'view document covid-yoko-sop.txt'. Note that this only works with documents listed under READ FILES. Similarly, to view the first paragraph of document covid-yoko-sop.txt, type 'view paragraph covid-yoko-sop.txt_parag0'. Note that this only works with documents listed under READ FILES. The 'set scope [integer]' command determines scope of the ranking results. As each document and paragraph in READ FILES is given a ranking score, the [integer] determines the cut-off of these scores. More specifically, the [integer] is the number of standard deviations above the mean ranking score. That is, a scope of 2 means that only documents and paragraphs that are two or more standard deviations above the mean score are returned. A scope of 0 means that all documents and paragraphs above the mean ranking score are returned. Generally, a higher scope means fewer documents and paragraphs returned, but these documents and paragraphs are much more relevant. Results To heuristically gauge the effectiveness of the reading assistant, each team member collected approximately 8-10 documents. These documents were loaded as the previously-read documents, and additional documents were provided as the unseen documents. From the preliminary examination, the results seem promising and inline with our qualitative evaluation of the documents. We found these results to be interesting and potentially be useful in a real world application. Using the results from our tool, one could easily find related previously read documents. Additionally if a paragraph was interesting one could find the similar passages. Alternatively, a document highlighter with links to related documents could be created. After creating the tool the existence of similar functionality became evident in other software such as Evernote, which shows similar notes to the one the user is currently viewing. The Evernote use case is not quite the same as our stated use case, but likely relies on some similar information retrievel techniques to generate the list of similar documents. Interestingly, the original impetus behind creating the tool was to find similar and different documents, however in the process of creating this tool we came to understand how the BM25 and LSI algorithms are powered towards similarities, not differences. The root of this is that there are only a few ways a document can be similar, but many ways documents can be different. This was an interesting realization, and further thought towards how to find useful differences could be discovered was an interesting thought experiment, although we did not make significant headway into how to solve that problem. If we were to continue to develop this project further, there are a few areas where we could easily improve the tool. One would be to integrate the data structures between the BM25 and gensim LSI algorithm so that the reading and memory usage was more efficient. Additionally, the LSI algorithm is capable of adding documents without needing to completely recreate it's underying data structures. Depending on how the tool would be used (if for instance documents would be frequently added) this would improve the efficiency of the tool. Manipulation of the parameters of the algorithms would be another area where improvements could be made. Manipulation of either with the k value of BM25, or the topic number of LSI coud lead to subtle improvements in results. Creating of a data set with user rankings for comparison to the results would also be very helpful in objectively analyzing the results of these tweaks. Creation of such a data set - with human-choosen similar paragarphs - would be time consuming to create but could result in use of comparison functions such as the F1 score which would facilitate further development. Overall we felt this tool was a strong starting point to further work in the realm of a reading assistant. Team Contributions All team members were active participants throughout the entire project lifecycle process. Our team worked well together and all members contributed meaningfully to our end result. All met via Zoom on the following days (30-60 minute meetings): Sep 10th: initial team meeting and plan for future meeting timeline Oct 3rd: draft concept of reading assistant formed Oct 9th: discussion of unit 1 concepts and relation to project Oct 21st: formalized topic and planned submission of topic to CMT Oct 24th: discussed status, potential roadblocks, and plan forward Nov 14th: reviewed TA comments and initial review of BM25 document-level rankings Nov 17th: discussed additional methods to rank articles Nov 29th: reviewed progress report, paragraph ranking, and formulated final plan for code breakdown Dec 6th: reviewed integration of gensim, paragraph ranking, CLI, and initial REPL Dec 11th: reviewed final product and discussed last touches necessary to complete project Dec 13th: recorded tutorial Specific Contributions All members contributed to write-ups, review of code, reviewing submission requirements, and ensuring deadlines were met. Kevin Ros: Created initial BM25 document-level code, with necessary ability to dynamically add and remove documents. Drafted initial documents (proposal, progress report) Added initial REPL interface Added standard deviation analysis of results to simplify interpretation of ranking data Zichun Xu Created paragraph level analysis of BM25 analysis method Modified gensim LSI analysis for paragraph level analysis Christopher Rock Added gensim LSI ranking methods Added CLI and finalized REPL Added HTML generator for better visual representation of results References 1https://github.com/meta-toolkit/metapy 2https://tac.nist.gov/2008/summarization/update.summ.08.guidelines.html 3Andrei V, Arandjelovic O. Complex temporal topic evolution modelling using the Kullback-Leibler divergence and the Bhattacharyya distance. EURASIP J Bioinform Syst Biol. 2016 Sep 29;2016(1):16. doi: 10.1186/s13637-016-0050-0. PMID: 27746813; PMCID: PMC5042987. 4Liu, Heng-Hui & Huang, Yi-Ting & Chiang, Jung-Hsien. (2010). A study on paragraph ranking and recommendation by topic information retrieval from biomedical literature. ICS 2010 - International Computer Symposium. 10.1109/COMPSYM.2010.5685393. 5https://radimrehurek.com/gensim/models/lsimodel.html"
https://github.com/AnantSharma18/CourseProject	Project Progress Report.pdf	Page 1 of 2 Project Progress Report Anant Ashutosh Sharma anantas2@illinois.edu Individual Project | Free Topic Text and Tweet Classification using Machine Learning 1) Which tasks have been completed ? The following tasks have been successfully completed and corresponding code has been pushed on the GitHub repo: 1. Created a custom dataset from public sources for Political and Non Political Tweets. Total corpus of 6060 tweets, with 4088 labelled as Political and 1972 labelled as Not Political. 2. Cleaned the following datasets for classification a. Spam SMS Dataset b. Offensive Language Dataset c. Political Tweets Dataset 3. Performed the following Pre-Processing on the text data a. Removed Stop Words b. Removed Non Alphabetic Characters c. Performed Stemming 4. Performed the following types of feature extractions: a. Count Vectors b. Word level TF-IDF c. N-Gram level TF-IDF d. Character Level TF-IDF 5. Trained the following models: a. Naive Bayes b. Linear Classifier c. SVM d. Random Forest 6. Produced Classification reports for each of the following trained models Note : The tasks 3 to 6 were performed on all the three datasets. Each model has a separate notebook on GitHub under the Helper Notebooks Directory. In each notebook, the currentDF (i.e. the current Data Frame can be changed to choose one among the three datasets. Page 2 of 2 2) Which tasks are pending ? Except CNN, all the models have been trained and tested on the datasets. They are showing good accuracy levels. The following tasks are still pending 1. Train a CNN Model to perform text classification 2. Analyse the accuracies of different model on different datasets with different features by creating a comparison table. 3. Train and Save the most accurate model for classifying political tweets (Indian Context). 4. Fetch Tweets from twitter using the twitter API and classify them as Political or Not Political using the saved model. 5. Create a comprehensive jupyter Notebook to cover all the task performed (for the purposed of presentation). Create readme file on GitHub (documentation with instructions) and Video Presentation for submission. 3) Are you facing any challenges ? Having no experience with sklean python library, it was initially a bit challenging to get things done. But as I progressed, things became much more clear. Implementing a CNN to classify texts also seemed a bit challenging at the beginning. However, I am confident that I will be able to accomplish the task. Also, looking forward to use the tweepy python library to extract tweets automatically.
https://github.com/AnantSharma18/CourseProject	Project Proposal.pdf	Page 1 of 5 Individual Project - Free Topic Anant Ashutosh Sharma anantas2@illinois.edu Topic Text and Tweet Classification using Machine Learning What is the task ? The task involves classifying texts into their relevant categories using different machine learning techniques. Text classification is one of the standard applications in text mining. . The objective of our text classification task is to find appropriate labels for previously unlabelled data from a predictive model which has been trained on a pre-labelled dataset. A series of necessary subtasks are performed to identify and extract relevant features from a given text, which can be further applied to train a predictive model The text to be classified can either be a sentence or a group of sentences i.e. a paragraph. Depending on the labels of the dataset, the text can be classified in binary labels or multiple labels. For instance, on training the models on SMS spam dataset, it will be able to accurately classify texts as Spam (spam) or Not Spam (ham), whereas on training the models on the Hate-Speech and Offensive Language Data, it will achieve the task of classifying text into hate speech, offensive language or neither As of now, I plan to train models to 1. Classify emails in to Spam or Not Spam 2. Classify text into different hate-speech, offensive language categories 3. Classify political tweets using a newly created dataset Why is it important or interesting ? The internet is a hub of textual information. Users are mostly overwhelmed with the amount of information that they have to go through every day. Classifying the text which users encounters into different buckets can give a boost to their efficiency as well as understanding. For instance, classification of news into different topic allows the users to only focus on the topics relevant to their interest. The core importance of textual classification lies in finding an appropriate representation of text data where interesting metrics (as measurements) can be used in order to compare different text data in accordance to their similarity to extract insights. Page 2 of 5 Further, classification of different texts from public social media platform can be tremendously useful in identifying the nature of a post / tweet. For instance, classifying tweets into different hate-speech categories can be used to automatically remove the tweet from the user's handle. The opportunities of text classification are endless. The interesting thing not only lies in just classifying texts into different buckets, but the analysis which can be on the basis of classification obtained. For instance, using the classifier to classify Political tweets can be used to identify politically vocal users and the models can further be improvised to provide analytical parameters for their political inclinations. What is your planned approach ? I plan to implement multiple models in order to classify textual data from publicly available datasets initially. A comparison study between the different models will allow to identify the most accurate model for a particular dataset. As of now, I plan to follow the following pipeline to categorize textual data 1. Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters 2. Extract features from the pre-processed data I plan to implement and compare the following vector representations a. Count Vectors b. TF-IDF Vectors i. Bag of words (word level) ii. Bag of n-grams (n-gram level) iii. Character level 3. Training Model - Learning a. Naive Bayes b. Linear Classification c. SVM d. Random Forest e. Convolutional Neural Network 4. Classification of text using the trained model 5. Evaluation and Comparison of different models Page 3 of 5 I plan to use the above approach for different datasets, namely, SMS Spam and hate- speech. Further, I will using the above approach on my newly generate dataset to classify political tweet in the Indian context. What tools, systems or datasets are involved? I will primarily be using Python. Within python, I plan to use the following libraries sklearn, seasborn, pandas, numpy, Spacy, nltk for getting stopwords corpus, tweepy etc. Datasets Involved * SMS Spam Collection http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam. Labels : spam / ham * Hate-speech and Offensive Language Dataset https://github.com/t-davidson/hate-speech-and-offensive- language/tree/master/data The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns: count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF). hate_speech = number of CF users who judged the tweet to be hate speech. offensive_language = number of CF users who judged the tweet to be offensive. neither = number of CF users who judged the tweet to be neither offensive nor non-class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither Labels : hate speech / offensive language / neither * Political Tweets Dataset - Custom (India) I plan to obtain the relevant political tweet (Indian context) from the publicly available dataset on Kaggle https://www.kaggle.com/codesagar/indian-political-tweets-2019-feb-to-may- sample Page 4 of 5 I will label them as political. Further, I will collect non-political tweets from publicly available twitter datasets and label them as non-political. What is the expected outcome? I expect to accurately classify the textual data in their respective categories using the trained models. Further, I intend to write a script which can automatically collect tweets based on some parameters and accurately classify them into political and non- political tweets. Inputs : Textual data from the respective datasets Expected outcomes as per the models trained on different datasets SMS Spam Data : spam / ham (Not spam) Hate Speech and Offensive Data : hate speech / offensive language / neither Political Tweets Dataset : political / non political How are you going to evaluate your work? I intend to evaluate the models by comparing their predictions using parameters like precision, recall, f1 scores etc. This evaluation will help identify the most suitable model for textual classification, depending on the datasets. At the time of training, the dataset will be divided into testing, training and validation sets. Which programming language do you plan to use? I plan to implement this project in python. Further, I am also planning to use Jupiter notebook for making the code more presentable and easy to demonstrate during the project presentation. Justification of Work Load S. No. Task Estimated Hours 1. Import and Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters 2 hrs 2. Extract features from the pre-processed data a. Count Vectors (0.5 hrs) b. TF-IDF Vectors (1.5 hrs) i. Bag of words (word level) 1.5 hrs Page 5 of 5 ii. Bag of n-grams (n-gram level) iii. Character level 3. Training Model - Learning a. Naive Bayes (0.5 hrs) b. Linear Classification (0.5) c. SVM (1 hr) d. Random Forest (1 hr) e. Convolutional Neural Network (4 hrs) 7 hrs 4. Classification of text using the trained model 2 hrs 5. Evaluation and Comparison of different models 4 hrs 6. Create new data set for classifying political tweets in India 3 hrs 7. Fetch tweets using different parameters and classify as Political / Non Political 3 hrs This above table only gives a rough time estimate of the tasks which will be involved in completing the project. It fulfils the 20+ hours workload as mentioned in the requirements.
https://github.com/AnantSharma18/CourseProject	README.md	CourseProject | Text and Tweet Classification using Machine Learning This project is focussed at classifying texts into their relevant categories using different machine learning techniques. Text classification is one of the standard applications in text mining. . The objective of our text classification task is to find appropriate labels for previously unlabelled data from a predictive model which has been trained on a pre-labelled dataset. A series of necessary subtasks are performed to identify and extract relevant features from a given text, which can be further applied to train a predictive model. The following Classifications have been accomplished in this project: 1. Classify SMS into Spam or Not Spam 2. Classify text into different hate-speech and offensive language category 3. Classify political tweets (Indian context) using a custom dataset Note : The purpose of this project is to present the different machine learning implementations to classify textual data. While the accuracies of the different models are mentioned in the result section, the project focussed more on the different ways machine learning can be used to perform text classification. Contribution Project by Anant Ashutosh Sharma Free Topic : Text and Tweet Classification using Machine Learning Course : CS 410 NetID : anantas2 The following documentation and demo is submitted to the GitHub Repo 1. Project Proposal 2. Project Progress Report 3. Self-Evaluation Report 4. Demo (Video) Demo Video : https://uillinoisedu-my.sharepoint.com/:v:/g/personal/anantas2_illinois_edu/ETcTrozhFLVFjqRJjMrkPTkBk-iMH54QljVBzl-KbDYWaA?e=7r6QDU Datasets The details for the datasets is as follows: - SMS Spam Collection Source: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ The SMS Spam Collection v.1 is a public set of SMS labeled messages that have been collected for mobile phone spam research. It has one collection composed by 5,574 English, real and non-enconded messages, tagged according being legitimate (ham) or spam. Labels : spam / ham Hate-speech and Offensive Language Source: Dataset https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns: count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF). hate_speech = number of CF users who judged the tweet to be hate speech. offensive_language = number of CF users who judged the tweet to be offensive. neither = number of CF users who judged the tweet to be neither offensive nor non-class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither Labels : hate speech / offensive language / neither Political Tweets Dataset - Custom | Indian Context The political tweets have been taken from a GitHub repo for Sentiment Analysis on Indian Political fetched tweets. Source: https://github.com/rohitgupta42/polity_senti Further, a set of General tweets was taken from the The Twitter Political Corpus Source: https://www.usna.edu/Users/cs/nchamber/data/twitter/ Both of these datasets were combined and cleaned to obtain the required datasets. Labels : POL / NOTPOL Methodology In this project, I have implemented multiple models in order to classify textual data from publicly available datasets initially. Each model has been implemented in a seperate Jupyter Notebook for clear understanding. A comparison study is also presented between the various models in the result section of this documentation. Pre-processing Cleaning of text Stop Words removal Stemming Removal of non-alphabetic characters Feature Extraction Count Vectors b. TF-IDF Vectors Bag of words (word level) Bag of n-grams (n-gram level) Character level Implemented Models Naive Bayes Linear Classification SVM Random Forest Convolutional Neural Network At the end of each model notebook, an accuracy report is present. The classification accuracy of different models is summarised in the results section below. Results The following table contain the F1 Scores for each of the predicted categories: Naive Bayes | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.88 | 0.96 | 0.97 | | Word TF-IDF | 0.83 | 0.93 | 0.92 | | N-Gram TF-IDF | 0.60 | 0.91 | 0.80 | | Char TF-IDF | 0.71 | 0.94 | 0.91 | Linear Classification | Feature / Cagtegory | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.91 | 0.97 | 0.96 | | Word TF-IDF | 0.81 | 0.97 | 0.97 | | N-Gram TF-IDF | 0.18 | 0.91 | 0.82 | | Char TF-IDF | 0.84 | 0.96 | 0.95 | SVM | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:-----------------:| | Count | 0.89 | 0.97 | 0.95 | | Word TF-IDF | 0.92 | 0.97 | 0.96 | | N-Gram TF-IDF | 0.79 | 0.92 | 0.81 | | Char TF-IDF | 0.88 | 0.97 | 0.94 | Random Forest | Model / Feature | SMS Spam | Offensive language | Political Tweet | |-----------------|:--------:|:------------------:|:---------------:| | Count | 0.86 | 0.97 | 0.95 | | Word TF-IDF | 0.85 | 0.97 | 0.96 | | N-Gram TF-IDF | 0.74 | 0.92 | 0.81 | | Char TF-IDF | 0.83 | 0.96 | 0.94 | Convolutional Neural Network (CNN) | CNN / Category | Spam SMS | Offensive Lang | Political Tweet | |---------------------|:--------:|:--------------:|:---------------:| | F1 Score | 0.91 | 0.97 | 0.95 | | Accuracy | 0.98 | 0.95 | 0.94 | Summarized Results The best performing features have the following F1 Scores in each of the models: | Model / Category | Spam SMS | Offensive Lang | Political Tweet | |-----------------------|:--------:|:--------------:|:---------------:| | Naive Bayes | 0.88 | 0.96 | 0.95 | | Linear Classification | 0.90 | 0.97 | 0.96 | | SVM | 0.92 | 0.97 | 0.96 | | Random Forest | 0.86 | 0.97 | 0.96 | | CNN | 0.91 | 0.96 | 0.95 | Political Tweets Prediction The Political Tweet Prediction notebook can be used to fetch and predict tweets into Political and Non Political Categories. The tweets are fetched using the tweepy package which uses the Twitter API to stream tweets pertaining to a certain tracking list. The most accurate model for political tweet prediction i.e. Linear Classification with Word Level TF-IDF features has been saved as a pickle model is used to categorise the tweets in pol / notpol categories. Installation Guide The project makes use of Jupyter Notebook to implement the different models. The following steps to install and run the code on your local machine. Running the Helper Notebooks Clone this repo onto your local machine. You will need the following pre-requisites installed in your local environment: python3, jupyter notebook, numpy, pandas, keras, tensorflow, sklearn, spacy, ntlk, string, pickle, twitter Once you have made sure that the above mentioned packages are installed in you environment, you can go ahead and launch jupyter notebook Each Helper Notebook can be executed seperately to get results for different models. In each of the Helper Notebook, the variable currentDF can be changed to either OffensiveLangDF | spamSmsDF | politicalDF to run the model on different datasets Note : You might have to download the spacy 'en' model seperately in order to run the notebooks Predicting Political Tweets The pre-trained model to predict political tweets is already saved in the folder 'Saved Model' under the name LR_Pol.plk In order to run the political tweet prediction notebook, you will have to first obtain credentials from Twitter API. The consumer_key, consumer_secret, access_token and access_token_secret variables need to be replaced with your unique Twitter API credentials. Guide to obtain Twitter API credentials can be found under references. Once you have replaced the XXXX with your Twitter API credentials, you can execute the notebook to obtain 4 different tweets using the tracking list and classfiy them into pol / notpol using the pre-trained model The trackingList1 and trackingList2 lists can be edited to stream different tweets. The n_tweets variable can be changed to the number of tweets you wish to obtain. Limitations The datasets being used to train the model can be further improved. They currently have very niche examples of the catergories. For instance, the political tweets dataset only contains political tweets pertaining to the Indian Context. Further, the number of training records are less. A higher number of traininf records may allow the models to perform better. The Convulutional Neural Network (CNN) implemented in this project is a simple and generic one. A much more complex and accurate CNN can be designed and fine tuned as per the requirements of each of the datasets. These limitations are present in this project since the purpose of this project is not to present any accurate model to classify text objects, but to present the different methods and ways in which machine learning models can be used to classify textual data. References https://rapidapi.com/blog/how-to-use-the-twitter-api/ http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data https://github.com/rohitgupta42/polity_senti https://www.usna.edu/Users/cs/nchamber/data/twitter/ scionoftech GitHub Repo for helping in eaxtracting different features for models
https://github.com/AnantSharma18/CourseProject	Self Evaluation Report.pdf	Page 1 of 2 Self Evaluation Report Anant Ashutosh Sharma anantas2@illinois.edu Individual Project | Free Topic Text and Tweet Classification using Machine Learning 1) Have you completed what you have planned ? Yes, I have successfully completed what I planned and presented in the Project Proposal as well as the Progress Report. The following table mentions the tasks which have been successfully completed in this project along with the checklist. (These were the same tasks as mentioned in the Project Proposal) S. No. Task Completion 1. Import and Pre-process the input textual data a. Cleaning of text b. Stop Words removal c. Stemming d. Removal of non-alphabetic characters Completed 2. Extract features from the pre-processed data a. Count Vectors b. TF-IDF Vectors i. Bag of words (word level) ii. Bag of n-grams (n-gram level) iii. Character level Completed 3. Training Model - Learning a. Naive Bayes b. Linear Classification c. SVM d. Random Forest e. Convolutional Neural Network Completed 1. Classification of text using the trained model Completed 2. Evaluation and Comparison of different models Completed 3. Create new data set for classifying political tweets in India Completed 4. Fetch tweets using different parameters and classify as Political / Non Political Completed Page 2 of 2 2) Have you got the expected outcome ? Yes, I have got the expected outcome from my proposed project. All the machine learning models successfully classify the textual data from three different datasets into the respective categories with good accuracies and F1 scores. The purpose of this project was to present the different machine learning models which can be used to perform text classification instead of comparing and competing them against one another. Further, as expected, the tweets are automatically streamed using the Twitter API and are classified into political and not political as expected. All the results and outcomes have been successfully presented in the GitHub repo (readme.md) as well as in the various Jupiter Notebooks.
https://github.com/ElizWang/CourseProject	Documentation-and-Software-Tutorial.pdf	"1) An overview of the function of the code (i.e., what it does and what it can be used for). Our code scrapes authors and paper titles from the DBLP dataset. We extract patterns using a Python wrapper for the SPMF library. From these extracted patterns, our code then removes redundant patterns -- we implemented both hierarchical microclustering and one-pass microclustering. Finally, we extract strongest context indicators, representative transactions, and semantically similar patterns. We can use this code to reproduce the DBLP experiment as described in section 5.1 of the paper. That is, we can take in an author or title pattern and find its strongest context indicators, most representative titles and authors / co-authors, and the most semantically similar title / author patterns. 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. Prerequisites: the libraries mentioned below (see answer #3), Python3, Java, shell Web Scraping (utils/build_data_from_web.py) * Build a CSV file called data/data.csv where each line is a list of comma-separated authors and a single title (aka all the information needed from a single paper). * These papers are pulled from the following text mining/database-related conferences: (aciids, icdm, sdm, dba, balt, dbsec, dbcrowd, pkdd, kdd, trec, cikm, sigir). * For each conference, we pull all papers from the most recent 10 events (workshops, talks, submissions, etc) * For each paper, we stem the titles using the nltk.stem.porter stemmer, remove commas and periods (because we are writing the data as a .csv file), and make all author names lowercase in order to avoid interpreting the same author names with different capitalizations as different authors. We also remove all spaces within a single author name and replace them with underscores (firstname_lastname) * Usage: python3 utils/build_data_from_web.py. Note that you must run this from CourseProject/, the data/ directory must already exist, and you must be using Python3. Utility file: Python wrapper for spmf.jar (utils/frequent_pattern_mining/spmf_python_wrapper.py) * Motivation: spmf is a Java library. To make everything run smoothly and to minimize the amount of work the user has to do to run the pattern mining algorithms and set everything up, we wrote a Python wrapper for spmf that runs Java as a subprocess. * Java -jar libs/spmf.jar run [alg_name] [input_file] [output_file] Frequent pattern mining: (utils/frequent_pattern_mining/build_frequent_patterns.py) * Builds frequent pattern files for authors and title terms and caches them to a file. * Maps all unique author names to non-negative integers and all unique title terms to non-negative integer IDs as well -- this is because of 2 reasons: first of all, it's more efficient to work with numbers rather than strings of a theoretically arbitrary length; also, spmf is a Java library that works with numbers rather than strings. Note that the two mappings are completely disjoint; in other words, an author name with an ID of 0 has nothing to do with a title term with an ID of 0. * Runs our Python wrapper for spmf.jar for FP-Close (to mine frequent patterns from author terms) and Clo-Span (to mine sequential patterns from title terms) to generate intermediate patterns we would then parse. * Files generated * data/frequent_author_patterns.txt: Frequent author patterns (via FP-Close), where each line is a pattern. Format: 11391 11393 11392 14928 #SUP: 9 * data/frequent_title_term_patterns.txt: Frequent title term patterns (via Clo-Span), where each line is a pattern. Format: 4 -1 226 -1 240 -1 #SUP: 32 * data/author_id_mappings.txt: Mapping from author ID to author name. Format: 0 helun_bu * data/title_term_id_mappings.txt: Mapping from title term ID to title term. Format: 0 toward Redundant pattern removal: (utils/remove_redundant_patterns.py) * Two utility methods for removing redundant patterns: * Eliminate redundancy using one-pass microclustering * Eliminate redundancy using hierarchical microclustering * The main method is currently written to support one-pass microclustering -- a threshold can be passed in or the default threshold can be used. The cleaned title term patterns are then written to a file. * Usage: * title_patterns = parse_author_file_into_patterns(FrequentPatternBuilder.TITLE_TERMS_OUTP UT_FILE_PATH) * minimal_patterns = find_one_pass_microclustering_patterns(title_patterns, 0.6) # To use one-pass microclustering * minimal_patterns = find_hierarchical_microclustering_patterns(title_patterns, 0.6) # To use hierarchical microclustering * write_patterns_to_file(MINIMAL_TITLE_TERMS_FILENAME, minimal_patterns) Utility file: Cosine similarity (utils/cosine_similarity.py) * Compute cosine similarity given two context vectors of the same length. Used as a utility method * Usage: consine_sim = compute_cosine_similarity(context_vec_1, context_vec_2) Utility file: Mutual information computation/caching manager (utils/mutual_information_manager.py) * Computes mutual information for two given ordered collections of patterns (which can be author-author patterns, author-title patterns, or title-title patterns) and caches the mutual information values to a file for each (pattern 1 ID, pattern 2 ID) pair, where each pattern ID just corresponds to its index in the ordered collection. * Each file begins with a value [0, 3], which corresponds to a mapping: 0 = author-author, 1 = author-title, 2 = title-author, 3 = title-title * Example file format: # 2 # 0 0 0.000016 # 0 1 0.000016 # 0 2 0.000016 # 0 3 0.000016 # 0 4 0.000016 # 0 5 0.000016 # 0 6 0.000016 # 0 7 0.000016 # 0 8 0.000016 # 0 9 0.000016 * Files generated # data/author_author_mutual_info_patterns.txt = Mutual information values for each author-author pattern pair, where index_1 <= index_2 # data/author_title_mutual_info_patterns.txt = Mutual information values for each author-title/title-author pattern pair # data/title_title_mutual_info_patterns.txt = Mutual information values for each title-title pattern pair, where index_1 <= index_2 * Usage: # transactions = TransactionsManager(""data/data.csv"", ""data/author_id_mappings.txt"", ""data/title_term_id_mappings.txt"") # mutual_info = MutualInformationManager(transactions, True) # mutual_info.compute_mutual_information(author_patterns) * Reads in the mutual information from one of the files described above, depending on what the type of manager (author-author, author-title, or title-title manager), and caches all the mutual information values in a large matrix -- which is triangular for the author-author/title-title cases and rectangular for the author-title case * Usage: * mutual_info = MutualInformationManager(MutualInformationManager.PatternType.X) * mutual_info.read_mutual_information_from_file() * mutual_info.get_mutual_information(1, 2) # to get mutual info for patterns 1 and 2 * Used to compute mutual information one time (which is expensive) and to abstract how mutual information is stored and managed into a class Utility file: Transaction/paper data parser/manager (utils/transactions_manager.py) * Parses and stores author-id mappings, title-term-id mappings, and a list of all papers, which are basically (author set, title-term sequence patterns) * Utility methods for * Compute context models for each paper's title terms given a list of frequent title patterns * Compute context models for each paper's authors given a list of frequent author patterns * Find all transaction IDs, which are represented as paper indices from the list of all papers parsed, that have a title pattern as a sequential subset * Find all transaction IDs, which are represented as paper indices from the list of all papers parsed, that have an author pattern as a non-sequential subset * Get author name from author ID, title term from author ID, get a paper's author from the paper ID, get a paper's title terms from the paper ID, and get the number of transactions (number of papers) * Usage: transactions = TransactionsManager(""data/data.csv"", ""data/author_id_mappings.txt"", ""data/title_term_id_mappings.txt"") Pattern annotator: Representative transaction extraction (pattern_annotators/representative_transaction_extractor.py) * Extracts representative transactions given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern * Usage: python pattern_annotators/representative_transaction_extractor.py [target_id] [k] [is author experiment] * Example: python pattern_annotators/representative_transaction_extractor.py 0 3 True * For an author experiment, the top k most representative titles from the transaction manager are displayed * For a title experiment, the top k most representative titles and the top k most representative authors from the transaction manager are displayed Pattern annotator: Semantically similar pattern extraction (pattern_annotators/semantically_similar_pattern_extractor.py) * Extracts semantically similar patterns given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern * Usage: python pattern_annotators/semantically_similar_pattern_extractor.py [target_id] [k] [is author experiment] * Example: python pattern_annotators/representative_transaction_extractor.py 1 10 False * For an author experiment, the top k most representative author patterns are displayed * For a title experiment, the top k most representative title term patterns are displayed Pattern annotator: Strongest context indicator extraction (pattern_annotators/strongest_context_indicator_extractor.py) * Extracts strongest context indicators given a pattern ID, a threshold for the number of representative transactions to extract, and a boolean value describing whether the pattern ID refers to an author pattern or a title-term pattern * Usage: python pattern_annotators/strongest_context_indicator_extractor.py [target_id] [k] [is author experiment] * Example: python pattern_annotators/strongest_context_indicator_extractor.py 2 15 True * For an author experiment, the given pattern is annotated with the top k strongest context indicators -- the top k author patterns and the top k title patterns * For a title experiment, the given pattern is annotated with the top k strongest context indicators -- the top k title patterns and the top k author patterns 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. Libraries Used * urlllib (web scraping from DBLP) * bs4 (parsing scraped data from DBLP) * nltk (stemming) * spmf (Java library to mine frequent patterns -- will be installed as a part of the setup step, see the next section) * Installation: pip install urllib/bs4/nltk Setup and Installation NOTE: Our data files are quite large; thus, we put the data/ directory in our .gitignore and we're subsequently not committing any of our data files or our library files to our repository, which is a standard in good software design. * git clone https://github.com/ElizWang/CourseProject.git # Clones repository * cd CourseProject # Navigate to repository * sh setup.sh # Sets up the data/ and libs/ directories and runs all scripts from the preprocessing step. Detailed description below: * Creates a data/ directory if one does not already exist * Builds a csv file called data/data.csv by scraping DBLP paper submissions from the web * Creates a libs/ directory if one does not already exist and pulls spmf as libs/spmf.jar * Mines frequent author patterns using FP-Close * Mines sequential title term patterns using Clo-Span * Removes redundancies from sequential title term patterns using microclustering * Computes and caches all mutual-information values between (author pattern, author pattern), (title term pattern, author pattern), and (title term pattern, title term pattern) 4) Brief description of contribution of each team member in case of a multi-person team. We worked together in understanding the paper and formulating pseudocode that we used as our overarching plan. We split the paper into two steps -- preprocessing (which includes data scraping, pattern mining, and similar pattern pruning) and pattern annotation. We worked as a team by calling each other on Zoom and pair programming. Elizabeth typed, while Steven frequently took remote control in implementing preprocessing, extracting representative transactions, and the extraction of strongest context indicators and semantically similar patterns. 5) Software demo link: https://www.youtube.com/watch?v=3v8M0sW3xHc"
https://github.com/ElizWang/CourseProject	Progress-Report.pdf	We have completed the implementation part of our project (i.e. the coding portion of the project is completed). In particular, we've mined author and title patterns using FP-Close and CloSpan respectively and implemented redundancy removal using one-pass microclustering, extracting strongest context indicators / representative transactions / semantically similar patterns, and used these to reproduce the DBLP experiment. We may need to touch up some documentation, and we need to record the tutorial presentation. Aside from potential technical issues with recording the tutorial (which would most likely be minor), we do not anticipate any challenges at this time.
https://github.com/ElizWang/CourseProject	Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? Elizabeth Wang, eyw3, Captain Steven Pan, stevenp6 2. Which paper have you chosen? We have chosen the following paper: ""Generating semantic annotations for frequent patterns with context analysis"" 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? No 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, a more recent version of the dataset that derives from the dataset used in the paper can be found here: https://dblp.org/faq/1474681.html 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A"
https://github.com/ElizWang/CourseProject	README.md	"CourseProject Authors Elizabeth Wang Steven Pan Proposal NOTE: We've uploaded a PDF (Proposal.pdf) with the same information as we hae below: What are the names and NetIDs of all your team members? Who is the captain? Elizabeth Wang, eyw3, Captain Steven Pan, stevenp6 Which paper have you chosen? We have chosen the following paper: ""Generating semantic annotations for frequent patterns with context analysis"" Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? No If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? Yes, a more recent version of the dataset that derives from the dataset used in the paper can be found here: https://dblp.org/faq/1474681.html If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A Demo https://www.youtube.com/watch?v=3v8M0sW3xHc Setup Install bs4, urllib, and nltk (if they're not already installed) Run setup.sh (sh setup.sh) from CourseProject/ to Build the csv file containing all (author list, title) entries. The code that builds this data file is here: utils/build_data_from_web.py. This script will create a directory called data/ and create a csv file called data.csv within that directory -- CSV file format: author1, author2, author3, ... etc, Title (where each line in the CSV file corresponds to a single paper) Create the libs/ directory and download spmf.jar, which is a JAR file for the SPMF library (download link is also here: http://www.philippe-fournier-viger.com/spmf/index.php?link=download.php) Builds frequent patterns for authors and title terms -- data/frequent_author_patterns.txt and data/frequent_title_term_patterns.txt, where all words are mapped to unique ids and the id mapping is cached in these 2 files respectively: data/author_id_mappings.txt and data/title_term_id_mappings.txt. The code that builds these files is here: utils/frequent_pattern_mining/build_frequent_patterns.py Removes redundancies from sequential frequent title patterns (data/title_term_id_mappings.txt) and creates a new file called data/minimal_title_term_patterns.txt containing these minimal patterns Builds files to cache all mutual information values between pairs of author patterns, between pairs of author-title patterns, and between pairs of title patterns RELEVANT OUTPUT FILES FOR NEXT STAGE: * data/frequent_author_patterns.txt (ID mappings: data/author_id_mappings.txt) * data/minimal_title_term_patterns.txt (ID mappings: data/title_term_id_mappings.txt) * data/author_author_mutual_info_patterns.txt * data/author_title_mutual_info_patterns.txt * data/title_title_mutual_info_patterns.txt NOTE: utils/parse_patterns.py contains utility methods to parse patterns into data structures and write them to files, you may find these methods useful"
https://github.com/bachman5/CS410-BiasDetector	CS410 Course Project Proposal.pdf	CS410 Course Project Proposal US News Political Bias Detector Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Sentiment Analysis Description: Our free topic is to design a system that can examine mainstream news headlines and determine if they contain any political bias. The bias would be determined using sentiment analysis and NLP techniques. Important or Interesting: The political tension between the Democrat and Republican parties in the US has never seemed higher. Mainstream news media reports on events with a palpable bias that slants heavily toward one party or the other. Many Americans want unbiased, factual news reports to avoid being manipulated. An example visualization of this political bias from AllSides.com is shown in Figure 1. These visualizations are created from community feedback data which can also be bias / not normalized. https://www.allsides.com/media-bias/media-bias-ratings Figure 1 Media Bias Visual Planned Approach: Our approach will be to text mine news headlines from 2020 to create a data set or find an existing dataset that has already been labeled. We will then build or modify an existing Sentiment Analysis Model and tune it for political bias. Other models that we might try in case we don't get good results are clustering based on latent Dirichlet allocation (LDA), logistic regression and deep learning. We then plan to visualize the results to see how various media outlets rank across a bias spectrum. Left and Right will be the sentiment classes. We might add Lean Left and Lean Right classes if deemed appropriate. Tools, systems, datasets: NLTK is a suite of python libraries that can be used for classification, tokenization, stemming, tagging, parsing and NLP. Google's Named Entities Sentiment Beta API for entity sentiment. Might use TensorFlow if we tried Deep Learning model. Gensim for LDA. We will high likely use Hybrid sentimental analysis algorithms which include stemming, tokenization, lexicon along with some automatic approaches available for public use like BERT. The final report might be displayed in form of BI visualization with Tableau or D3.js. Expected outcome: * Input * A New Article Headline * A fixed set of classes C = {c1,c2,..,cn} * Output o A predicted class c  C Project Timeline: Milestone Due Date Submit Proposal Oct 25th Working Prototype Nov 22th Project Progress Report Nov 29th Project Completion and Submission Dec 13th Programing Language: Python and NLTK (http://www.nltk.org/) Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Web scrapping for News Headlines 5 Collect and Label Test / Training Datasets (Corpus) 5 Build / Tune Sentiment Analysis Model 20 Train Sentiment Analysis Model (Iterative) 15 Test Sentiment Analysis Model 15 Visualization to display results (Tableau or Website) 8 Develop Software Documentation 2 Create Video Demonstration 2 Total 72
https://github.com/bachman5/CS410-BiasDetector	CS410-BiasDetector Final Report.pdf	"CS410 Course Project Final Report US News Political Bias Detector December 13th, 2020 Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Text Classification and Sentiment Analysis Public Repository: https://github.com/bachman5/CS410-BiasDetector Project Timeline: Milestone Due Date Submit Proposal Oct 25th - Complete Working Prototypes Nov 22th - Complete Code Video Demo Nov 28th - Complete Project Progress Report Submission Nov 29th - Complete 2 x Initial Peer Reviews Dec 2nd - Complete Project Completion and Submission Dec 13th - Complete 2 x Final Peer Reviews Dec 16th Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Clean News Headlines 4 hours - Complete Collect and Label Test / Training Datasets (Corpus) 10 hours - Complete Build / Tune Sentiment Analysis Technique 12 hours - Complete Build / Train / Tune Logistic & SVM Model 12 hours - Complete Build / Train / Tune Deep Learning Technique 12 hours - Complete Develop Software Documentation 2 hours - Complete Team Meetings 8 hours - Complete Create Video Demonstrations 5 hours - Complete Total 65 hours Project Motivation The political tension between the Democrat and Republican parties in the US has never seemed higher. Mainstream news media reports on events with a palpable bias that slants heavily toward one party or the other. Many Americans want unbiased, factual news reports to avoid being manipulated. Our free topic was to design a system that can examine mainstream news headlines and determine if they contain any political bias. The bias was determined using various sentiment analysis, shallow NLP and Machine Learning techniques. We then compared the results of three specific approaches by using all the knowledge gained from the CS 410 course. Collect and Label Datasets (Corpus) Raw Dataset: https://www.kaggle.com/snapcrack/all-the-news Our Corpus was taken from ~200,000 News Articles published in the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The bulk of headlines were taken from 2015-2017 during a heated US presidential election. Data Model Figure 1: Data collection, filtering and labeling workflow AWS SageMaker With AWS SageMaker, large labeling jobs can be broken up and assigned to public or private workforces. SageMaker increased the speed and accuracy of our labeling process. The jobs were broken up and assigned across all three team members. Each team member then recruited politically independent 3rd party members to assist with the large labeling task. We specifically tried to get labeling assistance from friends and family members that don't have strong political ties to either the Republican or Democratic ideologies. If the labeler has a strong political bias, then the resulting models could also reflect that bias. AWS GroundTruth AWS GroundTruth is the portal that private team members used to label our data. A simple dashboard was created for the labeler to view a single news headline at a time with no additional information like publisher that could introduce additional bias. The GroundTruth user had options to label the headline as either having Right Bias, Left Bias or Neutral. Final Labeled Test / Training Dataset: https://github.com/bachman5/CS410- BiasDetector/tree/main/data_labeled Our completed training data contained over 4,000 filtered, cleaned and labeled headlines from the workflow. News headlines were labeled as either Right Wing bias, Left Wing Political bias or Neutral using AWS Sagemaker and GroundTruth. Here is an example record correctly labeled with Left Wing bias: Lessons Learned AWS Ground Truth provides a great way to explain to the labeler exactly what you are looking for. You can even provide them examples of correct and incorrect label. These are great features, but they come at a high cost. Our team was unaware that AWS charges .08 cents per label. There is no warning or advertised cost until you get your monthly bill. 4,057 objects x .08 = $323.56. Category Classification + Sentiment Analysis Approach Language: Python Dependencies: nltk, nltk.vader, numpy, pandas Public Repository: https://github.com/bachman5/CS410-BiasDetector/blob/main/sentiment_test.py Primary Team Member: Nick Bachman Overview: VADER (Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is included in the NLTK package and can be applied directly to unlabeled text data. It can also be customized for specific domains and use cases. Our first approach was to see if we could design a system that combines topic category and sentiment to accurately predict political bias. Technical Approach: The goal of this model is to correctly determine if a text headline includes Left or Right political bias. In the headline ""Daily News mourns the death of the Republican Party, killed by epidemic of Trump"", the subject of the news article is the Daily News. The Reuters writer Lucas Jackson used the terms mourns, death, killed and epidemic to communicate significantly negative sentiment about the Republican Party and Donald Trump. Most people would agree that this headline supports a Left-Wing political position and thus includes a Left bias. {""source"":""69446,""Daily News mourns the death of the Republican Party, killed by epidemic of Trump'"",""class-name"":""Left""} As a group, we spent time thinking about the definition of political bias and how to design a labeling and detection system. The chart below summarizes our design decision. Sentiment alone is not enough to determine political bias. You also need a way to determine the category that sentiment is being directed toward. Topic Category Sentiment Bias Republican Negative :( Left Republican Positive :) Right Republican Neutral :| Neutral Democrat Negative :( Right Democrat Positive :) Left Democrat Neutral :| Neutral Table 1: Combining Category and Sentiment to determine Bias Figure 2: Combining Category and Sentiment to determine Bias Technical Challenges: Updating the category lexicon with descriptive terms and updating the VADER sentiment lexicon with popular political terms increased the accuracy by about 8-10%. Tunning the values for how much negative or positive sentiment yields a neutral response also helped because ""Neutral"" was the label with the most entropy. Example: ""Game of Thrones: Republicans hate it, Democrats love it"". GoT is the subject in this example. If the reader likes the HBO show Game of Thrones, it changes how they will label it. Both Republicans and Democrats are mentioned so there are multiple categories to attach the sentiment. Both Negative (hate) and Positive (Love) sentiment are expressed. The labeler may choose Neutral because they are unsure or do not know what Game of Thrones is in relation to this political topic. Complex sentiment and ambiguity make it very difficult to accurately label bias and accurately predict bias. Lessons Learned One additional feature that I added was putting a heavier weight on terms that occurred first in a headline. In English, most subjects occur to the left of the predicate. When both Democratic and Republican terms are mentioned, I subtracted the weights from each other. If the difference was minor, I added a category for ""Both"" that covers when Democrats and Republicans are both the subject (left of the predicate) and close together. This does not cover the minority cases when the subject is to the right of the predicate. This happens with questions, sentences that begin with ""Here"" or ""There"" and sentences beginning with one or more prepositional phrases. Additional rules could be added to check for these three exceptions to change the weighing when the subject occurs after the predicate. I tried using NLP external libraries such as Spacy but they also struggled with identifying the subject of many headlines. I think this is because news headlines are not using natural language. In other words, humans don't usually talk or write in the same way that news headlines are written to capture your attention. Example: ""Hillary Clinton: Republicans dishonor constitution by vowing to block Scalia replacement"" Hillary Clinton (Dem) is saying that Republicans are dishonoring the constitution. The colon is acting like the verb ""says"". My code correctly identifies Hillary Clinton (Left) as the subject and attaches the negative sentiment because of the word dishonor and determines that it's right bias. However, the reader sees that the negative sentiment is attached to Republicans and labels it as left bias. In this case the sentiment was not attached to the subject but rather what the subject said about another subject. This is just one example of the challenges faced by trying to attach sentiment to a subject. Results: biasdetective1 biasdetective2 biasdetective3 Right Precision .580 .487 .414 Right Recall .593 .633 .708 Right F1 .586 .550 .523 Left Precision .450 .575 .557 Left Recall .565 .542 .578 Left F1 .503 .558 .568 Overall Accuracy .521 .528 .483 Support Vector Machine Approach Language: Python Public Repository: https://github.com/bachman5/CS410-BiasDetector/tree/main/SVM_Model Dependencies: numpy, panda, glove, spacy, sklearn Primary Team Member: Anh Nguyen Overview: Support Vector Machine (SVM), a Discriminative Classification introduced in one of our lectures in Text Categorization, is another method that we decided to implement in our model. The method is mainly included in sklearn package. To improve the dataset and support model's accuracy, couple of NLP methods and Vector Embedding are also utilized and can be explain further in the technical approach Figure 3: SVM explanation Technical Approach: Figure 4: Overall Diagram for SVM Model 1. Preprocessing Data (NLP): (Using Spacy) The main goal of this step is using some NLP techniques to conduct meaningful patterns and themes for the text data using spacy package: tokenization (breaking news titles into token), stopword (removing common stop works in sentence), punctuation (clean sentence better by ignoring punctuation like marks and spaces) Before and after Word Tokenized Before and after Removing stopwords Word Tokenization appeared to perform accurately with the purpose of data cleaning. However, removing stopwords tend to remove all the context of the sentence. 2. Word embedding & Magnitude (Using Glove) Building Corpus After preprocessing data, we then store available data into three different text files: train.txt, text.txt and corpus.txt. Ideally Corpus should be a dictionary including all political words in bigger spectrum than only words in train & text data, however due to the limitation of the data our Corpus only built upon our available scope of data. Words in Corpus then mapped to corresponding vectors (Word Embedding/word Vectorization) with the hope of capturing the meaning of potential relationship of word in term of similar contexts/syntax/spelling/co-occurrence. Corpus (Dictionary) with vectorized words Testing word's distance: Result: Another method that we were using (using Glove package) was store our corpus to a magnitude file. Normally a corpus can be text-formatted but storing dictionary in magnitude file helps improving the processing time. Vectorizing titles Using MeanEmbeddingVectorizer & TF-IDF Embedding 3. Train/Tune Model (Using sklearn) Current Result TF_IDF (Preprocessing) Mean Embedding Vector (preprocessing) Accuracy 0.514379622021364 0.5308134757600658 Precision 0.5306949522525308 0.4542249353200501 Recall 0.514379622021364 0.5308134757600658 F1 Score 0.4751926328128259 0.48172315509176467 Technical Challenge/ Lesson Learn: The current accuracy is still in range 50% with TF-IDF embedding preprocessing as well as Vector preprocessing. While there is not much success in improving the accuracy levels. We started to doubt the truthfulness of our labelled data. To assert the validity of our model pipeline, we switched to benchmarking using a different dataset while keeping everything else the same. The dataset is ATIS Airline Spoken Language Intent (train / test) which classifies a passenger's inquiry into 1 of the possible 18 intents: flight, flight time, meal, etc. The data size is about 5000 records and 18 classes (intents). The performance came out significantly better than what observed from our dataset: Accuracy : 0.9113495200451722 Precision: 0.9107883118388134 Recall : 0.9113495200451722 F1 score : 0.9011684778567066 One more example of our doubt in data integrity is that that we discovered the differences labels in one title. However, we do not complete blame on this issue as in the real world many opinions can appear for the same information. 100300,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral 113763,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Left 100300,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral 113763,"" Republican, Democratic Senators Demand Inquiry Into Russian Election Interference"", Neutral Deep Learning: Recurrent Neural Network Classification Approach Language: Python Dependencies: TensorFlow Primary Team Member: Muhammad Rafay Overview: The approach uses a recurrent neural network (RNN) to classify news headlines as Left or Right biased. Technical Approach of RNN: Unlike the feed forward neural network we used an RNN for classifying news headlines. The difference in RNN verses feed forward is that output of an RNN is not just influenced by the input we just fed but it is influenced by the history of inputs we have fed earlier. An example is given below: So, it means RNN is suitable for classifying sequence data and news headline is a sequence. Since, neural network processes numbers we had to convert text to a sequence of token indices using an encoder. After the encoding is the embedding layer, this layer stores one vector per word after training words with similar meanings have similar vectors. Next comes the RNN with a stack of hidden layers, it processes the sequence of input by iterating through the elements. At each step output from the previous step is passed with the input of the next step. The sequence is converted to a single vector which is further converted to a single logit as the classification output. The architecture of our RNN is like the one given below: Lesson Learned: Our current classification accuracy is 53%. Not much different from other models. Since we are classifying based on political bias and not sentiments, the two classes: Left and Right has a lot of common vocabulary. Both text of both classes contains hate speech and words ""democrats"" and ""republican"" so in short, the vocabulary is not a very distinguishing feature for the two classes. The sentiment of bias is sometimes hidden behind the meaning of the sentence and it is not obvious e.g. ""Are Republicans more likely to prefer pulp in their orange juice?"" It is only obvious when something positive or negative is being said about one of the parties, and that involves recognizing the sentiment of the entity but not every headline is like that e.g. ""Republicans' patience with Trump may be running out"" We also suspected that the poor results are due to erroneous labeling of dataset. Hence, we explored and found another similar dataset Ideological Books Corpus (IBC). We trained/tested RNN on IBC dataset and found that might not me the case as the performance metrics for this dataset were like ours rather a bit poor So, we need a model that would learn the hidden meaning behind the headlines. On our own dataset Accuracy: 0.528 Precision: 0.507 Recall: 0.558 F1 score: 0.531 On our own dataset Accuracy: 0.504 Precision: 0.454 Recall: 0.432 F1 score: 0.442 -------------------------------------------- Deep Learning: Classification using BERT Language: Python Dependencies: TensorFlow Primary Team Member: Muhammad Rafay Overview: BERT is recent breakthrough in NLP. It makes use of a transformer mechanism that learns contextual relations between words. It has achieved state-of-the-art performance on several NLP tasks such as GLUE. As we looked forward to improving performance our next choice of model was BERT Technical Approach: As compared to an RNN based LSTMS which we implemented earlier BERT incorporates a non-directional encoder unlike LSTMS which read the test in sequence (left-to-right or right-to-left) BERT reads the entire sequence at once. This allows BERT to learn the context of a word based on all its surroundings. BERT models are pre-trained on a large corpus of text. They then fine-tuned for specific tasks Like the last model BERT also uses vector space representations of natural language to learn and predict. Lessons Learned: This approach validates our hypothesis for this kind of problem we need a model that better grasps the context of the sentence, rather than just learning from the word sentiment. BERT proved to be a significantly better classifier than the other three, due to its bidirectional training. But still, not good enough to be used in a real application. Current Result: On our own dataset Accuracy: 0.594 Precision: 0.568 Recall: 0.630 F1 score: 0.599 On IBC Dataset: Accuracy: 0.598 Precision: 0.558 Recall: 0.568 F1 score: 0.563 Video Demonstration Category + Sentiment Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing SVM Model Demo https://drive.google.com/file/d/1rGzV26i5q7GmMNR9MwLthTDqPrfe5H-4/view?usp=sharing Conclusion: We learned that getting a correctly labeled dataset is extremely important when training any ML model. Inaccuracy, duplication of records, ambiguity and bias can all negatively affect the outcomes. We also learned that how you create the labels are important. We chose three labels to categorize headlines but there were five or more separate clusters of word distributions that overlapped with each other. Discriminately classifying them and defining similarity was a significant challenge. The category/sentiment and SVM approach produced slightly lower accuracy than LSTM and BERT. Correctly identify the subject of the headline was difficult with shallow NLP which caused the sentiment to attach with the wrong subject and decreased the accuracy. One approach that we could have done is doing text categorization and sentimental analysis as two different steps of this project. This might be a future improvement that we can look into. Even though we didn't get to achieve the accuracy that we were hoping for, this project has brought much knowledge. It helps tie up all materials in class together from the way we do NLP, using TF-IDF to generative and classifier models like Naive Bayes, Logistic Regression, SVM, etc. It's like a perfect summary that we need to put everything we learnt in materials into practice. To conclude, we learned that bias detection is a harder text categorization problem than sentiment analysis because bias categories are weakly corelated to the surface features of the text such the sentiments of words in a sentence are not enough to predict bias. This hypothesis is validated by the result from BERT model. Since BERT learns contextual relationship between words it has performed better, and the accuracy has increased. Probably more fine-tuning can help it improve more. The other bottleneck includes amount of training data. Since we couldn't find the dataset, we were looking for we had to hand label it our self. Which bring us to another problem, we later found instances where our labeling was not accurate. So better training data set in terms of amount of data and better labeling would help improve bias detection as well. Category + Sentiment SVM LSTM BERT Overall Precision 0.512 0.530 0.507 0.568 Overall Recall 0.601 0.514 0.558 0.630 Overall F1 0.550 0.475 0.531 0.599 Overall Accuracy 0.521 0.514 0.528 0.594 References Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014. https://www.tensorflow.org/tutorials/text/classify_text_with_bert https://www.tensorflow.org/tutorials/text/text_classification_rnn https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and- implement-text-classification-in-python/ https://aws.amazon.com/sagemaker/groundtruth/ https://spacy.io/usage/spacy-101"
https://github.com/bachman5/CS410-BiasDetector	CS410-BiasDetector Progress Report.pdf	"CS410 Course Project Progress Report US News Political Bias Detector Team Name: Bias Detectives Names: Anh Nguyen, Muhammad Rafay, Nicholas Bachman NetID's: anhn4@illinois.edu, mrafay2@illinois.edu, bachman5@illinois.edu Team Captain: Nicholas Bachman Free Topic: Text Classification and Sentiment Analysis Public Repository: https://github.com/bachman5/CS410-BiasDetector Project Timeline: Milestone Due Date Submit Proposal Oct 25th - Complete Working Prototypes Nov 22th - Complete Code Video Demo Nov 28th - Complete Project Progress Report Submission Nov 29th - Complete 2 x Initial Peer Reviews Dec 2nd Project Completion and Submission Dec 13th 2 x Final Peer Reviews Dec 16th Workload Justification: N = 3 team members 3 * 20 = 60 hours Task Estimated Hours Build Text Mining / Clean News Headlines 4 hours - Complete Collect and Label Test / Training Datasets (Corpus) 10 hours - Complete Build / Tune Sentiment Analysis Technique 12 hours - Complete Build / Train / Tune Logistic & SVM Model 12 hours - Complete Build / Train / Tune Deep Learning Technique 14 hours - In progress Visualization to display results (Tableau or Website) ? Develop Software Documentation 2 Team Meetings 5 hours Create Video Demonstrations 4 hours Total Project Motivation Collect and Label Datasets (Corpus) Raw Dataset: https://www.kaggle.com/snapcrack/all-the-news Description: Our Corpus was taken from ~200,000 News Articles published in the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The bulk of headlines were taken from 2015-2017 during a heated US presidential election. Data Model Figure 1: Data collection, filtering and labeling workflow AWS SageMaker With AWS SageMaker, large labeling jobs can be broken up and assigned to public or private workforces. SageMaker increased the speed and accuracy of our labeling process. The jobs were broken up and assigned across all three team members. Each team member then recruited politically independent 3rd party members to assist with the large labeling task. We specifically tried to get labeling assistance from friends and family members that don't have strong political ties to either the Republican or Democratic ideologies. If the labeler has a strong political bias, then the resulting models could also reflect that bias. AWS GroundTruth AWS GroundTruth is the portal that private team members used to label our data. A simple dashboard was created for the labeler to view a single news headline at a time with no additional information like publisher that could introduce additional bias. The GroundTruth user had options to label the headline as either having Right Bias, Left Bias or Neutral. Final Labeled Test / Training Dataset: https://github.com/bachman5/CS410- BiasDetector/tree/main/data_labeled Description: ~4,000 filtered, cleaned and labeled headlines were created from the workflow. News headlines were labeled as either Right Wing bias, Left Wing Political bias or Neutral using AWS Sagemaker and GroundTruth. Here is an example record correctly labeled with Left Wing bias: Category + Sentiment Analysis Approach Language: Python Dependencies: nltk, nltk.vader, numpy, pandas Public Repository: https://github.com/bachman5/CS410-BiasDetector/blob/main/sentiment_test.py Primary Team Member: Nick Bachman Overview: VADER (Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. It is included in the NLTK package and can be applied directly to unlabeled text data. It can also be customized for specific domains and use cases. Our first approach was to see if we could design a system that combines topic category and sentiment to accurately predict political bias. Technical Approach: The goal of this model is to correctly determine if a text headline includes Left or Right political bias. In the headline ""Daily News mourns the death of the Republican Party, killed by epidemic of Trump"", the subject of the news article is the Republican party. The Reuters writer Lucas Jackson used the terms mourn, death, killed and epidemic to communicate significantly negative sentiment about the Republican Party and Donald Trump. Most people would agree that this headline supports a Left-Wing political position and thus includes a Left bias. {""source"":""69446,""Daily News mourns the death of the Republican Party, killed by epidemic of Trump'"",""class-name"":""Left""} As a group, we spent time thinking about the definition of political bias and how to design a labeling and detection system. The chart below summarizes our design decision. Sentiment alone is not enough to determine political bias. You also need a way to determine the category that sentiment is being directed toward. Topic Category Sentiment Bias Republican Negative :( Left Republican Positive :) Right Republican Neutral :| Neutral Democrat Negative :( Right Democrat Positive :) Left Democrat Neutral :| Neutral Table 1: Combining Category and Sentiment to determine Bias Figure 2: Combining Category and Sentiment to determine Bias Technical Challenges: Updating the category lexicon with descriptive terms and updating the VADER sentiment lexicon with popular political terms increased the accuracy by about 8-10%. Tunning the values for how much negative or positive sentiment yields a neutral response also helped because ""Neutral"" was the label with the most entropy. For example: ""Game of Thrones: Republicans hate it, Democrats love it"". If the reader likes the HBO show Game of Thrones, it changes how they will label it. Both Republicans and Democrats are mentioned so there are multiple subject categories. Both Negative (hate) and Positive (Love) sentiment are expressed. The labeler may choose Neutral because they are unsure or do not know what Game of Thrones is in relation to this political topic. Complex sentiment and ambiguity make it difficult to accurately label bias and accurately predict bias. Current Results: Test Dataset: biasdetective1.csv Classification Accuracy: 44% Test Dataset: biasdetective2.csv Classification Accuracy: 42% Test Dataset: biasdetective3.csv Classification Accuracy: 47% More advanced supervised and deep learning models discussed later in the report yield more accurate results. -------------------------------------------- Support Vector Machine Approach Language: Python Dependencies: numpy, panda, glove, spacy, sklearn Primary Team Member: Anh Nguyen Overview: Support Vector Machine (SVM) is a Discriminative Classification introduced in one of our lectures in Text Categorization. SVM uses classification algo to separate classes of data point then find the maximum margin of hyperplane between data points in their associated classes. The method below also includes some NLP methods + Vector Embedding to improve dataset and support model's accuracy Technical Approach: Follow by diagram below Overall Diagram 1. Prepare Data (NLP): (Using Spacy) Goal of this is to conduct a more meaningful patterns and themes for the text data using spacy. The main method for this data processing is tokenization was breaking news titles into token, clean sentence better by ignoring punctuation like marks and spaces. Before and after Word Tokenized Another method that I have tried is removing stopwords from sentences. However, it brings me to uncompleted sentences that are difficult to translate and understand. Before and after Removing stopwords OutPut: X : word in titles, Y : labels 2. Word embedding & Magnitude (Using Glove) Building Corpus After preprocessing data, I have built three different text files: train.txt, text.txt and corpus.txt. Ideally Corpus should be a dictionary including all political words in bigger spectrum than only words in train & text data, however due to the limitation of the data our Corpus only built upon our available scope of data. Words in Corpus then mapped to corresponding vectors (Word Embedding/word Vectorization) with the hope of capturing the meaning of potential relationship of word in term of similar contexts/syntax/spelling/co-occurrence. Corpus (Dictionary) with vectorized words Testing word's distance: Result: Another method that I'm using in this is store our corpus to a magnitude file. Normally a corpus can be text- formatted but storing dictionary in magnitude file helps improving the processing time. Vectorizing titles Using MeanEmbeddingVectorizer & TF-IDF Embedding 3. Train/Tune Model (Using sklearn) Current Result TF-IDF for preprocess + SVM classification Accuracy : 0.514379622021364 Precision: 0.5306949522525308 Recall : 0.514379622021364 F1 score : 0.4751926328128259 Mean Embedding Vector preprocess + SVM classification Accuracy : 0.5308134757600658 Precision: 0.4542249353200501 Recall : 0.5308134757600658 F1 score : 0.48172315509176467 The current accuracy is still in range 50% with TF-IDF embedding preprocessing as well as Vector preprocessing. While there is not much success in improving the accuracy levels. I started to doubt the truthfulness of our labelled data. To assert the validity of our model pipeline, I switched to benchmarking using a different dataset while keeping everything else the same. The dataset is ATIS Airline Spoken Language Intent (train / test) which classifies a passenger's inquiry into 1 of the possible 18 intents: flight, flight time, meal, etc. The data size is about 5000 records and 18 classes (intents). The performance came out significantly better than what observed from our dataset: Accuracy : 0.9113495200451722 Precision: 0.9107883118388134 Recall : 0.9113495200451722 F1 score : 0.9011684778567066 Challenges: NLP: Still facing challenging with precise deep Semantic Analysis & Bias in Labelled Data: -------------------------------------------- Deep Learning: Recurrent Neural Network Classification Approach Language: Python Dependencies: TensorFlow Primary Team Member: Rafay, Muhammad Overview: The approach uses a recurrent neural network (RNN) to classify news headlines as Left or Right biased. Technical Approach: Unlike the feed forward neural network we used an RNN for classifying news headlines. The difference in RNN verses feed forward is that output of an RNN is not just influenced by the input we just fed but it is influenced by the history of inputs we have fed earlier. An example is given below: So, it means RNN is suitable for classifying sequence data and news headline is a sequence. Since, neural network processes numbers we had to convert text to a sequence of token indices using an encoder. After the encoding is the embedding layer, this layer stores one vector per word after training words with similar meanings have similar vectors. Next comes the RNN with a stack of hidden layers, it processes the sequence of input by iterating through the elements. At each step output from the previous step is passed with the input of the next step. The sequence is converted to a single vector which is further converted to a single logit as the classification output. The architecture of our RNN is similar to the one given below: Current Results: Our current classification accuracy is 60% with the vanilla RNN and improved to 62% when we added two LSTM (Long Short-Term Memory) layers. Although the accuracy is not good enough for practical usage but this is the best, we have got among the models we tried. Challenges: Since we are classifying based on political bias and not sentiments, the two classes: Left and Right has a lot of common vocabulary. Both text of both classes contains hate speech and words ""democrats"" and ""republican"" so in short, the vocabulary is not a very distinguishing feature for the two classes. The sentiment of bias is sometimes hidden behind the meaning of the sentence and it is not obvious e.g. ""Are Republicans more likely to prefer pulp in their orange juice?"" It is only obvious when something positive or negative is being said about one of the parties, and that involves recognizing the sentiment of the entity but not every headline is like that e.g. ""Republicans' patience with Trump may be running out"" So, we need to involve feature engineering that would help the model learn the hidden meaning behind the headlines. Video Demonstration Rules + Sentiment Model Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing SVM Model Demo https://drive.google.com/file/d/16FyQPFDD6CV-WdYkEM-gWNPKaQADNHBd/view?usp=sharing Instruction for Testing SVM: $make install - installing all necessary packages in local dir $make data - build corpus $make train - train model $make test - test result Remaining Tasks In the next two weeks, we will continue to improve our accuracy. We look forward to use the BERT model a recent break thorough in machine learning. The BERT is a non- directional model instead of reading the words in a sequence it processes each token in context of all token before and after. Secondly, another bottle neck in improving accuracy is the accuracy of the data labeling. We suspect that some of the data has been incorrectly labeled as well that is confusing our classifier. Thirdly, we have about 4000 instances in our training data increasing the amount of training data might improve the results further. So, we are exploring new datasets to use with our model. References Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014. https://github.com/plasticityai/magnitude#file-format-and-converter"
https://github.com/bachman5/CS410-BiasDetector	README.md	"CourseProject CS 410 Group Project- Bias Detectives We have divided our work into three different technical approaches to determine which is the most accurate for our use case. If you would like to test the code, clone this repo and follow the instructions below. You can also download a video demo of Rules and SVM. Rules_Model Video Demo https://drive.google.com/file/d/1ycSjyZAlT915zT_RqlEKu-lhGaig8hyr/view?usp=sharing Instructions for testing Rules_Model: $pip3 install nltk $pip3 install numpy $pip3 install pandas $cd Rules_Model $python3 sentiment_test.py SVM_Model Video Demo https://drive.google.com/file/d/16FyQPFDD6CV-WdYkEM-gWNPKaQADNHBd/view?usp=sharing Instruction for testing SVM_Model: $make install - installing all necessary packages in local dir $make data - build corpus $make train - train model $make test - test result Deep_Learning_Models Video Demo - https://drive.google.com/file/d/1TRKzGCTPX2U6qpORR2SQ3iWYDhAv7TuA/view Instruction for testing Deep_Learning_Models: Install Jupyter Notebook: ""pip install notebook"" Go to notebook directory in terminal Install TensorFlow: ""pip install tensorflow==2.3"" Run ""jupyter notebook"""
https://github.com/rohankk2/Twitter-Recommendations-based-on-text	MicroForce- Twitter Recommendations based on text.pdf	Twitter Recommendations based on text Create suggestions on potential people/pages/ads a user would like to follow based on their tweet/post content. 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rohan Khanna - rohank2 (Team Captain) Cesia Bulnes - cbulnes2 Tyler Wong - tylercw3 2. What is your free topic? Please give a detailed description. Our free topic is a recommendation algorithm that based on the text contents of tweets makes appropriate recommendations to the user. The recommendations can range from other tweets similar to a user's tweet, advertisements or even possible pages to follow. Consider a user who frequently tweets about a soccer club then our algorithm should recommend other followers of that club or advertisements specific to that club's products. Another example is if I frequently post about the new gaming console I should get recommendations to follow sellers such as gamestop,amazon etc on twitter as they are likely to be talking about console sale dates. These recommendation are all based on the text of a tweet. You may ask, doesn't Twitter have that already? Twitter's recommendation system currently is one page, where they recommend multiple sets of people who may be of different categories. Currently: What we want: As you can see, the only relevant recommendation is the person who wrote it, or in many cases someone who was mentioned in the tweet. There is no recommendation of other people writing similar things. In the example below, you can see that Michelle obama has a similar tweet, therefore she would be one of the top results returned since the ML/NLP algorithm recommends this content with a high % of similarity. We can then show the top tweets which can generate follows to different types of celebrities/businesses/ads etc. Recommendation of Tweets: 3. What is the task? The task is to deliver working software code, software documentation, and a software usage presentation about our topic described above. The code will fulfil our primary use case so that as someone makes a tweet, a recommendation will show up based on that singular tweet. In addition, we want to be able to give a percentage of how much a tweet is related to another page, tweet, etc. 4. Why is it important or interesting? This project is important because we want to portray similarities between a person and other people around the world. It's a way to unite people based on one singular tweet, and for people to be aware that they are not alone in terms of a specific subject. We are focusing on social aspects this year because of controversial topics, such as politics(Trump/Biden), the Black lives matter and All lives matter movements, LGBTQ, children in ICE Detention centers and many more. We hope to examine a tweet that contains keywords, and recommend other personalities/pages to follow when that tweet is put in to grow a person's network. 5. What is your planned approach? We plan on taking an iterative approach throughout this software project so that we can quickly identify blockers and make consistent progress. With that being said, we will meet weekly to discuss our current progress and any blockers that we're experiencing. We will also be splitting up the work so that we can all work in parallel. In addition, we are consistently talking through in a group chat where we can quickly get feedback on an idea or a feature. We do have a defined due date, so we will use that date as a target to deliver a minimal viable product which will deliver the major functionality. We will add on more work to that minimal viable product if we underestimated our time or if we have more capacity than expected. 6. What tools, systems or datasets are involved? The first system that comes to mind is using Twitter Developer, since we could filter real time tweets, and cross examine them to make recommendations based on people tweeting similar content. I think most of the work needed to recommend would be coming from Twitter Developer's API's. We would also be using Pycharm to code in python to show our results through the terminal. We would use the nltk tool kits to remove stop words and appropriately tag the data. To store our test data and queries we will provision a non relational database which are available from major companies such as Microsoft,Mongo,Amazon etc. 7. What is the expected outcome? The expected outcome is for someone to make a tweet, and for them to have a recommendation immediately based on that singular tweet. Currently, Twitter has a recommendation page, but not recommendations based on tweets. People get passionate about topics and I feel like this feature would increase a person's usage of Twitter. 8. How are you going to evaluate your work? As discussed in class, evaluation of a text recognition system depends on its usefulness to the end users. We plan to create a google form where users can rate how useful the results were for a particular tweet. We would look to answer the following questions through the google form all on a scale of 0-10 : 1. How accurate are the results provided by our tool ? 2. Do the results relate to the category of your tweet ? 3. How likely are you to use this tool in a production environment ? 4. How satisfied are you with the speed of the system ? 5. Does the person/page recommended reflect your interests based on a tweet? 6. Would you use this feature if Twitter enabled it? 7. Do you think it is invasive or inappropriate? The next important criteria is the speed. Google delivers its search results in approximately 0.67 seconds. We aim at a minimum to have our results show within 2 seconds. Optimistically we will target results being shown within one second. 9. Which programming language do you plan to use? Python, JS, React 10. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Task Estimated Hours Set up Twitter API and get familiarized 15 (5 hours per teammate) Create Github repo and create base project components 2 hours Setting up Nonrelational DB 2 hours Create software usage tutorial presentation 3 hours Develop software documentation 12 (4 hours per teammate) Develop nlp code that can process tweets and categorize the contents 8 hours Based on the categories of the contents of the tweet scrape data from the twitter apis relevant to that content. 10 hours Program a ranking algorithm that gives : 1. Top users to follow, 2. Top posts the user may like 3. Top retail sites based on their tweet. (Ads) 24 hours Create google form for feedback evaluation 1 hour (Optional) create a web app to present all 10 hours results. Include setting up api and frontend Total 77 excluding optional
https://github.com/rohankk2/Twitter-Recommendations-based-on-text	Project Progress Report Submission for MicroForce.pdf	"Dev Design Doc for MicroForce - Twitter Tweet Recommendations based on Topics Tyler Wong - - tylercw3@illinois.edu Rohan Khanna - rohank2@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu Introduction on Topic: We are currently looking at topic modeling tweets and recommending the tweets or users that are most similar in a given tweet. We are looking at using a ranking algorithm such as Okapi BM25 learned in class for ranking tweets by ranking the most similar tweets given a tweet by a user, the category of the tweets that it is related to, the top ranked users in said category, and the top categories/topics in a given sample set. For example, had we borrowed tweets from November 6th,2020, the top topic would have been politics because of the USA elections. Versus November 26 probably having TheGrammys nominations as one of the most trending topics. It will be interesting to recommend other similar tweets/users to a category presented by a single query. We will be ranking and observing the results with a sample set of 2000 users for the time in this project, and observing the past 7 days of tweets by each said user. Currently: By the end of the week, we hope to finish the database setup. We decided to use SQLite to avoid any errors on setting up our computers. Simplifying the database will allow us to work on the algorithms needed in the last two weeks of the class. In addition, we're also working on gathering all the necessary data we need from the Twitter API and importing that data into the database. We have all created twitter developer API and have started planning what we each need to do come the last two weeks of class. We plan on using multiple different entities that the Twitter API provides, such as user data and Tweet annotations and entities. Concerns: As of now the concern is having a good ranking model. We want to make sure that the ranking model is correct in terms of how close tweets are to each other. It's imperative that our team take some time to validate returned ranks to make sure there are no discrepancies. For example a query that is exactly like a tweet in our database should have such a tweet ranked as 1 versus one that is highly similar but not exactly the same. Work to do: 1. Ranking algorithm to rank the query or tweet that the user has typed, displaying the ranked tweets in commonality. a. Going off of the query, or tweet, we want to be able to display common tweets from the pool of users and their respective tweets in the past 7 days. Say my query is ""I love seafood"". The ranking will be conducted among our pool of tweets and we would show the top tweets that may have similarity in the topic/category. 2. Get 2000 users, and their last 7 days of tweets a. For the purpose of the project we want to go ahead and observe ranking in a small-ish pool of people and tweets. I think it would be informative to observe the ranking when using a pool of 1000 users versus a pool of 1000 users for example. The more tweets are available, the more we should see a higher commonality between the tweets given back from our ranking. 3. Process data a. We should process the data of tweets that we receive to eliminate words or other elements in the tweet that are not helpful in terms of ranking the tweet for our query. b. Source or guidance for processing with python: https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d52 9e 4. We should get information from 2000 users and their past 7 days of tweets into the following tables: a. original_ tweets table: i. We want to obtain the information about a tweet, who wrote it, the time posted, the content type etc. This will be stored in the original_tweets table. b. processed_tweets table: i. We want to then process the tweet text and content in order to simplify our ranking model. c. users table: i. We want to create a users table so that we can see the correlated tweets per user. We can do this multiple ways: tie the tweet id's in an array that correspond to the user. We want to also include the location of the user, number of followers, etc of what's included in the user-object https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/overvie w/user-object d. user_category table: i. What kind of topics does this user rank mostly in? We should aggregate counts of topics that their previous seven days of tweets include in. This would be useful to observe at the end in our conclusion. If a user is most likely to post about politics, it would be interesting to see if they rank among the higher ranked tweets about politics in our ranking algorithm. e. tweet_category table i. Last but not least, we want to have a categories table where we can map the category, we can also create a list of tweets belonging in this table 5. Build an inverted index a. We should create an inverted index for every word presented in a tweet to further allow the ranking algorithm to rank a tweet accordingly. 6. Grab the top X users(on ranking BM25) that show up in terms of different categories. a. Use all of the categories (display invalid if not in the categories available) i. We want to be able to use all of the categories available in order to do ranking with the tweets related. If a query is present where it doesn't fit in with any of the categories, we shall show ""no category fits this tweet"" ii. Users being the top performing tweets for a given category 1. We want to also display top performing users for a given category. This can be done by collecting the overall topics this user tweeted about and presenting the ones with higher frequency under their id. iii. Top categories in sample set 1. Depending on the content that is aggregated in the past 7 days it would be informative to correlate these tweets with ongoing present news. Would be good to see if the ranking is similar to most searched on twitter charts and trending statistics. 7. Display top X on the terminal/UI. a. We will have an interactive terminal asking the user what they would like to display in terms of ranking as show in number 6."
https://github.com/rohankk2/Twitter-Recommendations-based-on-text	README.md	"Twitter Recommendations Based on Text Team Members: Rohan Khanna (Team Captain) - rohank2@illinois.edu Tyler Wong - tylercw3@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu For full documentation please read our pdf report uploaded in this repo: https://github.com/rohankk2/Twitter-Recommendations-based-on-text/blob/main/Twitter%20Recommendations%20based%20on%20Text%20Final%20Report.pdf Introduction Twitter is a social media platform allowing users to connect and share thoughts and information. A notable example of Twitter and content shared was seen in 2020, with the presidency and election. There are currently recommendations on who to follow in general for Twitter users. However, when a tweet is made, that tweet does not have suggested/similar tweets that a user can react to or retweet. The purpose of this project is to give users the ability to get recommendations based on their tweets. Suppose you write ""love hamburgers and fries"", you should expect to get back a topic, and if you run that same query with our ranker, you will get a list of 10 tweets that are closest in similarity, along with the 10 users that have tweets that are closest to the content of this query. Obtaining Twitter Data You will need Python installed and all the required packages installed. pip install -r requirements.txt Obtain a Twitter developer account through the Twitter Developer Portal if you haven't already. Add the consumer key, consumer secret, access token, access token secret, and bearer token to the ""twitter_utils.py"" main method's variables. These will be used to interact with the Twitter API. Run the code to get the users and tweets. The code will check to make sure not to regenerate these, so if you run it multiple times you will need to delete the files in the ""data"" directory of the repository. python src/twitter_utils.py Data Extraction For topic extraction we use the nltk toolkits and gensim. As we learnt in class LDA is an unsupervised machine learning algorithm that uses a mixture model to generate documents. Each topic can be assigned some prior probability and each topic consists of probabilities for generating a particular word from the vocabulary. Data Retrieval/Cleaning: We developed a few different functions to perform data retrieval and cleaning of the tweets: remove_emoji(text) remove_links(text) remove_users(text) clean_tweet(tweet) All of these functions are meant to clean up the data so that we can perform better analysis of the data with better accuracy. We process all of our original tweets from the SQLite database through these functions. The remove_emoji(text)method removes any emojis that are found in the tweet because we found that emojis didn't provide much meaningful information. The remove_links(text) method removes any HTTP or HTTPs links that are found in the tweet text since that data isn't useful when determining category. The remove_users(text)method removes any ""@"" mentions for any other user. The clean_tweet(tweet)method makes the tweet all lowercase, removes punctuation, removes any stopwords, and removes any words that 2 characters or less. How to run Topic Extraction: To run the code a user can put test tweets in the test tweets.json file and just execute the python3.8 topicdeterminant.py this will classify the tweet into the most likely topic. Using this topic modelling you can draw a relationship between people who have similar tweets. Execute the following command on the terminal python3.8 topicdeterminant.py Ranker of tweets and users After loading the tweet data and creating a map of the author id's to the author's screen name, the tweets are then using tf-idf weights per word to score shared words. Given the following tweet_query ""heat is cranking"" I want to return a recommendation of tweets that are similar to the tweet_query, along with the % of their similarity. 1) To begin with, once git cloned, go to data and unzip tweets.tar.gz, this will unzip tweets.json. tar -xzvf tweets.tar.gz 2) You can choose a query to substitute into tweet_query in the ranker.py code. You may replace the writing within this query to obtain similarities with different queries. 3) To execute simply write the following in the terminal: python ranker.py Sources https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ https://github.com/enoreese/topic-modelling/blob/master/preprocessor.py https://github.com/4OH4/doc-similarity/blob/master/examples.ipynb"
https://github.com/rohankk2/Twitter-Recommendations-based-on-text	Twitter Recommendations based on Text Final Report.pdf	"Twitter Recommendations based on Text Report Team Members: Rohan Khanna (Team Captain) - rohank2@illinois.edu Tyler Wong - tylercw3@illinois.edu Cesia Bulnes - cbulnes2@illinois.edu Introduction Twitter is a social media platform allowing users to connect and share thoughts and information. A notable example of Twitter and content shared was seen in 2020, with the presidency and election. There are currently recommendations on who to follow in general for Twitter users. However, when a tweet is made, that tweet does not have suggested/similar tweets that a user can react to or retweet. The purpose of this project is to give users the ability to get recommendations based on their tweets. Suppose you write ""love hamburgers and fries"", you should expect to get back a topic, and if you run that same query with our ranker, you will get a list of 10 tweets that are closest in similarity, along with the 10 users that have tweets that are closest to the content of this query. We would also get the topic of this tweet for classification purposes. Currently on twitter: What this project accomplishes: The following tweet will have a .89% of similarity. Twitter Data In order to get data from Twitter, we had to get approved for a developer key from Twitter. This is an application that we submitted which outlined our use case and was promptly approved. Even though we now have this API key, Twitter still limits your use of the platform through API. Most notably, they limit how much request you can do in one hour and they limit how much data you can get per request. Because of these request limits from the Twitter API, we decided to use an open source Python library to handle requesting data from Twitter's v1.1 API called python-twitter. For querying data from Twitter's v2 API, we directly used Twitter's HTTP API since it's so new, there aren't many open source tools to use. To get the users we decided to get 3000 random active Twitter users. In order to do this, we queried Twitter's sample stream which provides a subset of active tweets coming in as a stream. Once we obtained a tweet from the stream, we queried the language of the tweet and to check if the user has a public profile or not. If they were english speaking and had a public profile, we obtained their user information. Once we have our random users, we got the last 7 days of tweets from them. We did this by querying for the tweet IDs and then getting each associated tweet from the v2 API. This was a very expensive operation since twitter limits how many tweets you can get, but given enough time we were able to get all this data. We ended up with around 120,000 tweets as our full data set which is included in the ""tweets.tar.gz"" file in the ""data"" folder of our GitHub repository. Example of a single Tweet data with annotations and entities How it works 1) You will need Python installed and all the required packages installed. pip install -r requirements.txt 2) Obtain a Twitter developer account through the Twitter Developer Portal if you haven't already. Add the consumer key, consumer secret, access token, access token secret, and bearer token to the ""twitter_utils.py"" main method's variables. These will be used to interact with the Twitter API. 3) Run the code to get the users and tweets. The code will check to make sure not to regenerate these, so if you run it multiple times you will need to delete the files in the ""data"" directory of the repository. python src/twitter_utils.py Database SQLite was used to extract the tweets and user information from the Twitter API's response. We used the Standard v1.1 API to extract information on the users. This was mainly important to later map the user id's to the screen name, location, etc. In the future, this information can be used to build recommendations of tweets based on the location of a user. Say a user lives in Miami, FL, and they tweet about an upcoming event with a celebrity. There can be a recommendation of tweets with that same geographical location. Not only could Twitter use this for marketing, but also increasing the connectivity that other social media platforms like Facebook have taken as an approach. In addition to the information on the users, we also have the tweet data from the early access Twitter v2 API. This version of the API was used because it contains the entity and context information that provides data on the subject and relations of the tweet. Some examples of entities are: Barack Obama, IBM, Mountain Dew, and San Francisco. Some examples of context are: TV Shows, TV Episodes, Podcast, Holiday, Politicians, and Video Game. SQLite stored this information in the following tables: userTable and originalTweets Topic Extraction For topic extraction we use the nltk toolkits and gensim. As we learnt in class LDA is an unsupervised machine learning algorithm that uses a mixture model to generate documents. Each topic can be assigned some prior probability and each topic consists of probabilities for generating a particular word from the vocabulary. DATA RETRIEVAL/CLEANING: We developed a few different functions to perform data retrieval and cleaning of the tweets: * remove_emoji(text) * remove_links(text) * remove_users(text) * clean_tweet(tweet) All of these functions are meant to clean up the data so that we can perform better analysis of the data with better accuracy. We process all of our original tweets from the SQLite database through these functions. The remove_emoji(text)method removes any emojis that are found in the tweet because we found that emojis didn't provide much meaningful information. The remove_links(text) method removes any HTTP or HTTPs links that are found in the tweet text since that data isn't useful when determining category. The remove_users(text)method removes any ""@"" mentions for any other user. The clean_tweet(tweet)method makes the tweet all lowercase, removes punctuation, removes any stopwords, and removes any words that are 2 characters or less. CORE ALGORITHM: Figure:1 Picture reference: https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 We decided to go with 5 topics as this was a proof of concept and our test data was relatively small and could be described with 5 topics. After cleaning our data, we essentially create a document term matrix where the rows are the cleaned tweets and the columns are words. The matrix entries hold the count of those words in that particular tweet. For example matrix[i][j] will denote the count of that word in the tweet. This is essentially following the core concept of creating a bag of words model for our project. We then send this document count into the gensim lda model and it runs the LDA model to generate the topics. We run the lda with 20 passes for convergence and we decided to use gensim because it gives us the ability to store this model on the disk and hence reuse it for later applications. Moreover, we use the multicore lda as it can process the data parallely on multiple threads leading to better overall performance. After this for a new tweet we use this lda model and try to assign probabilities for which topic this new tweet could belong to. As shown in the demo video, we ran our topic classifier on the test tweet : Twitch is so cool! #twitch #twitchstreamer #digital Our classifier did a good job of classifying this to topic 4 which was : Topic: 4 Words: 0.020*""marketing"" + 0.013*""#digital"" + 0.008*""media"" + 0.007*""#youtube"" + 0.007*""#twitchde"" + 0.007*""#germanmediart"" + 0.007*""#twitch"" + 0.007*""#seo"" + 0.007*""#email"" + 0.007*""marketer"" Overall, this was a good approach as a proof of concept however, we definitely have a lot of room to develop on this project. How to run it: To run the code a user can put test tweets in the test tweets.json file and just execute the python3.8 topicdeterminant.py this will classify the tweet into the most likely topic. Using this topic modelling you can draw a relationship between people who have similar tweets. Execute python3.8 topicdeterminant.py on the terminal IMPROVEMENTS: 1. We could have worked more on cleaning the data to account for duplicate tweets in the form of retweets. Moreover, a lot of our accuracy issues were centered around bad data. We had tweets like : oo I know what I\u2019m going to do. These tweets are extremely hard to classify into any particular topic. There is no broad common topic that the above tweet could be classified into and hence such a tweet just corrupts our training data. 2. We could have worked to put a special emphasis on features of tweets like hashtags. Sources: 1. https://medium.com/@osas.usen/topic-extraction-from-tweets-using-lda-a997e4eb0985 2. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ 3. https://github.com/enoreese/topic-modelling/blob/master/preprocessor.py Ranker of tweets and users After loading the tweet data and creating a map of the author id's to the author's screen name, the tweets are then using tf-idf weights per word to score shared words. Given the following tweet_query ""heat is cranking"" I want to return a recommendation of tweets that are similar to the tweet_query, along with the % of their similarity. We introduced a reduce_by_lemma function to reduce words to their lemma. Stop words from english nltk are the following: {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}. We also lemma the stop words with stop_lemma. We then initialize TF-IDF Vector with the Lemma function under vectorize. The next thing to do here is create a vector between the tweet_query that we are looking for with the tweet data we collected. These vectors will be examined by creator cosine similarities between the vectors. Then we go through the flattened vectors to obtain the scores by the vectors. For score_titles, we want to get the score per tweet. Here we view tweet_data as the title since it's hard to give a title to one tweet. After, we print the authors associated with the recommended tweets based on similarity. These authors are printed for the user to know which authors may produce similar content as them. How it works 1) To begin with, once git cloned, go to data and unzip tweets.tar.gz, this will unzip tweets.json. tar -xzvf tweets.tar.gz 2) Like shown above, you can see that as a user, you can choose a query to substitute into tweet_query in the ranker.py code. You may replace the writing within this query to obtain similarities with different queries. 3) To execute simply write the following in the terminal: python ranker.py Depending on your python version, you may have to pip install some nltk libraries. You would get a message saying whether you have to pip install nltk for example. Follow the prompt in the terminal and that should be resolved. Upon executing this you will receive the following output: Like discussed above, the results result in the number of tweets, a sample of five tweets from the total number of tweets, the top 10 tweets with similarity of content to the original tweet_query, and the top 10 users who made the tweets with most similarity. For testers: edit the query to anything random that you may think of as a tweet. Run the ranker.py to get recommendations on similar tweets. Improvements: The performance of the ranker was where we missed the mark. We could have optimized the code to perform better. We originally wanted results in 2 seconds, but when doing a lemma, the results take longer than without the lemma. With a lemma it takes approximately 30 seconds longer to obtain the results. Sources: This work was done by following the following github tutorial which explained using TF-IDF with cosine similarities. https://github.com/4OH4/doc-similarity/blob/master/examples.ipynb Contributions of Team Members Team Member Hours Worked Contributions to Project Rohan Khanna (Team Captain) 20 hours 1. Performed partial cleaning of the tweets data, ie, removing emoticons, punctuations etc. 2. Wrote the code for the main lda algorithm used to classify a tweet into a topic 3. Worked on software documentation and software usage presentation. Tyler Wong 20 hours 1. Set up Twitter API for project and familiarized myself with both versions of the Twitter API. 2. Gathered 3000 active random Twitter users and their last 7 days of claned categorized tweets using the Twitter API. 3. Worked on software documentation and software usage presentation. Cesia Bulnes 20 hours 1. Set up the skeleton of the database 2. Worked on the ranking of tweet content, to recommend top tweets a user would like based on their query/tweet, and returned the top users 3. Worked on software documentation and software usage presentation"
https://github.com/nadiam2/CourseProject	CS 410 Project Documentation .pdf	"Final Project Documentation Project Contributors: Nadia Mohiuddin (nadiam2), Pallavi Narayanan (pallavi3), Thaniel Tong (tztong2) Overall Use: The purpose of this project is to classify tweets into categories SARCASM or NOT_SARCASM. The tweets that we categorize are written in response to other tweets, which are the context data. This is a form of sentiment analysis. How to Get the Code: How to Run the Code: Software Installation Details: * To install each Python library to use in our project, we ran ""pip install <library name>"" in our terminal before using the library in our actual code. * We then used regular import statements to use the libraries in our code. Model Used: * We used the SVM model from the sklearn.svm library. All the training was done through SVM. We did some preprocessing of the data in order to improve the performance. * First we tokenized all the words in the given tweets, then we removed stopwords, ""@USER"" and ""<URL>"" tags in the tweet. * Then we lemmatized the data so we could get the root word. * Finally to end the preprocessing, we converted emojis into strings which helped to increase the accuracy of classification, as emojis can often be a signal of sentiment. * In order to train on context data we appended the context data to the response data to create a more robust text data to train on. * We used TF and IDF weighting to convert the words to float probabilities, giving importance to frequent terms in a document, and infrequent terms across the entire collection. * We fine tuned the hyperparameters for SVC using the RandomSearchCV algorithm from sklearn. * We then used these hyperparameters to train our SVC model with the training TF-IDF scores, and then predicted the test data labels using the testing TF-IDF scores. * Lastly, we output the test labels to a text file. What We Tried: * We also tried to preprocess the responses and contexts by stemming each word and making each word lowercase. However, these methods did not give us the optimal F1 score. * We attempted to use other sklearn classifiers such as GaussianNB, MultinomialNB, and git clone <repository url> cd data python Sarcasm.py RandomForestClassifier, but none of these classifiers performed as well as SVC. * We tried to use the isalpha() function to remove all non alphabetic characters from the responses and contexts, but this also did not give us the optimal F1 score. * We tried using the default parameters for SVC, but those parameters did not perform as well as the hyperparameters we fine tuned using RandomSearchCV. * We also tried to fine tune parameters with GaussianNB, MultinomialNB, and RandomForestClassifier. We used GridSearch to find tune hyper parameters in Naive Bayes and a large multi-nested for-loop (mimicking GridSearch) for the RandomForestClassifier. However these still did not perform as well as SVC did. Final Data: Precision: 0.6468010517090271 Recall: 0.82 F1 score: 0.7231749142577168 Contributions: * The team started off working together, sharing ideas while one person typed with their screen shared to the other team members. * Later on, the team members split up to try different classifiers and methods of classifying the dataset. This approach was taken to save time as three computers train and test in parallel rather than waiting on one computer to run. Software Documentation/Tutorials: * Sklearn SVC * https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html * Sklearn RandomSearchCV Algorithm * https://intellipaat.com/community/18009/what-is-a-good-range-of-values-for-the-svm-svc -hyperparameters-to-be-explored-via-gridsearchcv * https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSea rchCV.html * Lemmatization * https://www.geeksforgeeks.org/python-lemmatization-with-nltk/ * Sklearn TfidfVectorizer * https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVect orizer.html * Pypi emoji * https://pypi.org/project/emoji/ How To Run the Project https://www.youtube.com/watch?v=z0zpcLdSXK4&feature=youtu.be&fbclid=IwAR0OEihoSxD9j94N_ Tx-AWhIP_ahQdAxT2Q-7moEabv1mOwIFmTX9URFQNg"
https://github.com/nadiam2/CourseProject	Project Progress Report.pdf	Project Progress Report 1) Progress Made: We parsed our json files/inputs, tokenized the inputs, stemmed the inputs, and tried out different methods to train on the train data and label the test data. 2) Remaining Tasks: We have to figure out how to use BERT to properly label the test data given contexts and responses. 3) Any challenges/issues faced: We realized that the Machine Learning model and library we were using was not suited to detect the sarcasm in the text data we had. We also found that we need to incorporate the context data which we were not able to with our original choice of sci-kit learn.
https://github.com/nadiam2/CourseProject	Project Proposal.pdf	Topic: Text Classification Competition In your project proposal, please answer the following questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Nadia Mohiuddin (nadiam2), Thaniel Tong (tztong2), Pallavi Narayanan (pallavi3) 2. Which competition do you plan to join? Text Classification Competition (Tweet Sarcasm Detection) 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. We are prepared to learn the most recent, state of the art network classifiers. Neural network classifiers we know are AlexNet, GoogLeNet, AlphaGo, and Deep Blue. Some deep learning frameworks we have heard of are Pytorch, Caffe, and TensorFlow. These are some resources to help train and create deep neural networks. We do not have any prior experience using such methods/classifiers. 4. Which programming language do you plan to use? Python What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Which competition do you plan to join? If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Which programming language do you plan to use?
https://github.com/nadiam2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/Finkoy/CourseProject	CS 410 Progress Report.pdf	CS 410 Progress Report 1. Which tasks have been completed? I have completed the task of preprocessing the data. I have also completed the task of implementing different models for the Text Classification Competition. I have tried using SVMCNN, CNN, and LSTM. I have also experimented a bit with logistic regression, RBF kernel SVM, and different combinations of those models. The baseline score has not been beat yet. 2. Which tasks are pending? Currently I am trying to fine-tune a BERT model on my preprocessed data. We will see if this beats the baseline or not. 3. Are you facing any challenges? Only with beating the baseline. The best F1 score achieved so far came from feature selection from a CNN trained on context data and TFIDF vectorization of the response data with LASSO to determine useful words. These features were combined and fed into a linear SVM.
https://github.com/Finkoy/CourseProject	proposal.pdf	CS410 Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. This team only contains one person, Brian Vien. The NetID is bvien2 and by default, he is the captain. 2. Which competition do you plan to join? I plan on joining the text classification competition. 3. If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods I am prepared to learn state-of-the-art neural network classifiers. I have multilayer perceptron, recurrent neural networks, and convolutional neural networks. I have heard of TensorFlow, Keras, and PyTorch. I have a little bit of prior experience with multilayer perceptron from coursework and have seen demos of TensorFlow. I hope to learn more through this competition. 4. Which programming language do you plan to use? I am planning on using Python3.8.
https://github.com/Finkoy/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/surajbisht1809/CourseProject	AmazingS3_Progress_Report_Nov29_2020.pdf	Team AmazingS3 Progress Report * Our team is Amazing S3 * Following are the team members o Sithira S Serasinghe sithira2@illinois.edu o Santosh Kore kore3@illinois.edu o Suraj Bisht surajb2@illinois.edu (Team Leader) * Project: Text classification competition Task completed: Team tried various models i.e. LSTM, sequence, RNN, Spacy, Naive, Embedding, Logistic regression, RandomForest, SVC and BERT After performing pre-processing and data cleaning with BERT model our team have crossed baseline, following is the current view Pending task: Team is currently working to optimization model i.e. using different BERT variant, tuning parameters and including context Challenges: After working on various model, BERT is working well for current assignment. BERT model requires heavy computing resources hence on desktop building and training model takes around three hrs. Our team is looking for some resources specially GPUs for tuning parameters and optimizing model
https://github.com/surajbisht1809/CourseProject	Proposal.md	Project Proposal Our team is Amazing S3 Following are the team members Sithira S Serasinghe sithira2@illinois.edu Santosh Kore kore3@illinois.edu Suraj Bisht surajb2@illinois.edu (Team Leader) We are joining Text classification competition Prior experience in Keras, TensorFlow and PyTorch frameworks Python will be used for this competition
https://github.com/surajbisht1809/CourseProject	README.md	"CS410 - Text Classification Competition Overview We participated in the Text Classification Competition for Sarcasm Detection in Tweets. Our team beat the baseline (0.723) and achieved an F1 score of 0.7542963307013469. The code can be used for training a preprocessing the given dataset (train.jsonl and test.jsonl) and train a BERT model. The usage of our solution can be found in the ""Source Code Walkthrough section"". Our Team: AmazingS3 Suraj Bisht surajb2@illinois.edu (Team Leader) Sithira Serasinghe sithira2@illinois.edu Santosh Kore kore3@illinois.edu Source Code Walkthrough 1. Prerequisites Anaconda 1.9.12 Python 3.8.3 PyTorch 1.7.0 Transformers 3.0.0 2. Install dependencies Make sure to run this program in an Ananconda environment (i.e. Conda console). This has been tested on *nix and Windows systems. 1. Libs ```bash pip install tweet-preprocessor textblob wordsegment contractions tqdm ```` 2. Download TextBlob corpora bash python -m textblob.download_corpora 3. Install PyTorch & Transformers bash conda install pytorch torchvision torchaudio cpuonly -c pytorch transformers If it complains that the transformers lib's not installed, try this command: bash conda install -c conda-forge transformers 3. Usage First, cd src and run the following commands, tl;dr bash python clean.py && python train.py && python eval.py This will preprocess, train and generate the answer.txt model which can be then submitted to the grader for evaluation. Description of each step: 1. Clean the dataset python clean.py Train the model python train.py Once the model is trained it will create an input/model.bin file which saves our model to a binary file. We can later load this file (in the evaluation step) to make predictions. Make predictions & create the answer.txt file python eval.py The answer.txt file is created at the output folder. The following section describes each of these steps in-depth. Data Cleaning / Preprocessing We perform data cleaning steps for both train.jsonl and test.jsonl so that they are normalized for training and evaluation purposes. The algorithm for cleaning the data is as follows: For each tweet: 1. Append all context to become one sentence and prefix it to the response. 2. Fix the tweet if it has special characters to support better expansion of contractions. 3. Remove all digits from the tweets. 4. Remove <URL> and @USER as they do not add any value. 5. Convert all tweets to lowercase. 6. Use NLTK's tweet processor to remove emojis, URLs, smileys, and '@' mentions 7. Do hashtag segmentation to expand any hashtags to words. 8. Expand contracted words. 9. Remove all special symbols. 10. Perform lemmatization on the words. Model Training A model can be built and trained with the provided parameters by issuing a python train.py command. The following steps are run in sequence during the model training. 1. Read in the train.csv from the prior step. 2. Training dataset (5000 records) is split into training and validation as 80:20 ratio. 3. Feed in the parameters to the model. 4. Perform model training for the given number of epochs. 5. Calculate validation accuracy for each run and save the best model as a bin file Tuning the model The following can be considered as parameters that could be optimized to achieve a better result. src/config.py python DEVICE = ""cpu"" # If you have CUDA GPU, change this to 'cuda' MAX_LEN = 256 # Max length of the tokens in a given document EPOCHS = 5 # Number of epochs to train the model for BERT_PATH = ""bert-base-uncased"" # Our base BERT model. Can plug in different models such as bert-large-uncased TRAIN_BATCH_SIZE = 8 # Size of the training dataset batch VALID_BATCH_SIZE = 4 # Size of the validation dataset batch src/train.py python L25: test_size=0.15 # Size of the validation dataset L69: optimizer = AdamW(optimizer_parameters, lr=2e-5) # A different optimizer can be plugging or a learning rate can be defined here L71: num_warmup_steps=2 # No. of warmup steps that need to run before the actual training step src/model.py python L13: nn.Dropout(0.1) # Configure the dropout value Evaluation of the model A high-level view of the sequence of operations run during the evaluation step is as follows. Load the test.csv file from the data transformation step. Load the best performing model from the training step. Perform predictions for each test tweet (1800 total records) Generate answer.txt that will be submitted to the grader to the ""output"" folder. Contributions of the team members Suraj Bisht surajb2@illinois.edu (Team Leader) Improve the initial coding workflow (Google Colab, Local setup etc.). Investigating Sequential model, Logistic Regression, SVC etc. Investigating bert-base-uncased model. Investigating data preprocessing options. Hyperparameter tuning to improve the current model. Sithira Serasinghe sithira2@illinois.edu Setting up the initial workflow. Investigating LSTM/BiDirectional LSTM, Random Forest etc. Investigating various data preprocessing options. Investigating bert-base-uncased model. Hyperparameter tuning to improve the current model. Santosh Kore kore3@illinois.edu Improve the initial coding workflow (Google Colab, Local setup etc.). Investigating Sequential models, SimpleRNN, CNN etc. Investigating bert-large-uncased model. Investigating data preprocessing options. Hyperparameter tuning to improve the current model. Future Enhancements Cleaning data further with different methods. Optimizing BERT model parameters and trying different BERT model (eg. RoBERTa) Re-use some of the tried models and optimizing to beat F1 scores. Extract Emoji's to add more meaning to the sentiments of the tweets. Data augmentation steps to prevent overfitting. Try an ensemble of models (eg. BERT + VLaD etc. ) Run our model on different test data and compare results against state-of-art. References/Credits The usage of BERT model is inspired by https://github.com/abhishekkrthakur/bert-sentiment"
https://github.com/falobba2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/czhu99/CourseProject	documentation.pdf	"Untitled December 13, 2020 1 Project 1.1 CS 410, Text Information Retrieval 1.1.1 University of Illinois at Urbana-Champaign, Fall 2020 Ramin Melikov, Chris Zhu, Francis Alloba Due: 12/13/2020, 11:59 PM CST 2 Reproduction of Mining Causal Topics in Text Data Kim, H. D., Castellanos, M., Hsu, M., Zhai, C. X., Rietz, T., & Diermeier, D. (2013). Mining causal topics in text data: Iterative topic modeling with time series feedback. In CIKM 2013 - Proceedings of the 22nd ACM International Conference on Information and Knowledge Man- agement (pp. 885-890). (International Conference on Information and Knowledge Management, Proceedings). https://doi.org/10.1145/2505515.2505612 3 Video Introduction to the Project https://www.youtube.com/watch?v=2RAoMGm07t8 Please watch the video overview of the project frst. 4 Repository Location https://github.com/czhu99/CourseProject 4.0.1 This is a KNIME Project KNIME is an Advanced Analytics Platform. It is available for free at http://www.KNIME.com After the KNIME is downloaded and installed, you have to import the workfow to see how it works. The workfow in in the repository. Workfow is named paper_replication.knwf. 1 5 Some code from our project The code below is used in KNIME to get the articles that are tagged with Bush or Gore and it then does some processing that produces a table with 2 columns: a date column and a string column. In the string column each row represents each article. Each string is lowercased, lematized, fltered, etc. See code for exact steps. from pandas import DataFrame import os from bs4 import BeautifulSoup import metapy import pandas as pd base_dir = 'D:/git/text_information_systems/project_files/project/nyt_corpus/data/2000' def extract_data(filename): return BeautifulSoup(open(filename, encoding = 'utf8')) def list_files(dir): return [os.path.join(r, n) for r, _, f in os.walk(dir) for n in f] blobs = [] for file_path in list_files(base_dir): blobs.append(extract_data(file_path)) filtered = [ blob for blob in blobs if [ person.get_text() for person in blob.find_all('person') if person.get_text() in ['Bush, George W (Gov)', 'Gore, Al (Vice Pres)'] 2 ] ] tokenized = {} date = [] articles = [] for article in filtered: doc = metapy.index.Document() year = article.find('meta', attrs = {'name':""publication_year""}).get(""content"") month = article.find('meta', attrs = {'name':""publication_month""}).get(""content"") day = article.find('meta', attrs = {'name':""publication_day_of_month""}).get(""content"") doc.content(article.body.get_text()) tok = metapy.analyzers.ICUTokenizer(suppress_tags=True) tok = metapy.analyzers.LowercaseFilter(tok) tok = metapy.analyzers.ListFilter(tok, ""D:/git/text_information_systems/project_files/project/nyt_corpus/data/lemur-stopwords.txt"", metapy.analyzers.ListFilter.Type.Reject) tok = metapy.analyzers.Porter2Filter(tok) tok = metapy.analyzers.LengthFilter(tok, min=2, max=30) tok.set_content(doc.content()) articles.append("" "".join([token for token in tok if not any(c.isdigit() or c == '.' for c in token)])) date.append(str(year) + '-' + str(month) + '-' + str(day)) tokenized['date'] = date tokenized['articles'] = articles output_table = pd.DataFrame.from_dict(tokenized) The following code gets the p-value for the Granger Causality test from statsmodels.tsa.stattools import grangercausalitytests import pandas as pd gr = grangercausalitytests(input_table[['price', 'topic_sum']], 1, verbose = False) p = gr[1][0]['ssr_ftest'][1] dict = {'p': p} output_table = pd.DataFrame(dict, index = [0]) 6 Team Contributions For our collaboration process during this project, we did pair programming with all three members present on a video call. Ramin did the majority of the coding on his machine while Chris and Francis viewed the screen and gave input and ideas. 3"
https://github.com/czhu99/CourseProject	Project Progress Report.pdf	CS 410 Francis Alobba (falobba2) Ramin Melikov (melikov2) Chris Zhu (cjzhu2) Project Progress Report 1) Which tasks have been completed? So far, we have successfully begun mining through the NYT Corpus data and have already gotten the specific data that we need to replicate the paper into our Jupyter Notebook. 2) Which tasks are pending? The next steps we need to complete include implementing the algorithm used in the paper and of course running the experiment from the paper using said algorithm. 3) Are you facing any challenges? No challenges thus far.
https://github.com/czhu99/CourseProject	project_proposal.pdf	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. duties than team members. Members: { 'net_id' : ['melikov2', 'falobba2', 'cjzhu2'], 'name' : ['Ramin Melikov', 'Francis Alobba', 'Chris Zhu'] } Captain: {'Chris Zhu'} Which paper have you chosen? Which paper have you chosen? Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback Which programming language do you plan to use? Which programming language do you plan to use? Python Can you obtain the datasets used in the paper for evaluation? Can you obtain the datasets used in the paper for evaluation? Yes If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? another dataset that is similar in nature)? N/A If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? introduced in the paper? N/A"
https://github.com/czhu99/CourseProject	README.md	"Project CS 410, Text Information Retrieval University of Illinois at Urbana-Champaign, Fall 2020 Ramin Melikov, Chris Zhu, Francis Alloba Due: 12/13/2020, 11:59 PM CST Reproduction of Mining Causal Topics in Text Data Kim, H. D., Castellanos, M., Hsu, M., Zhai, C. X., Rietz, T., & Diermeier, D. (2013). Mining causal topics in text data: Iterative topic modeling with time series feedback. In CIKM 2013 - Proceedings of the 22nd ACM International Conference on Information and Knowledge Management (pp. 885-890). (International Conference on Information and Knowledge Management, Proceedings). https://doi.org/10.1145/2505515.2505612 Video Introduction to the Project https://www.youtube.com/watch?v=2RAoMGm07t8 Please watch the video overview of the project first. Repository Location https://github.com/czhu99/CourseProject This is a KNIME Project KNIME is an Advanced Analytics Platform. It is available for free at http://www.KNIME.com After the KNIME is downloaded and installed, you have to import the workflow to see how it works. The workflow in in the repository. Workflow is named paper_replication.knwf. Some code from our project The code below is used in KNIME to get the articles that are tagged with Bush or Gore and it then does some processing that produces a table with 2 columns: a date column and a string column. In the string column each row represents each article. Each string is lowercased, lematized, filtered, etc. See code for exact steps. ````python from pandas import DataFrame import os from bs4 import BeautifulSoup import metapy import pandas as pd base_dir = 'D:/git/text_information_systems/project_files/project/nyt_corpus/data/2000' def extract_data(filename): return BeautifulSoup(open(filename, encoding = 'utf8')) def list_files(dir): return [os.path.join(r, n) for r, _, f in os.walk(dir) for n in f] blobs = [] for file_path in list_files(base_dir): blobs.append(extract_data(file_path)) filtered = [ blob for blob in blobs if [ person.get_text() for person in blob.find_all('person') if person.get_text() in ['Bush, George W (Gov)', 'Gore, Al (Vice Pres)'] ] ] tokenized = {} date = [] articles = [] for article in filtered: doc = metapy.index.Document() year = article.find('meta', attrs = {'name':""publication_year""}).get(""content"") month = article.find('meta', attrs = {'name':""publication_month""}).get(""content"") day = article.find('meta', attrs = {'name':""publication_day_of_month""}).get(""content"") doc.content(article.body.get_text()) tok = metapy.analyzers.ICUTokenizer(suppress_tags=True) tok = metapy.analyzers.LowercaseFilter(tok) tok = metapy.analyzers.ListFilter(tok, ""D:/git/text_information_systems/project_files/project/nyt_corpus/data/lemur-stopwords.txt"", metapy.analyzers.ListFilter.Type.Reject) tok = metapy.analyzers.Porter2Filter(tok) tok = metapy.analyzers.LengthFilter(tok, min=2, max=30) tok.set_content(doc.content()) articles.append("" "".join([token for token in tok if not any(c.isdigit() or c == '.' for c in token)])) date.append(str(year) + '-' + str(month) + '-' + str(day)) tokenized['date'] = date tokenized['articles'] = articles output_table = pd.DataFrame.from_dict(tokenized) ```` The following code gets the p-value for the Granger Causality test ````python from statsmodels.tsa.stattools import grangercausalitytests import pandas as pd gr = grangercausalitytests(input_table[['price', 'topic_sum']], 1, verbose = False) p = gr[1][0]['ssr_ftest'][1] dict = {'p': p} output_table = pd.DataFrame(dict, index = [0]) ```` Team Contributions For our collaboration process during this project, we did pair programming with all three members present on a video call. Ramin did the majority of the coding on his machine while Chris and Francis viewed the screen and gave input and ideas."
https://github.com/MLwithSandy/CourseProject	ProjectProposal_StockRecommenderSystem.pdf	"Project Topic: Stock Recommender System [ ] [ ] [ ] [ ] [ ] [ Team Members Background Proposal Market research High-level System design Project ] [ ] timeline FAQs Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. Background In the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investor of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors. Proposal We would like to propose a solution ""Stock recommender system"" to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combines stock rating data from various market analyst, market sentiments and company profile for determination of the cumulated rating and curation of the recommendation list. Market research For the viability of the stock recommender system, we researched features offered by existing tools (freely available tools) such as Yahoo Finance, eToro platform etc. Each of these tools offers all the information, required for decision making, in isolation and relies on the users to combine them for their investment decisions. As an example, above screenshots shows a recommender from Yahoo Finance which cumulates ratings from various market analysts but does not include current market sentiments and does not recommend similar stocks. Based on our analysis, we have not come across any product which provides sentiment weighted cumulated stock rating and recommendation for similar stocks which can be used by retail investors for faster and informed decision making. High-level System design There might be several possible approaches to solve the problem - to allow easy access to information for informed investment decision, we would like to propose following solution design, based on our learnings from the UIUC course ""CS 410 - Text Information System"". The solution design leverages following aspects of the CS410 course to build the Stock recommender system. Scraping of webpages for relevant information and collating the data Sentiment analysis based on user texts Recommender system based on content similarity Project timeline The implementation of the project is planned in seven phases. First phase is focused on project planning and task distribution. Subsequent four phases are aimed to development of major software components such as Ratings system, Sentiment Analysis etc. The last two phase focuses on UI and documentation which will be executed in parallel to component development. FAQs What is the function of the tool? The stock recommender system helps customers, who would like to invest in equity market, to identify best value stock for investment based on consolidated rating of various market analyst, who rates the stock after in-depth research, and based on current market sentiment, judged via twitter feeds. Who will benefit from such a tool? Retail investors, financial institution etc. Does this kind of tools already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? Based on our research, there are various tools in the market offering various features such as cumulated ratings. However, we have not come across any product which provides sentiment weighted average stock ratings and recommendation for similar stocks which can be used by retail investors for faster and informed decision making. What existing resources that we use? Sourcing of Market Analysts ratings by web scraping (e.g. Python Beautiful Soup, etc.): Stock Target Advisor https://www.stocktargetadvisor.com/ Market Beat https://www.marketbeat.com/ Market Sentiments Twitter Feeds We intend to use the Twitter dev API https://developer.twitter.com/en to ingest tweets for related companies to analyze user sentiments. This will help us create a sentiment index to obtain a positive / negative pulse on a given stock ticker symbol (e.g. AAPL) based on public opinion of users interested in a given stock. What techniques/algorithms will you use to develop the tool? Refer to Proposal and High-level System design to understand the approach we are planning to use for development of the tool. How will you demonstrate the usefulness of your tool? As part of this exercise, we may take feedback from our class about the usefulness of the tool and any improvement recommendation. A very rough timeline to show when you expect to finish what. Refer to Project Timeline for details"
https://github.com/MLwithSandy/CourseProject	ProjectReport_StockRecommenderSystem.docx	"Project Report Stock Recommender System CS 410: Text Information System University of Illinois, Urbana-Champaign Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. [ Team Members ] [ Project Overview ] [ Abstract ] [ Motivation ] [ Introduction ] [ High Level Design ] [ Component View (Deployment Perspective) ] [ SRE Front-end (UI) ] [ SRE Back-end (Rest API) ] [ Implementation details ] [ Rating System ] [ Sentiment Analysis ] [ Recommender System ] [ Libraries and Datasets ] [ Verification ] [ Conclusion and Future Work ] [ References ] Project Overview Abstract Our project is a novel text-mining application which combines 3 different functionalities around rating stocks into one application. Namely, it consists of a base analysis which is a two-tiered summary of sentiment of stock analyst ratings who are well known in the industry and rate stock ticker symbols, a twitter sentiment analysis aspect which scrapes tweets off of Twitter and analyzes them for sentiment about a particular stock symbol, and a recommendation engine which recommends stock symbols similar to a user-provided query stock symbol. There are many conceivable use-cases for an app such as ours upon further embellishment and improvements, such as individual investors in the stock market who want automated and instantaneous advice and suggestions that incorporates both analyst ratings from well-known analysts across the internet, public sentiment as embodied by tweets, and that produces not only concise summaries of these analyst ratings and Twitter sentiment but recommends similar stocks to their stock symbol (e.g. AAPL) of interest. Imaginably there may well also be corporate interest in incorporating an application such as ours into a larger pipeline which could be serving a huge myriad of purposes but that leverages actionable knowledge about the stock market into a larger purpose. Although our application is alpha version, it is not inconceivable that it could springboard academic research into these areas. Motivation In the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investors of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors. Introduction We propose ""Stock recommender system"" as a solution to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combines stock rating data from various market analyst, market sentiments and company profile for determination of the cumulated rating and curation of the recommendation list. High Level Design Component View (Deployment Perspective) Stock Recommendation System consists of two components - SRE Front-end and SRE Back-end. The SRE Front-end is the UI component for user interaction, developed using Angular. The Front-end relies on SRE Back-end for all its data needs for providing various user centric functions. The SRE Back-end is the main component, which implements all the necessary algorithm and business rules and finally exposes the data related to ratings, recommendations and tweets via Rest APIs. SRE Front-end (UI) The SRE Front-end is developed in Angular framework and is a single page application. It consists of the following components: A. app-container The main component to render the Stock Recommendation System web page B. app-navbar The component responsible for top navigation bar C. app-routing The component dealing with url based routing and takes care of routing to the home page or canonical ""page-not-found"" page. D. home.component This is the main component which takes user input, calls SRE backend services to fetch relevant data. E. p404.component This is the component for handling ""page not found"" scenario in case user enters incorrect URL not supported by SRE Front-end. SRE Back-end (Rest API) The SRE Back-end is a Flask based app, developed using Python and TinyDB which exposes various Rest APIs for the SRE Front-end. List of Rest APIs: Rest API Sample Response Get list of all stocks in the corpus (output abridged for succinctness) GET /stock/all [ { ""Symbol"":""AAPL"", ""Security Name"": ""Apple Inc"", ""Market"": ""NASDAQ"", ""Sector"": ""Technology"" } ] Get ratings for a given stock from a given market GET /stock/ratings/<market>/<stock_symbol> [ { ""stockSymbol"": ""AAPL"", ""marketPlace"": ""NASDAQ"", ""refreshData"": ""2020-11-25"", ""overallRating"": ""HOLD"", ""analystsRatings"": [ { ""level_0"": 0, ""index"": 1, ""ratingDate"": ""2020-11-19"", ""ratingAgency"": ""The Goldman Sachs Group"", ""ratingAssigned"": ""Sell"", ""newRatings"": -1, ""scaledRatings"": ""SELL"" } ] } ] Get a list of recommended stocks, similar to given stock GET /stock/recommendation/<stock_symbol> [ { ""seq"": 1, ""stockSymbol"": ""HAFC"", ""stockName"": ""Hanmi Financial Corporation"", ""sector"": ""Finance"", ""rating"": ""SELL"" }, ] Get Twitter sentiment for given stock 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Get list of user request log GET /requests/all [ { ""datetime"": ""2020-11-25T21:55:52.924706"", ""symbol"": ""TSLA"", ""market"": ""NASDAQ"" } ] List of the Rest APIs exposed out of the backend can be found on GitHub project repository: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/StockRatingsSystem The SRE Back-end is comprised of the following components and/or modules: A. Flask app This is the main component which exposes various Rest APIs for external consumers - in our case SRE Front-end. It integrates all other components such as ratings_system, recommender_system etc. to provide the necessary services. In addition, it also logs all requests to requestLogDB. B. ratings_system The ratings_system is the component which calculates the overall rating for the user selected (searched) stock, by scraping the required ratings data from a variety of market analyst websites and aggregating them. It also provides the list of our selected latest ten ratings from an assortment of various market analysts as a reference. C. recommender_system The recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected (searched) stock and a pre-defined similarity function. D. TwitterSentimentAnalysis TwitterSentimentAnalysis component fetches latest tweets for the user selected (searched) stock from Twitter which by making use of the Twitter API, performs sentiment analysis and returns overall sentiment for the given stock. It also provides the latest five tweets as a reference. E. nasdaq The nasdaq component is primarily used for preparation of data. It combines the nasdaq listed stocks with the dataset we used for identifying various feature parameters of a stock. By cleaning and combining our data sources, we were able to create a final dataset of 1462 stocks for use across our systems in our app (analyst ratings, twitter sentiment, recommender system). F. batchRatingProcess The batchRatingProcess is used to update ratings of all 1462 stocks in background. This is to ensure that the user does not face any significant delay in fetching the data from third party websites while interacting with our SRE app's UI. In addition, the ratings of a stock is not updated frequently and batch processing fits well from a requirements and solution design perspective. G. batchRecoDataPreparation The batchRecoDataPreparation is another batch process to prepare the data for the recommendation system as the recommendation engine relies on an assortment of analyst ratings of stocks as a feature in the similarity function. Implementation details Rating System A. Approach To calculate the ratings, the following algorithm is used: First of all, rating for the selected(searched) stock is searched in the ratings database. Case 1: ratings data is already available in the ratings database. In such a case, ratings data is fetched from the database. Case 2: ratings data is not available in ratings database. In this case, following step is executed to get the ratings. Read the website marketbeat.com which lists the ratings from various market analysts and scrape the page section containing ratings relevant date Data is cleansed, validated and structured as per requirement In case of more than 10 ratings, only latest 10 ratings are selected. Initially data from only current and previous months were considered but to increase the data quantity, this restriction is switched off in final product Final list of ratings is stored in the database Once the rating data is available, ratings from various market analysts are scaled to following ratings scale: {-1: SELL, 0: HOLD, 1: BUY} Finally, overall rating is calculated based on the selected scaled ratings with possible outcome as SELL, HOLD or BUY. B. Webpage Scraper Webpage scraper was developed using the learnings from MP 2.1 of the course CS 410: Text Information System. Python library BeautifulSoup and Chrome driver were primarily used to fetch the web document. Following URL builder code were used to get URL for various stocks: # MarketBeat URL builder def get_mb_url(market, stock_symbol): base_url = 'https://www.marketbeat.com/stocks/{market}/{stock_symbol}/price-target/' final_url = base_url.format(market=market, stock_symbol=stock_symbol) return final_url e.g. https://www.marketbeat.com/stocks/NASDAQ/AAPL/price-target/ As part of the exercise, following additional aspects were required to be taken care: 1. the time delay in reading the complete web document, and 2. the URL redirects - in case a particular stock was not found on the website, website would redirect the URL request to a default page. # create a webdriver object and set options for headless browsing def load_webdriver(): if str(filePath).find(""CS410"") == -1: options = webdriver.ChromeOptions() options.add_argument('--no-sandbox') options.add_argument('--headless') options.add_argument(""--disable-dev-shm-usage"") driver = webdriver.Chrome(chrome_options=options) else: options = Options() options.headless = True driver = webdriver.Chrome(filePath / 'chromedriver', options=options) return driver # read web document using beutifulsoup def get_js_soup(url_web, driver): driver.get(url_web) time.sleep(5) if url_web != redirected_url: print(""redirected URL : "" + redirected_url) new_url = redirected_url + ""price-target/"" print(""new URL for Analysts rating : "" + new_url) if new_url.find('NASDAQ/price-target/') == -1: driver.get(url_web) else: return Using the html div class identifier, relevant data were extracted from the webpage data and subsequently processed in panda dataframe. table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-left-column fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table""}) if table is None: print(stock_symbol + "": No table found for market analyst rating"") else: table_body = table.find('tbody') rows = table_body.find_all('tr') for row in rows: cols = row.find_all('td') cols = [ele.text.strip() for ele in cols] data.append([ele.split("""")[-1].strip() for ele in cols if ele]) C. Uniform Scaling of Ratings Uniform scaling of ratings was required to unify the ratings from various market analysts and ensure that the calculation of overall rating is without any bias and is not affected by some higher or lower ratings from some market analysts, only due to the fact that they use different scales for rating a stock. Various ratings were mapped to {-1: SELL, 0: HOLD, 1: BUY} using following mapping dictionary is used. ratings_dict = { ""SELL"": -1, ""STRONG SELL"": -1, ""BEARISH"": -1, ""UNDERPERFORM"": -1, ""SECTOR UNDERPERFORM"": -1, ""MODERATE SELL"": -1, ""WEAK HOLD"": -1, ""UNDERWEIGHT"": -1, ""REDUCE"": -1, ""HOLD"": 0, ""NEUTRAL"": 0, ""AVERAGE"": 0, ""MARKET PERFORM"": 0, ""SECTOR PERFORM"": 0, ""SECTOR WEIGHT"": 0, ""PEER PERFORM"": 0, ""EQUAL WEIGHT"": 0, ""IN-LINE"": 0, ""MARKET OUTPERFORM"": 1, ""OUTPERFORM"": 1, ""MODERATE BUY"": 1, ""ACCUMULATE"": 1, ""OVER-WEIGHT"": 1, ""OVERWEIGHT"": 1, ""STRONG-BUY"": 1, ""ADD"": 1, ""BULLISH"": 1, ""BUY"": 1, ""POSITIVE"": 1, ""STRONG BUY"": 1, ""TOP PICK"": 1, ""CONVICTION-BUY"": 1, ""OUTPERFORMER"": 1 } There were other options also considered e.g., scaling of all ratings on a scale of 1-5 or 1-3, weighted scale to reflect strong ratings e.g., strong buy or strong sell. However, based on our need to combine the analyst rating with twitter sentiment, scaling to {-1: SELL, 0: HOLD, 1: BUY} were selected for this exercise. D. Overall Ratings Calculation First of all, aggregated ratings of all ratings from various market analyst is calculated based on arithmetic mean of all selected scaled ratings. e.g., aggregated rating for following five scaled ratings {-1: SELL, 1: BUY, 0: HOLD, 1: BUY, -1: SELL} from various market analyst will be {0: HOLD}. Once the aggregated rating is calculated, it is combined with Twitter Sentiment analysis result as per below table for Overall ratings. Aggregated Rating Twitter Sentiment Overall Ratings -1: SELL POSITIVE HOLD -1: SELL NEUTRAL SELL -1: SELL NEGATIVE SELL 0: HOLD POSITIVE BUY 0: HOLD NEUTRAL HOLD 0: HOLD NEGATIVE SELL 1: BUY POSITIVE BUY 1: BUY NEUTRAL BUY 1: BUY NEGATIVE HOLD Sentiment Analysis Sentiment Analysis, also known as Opinion Mining, refers to the use of Natural Language Processing to computationally determine opinions and emotions of an opinion holder for an opinion target. A common use for this technology is to discover how people feel about certain topics, particularly through users' textual posts in Social Media space. To perform Sentiment Analysis to provide stock recommendation, Twitter has been considered as the Social Media space where users post their opinion as their tweets. As most of the elements in the opinion representation such as the opinion holder (Twitter users in this case) and opinion target (Stock to be recommended in this case) and the content and the context of the opinion (financial context) are already known, the main task is to decide opinion sentiment. So, this is a case of just using sentiment classification for understanding opinion where the input is opinionated text object, the output is a sentiment label i.e., polarity analysis with predefined categories such as positive, negative, or neutral. For this project, the sentiment analysis has been done through the process outlined below. Process Description A. Preparing the Data Set To build the model, training and testing data set is needed. Ideally for optimal performance the financial tweets need to be downloaded from tweeter and should be human evaluated to create the labels which can be used for training the model and later for testing the model. However, one needs to spend considerable amount of manual effort to build such data. To optimize time and resource for this project, a readily available downloadable training set (polarity dataset from Cornell university -) has been used. The data set contain 2000 processed down-cased text files used in Pang/Lee ACL 2004 [1]; the names of the two subdirectories in that folder, ""pos"" and ""neg"", indicate the true classification (sentiment) of the component files according to the automatic rating classifier the tweets of the data set have been all labeled as positive or negative, depending on the content. The data set have been persisted into pickle file (a binary representation of python structure) to optimize performance of the subsequent run to build the classifier. import pickle from sklearn.datasets import load_files #nltk.download('stopwords') #Import Dataset -> generate two classes one each for each sub directorties dataset = load_files('txt_sentoken/') X,y = dataset.data, dataset.target #store as pickle file, these are byte type file with open('X.pickle', 'wb') as f: pickle.dump(X,f) with open('y.pickle', 'wb') as f: pickle.dump(y,f) The same data has been split into training and testing data set following a 80-20 rule where 80% of the downloaded pre-labeled data has been used as training data set and 20% of the same downloaded data has been used as testing data set. B. Preprocessing the Data Set The downloaded data set has been preprocessed before feeding into the program to create the Classifier to remove all the non-word characters, to convert into lower case, to remove single characters (e.g. i, a etc.). import re import pickle with open('X.pickle', 'rb') as f: X=pickle.load(f) with open('y.pickle', 'rb') as f: y=pickle.load(f) corpus = [] for i in range(0,len(X)): data = re.sub(r'\W', ' ', str(X[i])) data = data.lower() data = re.sub(r'\s+[a-z]\s+', ' ', data) data = re.sub(r'^[a-z]\s+', ' ', data) data = re.sub(r'\s+', ' ', data) corpus.append(data) C. Building the BOW, TF-IDF and Logistic Regression Classifier Scikit-learn library (a free machine learning library for Python) has been used to create the Classifier. At first, the bag of words model has been created and later the bag of words model would be converted into TF-IDF model. To covert the data into bag of words model, classes from Scikit-learn has been used. First, a max feature has been set to 2000 which means 2000 most frequent words would be used as features. The min document frequency would ensure to exclude a word to be considered as 2000 features which appear 3 or less documents (to prevent a word to become a feature which is rare into the set but very popular within a certain document) and the max document frequency would ensure to exclude a word to be considered as 2000 features which appear 60% or more documents (to exclude the most common words like the, an etc.). Then Stop words has been removed that is defined in nltk corpus. Now to form the Bag of Words model the corpus from above steps has been used. from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer from nltk.corpus import stopwords nltk.download('stopwords') vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X = vectorizer.fit_transform(corpus).toarray()#will generate 2D array [len(X),len(X)], total number of docs = number of features = len(x) Finally, TfidfTransformer from sklearn is used to create TF-IDF model from Bag of Words model created earlier. from sklearn.feature_extraction.text import TfidfTransformer # convert BOW to TF-IDF transformer = TfidfTransformer() X = transformer.fit_transform(X).toarray() Logistic Regression (a Discriminative Classifier) is a classification algorithm (learning algorithm) which is used for binary classification problem. The task of sentiment analysis for this project can be thought as a binary classification problem where the goal is to predict positive or negative sentiment from a given tweets (a sentence in particular). Hence for the purpose of this project, a logistics regression classifier has been built where negative and positive sentiments have been denoted as 0 and 1 respectively. So, the Binary Response Variable Y Ie {0,1} needs to be calculated from the predictor X where X = { x1, x2 ..... x2000} (all the 2000 features) Hence, for Logistics Regression can be represented as below: So essentially using training data T, , parameters (M=2000) needs to be estimated. Hence the conditional likelihood estimate is: and The goal of Logistic Regression algorithm is to optimize the parameters using training data set. Once the optimal values of the parameters are found by the algorithm, y can be calculated for any unknown sentiment of a new sentence. If y>=0.5, then that sentiment is positive sentiment and if y<0.5, then that sentiment is negative sentiment. For this project, LogistricRegression from sklearn has been used to build the classifier. First the input data set has been split as - 80% of available data has been considered as training data and 20% of the available data has been considered as test data from sklearn.model_selection import train_test_split text_train,text_test,sentiment_train,sentiment_test = train_test_split(X,y,test_size=0.2,random_state=0)#80% training and 20% testing data Then classifier is built: from sklearn.linear_model import LogisticRegression # create the classifier using logistic regression classifier = LogisticRegression() classifier.fit(text_train,sentiment_train) Just to showcase the model performance, confusion_matrix class from sklearn has been used. With the input data, close to 85% accuracy has been achieved. from sklearn.metrics import confusion_matrix #testing model performance sentiment_prediction = classifier.predict(text_test) cm = confusion_matrix(sentiment_test,sentiment_prediction)# [[predicted as 0 and actually 0, predicted as 0 and actually 1], #[predicted as 1 and actually 0, predicted as 1 and actually 1]] print(""accuracy : "", (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,""%"") ''' [model predicted 0and actual 0 model predicted 1 nand actual 0 model predicted 1 and actual 0 model predicted 1 and actual 1] ''' Finally, the model and vectorizer have been stored as pickle file (binary representation of python objects) so that while calculating the real time tweets, the saved model can be used as it is. from sklearn.feature_extraction.text import TfidfVectorizer #store the classifier ....pickle file with open('classifier.pickle', 'wb') as f: pickle.dump(classifier,f) #store the TFIDF vectorizer vectorizerTFIDF = TfidfVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X_TDIDF = vectorizerTFIDF.fit_transform(corpus).toarray() with open('vectorizerTFIDF.pickle', 'wb') as f: pickle.dump(vectorizerTFIDF,f) D. Fetching the real time tweets A developer app has been created in Twitter for the purpose of this project. The OAuth2 mechanism has been used to make API calls to Twitter API for fetching the recent tweets. ConsumerKey and ConsumerSecret from the created app are used to generate bearer token in the runtime API call using https://api.twitter.com/oauth2/token Finally, the bearer token is used to call API to perform recent search (returns Tweets from the last 7 days that match a search query) using ""recent search"" API. https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent For this project, Max result of 100 has been set to perform recent search. This is to ensure throttling of number of results fetched (as for the basic access, twitter account has 500000 search results/month limitation) E. Performing Sentiment analysis of the fetched tweets To perform sentiment analysis in real time, firstly the Logistic Regression Classifier model and TF-IDF vectorizer is loaded from the saved pickle file. import pickle import re with open('classifier.pickle', 'rb') as f: clf=pickle.load(f) with open('vectorizerTFIDF.pickle', 'rb') as f: vectorizer=pickle.load(f) For a given stock the ""recent search"" API is used to search recent tweets. After fetching related tweets (max = 100), each tweet is preprocessed to create bag of words and to be represented as TF-IDF vectorizer. Then, prebuilt Logistic Regression Classifier is used to predict the sentiment of each tweets. for t in tweets_fetched: t = re.sub(r'^https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*$',' ',t) t = t.lower() t = re.sub(r""that's"",'that is',t) t = re.sub(r""there's"",'there is',t) t = re.sub(r""what's"",'what is',t) t = re.sub(r""where's"",'where is',t) t = re.sub(r""it's"",'it is',t) t = re.sub(r""who's"",'who is',t) t = re.sub(r""i'm"",'i am',t) t = re.sub(r""she's"",'she is',t) t = re.sub(r""he's"",'he is',t) t = re.sub(r""they're"",'they are',t) t = re.sub(r""who're"",'who are',t) t = re.sub(r""shouldn't"",'should not',t) t = re.sub(r""wouldn't"",'would not',t) t = re.sub(r""couldn't"",'could not',t) t = re.sub(r""can't"",'can not',t) t = re.sub(r""won't"",'will not',t) t = re.sub(r'\W', ' ', t) t = re.sub(r'\d', ' ', t) t = re.sub(r'\s+[a-z]\s+', ' ', t) t = re.sub(r'\s+[a-z]$', ' ', t) t = re.sub(r'^[a-z]\s+', ' ', t) t = re.sub(r'\s+', ' ', t) sentiment = clf.predict(vectorizer.transform([t]).toarray()) Finally, total positive and negative sentiments are calculated for total tweets fetched in runtime and the final sentiment of a particular stock has been calculated as positive = 1 (if the positive percentage > 65%), negative = -1 (if the positive percentage < 35%) or neutral = 0 (if the positive percentage is in between 35% and 65%). if (tot_positive+tot_negetive)>0 : positive_percentage = tot_positive/(tot_positive+tot_negetive) print(""positive_percentage :"",positive_percentage*100, ""%"") if positive_percentage>0.65 : print(""stock is buy"") return 1 elif positive_percentage<0.35 and positive_percentage>0: print(""stock is sell"") return -1 else: print(""stock is neutral"") return 0 Following end point has been built to provide twitter sentiment analysis result to a stock. Definition Get Twitter sentiment for given stock - 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> e.g. http://localhost:5000/stock/sentiments/NASDAQ/AAPL Response 200 OK on success json { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Sample run: Recommender System Approach The basic approach to the recommendation engine is to use a similarity function to compare pre-identified features of all stocks in our corpus against user-provided stock symbol features. While doing so, we are implementing one of the methods for building recommendation engine - Content based recommendation or Item similarity, which was part of Week 6 lecture of course CS 410 Text Information System. Recommender System recommends to the user the top 5 stock symbols which are most similar to the user provided stock symbol in terms of those underlying features and the similarity function. For the initial, Cosine similarity is used as similarity function and the following financial/economic features were selected as underlying features for similarity calculation. company is in S&P 500 company profitability over last three years revenue growth for last three years current market analyst ratings sector gross profit per market cap Analyst ratings are calculated by rating_system and same is reused also for recommendation system. Other features for the stock are calculated or derived using the financial data of the companies from year 2018. Source: Kaggle dataset available at https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018. As part of the exercise, significant amount of effort was used to identify a single data source for financial or economic data and also to understand various financial terms to find the right feature. Based on our current understanding of a company performance and its relation to stock price, we used above listed features. The features might be further optimized with right guidance from a financial analyst or person with insight of stock market and factors influencing the investor decisions. Libraries and Datasets Python Libraries Flask flask_cors markdown beautifulsoup4 selenium pandas pymongo tinydb misaka nltk requests sklearn flask-jsonpify Datasets https://www.marketbeat.com/ https://www.cs.cornell.edu/people/pabo/movie-review-data/ https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018?select=2018_Financial_Data.csv https://developer.twitter.com Verification The result of the recommender system was verified based on manual verification process by selecting some stocks at random and checking the result for individual section e.g., ratings, sentiments and overall ratings. For example, Overall rating for Facebook (FB) stock is BUY. Looking at individual ratings component, the result seems reasonable. Analyst Ratings: BUY Verification: In the list of market analyst ratings, all 10 analysts have rated it as BUY. Therefore, the aggregated rating is BUY. Tweet Sentiments: Neutral Verification: From the latest 100 tweets fetched (as shown in below screenshot), it seems reasonable to have a NEUTRAL sentiment Overall Rating: BUY Verification: Combining Analyst Ratings ""Buy"" with Twitter Sentiments ""NEUTRAL"", as per listed decision table, the result is BUY. Conclusion and Future Work Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating (e.g., buy, hold, sell) for the user entered Stock symbol and recommending five similar stocks. The result has been validated by some popular stocks. One of key challenges faced in building the system is the access on right financial data as well as good training and testing set to train and test the model for sentiment analysis. The choice we had to manually create the required data. However, to optimize the time and resource available, it has been decided to use readily available data in the web which might have not yielded the perfect recommendation. There are several interesting directions for the future version of recommender system. First, the overall functionality can be improved by considering user input of sectors and providing recommendation and trends specific to that sector. A second direction involves defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market and use the same to train and test the model. Given the current trend of machine learning algorithms, a third interesting research direction is to explore the timeseries data for stock adjacency. Finally, overall score can be improved further to consider more analysts' reports and microblogging websites and come up with more recommender categories i.e., Strong Buy, Buy, Hold, Sell, Strong Sell etc. At the end, it has been a great journey of ideation and learning in a collaborative manner to design and develop the stock recommender system. We thank different analysts' websites to make their ratings available publicly and twitter to grant access of real time tweets through their public API and last but not the least, we thank professor ChengXiang Zhai and all our TAs and all the reviewers to give the direction needed and to provide the valuable feedback. References [1] Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL '04). Association for Computational Linguistics, USA, 271-es. DOI:https://doi.org/10.3115/1218955.1218990 [2] Carbone, N. (2020, January 18). 200+ Financial Indicators of US stocks (2014-2018). Retrieved December 11, 2020, from https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018"
https://github.com/MLwithSandy/CourseProject	ProjectReport_StockRecommenderSystem.pdf	"Project Report Stock Recommender System CS 410: Text Information System University of Illinois, Urbana-Champaign Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. [ Team Members ] [ Project Overview ] [ Abstract ] [ Motivation ] [ Introduction ] [ High Level Design ] [ Component View (Deployment Perspective) ] [ SRE Front-end (UI) ] [ SRE Back-end (Rest API) ] [ Implementation details ] [ Rating System ] [ Sentiment Analysis ] [ Recommender System ] [ Libraries and Datasets ] [ Verification ] [ Conclusion and Future Work ] [ References ] Project Overview Abstract Our project is a novel text-mining application which combines 3 different functionalities around rating stocks into one application. Namely, it consists of a base analysis which is a two-tiered summary of sentiment of stock analyst ratings who are well known in the industry and rate stock ticker symbols, a twitter sentiment analysis aspect which scrapes tweets off of Twitter and analyzes them for sentiment about a particular stock symbol, and a recommendation engine which recommends stock symbols similar to a user-provided query stock symbol. There are many conceivable use-cases for an app such as ours upon further embellishment and improvements, such as individual investors in the stock market who want automated and instantaneous advice and suggestions that incorporates both analyst ratings from well-known analysts across the internet, public sentiment as embodied by tweets, and that produces not only concise summaries of these analyst ratings and Twitter sentiment but recommends similar stocks to their stock symbol (e.g. AAPL) of interest. Imaginably there may well also be corporate interest in incorporating an application such as ours into a larger pipeline which could be serving a huge myriad of purposes but that leverages actionable knowledge about the stock market into a larger purpose. Although our application is alpha version, it is not inconceivable that it could springboard academic research into these areas. Motivation In the year 2020, there has been huge surge in securities trading, driven by retail investors. The increase in the trading activity can be attributed primarily to the easy-to-use mobile based trading apps, offered by several FinTech companies such as eToro, Robinhood and InteractiveBrokers etc. The retail investors of today might lack time for in-depth research and/or even lack the necessary knowledge to analyze the financial standing of a company. In such a situation, many of the retail investors either rely on word of mouth or blindly following other investors on such trading platforms. This leads to investment decisions beyond the risk profile and/or risk appetite of the investors. Introduction We propose ""Stock recommender system"" as a solution to enable retail investors easy access to information, relevant for informed investment decisions. Based on user's preference of a stock, the stock recommender proposes a cumulated rating for the stock and also proposes other stocks with similar ratings. The recommender system combines * stock rating data from various market analyst, * market sentiments and * company profile for determination of the cumulated rating and curation of the recommendation list. High Level Design Component View (Deployment Perspective) Stock Recommendation System consists of two components - SRE Front-end and SRE Back- end. * The SRE Front-end is the UI component for user interaction, developed using Angular. The Front-end relies on SRE Back-end for all its data needs for providing various user centric functions. * The SRE Back-end is the main component, which implements all the necessary algorithm and business rules and finally exposes the data related to ratings, recommendations and tweets via Rest APIs. SRE Front-end (UI) The SRE Front-end is developed in Angular framework and is a single page application. It consists of the following components: A. app-container The main component to render the Stock Recommendation System web page B. app-navbar The component responsible for top navigation bar C. app-routing The component dealing with url based routing and takes care of routing to the home page or canonical ""page-not-found"" page. D. home.component This is the main component which takes user input, calls SRE backend services to fetch relevant data. E. p404.component This is the component for handling ""page not found"" scenario in case user enters incorrect URL not supported by SRE Front-end. SRE Back-end (Rest API) The SRE Back-end is a Flask based app, developed using Python and TinyDB which exposes various Rest APIs for the SRE Front-end. List of Rest APIs: Rest API Sample Response Get list of all stocks in the corpus (output abridged for succinctness) GET /stock/all [ { ""Symbol"":""AAPL"", ""Security Name"": ""Apple Inc"", ""Market"": ""NASDAQ"", ""Sector"": ""Technology"" } ] Get ratings for a given stock from a given market GET /stock/ratings/<market>/<stock_symb ol> [ { ""stockSymbol"": ""AAPL"", ""marketPlace"": ""NASDAQ"", ""refreshData"": ""2020-11-25"", ""overallRating"": ""HOLD"", ""analystsRatings"": [ { ""level_0"": 0, ""index"": 1, ""ratingDate"": ""2020-11-19"", ""ratingAgency"": ""The Goldman Sachs Group"", ""ratingAssigned"": ""Sell"", ""newRatings"": -1, ""scaledRatings"": ""SELL"" } ] } ] Get a list of recommended stocks, similar to given stock GET /stock/recommendation/<stock_symbol > [ { ""seq"": 1, ""stockSymbol"": ""HAFC"", ""stockName"": ""Hanmi Financial Corporation"", ""sector"": ""Finance"", ""rating"": ""SELL"" }, ] Get Twitter sentiment for given stock 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_s ymbol> { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Get list of user request log GET /requests/all [ { ""datetime"": ""2020-11-25T21:55:52.924706"", ""symbol"": ""TSLA"", ""market"": ""NASDAQ"" } ] List of the Rest APIs exposed out of the backend can be found on GitHub project repository: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/StockRatingsSystem The SRE Back-end is comprised of the following components and/or modules: A. Flask app This is the main component which exposes various Rest APIs for external consumers - in our case SRE Front-end. It integrates all other components such as ratings_system, recommender_system etc. to provide the necessary services. In addition, it also logs all requests to requestLogDB. B. ratings_system The ratings_system is the component which calculates the overall rating for the user selected (searched) stock, by scraping the required ratings data from a variety of market analyst websites and aggregating them. It also provides the list of our selected latest ten ratings from an assortment of various market analysts as a reference. C. recommender_system The recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected (searched) stock and a pre-defined similarity function. D. TwitterSentimentAnalysis TwitterSentimentAnalysis component fetches latest tweets for the user selected (searched) stock from Twitter which by making use of the Twitter API, performs sentiment analysis and returns overall sentiment for the given stock. It also provides the latest five tweets as a reference. E. nasdaq The nasdaq component is primarily used for preparation of data. It combines the nasdaq listed stocks with the dataset we used for identifying various feature parameters of a stock. By cleaning and combining our data sources, we were able to create a final dataset of 1462 stocks for use across our systems in our app (analyst ratings, twitter sentiment, recommender system). F. batchRatingProcess The batchRatingProcess is used to update ratings of all 1462 stocks in background. This is to ensure that the user does not face any significant delay in fetching the data from third party websites while interacting with our SRE app's UI. In addition, the ratings of a stock is not updated frequently and batch processing fits well from a requirements and solution design perspective. G. batchRecoDataPreparation The batchRecoDataPreparation is another batch process to prepare the data for the recommendation system as the recommendation engine relies on an assortment of analyst ratings of stocks as a feature in the similarity function. Implementation details Rating System A. Approach To calculate the ratings, the following algorithm is used: * First of all, rating for the selected(searched) stock is searched in the ratings database. o Case 1: ratings data is already available in the ratings database. In such a case, ratings data is fetched from the database. o Case 2: ratings data is not available in ratings database. In this case, following step is executed to get the ratings. # Read the website marketbeat.com which lists the ratings from various market analysts and scrape the page section containing ratings relevant date # Data is cleansed, validated and structured as per requirement # In case of more than 10 ratings, only latest 10 ratings are selected. # Initially data from only current and previous months were considered but to increase the data quantity, this restriction is switched off in final product # Final list of ratings is stored in the database * Once the rating data is available, ratings from various market analysts are scaled to following ratings scale: {-1: SELL, 0: HOLD, 1: BUY} * Finally, overall rating is calculated based on the selected scaled ratings with possible outcome as SELL, HOLD or BUY. B. Webpage Scraper Webpage scraper was developed using the learnings from MP 2.1 of the course CS 410: Text Information System. Python library BeautifulSoup and Chrome driver were primarily used to fetch the web document. Following URL builder code were used to get URL for various stocks: # MarketBeat URL builder def get_mb_url(market, stock_symbol): base_url = 'https://www.marketbeat.com/stocks/{market}/{stock_symbol}/price-target/' final_url = base_url.format(market=market, stock_symbol=stock_symbol) return final_url e.g. https://www.marketbeat.com/stocks/NASDAQ/AAPL/price-target/ As part of the exercise, following additional aspects were required to be taken care: 1. the time delay in reading the complete web document, and 2. the URL redirects - in case a particular stock was not found on the website, website would redirect the URL request to a default page. # create a webdriver object and set options for headless browsing def load_webdriver(): if str(filePath).find(""CS410"") == -1: options = webdriver.ChromeOptions() options.add_argument('--no-sandbox') options.add_argument('--headless') options.add_argument(""--disable-dev-shm-usage"") driver = webdriver.Chrome(chrome_options=options) else: options = Options() options.headless = True driver = webdriver.Chrome(filePath / 'chromedriver', options=options) return driver # read web document using beutifulsoup def get_js_soup(url_web, driver): driver.get(url_web) time.sleep(5) if url_web != redirected_url: print(""redirected URL : "" + redirected_url) new_url = redirected_url + ""price-target/"" print(""new URL for Analysts rating : "" + new_url) if new_url.find('NASDAQ/price-target/') == -1: driver.get(url_web) else: return Using the html div class identifier, relevant data were extracted from the webpage data and subsequently processed in panda dataframe. table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-left-column fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table fixed-header""}) if table is None: table = soup_obj.find(""table"", attrs={""class"": ""scroll-table sort-table""}) if table is None: print(stock_symbol + "": No table found for market analyst rating"") else: table_body = table.find('tbody') rows = table_body.find_all('tr') for row in rows: cols = row.find_all('td') cols = [ele.text.strip() for ele in cols] data.append([ele.split("""")[-1].strip() for ele in cols if ele]) C. Uniform Scaling of Ratings Uniform scaling of ratings was required to unify the ratings from various market analysts and ensure that the calculation of overall rating is without any bias and is not affected by some higher or lower ratings from some market analysts, only due to the fact that they use different scales for rating a stock. Various ratings were mapped to {-1: SELL, 0: HOLD, 1: BUY} using following mapping dictionary is used. ratings_dict = { ""SELL"": -1, ""STRONG SELL"": -1, ""BEARISH"": -1, ""UNDERPERFORM"": -1, ""SECTOR UNDERPERFORM"": -1, ""MODERATE SELL"": -1, ""WEAK HOLD"": -1, ""UNDERWEIGHT"": -1, ""REDUCE"": -1, ""HOLD"": 0, ""NEUTRAL"": 0, ""AVERAGE"": 0, ""MARKET PERFORM"": 0, ""SECTOR PERFORM"": 0, ""SECTOR WEIGHT"": 0, ""PEER PERFORM"": 0, ""EQUAL WEIGHT"": 0, ""IN-LINE"": 0, ""MARKET OUTPERFORM"": 1, ""OUTPERFORM"": 1, ""MODERATE BUY"": 1, ""ACCUMULATE"": 1, ""OVER-WEIGHT"": 1, ""OVERWEIGHT"": 1, ""STRONG-BUY"": 1, ""ADD"": 1, ""BULLISH"": 1, ""BUY"": 1, ""POSITIVE"": 1, ""STRONG BUY"": 1, ""TOP PICK"": 1, ""CONVICTION-BUY"": 1, ""OUTPERFORMER"": 1 } There were other options also considered e.g., scaling of all ratings on a scale of 1-5 or 1-3, weighted scale to reflect strong ratings e.g., strong buy or strong sell. However, based on our need to combine the analyst rating with twitter sentiment, scaling to {-1: SELL, 0: HOLD, 1: BUY} were selected for this exercise. D. Overall Ratings Calculation First of all, aggregated ratings of all ratings from various market analyst is calculated based on arithmetic mean of all selected scaled ratings. e.g., aggregated rating for following five scaled ratings {-1: SELL, 1: BUY, 0: HOLD, 1: BUY, -1: SELL} from various market analyst will be {0: HOLD}. Once the aggregated rating is calculated, it is combined with Twitter Sentiment analysis result as per below table for Overall ratings. Aggregated Rating Twitter Sentiment Overall Ratings -1: SELL POSITIVE HOLD -1: SELL NEUTRAL SELL -1: SELL NEGATIVE SELL 0: HOLD POSITIVE BUY 0: HOLD NEUTRAL HOLD 0: HOLD NEGATIVE SELL 1: BUY POSITIVE BUY 1: BUY NEUTRAL BUY 1: BUY NEGATIVE HOLD Sentiment Analysis Sentiment Analysis, also known as Opinion Mining, refers to the use of Natural Language Processing to computationally determine opinions and emotions of an opinion holder for an opinion target. A common use for this technology is to discover how people feel about certain topics, particularly through users' textual posts in Social Media space. To perform Sentiment Analysis to provide stock recommendation, Twitter has been considered as the Social Media space where users post their opinion as their tweets. As most of the elements in the opinion representation such as the opinion holder (Twitter users in this case) and opinion target (Stock to be recommended in this case) and the content and the context of the opinion (financial context) are already known, the main task is to decide opinion sentiment. So, this is a case of just using sentiment classification for understanding opinion where the input is opinionated text object, the output is a sentiment label i.e., polarity analysis with predefined categories such as positive, negative, or neutral. For this project, the sentiment analysis has been done through the process outlined below. Process Description A. Preparing the Data Set To build the model, training and testing data set is needed. Ideally for optimal performance the financial tweets need to be downloaded from tweeter and should be human evaluated to create the labels which can be used for training the model and later for testing the model. However, one needs to spend considerable amount of manual effort to build such data. To optimize time and resource for this project, a readily available downloadable training set (polarity dataset from Cornell university -) has been used. The data set contain 2000 processed down-cased text files used in Pang/Lee ACL 2004 [1]; the names of the two subdirectories in that folder, ""pos"" and ""neg"", indicate the true classification (sentiment) of the component files according to the automatic rating classifier the tweets of the data set have been all labeled as positive or negative, depending on the content. The data set have been persisted into pickle file (a binary representation of python structure) to optimize performance of the subsequent run to build the classifier. import pickle from sklearn.datasets import load_files #nltk.download('stopwords') #Import Dataset -> generate two classes one each for each sub directorties dataset = load_files('txt_sentoken/') X,y = dataset.data, dataset.target #store as pickle file, these are byte type file with open('X.pickle', 'wb') as f: pickle.dump(X,f) with open('y.pickle', 'wb') as f: pickle.dump(y,f) The same data has been split into training and testing data set following a 80-20 rule where 80% of the downloaded pre-labeled data has been used as training data set and 20% of the same downloaded data has been used as testing data set. B. Preprocessing the Data Set The downloaded data set has been preprocessed before feeding into the program to create the Classifier to remove all the non-word characters, to convert into lower case, to remove single characters (e.g. i, a etc.). import re import pickle with open('X.pickle', 'rb') as f: X=pickle.load(f) with open('y.pickle', 'rb') as f: y=pickle.load(f) corpus = [] for i in range(0,len(X)): data = re.sub(r'\W', ' ', str(X[i])) data = data.lower() data = re.sub(r'\s+[a-z]\s+', ' ', data) data = re.sub(r'^[a-z]\s+', ' ', data) data = re.sub(r'\s+', ' ', data) corpus.append(data) C. Building the BOW, TF-IDF and Logistic Regression Classifier Scikit-learn library (a free machine learning library for Python) has been used to create the Classifier. At first, the bag of words model has been created and later the bag of words model would be converted into TF-IDF model. To covert the data into bag of words model, classes from Scikit-learn has been used. First, a max feature has been set to 2000 which means 2000 most frequent words would be used as features. The min document frequency would ensure to exclude a word to be considered as 2000 features which appear 3 or less documents (to prevent a word to become a feature which is rare into the set but very popular within a certain document) and the max document frequency would ensure to exclude a word to be considered as 2000 features which appear 60% or more documents (to exclude the most common words like the, an etc.). Then Stop words has been removed that is defined in nltk corpus. Now to form the Bag of Words model the corpus from above steps has been used. from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer from nltk.corpus import stopwords nltk.download('stopwords') vectorizer = CountVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X = vectorizer.fit_transform(corpus).toarray()#will generate 2D array [len(X),len(X)], total number of docs = number of features = len(x) Finally, TfidfTransformer from sklearn is used to create TF-IDF model from Bag of Words model created earlier. from sklearn.feature_extraction.text import TfidfTransformer # convert BOW to TF-IDF transformer = TfidfTransformer() X = transformer.fit_transform(X).toarray() Logistic Regression (a Discriminative Classifier) is a classification algorithm (learning algorithm) which is used for binary classification problem. The task of sentiment analysis for this project can be thought as a binary classification problem where the goal is to predict positive or negative sentiment from a given tweets (a sentence in particular). Hence for the purpose of this project, a logistics regression classifier has been built where negative and positive sentiments have been denoted as 0 and 1 respectively. So, the Binary Response Variable Y Ie {0,1} needs to be calculated from the predictor X where X = { x1, x2 ..... x2000} (all the 2000 features) Hence, for Logistics Regression can be represented as below: So essentially using training data T, , parameters (M=2000) needs to be estimated. Hence the conditional likelihood estimate is: and The goal of Logistic Regression algorithm is to optimize the parameters using training data set. Once the optimal values of the parameters are found by the algorithm, y can be calculated for any unknown sentiment of a new sentence. If y>=0.5, then that sentiment is positive sentiment and if y<0.5, then that sentiment is negative sentiment. For this project, LogistricRegression from sklearn has been used to build the classifier. First the input data set has been split as - 80% of available data has been considered as training data and 20% of the available data has been considered as test data from sklearn.model_selection import train_test_split text_train,text_test,sentiment_train,sentiment_test = train_test_split(X,y,test_size=0.2,random_state=0)#80% training and 20% testing data Then classifier is built: from sklearn.linear_model import LogisticRegression # create the classifier using logistic regression classifier = LogisticRegression() classifier.fit(text_train,sentiment_train) Just to showcase the model performance, confusion_matrix class from sklearn has been used. With the input data, close to 85% accuracy has been achieved. from sklearn.metrics import confusion_matrix #testing model performance sentiment_prediction = classifier.predict(text_test) cm = confusion_matrix(sentiment_test,sentiment_prediction)# [[predicted as 0 and actually 0, predicted as 0 and actually 1], #[predicted as 1 and actually 0, predicted as 1 and actually 1]] print(""accuracy : "", (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,""%"") ''' [model predicted 0and actual 0 model predicted 1 nand actual 0 model predicted 1 and actual 0 model predicted 1 and actual 1] ''' Finally, the model and vectorizer have been stored as pickle file (binary representation of python objects) so that while calculating the real time tweets, the saved model can be used as it is. from sklearn.feature_extraction.text import TfidfVectorizer #store the classifier ....pickle file with open('classifier.pickle', 'wb') as f: pickle.dump(classifier,f) #store the TFIDF vectorizer vectorizerTFIDF = TfidfVectorizer(stop_words=stopwords.words('english'), max_df = 0.6, min_df = 3, max_features = len(X)) X_TDIDF = vectorizerTFIDF.fit_transform(corpus).toarray() with open('vectorizerTFIDF.pickle', 'wb') as f: pickle.dump(vectorizerTFIDF,f) D. Fetching the real time tweets A developer app has been created in Twitter for the purpose of this project. The OAuth2 mechanism has been used to make API calls to Twitter API for fetching the recent tweets. ConsumerKey and ConsumerSecret from the created app are used to generate bearer token in the runtime API call using https://api.twitter.com/oauth2/token Finally, the bearer token is used to call API to perform recent search (returns Tweets from the last 7 days that match a search query) using ""recent search"" API. https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get- tweets-search-recent For this project, Max result of 100 has been set to perform recent search. This is to ensure throttling of number of results fetched (as for the basic access, twitter account has 500000 search results/month limitation) E. Performing Sentiment analysis of the fetched tweets To perform sentiment analysis in real time, firstly the Logistic Regression Classifier model and TF-IDF vectorizer is loaded from the saved pickle file. import pickle import re with open('classifier.pickle', 'rb') as f: clf=pickle.load(f) with open('vectorizerTFIDF.pickle', 'rb') as f: vectorizer=pickle.load(f) For a given stock the ""recent search"" API is used to search recent tweets. After fetching related tweets (max = 100), each tweet is preprocessed to create bag of words and to be represented as TF-IDF vectorizer. Then, prebuilt Logistic Regression Classifier is used to predict the sentiment of each tweets. for t in tweets_fetched: t = re.sub(r'^https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*\s',' ',t) t = re.sub(r'\s+https://t.co/[a-zA-Z0-9]*$',' ',t) t = t.lower() t = re.sub(r""that's"",'that is',t) t = re.sub(r""there's"",'there is',t) t = re.sub(r""what's"",'what is',t) t = re.sub(r""where's"",'where is',t) t = re.sub(r""it's"",'it is',t) t = re.sub(r""who's"",'who is',t) t = re.sub(r""i'm"",'i am',t) t = re.sub(r""she's"",'she is',t) t = re.sub(r""he's"",'he is',t) t = re.sub(r""they're"",'they are',t) t = re.sub(r""who're"",'who are',t) t = re.sub(r""shouldn't"",'should not',t) t = re.sub(r""wouldn't"",'would not',t) t = re.sub(r""couldn't"",'could not',t) t = re.sub(r""can't"",'can not',t) t = re.sub(r""won't"",'will not',t) t = re.sub(r'\W', ' ', t) t = re.sub(r'\d', ' ', t) t = re.sub(r'\s+[a-z]\s+', ' ', t) t = re.sub(r'\s+[a-z]$', ' ', t) t = re.sub(r'^[a-z]\s+', ' ', t) t = re.sub(r'\s+', ' ', t) sentiment = clf.predict(vectorizer.transform([t]).toarray()) Finally, total positive and negative sentiments are calculated for total tweets fetched in runtime and the final sentiment of a particular stock has been calculated as positive = 1 (if the positive percentage > 65%), negative = -1 (if the positive percentage < 35%) or neutral = 0 (if the positive percentage is in between 35% and 65%). if (tot_positive+tot_negetive)>0 : positive_percentage = tot_positive/(tot_positive+tot_negetive) print(""positive_percentage :"",positive_percentage*100, ""%"") if positive_percentage>0.65 : print(""stock is buy"") return 1 elif positive_percentage<0.35 and positive_percentage>0: print(""stock is sell"") return -1 else: print(""stock is neutral"") return 0 Following end point has been built to provide twitter sentiment analysis result to a stock. Definition * Get Twitter sentiment for given stock - 1: Positive, 0: Neutral, -1: Negative GET /stock/sentiments/<market>/<stock_symbol> e.g. http://localhost:5000/stock/sentiments/NASDAQ/AAPL Response * 200 OK on success json { ""stockSymbol"": ""AAPL"", ""refreshDate"": ""2020-11-25"", ""sentiment"": 1 } Sample run: Recommender System Approach The basic approach to the recommendation engine is to use a similarity function to compare pre-identified features of all stocks in our corpus against user-provided stock symbol features. While doing so, we are implementing one of the methods for building recommendation engine - Content based recommendation or Item similarity, which was part of Week 6 lecture of course CS 410 Text Information System. Recommender System recommends to the user the top 5 stock symbols which are most similar to the user provided stock symbol in terms of those underlying features and the similarity function. For the initial, Cosine similarity is used as similarity function and the following financial/economic features were selected as underlying features for similarity calculation. 1. company is in S&P 500 2. company profitability over last three years 3. revenue growth for last three years 4. current market analyst ratings 5. sector 6. gross profit per market cap Analyst ratings are calculated by rating_system and same is reused also for recommendation system. Other features for the stock are calculated or derived using the financial data of the companies from year 2018. Source: Kaggle dataset available at https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018. As part of the exercise, significant amount of effort was used to identify a single data source for financial or economic data and also to understand various financial terms to find the right feature. Based on our current understanding of a company performance and its relation to stock price, we used above listed features. The features might be further optimized with right guidance from a financial analyst or person with insight of stock market and factors influencing the investor decisions. Libraries and Datasets Python Libraries Flask flask_cors markdown beautifulsoup4 selenium pandas pymongo tinydb misaka nltk requests sklearn flask-jsonpify Datasets * https://www.marketbeat.com/ * https://www.cs.cornell.edu/people/pabo/movie-review-data/ * https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks- 20142018?select=2018_Financial_Data.csv * https://developer.twitter.com Verification The result of the recommender system was verified based on manual verification process by selecting some stocks at random and checking the result for individual section e.g., ratings, sentiments and overall ratings. For example, Overall rating for Facebook (FB) stock is BUY. Looking at individual ratings component, the result seems reasonable. Analyst Ratings: BUY Verification: In the list of market analyst ratings, all 10 analysts have rated it as BUY. Therefore, the aggregated rating is BUY. Tweet Sentiments: Neutral Verification: From the latest 100 tweets fetched (as shown in below screenshot), it seems reasonable to have a NEUTRAL sentiment Overall Rating: BUY Verification: Combining Analyst Ratings ""Buy"" with Twitter Sentiments ""NEUTRAL"", as per listed decision table, the result is BUY. Conclusion and Future Work Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating (e.g., buy, hold, sell) for the user entered Stock symbol and recommending five similar stocks. The result has been validated by some popular stocks. One of key challenges faced in building the system is the access on right financial data as well as good training and testing set to train and test the model for sentiment analysis. The choice we had to manually create the required data. However, to optimize the time and resource available, it has been decided to use readily available data in the web which might have not yielded the perfect recommendation. There are several interesting directions for the future version of recommender system. First, the overall functionality can be improved by considering user input of sectors and providing recommendation and trends specific to that sector. A second direction involves defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market and use the same to train and test the model. Given the current trend of machine learning algorithms, a third interesting research direction is to explore the timeseries data for stock adjacency. Finally, overall score can be improved further to consider more analysts' reports and microblogging websites and come up with more recommender categories i.e., Strong Buy, Buy, Hold, Sell, Strong Sell etc. At the end, it has been a great journey of ideation and learning in a collaborative manner to design and develop the stock recommender system. We thank different analysts' websites to make their ratings available publicly and twitter to grant access of real time tweets through their public API and last but not the least, we thank professor ChengXiang Zhai and all our TAs and all the reviewers to give the direction needed and to provide the valuable feedback. References [1] Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL '04). Association for Computational Linguistics, USA, 271-es. DOI:https://doi.org/10.3115/1218955.1218990 [2] Carbone, N. (2020, January 18). 200+ Financial Indicators of US stocks (2014-2018). Retrieved December 11, 2020, from https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018"
https://github.com/MLwithSandy/CourseProject	ProjectStatusReport_StockRecommenderSystem.pdf	"Project Status Report: Stock Recommender System [ ] [ ] [ ] [ ] [ ] [ Team Members Project Overview Project Status Project Challenges Proposed resolution Next ] Steps Team Members Name NetID Email ID Ezra Schroeder ezras2 ezras2@illinois.edu Rasbihari Pal pal9 pal9@illinois.edu Sandeep Kumar kumar64 kumar64@illinois.edu Team captain marked in BOLD. Project Overview We are building a product ""Stock recommender system"", which will enable retail investors to make informed decision about their investment choices, based on market analysts ratings and market/investors sentiments at the moment. Based on user's preference of a stock, the product recommends 5 stocks which closely matches the user's preferred stock and ratings for them. The recommender system combines rating data from various market analyst, market sentiments using microblogging data (using twitter) and company profile for determination of the overall rating and curation of the recommendation list. Project Status The team has been making steadfast progress in various topics in order to deliver the project on time by the final project submission deadline. Progress on each individual topic can be seen in below actualized project planning chart. Percentage completion of the tasks under individual topics is mentioned in brackets () next to each topic name in above planning chart. Project Challenges At the conception of the project, we had foreseen some challenges but during the execution, we faced several challenges, which we had either not foreseen or assumed as minor challenges. Below, we list of the challenges which required significant time, effort and energy from all of us. Working with Twitter API and gathering training and test data for the model we prepared for sentiment analysis Collecting good quality financial data of various companies Scraping of market analysts websites Technical limitation to process the data Above challenges are in addition to a major hurdle related to working in a team with members sitting across Atlantic Ocean and having a time difference of 7 hours. Thanks to team members flexibility, team agility and technology, this challenge did not pose a big threat to our continuous progress. Proposed resolution We, as a team, invested significant amount of time working on above listed challenges. Some of the challenges we could overcome with sustained effort but for some of the challenges we had to find a middle ground to meet the project objective of delivering a 'reasonably' working solution and hoping to improvise the solution over time with larger effort. Working with Twitter API and gather training and test data for the classifier It required significant effort and multiple round of communication with Twitter team to get access to the Twitter API - an important part of our proposed solution for performing the market sentiment analysis for a given stock. Eventually we received the access but with restriction on the volume of tweets we can pull in a month. For training and test data, we used public data available in internet. We used the following polarity data set: https://www.cs.cornell.edu/people/pabo/movie-review-data/ This helped us to avoid the significant manual effort for downloading the twitter data and possibly human labeled them. Collecting good quality financial data of various companies For building a good recommendation engine, access to financial data of various companies is required. We could not find a single source with latest financial data for the listed companies. To avoid the trap of spending significant time and energy to resolve the issue, we eventually settled with a kaggle data source having financial data from 2018. But the solution is being designed in a way that we can update the data any time to reflect current reality. Scraping of market analysts websites Our initial idea was to scrape websites and reports from various market analysts for their ratings to various stocks. After collating the information for couple of stocks, we realized the vastness of this topic. For each stock, we needed to scrape data from various market analysts and each report following free text structure. For our project, we needed access to the assigned rating from various market analysts and we settlement with an established website, which collects and publishes such ratings in structured format. Technical limitation to process the data At the moment we have collected financial data for more that 1800 companies. To scrape the analyst ratings and get twitter feed for all these companies and then processing them for final result would require significant time and processing infrastructure. As a realistic approach, we have decided to limit the scope of the project to a limited list of stocks. This limit is only for practical purpose only. However, the solution is capable to manage more volume of data with parallel and batch processing. Next Steps At the moment, we do not see any major technical challenge in our way in coming days. We are planning to complete the remaining tasks from all topics and focus on final project delivery on time."
https://github.com/MLwithSandy/CourseProject	README.md	CourseProject: CS410 Text Information System, UIUC Project Topic: Stock Recommender System Team Members |--|---------------|----------|---------------------| | # | Name | Net ID | Email Id | | --- | --------------- | ---------- | --------------------- | | 1. | Ezra Schroeder | ezras2 | ezras2@illinois.edu | | 2. | Rasbihari Pal | pal9 | pal9@illinois.edu | | 3. | Sandeep Kumar | kumar64 | kumar64@illinois.edu | | -- | --------------- | ---------- | --------------------- | Project Documentation Project Final Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectReport_StockRecommenderSystem.pdf Software usage tutorial presentation video https://mediaspace.illinois.edu/media/1_zbfjmw9g Project Presentation - Powerpoint https://github.com/MLwithSandy/CourseProject/blob/main/StockRecommendationEngine_V1.3.pptx Technical Set-up Guide https://github.com/MLwithSandy/CourseProject/blob/main/Technical_set-up_guide.pdf User Guide https://github.com/MLwithSandy/CourseProject/blob/main/UserGuide.pdf Project Status Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectStatusReport_StockRecommenderSystem.pdf Project Proposal https://github.com/MLwithSandy/CourseProject/blob/main/ProjectProposal_StockRecommenderSystem.pdf
https://github.com/MLwithSandy/CourseProject	StockRecommendationEngine_V1.3.pptx	"Stock Recommendation Engine - A consolidated recommender Bringing market within the reach of commons 1 PROBLEM Lack of consolidated recommender More retail investors due to availability and easiness of trading platform The retail investor of today lacks time for in-depth research Investors lacks necessary knowledge to analyze the financial standing of a company. Too many analysis/reports in the web Stream of information in modern era of social computing 12.08.2020 2 12.08.2020 3 SOLUTION One stop window to provide stock recommendation Enable retail investors easy access to information to make informed investment decision The recommender system combines stock rating data from various market analyst, market sentiments in social media (Twitter) company profile and financial attributes 12.08.2020 4 System Design PRODUCT Analyst reports ratings_system is the component which calculates the overall rating for the user selected(searched) stock, by scraping the required ratings data from market analyst website and aggregating them. Market Sentiments TwitterSentimentAnalysis component fetches latest tweets for the user selected(searched) stock from Twitter used Twitter API, performs sentiment analysis and return overall sentiment for the given stock. Financial attributes of the company recommender_system takes a stock symbol as input and recommends 5 stocks matching the profile of the user selected(searched) stock and a pre-defined similarity function. 12.08.2020 5 Three aspects of parameters 12.08.2020 6 Project Documents User Guide https://github.com/MLwithSandy/CourseProject/blob/main/UserGuide.pdf Technical Set-up Guide https://github.com/MLwithSandy/CourseProject/blob/main/Technical_set-up_guide.pdf Project Report https://github.com/MLwithSandy/CourseProject/blob/main/ProjectReport_StockRecommenderSystem.pdf 12.08.2020 7 Installation & Setup Install Docker on your local machine https://docs.docker.com/get-docker/ Download the project from github https://github.com/MLwithSandy/CourseProject Go to the folder SRE: CourseProject > ProjectCode > SRE To start the Stock Recommendation System, run the start scripts start.bat for windows start.sh for linux To stop, run the stop scripts stop.bat for windows stop.sh for linux 12.08.2020 8 How to Use Open browser, preferably Chrome or FireFox Go to http://localhost:8080/home to access Stock Recommendation System UI Enter any stock symbols e.g. AAPL (NASDAQ Market) in the search box and click ""Search"" Stock Recommendation Engine will show Analyst ratings Twitter Sentiment Ratings Overall ratings Most recent five tweets Similar stock recommendation Use http://localhost:5000/ to see list of Rest API end points the product 12.08.2020 9 Live Demo - Use Case Data Used - Acknowledgements 10 https://www.marketbeat.com/ https://www.cs.cornell.edu/people/pabo/movie-review-data/ https://www.kaggle.com/cnic92/200-financial-indicators-of-us-stocks-20142018?select=2018_Financial_Data.csv https://developer.twitter.com OUR TEAM Sandeep Kumar Rasbihari Pal pal9@illinois.edu Ezra Schroeder ezras2@illinois.edu 12.08.2020 11 kumar64@illinois.edu Conclusion Future Work Considering user input of sectors and providing recommendation and trends specific to that sector Defining the test and train data and possibly human labeling twitter feed (for sentiment analysis) just for tweets related to stock market Explore the timeseries data for stock adjacency. Combine more analyst report and microblogging sites 12.08.2020 12 Overall, the Stock recommending system as described in this project report has yielded satisfactory result in recommending rating for the user entered Stock symbol and recommending five similar stocks. THANK YOU! 13"
https://github.com/MLwithSandy/CourseProject	Technical_set-up_guide.pdf	Technical set-up guide Stock Recommender System is packaged as docker images to ensure that the all its component can run on any machine, independent of OS e.g. Windows or Linux and without any need for special configuration and additional software dependencies. Listed below are the only two pre-requisites in order to set-up and run Stock Recommender System on your local machine. 1. Access to internet 2. Docker CLI or Docker Desktop installed on the local machine In case, you do not have Docker CLI or Docker Desktop installed on your local machine, you may install Docker Desktop from docker website: https://www.docker.com/products/docker-desktop Once above listed pre-requisites are fulfilled, you can proceed with following five steps in same sequence. Step 1: Download relevant scripts from project github Please download either the complete project or only the SRE directory from the GitHub repo: https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode SRE folder consists of following files / scripts, which are relevant to set-up and run the Stock Recommendation System: You may follow readme.md at GitHub repo additional details. https://github.com/MLwithSandy/CourseProject/tree/main/ProjectCode/SRE/readme.md Step 2: Run start script Depending on your operating system, please run start script in SRE directory as mentioned below: Windows: run start.bat in command prompt Mac OS, Linux: run start.sh in terminal The start script downloads docker images for Frontend and Backend components of Stock Recommender System from Docker Hub. Docker images are published by Stock Recommender System project team as a public repository. Size of docker images: sre-backend: 700 MB (approx.) sre-frontend: 18 MB (approx.) Depending on the speed of internet, it may take a while to download the two images. Once the two images are downloaded, the start script will run the docker images as docker container on your local machine. Step 3: Access the Stock Recommender System UI To access the recommender system UI, please open following link in your internet browser, preferable Chrome or Firefox. http://localhost:8080/home For details of the UI, please refer to User Guide. Step 4: Access the Stock Recommender System Rest APIs This is an optional step and required only when you are interested in using the Rest services. Please follow below link to access the list of Rest APIs exposed from sre backend. http://localhost:5000/ Step 5: Stop the Stock Recommender System components In order to stop the backend and frontend components of Stock Recommender System, please run stop script in SRE directory, corresponding to your operating system.
https://github.com/MLwithSandy/CourseProject	UserGuide.pdf	User Guide Welcome to the user guide of our Stock Recommender System. Before you start, please ensure that following pre-requisites are fulfilled: 1. Stock recommender backend component is running either on a server or your local machine 2. Stock recommender frontend component is running either on a server or your local machine User Screen When you go to the URL of Stock recommender system (e.g. http://localhost:4200/home), you will see a page such as below. Stock recommender system shows you overall rating and recommendation for Apple Inc (Stock symbol: AAPL) by default. You may enter a stock of your choice in given search area and click on Search. Stock recommender system will provide you all necessary information about searched stock and also recommend stocks similar to the one you are interested in. Various sections of the screen are described below: Overall Rating Overall Rating consists of three parts. * Analyst Rating Analyst rating is calculated based on the ratings of various market analysts, who publishes such ratings e.g. buy, sell, hold etc. for various listed companies. The recommender system aggregates last 10 ratings from various analysts to calculated weighted analyst rating. * Twitter Sentiments Current market sentiment is determined based on the Twitter trends. Recommender system fetches recent tweets from Twitter and perform sentiment analysis to provide current market sentiment for the given stock. * Overall Rating Recommender system combines analyst rating and twitter sentiment to provide you with an overall ratings of the stock you are interested in. Based on the recent market trend, both the parameters have been given equal weightage for calculation of overall ratings. Please refer to below table to understand the calculation of overall ratings. Aggregate rating based on various market analysts Market Sentiment Overall rating Buy Neutral Buy Buy Negative Hold Buy Positive Buy Hold Neutral Hold Hold Negative Sell Hold Positive Buy Sell Neutral Sell Sell Negative Sell Sell Positive Hold Analyst ratings & latest Tweets Analyst ratings & latest Tweets consists of following three section. * Rating chart Rating chart shows the distribution of ratings (buy, sell, hold) among the market analysts based on recently published ratings * Analysts rating Analysts rating shows the details - ratings (buy, sell, hold), rating agency (market analyst) and date of publication of the rating * Tweets List of latest 5 tweets (indicative only). Market sentiment is calculated on many more tweets. Similar stocks Similar stocks show list of 5 stocks which are similar to the one you are interested in. Similar stocks are identified using a similarity function based on following parameters. 1. company is in S&P 500 2. company profitability over last three years 3. revenue growth for last three years 4. current market analyst ratings 5. sector 6. gross profit per market cap
https://github.com/cf16-uiuc/CourseProject	Progress Report.pdf	Progress Report For my project I am working on the classification competition. I have made some significant progress on it over the last few weeks. The tasks I have completed are preprocessing the text - cleaning it up, converting emojis into text, and preparing the data to be used for training the model. I have also spent a lot of time reading about BERT and have trained an initial model using BERT. Tasks going forward are testing my code with the test data provided to see how my initial model performs to the baseline and optimizing the model to improve performance. Depending on how that performance goes I may explore other options for training the model as well. I then need to work on creating the documentation and cleaning up and organizing the code I have written. I am currently not facing any challenges as I work on my project.
https://github.com/cf16-uiuc/CourseProject	Project Proposal.pdf	Project Proposal CS410 Fall 2020 What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will be working on this project by myself. As the only team member I will be the captain. Name: Christine Frandsen NetID: cf16 Which competition do you plan to join? I am planning to join the text classification project If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods. Yes, I am excited to learn more about state-of-the-art neural network classifiers. Some neural classifiers I am looking at are the perceptron network, a simple model to set a baseline, an extreme learning machine (ELM) network, and a support vector machine (SVM), although this is not generally considered a neural network. I have had a few experiences with perceptron networks and SVMs and am eager to expand my knowledge to other neural network classifiers. In terms of deep learning frame works I have worked a little bit with TensorFlow and Keras, but am a little rusty. I am hoping to incorporate those as well as potentially bringing in PyTorch as well. Which programming language do you plan to use? I plan to use Python for this project.
https://github.com/cf16-uiuc/CourseProject	README.md	Text Classification Competition: Twitter Sarcasm Detection This project is for CS410 during the fall 2020 semester at UIUC. More details about this competition can be found at: https://github.com/CS410Fall2020/ClassificationCompetition Implementation Overview This classifier relies on the Simple Transformer package, based on the Transformer package from HuggingFace. The documentation for Simple Transformer can be found here: https://simpletransformers.ai/docs/classification-models/#classificationmodel. All packages needed can be installed with the package manager pip. The code is broken into two files - train.py and test.py. In train.py the focus of the file is training the model. Prior to training the model we do a little bit of preprocessing of the data. First, we remove all stop words, then we replace all emoticons with text that may be able to provide information to the model that is trained. For the training we rely on the Simple Transformer package. From that package we use the binary classifier. Their setup allows us to bring in any pretrained model and adjust to our data. We looked at multiple pretrained models from HuggingFace found here: https://huggingface.co/models. In the end we discovered that simply using the BERT model yielded the best results. To apply labels to a new set of tweets we can run the test.py file. This does a similar process of converting emoticons to text and removing stop words to preprocess the data. We then read in our trained model from train.py and can use the predict() function to predict the new labels. Running the Code To run the code you need to ensure that all the required packages are installed. The code can be run from the command line by running either python train.py or python test.py, depending on which file needs to be run. The code can also be run from python IDEs. The different variables, such as file name for training data, number of epochs, or learning rate can be adjusted within the file at the beginning of the file. The end result of running train.py will be created in a folder titled outputs. The test.py file reads from the outputs folder and will output and an answer.txt file. For a detailed overview of running the code, an instructional demo can be found here: https://youtu.be/hzyMMAHryAE Functions The code contains three main functions - convert_emojis (appears in both files), bert_training (train.py), and predict_sarcasm (test.py). convert_emojis(text): This function takes a single input, text. It cycles through the list of known emojis, looking for them in the text and replacing them to allow for consistency. bert_training(model_type, model_base, train_data, early_stop, early_stop_delta, overwrite, epoch, batch_size, learning_rate, output): This function is where the model is trained. Several inputs are required, all of which are defined at the beginning of the file. This allows for models to be trained for different model types, varying epochs, and different batch sizes easily. Also allows a user to specify where the output should be written out to, making it easy to save several different models without worrying about overwriting existing models. predict_sarcasm(data_path, results, model_loc, model): This function takes a model and input text and generated labels for whether or not a tweet is sarcastic or not. The inputs for this function allow the user to specify the location of the data, what the results file should be called, where the model is located, and what type of model it is. This allows the user to easily chagne various parameters to compare different models performance. Parameter Tuning and Model Exploration The main parameters that we focused on for this project were learning rate, batch size, number of epochs, and base models. We explored using Roberta, XLNET, and Electra, before finally deciding on BERT. We also looked at using an ensemble method combining results from BERT, Electra, and Roberta. However, those results were below the baseline, so we opted to just use the BERT model. We also looked at batch size. We found that at batch size of 100 performed well. The Classic Transformer package used 8 as default. This did well, however, not well enough to beat the baseline. When we increased the batch size by much more, we found that the model tended to predict everything to be sarcastic. Notes The final trained model was not uploaded to git. Please reach out to me if you would like to see the model or have questions about it. A similar model can be generated by first running train.py before running test.py. Useful Links and Sources https://huggingface.co/models https://simpletransformers.ai/docs/binary-classification/ https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3 https://github.com/ThilinaRajapakse/simpletransformers
https://github.com/seuaciuc/CourseProject	CS410 Fall 2020 Course Project Progress Report.pdf	"1 CS410 Fall 2020 Course Project Progress Report Author: Thiago Seuaciuc-Osorio E-mail: thiagos2@illinois.edu NetID: thiagos2 Team: Individual Programming Language: Python Topic: Reproducing the following paper on Latent Aspect Rating Analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 Completed Tasks The following initial tasks have been completed: * Reviewed paper, identified tasks and planned approach. * Obtained both relevant datasets (hotel and MP3 reviews). * Extracted and reviewed both datasets to understand format and accompanying metadata. Future Tasks * Fully ingest data and implement same pre-processing steps listed in the paper. After this step, it is expected that the dataset statistics will resemble that reported on Table 1 in the subject paper.  Expected timeframe: complete by December 2 * Implement posterior inference and model estimation steps (sections 4.1 and 4.2 of subject paper).  Expected timeframe: complete by December 7 * Apply the topical Latent Aspect Rating Analysis Model (LARAM) to both datasets (hotel and MP3 reviews) and calculate selected relevant metrics for assessment. Only the experimental results related to the subject method (LARAM) will be reproduced; experimental results obtained with other methods for comparison and reported in the paper (such as LDA, sLDA, LRR, etc) will not be reproduced. Specifically, the following results will be reproduced: o The LARAM results shown in table 2 o The LARAM results shown in table 4 o The LARAM results shown in figure 3 Two sets of results will not be reproduced: o The results in table 3, since they depend on ""ground truth"" generated by a LDA model. o The results in table 5, from the experiment where all reviews are concatenated and which was performed only to allow comparison with a Bootstrap+LRR model, since it is a degeneration of the more general case and similar metrics are already computed as part of table 4.  Expected timeframe: complete by December 10 2 * Prepare code documentation and presentation.  Expected timeframe: complete by December 14 Challenges Following the initial review and planning for the project, the following challenges were encountered: * Table 1 in the subject paper lists 2,232 different hotels reviewed. However, in reviewing the available dataset listed in the provided reference, review files for only 1,850 hotels were initially found. This does not pose a problem for the execution of the project, but it may lead to slightly different results than obtained in the subject paper. * The model equations described in section 4 of the subject paper are not fully clear or complete, as some are just referenced in other sources. These sources have been retrieved, but further study and analysis are necessary to fully determine the equations defining the LARAM model implementation."
https://github.com/seuaciuc/CourseProject	CS410 Fall 2020 Course Project Proposal.pdf	CS410 Fall 2020 Course Project Proposal Author: Thiago Seuaciuc-Osorio E-mail: thiagos2@illinois.edu NetID: thiagos2 Team: Individual Programming Language: Python Topic: Reproducing the following paper on Latent Aspect Rating Analysis Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 In this project, the Latent Aspect Rating Analysis generative model will first be implemented as described in the paper above and then applied on the same hotel and MP3 player review datasets, effectively replicating the results of the paper. The relevant datasets have been obtained from http://timan.cs.uiuc.edu/downloads.html
https://github.com/seuaciuc/CourseProject	CS410_Fall2020_CourseProject_Tutorial.pdf	CS410 Fall 2020 Course Project Thiago Seuaciuc-Osorio netID: thiagos2 Topic Paper * Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 * With considerable material from reference [3] in the paper on LDA: D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:993-1022, 2003 Framework Data Processing Model Building Analysis Raw Data Processed File (.pkl) Model File (.pkl) * Data transfer through files * No command line interface: inputs in first lines of scripts Data Processing Data Processing Raw Data Processed File (.pkl) * Input: raw file * Output: processed file * Reviews: list of dictionaries * ReviewText: review content (list of words) * 'Author' (if available) * 'Product' * 'Date' * 'Rating': list of floats; first is overall rating * Vocabulary: list of words in corpus * Term-Document Matrix: count of each term (column) in each document (row) * Processing: * Lower case = remove punctuation = word tokenize = remove stopwords (NLTK) = remove non-alphabetical terms * Filter reviews with less than 50 words, and terms appearing in less than 10 reviews * All files in the \data folder Data Processing Data Processing Raw Data Processed File (.pkl) * MP3 & Hotels reviews in different formats; need different processing * processHOTELreviews.py * processMP3reviews.py * processMP3reviews_split.py * Splits reviews based on rating (low/high) Provide name of raw input file here Provide name of output file to be created here Data Processing Data Processing Raw Data Processed File (.pkl) * Main available processed data files: * MP3reviews_low_100.pkl: processed data of a random sub- sample of 100 reviews with low rating (3 or lower). * MP3reviews_high_100.pkl: processed data of a random sub- sample of 100 reviews with high rating (higher than 3). * HotelReviews_100.pkl: processed data of a random sub-sample of 100 hotel reviews * Main available raw data files: * amazon_mp3_redux.txt: a small sub-sample of the amazon review dataset in its raw format that can be used to test the data processing codes. * Associated processing script: processMP3reviews.py or processMP3reviews_split.py * Test_redux (folder): a small sub-sample of the hotel review dataset in its raw format that can be used to test the data processing codes. * Associated processing script: processHOTELreviews.py Model Building * Follows process in subject paper: * Initialize corpus-level parameters * Compute review-level parameters * Initialize review-level parameters * Iteratively update until convergence * Corpus-level parameters are held constant in this process * Update corpus-level parameters * Review-level parameters are held constant * Recompute log-likelihood * Iterate until convergence * Input: processed data file (in \data folder) * Output: model file saved to \models folder * Corpus-level parameters * Review-level parameters Model Building Processed File (.pkl) Model File (.pkl) Model Building * estimateModel.py * Data files should be in the \data folder * Model file will be saved to the \models folder Model Building Processed File (.pkl) Model File (.pkl) Provide name of processed data file here Provide name of output file to be created here Define number of aspects here Model Building * Available models: * MP3model_low_100_3.pkl * Model built with 3 aspects on dataset with 100 MP3 reviews with low rating * Associated processed data file: MP3reviews_low_100.pkl * MP3model_high_100_3.pkl * Model built with 3 aspects on dataset with 100 MP3 reviews with high rating * Associated processed data file: MP3reviews_high_100.pkl * HotelModel_100_7.pkl * Model being built with 7 aspects on dataset with 100 hotel reviews * Associated processed data file: HotelReviews_100.pkl Model Building Processed File (.pkl) Model File (.pkl) Analysis * For various purposes * May take either or both of: * Processed data file * Model file * Available codes: * getStats.py: computes basic stats on the review data * getTopAspectWords.py: retrieves the top words of each aspect (based on aspect word distribution) Analysis Processed File (.pkl) Model File (.pkl) Analysis * getStats.py * Script will print on the terminal: * The number of reviews in the file * The number of unique items (products) reviewed * The average length (and standard deviation) of the reviews * The average and standard deviation of the overall ratings * No other outputs or files created * Data file should be in the \data folder Analysis Processed File (.pkl) Provide name of processed data file here * getTopAspectWords.py * Script print array of words to the terminal * No other outputs or files created * Data file should be in the \data folder * Model file should be in the \models folder Analysis Analysis Processed File (.pkl) Provide name of processed data file here Model File (.pkl) Provide name of output file to be created here Define number of words per aspect here Some Results - Review Statistics * Compared to the reported results (see Table 1 in subject report): * Slightly fewer reviews. For instance, Table 1 reports 2,232 reviewed hotels, but the available dataset only contains files for 1,850 (one is filtered out) * Higher average review length. This is likely because of the slightly different processing. For instance, here the NLTK stopwords were used, which will differ a little from that was used in the paper. * Ratings statistics are very similar. Dataset # Items # Reviews Avg Length (std) Avg Rating (std) MP3 676 16012 123.06 (98.00) 3.75 (1.42) Hotels 1849 47750 125.73 (99.02) 3.96 (1.22) Some Results: Top Words in MP3 Reviews Low Rating High Rating Zune Like Warranty Would Hours Software Bought Good Software Use Great Also Everything Time Work New Use One Get Product Apple Zune Itunes Like People One Buy Device Still Sound Like Get Im Great Player Quality Problem Screen Music Easy Battery Get One Use Battery Good Like Much Ipod Ipod Unit Player One Good Player player would ipod music player Some Results: Top Words in MP3 Reviews * Few similarities with Table 2 in the subject paper: * Low: problem, time, warranty * High: easy, sound, quality * Despite a few similarities, this list is mainly different from the one in Table 2 in the subject paper. Reasons could be: * Fewer aspects: the paper modeled 20 aspects and displayed the top 3, while here only 3 aspects were modeled. The higher number of aspects in the paper allow for better topic definition for each aspect, whereas here, the shown 3 topics need to account for all the content in the corpus. * The much smaller dataset. This was built on 100 reviews, while the paper used 16,680 divided between the two sub-groups (low/high rating) * The different list of stopwords; the list used here may have left more common English words than the list used in the paper. Note: time for model computation is the reason to have reduced the dataset (number of reviews) and the number of aspects modeled. Challenges * Model complexity * A number of high dimensional numerical optimizations at repeated iterations leads to high computation time to build models * This gets worse as the dataset and number of aspects increase * This has hampered the development of the model on the hotel dataset, where at least 7 aspects are needed for meaning assessment * Model build code has not finished running (on set with 100 reviews) * Not possible yet to perform the aspect rating analysis * Even when the model finishes, the low number of reviews will likely make the results not very robust * To solve this, efforts are needed for code optimization * Most of the time in the project was spent on research to understand the model and approach to be able to make an initial implementation of it * Better computation resources would also help Suggested Testing Procedure The steps below allow for all scripts provided to be tested without taking too much time. They are already set with the inputs corresponding to this, so they can be run without changes. * Data processing scripts * Run processMP3reviews.py to process the MP3 reviews in amazon_mp3_redux.txt. This will generate the MP3reviews_redux.pkl. All these files are in the \data folder. * Run processHOTELreviews.py to process the hotels reviews in the folder \Texts_redux. This will generate the HotelReviews_redux.pkl. All these files are in the \data folder. * Model building scripts * Run estimateModel.py on the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model file MP3model_redux.pkl. The suggested number of aspects is 3. * Analysis scripts * Run the getStats.py on MP3reviews_high_100.pkl to obtain the statistics on that smaller set of reviews. * Run the getTopAspectWords.py using the data MP3reviews_high_100.pkl and model MP3model_high_100_3.pkl to obtain the top 10 words in each of the 3 aspects of this model on these reviews.
https://github.com/seuaciuc/CourseProject	CS410_Fall2020_CourseProject_Tutorial.pptx	CS410 Fall 2020 Course Project Thiago Seuaciuc-Osorio netID: thiagos2 Topic Paper Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. DOI=10.1145/2020408.2020505 With considerable material from reference [3] in the paper on LDA: D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:993-1022, 2003 Framework Raw Data Processed File (.pkl) Model File (.pkl) Data transfer through files No command line interface: inputs in first lines of scripts Data Processing Raw Data Processed File (.pkl) Input: raw file Output: processed file Reviews: list of dictionaries ReviewText: review content (list of words) 'Author' (if available) 'Product' 'Date' 'Rating': list of floats; first is overall rating Vocabulary: list of words in corpus Term-Document Matrix: count of each term (column) in each document (row) Processing: Lower case  remove punctuation  word tokenize  remove stopwords (NLTK)  remove non-alphabetical terms Filter reviews with less than 50 words, and terms appearing in less than 10 reviews All files in the \data folder Data Processing Raw Data Processed File (.pkl) MP3 & Hotels reviews in different formats; need different processing processHOTELreviews.py processMP3reviews.py processMP3reviews_split.py Splits reviews based on rating (low/high) Provide name of raw input file here Provide name of output file to be created here Data Processing Raw Data Processed File (.pkl) Main available processed data files: MP3reviews_low_100.pkl: processed data of a random sub-sample of 100 reviews with low rating (3 or lower). MP3reviews_high_100.pkl: processed data of a random sub-sample of 100 reviews with high rating (higher than 3). HotelReviews_100.pkl: processed data of a random sub-sample of 100 hotel reviews Main available raw data files: amazon_mp3_redux.txt: a small sub-sample of the amazon review dataset in its raw format that can be used to test the data processing codes. Associated processing script: processMP3reviews.py or processMP3reviews_split.py Test_redux (folder): a small sub-sample of the hotel review dataset in its raw format that can be used to test the data processing codes. Associated processing script: processHOTELreviews.py Model Building Follows process in subject paper: Initialize corpus-level parameters Compute review-level parameters Initialize review-level parameters Iteratively update until convergence Corpus-level parameters are held constant in this process Update corpus-level parameters Review-level parameters are held constant Recompute log-likelihood Iterate until convergence Input: processed data file (in \data folder) Output: model file saved to \models folder Corpus-level parameters Review-level parameters Processed File (.pkl) Model File (.pkl) Model Building estimateModel.py Data files should be in the \data folder Model file will be saved to the \models folder Processed File (.pkl) Model File (.pkl) Provide name of processed data file here Provide name of output file to be created here Define number of aspects here Model Building Available models: MP3model_low_100_3.pkl Model built with 3 aspects on dataset with 100 MP3 reviews with low rating Associated processed data file: MP3reviews_low_100.pkl MP3model_high_100_3.pkl Model built with 3 aspects on dataset with 100 MP3 reviews with high rating Associated processed data file: MP3reviews_high_100.pkl HotelModel_100_7.pkl Model being built with 7 aspects on dataset with 100 hotel reviews Associated processed data file: HotelReviews_100.pkl Processed File (.pkl) Model File (.pkl) Analysis For various purposes May take either or both of: Processed data file Model file Available codes: getStats.py: computes basic stats on the review data getTopAspectWords.py: retrieves the top words of each aspect (based on aspect word distribution) Processed File (.pkl) Model File (.pkl) Analysis getStats.py Script will print on the terminal: The number of reviews in the file The number of unique items (products) reviewed The average length (and standard deviation) of the reviews The average and standard deviation of the overall ratings No other outputs or files created Data file should be in the \data folder Processed File (.pkl) Provide name of processed data file here getTopAspectWords.py Script print array of words to the terminal No other outputs or files created Data file should be in the \data folder Model file should be in the \models folder Analysis Processed File (.pkl) Provide name of processed data file here Model File (.pkl) Provide name of output file to be created here Define number of words per aspect here Some Results - Review Statistics Compared to the reported results (see Table 1 in subject report): Slightly fewer reviews. For instance, Table 1 reports 2,232 reviewed hotels, but the available dataset only contains files for 1,850 (one is filtered out) Higher average review length. This is likely because of the slightly different processing. For instance, here the NLTK stopwords were used, which will differ a little from that was used in the paper. Ratings statistics are very similar. Some Results: Top Words in MP3 Reviews Some Results: Top Words in MP3 Reviews Few similarities with Table 2 in the subject paper: Low: problem, time, warranty High: easy, sound, quality Despite a few similarities, this list is mainly different from the one in Table 2 in the subject paper. Reasons could be: Fewer aspects: the paper modeled 20 aspects and displayed the top 3, while here only 3 aspects were modeled. The higher number of aspects in the paper allow for better topic definition for each aspect, whereas here, the shown 3 topics need to account for all the content in the corpus. The much smaller dataset. This was built on 100 reviews, while the paper used 16,680 divided between the two sub-groups (low/high rating) The different list of stopwords; the list used here may have left more common English words than the list used in the paper. Note: time for model computation is the reason to have reduced the dataset (number of reviews) and the number of aspects modeled. Challenges Model complexity A number of high dimensional numerical optimizations at repeated iterations leads to high computation time to build models This gets worse as the dataset and number of aspects increase This has hampered the development of the model on the hotel dataset, where at least 7 aspects are needed for meaning assessment Model build code has not finished running (on set with 100 reviews) Not possible yet to perform the aspect rating analysis Even when the model finishes, the low number of reviews will likely make the results not very robust To solve this, efforts are needed for code optimization Most of the time in the project was spent on research to understand the model and approach to be able to make an initial implementation of it Better computation resources would also help Suggested Testing Procedure The steps below allow for all scripts provided to be tested without taking too much time. They are already set with the inputs corresponding to this, so they can be run without changes. Data processing scripts Run processMP3reviews.py to process the MP3 reviews in amazon_mp3_redux.txt. This will generate the MP3reviews_redux.pkl. All these files are in the \data folder. Run processHOTELreviews.py to process the hotels reviews in the folder \Texts_redux. This will generate the HotelReviews_redux.pkl. All these files are in the \data folder. Model building scripts Run estimateModel.py on the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model file MP3model_redux.pkl. The suggested number of aspects is 3. Analysis scripts Run the getStats.py on MP3reviews_high_100.pkl to obtain the statistics on that smaller set of reviews. Run the getTopAspectWords.py using the data MP3reviews_high_100.pkl and model MP3model_high_100_3.pkl to obtain the top 10 words in each of the 3 aspects of this model on these reviews.
https://github.com/seuaciuc/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. General Info Project material is structured in 3 folders: - Data: contains all data, both raw and processed. Smaller sub-samples of the datasets are provided. - Models: contains all models (pickle files with estimated model paramters). - Codes: all scripts are here. Each folder contains a README.md with specific information about their contents and structure. All code is done in Python 3.8. Data (information from one step to another) is shared through pickle files, so typically the codes take one file as input and generate others as output. Codes are set to the folder structure of this repository, so it needs to be maintained to be able to run the scripts without modification. Interface There is no command line interface built for this yet. Inputs are provided directly in the files, by modifying the first few lines of the scripts. The most common inputs are clearly identified in the beginning of the scripts. Other ""hyper-parameters"" (such as maximum number of iterations, minimum review length, etc) will appear immediately after the primary input section of the code. These typically do not need to be changed unless fine-tuning is desired. Whenever the first import command is reached, all possible inputs are done. In general, the scripts are ready to be run on some of the sample files provided if the folder structure of this repository is maintained. Most of the information in this readme and the others is summarized in the accompanying presentation slides (CS410_Fall2020_CourseProject_Tutorial.pdf or .pptx). That is probably the best place to start. Suggested Testing Procedure Because of the time for computation and size of some of the files, not all aspects of the project can be reproduced. Below is a suggested testing procedure that uses all scripts in the project to assess their functionality. The scripts provided are set to run the steps in this procedure without change if the folder structure is maintained. Data Processing Scripts Processing the entire datasets take considerable time and resources. This has been done, but the resulting files are larger than the allowable limite in GitHub. To test the codes, you can run the data processing scripts on the provided smaller datasets. These two steps will run the scripts provided to process each of the two datasets: - Run processMP3reviews.py to process the MP3 reviews in amazon_mp3_redux.txt. This will generate the MP3reviews_redux.pkl. All these files are in the \data folder. - Run processHOTELreviews.py to process the hotels reviews in the folder \Texts_redux. This will generate the HotelReviews_redux.pkl. All these files are in the \data folder. Model Building Building the model takes time. Because of this, a pre-model was built on a random sample of 100 reviews. That can be done, but it still takes a little time. To test the functionality of the model building code, you can build a model on one of the reduced datasets. - Run estimateModel.py on the reduced MP3 dataset (MP3reviews_redux.pkl) to generate the model file MP3model_redux.pkl. The suggested number of aspects is 3. Analysis Run the getStats.py on MP3reviews_high_100.pkl to obtain the statistics on that smaller set of reviews. Run the getTopAspectWords.py using the data MP3reviews_high_100.pkl and model MP3model_high_100_3.pkl to obtain the top 10 words in each of the 3 aspects of this model on these reviews."
https://github.com/Sembian2-CS410Fall2020/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Project Documentation: https://github.com/Sembian2-CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_documentation.pdf
https://github.com/Sembian2-CS410Fall2020/CourseProject	sembian2_cs410f2020_project_documentation.pdf	"CS410 - Course Final Project Project Documentation Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) Project Installation guide The project code is completely executed in a google colab environment, please download the ipynb file and upload to google, you can also make a copy directly from the google colab link https://colab.research.google.com/drive/1gzwQJeSNKXulIjOX34z-quQePByhPt75?usp=sharing Download the ipynb and load into google colab ( https://colab.research.google.com/) and enable the GPU runtime **Project Documentaiton:** https://github.com/Sembian2- CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_documentation.pdf **Project Progress Report:** https://github.com/Sembian2- CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_progress_report.pdf **Project Proposal Document:** https://github.com/Sembian2- CS410Fall2020/CourseProject/blob/main/sembian2_cs410f2020_project_proposal.pdf **Project Presentation Video** (YouTube Link) : https://www.youtube.com/watch?v=cH2tZB5n_8Y Motivation & Dataset: The text classification competition involves a binary classification of tweets with a balanced training including labels indicating SARCASM(0), NOT_SARCASM(1), I used the state-of-the-art Transformers, pytorch libraries with BERT Embeddings. The training dataset had 5000 labelled samples with balanced label distribution of 50 % and the test dataset had 1800 rows with additional id field. Approach: The training dataset is loaded into pandas DataFrame and the response column was pre-processed with text cleaning including removing @USER and @URL, expanding shortwords, expanding emoji's, and removing any stopwords using nltk library, removing special characters and punctuations. After pre-processing the response tweets column and label column is split into training and validation I used a .33% validation and .77% training data set. Classification Methods: The First approach is to use the Multinomial Naive Bayes by applying TF-IDF and got a baseline AUC score of .8118 the accuracy was around 72% I used this as a baseline and tried improving the baseline using BERT embeddings and a feed forward neural network. Text Classification with Transformers in PyTorch: BERT The transformer-based LM(Language models) has shown promising progress on number of NLP benchmarks. By combining transfer learning methods with large-scale transformer language model is becoming a standard in modern NLP compared to traditional classification approaches. In this final approach to improve the baseline score of 72.24% from the MultinomialNB approach we will attempt to increase the accuracy score by implementing a transformer architecture and fine-tuning of the pre-trained BERT model for classification. The two important complimentary concepts in Natural Language Processing: * Word embeddings * Language Model Transformers are used to build the language model and embeddings can be retrieved as the by-product of pretraining. Transformers architecture implements so-called attention mechanism to include an entire sequence as a whole enabling training in parallel when compared to traditional LSTM approaches. The huggingface transformers library has a huge collection of the language models and embeddings and makes it easier for implementing using pytorch in python. BERT BERT( Bidirectional Encoder Representations from Transformers) is a mothod of pretraining language representation. BERT does not have a decoder but stacks 24 layer encoders for bert-uncased-large) #Sample code showing the import and instantiation of BERT Model from transformers. import torch import torch.nn as nn from transformers import BertModel # Instantiate BERT model self.bert = BertModel.from_pretrained('bert-large-uncased') BERT Tokenizer and Netowrk Architecture The important limitation of BERT is that the maximum sequence length is 512 tokens, the shorter sentences are added with [PAD] and there is also a [CLS] token for indicating beginning of the sentence and [SEP] token at the end of sentence the tokenized sentence is then encoded using BERT Embeddings the bert-large has 1024 embeddings While there are multiple approaches I used a custom BertClassifier with a single feedforward neural network with # Specify hidden size of BERT, hidden size of our classifier, and number of labels # BERT-Large, Uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters D_in, H, D_out = 1024, 50, 2 # Instantiate an one-layer feed-forward classifier self.classifier = nn.Sequential( nn.Linear(D_in, H), #https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear nn.ReLU(), #https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU nn.Linear(H, D_out), #https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear ) The final layer out put is passed thru a ReLU activation layer and output dimensions of 2 indicating the 2 labels[SARCASM-0, NOT_SARCASM-1] , the BERT tokenizer is applied on all responses of the training data and map tokens into WordPiece embeddings using the encode_plus function, the following parameters were used for training. LearningRate 5e-5 Max Sequence Length 89 Batch Size 32 No. of Epochs 4 The model is then trained for 4 epochs and achieved a score of 81.17% on the training set that is almost 10 point increase from the baseline MultiNomialNB model. For HyperParameter tuning I used wandb.com ( weights and Biases) to report out the various runs and compared the best score and the run named revivedpthunder-388 scored the highest and achieved a 81.17 validation accuracy and 75.19 Test Accuracy in the leaderboard Report Link to wandb Tables Training Accuracy TF-IDF Vectorizer and Multinomial Naive Bayes 72.24% Transformers with BERT_large_uncased embeddings 81.17% Test Accuracy of 75.18% - Position 10 on Leaderboard as of 12.02.2020 Using the Transformers, Pytorch and BERT Classification model I was able to beat the baseline score on the leaderboard and improved the score by repeating the training with Hyper Parameter tuning and text pre- processing techniques and achieved a score of 75.18% Test Accuracy, and have no challenges. References: * Images for illustration are taken from the original BERT paper (Devlin et al. 2018). * Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,Bert: Pre-training of deep bidirectional transformers for languageunderstanding, 2018. * Weights and Biases: https://wandb.ai/cayush/bert-finetuning/reports/Sentence-Classification-With- Huggingface-BERT-and-W-B--Vmlldzo4MDMwNA * The Role of Conversation Context for Sarcasm Detection in Online Interactions: https://arxiv.org/pdf/1707.06226.pdf * Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. ""What does bert look at? an analysis of bert's attention."" arXiv preprint arXiv:1906.04341 (2019). * Sileo, Damien. ""Understanding bert transformer: Attention isnt all you need."" Towards Data Science (2019). * Weiss, Karl, Taghi M. Khoshgoftaar, and DingDing Wang. ""A survey of transfer learning."" Journal of Big data 3, no. 1 (2016): 9. * Biewald, Lukas. ""Experiment Tracking with Weights and Biases."" (2020). * Text Tweet Pre processing - https://github.com/digitalepidemiologylab * The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) - https://jalammar.github.io/illustrated-bert/ * Pytorch.org - https://pytorch.org/docs/stable/index.html * BERT Fine-Tuning Tutorial with PyTorch for Text Classification: https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text- classification-on-the-corpus-of-linguistic-18057ce330e1 * Huggingface transformers library - https://github.com/huggingface/transformers * BERT for Advance NLP with Transformers in Pytorch - https://www.linkedin.com/pulse/part1-bert- advance-nlp-transformers-pytorch-aniruddha-choudhury/ * Attention Is All You Need; Vaswani et al., 2017. - https://arxiv.org/pdf/1706.03762.pdf * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding; Devlin et al., 2018. - https://arxiv.org/pdf/1810.04805.pdf * Encoder-Decoder Architecture& Bert Paperon the full research. - https://arxiv.org/pdf/1810.04805.pdf * Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). [Tackling the poor assumptions of naive bayes text classifiers. In ICML](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) (Vol. 3, pp. 616-623). * Text classification with transformers in Tensorflow 2: BERT, XLNet - https://atheros.ai/blog/text- classification-with-transformers-in-tensorflow-2"
https://github.com/Sembian2-CS410Fall2020/CourseProject	sembian2_cs410f2020_project_progress_report.pdf	CS410 - Course Final Project Project Progress Report Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) Which tasks have been completed? The Final project was planned and split into 18 Tasks and over six 2 week sprints, below are the tasks and child issues completed with Sprint reports For HyperParameter tuning I used wandb.com ( weights and Biases) to report out the various runs and compared the best score and the run named revivedpthunder-388 scored the highest and achieved a 81.17 validation accuracy and 75.19 Test Accuracy in the leaderboard Report Link to wandb Test Accuracy of 75.19% - Position 6 on Leaderboard as of 11.26.2020 Which tasks are pending? The following tasks are in progress and I am on track for completing the project code documentation and presentation. Are you facing any challenges? Using the Transformers, Pytorch and BERT Classification model I was able to beat the baseline score on the leaderboard and improved the score by repeating the training with Hyper Parameter tuning and text pre- processing techniques and achieved a score of 75.19% Test Accuracy, and have no challenges.
https://github.com/Sembian2-CS410Fall2020/CourseProject	sembian2_cs410f2020_project_proposal.pdf	CS410 - Course Final Project Project Proposal Sembian2@illinois.edu Project Option 4: Competitions - Text Classification Competition Team Name: Sembian2 ( Individual ) What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Name: Sembian Balasubramanian NetID: sembian2 I will doing this project as individual, I will ensure all administrative tasks are completed and will be submitting the deliverables as per schedule. Which competition do you plan to join? I am really interested in the text classification competition, this would allow me to learn and build, train a state of the art classifier model using industry leading libraries implementing methods like LSTM and Convolutional Neural networks using TensorFlow-Keras / Pytorch transformers and using word embedding techniques - I will also experiment and train the model using Activations like sigmoid, ReLU, softmax and exploring evaluations of accuracy and loss of training and validation , performing parameter tuning updates and using optimizers While the technology is complex, after watching the coursera lectures on text classification, I am comfortable in text classification methods, also at my I am leading the multi-channel measurement strategy and would apply my learning from this project to propose state of the art classification models and hyperparameter tuning for sentiment, sarcasm & emotion detection Which programming language do you plan to use? Python /Jupyter Notebook / Google Colab with cuda for model training
https://github.com/mkhanal2/CourseProject	CS410 Project - Progress Report.pdf	CS 410: Text Information Systems Course Project Progress Report Text Classification Competition: Twitter Sarcasm Detection Submitted By: Mohan Khanal (mkhanal2) mkhanal2@illinois.edu 1) Which tasks have been completed? * Reading the tweets/response from the train and test file, cleaning the data and producing the proper training / testing vectors. * Applying deep neural network (LSTM) on the training set and using the trained model to get the prediction for the test set. * Achieved between 0.60 to 0.70 accuracy and f1 score on current progress, during multiple iteration. 2) Which tasks are pending? * Fine tune the model implemented to get higher accuracy and f1 score to beat the benchmark. * Document the approaches that was tried and the results that I got for the final report. * Create final report / presentation to be submitted at the end of the project. 3) Are you facing any challenges * Not a big issue, but training sometime takes longer time in my laptop so iterating through different ideas takes little bit longer time, especially if the parameter sets are in higher range.
https://github.com/mkhanal2/CourseProject	CS410 Project Documentation.pdf	"2020 Text Classification Competition: Twitter Sarcasm Detection CS410 - COURSE PROJECT FINAL DOCUMENT MOHAN KHANAL (MKHANAL2@ILLINOIS.EDU) 1 Contents How to use/run the code ................................................................................................................ 2 Background ..................................................................................................................................... 2 Model Design .................................................................................................................................. 3 Detail Approach / Code Walkthrough............................................................................................. 4 Data Pre-processing .................................................................................................................... 4 Data preparation for training ...................................................................................................... 6 Model Building and Training ....................................................................................................... 8 Prediction for Test set ............................................................................................................... 14 Prediction Results ......................................................................................................................... 15 Other Approaches ......................................................................................................................... 15 Conclusion ..................................................................................................................................... 16 References .................................................................................................................................... 16 2 How to use/run the code The code is developed using python 3 (Jupiter notebook). There is folder called source_code on the GitHub (https://github.com/mkhanal2/CourseProject) where we put all the documentation and source-code for the project. Follow following instruction to run the code: * Download the folder ""source_code"" from GitHub (link above) * Makes sure you have folder called ""data"" under source_code folder which has (train.jsonl, test.jsonl) files , all of these files and folder are already in GitHub. * We would need one for file in this data folder. Go to the link below to download the ""GloVe"" twitter file we need ""glove.twitter.27B.25d.txt"". After downloading the file for the link, unizip the file and copy the file ""glove.twitter.27B.25d.txt"" onto the data folder. o Direct Link - http://nlp.stanford.edu/data/glove.twitter.27B.zip o OR: Go to below link and download below mentioned file. # https://nlp.stanford.edu/projects/glove/ # Download ""glove.twitter.27B.zip"" form this site. o Note: Chrome didn't work for me while downloading, so I used ""Microsoft Edge"" to download the above file. * Now open Jupiter Notebook on you laptop (usually within Anaconda). * Using Jupiter Notebook open the source code from the downloaded folder (source code file name: Project Source Code.ipynb) * After that you can run the code. Please make sure that all the packages used in the 2nd cell of the notebook are already installed. I have provided the instruction on 1st cell of the notebook on how to install the packages. Background As part of the final project for course CS410-Text Information System, I have done project for Twitter Sarcasm Detection which is part of the Text Classification competition. As part of this project there were two sets of data file given to us. One was for training and another was for testing (for which we would have to do our prediction). These data files contained the twitter responses. These response text were in context to some conversation happening in twitter feed. These data file contained those context conversation as-well. So, we could use both response text and context text if we want to for training our model for this classification. Since this was a classification task, the training file also had the label for each data point as ""SARCASM"" or ""NOT_SARCASM"". Our job was to predict the same thing for all the records present in the testing file. Training file had 5000 labelled data-set and testing set had 1800 data-set. So, at the end we would have to predict the label for those 1800 test data-set. This project required some machine learning task to train the model using the training data and finally predict the outcome for test-data set using that trained classification model. This 3 document explains all the details around the different approaches that were carried out to train the model and will list out the final results that I got from those trained model. Model Design As part of this project, I have used recurrent neural network models called LSTM (Long Short Term Memory) model's. These models are mostly used for sequential data. Since, the text that we are using as our input is sequential and sarcasm might depend on how the sentence is structured, so using LSTM would help us utilize the sequential nature of the data. As explained in previous section, we have ""response"" text as one set of input data that we can use and another is we have ""context"" text as another set of input that we can use. So, here were the first two sets of model that we can design using these inputs. Model-1 * Using response only as part of our input and feeding that data into the model to predict if that response is sarcastic or not. Below diagram gives the flow of the model. Model-2 * Another approach is rather than only using the response as an input we can use both response and context as our input to train our model and get output from there. Below diagram show the design for the same. Results Combined/Aggregated * Finally we can decide to combine the results from Model-1 and Model-2 and then get our final aggregated results. Combining both Model-1 and Model-2 was able to beat the baseline score for me. Below is the diagram for the combined results model. I will be discussing the detail around this model in another section. Model-1 (LSTM) Output-1 Response(text) Model-2 (LSTM) Output-2 Response(text) Context(text) Model-1 Model-2 Output-1 Output-2 Final Output Response(text) Response(text) Context(text) 4 Detail Approach / Code Walkthrough As stated in the previous section I have used LSTM as a model for training my classification model. The program is written in python (Jupter Notebook). I will walk-over the detail steps that was carried out as part of my code in this section. Here are the steps that were carried out , which is going to be covered in details: 1. Data Pre-processing - Reading the file, and cleaning the data for training 2. Data preparation for training - Creating the vocabulary, creating the input vector for text. 3. Model Building and Training - Building different models and training them with the data/label prepared 4. Prediction for Test set - Combing results from multiple model to improve prediction score. Data Pre-processing Data pre-processing is the first process that is carried out before building the model. As part of data pre-processing step , following steps are carried out: * Read the training file provided (""train.jsonl"" - 5000 data records). This is a json file, and contains attributes (""label"", ""response"" and ""context""). I have used pandas json file reader function (read_json) to read the json file. * After reading the file, the emoji and hashtags are read separately using custom built functions for reading emoji and hashtags and placed on separate columns in pandas data frame, which could be utilized to use as input for a model. The final model used for this project doesn't utilize hashtags or emoji, but emoji's were utilized on some of my model's for testing, but was not carried out to the final model. * After that the text ""response"" , which is one of the main input data-set for our model is cleaned, using the custom build function. This function will remove any junk characters and rephrase some of the wordings , trim the text to remove white-spaces, remove the stops words like (the, is, etc.) and lemmatize the text so that the similar type of words are represented the same. * After that, context goes through the same clean-up process as that for response. Before that, context data is a list/array. Which means that there could be multiple level of replies for the tweet and the response above was the reply for the last reply from the context. So, we use the list object to get the last reply from it to set the context for us. After that it goes through the same clean-up process described above for response. * Similar to the train data, test data goes through the same pre-processing step. ""test.jsonl"" (1500 data records) is similar structure of training data (only thing different is, this data doesn't have label). Below is the code screenshot for the steps explained above. 5 Data-read and clean-up code (train.jsonl): Clean-up functions: 6 Data-read and clean-up code (test.jsonl): Data preparation for training After pre-processing for the data is completed, we would need to build a input vector that can be used for training. As currently after pre-processing we only have text data, and we would need to convert it into some numeric form to build the model. So, we would need to build a input vector represented in numeric form for this. There are multiple options for building a training vector. We can tokenize the words into integer's and then choose to use a embedding layer in our network to learn about the embedding vector. or we can choose to use some pre-trained embedding vector. While both approaches were tried as part of this project, I found that pre- trained embedding vector had better results than the self-trained one, so I am using the pre- trained embedding vectors. I am using the pre-trained vector from ""GloVe"". This embedding layer part is covered in more detail in next section. Here are the steps required to build a input vector from the pre-processed text. * After text pre-processing is completed, we need to build a word vocabulary which contains all the words that we are using for training and testing as-well. * The word vocabulary in then tokenized (into integers). * Then we are going to set the length of the input vector that we are going to feed to our model/network. While looking into pre-processed response and context, the max length of response was around 200 range and for context was around 250 range. So, I am using input length of 200 for response and 250 for context. * X and X_cntx vector with length 200 and 250 are developed. X is for response and X_cntx is for context. The vector X may look line this [0 0 0 ..... 25 34 26 27], where 0 at the left are '0' padded to make the length of the vector 200. The integer on the vector are the ones that were tokenized form of the word, so each word is converted in specific integer by the tokenize function. Similarly X_cntx is in similar structure, only difference it's its length is 250. Below is the code screenshot for the steps explained above. 7 Build a word vocabulary (code) Find max length's for input vector's (Code) Building Input Vector's (Code) 8 Model Building and Training After data clean-up and preparing for the input vector, we now need to represent our word integers as vectors. What this will do is that it will build a vector which will have related word together and unrelated word farther apart, that way the numeric representation of the words becomes more meaningful for training. As previously mentioned, we have two ways to do that. Once is to train that word-vector using training data (Embedding layer) another is to use the pre- trained data. For this project I tried both, at the end the pre-trained model worked better than the self-training. So, I am using the pre-trained data for word-embedding. I am using GloVe (Global Vectors for Word Representation) for that. GloVe has multiple files that we can use. For this project I have use the glove twitter data-set with 25d vector. (glove.twitter.27B.25d.txt) Glove site: https://nlp.stanford.edu/projects/glove/ Using GloVe we will be building a embedded layer in our model, what that embedding layer will do is, it will take the input of 1-d integer vector that we build (X, X_cntx) and then convert them into 2-d vector's where each integer is converted into another vector with length 25. That 2-d vector is then passed to the neural network model. After embedding layer is built we will then build two different model's one with ""Response"" as an input and another with ""Response"" and ""Context"" as input. Below is the detail diagram of both the model's. As mentioned previously I am using LSTM model for this project. Model-1: Model-2: Response (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM Output (sigmoid activation) Response (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM Output (sigmoid activation) Context (Text) - Integer Vector Embedding Layer (GloVe) Bidirectional LSTM 9 Below is the details steps done as part of model build and training: * The input-vector which is a integer conversion from text is first converted into pre-trained word-vector's. I am using GloVe (Global Vector for Word Representation) for this. * We need to read the GloVe vector from a file. After that all the words that were part of our word vocabulary is been fetched and embedding matrix is built with that. * The embedding matrix build will be used to create a embedding layer for our neural network model. This embedding layer's job would be to convert the integer vector into the GloVe vector representation, which can then be feed into neural network (LSTM). * After building the embedding layer, we now build an actual model. * Frist model, is a simple LSTM model which takes the response as input pass it to embedding layer and then into the LSTM network and finally a output is generated from that model. * Second model is a combination model, where I have built a two different LSTM model's one for Response as input (similar to first model) and another model is also similar LSTM but with Context as input. Finally these two LSTM's are combined to get the output for this second model. * After both the model's are built. We now have to prepare the training and testing data- set. As we have prepared the vectors X, X_cntx which has 5000 data-elements previously. We will split this data into 4000 training set and 1000 test-set (This is the test set used in training for validation). * After the split of training and test-set for validation. We now train both the model's model-1 and model-2 with these data. Below is the code snapshot for the above explained steps. Reading GloVe vector from the downloaded files (Code): 10 Building embedding matrix and Embedding Layer with GloVe vector(Code): Building First Model (Code): 11 Building Second Model (Code): Training and Test(Validation) data split from training data (Code) 12 Training Model-1 (Code): Below is the graph drawn from the training of Model-1: 13 Training Model-2 (Code): Below is the graph drawn from the training of Model-2: 14 Prediction for Test set Now, with both the model built and trained with training data, we will now predict the results for the test-set (1800 data set's that were read from train.josnl). Below is the details steps of what we do for prediction: * As we did for the training set we will first vectorized the text into integers before passing to the model as that is the input to our model. We build both response and context vectors as Z, and Z_cntx. * After building a vector now we pass the vector (Z) i.e. response only into model_1 for prediction. Similar we pass (Z and Z_cntx both) into second model for prediction. With this we now have two sets of prediction which is from model-1 and model-2. * Finally we combine the model-1 and model-2 prediction into single column, and then use some threshold defined to predict the result. * Since we are combining the results form model-1 and model-2 the aggregated value of the output will range from 0 to 2. The half-way cut-off value is 1.0, but instead of using 1.0 as our cut-off we will be using 0.5 as our cut-off to determine if it's sarcastic or not, we are doing this because this will increased our re-call by a lot with very less impact in the precision (that is what I found during the training/testing). This will increase our overall scoring of the model. * Finally, we will generate a file ""answer.txt"" with all this prediction for each of the data- set given and then the file is uploaded into GitHub to get score from LiveDataLab. Below is code for the steps explained above: Building Input Vector for test data set (Code): Predicting using Model_1 and Model_2 (Code): Combine Results from Model_1 and Model_2 (Code): Exporting the results (answer.txt) (Code): 15 Prediction Results There were multiple runs / iteration that was carried out as part of this project to beat the baseline score. The model explained above was able to beat the baseline score, that is combining two different model's and aggregating the result to get the final output. Below is the screenshot from the LiveDataLab that shows the overall score that I had which is above the baseline score. This score is as of 12/11/2020: I have also uploaded the answers.txt file in the GitHub with the source-code. https://github.com/mkhanal2/CourseProject/blob/main/source_code/answer.txt Other Approaches Apart from the approach that was discuss above (which was able to beat the benchmark score), I tried other approaches as-well. Other approaches that I tried were not able to beat the benchmark score. * Other Approach -1 (LSTM - ""response"" only model) o This approach is similar to the model-1 that I implemented above, but this approach alone was not able to beat the baseline score. The best f1 score for this model was around 0.70 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. * Other Approach -2 (LSTM - ""response"" and ""context"") o This approach is similar to the model-2 that I implemented above, but this approach alone was not able to beat the baseline score. The best f1 score for this was also around 0.70 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. * Other Approach -3 (LSTM - ""response"" and ""emoji"" model) o This model was based on model-1 above, and also one additional model which was used to train the data with ""emoji's"" only , since the emoji containing response were limited, so this model was also not able to beat the baseline score. Although this model was better from previous two approaches, the best f1 score for this was around 0.71 range. o I tried this model with both self-training embedded layer and pre-trained embedded layer. 16 Conclusion After testing through multiple models of LSTM , the combined model with ""response"" only as input and ""response and context"" as input was able to beat the baseline score. So, combining different model's with different inputs and then combining the results of output made model more efficient. Similar we could further tune/iterate through this model , or introduce new features like emoji's to possibly improve the score for this model. Also, there are other model that can be tried with this data-set , like BERT. Due to the limited time for this project, my work has been only limited to LSTM model explained above. References There were lots of learning and references that I took as part of this project, as LSTM / neural- network was kind of new topic for me. I have also taken the code snippet from some of the references list below. Listed below are websites/codes that I took reference and learned from: * https://adventuresinmachinelearning.com/keras-lstm-tutorial/ * https://machinelearningmastery.com/crash-course-deep-learning-natural-language- processing/ * https://www.aclweb.org/anthology/C16-1231.pdf * https://machinelearningmastery.com/timedistributed-layer-for-long-short-term- memory-networks-in-python/ * https://www.kaggle.com/mkowoods/deep-learning-lstm-for-tweet-classification * https://github.com/AniSkywalker/SarcasmDetection * https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281 * https://arxiv.org/pdf/1911.10401.pdf * https://www.aclweb.org/anthology/2020.figlang-1.11.pdf * https://github.com/MirunaPislar/Sarcasm-Detection * https://www.youtube.com/watch?v=pMjT8GIX0co * https://towardsdatascience.com/lstm-vs-bert-a-step-by-step-guide-for-tweet- sentiment-analysis-ced697948c47?gi=5e1b3ad1bacc * https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ * https://github.com/Suji04/NormalizedNerd/blob/master/Introduction%20to%20NLP/Sa rcasm%20is%20very%20easy%20to%20detect%20GloVe%2BLSTM.ipynb * https://rcciit.org/students_projects/projects/cse/2018/GR6.pdf"
https://github.com/mkhanal2/CourseProject	CS410 Project Proposal.pdf	CS 410: Text Information Systems Course Project Proposal - Text Classification Competition: Twitter Sarcasm Detection Submitted By: Mohan Khanal (mkhanal2) mkhanal2@illinois.edu 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. o Team Name: MK (Individual) o NetIDs for all team: (only myself) # Mohan Khanal (NetID: mkhanal2) o Captain: Myself (mkhanal2) 2. Which competition do you plan to join? * Text Classification Competition: Twitter Sarcasm Detection 3. Are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods * Yes, I am prepared to learn state-of-the-art neural network classifiers * Framework/Architecture looking to learn and utilize: Recurrent Neural Networks (RNN) , Bidirectional Encoder Representations from Transformers (BERT). * I have done machine learning project previous for Intrusion Detection system, but it was not related to text classification. I am familiar with how machine learning models / neural network works. So, will be utilizing that knowledge to work on this text classification project and will also learn new things to carry out the project. 4. Which programming language do you plan to use? * Python
https://github.com/mkhanal2/CourseProject	README.md	"CourseProject Team Members Mohan Khanal (mkhanal2@illinois.edu) Text Classification Competition: Twitter Sarcasm Detection Model Used: LSTM All the details about the project has been documented in the ""CS410 Project Documentation.PDF"" file out here. https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Documentation.pdf Documents Project Proposal : https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Proposal.pdf Project Progress Report : https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20-%20Progress%20Report.pdf Project Final Document: https://github.com/mkhanal2/CourseProject/blob/main/CS410%20Project%20Documentation.pdf Source Code Python Juputer Notebook Code: https://github.com/mkhanal2/CourseProject/blob/main/source_code/Project%20Source%20Code.ipynb Video Presentation Link --> https://mediaspace.illinois.edu/media/1_3i2g9ehq Output File answer.txt under source_code folder Link: https://github.com/mkhanal2/CourseProject/blob/main/source_code/answer.txt How to use/run the code The code is developed using python 3 (Jupiter notebook). There is folder called source_code on the GitHub (https://github.com/mkhanal2/CourseProject) where we put all the documentation and source-code for the project. Follow following instruction to run the code: - Download the folder ""source_code"" from GitHub (link above) - Makes sure you have folder called ""data"" under source_code folder which has (train.jsonl, test.jsonl) files , all of these files and folder are already in GitHub. - We would need one for file in this data folder. Go to the link below to download the ""GloVe"" twitter file we need ""glove.twitter.27B.25d.txt"". After downloading the file for the link, unizip the file and copy the file ""glove.twitter.27B.25d.txt"" onto the data folder. - Direct Link - http://nlp.stanford.edu/data/glove.twitter.27B.zip - OR: Go to below link and download below mentioned file. - https://nlp.stanford.edu/projects/glove/ - Download ""glove.twitter.27B.zip"" form this site. - Note: Chrome didn't work for me while downloading, so I used ""Microsoft Edge"" to download the above file. - Now open Jupiter Notebook on you laptop (usually within Anaconda). - Using Jupiter Notebook open the source code from the downloaded folder (source code file name: Project Source Code.ipynb) After that you can run the code. Please make sure that all the packages used in the 2nd cell of the notebook are already installed. I have provided the instruction on 1st cell of the notebook on how to install the packages. Text Classification Competition: Twitter Sarcasm Detection Dataset format: Each line contains a JSON object with the following fields : - response : the Tweet to be classified - context : the conversation context of the response - Note, the context is an ordered list of dialogue, i.e., if the context contains three elements, c1, c2, c3, in that order, then c2 is a reply to c1 and c3 is a reply to c2. Further, the Tweet to be classified is a reply to c3. - label : SARCASM or NOT_SARCASM id: String identifier for sample. This id will be required when making submissions. (ONLY in test data) For instance, for the following training example : ""label"": ""SARCASM"", ""response"": ""@USER @USER @USER I don't get this .. obviously you do care or you would've moved right along .. instead you decided to care and troll her .."", ""context"": [""A minor child deserves privacy and should be kept out of politics . Pamela Karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it ."", ""@USER If your child isn't named Barron ... #BeBest Melania couldn't care less . Fact . ""] The response tweet, ""@USER @USER @USER I don't get this..."" is a reply to its immediate context ""@USER If your child isn't..."" which is a reply to ""A minor child deserves privacy..."". Your goal is to predict the label of the ""response"" while optionally using the context (i.e, the immediate or the full context). Dataset size statistics : | Train | Test | |-------|------| | 5000 | 1800 | For Test, we've provided you the response and the context. We also provide the id (i.e., identifier) to report the results. Submission Instructions : Follow the same instructions as for the MPs -- create a private copy of this repo and add a webhook to connect to LiveDataLab.Please add a comma separated file named answer.txt containing the predictions on the test dataset. The file should have no headers and have exactly 1800 rows. Each row must have the sample id and the predicted label. For example: twitter_1,SARCASM twitter_2,NOT_SARCASM ..."
https://github.com/mattzeeee/TextInformationSystemsCourseProject	1. Project Report and Classifier Code Team The Classifiers - Twitter Sarcasm Detection.pdf	"CS 410: Text Information Systems Final Project - Final Report Project Type: Classification Competition Team: The Classifiers November 27, 2020 Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) References and web-sites used in this code extensively: https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db https://simpletransformers.ai/docs/usage/ Key Libraries Used in this Project to Date: As part of our project certain libraries are being used extensively. For the core traditional ML models as well as feature vector transformations: SKLearn Library For the implementation of the RoBERTa pre-trained neural network architecture, the following libaries are used: HuggingFace Transformer Library SimpleTransformer.ai Library Description of Work In This Notebook: This code notebook explains the next steps we followed post the completion of the project progress report notebook and should be seen (or read) as a follow on to that piece of work. Our total project documentation is covered by both notebooks as well additional descriptions of further testing we performed/optimisation as part of the project. This notebook focuses on the final implementation of the chosen RoBERTa transformer model implementation designated by the project team for investigation and use in the classification competition and leaderboard. Optimisation and experimentation is described in another document, however this code focuses on the following: 1. The creation and learning of an existing RoBERTa model widely used and available in the simpletransformers.ai library 2. Some feature engineering in the form of stemming, stop word removal and general cleaning of text input data 3. The inclusion of Context in the model to enhance the model performance Additional commentary and description can be found in the document below code snippets as to what the project team found to be working or not working. Import key python libraries for this project The below cell installs all necessary key libraries used in this notebook first should they not already be installed: The below cell imports all necessary libraries used in this notebook : In [ ]: !pip install jsonlines !pip install pandas !pip install transformers !pip install nltk !pip install torch In [1]: import os import jsonlines import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import precision_recall_fscore_support from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix import simpletransformers import torch #libraries for RoBERTa Importing of training and test data The following code reads both the train and test JSON files and imports the data into a python dictionary format: Count of training data entries: 5000 Count of test data entries: 1800 The following code converts the training and test data dictionaries into a Pandas DataFrame format for use later in the RoBERTa model Training and Test Datasets converted to Pandas DataFrames... from simpletransformers.classification import ClassificationModel,ClassificationArgs from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score In [2]: test_file = 'data/test.jsonl' train_file = 'data/train.jsonl' data_train = [] iter = 1 with jsonlines.open(train_file) as f: for line in f.iter(): iter +=1 data_train.append(line) data_test = [] iter = 1 with jsonlines.open(test_file) as f: for line in f.iter(): iter +=1 data_test.append(line) print(""Count of training data entries:"") print(len(data_train)) print(""Count of test data entries:"") print(len(data_test)) In [3]: train_data_pd = pd.DataFrame.from_dict(data_train) test_data_pd = pd.DataFrame.from_dict(data_test) print(""Training and Test Datasets converted to Pandas DataFrames..."") Adding Context to training and test datasets as well as basic NLP processing This code will append the context data to the response/text data linearly so as to add context information to the various model input feature vectors and capture the additional information therein. Additionally, some basic processing such is performed on the raw data such as stemming, removal of stop words [nltk_data] Downloading package stopwords to C:\Users\User- [nltk_data] PC\AppData\Roaming\nltk_data... [nltk_data] Package stopwords is already up-to-date! [nltk_data] Downloading package punkt to C:\Users\User- [nltk_data] PC\AppData\Roaming\nltk_data... [nltk_data] Package punkt is already up-to-date! The next code snippet performs the addition of context information which you should note ONLY includes the addition of the preceding CONTEXT sentence to the response and not the sentence before that as well (i.e. the model uses only one of the two additional CONTEXT sentences): In [4]: import nltk nltk.download('stopwords') nltk.download('punkt') from nltk.corpus import stopwords from nltk.tokenize import word_tokenize, SpaceTokenizer from nltk.stem import PorterStemmer In [5]: all_stopwords = stopwords.words('english') tk = SpaceTokenizer() ps = PorterStemmer() for i in range(len(train_data_pd)): train_data_pd['response'][i]=train_data_pd['response'][i]+train_data_pd['context'][i][1] train_data_pd['response'][i]=train_data_pd['response'][i].replace('@USER', '').strip().lower() text_tokens = tk.tokenize(train_data_pd['response'][i]) tokens_without_sw = [word for word in text_tokens if not word in all_stopwords] test4="""" for i in tokens_without_sw: test4 = test4 + "" ""+ps.stem(i) test4.strip() train_data_pd['response'][i]=test4 for i in range(len(test_data_pd)): test_data_pd['response'][i]=test_data_pd['response'][i]+test_data_pd['context'][i][1] test_data_pd['response'][i]=test_data_pd['response'][i].replace('@USER', '').strip().lower() Converted response PD data to include Context Data Converted response to lowercase and removed stop words as well as @USER Converted response to stem words using PortStemmer Perform a random test print of the constructed response+context after NLP pre-processing to see if all is ok: define this way : 1 . desiring the good of the other ; wanting them to thrive / flourish , which means they'd get free from the at titudes you mention ; 2 . doing whatever's in your control / power to advance their good ; at least * not * wishing them ill , * n ot * hating them . ok , you ' re right , but how do you love someone who hates you , and wants you to not exist ? how do you love someone doesn ' t share basic morals ? Here we turn the labelled training data text format 'SARCASM' or 'NOT SARCASM' into a binary 1/0 list to for easier input into the models: Splitting of the project training dataset randomly The code below splits the training dataset into a random (new) training and test dataset (i.e. only using the course project training data to train and test locally so far): NOTE THAT RANDOM_STATE is set to zero to provide a fixed seed to the random generator for consistent results text_tokens = tk.tokenize(test_data_pd['response'][i]) tokens_without_sw = [word for word in text_tokens if not word in all_stopwords] test4="""" for i in tokens_without_sw: test4 = test4 + "" ""+ps.stem(i) test4.strip() test_data_pd['response'][i]=test4 print(""Converted response PD data to include Context Data"") print(""Converted response to lowercase and removed stop words as well as @USER"") print(""Converted response to stem words using PortStemmer"") In [6]: print(test_data_pd['response'][10]) In [7]: #Define the vector of actual results: Actual_Results = [] for l in data_train: if l['label'] == 'SARCASM': Actual_Results.append(1) else: Actual_Results.append(0) Training dataset split into this many train samples: 4750 Training dataset split into this many test/validation samples: 250 Final Model : RoBERTa The below code implements the SimpleTransformers.ai implementation of RoBERTa and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [8]: #getting training dataset features and labels features = train_data_pd['response'] labels = train_data_pd['label'] labels = Actual_Results # Splitting of training data into train and test data rawdata_train, rawdata_test, rawlabels_train, rawlabels_test = train_test_split(features, labels, test_size = .05, random_state = print(""Training dataset split into this many train samples:"") print(len(rawdata_train)) print(""Training dataset split into this many test/validation samples:"") print(len(rawdata_test)) In [9]: #Import the simpletransformers model library from simpletransformers.classification import ClassificationModel,ClassificationArgs In [10]: #Create a training dataset in a Panda DataFrame format ready for the model train_df = pd.DataFrame({ 'text': rawdata_train.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_train }) In [11]: #Create a test dataset in a Panda DataFrame format ready for the model test_df = pd.DataFrame({ 'text': rawdata_test.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_test }) Set up RoBERTa transformer model The below code establishes and imports the RoBERTa transformer model. There are two possible options here, one including cuda support (if you have this available) and one excluding cuda support. The difference is primarily the speed of model training/estimation. the code has been set up to use or not use Cuda depending on the machine where the code is being executed (this is done through the torch.cuda.is_available() function which returns true if cuda is an option). Does system have CUDA support? True Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.b ias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weigh t'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly init ialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Perform model training (this may take some time if non CUDA support): In [12]: # Create a ClassificationModel # for training from scratch import torch cuda_available = torch.cuda.is_available() print(""Does system have CUDA support?"") print(cuda_available) model = ClassificationModel('roberta', 'roberta-base', use_cuda=cuda_available) # You can set class weights by using the optional # for loading my pre-trained model #model = ClassificationModel('roberta', 'outputs/checkpoint-594-epoch-1', use_cuda=False) In [13]: model_args = { ""reprocess_input_data"": True, ""overwrite_output_dir"": True, ""num_train_epochs"": 1, ""model_args.lazy_loading"" : True } C:\Users\User-PC\anaconda3_SP2\envs\SP 36\lib\site-packages\torch\optim\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) (594, 0.5382462045936672) Evaluate the model performance on the training dataset: Through printing of the below output of result_train, we can see relevant accuracy, precisionm, recall and F1 scores for the model based on predictions on the training dataset. A resulting f1 score of 0.84 is observed indicating a high degree of accuracy on the training dataset for the model priediction. {'mcc': 0.6909803679892279, 'tp': 2196, 'tn': 1798, 'fp': 573, 'fn': 183, 'f1': 0.8531468531468531, 'acc': 0.840842105263158, 'prec': 0.7930660888407367, 'rec': 0.9230769230769231, 'eval_loss': 0.3543026091762966} Next we need to evaluate the model performance on the test dataset (the test set randomly sampled from the trainign data...not the course test set # Train the model model.train_model(train_df,args=model_args) Out[13]: In [14]: # Evaluate the model - training result_train, model_outputs_train, wrong_predictions_train = model.eval_model(train_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) In [15]: #Print training result metrics result_train Out[15]: for the leaderboard): From printing the below result_test metrics, we can see that the model appears to perform relatively well with an F1 score 0f 0.81...slightly lower than training set but still relatively good. This is a good result and indicative of a model that hopefully was not overfit to the training data {'mcc': 0.602253199474844, 'tp': 106, 'tn': 93, 'fp': 36, 'fn': 15, 'f1': 0.8060836501901141, 'acc': 0.796, 'prec': 0.7464788732394366, 'rec': 0.8760330578512396, 'eval_loss': 0.4268150741700083} Performing predictions on project test data for LiveDataLab Perform predictions on the project test data for the competition for upload to LiveDataLab for grading Perform RoBERTa predictions for project test set In [16]: # Evaluate the model - testing result_test, model_outputs_test, wrong_predictions_test = model.eval_model(test_df, f1=f1_score, acc=accuracy_score, prec= precision_score, rec= recall_score) In [17]: #Print out the model performance metrics on the test set result_test Out[17]: In [18]: # getting training dataset features and labels features_test = test_data_pd['response'] predictions_test, raw_outputs_test = model.predict(features_test) Write the predictions from the RoBERTa model to an output text file called 'RoBERTa_answers.txt' for storage and uploading the the course competition leaderboard: By taking the resulting output file generated above and submission to the project LiveDataLab scoreboard, a resulting model score of F1 = 0.737 was achieved (beating the baseline score of 0.723). A sample screenshot of the model is included below and can be seen under the username PIQUITO2 at rank 28 In [20]: #Writing the RoBERTa Classifier predictions to the output file: RoBERTa_answers.txt y_pred = predictions_test f = open(""RoBERTa_answers.txt"", ""w"") id_final_test = test_data_pd['id'] for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() In [ ]:"
https://github.com/mattzeeee/TextInformationSystemsCourseProject	Appendix I.The progress report - initial experimentation.pdf	"CS 410: Text Information Systems Final Project - Progress Report Project Type: Classification Competition Team: The Classifiers November 27, 2020 Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) References and web-sites used in this code extensively: https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis- 84cb013bb6db https://simpletransformers.ai/docs/usage/ Key libraries used in this project to date: As part of our project certain libraries are being used extensively. For the core traditional ML models as well as feature vector transformations: SKLearn Library For the implementation of the RoBERTa pre-trained neural network architecture, the following libaries are used: HuggingFace Transformer Library SimpleTransformer.ai Library Description of work to date: To start our project off, we first performed research into the various machine learning model types that existing literature appeared to favour when it came to binary classification tasks such as SARCASM or NOT SARCASM detection. Post this review, we decided to focus our investigation to a limited set of models which included a range of descriminitive, generative as well as neural network based options in order to cover a spectrum of the different types. For this, we implemented four models using the SKLearn library being Linear Support Vector Machine, Gussian Naive Bayes, Logistic Regression and Random Forest models. As inputs/feature vectors to these models, we utilised TFIDF non-linear transformation on the training dataset from which our vocabulary was derived. Some cleaning of the source text data was performed, however this was not the focus of our project at the progress report stage (this will likely be investigated under the next stops of the project to improve model performance). To compliment the traditional ML models we used from SKLearn, we investigated and also implemented a first attempt at a BERT/RoBERTa model using transfer learning. It should also be noted that, for the above models, we have only used the target tweet text as a model input. We have purposely left the inclusion of the context data towards the later part of our project in order to compare and contrast model performance at this interim step. The intention is to use the performance metrics observed in our results below as a base line against which further feature engineering, model changes and the incorporation of context data will be measured. In the code below we develop the baseline ML models out of the SKLearn library to test and compare to the target BERT model that will be fully investigated as part of the project. Models considered are: 1. Learn SVM 2. Naive Bayes 3. Logistic Regression 4. Random Forest 5. RoBERTa Progress dade - results so far As we show in the charts below, RoBERTa, without context, pre-processing, or hypterparameter tuning, considerably outperforms the base line models. Please note that the model performance measures shown in the graphs below are based currently on a split of the provided training dataset only and are not reflective of the project test set results (which may certainly differ). When measuring accuracy of predictions, the clear preferred model of the five implemented shows that RoBERTa appears to take clearly take the lead relative to the other discriminative/generative models. Certain literature sources research suggest that this may be due to the model's pre-learning of general language and syntactical structure on a large data source and stored in the neural network model structure. This however is still an area of research with no definitive answers as yet. Out of the remaining models, Linear SVM appears to be the preferred traditional model relative to its peer models in the SKLearn library. This again is consistent with the observations found in our research on the topic (i.e. LSVM appears to be the strongest discriminative model). In order to more accurately measure model performance, the precision, recall and F1 scores of the respective models are calculated and shown on the second graph. The results yield a fairly similar estimation of model performance to that of accuracy above with RoBERTa the clear winner out of the model set. Next steps Given that RoBERTa, the large pre-trained language model with transformer deep neural network architecture, is the most promising direction we will use the simpletransformers framework and try to improve the models performance over the following dimensions. 1. Hyperparameter optimization 2. Data preparation 3. Adding context 4. RoBERTa model selection Focus will be given to the individual improvements along the above dimensions and then the possible combinations thereof in order to achieve the target base line F1 score for the competition. No further work will be performed on the other models which will be used as baselines for the final project submission Expected challenges/issues To date, all the models above have been tested in LiveDataLab on the project test set and have yielded a variety of results with no F1 score above the base line score 0.723 as yet. This contrasts of course to the F1 scores (notably for the RoBERTa model in particular) which appear to be higher on the training dataset split. Overfitting and a lack of good generationalisation appears to be a concern currently and part of our further investigation will be to potentially address this. Since we have chosen to focus on the RoBERTa model (and its possible variants), computational power and time to sufficiently train a neural network of this size and complexity is expected to present a challenge as we progress further. Various options such as the use of Google Colab and other cloud based solutions may need to be considered. Detailed Code Appendix Import key python libraries for this project The below cell imports all necessary key libraries used in this notebook first: In [1]: import os #import json import jsonlines import numpy as np import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import precision_recall_fscore_support #libraries for RoBERTa from simpletransformers.classification import ClassificationModel,Classific ationArgs from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score Importing of training and test data The following code reads both the train and test JSON files and imports the data into a python dictionary format: In [6]: test_file = 'data/test.jsonl' train_file = 'data/train.jsonl' data_train = [] iter = 1 with jsonlines.open(train_file) as f: for line in f.iter(): #data = json.load(line) #print(line) # or whatever else you'd like to do #print('processing training line: ' + str(iter)) iter +=1 data_train.append(line) #data = json.loads(line) #print(data) data_test = [] iter = 1 with jsonlines.open(test_file) as f: for line in f.iter(): #data = json.load(line) #print(line) # or whatever else you'd like to do #print('processing test line: ' + str(iter)) iter +=1 data_test.append(line) #data = json.loads(line) #print(data) print(""Count of training data entries:"") print(len(data_train)) print(""Count of test data entries:"") print(len(data_test)) Count of training data entries: 5000 Count of test data entries: 1800 The following code converts the training and test data dictionaries into a Pandas DataFrame format for use later in the SKLearn ML models In [8]: train_data_pd = pd.DataFrame.from_dict(data_train) test_data_pd = pd.DataFrame.from_dict(data_test) print(""Training and Test Datasets converted to Pandas DataFrames..."") Training and Test Datasets converted to Pandas DataFrames... In this piece of code, I manually created a vocabulary list based on all words I could find in both the test and training data. This isn't really used later in the SKLearn models but thought it handy if we extended the models somehow or customised the vocabulary In [9]: ##Create a vocabulary and transform JSON data in word vectors vocabulary = [] vocabulary_size = 0 def build_vocabulary(data,vocab_size): for l in data: for w in (l['response'].strip().split("" "")): if w not in vocabulary: #print(w) vocabulary.append(w) vocab_size = vocab_size + 1 return(vocab_size) vocabulary_size = build_vocabulary(data_train,vocabulary_size) print('Vocabulary size after loading training data only:') print(vocabulary_size) print(len(vocabulary)) vocabulary_size = build_vocabulary(data_test,vocabulary_size) print('Vocabulary size after loading training and test data:') print(vocabulary_size) print(len(vocabulary)) Vocabulary size after loading training data only: 16707 16707 Vocabulary size after loading training and test data: 20446 20446 In this piece of code we built a term document matrix (as per the MP3 assignment) to represent the raw Bag of Words vector format of the training and test data. In [9]: ##Construct the word/term vector matrix def build_term_doc_matrix(data,vocab_size): """""" Construct the term-document matrix where each row represents a docu ment, and each column represents a vocabulary term. self.term_doc_matrix[i][j] is the count of term j in document i """""" term_doc_matrix = np.zeros([len(data), vocab_size]) j_count = 0 for j in vocabulary: i_count = 0 for i in data: w_count = i['response'].count(j) term_doc_matrix[i_count,j_count]=w_count i_count += 1 j_count += 1 return(term_doc_matrix) train_vector = build_term_doc_matrix(data_train,vocabulary_size) print(""Length of training vector TDM matrix:"") print(len(train_vector)) print(""# of entries of training vector TDM matrix:"") print(train_vector.size) test_vector = build_term_doc_matrix(data_test,vocabulary_size) print(""Length of test vector TDM matrix:"") print(len(test_vector)) print(""# of entries of test vector TDM matrix:"") print(test_vector.size) Length of training vector TDM matrix: 5000 # of entries of training vector TDM matrix: 102230000 Length of test vector TDM matrix: 1800 # of entries of test vector TDM matrix: 36802800 Here we turn the labelled training data text format 'SARCASM' or 'NOT SARCASM' into a binary 1/0 list to for easier input into the models: In [12]: #Define the vector of actual results: Actual_Results = [] for l in data_train: if l['label'] == 'SARCASM': Actual_Results.append(1) else: Actual_Results.append(0) This code imports all the SKLearn model libraries required for the testing of the chosen ML models In [10]: ## Import the various SKLearn ML models for testing: from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.svm import LinearSVC from sklearn.model_selection import cross_val_score from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier Creation of TFIDF Word Vectors for training dataset In this code, I take the training data and split it onto a features and labels seperate vectors. I then use the the TfidVectorizer() function from SKLearn to construct a TFIDF word vector representation of the training dataset. Note that I've set the max features (unique words) to 5000 here in the function while we can see our vocabulary is closer to 20,000 features. This might be worth playing with a bit later for incremental accuracy. I use the 'features2' vector (in an array format) for the models input. In [13]: # getting training dataset features and labels features = train_data_pd['response'] labels = train_data_pd['label'] labels = Actual_Results # getting final test dataset features as well as features_final_test = test_data_pd['response'] id_final_test = test_data_pd['id'] ## Stemming our data #ps = PorterStemmer() #features = features.apply(lambda x: x.split()) #features = features.apply(lambda x : ' '.join([ps.stem(word) for word in x ])) # vectorizing the data #from sklearn.feature_extraction.text import TfidfVectorizer tv = TfidfVectorizer(max_features = 5000) #tv = TfidfVectorizer() #tv = TfidfVectorizer(max_features = 5000, vocabulary = vocabulary) features2 = list(features) features2 = tv.fit_transform(features2).toarray() print(""TFIDF Training dataset features vector succesfully created..."") #Turn the test dataset into the vectorized format as fitted to 'tv' features_final_test2 = list(features_final_test) features_final_test2 = tv.transform(features_final_test).toarray() print(""TFIDF Test dataset features vector succesfully created..."") TFIDF Training dataset features vector succesfully created... TFIDF Test dataset features vector succesfully created... Splitting of training dataset randomly The code below splits the features2 and labels datasets into a random training and test dataset (i.e. only usinfg the course project traiing data to train and test locally so far): NOTE THAT RANDOM_STATE is set to zero to provide a fixed seed to the random generator for consistent results In [16]: # Splitting of training data into train and test data features_train, features_test, labels_train, labels_test = train_test_split (features2, labels, test_size = .05, random_state = 0) rawdata_train, rawdata_test, rawlabels_train, rawlabels_test = train_test_s plit(features, labels, test_size = .05, random_state = 0) print(""Training dataset split into this many train samples:"") print(len(features_train)) print(len(rawdata_train)) print(""Training dataset split into this many test/validation samples:"") print(len(features_test)) print(""# of features:"") print(len(features_train[1,:])) print(""Final test dataset is this many samples:"") print(len(features_final_test2)) print(""# of features:"") print(len(features_final_test2[1,:])) Training dataset split into this many train samples: 4750 4750 Training dataset split into this many test/validation samples: 250 # of features: 5000 Final test dataset is this many samples: 1800 # of features: 5000 Model 1: Linear SVM The below code implements the SKLearn implementation of LSVM and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [34]: # Using linear support vector classifier lsvc = LinearSVC() # training the model lsvc.fit(features_train, labels_train) # getting the score of train and test data print(lsvc.score(features_train, labels_train)) # 96.04 print(lsvc.score(features_test, labels_test)) # 74.00 LSVM_train_acc= lsvc.score(features_train, labels_train) LSVM_test_acc= lsvc.score(features_test, labels_test) y_pred = lsvc.predict(features_train) y_true = labels_train LSVM_out_train = precision_recall_fscore_support(y_true, y_pred, average='b inary') print(LSVM_out_train) #96.06 y_pred = lsvc.predict(features_test) y_true = labels_test LSVM_out_test = precision_recall_fscore_support(y_true, y_pred, average='bi nary') print(LSVM_out_test) #72.80 0.960421052631579 0.74 (0.9581764951902969, 0.9630096679277007, 0.960587002096436, None) (0.7372881355932204, 0.71900826446281, 0.7280334728033473, None) Model 2: Guassian Naive Bayes The below code implements the SKLearn implementation of Guassian Naive Bayes and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [35]: # model 2: # Using Gaussian Naive Bayes gnb = GaussianNB() # training the model gnb.fit(features_train, labels_train) # getting the score of train and test data print(gnb.score(features_train, labels_train)) # 82.96 print(gnb.score(features_test, labels_test)) # 67.6 GNB_train_acc= gnb.score(features_train, labels_train) GNB_test_acc= gnb.score(features_test, labels_test) y_pred = gnb.predict(features_train) y_true = labels_train GNB_out_train = precision_recall_fscore_support(y_true, y_pred, average='bi nary') print(GNB_out_train) # 85.47 y_pred = gnb.predict(features_test) y_true = labels_test GNB_out_test = precision_recall_fscore_support(y_true, y_pred, average='bin ary') print(GNB_out_test) #69.43 0.8296842105263158 0.676 (0.7462358845671268, 1.0, 0.8546793605173344, None) (0.6388888888888888, 0.7603305785123967, 0.6943396226415093, None) Model 2: Logistic Regression The below code implements the SKLearn implementation of Logistic Regression and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [36]: # model 3: # Using Logistic Regression lr = LogisticRegression() # training the model lr.fit(features_train, labels_train) # getting the score of train and test data print(lr.score(features_train, labels_train)) # 85.85 print(lr.score(features_test, labels_test)) # 72.00 LR_train_acc= lr.score(features_train, labels_train) LR_test_acc= lr.score(features_test, labels_test) y_pred = lr.predict(features_train) y_true = labels_train LR_out_train = precision_recall_fscore_support(y_true, y_pred, average='bin ary') print(LR_out_train) # 86.15 y_pred = lr.predict(features_test) y_true = labels_test LR_out_test = precision_recall_fscore_support(y_true, y_pred, average='bina ry') print(LR_out_test) # 71.07 C:\Users\User-PC\Anaconda3\lib\site-packages\sklearn\linear_model\logistic. py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. S pecify a solver to silence this warning. FutureWarning) 0.8585263157894737 0.72 (0.8448484848484848, 0.8789407313997478, 0.8615574783683561, None) (0.7107438016528925, 0.7107438016528925, 0.7107438016528925, None) Model 4: Random Forest Classifier The below code implements the SKLearn implementation of Random Forest Classifier and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [37]: # model 4: # Random Forest Classifier rfc = RandomForestClassifier(n_estimators = 10, random_state = 0) # training the model rfc.fit(features_train, labels_train) # getting the score of train and test data print(rfc.score(features_train, labels_train)) # 99.09 print(rfc.score(features_test, labels_test)) # 66.8 RFC_train_acc= rfc.score(features_train, labels_train) RFC_test_acc= rfc.score(features_test, labels_test) y_pred = rfc.predict(features_train) y_true = labels_train RFC_out_train = precision_recall_fscore_support(y_true, y_pred, average='bi nary') print(RFC_out_train) # 99.09 y_pred = rfc.predict(features_test) y_true = labels_test RFC_out_test = precision_recall_fscore_support(y_true, y_pred, average='bin ary') print(RFC_out_test) # 63.11 0.9909473684210526 0.668 (0.9944961896697714, 0.987389659520807, 0.9909301835055896, None) (0.6826923076923077, 0.5867768595041323, 0.6311111111111112, None) Model 5: RoBERTa The below code implements the SimpleTransformers.ai implementation of RoBERTa and calculates a number of metrics including: 1. Accuracy on both train and test sets 2. The precision, recall and F1 score on the training data 3. The precision, recall and F1 score on the test data In [18]: from simpletransformers.classification import ClassificationModel,Classific ationArgs In [19]: train_df = pd.DataFrame({ 'text': rawdata_train.str.replace('@USER', '', regex=False).str.strip() , 'labels': rawlabels_train }) #rawdata_train, rawdata_test, rawlabels_train, rawlabels_test In [27]: test_df = pd.DataFrame({ 'text': rawdata_test.str.replace('@USER', '', regex=False).str.strip(), 'labels': rawlabels_test }) In [22]: # Create a ClassificationModel # for training from scratch model = ClassificationModel('roberta', 'roberta-base', use_cuda=False) # Yo u can set class weights by using the optional weight argument # for loading my pre-trained model #model = ClassificationModel('roberta', 'outputs/checkpoint-594-epoch-1', u se_cuda=False) Some weights of the model checkpoint at roberta-base were not used when ini tializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense .weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer _norm.bias', 'lm_head.decoder.weight'] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another arch itecture (e.g. initializing a BertForSequenceClassification model from a Be rtForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassifica tion from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceC lassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifie r.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'cl assifier.out_proj.bias'] You should probably TRAIN this model on a down-stream task to be able to us e it for predictions and inference. In [23]: #model_args = ClassificationArgs() #model_args.reprocess_input_data = True #model_args.overwrite_output_dir = True model_args = { ""reprocess_input_data"": True, ""overwrite_output_dir"": True } # Train the model model.train_model(train_df,args=model_args) C:\Users\User-PC\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py:21 6: UserWarning: Please also save or load the state of the optimizer when sa ving or loading the scheduler. warnings.warn(SAVE_STATE_WARNING, UserWarning) Out[23]: (594, 0.539390764095717) In [24]: # Evaluate the model - training result_train, model_outputs_train, wrong_predictions_train = model.eval_mod el(train_df, f1=f1_score, acc=accuracy_sc ore, prec= precision _score, rec= recall_sco re) In [26]: result_train Out[26]: {'mcc': 0.7212081013757768, 'tp': 2116, 'tn': 1969, 'fp': 402, 'fn': 263, 'f1': 0.8642025730038799, 'acc': 0.86, 'prec': 0.840349483717236, 'rec': 0.8894493484657419, 'eval_loss': 0.325839846517251} In [40]: roberta_train_result=result_train roberta_train_result['acc'] Out[40]: 0.8397894736842105 In [28]: # Evaluate the model - training result_test, model_outputs_test, wrong_predictions_test = model.eval_model( test_df, f1=f1_score, acc=accuracy_sc ore, prec= precision _score, rec= recall_sco re) In [29]: roberta_test_result=result_test roberta_test_result['acc'] result_test Out[29]: {'mcc': 0.6238264579949548, 'tp': 106, 'tn': 96, 'fp': 33, 'fn': 15, 'f1': 0.8153846153846155, 'acc': 0.808, 'prec': 0.762589928057554, 'rec': 0.8760330578512396, 'eval_loss': 0.4293329161591828} Performing predictions on project test data for LiveDataLab Perform predictions on the project test data for the competition for upload to LiveDataLab for grading In [38]: #Writing the LSVM predictions to the output file: LSVM_answers.txt y_pred = lsvc.predict(features_final_test2) f = open(""LSVM_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Guassian NB predictions to the output file: GNB_answers.txt y_pred = gnb.predict(features_final_test2) f = open(""GNB_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Linear Regression predictions to the output file: LR_answers.t xt y_pred = lr.predict(features_final_test2) f = open(""LR_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() #Writing the Random Forest Classifier predictions to the output file: RFC_a nswers.txt y_pred = rfc.predict(features_final_test2) f = open(""RFC_answers.txt"", ""w"") for i in range(len(id_final_test)): i_result = y_pred[i] pred_id = id_final_test[i] if i_result == 1: f.write(pred_id + ',' + ""SARCASM"" +""\n"") else: f.write(pred_id + ',' + ""NOT_SARCASM"" +""\n"") f.close() Comparison of baseline models In [41]: import matplotlib.pyplot as plt; plt.rcdefaults() #import numpy as np import matplotlib.pyplot as plt #Plot accuracy on training set for each model objects = ('LSVM', 'Gussian NB', 'Linear Regression', 'Random Forest','RoBE RTa') y_pos = np.arange(len(objects)) performance = [LSVM_train_acc,GNB_train_acc,LR_train_acc,RFC_train_acc,robe rta_train_result['acc']] plt.bar(y_pos, performance, align='center', alpha=0.5) plt.xticks(y_pos, objects) plt.ylabel('% accuracy') plt.title('% accuracy on training set') plt.show() #Plot accuracy on test set for each model objects = ('LSVM', 'Gussian NB', 'Linear Regression', 'Random Forest','RoBE RTa') y_pos = np.arange(len(objects)) performance = [LSVM_test_acc,GNB_test_acc,LR_test_acc,RFC_test_acc,roberta_ test_result['acc']] plt.bar(y_pos, performance, align='center', alpha=0.5) plt.xticks(y_pos, objects) plt.ylabel('% accuracy') plt.title('% accuracy on test set') plt.savefig('acc_test.png') plt.show() # Plot the precision, recall and F1 score on the test n_groups = 5 Prec_data = (LSVM_out_test[0],GNB_out_test[0],LR_out_test[0],RFC_out_test[0 ],roberta_test_result['prec']) Rec_data = (LSVM_out_test[1],GNB_out_test[1],LR_out_test[1],RFC_out_test[1] ,roberta_test_result['rec']) F1_data = (LSVM_out_test[2],GNB_out_test[2],LR_out_test[2],RFC_out_test[2], roberta_test_result['f1']) #GNB_data = (GNB_out_test(1:3)) #LR_data = (LR_out_test(1:3)) #RFC_data = (RFC_out_test(1:3)) # create plot fig, ax = plt.subplots() index = np.arange(n_groups) bar_width = 0.25 opacity = 0.8 rects1 = plt.bar(index, Prec_data, bar_width, alpha=opacity, color='b', label='Precision') rects2 = plt.bar(index + bar_width, Rec_data, bar_width, alpha=opacity, color='g', label='Recall') rects3 = plt.bar(index + 2*bar_width, F1_data, bar_width, alpha=opacity, color='r', label='F1 Score') plt.xlabel('ML Model') plt.ylabel('Scores') plt.title('Precision, recall and F1 score per model') plt.xticks(index + bar_width, ('LSVM', 'GNB', 'LR', 'RFC','RoBERTa')) plt.legend() plt.tight_layout() plt.savefig('pr_rec_f1.png') plt.show() In [ ]"
https://github.com/mattzeeee/TextInformationSystemsCourseProject	ProjectProposal_TheClassifiers.docx	CS 410: Text Information Systems Final Project - Proposal Team: The Classifiers October 22, 2020 Members Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) - captain Project Topic Option 4: Text Classification Competition Are you prepared to learn state-of-the-art neural network classifiers? All team members agree to put it in the necessary effort to learn applying state-of-the-art neural network classifiers to compete in completing the competition's task. Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods (in parentheses). Scipy (Experienced) Keras (Intermediate) TensorFlow (Intermediate) PyTorch (Intermediate) Metapy (Intermediate) Scikit-learn (Experienced) LSTM (Beginner) NLTK (Beginner) BERT (Beginner) XLNET (none) GPT2 (none) H20.GBM (none) H20.AutoML (none) Which programming language do you plan to use? Python
https://github.com/mattzeeee/TextInformationSystemsCourseProject	README.md	"CS 410 Text Information Systems Final Course Project Team: The Classifiers Members Praveen Pathri (ppathri2@illinois.edu) Steven Piquito (piquito2@illinois.edu) Mattias Rumpf (mrumpf2@illinois.eu) - captain Project Topic Option 4: Text Classification Competition - Sarcasm Detection on Twitter data Project Setup and Individual Contributions The submitted work represents a well balanced team effort: Each of the members spend considerable time on researching potential models and frameworks. Praveen and Steven worked on and tested baseline models like SVM and random forest, Matthias focused on pre-trained models. When BERT type models turned out to be the most promising, we jointly concentrated on improving the performance of a fine tuned RoBERTa model. Praveen looked into improving accuracy via data preparation, Steven added context, Matthias experimented with hyperparameter tuning. What we do We are fine tuning RoBERTa. RoBERTa, a neural language model that extends BERT and was pre-trained on even more massive amounts of text data, e.g. Wikipedia and news articles with a dynamically chaning masking pattern for the missing word prediction task. We have experimented with basic NLP models such as SVM, Random Forest, Naive Bayes (see our progress report in Appendix I) We do employ some basic data cleaning like stop word removal, further experimentation into data preparation has not improved model performance. We simply concatenated all context to the twitter response that was available. We experimented with hyperparameter tuning on google colab (to exploit their GPU offerings) and found a learning rate of 2.741032760877178e-05 and number of epochs equal 4 for fine-tuning to allow us generating predictions that beat the baseline (see appendix II and III). Result We beat the baseline using the fine-tuned RoBERTa model (user mattblack): Running our Code and replicating results Screencast tutorial You can follow the screen cast presentation, available at * https://youtu.be/K9CPy_nm3vs Install instructions To install all required libraries we assume that users have a recent install of the Anaconda distribution with Python <=3.7 If you run into trouble feel free to contact use (preferably Matthias: mrumpf2@illinois.edu) Instructions for windows users (for other OS adapt accordingly) * copy / clone the project repository to your working directory e.g.: C:\UIUC\TextMining\FinalProjectSubmission * Open Anaconda Navigator and create a new python 3.7 environment * Install and open the Anaconda App ""cmd prompt"", check that the newly created environment is activated * go to https://pytorch.org/get-started/locally/ and get your install command e.g. conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch or without cpu conda install pytorch torchvision torchaudio cpuonly -c pytorch * run the install command in the cmd prompt * enter in cmd prompt: pip install simpletransformers jsonlines pandas transformers nltk torch * cd to your working directory e.g.: cd C:\UIUC\TextMining\FinalProjectSubmission * then start notebooks by entering in cmd prompt: jupyter notebook Main documents Project Report Team The Classifiers - Twitter Sarcasm Detection (with a fine-tuned RoBERTa model) We use RoBERTa as our workhorse model, see https://paperswithcode.com/method/roberta To replicate our results run the following notebook * Jupyter Notebook see 1. Project Report and Classifier Code Team The Classifiers - Twitter Sarcasm Detection.ipynb * PDF see 1. Project Report and Classifier Code Team The Classifiers - Twitter Sarcasm Detection.pdf Appendix I.The progress report - initial experimentation Jupyter Notebook see Appendix I.The progress report - initial experimentation.ipynb PDF see Appendix I.The progress report - initial experimentation.pdf Appendix II. Google Colab Hyperparameter tuning use colab at https://colab.research.google.com/ if you want to test the code, note that you need to upload the data folder to your google drive, replace folder paths, and connect to google drive from with the colab notebook * Jupyter Notebook see Appendix II. Hyperparameter tuning.ipynb * PDF see Appendix II. Hyperparameter tuning.pdf Appendix III. Google Colab Roberta Training and Sarcasm Prediction use this code if you don't have a GPU to speed up training the model and retrieving the prediction results use colab at https://colab.research.google.com/ if you want to test the code, note that you need to upload the data folder to your google drive, replace folder paths, and connect to google drive from with the colab notebook * Jupyter Notebook see Appendix III. Roberta Training and Sarcasm Prediction.ipynb * PDF see Appendix III. Roberta Training and Sarcasm Prediction.pdf"
https://github.com/seanlai12/CourseProject	Documentation.docx	"Sean Lai Course Project CS410 Documentation Summary: This project is to improve the functionality of to save next slides in memory so users do not need to load each new slide every click. I have implemented this feature by using in-memory list within python when server is running, to load multiple slide pages at once, and as a queue, the system iterate through the list when the user clicks ""Next"" button on the EducationalWeb page. In the original, the EducationalWeb system would resolve each slide at the moment of user click, which would only allow a page to load one at a time, causing delays and negative user experience. Implementation: I have created a new function on app.py, which is the main function python file for the system, called `buffer_new_slides`. This function will iterate a number of slides, with number determined by the global variable SLIDE_BUFFER_SIZE, to fill up a python list SLIDE_BUFFER_LIST with generated slide renders. With this, users would experience a longer load when they first click through the slides, as the system would buffer multiple slide renders at once and save it to memory, but further iteration through the slides will be very fast as it's in memory. The list will work like a queue, so whenever a slide is read and ready to move on, the rendered slide is removed from the list, and a new one will be buffered for the user. This allows an in-memory speed of slide viewing experience for the user, instead of must loading a rendered slide object with `resolve_slides`, on every click. Please see my video demonstration for the code and operation for more details. Challenges: Because I did not a have a web server to test this on, the localhost server proved difficult to test if my change really made the slide viewing faster. This is due to the fact that original slide viewing was already fast enough since all the slide files were local. However, it is very small difference with speed with new change, and I expect it to be much more noticeable when the slides are up on a web server. Out of Scope: This implementation would only work on ""Next"" slide button, not going backward in pages with ""Prev"". The Prev button would work, however would mess up the order of the list. This can be fixed by implementing similar queuing system for Prev in the same way as Next, however due to time constraints I am only going to demo for Next button. How to use the software: # Documentation/Instructions The following instructions have been tested with Python2.7 on Linux and MacOS 1. You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html 2. Create the index in ElasticSearch by running `python create_es_index.py` from `EducationalWeb/` 3. Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under `EducationalWeb/static` 4. Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under `EducationalWeb/pdf.js/static/slides/` 5. From `EducationalWeb/pdf.js/build/generic/web` , run the following command: `gulp server` 6. On line 38 on `EducationalWeb/app.py`, edit the value here to an Integer for ""number of slides to buffer at a time"". I have set the default to ""5"", so you can also leave it as is. 7. In another terminal window, run `python app.py` from `EducationalWeb/` 8. The site should be available at http://localhost:8096/"
https://github.com/seanlai12/CourseProject	Presentation.pptx	Course Project Demo By: Sean Lai CS 410 Improving EducationalWeb Improve the functionality to save next slides in memory so users do not need to load each new slide every click. Faster loading for slides in bulk Better user experience Design and Implementation Original Design and Implementation After Demo Faster? It is hard to prove this on local server, since slide loading is already fast to begin with. However, I added logging message to the code so you can see how often the system renders before and after. However since the rendered slides are saved in a in-memory list, by design it should be faster. Challenges Setting up the environment took forever! Large amount of time spent on understanding the code. Load time on local server vs web server differs a lot, hard to see difference from my feature. Only had time to work on Next slide (going forward), not Prev. However, it should be implemented in a similar fashion (in-memory list rendering) Thank you
https://github.com/seanlai12/CourseProject	Project progress report.docx	Sean Lai CS410 Project Progress Report 1) Which tasks have been completed? 2) Which tasks are pending? 3) Are you facing any challenges? Completion summary of progresses using the Task that I listed in my original proposal, Spike the usage on the original system. 3h Set up environment for the project. 1h (This took much longer than estimate) Look for function and area that needs to be updated. 2h Coding and testing. 10h (About 50% done, 5h left) Collect data for time comparison. 2h Create documentation, report, and presentation material. 2h Currently, I used bulk of my time setting up the environment on Pycharm/terminal, going through the functionality, installing dependencies, and writing few logging lines in the code for better information gathering. I was underestimating the environment setup portion which took me much longer time, where I encountered many difficulties to get the local server to actually load, due to having to reinstall python and troubleshoot few things online. However, I finally got it to work on local server and can test code now which I am happy about. I am also getting more familiar with codebase and have pretty good idea on where to add/update code to complete my project. Pending tasks. Currently I am still not complete in the coding and testing part, but I got good plans laid out on what to do. Currently I am able to manipulate the slide pages on next and prev slides, skip a slide, etc. I am thinking to implement a list or queue dataset to pre-load multiple slides when moving slides. I am still implementing and testing so don't have much more details for now, but don't seem to face any big challenges that I know of. Challenges. I think the biggest challenge is to correctly gather timing for my final report, since the local server and web server version has different timings. However, if I can compare either one of them and find my implementation speed up the program, I should be happy enough to report that as successful. Please let me know if you have questions or need further details on anything. Thanks for reading.
https://github.com/seanlai12/CourseProject	Project proposal.docx	Sean Lai CS410 Project Proposal Improving a System: Educational Web System What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. What system have you chosen? Which subtopic(s) under the system? Briefly describe the datasets, algorithms or techniques you plan to use If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly Which programming language do you plan to use? Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Sean Lai - seanlai2 (Individual project) Educational Web System. Planning to improve the functionality of allowing downloading slide in bulk. Specifically, I am thinking to implement a buffer loading for previous and next slides so user experiences are better. Currently the system only allow one slide loading at a time and will only load the next or previous slide if user makes a click. Using data structures to store preloaded slides for the interface, possibly a list. Using updated functionality to improve the system on web interface. The dataset will be the slides to load on user clicks. This can be demonstrated by comparing the average time loading a slide in the original system, to the new system. This can be done in a report, and also video demonstration if required. My code will likely update a function within the original system, replacing or adding more onto the original logic. I will fork my own repo and make my update on my own environment. Python, JavaScript, and whatever else is needed. Task and hours rough estimate Spike the usage on the original system. 3h Set up environment for the project. 1h Look for function and area that needs to be updated. 2h Coding and testing. 10h Collect data for time comparison. 2h Create documentation, report, and presentation material. 2h Educational Web - http://timan102.cs.illinois.edu/explanation//slide/cs-410/0 Original Github Repo - https://github.com/CS410Fall2020/EducationalWeb
https://github.com/seanlai12/CourseProject	README.md	"CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Author - Sean Lai Email - seanlai2@illinois.edu CS410 Improving the Educational Web System. This project is to improve the functionality of to save next slides in memory so users do not need to load each new slide every click. Please read the documentation and instruction below on how to use EducationalWeb. Please take a look at Documentation.docx, for more detail on how to use the software and how the software is implemented. Documentation/Instructions The following instructions have been tested with Python2.7 on Linux and MacOS You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web , run the following command: gulp server On line 38 on EducationalWeb/app.py, edit the value here to an Integer for ""number of slides to buffer at a time"". I have set the default to ""5"", so you can also leave it as is. In another terminal window, run python app.py from EducationalWeb/ The site should be available at http://localhost:8096/"
https://github.com/yil7/CourseProject	Documentation.docx	"Text Classification Competition The main goal of this project is to develop a learning system that predicts the label of response on Tweet to be 'SARCASM' or 'NOT SARCASM' while optionally using the context by learning from responses labeled already. The trained model should achieve a fa score above 0.723 on the test data. We trained a machine learning model first, which is XGBoost as our baseline model. It can only achieve a 0.65 f1 score and overfit a lot. Therefore, we have been focused on deep learning models and tuning the parameter after more than 140 times, and we finally beat the baseline. We used the Convolutional Neural Network (CNN) to accomplish this text classification task and achieve an accuracy of 0.7247, which beats the baseline of 0.723. CNN is a class of deep neural network that can take in an input, assign importance to various objects in the input, and be able to differentiate one from the other. It can automatically learn a large number of filters in parallel specific to a training dataset under the constraints of a particular predictive modeling problem. We utilized Python and Jupyter Notebook to develop our learning system. The libraries related were Json, Pandas, String, Nltk, Numpy, Matplotlib, Sklearn, Re, Keras, GENISM, etc. The steps are as follows: Load data: We loaded the data line by line and converted the raw train.jsonl data to a data frame called ""reviews"". Clean and pre-process the data: We made a clean function to remove URL text such as http, @, #, and any numbers. The information from ""response"" and ""context"" was combined into a new column: ""text"". Then we tokenized the texts by using NLTK's word_tokenize so that a sentence is divided into single words. We also turned all letters to lower case. Next, we added two new columns to the 'reviews' data frame to prepare for the binary classification. Prepare Tain and test sets: There were 5000 objects to train the system and 1800 objects to test. We built training vocabulary and got maximum training sentence length (73) and the total number of words in the training set (12526). Load Google News Word2Vec model and trained word embeddings: After that, we loaded the Google News Word2Vec model and trained our word embeddings. Tokenize and Pad sequences: We assigned an integer to each word and put that integer in a list. We padded the sentences so that all training sentences had the same input shape (50). We got embeddings from the Google News Word2Vec model and saved them corresponding to the sequence integer assigned to each word. If there were no embeddings, a random vector was saved for that word. Define CNN: the content of 'text' as a sequence was passed to a CNN. The embeddings matrix was passed to embedding_layer. We applied five different filter sizes to each content and GlobalMaxPooling1D layers to each layer. Outputs were concatenated. Dropout layer - Dense layer - Dropout layer - final Dense layer was applied. We also printed a summary of all the layers with corresponding output shapes. Train CNN: The number of epochs and batch size we utilized were 3 and 80, which means our model will loop around and learn three times, and eighty data will be viewed at a single time. Since the training dataset was small, we took this relatively small number of epochs to avoid overfitting. Since sarcasm is hard to detect, we also lower the confidence level from 0.5 to 0.38 during the inference phase, which means if the classifier is 38 percent sure this sentence is sarcastic, then this sentence will be predicted as sarcasm. Test: We used the model to predict the label of data in the test set and get an accuracy of 0.7247. The CNN system could be utilized on other tasks, such as image recognition, Electromyography (EMG) recognition, which relates to identifying human intention to control assistive devices, video analysis, drug discovery, health risk assessment, and biomarkers of aging discovery, etc. Except for the CNN model, we have also tried another deep learning model: BERT (Bidirectional Encoder Representations from Transformers) model, though it didn't beat the baseline score. Bert model is a pre-trained model, and it typically has several advantages over other NLP models, including quicker training, smaller data size requirement, and higher prediction result. It is widely used in text feature extraction and text classification tasks. However, it performed well in our validation datasets but had inferior prediction results on the test data, which was surprising. To run a BERT model, we utilized the free GPU offered by Google Colab. The libraries we mainly used were TensorFlow, Torch, and Transformers package from Hugging Face library. We used the same data cleaning approaches as the CNN model and did not remove the stop words. We then used the pretrained Bert tokenizer method to convert the response into tokens. Each response was split into tokens of the same length. The shorter response text would be padded with token zero, and we added attention masks to differentiate the padded tokens from the true tokens. The maximum response size was 85, and we set the fixed size to be 120 in case the size of test text data is larger. We split the 5000 training data into 80% training data and 20% test data. The classification model we chose is BertForSequenceClassification. We tried to tune the hyperparameters such as batch size, learning rate, and the number of epochs. However, the best-tuned parameters made the test prediction results worse. In the end, we used batch size = 16, learning rate lr = 2e-5, eps = 1e-8 and epochs = 2. The metric we used to evaluate was the f1 score. When validating the model results from training, the f1 score we had was around 80%. Finally, we did the same procedure to the test data before modeling, and as mentioned above, the results were surprisingly not satisfactory. Data cleaning techniques and tuning parameters also didn't improve the results. Future attempts might include trying different BERT models and padding choices. Reference: CNN: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7 BERT: https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=DEfSbAA4QHas"
https://github.com/yil7/CourseProject	Progress Report.docx	"1. What we have done? We imported twelve necessary packages. We loaded data line by line and convert the raw train.jsonl data to a data frame called ""reviews"". Next, we made a clean function to remove URL text such as http, @, #, and any numbers. The information from ""response"" and ""context"" was combined into a new column: ""text"". We processed the information from ""text"" with our clean function. We also turned the ""label"" to 0-1 from ""SARCASM/NOT_SARCASM"". Then we separated both information from ""text"" and ""label"" into two sets: ""test"" set with 20% of raw data and ""train"" set with 80% of raw data for validation test. We transformed information from text train set and text test to vector with TfidfVectorizer (stopword-'English', range of words to be extracted is 1-3). We then applied the XG Boost model and find out the training accuracy is 0.96125 and test accuracy is 0.665. We also loaded raw test.jsonl data in to a data frame called ""test"" to do the predicted test and processed it with in the same way. We applied the XG Boost model to predict the label and convert 0-1 to SARCASM/NOT_SARCASM. 2. Remaining task We will try to find out whether more pre-processing could be done. The parameters of vectorizer may still be adjusted. We will also try to build a deep learning model - LSTM to better improve the accuracy of our model. 3. Challenges The accuracy of train set is good enough whereas the accuracy of test set is much lower. That means we might overfit."
https://github.com/yil7/CourseProject	project proposal.docx	In your project proposal, please answer the following questions: What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Which competition do you plan to join? If you choose the IR competition, are you prepared to learn state-of-the-art IR methods like query expansion, feedback, rank fusion, learning to rank, etc.? Name some more concrete methods or tools that you may have heard of. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Which programming language do you plan to use? 1. Team Members Wenxin Fang wenxinf2 Shengyi Wang shengyi4 (Captain) Yi Li yil7 2. Text classification competition 3. Baseline model: Logistic regression, Naive Bayes Our deep learning framework: LSTM Deep learning We have some experience in implementing and training logistic regression on a toy dataset. However, we never train and test this model on text data before. We haven't had some experience in Naive Bayes and LSTM deep learning framework. But we are thrilled to learn more about these machine learning approaches and look forward to seeing how it performs on the tweet dataset. 4. Python
https://github.com/yil7/CourseProject	README.md	CS 410 Final Project: Classification Competition Preparation Datasets used are test.jsonl and train.jsonl in the data folder. Required Packages scikit learn numpy xgboost Pytorch matplotlib seaborn nltk json pickle pandas re math tensorflow sys string gensim os keras Baseline Model All the work done within the jupyter notebook Text Classification baseline.ipynb CNN Model All the work done within the jupyter notebook CNN-final.ipynb BERT Model All the work done within the jupyter notebook BERT.ipynb Documentation Documentation.docx Video Illustration https://youtu.be/UJcDlOYOAZY Reference CNN: https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7 BERT: https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=DEfSbAA4QHas
https://github.com/yy228731/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities.
https://github.com/bo8b/CourseProject	Green Koalas Project Documentation.docx	"CS 410: Text Information Systems Group Project Documentation Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Project Installation: The following instructions have been tested with Python2.7 on Linux and MacOS Source code (including Team Green Koala enhancements) can be found at https://github.com/CS410Fall2020/CourseProject You should have ElasticSearch installed and running -- https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html Create the index in ElasticSearch by running python create_es_index.py from EducationalWeb/ Download tfidf_outputs.zip from here -- https://drive.google.com/file/d/19ia7CqaHnW3KKxASbnfs2clqRIgdTFiw/view?usp=sharing Unzip the file and place the folder under EducationalWeb/static Download cs410.zip from here -- https://drive.google.com/file/d/1Xiw9oSavOOeJsy_SIiIxPf4aqsuyuuh6/view?usp=sharing Unzip the file and place the folder under EducationalWeb/pdf.js/static/slides/ From EducationalWeb/pdf.js/build/generic/web, run the following command: gulp server In another terminal window, run python app.py from EducationalWeb/ The site should then be available at http://localhost:8096/ How to use: The EducationalWeb system was developed by some of the students in Prof. Zhai's research group for navigating through course slides. At present, it only contains the slides for CS410, but could be expanded in the future to include other courses. Note that if more courses are added in the future, the names for their directories and slides must follow the same naming conventions as used for CS410: Directory name can really be anything, but it may be helpful if it is a descriptive name of the lecture series. For instance, one CS410 directory was named ""06_lesson-3-6-evaluation-of-tr-systems-practical-issues_3.6_Evaluation_of_TR_Systems_-_Practical_Issues.txt"". Using a name this long is not required, nor is it even recommended, as it causes problems for Windows users. Then within that directory, the slide names must be sequentially numbered, starting at zero (""0"") in this format: class name (""cs-410"") followed by four dashes (""----"") followed by directory name followed by four dashes (""----"") followed by ""slide"" + number + "".pdf"" Below are some of the features of the EducationalWeb tool: Choosing a lecture using the drop-down list in the navigation bar Sequentially navigating through the lectures/slides using the Next and Prev buttons at the bottom of a slide Searching for relevant slides using the search bar at the bottom of a slide Navigating to a related/recommend slide from the column on the right Finding an explanation of a term/phrase on the slide by highlighting it and then clicking on the ""Explain selected text"" button on the top-right of a slide. It will try to retrieve a relevant section from Professor's textbook that contains an explanation of the selected phrase. Downloading the current slide being viewed by clicking on the ""Download"" button, to the right of the ""Explain selected text"" button. Downloading all slides for the lecture for the current slide being viewed by clicking on the ""Download All"" button, to the right of the ""Download"" button. (***NOTE: this is the function that was added by Team Green Koalas***) Download Download Explain selected text Explain selected text Download All Download All Implementation: First, in the /build/generic/web/viewer.js file, we included some code that allows us to see the url for the slide currently being viewed. Then, in the /build/generic/web/viewer.html file, we updated some code and wrote several new functions: alertAll() is used to get the current-slide URL we provided in viewer.js (above) and convert it to a usable format for our purposes. getURLs() converts the formatted current-slide URL into a list of potential slides that may be contained within that file's folder. zipItUp() takes that list of potential file names and adds any existing files to a zip file (""slide-series.zip"") and then provides the opportunity for a user to save that zip file through their browser. We also added a new button ""Download All"" to initiate this function. Finally, in the /build/generic/web/viewer.css file, we made changes necessary to format the ""Download All"" button referenced above. We also fixed the graphic for the existing button for the ""Explain Selected Text"" button to allow it to fit properly. Justification: Our intention was to also take on one additional improvement (performance, enabling new courses, etc.), but ran into several impediments that affected our ability to do so. First, setup of the EducationalWeb environment took much, MUCH longer than anticipated. One team member was tracking his time and can show (in Timesheet.xlxs) that he spent over 10 hours just trying to configure the system. Most of the problems were caused either by attempting to set up the tool in a Windows environment or using a newer version of Python. Another team member finally had success on a Mac, so the third team member borrowed a Mac computer, which allowed him to finish setup relatively quickly after that. Also, we did not realize when taking on this project that the work that needed to be done was almost entirely in Javascript. One team member has no experience in Javascript, and the other two have only slight knowledge, so it took a while just to get up-to-speed with this technology. If we had known more, we would not have been so cavalier in just assuming we'd be able to zip an entire folder at once or else get a list of files from a server folder on a client Javascript - which we now know we cannot do. MUCH time was spent researching ways to do this, including Node.js, JQuery, PHP, Ajax, and Kintone. We finally reverted to just using the file naming convention in our favor and assuming there would be fewer than 99 files in any lecture directory. According to the Timesheet.xlsx file, one team member spent over 30 hours on this project, which exceeds the requirements for this assignment, so we did not address any issue beyond the multiple-slide download enhancement."
https://github.com/bo8b/CourseProject	Green Koalas Project Progress Report.docx	CS 410: Text Information Systems Group Project Progress Report Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Project Progress Report: 1) Which tasks have been completed? Two of the three team members have successfully installed the EducationalWeb tool in our local environments. 2) Which tasks are pending? Work has begun on enabling downloads for multiple slides at a time, but it is not completed yet. We need to complete that and (possibly) work on a performance improvement. We then need to produce the video talk. 3) Are you facing any challenges? Getting the EducationalWeb system up and running in local proved to be more difficult than expected, particularly in Windows. One team member is still struggling with this, but we are helping him through the last of his issues. Also, none of us are Javascript experts, so it is taking more time than we originally thought even to do a relatively simple enhancement. We are committed to completing on time, though.
https://github.com/bo8b/CourseProject	Green Koalas Project Proposal.docx	CS 410: Text Information Systems Group Project Proposal Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Proposal: For this group project, Team Green Koalas will be improving the Educational Web System. The Educational Web System is a tool recently developed by some of the students in Professor Zhai's research group that allows students to view all lecture slides from course CS 410 (Text Information Systems). This tool supports choosing a specific lecture's worth of lecture slides, sequential navigation through individual slides, searching for relevant slides, navigating to other recommended slides, and finding term and phrase definitions from the textbook. Specifically, we will develop the ability to allow for bulk downloading of slides. This simple enhancement would provide concrete value to future students, instructors, and other users of this system for years to come. Understanding that the expectation is for each group member to spend around 20 hours of effort on this assignment, if we are able to complete the bulk download in less than 60 hours (which seems likely), we will then move onto another topic, such as improving performance or expanding the content to include slides and textbooks from other courses. This could potentially expand the user base to students and instructors of other courses as well. To the best of our knowledge, no other tool of this type exists, at least specific to the lecture data for which we are using it. In terms of existing resources, we would obviously be modifying the source code for the tool. If we do take on the task of expanding to include other courses, we will require access to the lecture slides and possibly the textbooks for any additional classes. The first step would have to be securing permission from the owner of the lectures and textbooks to use these resources, to ensure we do not violate Intellectual Property rights. For downloading slides in batches, it should be easy enough to do through stitching individual slides into a single PDF or some other method. For improving performance, we would investigate buffering next slide and previous slide to speed up that portion of the application. If we expand to new courses, then no technology would change, we would merely be making sure the application is scalable enough to handle the new data. It should be a trivial effort to show the usefulness of bulk downloading or improved performance through a recorded demonstration comparing the current state with the new functionality. If we expand to include other classes, perhaps we could include a student from the other class or another student from our class (if the new course is not currently in session) and record a quick usability study. We expect to have a general understanding of the existing application by week 10. We will take weeks 11 and 12 to enable bulk download. Weeks 13 and 14 can then be used for one additional enhancement, followed by Week 15, which will be set aside for the recording the demonstration.
https://github.com/bo8b/CourseProject	Green Koalas Project Tutorial Video.docx	CS 410: Text Information Systems Group Project Proposal Team Green Koalas Team Members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Tutorial Video Link: https://youtu.be/geXK8Ohcg3c
https://github.com/bo8b/CourseProject	README.md	"Team Green Koalas CS410 Group Project For this group project, Team Green Koalas will be improving the Educational Web System. The Educational Web System is a tool recently developed by some of the students in Professor Zhai's research group that allows students to view all lecture slides from course CS 410 (Text Information Systems). This tool supports choosing a specific lecture's worth of lecture slides, sequential navigation through individual slides, searching for relevant slides, navigating to other recommended slides, and finding term and phrase definitions from the textbook. Specifically, we will develop the ability to allow for bulk downloading of slides. This simple enhancement would provide concrete value to future students, instructors, and other users of this system for years to come. This enhancement required each group member to spend around 20 hours of effort on this assignment, so we were not able to move onto any another topic. Team members: Yuxiang Huang - yh43@illinois.edu Bob Manasco - manasco2@illinois.edu - Group Coordinator Cullen Stone - cpstone2@illinois.edu Submission Files: Project Proposal - ""Green Koalas Project Proposal.docx"" Project Progress Report - ""Green Koalas Project Progress Report.docx"" Project Documentation - ""Green Koalas Project Documentation.docx"" Project Tutorial Video"
https://github.com/purecod3/CourseProject	ProgressReport.pdf	Progress Report Ed Pureza (epureza2@illinois.edu) Dansi Qian (dansiq2@illinois.edu) Joe Everton (everton2@illinois.edu) Change of Scope We initially planned to reproduce Latent Aspect Rating Analysis without Aspect Keyword Supervision. We read through the paper, discussed among ourselves, and documented our understandings here. We had an hour-long conversation with Prof. ChengXiang Zhai (one of the authors of the paper) and email correspondence with Prof. Hongning Want (the main author), and eventually decided that none of us had the substantial math background required to reproduce the paper. Instead, we decided to reproduce Latent Dirichlet Allocation (https://dl.acm.org/doi/pdf/10.5555/944919.944937 and http://times.cs.uiuc.edu/course/510f18/notes/lda-survey.pdf), which was referenced by the original paper and also described briefly in week 9 of the course. Which tasks have we completed? * We have read both papers for Latent Dirichlet Allocation (LDA) and documented our understanding here. There is no analytical solution to the E-M algorithm in LDA. Instead, the optimization can be done using variational inference or Gibbs Sampling. * We have implemented LDA using Gibbs Sampling (an initial version without learning rate, and a subsequent version that applies a learning rate across iterations). * We have implemented pre-processing for text-classification datasets, the LDA model training on term frequencies, the inference of new documents using trained model (note that LDA is generative), and compared the classification accuracy of Support Vector Machines (SVM) using term frequencies and using topic weights. There were no significant differences in accuracy between the two for the text message spam filter dataset (short documents) or for the fake news detection dataset (long documents). Which tasks are pending? * We are working on the variational inference version of LDA (https://github.com/purecod3/CourseProject/blob/main/lda_var_inf.py). * We plan to compare the accuracy of SVM classification using the variational inference version of LDA with the baseline (SVM using term frequencies). What challenges do we face? * There do not seem to be canonical ways to implement either the variational inference or the Gibbs sampling for LDA. There are some resources on implementation details but they are not necessarily correct or perform well. As a result, we need to compare multiple resources based on our understanding of the algorithm, and potential implement multiple versions and compare their results. * There is a significant amount of hyperparameter tuning (number of topics, learning rate (and decay) for Gibbs Sampling, stop criterion definition and threshold for the variational inference, etc.). There is very little literature about how the hyperparameters should be set or tuned for LDA in practice so it would require trail and errors on our side.
https://github.com/purecod3/CourseProject	Project Proposal_ Reproduce Latent aspect rating analysis without aspect keyword supervision.pdf	Project Proposal: Reproduce Latent aspect rating analysis without aspect keyword supervision Team Members: * Ed Pureza epureza2@illinois.edu (Team Leader) * Dansi Qian dansiq2@illinois.edu * Joe Everton everton2@illinois.edu Paper Chosen: * Latent aspect rating analysis without aspect keyword supervision * Latent aspect identification (without supervision) * Latent aspect rating * Latent aspect weighting * LARAM vs. existing solutions / LARA (paper 23) OR newer for reference / comparison Programming Language * Python Notable Resources / Libraries * Numpy Dataset for Evaluation * Original dataset used in the paper: http://times.cs.uiuc.edu/~wang296/Data/ * TripAdvisor * Amazon Rough Timeline 11/01/2020 Kickoff 11/22/2020 MVP Ready 11/29/2020 Code complete and ready for tuning if necessary 12/06/2020 Code complete and final results ready 12/13/2020 Final report ready
https://github.com/purecod3/CourseProject	ProjectDocumentation.pdf	"Topic Mining with LDA (and LARAM) CS 410 Text Information Systems Course Project December 2020 Authors: Dan Qian (dansiq2), Joe Everton (everton2), Ed Pureza (epureza2) CMT ID: 84 Introduction 1 Latent Aspect Rating Analysis without Aspect Keyword Supervision 2 Latent Dirichlet Allocation 2 Objective 3 Data 3 Evaluation criteria 3 Tools 4 Methods 4 Variational Inference 5 Gibbs Sampling 5 Experiment Results 6 Conclusion 8 Successes 9 Opportunities 9 References 9 Appendix 10 LDA VS PCA 10 Introduction Our team originally selected the research paper titled ""Latent Aspect Rating Analysis without Aspect Keyword Supervision"" by Honging Wang, Yue Lu, and ChengXiang Zhai. After spending a considerable amount of time on attempting to understand the implementation details, we decided there were too many obstacles to continue recreating the research paper's results. Taking into consideration Professor Zhai's counsel, we decided to switch our topic to Latent Dirichlet Allocation. Our work is based on ""Latent Dirichlet Allocation"" by David Blei, Andrew Ng, and Michael Jordan [3]. Latent Aspect Rating Analysis without Aspect Keyword Supervision When a review is submitted for a product, it can be assumed that the reviewer had different aspects of the product in mind and a priority on those aspects that led to a given overall rating. The objective of latent aspect rating analysis without aspect keyword supervision is to identify the following in a set of reviews: * Main aspects that the reviewers considered * Weights of the identified aspects in each review and overall * Ratings given to each aspect identified The generative model involved inferring the latent aspect assignments (i.e., probability of a word drawn from a topic) and the aspect weights for each document and estimating the following corpus-level parameters: * Distribution of vocabulary words in each aspect * Prior distribution of aspects for the whole corpus * Sentiment polarity of each vocabulary word for each aspect * Average weight of each aspect in the ratings * Variance of weights for each aspect * Variance of the ratings Our main challenges included implementing the log-likelihood of each review that required an understanding of Jensen's inequality for convex functions and finding the maximums of the latent aspect assignments and prior distribution of aspects that involved implementing gradient-based optimization solutions. After an office hour session with Professor Zhai, correspondence with Dr. Hongning Wang and evaluating the effort required to bring this type of project to completion, we decided to switch our focus to Latent Dirichlet Allocation, which we found the implementation to be more feasible given the timeline and amount of available information on the subject. Latent Dirichlet Allocation Latent Dirichlet Allocation (LDA) is a generative probabilistic model for identifying topics in a collection of documents using Bayesian modeling. LDA draws the topic assignment of a word from a distribution of topics and the word from the word distribution of topics for a document. Unlike PLSA where the documents in the corpus define the probabilities, LDA can be used to generalize new documents by sampling from the distributions found by the model (i.e., the dataset used to generate the model serves as training data for new data). This is possible thanks to hyperparameters to Dirichlet distributions that generate the multinomial distributions, rather than basing them on training documents. Objective Our first objective was to have LDA find k topics from a corpus of documents. This would give us a sense at how well our implementation is doing at learning the topics from the corpus. The model will also be able to predict topic weights for new documents. Our objectives also include applying document topic weights predicted by the model to a text classification task. One way of evaluating the utility of the model discussed in the paper is to see how well the topic models can be used to to train a document classifier, compared to classifying with a plain bag of words feature set based on the same documents. Using the topic distribution inferred by the model and labeled data, a classification model can be generated. The two classification applications covered in this paper include the identification of fake news and spam messages. It is interesting to compare and contrast this approach of classification with LDA to Principal Component Analysis. We explore this relationship in the appendix. Data The original data set could not be found. We decided to use two other publicly available datasets from Kaggle. Evaluation criteria We will use Support Vector Machine (SVM) as our classification model for both LDA outputs and document terms. Using document terms to measure a baseline, the following results were obtained: Dataset Source # of Docs # of Terms Classes Spam https://www.kaggle.com/team -ai/spam-text-message-classif ication 5,157 8,741 Spam: 13% Not Spam: 87% Fake News https://www.kaggle.com/moha madalhasan/a-fake-news-dat aset-around-the-syrian-war 804 10,157 Fake: 47% Not Fake: 53% Dataset Number of Features Training Data Size Precision Recall Average F1 Fake News 2,020 200 0.562 0.694 0.621 Our success criteria is to achieve better or equal performance metrics with LDA. Doing so would prove that similar or better classification accuracy can be achieved using significantly less features. Tools Python was the programming language for this project. The following packages were used: numpy scipy math pandas re random sklearn time Methods We attempted two different methods for creating the LDA models. The first method uses variational inference as explained in the original Latent Dirichlet Allocation paper [3] and further explained by Chase Geigle [4] to calculate the word assignment to a topic and the topic distribution for each document (posterior inference). The general steps are outlined in the figure below (from [3]). The second method uses Collapsed Gibbs Sampling for the posterior inference which resembles a Markov-chain Monte Carlo method. The method is described in Geigle's paper and the implementation is detailed in Sterbak's' post [7]. Our implementation differed from the original LDA paper in the following ways: 1. We used random initialization for word topic assignments and document topic distributions 2. We borrowed the method implemented in gensim for updating alpha which claims [8] to use the Newton-Raphson method described in [3]. This method uses an additional Spam 892 2,000 0.992 0.855 0.919 learning rate parameter . In some cases this alpha updating code did not work well; to  get around this, we used a simpler linear combination to calculate alpha. 3. We replaced the log likelihood estimation with a distance measurement for the document topic distribution. The decision to deviate from the papers was based on our level of understanding, the level of complexity involved in original papers, and the amount of time allocated for the project. Variational Inference This method uses an EM algorithm similar to what is used in PLSA with the exception of the variables updated and parameters estimated. The corpus level parameters include the following priors: * : the parameter to the dirichlet distribution which forms the topic multinomial distribution in the training data * b: the parameter to the dirichlet distribution which forms the word multinomial distribution for each topic These parameters are estimated in the M step. The following document level variables are inferred the E step: * F: word topic assignment * : topic assignment Gibbs Sampling The difference in this method is the use of random sampling given the conditional distributions calculated in iterative steps. The word topic assignment is the main parameter updated with each iteration. Snippet from lda_var_inf_without_smoothing.py Experiment Results For our experiments, we set the number of topics (k) to 15. The max number of EM iterations was set to 10 as well as the maximum number of E step iterations. Precision, Recall, and F1 Scores Feature set Data set Training Set Size Vocabulary Size Num Topics Precision Recall F1 Baseline (bag of words) Spam 2000 892 N/A 0.992 0.855 0.919 Variational Inference Spam 2000 892 5 0.889 0.526 0.661 Variational Inference Spam 2000 892 10 N/A 0 N/A Variational Inference Spam 2000 892 15 N/A 0 N/A Gibbs Sampling Training + Variational Inference Spam 2000 892 5 0.899 0.763 0.826 Gibbs Sampling Training + Variational Inference Spam 2000 892 10 0.912 0.822 0.865 Gibbs Sampling Training + Variational Inference Spam 2000 892 15 0.930 0.789 0.854 Gibbs Sampling Spam 2000 892 5 0.886 0.770 0.824 Gibbs Sampling Spam 2000 892 10 0.919 0.895 0.907 Gibbs Sampling Spam 2000 892 15 0.941 0.842 0.889 Our setup for evaluation is different from the original paper in two ways: * Different datasets were used as shown in the figure below (extracted from [3]) * Size of dataset and proportion of data used for training Blei, Ng, and Jordan's paper produced good accuracy results with SVM classification as described above using topic weights as parameters. Their classifier outperformed the complete bag-of-words SVM classifier most of the time. These plots use proportion of training data as Feature set Data set Training Set Size Vocabulary Size Num Topics Precision Recall F1 Baseline (bag of words) Fake News 200 2020 N/A 0.562 0.694 0.621 Variational Inference Fake News 200 2020 5 0.557 0.874 0.681 Variational Inference Fake News 200 2020 10 0.605 0.802 0.690 Variational Inference Fake News 200 2020 15 0.570 0.694 0.626 Gibbs Sampling Training + Variational Inference Fake News 200 2020 5 0.555 0.640 0.594 Gibbs Sampling Training + Variational Inference Fake News 200 2020 10 0.576 0.784 0.664 Gibbs Sampling Training + Variational Inference Fake News 200 2020 15 0.619 0.586 0.602 Gibbs Sampling Fake News 200 2020 5 0.563 0.721 0.632 Gibbs Sampling Fake News 200 2020 10 0.570 0.766 0.650 Gibbs Sampling Fake News 200 2020 15 0.587 0.550 0.567 their x axis, which we did not analyze, but they give some good references for relative accuracy between bag-of-words SVM and LDA-SVM. For our reproduction of their paper, we used two other data sets because we could not find the one cited in the paper. Our results are not directly comparable to Blei et al's. However, our LDA/SVM classifier did outperform the bag-of-words/SVM classifier baseline on the fake news dataset we used, which approaches Blei's findings. With more tuning, we may nudge our results even closer. Our results are in the table above. Improvements are in green. These results show that we have likely reproduced the paper reasonably well. We note that for the Spam/Ham dataset, the results are consistently worse than the baseline. This indicates that this LDA/SVM classifier does not perform as well on shorter documents with smaller vocabularies. The topics found are probably less useful for classification. This is not inconsistent with Blei's findings; their dataset was based on newspaper articles, and probably closer to the fake news dataset we used that had better results. Conclusion 1. Aspect weights outperform the baseline (bags of words) as features for text classification, when the documents are long and the vocabulary is large (news articles in our experiments). 2. Aspect weights underperform the baseline (bags of words) as features for text classification when the documents are short and the vocabulary is small (text messages in our experiments). 3. Using more aspects does not necessarily improve the quality of the aspect weights as features. Successes We successfully implemented Latent Dirichlet Allocation, following as much as possible Blei, Ng, and Jordan's paper [3], and using two different approaches: collapsed Gibbs sampling, and variational inference. We drew conclusions based on experiments with each approach across two data sets. Interestingly, cutting the features down from thousands (bag of words) to around 10 (topics) gave comparable, and for longer documents, better results. Opportunities If we had more time, it would be nice to try the following: * Complete the implementation of LARAM, our original intended paper. This paper required more understanding of variational inference, and left the exercise of calculating a complicated log likelihood and some parts of the E and M steps to the reader. With more time, we might figure these details out. * Complete or borrow the log-likelihood calculation for LDA for proper termination of the EM algorithm. * Compare our results against library implementations of LDA, including Gensim and NLTK. References 1. H. Wang, Y. Lu, C Zhai. Latent aspect rating analysis without aspect keyword supervision https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/2020408.2020505 2. S. Kapadia. Evaluate Topic Models: Latent Dirichlet Allocation (LDA). A step-by-step guide to building interpretable topic models https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation -lda-7d57484bb5d0 3. D. Blei, A. Ng, M. Jordan. Latent Dirichlet Allocation https://dl.acm.org/doi/pdf/10.5555/944919.944937 4. C. Geigle. Inference Methods for Latent Dirichlet Allocation. http://times.cs.uiuc.edu/course/510f18/notes/lda-survey.pdf 5. Various. StackExchange https://stats.stackexchange.com/questions/126268/how-do-you-estimate-alpha-paramet er-of-a-latent-dirichlet-allocation-model 6. Various. Github: RaRe Technologies - gensim. https://github.com/RaRe-Technologies/gensim/blob/6c80294ad8df16a878cb6df586c797 184b39564a/gensim/models/ldamodel.py#L434 7. T. Sterbak. Latent Dirichlet Allocation from Scratch https://www.depends-on-the-definition.com/lda-from-scratch/ 8. How do you estimate a parameter of a latent dirichlet allocation model? https://stats.stackexchange.com/questions/126268/how-do-you-estimate-alpha-paramet er-of-a-latent-dirichlet-allocation-model Appendix LDA VS PCA It is interesting how LDA can be viewed in this setting as a substitute for Principal Component Analysis (PCA). PCA is a more general technique that can be used to reduce the dimensionality of data. A bag of words implementation may work with some classification algorithms and some data sets, but the matrices can become prohibitively large, sometimes making it difficult to train a model. PCA can help by reducing the features from |V| to some smaller k, without losing very much of the variation of the data, or its prediction power, even with surprisingly low values of k. This can work fairly well even when k is 5% or less of |V|. The downside of PCA is that the compressed feature set is not as interpretable. The k new features will usually map to several words that may not be related, or form any topic meaningful to humans. In spite of this, the compressed features are often still useful for prediction. Compared to using LDA as outlined above to generate topic distributions of documents, and then classify those distributions rather than the entire bag of words, LDA is also reducing the dimensionality from |V| to k, but it is designed to result in k topic  distributions that should be more explainable in terms of topics than the K dimensions PCA created. To put this in a concrete example, a spam classifier might take as inputs, the weights of 10 topics in each new document, as predicted by the LDA model. These 10 weights could then be used as features in the spam classifier. PCA might do just as well at this compression of features, but its features will not be explainable as topics."
https://github.com/purecod3/CourseProject	ProjectTutorial.pdf	How to Use Programming Language and Packages * Python 3.x * Packages: pandas, numpy, scipy, sklearn, math, re, random, time Executing Code Fork or download Github repo. Open in IDE and run file(s) or use command prompt (e.g., python lda_var_inf_without_smoothing.py). Start with lda_var_inf_without_smoothing.py. If unexpected results are encountered, try lda_var_inf_without_smoothing_v2.py. Optionally, you can also try the other variations with lda_gibbs_sampling.py and lda_var_inf.py. To use a different input dataset, your file will need text and classification columns. Modify the source file (input_path) and column settings (text_column, label_column) in the load_csv function call. (vocabulary_size, training_term_doc_matrix, training_labels, testing_term_doc_matrix, testing_labels, vocabulary) = load_csv(input_path = 'FA-KES-Dataset.csv', test_set_size=100, training_set_size=200, num_stop_words=50, min_word_freq=5, text_column='article_content', label_column='labels', label_dict = {'1': 1, '0': 0}) lda_var_inf_without_smoothing_v2.py has both datasets (fake news and spam) coded. Comment/uncomment to switch between datasets. Setting Parameters Set the following parameters to tune the model: * num_topics: number of topics to model lda.train(num_topics=10, term_doc_matrix=training_term_doc_matrix, iterations=20, e_iterations=10, e_epsilon=0.1, initial_training_set_size=50, initial_training_iterations=20) See video walk-thru for additional information.
https://github.com/purecod3/CourseProject	README.md	Latent Dirichlet Allocation This project is based on the paper written by D. Blei, A. Ng, and M. Jordan - Latent Dirichlet Allocation. https://dl.acm.org/doi/pdf/10.5555/944919.944937. Latent Dirichlet Allocation estimates topic disributions and topic word distributions in a generative model that can be used to infer topic distributions and word topic assignments for new documents. We use this modeling capability to evaluate the application of LDA on classification of documents using a significantly reduced number of features as compared to a bag of words based classification method. Team Members Ed Pureza, epureza2 (captain) Dan Qian, dansiq2 Joe Everton, everton2 Files |Deliverable|File|Description| |----------|----|-----------| |Project Proposal|Project Proposal_Reproduce Latent aspect rating analysis without aspect keyword supervision.pdf|Original project proposal submitted on October 24, 2020| |Progress Report|ProgressReport.pdf|Progress report with accomplishments, challenges, and remaining planned activities as of November 29, 2020| |Project Documentation|ProjectDocumentation.pdf|Project documentation submitted December 8, 2020| |Project Video Walk-through|https://mediaspace.illinois.edu/media/t/1_jbzbbspv|Video presentation of project| |Project Tutorial|ProjectTutorial.pdf|Project tutorial for reproducing experiments (also outlined below)| |LDA without Smoothing|lda_var_inf_without_smoothing.py|Code for running LDA using variational inference and gensim-based alpha update method| |LDA without Smooting v2|lda_var_inf_without_smoothing_v2.py|Code for running LDA using variational inference. Use if Python environment setup issues are encountered.| |LDA with Collapsed Gibbs Sampling|lda_gibbs_sampling.py|LDA implementation using Collapsed Gibbs Sampling| |Original LDA Code with Variational Inference|lda_var_inf.py|First attempt for implement LDA with variational inference method| |Fake News Dataset|FA-KES-Dataset.csv|Input dataset with news articles classified as fake news or not fake news| |Spam Dataset|spam.csv|Input dataset with news articles classified as spam or ham (not spam)| How to Use Progamming Language and Packages Python 3.x Packages: pandas, numpy, scipy, sklearn, math, re, random, time Executing Code Fork or download Github repo. Open in IDE and run file(s) or use command prompt (e.g., python lda_var_inf_without_smoothing.py). Start with lda_var_inf_without_smoothing.py. If unexpected results are encountered, try lda_var_inf_without_smoothing_v2.py. Optionally, you can also try the other variations with lda_gibbs_sampling.py and lda_var_inf.py. To use a different input dataset, your file will need text and classification columns. Modify the source file (input_path) and column settings (text_column, label_column) in the load_csv function call. python (vocabulary_size, training_term_doc_matrix, training_labels, testing_term_doc_matrix, testing_labels, vocabulary) = load_csv(input_path = 'FA-KES-Dataset.csv', test_set_size=100, training_set_size=200, num_stop_words=50, min_word_freq=5, text_column='article_content', label_column='labels', label_dict = {'1': 1, '0': 0}) lda_var_inf_without_smoothing_v2.py has both datasets (fake news and spam) coded. Comment/uncomment to switch between datasets. Setting Parameters Set the following parameters to tune the model: - num_topics: number of topics to model python lda.train(num_topics=10, term_doc_matrix=training_term_doc_matrix, iterations=20, e_iterations=10, e_epsilon=0.1, initial_training_set_size=50, initial_training_iterations=20) See video walk-thru for additional information.
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	CS 410 Porject Proposal.pdf	Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Matthew Walowski mlw6@illinois.edu Captain Zoheb Satta satta2@illinois.edu Jiacheng Guo jg17@illinois.edu 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic (Free Topic): Our application will have the user rank a set of movies/tv shows in their preferred order (favorite to least favorite). We will then use sentiment analysis on publicly available review data for the provided movies/tv shows to predict how the average person would rank these movies. From this, we can inform the user how unique their movie tastes are. I.E. we can tell the user if they ranked the movies similarly to the public. Interest: Making an analysis of the sentiment of these movie reviews could help people know the general category of these movies in advance. It will help people to judge if they would like to watch this movie and choose their own favorite movie. Approach\Tools: We will utilize The Movie Database API(https://developers.themoviedb.org/) to get review information about each of the movies. To perform sentiment analysis we will use an existing python library (NTLK https://www.nltk.org/howto/sentiment.html). We will experiment with different modes of feedback for the user (how do we express the similarity between their ranking and the general ranking). 3. Which programming language do you plan to use? Python backend to do all the data processing, javascript/html (react or angular) for frontend. 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Workload: Scraping API data - 10 hours - Learn specifics about API usage - Get movie ID from movie title - Get reviews from movie ID - Download and parse movie reviews Sentiment Analysis - 15 hours - Given a list of reviews, perform sentiment analysis on all - Aggregate results - Output ranking of movies Similarity Measure - 10 hours - Take a list of user rankings and list of generated rankings - Output a numerical representation of similarity - This requires a bit of experimenting so it will take extra time, we test multiple techniques to find a suitable solution: - Ex: Try MAP, gMAP, simple difference, VSM,... Web Interface - 30 hours - Build a web interface to interface with the user - Problems to solve: - How to make it intuitive for users? - How to communicate between python backend and javascript frontend Total Hours: 65 hours
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	CS410 Progress Report.pdf	"Overview We are creating a web app that allows users to rank their favorite movies. We will then do sentiment analysis on reviews of those movies to show the user how similar their ranking is to how the general public would rank those movies. There are 4 components to this project * Frontend * Backend REST Endpoints * Review Scraping * Sentiment Analysis Frontend The frontend is finished. It's also been integrated with a mock endpoint until we create the real endpoint. See image below for screenshot of frontend. Backend REST Endpoints The web frontend will do the sentiment analysis by sending the user data to the backend which will run our python scripts. Right now, the frontend is communicating with a backend endpoint that returns fake data. We just need to drop in the real script once it's done. Review Scraping The scraping of data was done using BeautifulSoup and Python Requests. We scrape the Metacritic website for user reviews as well as critic reviews then combine them into one single array containing all the arrays as strings. Sentiment Analysis Sentiment analysis (also known as opinion mining is a text analysis technique that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph, sentence, or clause. Understanding people's emotions is essential for businesses since customers express their thoughts and feelings more openly than ever before. In sentiment analysis, we use some packages like nltk.classify, nltk.corpus, nltk.sentiment, nltk.sentiment.util. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings, like the following example (['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj'). We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets like the following train_subj_docs = subj_docs[:80]; test_subj_docs = subj_docs[80:100]. Then, we use simple unigram word features, handling negation: sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats) and apply features to obtain a feature-value representation of our datasets: training_set = sentim_analyzer.apply_features(training_docs); test_set = sentim_analyzer.apply_features(testing_docs). We can now train our classifier on the training set, and subsequently output the evaluation results, which is shown as the follows: Accuracy: 0.8, F-measure [obj]: 0.8, F-measure [subj]: 0.8, Precision [obj]: 0.8, Precision [subj]: 0.8, Recall [obj]: 0.8, Recall [subj]: 0.8 Challenges The biggest challenge is to familiar myself with the NLTK library during our sentiment analysis. The other challenge is due to the lack of api, I have to create a function that will translate the input movie title, into a url-appropriate title. For example: ""The Matrix"" -> ""the-matrix"" Future Steps * We need to decide how we want to display the result to users * There may be some small changes to the frontend for showing results"
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	PROGRESS_UPDATE.md	"Overview We are creating a web app that allows users to rank their favorite movies. We will then do sentiment analysis on reviews of those movies to show the user how similar their ranking is to how the general public would rank those movies. There are 4 components to this project - Frontend - Backend REST Endpoints - Review Scraping - Sentiment Analysis Frontend The frontend is finished. It's also been integrated with a mock endpoint until we create the real endpoint. See image below for screenshot of frontend. Backend REST Endpoints The web frontend will do the sentiment analysis by sending the user data to the backend which will run our python scripts. Right now, the frontend is communicating with a backend endpoint that returns fake data. We just need to drop in the real script once it's done. Review Scraping The scraping of data was done using BeautifulSoup and Python Requests. We scrape the Metacritic website for user reviews as well as critic reviews then combine them into one single array containing all the arrays as strings. Sentiment Analysis Sentiment analysis (also known as opinion mining is a text analysis technique that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph, sentence, or clause. Understanding people's emotions is essential for businesses since customers express their thoughts and feelings more openly than ever before. In sentiment analysis, we use some packages like nltk.classify, nltk.corpus, nltk.sentiment, nltk.sentiment.util. Each document is represented by a tuple (sentence, label). The sentence is tokenized, so it is represented by a list of strings, like the following example (['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], 'subj'). We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets like the following train_subj_docs = subj_docs[:80]; test_subj_docs = subj_docs[80:100]. Then, we use simple unigram word features, handling negation: sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats) and apply features to obtain a feature-value representation of our datasets: training_set = sentim_analyzer.apply_features(training_docs); test_set = sentim_analyzer.apply_features(testing_docs). We can now train our classifier on the training set, and subsequently output the evaluation results, which is shown as the follows: Accuracy: 0.8, F-measure [obj]: 0.8, F-measure [subj]: 0.8, Precision [obj]: 0.8, Precision [subj]: 0.8, Recall [obj]: 0.8, Recall [subj]: 0.8 Challenges The biggest challenge is to familiar myself with the NLTK library during our sentiment analysis. The other challenge is due to the lack of api, I have to create a function that will translate the input movie title, into a url-appropriate title. For example: ""The Matrix"" -> ""the-matrix"" Future Steps We need to decide how we want to display the result to users There may be some small changes to the frontend for showing results"
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	PROPOSAL.md	Project Overview Technologies Backend Use Python 3.7 for all of backend (scraping data, sentiment analysis, similarity measure). Flask for the backend. Frontend NodeJS 14.15.0 with ReactJS for the frontend Yarn for package manager Scraping API data - 10 hours (Zoheb) Learn specifics about API usage Get movie ID from movie title Get reviews from movie ID Download and parse movie reviews Data from https://developers.themoviedb.org/ Sentiment Analysis - 15 hours (Leo) Given a list of reviews, perform sentiment analysis on all Aggregate results Output ranking of movies Analyzer from https://www.nltk.org/howto/sentiment.html Similarity Measure - 10 hours (Zoheb) Take a list of user rankings and list of generated rankings Output a numerical representation of similarity This requires a bit of experimenting so it will take extra time, we test multiple techniques to find a suitable solution: Ex: Try MAP, gMAP, simple difference, VSM,... Web Interface - 30 hours (Matthew) Build a web interface to interface with the user Problems to solve: How to make it intuitive for users? How to communicate between python backend and JavaScript frontend Timeline Nov 29 - Progress Report Dec 1 - Goal Finish Date (for code) Dec 13 - Project Due
https://github.com/LeoGJC/Movie-Sentiment-Analysis-Engine	README.md	CourseProject Overview of the project This repository consists of two applications -- a web frontend and a python backend. Given a list of movies from the user on the frontend, it will be sent to the backend for processing. The backend will scrape reviews of the movies, do sentiment analysis, and compose a ranking of the movies based on this analysis. Then the user can compare their ranking of movies with the sentiment analyzer's rankings. Presentation See the presentation here! If you have any issues or want a live demo, email mlw6@illinois.edu Member Contributions See the Proposal.md file in the root of the repository for a breakdown of what each member contributed. Implementation Details about how the software work and its implementation can be found in Proposal.md and PROGRESS_UPDATE.md Running the project Prereqs Must have the following software installed: * Python 3.7 * Pip 3 * NodeJS 14.15.0 * Flask (See below) * Yarn (See below) Installing Flask and Yarn From anywhere in your command line, to install flask (and other dependencies), run pip install flask pip install flask_cors pip install requests pip install bs4 pip install html5lib pip install nltk And to install yarn, run npm install -g yarn Running the Backend Navigate to the backend/ directory and run the following command: flask run This will start the backend on port 5000 Running the Frontend Navigate to the frontend/ directory and run the following two commands in order: yarn yarn start This will start the frontend on port 3000.
https://github.com/dixonliang/CS410CourseProject	ChelseaLeeds_Presentation.pptx	"Sentiment Analysis of Soccer Games Natural Language Processing of Twitter in Python using Tweepy, TextBlob, and BM25Okapi Dixon Liang University of Illinois Urbana-Champaign MCS - CS 410 General Information Example Game: Chelsea vs. Leeds United (12/5/2020), Chelsea won 3-1. Ziyech and Koch went off injured Player Ratings Consensus: Entire Chelsea team played well Several players could have been MOTM Entire Leeds team was OK Two Parts Sentiment Analysis using TextBlob Ranking using BM25Okapi Everything can be found on my GitHub (https://github.com/dixonliang) File used: ""12_5_20_ChelseaLeeds_Demo.ipynb"" Empty demo: ""Demo.ipynb"" Open to collaboration  Setting Game Parameters Sentiment Algorithm Parameters Number of tweets to be retrieved for each player (limited by API) Threshold for subjectivity Date range Sentiment Algorithm: Part 1 Sentiment Algorithm: Part 2 Sentiment Results Chelsea Example Tweets Kante (CM): [['RT @StatmanDave: N'Golo Kante vs. Leeds United [Chelsea rank]:\n\n12 ball recoveries [1st]\n3 tackles won [=1st]\n2 interceptions [=1st]\n\nDoing...', -0.8] ['RT @marshyleeds: Kante was the best player on the field and made the difference for Chelsea. Has that change of pace in midfield we sometim...', 1.0]] The negative sentiment tweet does not appear to be accurate... We can use BM25Okapi to further investigate... Mendy (GK): [@goal Our starting 11 have all scored\nRemains Mendy\nHold on a second, he is keeping goals from entering, win win for Chelsea', 0.4], ['The presence of Mendy and Thiago have given Chelsea a huge impact at defensive line https://t.co/CD93QjlYLc', 0.4]"" Did not have any negative sentiment tweets Leeds Example Tweets Philips (CM): [Kevin Phillips deserves to play for a top 5 team, the guy is so good. He literally runs this leeds team.', 0.6] Some negative tweets, but not particularly focused on him Likely best player on the field for Leeds based on this - seemed to be agreed by other match ratings Cooper (CB): [['@leeds_lord Cooper was terrible. Hideously exposed.', -1.0], ['@kennybrown1964 @maz7555 Llorente was class. It's cooper who is and always has been the weak link. Hopefully that's... https://t.co/68jwJ2rX0W', -0.375] [""@Laurencewegner I thought he was good Cooper did well, yeah but for me have to credit Chelsea's movement in the box... https://t.co/JjWhYFbfGW"", 0.7] Some very negative tweets, some positive tweets Implies wasn't great, but wasn't terrible as some suggest BM25Okapi for Context Chelsea BM25Okapi Results Kante matching most of the positive terms Giroud, Werner, Chilwell all had decent games as well Werner seems to have had a great game taking the two into consideration Ziyech injury, Chilwell foul (VAR) Leeds BM25Okapi Results For most part matches the sentiment rating Bamford was the goal scorer for Leeds Phillips confirmed best player for Leeds based on the two Koch injury Dallas matches the most ""bad terms"", was one of the lowest ranked players in sentiment Conclusion and Improvements Provides accurate idea of what happened in each game BM25Okapi is flexible, will provided needed context in the cases where sentiment analysis might not make sense Can easily be adapted to other sports Can be extended for multiple games or even an entire season Algorithm likely needs to be improved Efficiency poor and API limitations Best to be used with statistical analysis Open to work with others! "
https://github.com/dixonliang/CS410CourseProject	Progress Report.docx	11/30/20 Which tasks have been completed? Pretty much all the source code has all been finished. The remaining coding work will be if I want to make any additional improvements. I have also implemented all the code into a Jupyter Notebook which I will use for a demo. Most of the tutorial and description in that has been finished. I have broken the code down into two parts: basic sentiment analysis and implementation of BM25Okapi to give some more context of the contents of score. The first part of the code uses Tweepy to source tweets and then Textblob's Sentiment Analysis to classify. Every starting player from each team will receive an average sentiment score based on the classification of these tweets. There is also code that outputs the results visually. The second part of the code uses BM25Okapi ranking to find specific tweets that might have contributed to the sentiment analysis. These findings are also visualized. Which tasks are pending? The two main tasks that are pending are the demo run in video and documentation. If I have some more time, perhaps more can be done to improve the overall implementation / clean up of the code for efficiency in the future. I am currently working through the documentation on my GitHub page which is part of the README. The documentation will provide further detail as to what exactly each part of the code does as well as my thought process. I have also chosen to use a Jupyter Notebook to provide a step by step on how to run through my code. I have written most of the instructions and tutorial in that but will likely refine it some more. Given the limitations of the Twitter API access (more below), I will likely wait to run the demo along with the video closer to the due date. I will likely provide one notebook with the results of my run through so others can follow along with the results and then a notebook with a blank implementation for free use. Are you facing any challenges? There are a few challenges that either fall into the category of text retrieval regarding the API or further improvement of the algorithm. The Twitter API access is an issue as the free version only retrieves a limited number of tweets over only the past 7 days. This makes it a bit difficult to set a demo of the code, but I will likely just have to set a game that I want to use for the demo run in the coming week to not cut it too close to the deadline. The other challenge is that Tweepy is slow in retrieving many tweets, especially given the Twitter API limitations of how many times I can call it. For speed purposes, I will likely have to set the number of tweets per player at 100 for my demo. As for the algorithm, there are some areas that could be improved beyond the basic implementation. Teams and players can go by a handful of names so it would be useful to be able to count all occasions. However, for the most part, there is usually a more popular name that each team or player is referred to. As for most NLP, more work can be done in finding deeper meaning in words especially when the tweets are more contextually complex. If multiple players are mentioned in a tweet or previous references are made, it can be difficult to classify if a tweet is positive or negative for a particular player.
https://github.com/dixonliang/CS410CourseProject	Progress Report.pdf	11/30/20 Which tasks have been completed? Pretty much all the source code has all been finished. The remaining coding work will be if I want to make any additional improvements. I have also implemented all the code into a Jupyter Notebook which I will use for a demo. Most of the tutorial and description in that has been finished. I have broken the code down into two parts: basic sentiment analysis and implementation of BM25Okapi to give some more context of the contents of score. The first part of the code uses Tweepy to source tweets and then Textblob's Sentiment Analysis to classify. Every starting player from each team will receive an average sentiment score based on the classification of these tweets. There is also code that outputs the results visually. The second part of the code uses BM25Okapi ranking to find specific tweets that might have contributed to the sentiment analysis. These findings are also visualized. Which tasks are pending? The two main tasks that are pending are the demo run in video and documentation. If I have some more time, perhaps more can be done to improve the overall implementation / clean up of the code for efficiency in the future. I am currently working through the documentation on my GitHub page which is part of the README. The documentation will provide further detail as to what exactly each part of the code does as well as my thought process. I have also chosen to use a Jupyter Notebook to provide a step by step on how to run through my code. I have written most of the instructions and tutorial in that but will likely refine it some more. Given the limitations of the Twitter API access (more below), I will likely wait to run the demo along with the video closer to the due date. I will likely provide one notebook with the results of my run through so others can follow along with the results and then a notebook with a blank implementation for free use. Are you facing any challenges? There are a few challenges that either fall into the category of text retrieval regarding the API or further improvement of the algorithm. The Twitter API access is an issue as the free version only retrieves a limited number of tweets over only the past 7 days. This makes it a bit difficult to set a demo of the code, but I will likely just have to set a game that I want to use for the demo run in the coming week to not cut it too close to the deadline. The other challenge is that Tweepy is slow in retrieving many tweets, especially given the Twitter API limitations of how many times I can call it. For speed purposes, I will likely have to set the number of tweets per player at 100 for my demo. As for the algorithm, there are some areas that could be improved beyond the basic implementation. Teams and players can go by a handful of names so it would be useful to be able to count all occasions. However, for the most part, there is usually a more popular name that each team or player is referred to. As for most NLP, more work can be done in finding deeper meaning in words especially when the tweets are more contextually complex. If multiple players are mentioned in a tweet or previous references are made, it can be difficult to classify if a tweet is positive or negative for a particular player.
https://github.com/dixonliang/CS410CourseProject	Project Proposal.docx	"What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Dixon Liang dixonl2@illinois.edu What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free topic is to create a sentiment analysis of soccer games from the English Premier League using Twitter. Specifically, I am interested in discovering which players from either team had a good or poor game based on sentiment of tweets. This is important or interesting because based on this analysis, we can use it to come up with a detailed ""form"" analysis to see which players have been playing well over an extended period. Although specific individuals from Twitter might not be the best pundits of games, I will be using a ""wisdom of the crowd"" type approach on the quality of the data gathered. The main task I will be doing is taking tweets from a sample game in the past and categorizing the words in tweets related to certain players for a positive or negative sentiment. After this categorization, I will be able to aggregate and determine which players had a good or poor game. My planned approach is to use a particular game in the past few weeks as a demo. I would text mine all the tweets related to the game using a filter of the time period around / during the game and then those using a hashtag related to the game. I would then further analyze the tweets that have mentions of specific players and the words in context. Based on the context of the tweets, I would categorize the tweets related to players as either ""positive', ""neutral"", or ""negative"". Totaling the sentiments for each player during the game should give me a classification for each individual player determining their performance. I will likely to be able to further quantify based on some measure on how many ""positive"" or ""negative"" tweets each player has been categorized. The main tool I will be using is ""Tweepy"" which is a Python package to read tweets from the Twitter API. The main dataset will be the tweets from the time period around the games that I have chosen, and that I have categorized as relevant. If I have the time, I would also like to incorporate one of the functions from the course into my project. I will have a better idea through the planning process, but as of now, I would likely treat each tweet as a ""document"". The most likely adaptation will be to create a likelihood model using the game tweets as the primary data set. An interesting application would be to try to categorize tweets relating to players to specific parts of the game which would be the ""topics"". My expected outcome is to produce a report detailing the findings from one or several games. I should have enough data per game to show all the players' performances who were involved in the game. In this report, I will show which players were categorized as having good games or poor games based on the categorizing of tweets. In a further breakdown, by using a likelihood model, I will be also be able to show the topics where a player performed well or poor. As an example, a positive tweet might be relating to a specific player's passes during the game. I will evaluate my work based on reviewing some ""match ratings"" by pundit type publications to see if my reviews based on sentiment are in line. Although I anticipate some differences, if this works successfully, there should not be big differences in the way each player performance is viewed against the experts. I will also try to watch the games that I use to review myself if my ratings make basic sense. Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I will be working alone on this project. I anticipate the breakdown of time spent on the project as follows: Initial Research and Outline (2-4 hours) Familiarity of Tweepy and Other Tools (2-4 hours) Text Retrieval and Data Cleaning (2-4 hours) Initial Implementation of Algorithm (10-20 hours) Testing and Improvements (10-20 hours) Final Reports, Documentation, and Demo (5-10 hours)"
https://github.com/dixonliang/CS410CourseProject	Project Proposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. * Dixon Liang o dixonl2@illinois.edu 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? My free topic is to create a sentiment analysis of soccer games from the English Premier League using Twitter. Specifically, I am interested in discovering which players from either team had a good or poor game based on sentiment of tweets. This is important or interesting because based on this analysis, we can use it to come up with a detailed ""form"" analysis to see which players have been playing well over an extended period. Although specific individuals from Twitter might not be the best pundits of games, I will be using a ""wisdom of the crowd"" type approach on the quality of the data gathered. The main task I will be doing is taking tweets from a sample game in the past and categorizing the words in tweets related to certain players for a positive or negative sentiment. After this categorization, I will be able to aggregate and determine which players had a good or poor game. My planned approach is to use a particular game in the past few weeks as a demo. I would text mine all the tweets related to the game using a filter of the time period around / during the game and then those using a hashtag related to the game. I would then further analyze the tweets that have mentions of specific players and the words in context. Based on the context of the tweets, I would categorize the tweets related to players as either ""positive', ""neutral"", or ""negative"". Totaling the sentiments for each player during the game should give me a classification for each individual player determining their performance. I will likely to be able to further quantify based on some measure on how many ""positive"" or ""negative"" tweets each player has been categorized. The main tool I will be using is ""Tweepy"" which is a Python package to read tweets from the Twitter API. The main dataset will be the tweets from the time period around the games that I have chosen, and that I have categorized as relevant. If I have the time, I would also like to incorporate one of the functions from the course into my project. I will have a better idea through the planning process, but as of now, I would likely treat each tweet as a ""document"". The most likely adaptation will be to create a likelihood model using the game tweets as the primary data set. An interesting application would be to try to categorize tweets relating to players to specific parts of the game which would be the ""topics"". My expected outcome is to produce a report detailing the findings from one or several games. I should have enough data per game to show all the players' performances who were involved in the game. In this report, I will show which players were categorized as having good games or poor games based on the categorizing of tweets. In a further breakdown, by using a likelihood model, I will be also be able to show the topics where a player performed well or poor. As an example, a positive tweet might be relating to a specific player's passes during the game. I will evaluate my work based on reviewing some ""match ratings"" by pundit type publications to see if my reviews based on sentiment are in line. Although I anticipate some differences, if this works successfully, there should not be big differences in the way each player performance is viewed against the experts. I will also try to watch the games that I use to review myself if my ratings make basic sense. 3. Which programming language do you plan to use? * Python 4. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. I will be working alone on this project. I anticipate the breakdown of time spent on the project as follows: i. Initial Research and Outline (2-4 hours) ii. Familiarity of Tweepy and Other Tools (2-4 hours) iii. Text Retrieval and Data Cleaning (2-4 hours) iv. Initial Implementation of Algorithm (10-20 hours) v. Testing and Improvements (10-20 hours) vi. Final Reports, Documentation, and Demo (5-10 hours)"
https://github.com/dixonliang/CS410CourseProject	README.md	"Sentiment Analysis for Soccer Games (CS 410 Course Project) and Documentation NOTE: You will need to provide your own Twitter API keys if you want to run the blank demo file. I have provided a notebook with already run code in file ""12_5_20_ChelseaLeeds_Demo.ipynb"" which goes along with the demo materials (video and presentation) for grading purposes to avoid having to go through the process. Introduction This is the repo for my course project for CS 410 Text Information Systems for my Masters in Computer Science at University of Illinois Urbana-Champaign. The main idea of this repo is to provide the code, documentaion, and demo for a basic sentiment analysis for soccer games using Python, Tweepy, TextBlob, and BM25Okapi. The easiest way to use this code is to use the Jupyter Notebook demo in this repo. A video tutorial on Youtube is also provided. The source code is available as well. Please feel free to reach out to me if you would like to collaborate :) . Files YouTube Demo Link: https://www.youtube.com/watch?v=UuY7dO8bq0M&ab_channel=DixonLiang ChelseaLeeds_Presentation.pptx - The powerpoint presentation used in the video tutorial 12_5_20_ChelseaLeeds_Demo.ipynb - Demo file that goes along with the presentation and video link Project Proposal.pdf - The initial project proposal. Progress Report.pdf - Progress Report as of 11/30/20. maincode.py - The main source code demo.ipynb - Empty demo code in Jupyter Notebook for free use Tweepy Review.docx - Review done on the Tweepy package for the course team1_sentiment.png - Example sentiment bar chart for Team 1 (new file will be saved down if main code is run) team1_BM25positive.png - Example positive BM25 average ranking for Team 1 (new file will be saved down if main code is run) team1_BM25negative.png - Example negative BM25 average ranking for Team 1 (new file will be saved down if main code is run) team2_sentiment.png - Example sentiment bar chart for Team 2 (new file will be saved down if main code is run) team2_BM25positive.png - Example positive BM25 average ranking for Team 2 (new file will be saved down if main code is run) team2_BM25negative.png - Example negative BM25 average ranking for Team 2 (new file will be saved down if main code is run) Background We will be using Tweepy to source tweets from the Twitter API and TextBlob to provide a framework for natural language processing to provide sentiment analysis. In addition, we will use PyPi's implementation of BM25Okapi to provide context of the sentiment analysis. Ideally, the result of this code will show the relative sentiment of a player's performance during a recent game. By using wisdom of the crowds, we hope to gain an idea of how the player performed. Using BM25Okapi, we will also be able to use relevant terms to see what might have caused sentiment to go way or another (ex. player scored a goal or provided an assist, etc.) Using PyPlot, we will also be able to visualize the results. Technically, this code can be used for any soccer game, but given the popularity and language barrier, EPL games are likely to provide the most meaningful results. Adjustments could be made for La Liga or Serie A using Spanish or Italian NLP. Please feel free to reach out as I welcome any collaboration as the code can be improved and applied to different sports or different applications all together :) . A run through of the source code is provided below. Code Documentation Introduction Packages Needed: To begin, we need several packages installed and imported. These are: Tweepy, TextBlob, Numpy, Rank_BM25, and Matplotlib.pyplot. Documentation and links are found here: http://docs.tweepy.org/en/latest/api.html https://textblob.readthedocs.io/en/dev/api_reference.html https://numpy.org/doc/ https://pypi.org/project/rank-bm25/ https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.html Most importantly, we will need access to the Twitter API, which can be gained by having a Twitter profile. You will be provided four keys of strings of letters and numbers which you will need to enter in the box below: consumer key, consumer secret, access token, access token secret. These will be used in the below code area. ```shell consumer_key = """" consumer_secret = """" access_token = """" access_token_secret = """" auth = tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) api = tweepy.API(auth,wait_on_rate_limit=True) ``` Game Parameters We will need to set the parameters for the game we are interested in; this includes the two teams names and the starting 11 for each team. ```shell team1 = """" team2 = """" team1 team1_Player1 = """" team1_Player2 = """" team1_Player3 = """" team1_Player4 = """" team1_Player5 = """" team1_Player6 = """" team1_Player7 = """" team1_Player8 = """" team1_Player9 = """" team1_Player10 = """" team1_Player11 = """" team2 team2_Player1 = """" team2_Player2 = """" team2_Player3 = """" team2_Player4 = """" team2_Player5 = """" team2_Player6 = """" team2_Player7 = """" team2_Player8 = """" team2_Player9 = """" team2_Player10 = """" team2_Player11 = """" ``` After setting the game parameters, there are a few algorithm paramters we will need to set. To begin, the number of tweets that we want to retrieve is set as a parameter for the algorithm. This may also affect how quickly the algorithm runs because of limitations in the package and the free version of the Twitter API. The threshold for objectivity/subjectivity is also set. 0 is defined as purely objective and 1 is defined as subjective. Ideally for the most results, we want a low threshold, 0.10 has been suggested, but any threshold can be set. The date periods for when we want to retrieve tweets from is also se; for best results, it is suggested to only use the day of the game and the day after the game. The free version of the Twitter API limits searches to within the 7 days. Sentiment Analysis: ```shell define the number of tweets we want to sort for and subjective threshold number_of_tweets = 100 # how many tweets we want to search for threshold = 0.10 # threshold for subjectivity [0,1] setting date range, ideally run day after the game date_since = ""2020-11-21"" date_until = ""2020-11-22"" ``` For the BM25Okapi algorithm, there are just two sets of parameters we must set. The first is the set positive terms we want to use for context. Some suggestions are in the default query already. Similarily, for the second set of parameters, it is a set of negative terms. BM25Okapi: shell positive_terms = ""assist good excellent great"" # search queries, positive terms negative_terms = ""poor bad miss own awful"" # negative terms The BM25Okapi portion of the code will combine all of the tweets for every player together which will they treat each tweet as a document as part of a corpus. Then using the positive array, it will then go through each document ranking it based on how many of the positive terms each tweet matches. The higher ranked the tweet is, the more relevant it is to that query. Given we are using positive terms, the idea is that the tweet is more reflective of positive results in relation to those terms during the game for the respective player. The same will be done with the negative query. Once the rankings are done, each players average ranking for each query is provided, similarily to the sentiment array above. These two arrays will then be used for charting. BM25 incorporates search ranking concepts such as IDF (inverse document frequency), which is a filter for commonly used terms as well as TF (term frequency), which gives higher ranking for more matching of terms. A brief summary of how exactly the formula ranks can be found here: https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html Running the Code After setting the above parameters, the entire ""maincode.py"" can be run which will then output the relevant visualizations for this task. The code will retrieve the set number of tweets for each player and then use TextBlob's sentiment analysis tool to rate the sentiment of each tweet. If the tweet crosses the set threshold, the senitment for that tweet will be used for an average of all of the sentiment for that respective player. This array of sentiments of players will then be used for our graphs below. The functions that are used for the generation of these visualizations are listed below. Visual Output Functions plot_bar_team1_sentiment(): Using pyplot, this function will chart Team 1's senitment by player in the form of a horizontal bar chart. The function will take the sentiment array as mentioned above and plot the respective average for each player. If the sentiment is more towards the right, the player's sentiment for that game will be more positive. If the senitment is more towards the left, the player's sentiment for that game will be more negative. plot_bar_team2_sentiment(): Same as the above but with the players for Team 2. plot_bar_team1_BM25positive(): Using pyplot again, this function will chart Team 1's BM25Okapi rankings in the form of a horizontal bar chart. plot_bar_team2_BM25positive(): Same as above but for Team 2. plot_bar_team1_BM25negative(): Same as above but for the negative query and Team 1. plot_bar_team2_BM25negative(): Same as above but for the negative query and Team 2. Text Output Functions display_tweets(team, player_number): This function will take in two arguments, the team name and the player number (which can be referenced above on the parameters). The function will then display the ten highest and ten lowest sentiment tweets for that player. shell ['RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'RT @goal: Thiago Silva \nHavertz \nPulisic \n\nRudiger \nChilwell \nWerner \n\nChelsea reveal their team to play Newcastle \n\n#NEWCHE https:/...', '@ChelseaFC Chelsea had a clean with Rudiger and Zouma playing together.  We are winning this league', 'RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'NEWCASTLE 0-2 CHELSEA: GODFREY  ""The only player wey dun improve Chelsea na Mendy, ZOUMA AND RUDIGER STILL NO GET... https://t.co/TYZOd3mZ9X', 'RT @kingmali_: @ChelseaFC MOTM kante\nLovely clean sheet Mendy\nWell done Tammy\nRudiger is not fit to be a Chelsea player PERIOD!\nEmerson is...', 'RT @SiPhillipsSport: Chelsea keep the ball for about 5 minutes, thennnnn Rudiger.', 'RT @AbsoluteChelsea: Frank Lampard says Antonio Rudiger was brilliant on his first Premier League start of the season for #Chelsea against...', ""Are you more confident about Chelsea's defensive options and depth than at the start of the season?\n\nhttps://t.co/enuSsURsmJ""] rank_top(corpus,terms): This function is in relation to the BM25Okapi rankings. It takes in two arguments, a corpus (in this case, will be a series of tweets) and then a search query (in this case, positive or negative term array). This function will display the top ten ranked tweets in the corpus given the query. An example would be if we wanted to see the top ranked tweets for a specific player. shell ['@Chelsea_Era @EBL2017 Werner was playing bumdesliga, I don't doubt he's got a good scoring record in that league. H... https://t.co/veBdvRxnxQ', ""https://t.co/cTxtOa9fGf\nMendy &amp; Chilwell both had their 'worst' game in a Chelsea shirt today, and were still excel... https://t.co/gmjF62mTX3"", '@AlexGoldberg_ Kovacic done ok today but gives the ball away too much in dangerous areas, against a better team Che... https://t.co/d0y3hpDjgX', '@afcjxmes Kovacic was Chelsea's worst midfielder today, gave the ball away in dangerous areas too many times, Kante... https://t.co/rPLqctpkjc', ""Timo Werner is 'undroppable'.\n\nN'Golo Kante is back doing what he does best.\n\nFrank Lampard is about to settle on a... https://t.co/oXxsMrxKh7"", ""Timo Werner is 'undroppable'.\n\nN'Golo Kante is back doing what he does best.\n\nFrank Lampard is about to settle on a... https://t.co/oXxsMrxKh7"", 'RT @Football__Tweet: Edouard Mendy has kept 7 clean sheets in his first 9 Chelsea games.\n\nTalk about an upgrade on the most expensive goalk...', '@tessderry1 Ths international break suckssss...timo chilwell mount grealish theirlegs lookd tired.....\n\nNext match... https://t.co/L57jzK0DyO', 'Frank Lampard expressed his delight as Chelsea kept another clean sheet in their 0-2 win against Newcastle at St Ja... https://t.co/d3HMLpYWoX', 'Saturdays added assist:\nMount (Chelsea v Newcastle) pass leading to own goal. https://t.co/xtIdUJXHLQ'] Helper Functions sentiment_element(element): This is a simple function that will be used for Python's sort implementation for an array. In this case, we are interested in sorting be the second element (for each entry in the serntiment array is the sentiment score) which is what this function does. rank_scores(corpus,terms): This function is in relation to the BM25Okapi rankings. It takes in two arguments, a corpus (in this case, will be a series of tweets) and then a search query (in this case, positive or negative term array). It is the function that will actually use PyPi's implementation of BM25Okapi to give each tweet a rank in relation the entire corpus. Before passing into the implementation, both the corpus and term query will be tokenized."
https://github.com/dixonliang/CS410CourseProject	Tweepy Review.docx	"Intro Tweepy is a Python library to access the Twitter API. The Tweepy library provides functions to easily retrieve text from Twitter in addition to other useful methods to interact with the application. Given the extensive use of social media and social media's increasing relevance in today's world for all kinds of data, Tweepy can be used in instrumental ways for text retrieval for all kinds of analysis. Getting Started and API Authentication is done through your own Twitter profile using tokens. Authentication is easy as it uses OAuth 1a with the tokens simply given to you in your account and then can be applied directly with the following two lines of code. auth = tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token(access_token, access_token_secret) This is straight forward as a user just needs a Twitter account and ""apply"" for a developer account which just consists of some basic questions as to what you want to do with the API. Tweepy provides several interesting features including being able to interact through a user profile such as posting tweets, retweeting, interacting with other users, etc. The most basic feature seems to be interacting with your own timeline. The first example that the documentation uses is to print the first 20 tweets from your home timeline. However, given the context of this class, we are most interested in the ""search"" features. Search Features An important note to keep in mind is that the search methods of Tweepy are limited based on what type of API account you have - ranging from a free account to enterprise / premium account. For a full historical search, an enterprise / premium account is needed which allows a user to search through the entirety of the history of Twitter from 2006. Unfortunately, in a free search, not all tweets will be made available - however, for most general purposes in analyzing trends or sentiment, this should be good enough. The search function takes in a total of ten arguments which the user can declare. API.search(q[, geocode][, lang][, locale][, result_type][, count][, until][, since_id][, max_id][, include_entities]) The query can be up 500 characters maximum. For example, we can use an array of search terms if interested in several queries. The rest of the arguments act as filters. We can filter for things such as location or language or a time interval. Unless using a premium account, tweet history is limited to just the past seven days. Upon completing a search, Tweepy returns what is called a SearchResults object. The best way to interact with this object is through calling the search function in Tweepy's pagination method. Pagination Tweepy also provides an easy to iterate through different objects in Twitter using something called a Cursor object. for status in tweepy.Cursor(api.user_timeline).items(): By using a cursor object, we can treat each tweet as an ""item"" which allows us to specify exactly how many tweets we want to use for analysis per search. This makes iterating a lot easier. This feature can also be used for other features of Tweepy that requires iteration. For example if we wanted the last 100 tweets about 'cars"", we would query ""cars"" in the search method and enter ""100"" in the items() field. Additional Features Streaming may be of interest for users that want to interact or analyze in real time. Everything real time related is done through StreamListenter. Through this instance, we can then filter out exactly which tweets we might be interested; we can filter for certain users or certain keywords. myStreamListener = MyStreamListener() myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener) For those who want to interact with Twitter using ""bot"" like control, Tweepy also provides many methods that are useful. We can interact with our own profile without ever using the Twitter site from posting our own updates to adding followers. There are also methods that allow us to interact with our direct messages. Conclusion Tweepy provides a very easy to use package for those interested in mining tweets from Twitter. With regards to text retrieval, Tweepy makes it quite easy especially with the cursor method which helps with pagination. If combined with another type of text package such as TextBlob, there are a lot of possibilities in what we can do such as conducting sentiment analysis of events or subjects."
https://github.com/hpandeycodeit/CourseProject	CS410_ProjectProposal.pdf	"Project Proposal: CS 410 Topic: Sentiment Analysis Team Canyon What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - Amrutha Gujjar agujjar2@illinois.edu - Balakumar Balasubramaniam bbalasub@illinois.edu - Himanshu Pandey hpandey3@illinois.edu Himanshu Pandey is the Captain What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? Topic Name: Sentiment Analysis The task is to capture the sentiments on the data. For eg: In current Presidential Campaign, we read the news through different channels including social media. Streaming the social media data like Twitter, can give a dataset on the topic ""Presidential Campaign"" and then we will process the data to find and evaluate the sentiments of the users on both the Presidential Campaign. Dataset: We will be using the twitter data for this project. Tools: Scrapping/Streaming the twitter data can be done through R or Python in Real Time. Evaluation: Precision/Recall Measures Which programming language do you plan to use? Python Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. The task is divided into following sub tasks: - Data streaming and cleaning. We are planning on stream the data at different times from the Twitter and once we have 30-40k rows, we can start cleaning the data. 8 hours - Data cleaning and Analysis 6 - 8 hours - Writing the code to train the data 20 hours - Test and refine 8 - 10 hours - Improving the model accuracy 6 - 8 hours - Evaluation of the test results 6 - 8 hours - Generating charts 6 - 8 hours"
https://github.com/hpandeycodeit/CourseProject	data_description.md	columns = [Author, Date, Tweet] filename nrows start_time end_time isRowOrderReversed twitter.csv 17720 2020-11-04 06:02:56 2020-11-04 06:24:23 Yes twitter_2.csv 2655 2020-11-04 06:38:39 2020-11-04 06:38:40 Yes RealTime.csv 186323 2020-11-04 07:54:55 2020-11-04 08:26:11 No RealTime2.csv 658280 2020-11-04 08:32:26 2020-11-04 10:16:56 No
https://github.com/hpandeycodeit/CourseProject	ProgressReport.pdf	"Project Progress Report: CS 410 Topic: Sentiment Analysis Team Canyon Which tasks have been completed? Following Tasks have been completed: - Data Streaming/Scrapping - Data Cleaning and - Analysis Data Streaming/Scrapping: We used the twitter to stream real time data during the presidential elections 2020. The raw data contains following three columns ""Author,Date,Tweet"" tweets with hashtags ""'Elections2020', 'ElectionNight', 'Elections', 'Trump', 'Biden'"" To real time stream data from Twitter, we have created an app in Twitter. Using ""tweepy"" library in Python and using the ""consumer_key"", ""consumer_secret"", ""access_token"", and ""access_token_secret"" from the app created in Twitter, we have streamed the live data during the event to fetch around 658280 Tweets. Data Cleaning: We used the MeTA analysis toolkit to clean the raw data that was scraped from Twitter. This involved using 'stemming' to treat base words as the same, in order to reduce the amount of noise in the analysis. Analysis: Current data is analyzed to create time-series of n-gram counts charts are plotted and embedded into html files. These Plots contain the top-20 most popular n- grams. We have computed for 2,3,4 and 5-grams. As per the proposal we have created the initial Sentiment Analysis for Presidential Elections 2020. Which tasks are pending? - Evaluation of the test results using Precision/Recall Measures are pending. - Need to test the generation of charts on a different dataset to test the streamline working of the project. Are you facing/faced any challenges? There is no cap on the amount of data that needs to be collected to generate the correct sentiments. In other words, when should we stop scrapping the data from Twitter? This was one of the challenges we faced while scrapping the data from Twitter. In addition to the above, the limitations from Twitter to the number of calls that can be made to collect the data. Data cleaning is an issue as all the data from twitter is not text. It includes images/gifs/smiles and videos too. Cleaning such data was not straight forward. Still need to integrate the Evaluation of results using Precision and Recall measures. We are hoping to complete this in time."
https://github.com/hpandeycodeit/CourseProject	README.md	"About the project This project provides a general framework for capturing sentiment trends from streamed twitter data. We demonstrate our software by capturing the US 2020 election related tweets, in the morning after the election, and applying n-gram frequency trends, and PLSA. The framework is written to be such that it is easy to add new modules, and perform new analytics. Using this dataset, we show that n-gram analysis captures many of the prominent characteristics of the election -- including ""biden win"", ""claim victory"", and ""trump premature claim"". We then apply a sentiment analysis, and show that the positive sentiment towards trump decreases between 840-1000 AM, while biden's positive sentiment marginally increases. Finally, we do a PLSA analysis on the data to identify the top-10 topics that are of the greatest importance. We show that the PLSA analysis captures biden's win in georgia, importance of swing states pennsylvania, and wisconsin, and vote count stop related messages. Interestingly, the top topic turned out to be ""claims of a premature result"". This package can be used as a template for processing any other twitter data stream. DATA Collection Data was collected using the Twitter API and python code. The code ""twitter_data.py"" was run during the Presidential Election Night 2020 to collect the most relevant tweets that covered hashtags such as 'Elections2020', 'ElectionNight', 'Elections', 'Trump', 'Biden'. In all, we collected approximately 650K tweets. Data Cleaning We used metapy for initial clean up of the data. We used the MeTA analysis toolkit to clean the raw data that was scraped from Twitter. This involved using 'stemming' to treat base words as the same, in order to reduce the amount of noise in the analysis. One important aspect of this work is the necessity to remove ""emojis"". These contaminate the data, and create biases in the results. So, we removed any emojis with the text. DATA Analysis Current data is analyzed to create time-series of n-gram counts at 5 minute intervals. Clickable/zoomable charts are automatically generated, and embedded into html files. These Plots contain the top-20 most popular n-grams (for n=2,3,4,5). As per the proposal we have created the sentiment trend analysis results for the US presidential elections in 2020. Finally, PLSA analysis picks up the most salient topics. Requirements Check our requirements file for the required libraries and run pip install requirements.txt. Following are the required libraries for this project. metapy emoji pandas plotly textblob Run the project Clone the project and follow the steps below: Run python driver_twitter.py only. This python file does the following tasks: It takes the input ""RealTime2.csv"" file Cleans the Data by running ""data_cleaner.py"" After, cleaning runs ""ngram_analyzer.py"" to analyze ngram frequencies Calculates the sentiments by running ""aggregate_sentiment_analyzer.py"" Stores the charts/graphs in ""./figures/html/"" directory TOOLs/Languages Python Twitter API ## Team Amrutha\ Bala\ Himanshu ## Video Link Presentation"
https://github.com/97agupta/CourseProject	CS 410 Project Proposal.pdf	"Team 'Buddie' Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Aman Gupta, amang5 (Captain) b. Venkata Sandeep Chillara, vsc5 c. Katie Shin, ks56 2. Which paper have you chosen? Causal topic modeling Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 3. Which programming language do you plan to use? Python 4. Can you obtain the datasets used in the paper for evaluation? a. Yes there are two datasets mentioned in the paper, a New York Times article dataset and a stock time series dataset. The NYT dataset is available here: http://www.ldc.upenn.edu/Catalog/ CatalogEntry.jsp?catalogId=LDC2008T19. A stock time series dataset can be requested and pulled from here: https://finance.yahoo.com/quote/AAPL/history/ 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? N/A 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? N/A"
https://github.com/97agupta/CourseProject	Final documentation.pdf	Team Buddie Final Documentation Each team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). main.py  compiles code above to automate the process of running PLSA without priors first and then with priors until we get the convergence we want (0.95+). This is to determine the correlation between US Stock data and New York Times articles per date. If the file structures match up, we can replace either the stock data or articles with other data sources to get the correlation between those two data sets also. This project is an attempt to reproduce the following paper: https://dl-acm-org.proxy2.library.illinois.edu/doi/10.1145/2505515.2505612 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. - plsa_without_prior.py : initial run of PLSA without any priors - plsa_with_prior.py : subsequent PLSA runs with priors determined from Granger and Pearson tests - word_retriever.py : retrieves various info required such as word frequency per day - analysis.py : contains code for running the Granger and Pearson coefficient tests - analysis.ipynb : A jupyter notebook containing the same code in analysis.py allowing for more exploration and changes as needed. We utilized this notebook to explore our CSVs, view dataframes, and run our analysis. - main.py : 1. Retrieve and normalize stock data 2. Initially run PLSA without prior, and run the analysis (Granger and Pearson coefficient tests) to retrieve priors 3. Using the priors retrieved above, iterate with the PLSA with prior until the desired convergence is achieved. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. How to run the project: - Pull from github repo https://github.com/97agupta/CourseProject - Install the necessary libraries, with the most notable one being - pip install plsa - Place NYT corpus in the correct folder format - Can be retrieved from https://catalog.ldc.upenn.edu/LDC2008T19 - To run the iterative code, run 'python3 main.py' - To run sections of the iterative code, run 'python3 <filename>.py' Sidenote: - After running the granger test, the F values for different topics need to be manually inspected and relevant topics need to be parsed out. These are then used to pull top words which are used for a pearson test. - Running the PLSA as written can take multiple hours, so we have provided the end results for our 1st iteration in the /data folder for your viewing 4) Brief description of contribution of each team member in case of a multi-person team - Katie - Ran initial PLSA algorithm without any priors - With the relevant topics retrieved from Granger test, retrieved the top 20 relevant words per topic and their frequencies per date. - Aman: - Parsed Iowa Stock Exchange data - Used external time series & PLSA topics to write functions for a Granger test to find relevant topics - Used external time series & top words per topic to write functions for a Pearson test to create sub-topics from our topics with positively and negatively correlated words for use as a prior - Sandeep - The PLSA library we were using did not have a provision to take priors and use that as part of the algorithm. So extended the library to accept priors and use them as part of the M-step of PLSA using numpy einsum. - Read the priors as input from csv of the previous steps, filter only the words that exist in the particular day's corpus and feed them into the PLSA step and do multiple iterations.
https://github.com/97agupta/CourseProject	Final Presentation Slides.pdf	Team Buddie Topic: Reproducing a Paper (Casual Topic Mining) Katie Shin, Sandeep Venkata, Aman Gupta General Idea * Can we combine probabilistic topic models and causal analysis with external time series to find topics in a corpus of documents that are both coherent semantically and correlated with the time series? * Reproduced this paper by utilizing two data sets from the paper: * Corpus: NYT Articles * Time Series: Iowa Presidential Stock Markets * Looking for topics that specifically caused support for Bush or Gore to change. Based on: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management (CIKM '13). Association for Computing Machinery, New York, NY, USA, 885-890. DOI:https://doi.org/10.1145/2505515.2505612 How to Use Software - Pull from github repo https://github.com/97agupta/CourseProject - Install the necessary libraries, with the most notable one being - pip install plsa - Place NYT corpus in the correct folder format - Can be retrieved from https://catalog.ldc.upenn.edu/LDC2008T19 - Folder: CourseProject/data/<month>/<day>/<filename>.xml - To run the iterative code, run 'python3 main.py' - To run sections of the iterative code, run 'python3 <filename>.py' (Note: This takes hours to run, and therefore we have provided a sample run w/ end-results here) Implementation Part 1 Part 2 Part 3 Part 4 Part 5 Implementation Part 1 * For each group of xml files that belong to a single day (05/01/2000 - 10/31/2000), we ran and picked the best model out of 5 PLSA models. * From the top model, we extract the top 5 highest probability words for each date that will be considered the topic. * Library used: PLSA Implementation (Part 2) Part 2 * Based on topics identified during PLSA, we identify related topics by running a Granger Test between the change in probability of topics and the normalized price of one candidate. * We use a lag of 5 days & create stationary time series for both of our Granger Test Variables. * Not every topic has enough data points for testing, so we skip over topics where the Granger Test fails * Based on the highest F-value for the each Granger Test, we determine if a topic is or isn't correlated with the external time series. Implementation (Parts 3 & 4) Part 3 Part 4 * From the best PLSA model (refer to Part1), we retrieved the top 20 relevant words (i.e. highest probability) per topic that were deemed related from the Granger test. * For each word series, we run a pearson coefficient test comparing the external time series and the frequency of top words of each topic. * We then segment these words into negative and positive correlations, returning words that when combined, meet our probability threshold (0.75) * These words and then used as a Prior for the PLSA algorithm. Implementation (Part 5) Part 5 * We used the PyPI package for PLSA but it didn't offer support for including priors to guide the PLSA as per user inputs. * So we enlisted the source code and extended the PLSA algorithm to overwrite the M step. * The library essentially uses numpy 'einsum' to efficiently perform multi-dimensional matrix operations. * We modified the M-step of the algorithm to include the parameters (m) and the pseudo counts. * The key part was to look for only the words that occured in the day. Results (1st Iteration) PLSA without prior * After running our first PLSA on the corpus of NYT documents, we found 343 topics. Of these topics, only 4 topics showed causality with our external time series: bush, gore, campaign, and clinton. * For each of these topics, we extracted the top words and segmented each topic into two topics made up of positively and negatively influenced words. These new topics were used a prior for our second iteration. Here is an example of the Gore topic, being split into two: Results (2nd Iteration) * With a new iteration using PLSA with prior, we find the following relevant topics using our granger test: * Teacher, oil, drug, debate * This is a clear improvement over our original PLSA, as it delves deeper into the issues that moved the campaign likelihoods for each candidate * Bush, Gore, Campaign, Clinton -- This clearly shows that our model is finding topics that are more related to the campaign issues & with further iterations we expect this to further improve. Next Steps * We would want to run further iterations on these topics. * Additionally, instead of using the full NYT corpus we could identify paragraphs that mention Bush, Gore, or Presidential to create a smaller corpus from which to topic mine. * We would want to find a more robust method of comparing and understanding the different F-Scores from each lag of our Granger Test. Thanks! (This is Buddie)
https://github.com/97agupta/CourseProject	Progress Report.pdf	Team Buddie Progress Report Topic: Reproducing a Causal Topic Mining Paper Team Members: - Aman Gupta (@amang5) - Katie Shin (@ks56) - Venkata Sandeep Chillara (@vsc5) Please upload your progress report to the Github repo shared on CMT. The progress report should give us an idea of how you're implementing your proposal. It should answer 3 main questions: 1) Which tasks have been completed? - Implemented the initial PLSA algorithms using https://github.com/yedivanseven/PLSA. Modified the source code to include the prior. (Our initial implementation for PSLA can be found in main.py) - Ingested and parsed stock data for the 2000 presidential election using daily markets in a Jupyter Notebook. Calculated normalized price for each candidate on a daily basis and stored this data in a data frame. This can be seen in (Stock Data Parsing & Normalization.ipynb with stock data in the stock_data folder) - Explored using https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.grangercausality tests.html for running granger tests to find casual topics based on a time series 2) Which tasks are pending? - Organize the PLSA results in a Date-Topic-Probability format so that it can be used in the Granger test Run granger tests for topics using results from first task with time series PLSA output - Understand and run pearson coefficients tests to find casual words from relevant documents - Use output from pearson coefficient tests as prior for PLSA. - Add README.md with steps to organize NYT corpus data in and run the code 3) Are you facing any challenges? - PLSA process time for the NYT corpus times out when run on a local machine. One option is to look into https://colab.research.google.com/ - Since we need the Date-Topic-Probability, we don't have to run PLSA on the entire dataset - The version of PLSA with priors isn't readily available, we had to implement that on our own.
https://github.com/97agupta/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. NYT Corpus folder structure In order to run the PLSA algorithm, the NYT corpus structure has to be '../CourseProject/data//' Stock data folder structure We save the stock prices from May 2000 to Oct 2000 as htm files in the format of '_2000.htm' in a folder called 'stock_data'. The htm files can be saved from https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 Code content plsa_without_prior.py: initial run of PLSA without any priors plsa_with_prior.py: subsequent PLSA runs with priors determined from Granger and Pearson tests word_retriever.py: retrieves various info required such as word frequency per day analysis.py: contains code for running the Granger and Pearson coefficient tests main.py: Retrieve and normalize stock data Initially run PLSA without prior, and run the analysis (Granger and Pearson coefficient tests) to retrieve priors Using the priors retrieved above, iterate with the PLSA with prior until the desired convergence is achieved. How to run code Once the data files are saved in the structure defined above, you should be able to run python3 main.py which will converge after a desired convergence has been retrieved.
https://github.com/JamesFCoffey/CourseProject	Progress Report.pdf	"Progress Report CS 410 Text Information Systems - Fall 2020 Team: The Electric Moccasins James Coffey, NetID: jamesfc2 - Captain and Praveen Bhushan, NetID: bhushan6 Goal The function of the tool being implemented is to ""combine probabilistic topic modeling with time series causal analysis to uncover topics that are both coherent semantically and correlated with time series data."" (Kim et al., DOI=10.1145/2505515.2505612) This tool is being implemented in Python instead of the R implementation done in the paper to make it easier for software deployment. The techniques/algorithms being used are PLSA topic model and Granger testing. Progress made thus far * Implementation in Python Jupyter Notebook o Acquired needed datasets: New York Times Annotated Corpus (NYTAC), IEM 2000 U.S. Presidential Election ticker, and stock tickers for AAPL and AAMRQ o Wrote script for determining significant Granger causality at different lag values o Wrote function for calculating impact value using Granger causality o Wrote function for calculating topic purity o Wrote script to trim and organize NYTAC to data subset needed o Imported PLSA class from MP 3 and modified the build corpus to use NLTK and multiprocessing. o Wrote function for topic level causality. Remaining tasks * Compare results of Python implementation with reference paper. * Write command line interface for tool. * Write documentation and record tutorial presentation. Challenges/issues being faced * Even with multiprocessing, building corpus takes a while on 4 core Intel i7 CPU * EM algorithm takes a while. Could this be sped up by using CuPy instead of NumPy so that it uses the GPU? * Not sure if it is one iteration of EM for each update of the topic prior or if it is multiple EM iterations for one topic prior update. * Not sure how to incorporate the topic prior into the PLSA algorithm."
https://github.com/JamesFCoffey/CourseProject	Proposal.pdf	"Project Proposal CS 410 Text Information Systems - Fall 2020 Team: The Electric Moccasins James Coffey, NetID: jamesfc2 - Captain and Praveen Bhushan, NetID: bhushan6 We have chosen to reproduce a listed paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The function of the tool that we will implement from this paper is to ""combine probabilistic topic modeling with time series causal analysis to uncover topics that are both coherent semantically and correlated with time series data."" (Kim et al.) This tool can be used to find positive and negative correlation of topics in textual data with time series data. This could be useful for example in predicting stock prices by a real user such as an investment bank. Our tool will be different in that we plan to use Python for implementation instead of the R implementation done in the paper. This will be impactful as Python is a general-purpose programming language and would make it easier to deploy in more programs. Lastly, the techniques/algorithms we will use are those in the paper: PLSA topic model, Pearson correlation coefficients, and Granger testing. We can obtain the datasets used in the paper. We can get the New York Times Annotated Corpus from https://catalog.ldc.upenn.edu/LDC2008T19, IEM 2000 U.S. Presidential Election: Winner-Takes-All Market from https://iemweb.biz.uiowa.edu/closed/pres00_WTA.html, and historical stock prices for AAPL and AAMRQ from https://finance.yahoo.com/ and https://thestockmarketwatch.com/. Using this data, we can demonstrate the usefulness of our tool be replicating the results published in the original paper on the same datasets. Our rough timeline for the proposed project is as follows: * Nov. 1st - Implement paper in Python. * Nov. 8th - Verify performance of against paper and compare results. * Nov. 15th - Write interface for using tool on new data to generate results * Nov. 22nd - Troubleshoot and bug fix. * Nov. 29th - Submit progress report. * Dec. 6th - Finish documentation and record presentation. * Dec. 9th - Submit code with documentation and tutorial presentation."
https://github.com/JamesFCoffey/CourseProject	README.md	CS 410: Text Information Systems Course Project Documentation Tutorial presentation video Microsoft Hosting https://web.microsoftstream.com/video/625f5bff-8451-45b6-881b-207b9590da96 YouTube Hosting https://youtu.be/DQfXFzNvuAc Overview of the function of the code This code implements the following paper: Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612 The code takes times series data and a corpus of text documents both with time stamps from the same period as input and outputs a set of causal topics with the lag that correlates them to the time series data. It does this through the iterative algorithm given below: 1. Apply probabilistic latent semantic analysis (PLSA) to the document corpus to generate a preselected number (input parameter) of topics topics. 2. Use the Granger causality test correlate topics with the time series data and output correlations with a significance (one minus p-value) above a cutoff value (0.95 default) to a list of (topic, lag) tuples. 3. For each topic in the list of (topic, lag) tuples, use the Granger causality test to find causal words with above signifcance above the cutoff among top words. By default the top words are the those with the highest probability words whose summed probability does not surpass 50%. 4. Separate positive and negative impact words of each topic into individual topics and ignore the lesser topic if it is below a ratio of the major topic (by default, less than 0.1). 5. With the newly separated topics from step 4, define prior word probabilities according the significances found in step 3. 6. Apply PLSA to the document corpus using the prior word probabilities incorporated into the M-step of the EM algoritm. 7. Repeat 2-6 until a spectified number of iterations is performed. This code can be used to text mining as to find causal topics correlated with external time series data. For example, it can be used to find topics correlated with the movement of stock prices. It can also be used to find topics correlated with the outcome of an election. Implementation The code is implemented in an Python class called ITMTF in a module called causal_topic_mining.py. A portion of the code was adapted from CS 410 MP3. The class is composed of the following: Class Variables self.documents: The text document corpus. self.doc_timestamps: The timestamps of the documents in the corpus. self.vocabulary: The vocabulary. self.likelihoods: The log likelihoods. self.documents_path: The path to the text document corpus. self.term_doc_matrix: The term document matrix. self.document_topic_prob: P(z | d) self.topic_word_prob: P(w | z) self.topic_prob: P(z | d, w) self.number_of_documents: The number of documents. self.vocabulary_size: The size of the vocabulary. self.lag (default, 5): The maximum lag to which evaluate Granger causality. self.sig_cutoff (default, 0.95): The significance cutoff. self.probM (default, 0.5): The maximum sum of the probabilities of the top words for a topic. self.min_impact_ratio (default, 0.1): The ratio below which to discard minority impact words in a topic. self.topic_prior: The prior word probabilities. self.mu: The strength of the prior in each iteration. self.ct: The list of (topic, lag) tuples for causal topics discovered by the algorithm. self.average_entropy: The average entropy of the topics for each iteration of the algorithm. self.average_topic_purity: The average topic purity of the topics for each iteration of the algorithm. self.average_causality_confidence: The average causality confidence of the topics for each iteration of the algorithm. self.time_series: The time series data saved on intialization of the class. Class Functions normalize(input_matrix): Normalizes rows two-dimensional matrix to sum to one. token_pipeline(line): Tokenizes line of input text. build_corpus(): Reads documents from the corpus, tokenizes them, and stores them. It also stores the timestamps of the documents and records the number of documents. build_vocabulary(): Constructs the vocabulary and records the size of the vocabulary. build_term_doc_matrix():Constructs the term document matrix. initialize_randomly(number_of_topics): Randomly initializes the probability distributions for P(z | d) and P(w | z). It also intializes the topic prior. initialize_uniformly(number_of_topics): Uniformly initializes the probability distributions for P(z | d) and P(w | z). It also intializes the topic prior. initialize(number_of_topics, random=False): Calls an intialization function. expectation_step(): The E-step of the EM algorithm for PLSA where it updates P(z | w, d). maximization_step(number_of_topics): The of the EM algorithm for PLSA where it M-step updates P(w | z) and P(z | d). process(number_of_topics, max_plsa_iter, epsilon, mu, itmtf_iter): The master control loop for the iterative topic modeling with time series feedback algorithm. impact_value(data, lag): Calculates the impact value. build_TS(): Builds a topic stream. topic_level_causality(): Finds the causal topics from the topic streams and time series data. top_words(topic): Finds the top words in a topic. build_WS(tw): Builds a word stream. word_level_causality(): Finds the significant words and their impact value for each topic using the word streams and time series data. generate_topic_prior(wc): Generates the topic prior. create_eval_df(): Creates a dataframe from which to evaluate metrics. metrics(): Evaluates the average entropy, average topic purity, and average causality confidence of the topics. Usage of the software Installation Download the module causal_topic_mining.py to your project folder. The module uses Python 3.7 and requires the following libraries: - numpy - pandas - statsmodels - nltk - normalise - tqdm demonstration_notebook.ipynb requires the additional library: - matplotlib Demonstration A demonstration of the use of the class ITMTF is given in the Jupyter notebook demonstration_notebook.ipynb. Import the module. from causal_topic_mining import ITMTF Create an object from ITMTF class itmtf = ITMTF(documents_path, time_series) The path is to the text corpus of text files named YYYY-MM-DD.txt (YYYY = 4 digit year, MM = 2 digit month, DD = 2 digit day) and each line in the text file is a separate document. The time series is a Pandas series where the indicies are the dates in pd.dateime format and the time series is a stationary series. Tokenize the text corpus and record document timestamps itmtf.build_corpus() Build vocabulary itmtf.build_vocabulary() Run ITMTF algorithm itmtf.process(number_of_topics = 30, max_plsa_iter = 1, epsilon = 0.001, mu = 1000, itmtf_iter = 5) Access the results by calling the class object parameters desired itmtf.average_topic_purity for example. Team Contributions James Coffey (Captain) Wrote Proposal.pdf Wrote Progress Report.pdf Wrote causal_topic_mining.py Wrote demonstration_notebook.ipynb Wrote README.md documentation Gave tutorial presentation Praveen Bhushan Gave tutorial presentation
https://github.com/Parkkeo1/CourseProject	CS 410 Project Progress Report - Keon Park.pdf	CS 410 FA20 - Project Progress Report 2.2 ExpertSearch System - Extracting relevant information from faculty bios Keon Park, keonp2@illinois.edu Background / Overview My project is a system based on ExpertSearch  that, when finished, will automatically scrape and parse UIUC faculty pages from user-provided URLs to extract key information about faculty members. My original proposal is available here: https://github.com/Parkkeo1/CourseProject. Progress So Far My work on the project so far has focused on text data retrieval, processing, and some keyword extraction and topic mining. My code is still in a WIP Jupyter Notebook and has not yet been integrated into a full application. To get started, I am currently using my dataset of UIUC CHBE faculty text data from my MP2.1. Using this dataset, I have built a web scraper that scrapes text data from HTML tags whose CSS classes match common faculty profile sections (i.e. scrape only relevant content from the page). Because faculty pages for a given department have the same or very similar format, I have implemented the ability to customize the web scraper's list of key CSS classes to match for each department at UIUC (currently only have one, for CHBE). Using the basic CHBE faculty data, I have also developed working implementations of the following three key components in my application: 1. Tokenizer, using spaCy, with stop words + punctuation filtering and lemmatization. 2. TF-IDF weighting, using scikit-learn; the trained vocabulary/index is able to be saved and loaded for use on new faculty data. 3. LDA, using scikit-learn, the topic distributions are calculated using the training data, and the topic coverage for a new doc can be calculated too. With these parts working, I am able to scrape relevant text data from a faculty member page URL not in the training data, tokenize/clean it, and add it to the existing TF-IDF index/vocabulary to calculate its top weighted keywords and its topic coverage using LDA. Remaining Tasks 1. Refactor the existing components: scraper, tokenizer, TF-IDF, and LDA to work in a smooth pipeline for both training data and new user-input data such that: a. Refine the scraper's ability to match relevant HTML tags for more different formats of faculty pages than just CHBE's. b. For new faculty's text data, use spaCy to find named entities and search for email and phone number tokens during the tokenization step, 2. Test out the above functionality with a much bigger dataset (UIUC faculty bios from MP2.3) and refine/fix things as needed (the final application's TF-IDF weights should be based on this as it should be more comprehensive over more different departments. 3. Build out final backend and frontend to package work into a full application. Challenges The main challenge I foresee in finishing this project is the frontend/UI and how I should approach designing and implementing the data visualizations for the faculty text mining results. Given that I have already put in a lot of focus and effort on the text retrieval and mining components of the project, I may have settle for more basic data visualizations in the frontend than I originally planned, in order to keep my total project development time reasonable.
https://github.com/Parkkeo1/CourseProject	CS410 Project Proposal - Keon Park.pdf	"CS 410 FA20 - Project Proposal 2.2 ExpertSearch System - Extracting relevant information from faculty bios Keon Park, keonp2@illinois.edu (Individual; Captain) Overview For my project, I plan to build upon ExpertSearch's extraction features and develop a new system that, given a faculty webpage's URL, extracts not only names and emails but also other relevant information such as research and instruction areas by implementing keyword extraction on the text data. These extracted keywords will then function as ""tags"" that indicate faculty members' top topics/research areas. Once finished, these features could be integrated into ExpertSearch to enhance its existing extraction utility. However, for now, my project will be a standalone system that will serve as a prototype for the new keyword extraction functionality. Impact My system will tackle the problem of converting unstructured text into structured data by automatically and quickly summarizing faculty text data into topics/keywords for users to more easily understand. This will be better than what ExpertSearch currently does; apart from the faculty name/email extraction, it has users manually visit and read the faculty webpages if they desire to know more information, which may be burdensome as faculty webpages are often varied in format and content. Thus, my system will allow users to more easily learn about faculty members with less manual effort. To demonstrate my system's usefulness, I will compare its results (containing structured information and topics) with ExpertSearch's results (basic word matches) for a given set of UIUC faculty. Architecture My new system will be implemented as a web application, with a React.js frontend and Python-Flask backend. Through the frontend, the user will be able to enter a URL of a faculty webpage, which the backend will then scrape and extract information from. Although my system will be separate from the existing ExpertSearch system, I plan to use its faculty name/email extraction code as a starting point/reference for my own implementation. Data / Techniques I plan to mainly use UIUC faculty and their webpages during development to test my system. For scraping text from faculty webpages, I will use my work from MP2.1 as a reference. To help implement my extraction techniques, I plan to use existing libraries such as NLTK and TextBlob. Key Tasks + Timeline 1. Build web scraper with ability to detect and extract basic information from faculty profile sections (name, title, research, bio, contact, etc), using NER and HTML tags - 5 hours 2. Implement keyword extraction to tag faculty to relevant topics (research areas, areas of expertise, etc) - 4 hours 3. Test and refine web scraper and extraction functionality - 2 hours 4. Incorporate code from steps 1-3 into Flask backend - 2 hours 5. Implement frontend for faculty webpage URL queries and displaying results - 6 hours 6. Integrate backend with frontend and finalize system - 3 hours"
https://github.com/Parkkeo1/CourseProject	README.md	"Keon Park - CS 410 CourseProject Extracting relevant information from faculty bios (2.2) This repository contains the source code, data, and documentation for Keon (Isaac) Park's final project for CS 410 Fall 2020 at UIUC. This project is an extension/spin-off of the ExpertSearch system that seeks to build upon ExpertSearch's NLP features by extracting not only names and emails from faculty pages but also keywords, named entities, and topics in order to provide users with a more comprehensive overview without having to manually visit the page. This project was an individual effort by Keon (Isaac) Park. Software Usage Tutorial Video The link to the tutorial video for this project is here. The video is a brief explanation of how to locally install/run and use the project code, including a example use case. The rest of this README document also provides details regarding this software's functionality and implementation. Overview As previously mentioned, this project is a standalone extension of the ExpertSearch system that provides improved NLP features to better analyze text in faculty web pages. In its current implementation, ExpertSearch only extracts the name and email of the faculty member from the page, limiting its use as a tool for users seeking more in-depth overviews of faculty members and their biographies. As a result, this project was developed with more advanced text retrieval and mining features to automatically provide users with a useful ""snapshot"" of faculty page content. It accomplishes this by scraping, processing, and analyzing text data from faculty pages via URLs entered by the user using BeautifulSoup, spaCy, scikit-learn, and pre-trained TF-IDF and LDA models (available as .pkl files in the server/data directory) to automatically extract/calculate relevant keywords, named entities, and topics. For example, here is a screenshot of the system's results for UIUC Professor Tarek Abdelzaher: As shown above, this software, like ExpertSearch includes the likely name and emails of the faculty member. It also provides detailed overviews of the system's calculated keywords, named entities, and topics with specific numbers to provide users with an effective ""snapshot"" of the page without visting it manually. For example, the user can deduce that Professor Abdelzaher is likely to be an engineering (likely CS or CompE, due to keywords like system, transactions, and data) research professor involved/related with the Institute of Electrical and Electronics Engineers (IEEE). Pipeline The current pipeline of this sytem for retrieving, processing, and analyzing text data via user-provided URLs is as follows: User enters a URL of a faculty web page Web-scrape the URL and retrieve HTML source of the page Retrieve text from ""relevant"" HTML tags containing useful information (match CSS classes of HTML tags; nlp/html_parser.py) Perform NER to find names/organizations and find emails/links from text data using spaCy Tokenize text data into unigrams using spaCy, keeping only alphabetic, non-stopword, noun/verb/adj tokens Use tokenized documents for pre-trained scikit-learn TF-IDF vectorizer (trained on ~600 UIUC faculty bios) to find keywords Use TF-IDF weights of document to calculate its topic coverage using pre-trained scikit-learn LDA model and topic distributions Compile results into dataframes, which are then converted into html and rendered in the results page. The code for this pipeline can be roughly traced in the query() route handler function in app.py. Implementation This project/system is implemented as a Flask web application with a Python backend and HTML/CSS frontend. The code is organized like a standard Flask application, in the following directory structure (not all files shown): CourseProject/ | README.md | .gitignore | ... +---server/ | app.py | train.py | config.py | requirements.txt +---data/ | tfidf.pkl | uiuc_bios.txt | ... +---nlp/ | scraper.py | html_parser.py | tfidf_lda.py | ... +---static/ | main.css +---templates/ base.html ... app.py houses the Python code directly responsible for running the Flask web application, including route handlers for the home and results pages. When the user enters and submits a URL through the form in the home page, the Flask app redirects the user to the /query endpoint with the provided URL to invoke the corresponding route handler query() that is responsible for running the text retrieval and mining pipeline of the application. Thus, most future changes/tweaks to this system's NLP pipeline will originate in this function (query()). train.py contains the Python code necessary to train and save TF-IDF and LDA models using the UIUC faculty bios text data in the data directory; the model files generated by train.py are used by the Flask application. config.py contains all of the constants used by both the Flask application and train.py as configuration for various parameters during text retrieval and mining. For example, this file can be customized to tweak max_df and min_df used by the TF-IDF vectorizer. The nlp directory contains all of the NLP-related code of the system, divided into individual modules/files that is used to perform scraping, tokenization, NER, keyword extraction, and topic mining. The functions in this package implement and use various NLP and data libraries, such as: BeautifulSoup (for web-scraping), spaCy (for tokenization and NER), scikit-learn (for TF-IDF and LDA), and pandas (for compiling results into dataframes). Thus, most future changes to the specific NLP techniques used by this project will originate in this package and its specific modules. The data directory contains the trained TF-IDF vectorizer and LDA models as pickle files that are loaded in by the Flask application when calculating TF-IDF weights of keywords and topic coverages for the faculty page provided by the user. This directory also contains text files of UIUC faculty bios and urls compiled from the MP2.3 dataset; these data files are used by train.py to generate the .pkl files for the trained models. The static and template directories contain HTML/CSS code for the Flask frontend. Flask uses Jinja2 templates. Local Setup These instructions assume the user is already knowledgeable of git and Python environments and has Python 3 (note: 3.7 was used during development) with pip installed. Clone this repository bash git clone https://github.com/Parkkeo1/CourseProject.git cd CourseProject Create a new Python venv and install dependencies bash python -m venv venv source venv/Scripts/activate pip install -r server/requirements.txt python -m spacy download en_core_web_lg Launch the software. bash cd server python train.py // if you want to to newly train and save a TF-IDF and LDA model based on data/uiuc_bios.txt to be later used by the Flask app. python app.py // if you want to launch the project's main application, the Flask app. If you launched the Flask app, navigate to localhost:5000 in your web browser to view and use the Flask app."
https://github.com/sajidws/CourseProject	CS410-Project-Documentation-Data-Miners.pdf	"1. Executive summary Wasique Ahmad and Sajid Shaikh formed the Data Miners team. We chose the project to enhance the Educational Web system as we saw a lot of potential in it for UIUC students. We observed the opportunity to make several changes to make the application more useful, along with adding material from more courses thus expanding the scope and reach of the application. 2. Functionality completed The Data Miners team managed to complete the following enhancements to the Educational Web system. The source code is located at: https://github.com/sajidws/CourseProject (branch: master) a) Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly o Each entry used to start with something like '02 Week 1 02 Week 1 Lesson 01 Lesson 1...'. Removed the redundant parts and made it more intuitive b) Make the slide material from another course (CS 425: Distributed Systems) available to the users so that users can expand their learning to other courses o As part of this story, added 'CS 425' in the 'Courses' dropdown list c) Improve presentation of information the following screens to make the application more user- friendly: o Add week number, lecture number and topic to the current slide so that the user knows which week and lecture covers the current slide. o Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed. o Add miscellaneous UX improvements such as capitalize 'CS' (for Computer Science) wherever it appeared. d) Add a home / landing page to the application that lists the courses available to that users as soon as they 'arrive' in the application. e) Allow users to go back to the home / landing page from any pages so that users can navigate easily between different parts of the application. Sajid Shaikh completed the user stories a, b and c. Wasique Ahmad completed the user stories d and e. Detailed instructions on how to deploy and run the application have been updated in the project 'README.md', including updates for Windows 10 platform. 3. Breakdown of the tasks Here's a breakdown of the amount of effort spent by the team in various activities of the project, including comparison with the original estimates. # Task Original estimate (Hrs) Actual effort (Hrs) Status 1 Writing proposal, user stories 2 2 Done 2 Set up and build the code 6 20 Done 3 Understanding the existing system and design for the scope of our project 6 14 Done Design of enhancements 8 8 Done Development, code reviews and unit-testing 20 30 Done System / QA verification 6 5 Done Documentation 8 10 Done Demonstration preparation 4 5 Done Communication 5 Done Total 60 99 4. Notes and Experiences As can be seen from above, we spent far more time than we had estimated. One of the primary reasons for this turned out that the project had previously been tested only on Linux and MacOS, but not on Windows and the Data Miners team was more familiar on Windows platform. We decided to take up the challenge anyway to set up and deliver the project on Windows 10. Expectedly, we faced many hurdles as we were in uncharted territory, but with help from each other, course staff and a bit of luck, managed to complete the deployment of the project on Windows 10. We have updated the project README with detailed instructions (including various package versions, paths, etc.) so that future students have an easier time. Even after deploying though, some of the functionality such as Explanations and Search didn't work as expected. We started to fix those, ran into more issues (e.g. after fixing the URL for explanations, the current ranking function hung). Rather than spend more time in what could have been a long and unknown effort, we decided to focus on implementing the functionality we had planned. There were some other missteps along the way (e.g. tried to change the directory names to improve the lecture name entries in the dropdown, but that broke other functionality), but each time we learned something new and continued forward. It's not unusual in any software project to have new discoveries during execution and our experience was no different. The above experience resulted in spending much more time than we had planned but it turned out to be a learning experience and we're glad if our efforts could simplify the life of future students. 5. User story details In this section, we will dive into the details of each user story implemented, including the technical portions as well as UI screen shots of the application. A majority of the changes made to the code were in the HTML templates (*.html) and Python files, especially model.py and app.py. Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly Initially, we tried to simplify the directory names as that allowed us more control over the names that could be presented to the user. We then realized there are many other dependencies on the names of the directories (e.g. Related Slides weren't appearing). Ultimately, we changed the manner in which 'lec_names' are displayed in the UI by changing 'slide.html'. The highlight of the change was: Line 7: <li><a href=""{{base_url}}/slide/{{course_name}}/{{i}}"">{{' '.join(lec_names[i].split('_')[3].split('- ')).title()}}</a></li> Trimmed entries, easy to read and see fully! Make the slide material from another course (CS 425: Distributed Systems) available to the users We initially thought just adding a folder such as 'cs-425' in 'static/slides' directory will do the job. However, that didn't work and we had to make changes in 'model.py' to remove hard-coded checks of 'cs-410'. Ultimately, this turned out to be a time-consuming activity. The PDF files from the CS 425 Distributed Systems lectures had to be split into individual pages, each named in a certain format. We have packaged a new archive 'cs-425.zip' and included its location and instructions to deploy in the README file. cs425.zip is available here: https://drive.google.com/file/d/1IWxuYF1fGHlU1VZn5xfXfyCJxUV- sIRI/view?usp=sharing CS 425 (Distributed Systems) is now available via Educational Web! The entire CS 425 course material! Add week number, lecture number and title to the current slide so that the user knows which week, lecture and topic covers the current slide While this change was relatively straightforward as we had to extract the week and lesson number by making changes in 'slide.html', it adds quite some value to the user to see which topic the current slide is covered under and to quickly determine the week and lecture number. Week, Lecture # and Topic makes it easy to see where you are! Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed The changes made here were similar by extracting the week and lecture number in 'slide.html' Add miscellaneous UX improvements such as capitalized 'CS' (Computer Science) A relatively easy change by changing 'title()' to 'upper()' in multiple places where 'CS' is going to be displayed. Easy to see week and lecture number in 'Related slides' 'CS' appears better than 'Cs' everywhere Adding landing page This is will be the landing page. List of the courses offered by EducationalWeb application will be shown here. The file 'LandingPage.html' was added in the 'templates' folder to implement the above functionality. This page extends 'base.html'. In order to load this page when the user visits the base URL ('/'), 'app.py' was modified to render 'LandingPage.html' instead of 'base.html'. This will be shown as a 1st slide after selecting course 'CS 410' from the above page. Clicking on 'Educational Web' in the top bar from any page will bring users back to the landing page. In order to implement the above change, 'base.html' was modified to change the link for 'Educational Web' to '/' instead of '#'."
https://github.com/sajidws/CourseProject	Data-Miners-Educational-Web-Enhancements.pdf	Team name: Data Miners Team Composition What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Wasique Ahmad: wasique2 Sajid Shaikh: sajidas2 (captain) System and Topics What system have you chosen? Which subtopic(s) under the system? Educational Web System: improving the usability and reach of the existing system. - Trim the entries in the 'Lectures' dropdown. Each line starts with something like '02 Week 1 02 Week 1 Lesson 01 Lesson 1...'. Remove the redundant parts - Add material from a couple of other courses such as CS 425: Distributed Systems, CS 445 Computational Photography, etc. with all current functionality applicable (Related slides, Download, Explanation, ...). Adding crawling or adding more courses can be considered after improving the performance of the system. - Add a home / landing page that lists the courses included in this system. Add a link to the home page so users can easily navigate back to the home page. - Improve presentation on a few screens - Add week / lecture number to 'Related slides' - Add week / lecture number to 'Search results' - Add week / lecture number to current slide shown - Bulk download - Download entire lecture slide and video - Download entire week slides and video - Download entire course slides and video (multiple files separated by each week) - Explanations: add more context and improve the presentation Datasets and Algorithms Briefly describe the datasets, algorithms or techniques you plan to use Datasets: training slides for courses such as Distributed Systems, Computational Photography, ... Algorithms and Techniques: we will explore, understand and build on the existing algorithms and techniques that are currently employed in the system. What is the function of the tool The Educational Web system is a tool to help students learn from course slides. It has two main functionalities currently: 1) Retrieve and 2) recommend relevant slides for each slide. Who will benefit from such improvement Students, Educators, Researchers etc. @ University of Illinois Demonstration If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? Demonstrate live using the http://timan102.cs.illinois.edu/ system Code interaction with system How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly We will add code and data sets to the existing system to enhance the functionality and usability described above. Programming language Which programming language do you plan to use? Python Work breakdown structure Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. # Hours Task 1 2 Proposal, Writing user stories 2 6 Set up and build the code 3 6 Understanding the existing system and design 4 8 Design of enhancements 5 20 Development, code reviews and unit-testing 6 6 Testing: Verification, Load testing 7 8 Documentation 8 4 Demonstration preparations 9 4 Presentation Tota l 64
https://github.com/sajidws/CourseProject	Progress-Report-Data-Miners-Educational-Web-20201129.pdf	Interim Progress Report as of 11/29/2020 The Data Miners team has completed writing the user stories and deployed the code in their respective local systems. Based on the current understanding, design of the enhancements has started, along with some code changes and unit-testing. Because the team is using a Windows 10 environment (and the current application has been tested on Linux and MacOS so far), we encountered unforeseen issues and the amount of time spent is expected to be more than estimated for most tasks. The table below illustrates all the planned tasks, the progress made thus far and the estimated remaining work (NOTE: WIP stands for Work In Progress). All the user stories are listed below the table on the following pages. # Task Original Estimate (Hrs) Time so far (Hrs) Remaining Estimate (Hrs) Progress Status 1 Proposal, Writing user stories 2 2 0 Done 2 Set up and build the code 6 15 0 Done 3 Understanding the existing system and design 6 5 15 WIP 4 Design of enhancements 8 1 10 WIP 5 Development, code reviews and unit-testing 20 1 30 WIP 6 Testing: Verification, Load testing 6 10 Not started 7 Documentation 8 1 10 WIP 8 Demonstration preparations 4 5 Not started 9 Presentation 4 5 Not started Total 64 25 85 The audience of the following user stories is MCS students at UIUC. User stories - Trim the entries in the 'Lectures' dropdown so that the list is more usable and easier to scan quickly. - Add week number and lecture number to the current slide so that the user knows which week and lecture covers the current slide. - Add week number and lecture number to the list of slides shown in the 'Related slides' section so that the user knows which week and lecture covers each slide listed. - Add week number and lecture number to the list of slides shown in the 'Search Results' section so that the user knows which week and lecture covers each slide listed. - Allow users to download all the slides of the current lecture (instead of only the current slide), so that they do not have to download each slide individually for the lecture. Build this incrementally as follows: - Download each page of the lecture as separate PDF files. - Allow users to specify a different lecture number (defaulting to the current lecture) so that they have more flexibility of which lecture's materials to download. - Allow users to specify whether all the pages of the lecture should be downloaded as a single PDF file. - Allow users to download all the slides of the current week (instead of only the current slide), so that they do not have to download each slide individually for the week. Build this incrementally as follows: - Download each page of the week as separate PDF files. - Allow users to specify a different week number (defaulting to the current week) so that they have more flexibility of which week's materials to download. - Allow users to specify whether all the pages of the week should be downloaded as a single PDF file. - Allow users to download the video of the current lecture - Allow users to specify a different lecture for downloading a video, so that they have more flexibility of which lecture's video file to download. - Allow users to download the video of the current week as separate video files. - Allow users to specify a different week for downloading a video, so that they have more flexibility of which week's video files to download. - Do not allow users to download the video files of the entire course as it may hog the bandwidth unnecessarily and this functionality can be achieved by downloading each week's video files separately if really required. - Improve the Explanations by adding more context (such as week and lecture numbers) and improve the presentation (such as paragraphs) to so that the content is more usable and easier to understand for the audience. - Make the slides from another course (CS 425: Distributed Systems) available to the users (functionally at par with CS 410) so that users can expand their learning to other courses - As part of this story, add 'CS 425' in the 'Courses' dropdown list - Add a home / landing page to the application that lists the courses available so that users are aware of which courses are made available with this functionality. - Add a link back to the home / landing page from all pages so that users can navigate easily between different parts of the application. Challenges during setup I encountered a few challenges during setup. - In case of Gulp, I had to keep Path in a certain order. C:\Users\<NAME>\AppData\Roaming\npm\node_modules\ C:\Users\<NAME>\AppData\Roaming\npm - In case of Numpy, Numpy 1.19.4 was not compatible . So I uninstalled 1.19.4 and installed Numpy 1.19.3.
https://github.com/sajidws/CourseProject	README.md	CourseProject The project documentation is in the file 'CS410-Project-Documentation-Data-Miners.pdf'. The demo recording links for media space is in the text file 'Demo-Recording-Link-DataMiners-Educational-Web-Enhancements.txt' The other two documents are from previous milestones: project proposal and progress report The source code for this project is in the 'master' branch.
https://github.com/Diegoma89/CS410_CourseProject_DM	1014052.1014150.pdf	"A Cross-Collection Mixture Model for Comparative Text Mining ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Atulya Velivelli Department of Electrical and Computer Engineering University of Illinois at Urbana Champaign Bei Yu Graduate School of Library and Information Science University of Illinois at Urbana Champaign ABSTRACT In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent com- mon themes across all collections as well as summarize the similarity and differences of these collections along each com- mon theme. This general problem subsumes many interest- ing applications, including business intelligence and opinion summarization. We propose a generative probabilistic mix- ture model for comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algo- rithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. Categories and Subject Descriptors: H.3.3 [Informa- tion Search and Retrieval]: Text Mining General Terms: Algorithms Keywords: Comparative text mining, mixture models, clus- tering 1. INTRODUCTION Text mining is concerned with extracting knowledge and patterns from text [5, 6]. While there has been much re- search in text mining, most existing research is focused on one single collection of text. The goals are often to extract basic semantic units such as named entities, to extract rela- tions between information units, or to extract topic themes. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD'04, August 22-25, 2004, Seattle, Washington, USA. Copyright 2004 ACM 1-58113-888-1/04/0008 ...$5.00. In this paper, we study a novel problem of text mining re- ferred to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differ- ences of these collections along each common theme. Specif- ically, the task involves: (1) discovering the different com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. The need for comparative text mining exists in many different applica- tions, including business intelligence, summarizing reviews of similar products, and comparing different opinions about a common topic in general. In this paper, we study the CTM problem and propose a generative probabilistic mixture model for CTM. The model simultaneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. The mixture model is based on component multinomial distribution models, each characterizing a different theme. The common themes and collection-specific themes are explicitly modeled. The pro- posed model can be estimated efficiently using the Expectation- Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model. The rest of the paper is organized as follows. In Section 2, we briefly introduce the problem of CTM. We then present a baseline simple mixture model and a new cross-collection mixture model in Section 3 and Section 4. We discuss the experiment results in Section 5. 2. COMPARATIVE TEXT MINING 2.1 A motivating example With the popularity of e-commerce, online customer eval- uations are becoming widely provided by online stores and third-party websites. Pioneers like amazon.com and epin- ions.com have accumulated large amounts of customer input including reviews, comments, recommendations and advice, etc. For example, the number of reviews in epinions.com 743 Research Track Poster is more than one million[4]. Given a product, there could be up to hundreds of reviews, which is impossible for the readers to go through. It is thus desirable to summarize a collection of reviews for a certain type of products in order to provide the readers the most salient feedbacks from the peers. For review summarization, the most important task is to identify different semantic aspects of a product that the reviewers mentioned and to group the opinions accord- ing to these aspects to show similarities and differences in the opinions. For example, suppose we have reviews of three different brands of laptops (Dell, IBM, and Apple), and we want to summarize the reviews. A useful summary would be a tab- ular representation of the opinions as shown in Table 1, in which each row represents one aspect (subtopic) and differ- ent columns correspond to different opinions. Table 1: A tabular summary Subtopics Dell IBM Apple Battery life long enough short short Memory good bad good Speed slow fast fast It is, of course, very difficult, if not impossible to pro- duce such a table completely automatically. However, we can achieve a less ambitious goal - identifying the semantic aspects and identifying the common and specific character- istics of each product in an unsupervised way. This is a concrete example of comparative text mining. 2.2 The general problem The example above is only one of the many possible appli- cations of comparative text mining. In general, the task of comparative text mining involves: (1) discovering the com- mon themes across all the collections; (2) for each discovered theme, characterize what is in common among all the col- lections and what is unique to each collection. It is very hard to precisely define what a theme is, but it corresponds roughly to a topic or subtopic. The granularity of themes is application-specific. CTM is a fundamental task in ex- ploratory text analysis. In addition to opinion comparison and summarization, it has many other applications, such as business intelligence (comparing different companies), cus- tomer relationship management (comparing different groups of customers), and semantic integration of text (comparing component text collections). CTM is challenging in several ways: (1) It is a completely unsupervised learning task; no training data is available. (It is for the same reason that CTM can be very useful for many different purposes - it makes minimum assumptions about the collections and in principle we can compare any arbitrary partition of text.) (2) We need to identify themes across different collections, which is more challenging than identifying topic themes in one single collection. (3) The task involves a discrimination component - for each discov- ered theme, we also want to identify the unique information specific to each collection. Such a discrimination task is dif- ficult given that we do not have training data. In a way, CTM goes beyond the regular one-collection text mining by requiring an ""alignment"" of multiple collections based on common themes. Since no training data is available, in general, we must rely on unsupervised learning methods, such as clustering, to perform CTM. In this paper, we study how to use prob- abilistic mixture models to perform CTM. Below we first describe a simple mixture model for clustering, which repre- sents a straightforward application of an existing text min- ing method, and then present a more sophisticated mixture model specifically designed for CTM. 3. CLUSTERING WITH A SIMPLE MIXTURE MODEL   th  th  th "" $ % th ' th Figure 1: The Simple Mixture Model A naive solution to CTM is to treat the multiple collec- tions as one single collection and perform clustering. Our hope is that some clusters would represent the common themes across the collections, while some others would rep- resent themes specific to one collection (see Figure 1). We now present a simple multinomial mixture model for clus- tering an arbitrary collection of documents, in which we assume there are k latent common themes in all collections, and each is characterized by a multinomial word distribu- tion (also called a unigram language model). A document is regarded as a sample of a mixture model with these theme models as components. We fit such a mixture model to the union of all the text collections we have, and the obtained component multinomial models can be used to analyze the common themes and differences among the collections. Formally, let C = {C1, C2, ..., Cm} be m comparable col- lections of documents. Let th1, ..., thk be k theme unigram language models and thB be the background model for all the collections. A document d is regarded as a sample of the following mixture model (based on word generation). pd(w) = lBp(w|thB) + (1 - lB) k j=1 [pd,jp(w|thj)] where w is a word, pd,j is a document-specific mixing weight for the j-th aspect theme, and k j=1 pd,j = 1. lB is the mix- ing weight of the background model thB. The log-likelihood of all the collections C is log p(C|L) = m i=1 dCi wV [c(w, d) x log(lBp(w|thB) + (1 - lB) k j=1 (pd,jp(w|thj)))] where V is the set of all the words (i.e., vocabulary), c(w, d) is the count of word w in document d, and L = ({thj, pd,j}k j=1 744 Research Track Poster is the set of all the theme model parameters. The purpose of using a background model is to ""force"" clustering to be done based on more discriminative words, leading to more informative and more discriminative component models. We control this effect through thB. The model can be estimated using any estimator. For example, the Expectation-Maximization (EM) algorithm [3] can be used to compute a maximum likelihood estimate with the following updating formulas: p(zd,w = j) = p(n) d,j p(n)(w|thj) k j'=1 p(n) d,j'p(n)(w|thj') p(zd,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j p(n)(w|thj) p(n+1) d,j = wV c(w, d)p(zd,w = j) j' wV c(w, d)p(zd,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,w = B))p(zd,w = j) w'V m i=1 dCi c(w', d)(1 - p(zd,w' = B))p(zd,w' = j) This mixture model is closely related to the probabilis- tic latent semantic indexing model (PLSI) proposed in [7] and treats CTM as a single-collection text mining problem. However, such a simple model is inadequate for CTM for two reasons: (1) We have completely ignored the structure of collections. As a result, we may have clusters that repre- sent only some, not all of the collections. (2) There is no easy way to identify which theme cluster represents the common information across collections and which represents specific information to a particular collection. Below we present a more sophisticated coordinated mixture model, which is specifically designed for CTM and addresses these two defi- ciencies. 4. CLUSTERING WITH A CROSS- COLLECTION MIXTURE MODEL  th th    th     th    th    th    th    th    th Figure 2: The Cross-Collection Mixture Model 4.1 The model Our main idea for improving the simple mixture model for comparative text mining is to explicitly distinguish com- mon theme clusters that characterize common information across all collections from special theme clusters that char- acterize collection-specific information. Thus we now con- sider k latent common themes as well as a potentially dif- ferent set of k collection-specific themes for each collection (illustrated in Figure 2). These component models directly correspond to all the information we are interested in discov- ering. The sampling distribution of a word in document d (from collection Ci) is now collection-specific. Specifically, it involves the background model (thB), k common theme models (th1, ..., thk), and k collection-specific theme models (th1,i, ..., thk,i), which are to capture the unique information about the k themes in collection Ci. That is, pd(w|Ci) = (1 - lB) k j=1 [pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))] +lBp(w|thB) where lB is the weight on the background model thB and lC is the weight on the common theme model thj (as opposed to the collection-specific theme model thj,i). Intuitively, when we ""generate"" a word, we first decide whether to use the background model thB according to lB; the larger lB is, the more likely we will use thB. If we decide not to use thB, then we need to decide which theme to use; this is controlled by pd,j, the probability of using theme j when generating words in d. Finally, once we decide which theme to use, we still need to decide whether we should use the common theme model or the collection-specific theme model, and this is con- trolled by lC, the probability of using the common model. The weighting parameters lB and lC are intentionally to be set by the user, and their interpretation is as follows. lB reflects our knowledge about how noisy the collections are. If we believe the text is verbose, then lB should be set to a larger value. In our experiments, a value of 0.9 - 0.95 often works well. lC indicates our emphasis on the commonality, as opposed to the speciality in comparative text mining. A larger lC would allow us to learn a richer common theme model, whereas a smaller one would learn a weaker com- mon theme model, but stronger special models. The optimal value depends on the specific applications. According to this generative model, the log-likelihood of the whole set of collections is log p(C) = m i=1 dCi wV [c(w, d) log[lBp(w|thB) +(1 - lB) k j=1 pd,j(lCp(w|thj) + (1 - lC)p(w|thj,i))]] 4.2 Parameter estimation We estimate the background model thB using all the avail- able text in the m text collections. That is, ^p(w|thB) = m i=1 dCi c(w, d) m i=1 dCi w'V c(w', d) Since lB and lC are set manually, this leaves us with the following parameters to estimate: (1) the common theme models, th = {th1, ..., thk}; (2) the special theme models for each collection Ci, thCi = {th1,i, ..., thk,i}; and (3) the theme mixing weights for each document d: pd = {pd,1, ..., pd,k}. 745 Research Track Poster p(zd,Ci,w = j) = p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) k j'=1 p(n) d,j'(lCp(n)(w|thj') + (1 - lC)p(n)(w|thj',i)) p(zd,Ci,w = B) = lBp(w|thB) lBp(w|thB) + (1 - lB) k j=1 p(n) d,j (lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i)) p(zd,Ci,j,w = C) = lCp(n)(w|thj) lCp(n)(w|thj) + (1 - lC)p(n)(w|thj,i) p(n+1) d,j = wV c(w, d)p(zd,Ci,w = j) j' wV c(w, d)p(zd,Ci,w = j') p(n+1)(w|thj) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)p(zd,Ci,j,w = C) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)p(zd,Ci,j,w' = C) p(n+1)(w|thj,i) = m i=1 dCi c(w, d)(1 - p(zd,Ci,w = B))p(zd,Ci,w = j)(1 - p(zd,Ci,j,w = C)) w'V m i=1 dCi c(w', d)(1 - p(zd,Ci,w' = B))p(zd,Ci,w' = j)(1 - p(zd,Ci,j,w' = C)) Figure 3: EM updating formulas for the cross-collection mixture model As in the simple mixture model, we can also use the EM algorithm to compute a maximum likelihood estimate. The updating formulas are shown in Figure 3. Each EM iteration involves scanning all the text once, so the algorithm is quite scalable. 4.3 Using the model Once the model is estimated, we will have k collection- specific models for each of the m collections and k common theme models across all collections. Each of these mod- els is a word distribution or unigram language model. The high probability words can characterize the theme/cluster extracted. Such words can often be used directly as a sum- mary or indirectly (e.g., through a hidden Markov model) to extract relevant sentences to form a summary of the cor- responding theme. The extracted word distributions can also be used in many other ways, e.g., to classify other text documents or to link the related passages in the text collec- tions so that a user can navigate the information space for comparative analysis. We can input our bias for CTM through setting lB and lC manually. Specifically, lB allows us to input our knowledge about the noise (stop words) in the data - if we know the text data is verbose, then we should set lB to a high value, whereas if the data is concise and mostly content-bearing keywords, then we need to set lB to a smaller value. Sim- ilarly, lC allows us to input a trade-off between extracting common theme models (setting lC to a higher value) vs. ex- tracting collection-specific models (setting lC to a smaller value). Such biases cannot be learned by the maximum like- lihood estimator. Indeed, maximizing the data likelihood is only a means to achieve our ultimate goal, which is why we want to regularize our model in a meaningful way so that we can impose certain preferences while maximizing the data likelihood. The flexibility and control provided by lB and lC make it possible for a user to control the focus of the results of comparative text mining. 5. EXPERIMENTS AND RESULT ANALYSIS We evaluated the Simple Mixture model (SimpMix) and the Cross-Collection Mixture model (CCMix) on two do- mains - war news and laptop reviews. 5.1 War news The War news data consists of news excerpts on two com- parable events: (1) Iraq war and (2) Afghanistan war, both of which occurred in the last two years. The Iraq war news excerpts were a combination of 30 articles from the CNN and BBC web sites over the last one year span. The Afghanistan war data consists of 26 news articles downloaded from the CNN and BBC web sites for one year starting from Nov. 2001. Our goal is to compare these two wars and find out their common and specific characteristics. The results of using either the simple mixture model or the cross-collection mixture model are shown in Table 2, where the top words of each theme model are listed along with their probabilities. We set lB = 0.95 for SimpMix and set lb = 0.9, lC = 0.25 for CCMix; in both cases, the number of clusters is fixed to 5. Variations of these parameters are discussed later. We see that although there are some interesting themes in the results of SimpMix (e.g., cluster3 and cluster4 appear to be about American and British inquiry into the pres- ence of weapons in Iraq, respectively, while cluster2 suggests the presence of British soldier in Basra, a town in southern Iraq), they are all about Iraq war. We do not see any obvi- ous theme common to both Iraq war and Afghanistan war. This is expected given that SimpMix pools all documents together without exploiting the collection structure. In contrast, the results of CCMix explicitly suggest the common themes and the corresponding collection-specific themes. For example, cluster3 clearly suggests that in both wars, there has been loss of lives. Furthermore, the top words in the corresponding Iraq theme include names of some key defense people that are involved in the Iraq war (e.g., ""Hoon"" is the last name of the british defense secre- tary and ""Sanchez"" is the last name of the U.S General in Iraq). In comparison, the top words in the corresponding Afghanistan theme includes the name of the U.S Defense secretary who had an important role in the Afghan war. Cluster4 and cluster5 are also meaningful themes. The common theme captured in Cluster4 is the Monday briefings by an official spokesman of a political administration during both wars; the corresponding special themes indicate the dif- ference in the topics discussed in the briefings (e.g., weapon inquiry for Iraq war and Bin Laden for Afghanistan war). The common theme of Cluster5 is about the diplomatic role 746 Research Track Poster Table 2: War news results using SimpMix model (top) vs. CCMix model (bottom) Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common will 0.019 british 0.017 weapons 0.022 inquiry 0.052 countries 0.026 theme let 0.012 soldiers 0.015 kay 0.021 intelligence 0.036 contracts 0.023 words united 0.012 baghdad 0.015 rumsfeld 0.017 dossier 0.024 allawi 0.012 god 0.011 air 0.011 commission 0.014 hutton 0.021 hoon 0.012 inspectors 0.011 basra 0.011 group 0.014 claim 0.019 russian 0.010 your 0.010 mosque 0.010 senate 0.011 wmd 0.019 international 0.010 nation 0.010 southern 0.01 survey 0.010 mps 0.018 russia 0.009 n 0.010 fired 0.010 paper 0.010 committee 0.017 reconstruction 0.009 Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Common us 0.042 mr 0.029 killed 0.036 monday 0.036 united 0.042 theme nation 0.030 marines 0.025 month 0.032 official 0.032 nations 0.04 words will 0.024 dead 0.023 deaths 0.023 i 0.029 with 0.03 action 0.022 general 0.022 one 0.023 would 0.028 is 0.025 re 0.022 defense 0.019 died 0.022 where 0.025 it 0.024 border 0.019 key 0.018 been 0.022 do 0.025 they 0.023 its 0.017 since 0.018 drive 0.018 spokesman 0.022 diplomatic 0.023 ve 0.016 first 0.016 according 0.015 political 0.021 blair 0.022 Iraq god 0.022 iraq 0.022 troops 0.016 intelligence 0.049 n 0.03 theme saddam 0.016 us 0.021 hoon 0.015 weapons 0.034 weapons 0.024 words baghdad 0.013 baghdad 0.017 sanchez 0.012 inquiry 0.028 inspectors 0.023 your 0.012 nato 0.015 billion 0.01 commission 0.017 council 0.016 live 0.01 iraqi 0.013 spokeswoman 0.008 independent 0.016 declaration 0.015 Afghan paper 0.021 story 0.028 taleban 0.026 bin 0.031 northern 0.040 theme afghan 0.019 full 0.026 rumsfeld 0.020 laden 0.031 alliance 0.040 words meeting 0.014 saturday 0.016 hotel 0.012 steinberg 0.027 kabul 0.030 euro 0.012 e 0.015 front 0.011 taliban 0.023 taleban 0.025 highway 0.012 rabbani 0.012 dropped 0.010 chat 0.019 aid 0.020 played by the United Nations (UN). The corresponding spe- cial themes again suggest the difference between the two wars. The Iraq theme indicates the role of UN in sending weapon inspectors to Iraq; the Afghanistan theme refers to Northern Alliance that received aid from the UN and came to power in Afghanistan after the defeat of Taliban. 5.2 Laptop customer reviews This data set was constructed to test our models for com- paring opinions of customers on different laptops. We man- ually downloaded the following 3 review sets from epin- ions.com [4], filtering out the misplaced ones: Apple iBook (M8598LL/A) Mac Notebook (34 reviews), Dell Inspiron 8200 (8TWORH) PC Notebook (22 reviews), IBM ThinkPad T20 2647 (264744U) PC Notebook (42 reviews). The results on this data set are generally similar to those on war news. Due to the limit of space, we only show the CCMix results in Table 3, which are obtained by setting lC=.7 and lB=.96 and fixing the number of clusters to 8. Here we again see many very interesting common themes; in- deed, the top two words in the common themes can provide a very good summary of the themes (e.g., ""sound and speak- ers"" for cluster1, ""battery hours"" for cluster5, and ""Mi- crosoft Office"" for cluster8). However, the special themes, although suggesting some differences among the three lap- tops, are much harder to interpret. This may be because there is a great deal of variation in product-specific opin- ions in the data, which makes the data extremely sparse for learning a coherent collection-specific theme for each of the eight themes. 5.3 Parameter tuning When we vary lB and lC in CCMix, the results are gen- erally different. Specifically, when lB is set to a small value, non-informative stop words tend to show up in common themes. A reasonable value for lB is generally higher than 0.9 - in that case, the model automatically eliminates the non-informative words from the theme clusters, allowing for more discriminative clustering. Indeed, in all our experi- ments, we have intentionally retained all the stop words, and the model is clearly able to filter out non-informative words, though in some cases, they still show up as top words in the common themes of the news data. They can be ""eliminated"" by using an even higher lB, but then we may end up having insufficient information to learn a common theme reliably. lC affects the vocabulary allocation between the common and collection-specific themes. In the news data experiments, when we change lC to a value above 0.4, the collection-specific terms would dominate the common theme models. In the laptop data experiments, when lC is less than 0.7, we lose many content keywords of the com- mon themes to the corresponding collection-specific themes. Both lB and lC are intentionally left for a user to tune so that we can incorporate application-specific bias into the model. 6. RELATED WORK The most related work to our work is the coupled clus- tering method presented in [8], which appears to be one of the very few studies considering the clustering problem in multiple collections. They extend the information bottle- neck approach to discover common clusters across different collections. Comparative text mining goes beyond this by analyzing both the similarities and collection-specific differ- ences. We also use a completely different approach based on probabilistic mixture models. Another related work is [10], where cross-training is used for learning classifiers from mul- tiple document sets. Our work differs from it in that we per- form unsupervised learning. The aspect models studied in [7, 2] are also related to our work but they are closer to our baseline model and are not designed for comparing multiple collections. There are many studies in document clustering [1]. Again, the difference lies in that they consider only one collection and thus are similar to the baseline model. Our work is also related to document summarization, es- pecially multiple document summarization (e.g.,[9, 12]). In- deed, we can the results of CTM as a special form of sum- mary of multiple text collections. However, an important difference is that while a summary intends to retain the ex- plicit information in text (to maintain fidelity), CTM aims at extracting non-obvious implicit patterns. 7. CONCLUSIONS AND FUTURE WORK In this paper, we define and study a novel text mining problem referred to as comparative text mining. It is con- 747 Research Track Poster Table 3: Laptop review results using CCMix model Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Cluster6 Cluster7 Cluster8 C sound 0.035 port 0.023 ram 0.105 m 0.027 battery 0.129 t 0.039 cd 0.095 office 0.037 O speakers 0.035 jack 0.021 mb 0.037 trackpad 0.018 hours 0.080 modem 0.017 drive 0.076 microsoft 0.021 M playback 0.034 ports 0.018 memory 0.034 chip 0.013 life 0.060 internet 0.017 rw 0.055 little 0.018 M feel 0.019 will 0.018 256mb 0.027 improved 0.012 5 0.038 later 0.014 dvd 0.049 basic 0.015 O pros 0.017 your 0.017 128mb 0.021 volume 0.012 end 0.016 configuration 0.014 combo 0.025 6 0.014 N cons 0.017 warm 0.013 tech 0.020 did 0.011 3 0.016 free 0.013 drives 0.023 under 0.013 market 0.017 keep 0.012 128 0.020 latch 0.011 high 0.015 vga 0.012 rom 0.020 mhz 0.012 size 0.014 down 0.012 support 0.018 make 0.010 processor 0.014 were 0.012 floppy 0.017 word 0.011 D rests 0.026 banias 0.019 options 0.039 inspiron 0.061 dells 0.032 fans 0.019 apoint 0.017 0 0.046 E palm 0.022 svga 0.014 sodimm 0.025 pentium 0.052 ran 0.017 shipping 0.017 blah 0.015 angle 0.018 L 9000 0.020 record 0.014 eraser 0.021 8200 0.03 prong 0.015 2nd 0.016 hook 0.011 portion 0.0154 L smart 0.018 supposedly 0.013 crucial 0.018 toshiba 0.027 requiring 0.014 tracking 0.015 tug 0.011 usb 0.0153 reader 0.018 rebate 0.013 sdram 0.018 440 0.026 second 0.011 spoke 0.015 2499 0.011 specials 0.014 A magazine 0.011 osx 0.040 macos 0.019 macos0.016 g4 0.016 iphoto 0.031 airport 0.075 appleworks 0.060 P ipod 0.010 quartz 0.015 personal 0.018 netscape 0.013 interlaced 0.016 itunes 0.027 burn 0.035 word 0.021 P strong 0.01 instance 0.014 shield 0.016 apache 0.009 mac 0.016 import 0.021 4x 0.018 result 0.016 L icon 0.009 underneath 0.012 airport 0.016 ie5 0.008 imac 0.014 book 0.018 reads 0.014 spreadsheet 0.013 E choppy 0.008 cooling 0.012 installation 0.015 ll 0.008 powermac 0.012 quicktime 0.016 schools 0.013 excel 0.012 I technology 0.023 rj 0.033 exchange 0.023 company 0.021 thinkpad 0.077 thinkpads 0.020 t20 0.04 list 0.015 B outdated 0.020 chik 0.018 hassle 0.016 570 0.017 ibm 0.047 connector 0.018 ultrabay 0.030 factor 0.013 M surprisingly 0.018 dsl 0.017 disc 0.015 turn 0.017 covers 0.029 connectors 0.018 tells 0.021 months 0.013 trackpoint 0.014 45 0.015 t23 0.012 buttons 0.015 lightest 0.028 bluetoot 0.018 device 0.021 cap 0.013 recommend 0.013 pacbell 0.012 cdrw 0.015 numlock 0.012 3000 0.027 sturdy 0.011 number 0.020 helpdesk 0.0128 cerned with discovering any latent common themes across a set of comparable collections of text as well as summariz- ing the similarities and differences of these collections along each theme. We propose a generative cross-collection mixture model for performing comparative text mining. The model simul- taneously performs cross-collection clustering and within- collection clustering, and can be applied to an arbitrary set of comparable text collections. We define the model and present the EM algorithm that can estimate the model ef- ficiently. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method based on a simple mixture model. Experiment results show that the cross-collection mixture model is quite effective in dis- covering the latent common themes across collections and performs significantly better than the baseline simple mix- ture model. The proposed model has many obvious applica- tions in opinion summarization and business intelligence. It also has many other less obvious applications in the general area of text mining and semantic integration of text. For example, our model can be used to compare the course web pages from the major computer science department web sites to discover core computer science topics. It can also be used to compare literature collections in different communities to support concept switching [11]. The work reported in this paper is just an initial step toward a promising new direction. There are many interest- ing future research directions. First, it may be interesting to explore how we can further improve the CCMix model and its estimation. One interesting direction is to explore the Maximum A Posterior (MAP) estimator, which would allow us to incorporate more prior knowledge in a princi- pled way. For example, a user may already have certain thematic aspects in mind. With MAP estimation, we can easily add that bias to the component models. Second, we can generalize our model to model semi-structured data to perform more general comparative data mining. One way to achieve this goal is to introduce additional random variables in each component model so that we can model any struc- tured data. Finally, it would be very interesting to explore how we could exploit the learned theme models to provide additional help to a user who wants to perform comparative analysis. For example, the learned common theme models can be used to construct a hidden Markov model (HMM) to identify the parts in the text collections about the common themes, and to connect them through automatically gener- ated hyperlinks. This would allow a user to easily navigate through the common themes. 8. REFERENCES [1] D. Baker and A. McCallum. Distributional clustering of words for text classification. In Proceedings of ACM SIGIR 1998, 1998. [2] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, 2003. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statist. Soc. B, 39:1-38, 1977. [4] epinions.com, 2003. http://www.epinions.com/. [5] R. Feldman and I. Dagan. Knowledge discovery in textual databases. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, 1995. [6] M. A. Hearst. Untangling text data mining. In Proceedings of ACL'99, 1999. [7] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of ACM SIGIR'99, pages 50-57, 1999. [8] Z. Marx, I. Dagan, J. Buhmann, and E. Shamir. Coupled clustering: a method for detecting structural correspondence. Journal of Machine Learning Research, 3:747-780, 2002. [9] K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. E. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of AAAI-99. [10] S. Sarawagi, S. Chakrabarti, and S. Godbole. Cross-training: Learning probabilistic mappings between topics. In Proceedings of ACM SIGKDD 2003. [11] B. R. Schatz. The interspace: Concept navigation across distributed communities. Computer, 35(1):54-62, 2002. [12] H. Zha. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of ACM SIGIR 2002. 748 Research Track Poster"
https://github.com/Diegoma89/CS410_CourseProject_DM	Project Documentation.pdf	"Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Documentation Reproducing a Paper: A Cross-Collection Mixture Model for Comparative Text Mining Introduction My project selection is the reproduction of the paper titled ""A Cross-Collection Mixture Model for Comparative Text Mining"" cited at the end of this document. The idea is to propose an improvement over simple mixture model for comparative text mining by explicitly distinguish the collections being compared. This allows to generate topic language models that are specific to each collection along with a common topic language model that describes all collections for each cluster (See Figure 2). The model In a simple mixture model, the probability of a word is given by the sum of the portion of probability of being generated by a background model plus the probability of being generated by a topic model. Notice lB below is acting as a parameter that tunes the weight being applied to the background model. The parameter p is present on both models and simply describes the mix of topics of each document. In the proposed cross-collection mixture model, the number of topic language models grows from k in the simple mixture mode to k + mk, being k the number of topics, V the vocabulary size and m the number of collections. We also add another tunable parameter lC that acts as the weight we give to the common topic models. So, in simple term, the probability of a word in a given collection is the Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC sum of it being generated by the background model plus the probability of being generated by either the common topic model or the collection specific topic model. Implementation For the implementation I used as base the PLSA programming assignment. In the assignment we did not have a background model so a function ""build_background_model"" was implemented using the whole corpus as follows: The functions ""build_corpus"", ""build_vocabulary"", ""build_term_doc_matrix"" and ""initialize_randomly"" were mostly kept the same as they are also necessary for this model. They were only modified to explicitly separate the different m collections. For example, instead of relying in the ""self.documents"" matrix that contained all the documents, ""self.documents_collections"" was created which is a list of arrays that contain the documents of length m. A similar approach was used on the other mentioned functions. The EM algorithm was heavily modified as the number of parameters grew considerably compared to the PLSA assignment. The following formulas were used to update the E and M step: Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC However, I identified a mistake in the last formula that suggests a sum across collections to calculate the specific topic model probabilities which would end up removing the distinctions between collections and also removing one dimension of parameters. This was confirmed by the TAs as a mistake. Finally, to monitor the performance of the algorithm, a log-likelihood function essentially the same as the one on PLSA assignment but with the additional parameters was implemented following the given formula on the paper: I also made a simple mixture model implementation to compare the results, the EM updating formulas and log-likelihood estimation con be found on the referenced paper. Experiments The model was tested with 2 data sets, one with a collection of 59 news articles of the Iraq and Afghanistan war (26 from Afghanistan and 33 from Iraq war) from 2 different sources (BBC and CNN). The second data set are laptop reviews of 3 different models (a MacBook, a Dell XPS and a Lenovo Yoga) with a total of 250 reviews obtained from Amazon.com. lC and lB have to be tuned by the user. We can think of lB as how verbose the text is, the more it is, the more non-significant words we will find which in turn means we want to select a higher value. Generally, something higher than 0.9 is recommended. For lC, as we go higher, we enrich the common topic models but hurt the specific, and vice versa, so it should be set very carefully based on the data and where we want the most emphasis. The data sets used in the paper are not explicitly referenced so it was impossible to obtain the same exact data, which means I'm unable to reproduce the exact same response as in the paper and also have to tune my own l parameters. Here are my results: Table 1. Simple mixture model war news Word kit wolfowitz clothing nbc desert buy equipmentlogistics Prob 0.03732396 0.03374832 0.02093988 0.01928082 0.01928082 0.01456099 0.0134511 0.0128544 Word ghraib abu symbol prison guards photo demolishedusa Prob 0.06427516 0.04786124 0.02022767 0.01926176 0.01506055 0.01155932 0.0115593 0.0092431 Word sarin shell nerve neill bowden filled agent o Prob 0.0378052 0.03047289 0.02514473 0.02077332 0.01689814 0.0160316 0.014525 0.013795 Word taliban al laden bin alliance northern he hussein Prob 0.01549612 0.00955844 0.00785728 0.00769362 0.00731475 0.00660803 0.0056029 0.0055208 Word eta propaganda black trains legality undermine planting options Prob 0.03331262 0.02464671 0.01716214 0.0166588 0.01129754 0.01129754 0.0112975 0.0112975 Theme 1 Theme 2 Theme 3 Theme 4 Theme 5 Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC While clustered words talk about certain topics common between both wars, it is not possible to make a distinction of which words belong to which collection. Table 2. Cross-collection mixture model war news In the cross-collection model we can for example see cluster 2, where the common theme talks about prisoners, prison, detainees, detention, guards, while in the corresponding collection specific cluster we can see the location were these prisoners were being held. The Guantanamo bay detention camp is referenced in the Afghanistan cluster, which is relevant, while the Iraq war references Abu Ghraib, which is also a prison relevant to Iraq war. The output of the model is a text file named either ""SimpMix_output.txt"" for the simple model and ""CCMix_output.txt"" for the cross-collection model. Also, due the nature of randomly initializing parameters, runs can vary so I included a few other runs in the ""test"" folder. In the same folder, output for the laptop reviews can be found that were omitted in this documentation . Challenges and opportunities for improvement During the implementation I encountered 2 issues that persisted and was not able to figure out. The first one is related to my log-likelihood estimation which as stated in the lectures, the EM algorithm monotonically increases the likelihood. I ran into the problem that at some point relatively early in the Word bin laden saudi jihad video arabia plot family father authorities Prob 0.18939329 0.0981552 0.03564389 0.03239276 0.03145353 0.02699685 0.0189802 0.0143503 0.01357955 0.01339957 Word laden saudi recorded sudan yemen Prob 0.1028003 0.02914018 0.02332654 0.02035473 0.02033913 Word data ritter bowden mi dubious Prob 0.03806887 0.03382954 0.03000792 0.02535748 0.02112141 Word prisoners prison detainees detention ghraib guards executed rights amnesty geneva Prob 0.12874343 0.08034044 0.03878431 0.02395434 0.02115945 0.02067663 0.01651596 0.01642986 0.01595376 0.01563221 Word treated guantanamo cuba base bay Prob 0.03936906 0.03658937 0.03625978 0.02769193 0.02742886 Word ghraib abu symbol usa photo Prob 0.07908194 0.07583343 0.02150438 0.01337695 0.01223926 Word blasts spanish aznar seven eta king policies condolences madrid bombs Prob 0.03022417 0.02932321 0.02662327 0.02131074 0.01481973 0.01430479 0.01400754 0.0121993 0.01211067 0.01150711 Word cache bank demonstrators imf southeastern Prob 0.03169261 0.02730445 0.02130979 0.01598093 0.01584349 Word eta trains spain express karbala Prob 0.03305906 0.0198423 0.01936481 0.01058035 0.01055226 Word pentagon cheney wolfowitz libya europe abroad japan controversial vice rumsfeld Prob 0.04913761 0.03856055 0.02993884 0.02717369 0.02278614 0.02217882 0.01865351 0.01719155 0.01598577 0.01430421 Word french karachi propaganda black bus Prob 0.06793751 0.05502965 0.03675501 0.03230034 0.02751049 Word wolfowitz libya japanese gadhafi dispatch Prob 0.06058456 0.05405023 0.02317438 0.02227723 0.01039494 Word the of to that and in is was a we Prob 0.07975547 0.03963266 0.03214286 0.02182307 0.0187433 0.01801543 0.01676557 0.01557104 0.01537607 0.01396474 Word taliban afghanistan alliance northern kandahar Prob 0.08405706 0.03487229 0.03465308 0.03116097 0.02699336 Word iraq saddam iraqi hussein i Prob 0.04380953 0.0299254 0.02065532 0.01254649 0.00693652 Cluster 5 - Collection Afghanistan war Cluster 1 - Collection Iraq war Cluster 2 - Collection Iraq war Cluster 3 - Collection Iraq war Cluster 4 - Collection Iraq war Cluster 5 - Collection Iraq war Common theme cluster 1 Common theme cluster 2 Common theme cluster 3 Common theme cluster 4 Common theme cluster 5 Cluster 1 - Collection Afghanistan war Cluster 2 - Collection Afghanistan war Cluster 3 - Collection Afghanistan war Cluster 4 - Collection Afghanistan war Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC iterations, the likelihood decreases for a few iterations and then goes back to improve without interruption until convergence. I was unable to find any issues with my EM algorithm or the calculation of log-likelihood. I'm inclined to believe this could be an underflow issue during the E-step. I tried implementing the normalization to avoid underflow technique in lecture 10.4 but it did not make a difference, so the cause remains unknown. The second problem was undefined divisions (division by 0) which caused the algorithm to crash. I was able to patch this issue by implementing pseudo counts in the M-step by adding a uniformly distributed prior with a parameter u = 1 as seen in lecture 9.9. This essentially guarantees each word ""appears"" at least once and avoids divisions by 0. I don't believe this diverges too much from reality so I thought it was a reasonable fix. Would be interested if the authors ran into a similar issue or if this is an implementation problem on my side. Conclusion The cross-collection mixture model can be a powerful tool when tuned optimally to create more meaningfully comparisons than the simple mixture model. In the experiments, for example the laptop reviews output, can be clearly used by manufactures to identify the weaknesses and strengths of their product while also learning valuable information of their competitors as analyzing reviews individually or pooled can easily misdirect us from what is really important. But clearly we can extract insights from any comparable collections, maybe m different medical treatments response based on testimony from patients, political ideologies on social media and where they intersect or differ, etc. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Reference ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150"
https://github.com/Diegoma89/CS410_CourseProject_DM	Project Progress.pdf	"Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Progress Report Reproducing a Paper: A Cross-Collection Mixture Model for Comparative Text Mining 1) Which tasks have been completed? * Similar test data has been compiled for both experiments described in the paper. I'm using about ~30 news articles per event (Afghanistan and Iraq war) from BBC and CNN news and a combined total of 250 reviews of 3 recent model of laptops (Lenovo, Dell and Apple) from Amazon.com. * Data has been roughly curated. Major spelling errors were removed. * I'm using the MP3 skeleton code as base since it is very similar to the paper structure-wise and can help me and reviewers follow along the code. * The following functions have been added or implemented: o Init variables - Added needed variables o Build corpus - Builds whole corpus and individual collections. Cleans data of punctuation and digits o Build vocabulary - Same as MP3 o Build term matrix - Added term matrix per individual collection o Build background model - This is a new function o Random initialization of parameters with normalization o Expectation step - First implementation o Maximization step - First implementation * Currently writing the clustered words to 2 different text files, one for the top ten words per topic on ""common.txt"" and top 10 words per collection per topic on ""specific.txt"" 2) Which tasks are pending? * Implement log likelihood function * Check for errors in algorithm (see question 3, challenges faced) * Interpret and report results in a friendly manner * Tune Lambda B and C parameters (background and Collections ""weights"") * Clean code 3) Are you facing any challenges? In the EM updating formulas presented on the paper, I have not figured out one operation circled in the image below. The formula states to sum the terms across all d's in Ci across all Cm (collections). If my understanding is correct, this is wrong as that would pool all the documents in the corpus together and lose the focus to a specific collection ending up with only ""k"" themes. My current implementation only sums across all d's in Ci and normalizes based on this sum across words. This results in ""k"" times ""number of collections"" specific theme models where all probabilities sum to 1 within each of them. Will reach out to TA if I get stuck debugging. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC"
https://github.com/Diegoma89/CS410_CourseProject_DM	Project Proposal.pdf	"Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC Project Proposal Reproducing a Paper: Contextual text mining 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Will work individually on the project. Captain: Diego Millan NetID: diegom3 2. Which paper have you chosen? The paper selected is one of the contextual text mining subtopic options, specifically the following: ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150. 3. Which programming language do you plan to use? I plan on using Python. 4. Can you obtain the datasets used in the paper for evaluation? The paper references 2 datasets, the first one is about war news that compares the Iraq and Afghanistan war. The second dataset compares 3 laptop model reviews. I am not able to obtain the exact same datasets used on the paper. Diego Millan - NetID: diegom3 CS410: Text Information Systems - UIUC 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? The paper references the BBC and CNN websites as the source of the news articles, also mentioning how many articles from each were selected and the time span (30 articles starting 1 year before the paper publication for the Iraq war and 26 articles on a 1 year span starting November 2001 for the Afghanistan war). Unfortunately, there is no way to know the exact articles used but a similar sample can be obtained from the same sources (will be using Google news to obtain random articles on the specified time span). Regarding the laptop dataset, the review source website (epinions.com) no longer exists which makes it impossible to get the same dataset. Since laptop reviews are readily available in numerous other sites (amazon.com could be a good replacement), my plan is to use more recent data to replace this dataset. I will be comparing a newer model of each of the 3 laptop brands referenced on the paper (Apple, Dell, and IBM). 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? Even though the exact same datasets cannot be obtained, the alternatives should be close enough to get comparable results to the ones concluded on the paper."
https://github.com/Diegoma89/CS410_CourseProject_DM	README.md	CourseProject Link to Demo Video https://youtu.be/O_R6IzCWMGM This is the course project for CS410 Fall 2020 at UIUC. The project intent is to reproduce a paper on contextual text mining, specifically the refence below. ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. DOI=10.1145/1014052.1014150 Please see Project Documentation.pdf for full explanation
https://github.com/cds95/CourseProject	progress-report.pdf	CS 410 Project Progress Report Christopher Dimitri Sastropranoto Project description Create a google chrome extension that can extract the text from a CNN news article and give a text summary about it. Progress My main goal for the early part of the project was to gain some experience using python's NLTK library to process and summarize a given piece of text. I've spent most of the time going over online tutorials and documentation about NLTK. In addition to this, I've also spent time researching the different types of ways that text is usually summarized. These methods include Abstractive and Extractive methods. Abstractive summarization is done by analyzing the text to determine it's meaning, which is then used to generate a summary. Extractive methods rely on scoring the words and sentences in a passage to determine important parts. These scoring functions are similar to those presented in CS 410. For the purposes of this project I will be using the latter method. After going through the documentation, I went ahead and created a Jupyter Notebook to start getting some hands on experience using NLTK. I've done some basic preprocessing steps to tokenize and build a document-term matrix out of a sample article I pulled from CNN. Challenges The biggest challenge so far has been familiarizing myself with the NLTL library. One thing that was hard was to setup the library in the beginning as the installation steps were not as simple as just using pip. Remaining Work The last remaining step to completing the Jupyter notebook is to implement the scoring function and actual text summarization steps. Fortunately NLTK has some built in libraries that can help with this meaning that it should not take up too much time. Once this step has been completed, the following things need to be done. 1. Package Jupyter Notebook code as an API. 2. Build google chrome extension. The extension will consist of a simple UI with a button allowing the user to summarize the article. All it does is just scrape the text from the screen, pass it into the API from step 1 and finally outputting the result.
https://github.com/cds95/CourseProject	project-documentation.pdf	CS 410 Project Documentation Christopher Dimitri Sastropranoto Project Goal The goal of the project was to build a google chrome extension that can summarize a CNN article. Architecture The initial plan was to host a python API that would accept GET requests from the extension. The extension would pass in the raw text to the API and get the summarized text back. I was unfortunately unable to successfully host the python server due to time constraints and a lack of experience in using either Django or Flask. In the end I worked around this issue by having all the code to summarize text live in the chrome extension. Implementation The project code is mainly split into two parts. I first did some exploratory work on how to do extractive text summarization using the NLTK python library in a Jupyter Notebook environment. The notebook can be found in the exploration folder. The next step was to actually build the chrome extension. The code for the extension lives in the chrome-extension folder. Folder Structure exploration test-article.txt --> Test article used for exploratory purposes text_summarization_exploration.ipynb --> Jupyter notebook chrome-extension manifest.json --> Configuration file for chrome extension content.js --> Code for scraping the page content and summarizing it user-interface.js --> Code to hand UI interactions user-interface.css --> Stylesheet for extension. user-interface.html --> HTML file for extension UI. icon.png --> Icon for extension. Summarization Algorithm The program relies on extractive summarization to summarize text. In extractive summarization, each sentence is given a score and the highest scoring sentences are used in the summary. This means that the summary is only limited to the content in the text. Sentences are scored based on the frequency counts of each of their non-stop word terms. 1. Tokenize words and sentences 2. Remove stop words from tokenized word list. 3. Calculate the frequency of each word and put into a dictionary. The dictionary keys are the words and the values are the counts. 4. Normalize the counts in the dictionary from step 2. 5. Calculate the score of each sentence. The score is calculated by adding up the scores of each word in a sentence. 6. Sort sentences by descending order of score. 7. Take the top N sentences by score but preserve their order. 8. Output step 7 as the result. Improvements * Incorporate inverse document frequency into the scoring function. * Use abstractive summarization instead of extractive summarization. Abstractive summarization uses deep learning models to decipher the meaning of the passage and generate new sentences for the summary. Self Evaluation I was able to accomplish the goals of the project as the extension I've implemented successfully scrapes the text from a CNN news article and manages to give a summary for it. Running the program Refer to the instructions provided in the README.MD section.
https://github.com/cds95/CourseProject	project-proposal.pdf	CS410 Project Proposal Christopher Dimitri Sastropranoto 678097021 Project Description The proposed project is to implement a google chrome extension that can quickly summarize a Medium articles. Who will benefit from such a tool? Almost anyone can benefit from such a tool. For example, I am an avid user of Medium but feel that some articles are unnecessarily lengthy and would love to save some time just getting the gist of an article by having a quick summary of what it is about. If I'm interested in the summary then I could go ahead and read the full article. Existing Tools Doing a quick google search resulted in several chrome extensions and websites that do something similar. The biggest drawback from most of them was that they required the user to either enter in a link to an article, or copy and paste the article's contents. The goal is for my tool is to allow the user to skip those steps and simply click a button to summarize the article's contents whilst on the browsing. Programming Language and other Other resources There are currently several online services that I can use to help implement this project. My plan is to use the python NLTK library to help with summarizing text and packaging that inside a Chrome Extension UI. In addition to this, I plan to do my own online research by utilizing YouTube, Medium and other websites to augment what I've learnt in CS410. Techniques and Algorithms From my research so far, summarizing text requires some preprocessing steps such as lemmatization and POS tagging. Once this is done, the next step would be to model topic distributions in the document. In addition to this, there are several summarization techniques that need to be explored. These are Extraction-based summarization, Extractive summary and Abstract based summarization. How will you demonstrate the usefulness of your tool? If successful, I plan to demonstrate the usefulness of my tool by creating a video and also releasing it in the Google Extension store. Project timeline Project start date: 10/26/20 Work Justification I feel that the scope of the project sufficiently fulfills the 20 hour requirement as it involves multiple components. This includes doing further research into the summarization techniques, preprocessing the data, training it using NLTK and finally implementing a Chrome Extension UI for it. Milestone Target Date Complete research into summarization techniques 10/28 Preprocess Data 11/4 Train text summarizer. This step will be done in isolation using JupyterNotebook. 11/8 Build chrome extension and hook up to text summarizer. This step requires implementing a way for the extension to scrape the article's contents from the website. 11/13 Testing and Bugfixes if needed 11/16
https://github.com/cds95/CourseProject	README.md	CourseProject Running the project Jupyter Notebook Exploration 1) Install Jupyter Notebook by following instructions here https://jupyter.org/install. From the root of the project directory. 2) cd exploration 3) jupyter-notebook Chrome Extension Refer to the demo to see how to run the extension. Link to demo: https://www.youtube.com/watch?v=PoMi_EibtCs
https://github.com/rohang62/CourseProject	CS 410 Project Progress Report.pdf	Rohan Goel, Paru Swaminathan, Satej Shah 1. Progress made thus far Thus far, we have set up our working environments and installed the necessary software. We have understood the task at hand and created a plan to move forward. We have cleaned the training data by removing stop words and brackets and by converting the data into word2vec vectors. 2. Remaining tasks We will now be building a model to learn to classify whether the response with context is sarcastic or not. We are planning on trying out Naive Bayes, Linear Regression, RNNs, a simple Neural Network, and a simple Random Forest Classifier. We plan to test each of these models in the week of November 30th, and are hoping to move forward with one of them, and begin hyperparameter tuning by December 7th. 3. Any challenges/issues being faced Currently, we are facing issues with how to utilize both the response and context. There are many tutorials online that help with understanding how to use one column, however, working with multiple features proves to be more challenging.
https://github.com/rohang62/CourseProject	cs 410_ report.pdf	"Rohan Goel, Satej Shah, Paru Swaminathan CS 410: Report An overview of the function of the code (i.e., what it does and what it can be used for). At a high level, the function of the code is to predict whether a piece of text (tweets and responses) is sarcastic or not sarcastic. Our program classifies the text (context) and associates a certain label (SARCASM or NOT_SARCASM) using an id. The input of our code is a json file named test.jsonl which contains responses with an associated id and context, and another train.jsonl file with responses, id, context, and a label. The output of our code is a comma separated file named answer.txt which contains the predictions on the test dataset. The file has exactly 1800 rows and each row has an id and the predicted label. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. The first step of our development process was to create a plan. Researching the different types of classification models, we were able to distinguish a couple of models that seemed promising to complete the task of classifying sarcasm of tweets. After testing some of them, we decided to proceed with the BERT language model and NN algorithm. Our first test was with FastText, however, the performance of this algorithm reached an upper limit, and therefore, we couldn't move forward with it. We also tried using K means with two centers, however we were unable to reach baseline with this. Therefore, we opted to go with using the BERT language model. Using the pre-trained BERT model provided the feature vectors which we then used to train the Neural Network. We first used a Logistic Regression model, however, due to the simplicity of the model, it did not perform well enough. We then decided to use a Neural Network with 3 layers. Through tweaking parameters, we were able to finally get our average accuracy above the baseline. In the code, we first read in the training and test data from the given folder. We then parsed the data. After this, we assigned binary values: 1 or 0 to the labels in the training data. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. To install and run this software, you can view the code through Github (https://github.com/rohang62/ClassificationCompetition). After cloning the repository onto your device locally, you can install Jupyter Notebook and open it through your browser. From here you can run the code blocks using the play button on the top of the screen. In the implementation of our code, we use many frameworks and libraries that may need to be installed locally such as PyTorch, numpy, pandas, transformers, and sklearn. After running all of the blocks of code, this will create a file ""answer.txt"" in which you can view the results of the program. Within this file, you will see a line for each situation and its resulting classification. Brief description of contribution of each team member in case of a multi-person team. Each member of the team learned and contributed to the project very well! Everyone was very enthusiastic, encouraging, and open to ideas. Working remotely, communication was very imperative to doing well. Rohan took lead on the development side, and Paru and Satej focused on the research and documentation. Although, all three members of the team helped each other out whenever needed. This team worked well together and was able to support each other throughout the course of the project."
https://github.com/rohang62/CourseProject	Project Proposal.pdf	Project Proposal: Team PRS 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. Rohan Goel: rohang4@illinois.edu (Team Captain) Satej Shah: sshah273@illinois.edu Parvathi Swaminathan: ps13@illinois.edu 2. Which competition do you plan to join? We plan to join the Text Classification competition. 3. If you choose the classification competition, are you prepared to learn state-of-the-art neural network classifiers? Name some neural classifiers and deep learning frameworks that you may have heard of. Describe any relevant prior experience with such methods Yes, we are prepared and excited to learn! Neural classifiers that we have heard of are BERT and LSTM. Some deep learning frameworks that we have heard of are PyTorch, TensorFlow, and Keras. Rohan has worked with TensorFlow and Pytorch in the past. Satej has not worked with such models. Parvathi has minimal prior experience with such models in an applied machine learning course. 4. Which programming language do you plan to use? We will be using Python in order to complete this project.
https://github.com/rohang62/CourseProject	README.md	CourseProject Final Project Video: https://www.youtube.com/watch?v=6Em313aRmys
https://github.com/Jon-LaFlamme/CourseProject	CS410FinalProjectDocumentation.pdf	CS410 Project Presentation: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAM Team Members: Jon LaFlamme (captain) jml11@illinois.edu Pradeep Sakhamoori ps44@illinois.edu Rahul Sharma rahul9@illinois.edu Project Overview: This application is a vertical search engine for food recipes. Special features include auto-complete and auto-suggestion in the search bar achieved with character and word n-grams. Search results are rendered on the same page as the search bar. Our system is configured with and run on Elasticsearch. Included with the project but not integrated with the search application is a content-based recommender system. Project Review Roadmap Special Consideration: Our team originally consisted of four members through the formation, proposal, development and progress report phases of the project, but one of our team members dropped this course prior to providing assigned deliverables. Per the TA, we were instructed to notify our reviewers to take this development into consideration in the evaluation of our final submission, as it has necessitated some adjustments to the scope of this project. Application/Software Functionality: This application is a food recipe search engine with a dataset of over 65,000 food recipes. Word and character-based autosuggest and autocomplete are supported dynamically in the search bar. Search results are dynamically displayed on the same page as the search bar, with matched query terms Recommended Supplementary Project overview video: https://youtu.be/8MrQOFWIooo Installation instructions: README.md Video install guides in README.md Recommender system video demo: https://youtu.be/pEuqzOdScEQ Recommender system writeup: Recommender_System.pdf Search engine and project architecture: https://youtu.be/8rgRQOhtluQ highlighted in the title field of each recipe that is rendered in the search results. Additionally, a content-based recommender system was developed and validated on the author's local machine. It is not integrated with our search application and is thus considered an 'extra' in terms of our project submission. To review the recommender module, simply watch the demo video and read the pdf. Application/Software Implementation: Software architecture: batch -> data ingestion modules; recommender module dataset -> food recipe datasets in JSON format es-setup -> Elasticsearch configuration files search-engine-webapp -> app -> application driver and API, browser files and templates search_engine_egg.info -> Application setup files user-profile -> our beta-recommender with user-profile as JSON AUTHORS.txt -> source code attributions README.md -> Installation instructions Dockerfile.amd -> Docker build script pdfs (proposal, progress report, final documentation, recommender system) Description of Team Member Contributions: All members: Weekly team meetings, topic exploration, identification of datasets, learning Elasticsearch and Kibana, shared contributions to proposal, progress report and final submission documents. Jon: Administrative tasks of coordinating, authoring or editing and submitting project deliverables. Assisted with troubleshooting and documenting Docker installation requirements on Mac. Spent several hours familiarizing myself Elasticsearch Assisted with automation of Elasticsearch data ingestion with data_loader.py. Assisted with user interface development (parsing JSON search results, and displaying search results as hyperlink recipe titles on same page as search bar). Assisted with bug fixes recommender API. Generated ~1500 test queries by collecting most popular 'recipe' queries using Google Trends (not implemented in final submission). Developed algorithm for sorting recipes search results based on complexity/difficulty (not implemented in final submission). Developed content-based recommender system (included in project but not integrated with our final application.) Pradeep: Started with spending time brainstorming project architecture, design discussions, planning, task allocation, internal milestones and deadlines (We used Microsoft teams to collaborate, drive weekly tasks and discussions). Initially started with contributions to look for available open source food recipe datasets, found couple of them which are publicly available (from Kaggle) and analyzing its format and its usability. We used Elasticsearch (ES) engine, python and flask interface for programming - Initial challenge is to get the environment ready to unblock others with a base line application (by pulling initially committed Skelton changes from team). Then spent time containerizing the complete application. Spent time understanding on launching elastic server as backend process from Docker with Ubuntu 16.04 environment - figuring out missing components in terms of launching web app and tunneling ES web server to localhost machine web-server - Debugging, fixing bug and released initial docker containerized app which was quite challenging. Developed/Coded basic/ initial pipeline for recommender system (contextual based) by pre- populating UserProfile (static) with specific food interest and which will be used for querying from ES(which is current a standalone module - which user can trigger from docker bash). Spent time supporting team with guidance on fixing docker issues on Mac. Rest of the time spent in testing application on Ubuntu. Rahul: Assisted with identification of dataset, set up project collaboration space on Trello. Built project architecture template for search application with Flask. Designed and authored Elasticsearch index configuration and ingestion scripts. Authored setup instructions and scripts for application build without Docker. Designed and authored user interface for search application. Designed and authored autosuggest and autocorrect features with Elasticsearch and Flask. Methodology and analysis: Please see included supplementary videos and documents for a more in-depth coverage of design choices, implementation details and results analysis.
https://github.com/Jon-LaFlamme/CourseProject	CS410ProjectProgress ReportFoodRecipeSearchEngine-2.pdf	CS410 Project Progress Report: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAM Team Members: Jonathan LaFlamme(Captain) jml11@illinois.edu Pradeep Sakhamoori ps44@illinois.edu Rahul Sharma rahul9@illinois.edu Rohan Khatu khatu2@illinois.edu Project Summary: Our original project proposal described a vertical search engine, specializing in Indian Cuisine that supports both queries and a recommender system. Our project scope has since narrowed due to time constraints. Our current project expectation is a working search engine that supports full text search with auto-suggestion as well some advanced filtering/query options. The application is run on Elasticsearch and containerized on Docker to run locally with minimal installation steps. Progress made thus far: * Identified our food recipe dataset * Developed search engine application using Elasticsearch and Kibana (for index file generation) * Dockerizing (container) to launch Elasticsearch server in the background and tunneling food recipe application to host machine on 127.0.0.1:5000 port using Flask * Validating and debugging container on Ubuntu and MacOS * Additional configurations and installations required for Mac users to get the tunneling of webserver onto localhost * Python notebook to injest data * Preprocessing and inverted index configuration tested on Kibana * Limited autosuggest and autocomplete features are deployed * Github project repo with build and deploy instructions Remaining Tasks: * Formatting search results page (html) * Organizing recipe links with single click URL * Displaying results on the same page as the query * Improving and enhancing the autocomplete and autosuggest features * Python script automating the preprocessing and inverted index generation * Optional inclusion of images with search engine, if possible Challenges and Issues: * Bug fixing in terms of adding missing python packages for deployment of automation pipeline with container * Running elasticsearch in background with Flask * Identifying search and autosuggest algorithm (eg. n-gram, character n-gram, stemming, lemmitization) * Team members learning and deploying Elasticsearch and Kibana toolkits. * Enabling Docker on Ubuntu Linux, specifically identifying the correct docker run options for routing Docker network traffic to localhost with '--network host' option. * Enabling Docker on Mac OS, specifically since docker runs as a virtual machine on Mac, additional installs and procedures are required for docker container to have access to mac host as a localhost * Integrating python Flask app with Elasticsearch and front-end * Identifying correct method for building Elasticsearch index
https://github.com/Jon-LaFlamme/CourseProject	CS410ProjectProposalFoodRecipeSearchEngine.pdf	"CS410 Project Proposal: Food Recipe Search Engine Team name: F20_TIS_DEV_TEAM 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team member s Team Members: Jonathan LaFlamme (Captain) jml11@illinois.edu Pradeep Sakhamoori ps44@illinois.edu Rahul Sharma rahul9@illinois.edu Rohan Khatu khatu2@illinois.edu Project Overview (What is the function of the tool?) : 2. What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? What is your free topic? Please give a detailed description. What is the task? Our project proposal is a vertical search engine that specializes in retrieving Indian Cuisine food recipes in a collection aggregated from multiple sources. Our goal is to support both push and pull retrieval models. We think that the specialization of this search tool is what will allow us to return more relevant documents to our users compared with existing food recipe search platforms that are broader in scope. For instance, we can build a user profile (essentially a customized background language model) with a brief initialization phase (ask the user a series of questions when they set up their profile) that should improve accuracy with each subsequent query (since we will gain a better understanding of our user's palette in the form of language modeling with user feedback/relevance judgments). We may also include sentiment analysis of recipe reviews as part of our document ranking algorithm at a later stage, but we are not yet committed to that. It will depend on available time. This project proposal is being submitted under the ""Free Topics"" category. Why is it important or interesting? This project holds tremendous academic value for our group in the context of this course as it requires both a high level understanding information retrieval as well as many opportunities to wrestle with challenging design questions specific to our context and application, such as language modeling, feedback mechanisms, ranking functions, classification, recommender systems and evaluation techniques. In a broader sense, we think that our application should outperform more generic food recipe search engines because we will be able to tune our search results based on a narrower context. But more specifically, we think that in the context of food recipes this will be especially advantageous due to the lower amount of noise present in our user's background language model. For instance, a user who routinely searches for American, Indian and Italian cuisine would add a lot of misdirection to their background language model compared with a user who only ever searched for Indian cuisine. Our system would begin to develop a more sophisticated picture of a user's Indian cuisine preferences and consequently retrieve more relevant recipes. Target User (Who will benefit from such a tool?) : We think this application would appeal to at-home cooks and chefs who are interested in Indian Cuisine. Originality of Tool (Does this kind of tool already exist? If similar tools exist, how is your tool different from them?  Would people care about the difference?) : We could not find a major food recipe search engine that specifically focuses only on Indian Cuisine. However, many of the most popular food recipe websites do provide search filters for querying or browsing Indian Cuisine. Yet, as mentioned in an earlier section, we think that our limited scope of topic coverage will allow us to tune our retrieval models to a greater degree of accuracy than would be possible on a platform that supports all types of cuisine. Another advantage of our tool is that it aggregates recipes in this one category across multiple sources. This adds a level of convenience and efficiency for our users who can query or browse a much larger collection of recipes from a single source that would otherwise be a tedious process working across multiple websites with different types of interfaces and retrieval models. Resource Utilization (What existing resources can you use? What techniques/algorithms will you use to develop the tool?) : We think that we can use the META toolkit for many of the preprocessing tasks involved, including web scraping, tokenizing, POS tagging, indexing and ranking. The BM25F ranking algorithm is an obvious candidate for our ranking algorithm because recipes naturally translate well into structured documents (eg. list of ingredients; title; description; cook time; etc.). As alluded to earlier, our document collection will be aggregated from multiple sources. One obvious early source will be a Kaggle dataset of 250,000 food recipes, but we hope to find additional recipes through other sources or from scraping food recipe websites. We hope to dynamically update our background language model in some fashion based on user feedback, but the exact formula/algorithm we will employ is still a matter under consideration. For our push recommendation system, we hope to test and compare both content and collaborative models of filtering. We will implement the better performing model for our final submission or consider implementing a combination of the two models. Validation (How will you demonstrate the usefulness of your tool?) For the validation phase, we will solicit 10 volunteers to generate and run their own set of 10 queries and make explicit relevance judgments about the top 10 ranked results for each query. Five of the queries will be fed to competing search engines with top ten results returned and judged by our users. Ideally, this will be done at random so as to obfuscate to the user which search engine they are using at any given time. We will be using statistical methods to evaluate our search engine and compare with competing engines (using precision and recall as the primary metrics). Once the queries and judgments are complete, we will feed our participant's user data into our recommendation system and send an email to each of the participants with five recipe recommendations. This email will ask for relevance judgments from each of the five recipes. We will also send another set of emails based on a collaborative model once all participants have completed their queries and associated relevance judgments. 3. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. Project Timeline (A very rough timeline to show when you expect to finish what.): Complete Proposal (By Oct 25) Architecture document with API level information (Python) and Test Scenarios (By Nov 1, 12 hours expected) Architecture block diagram showing end2end pipeline (Interface to search results representation) High level Input and Output data for each function block Sample TestCase (""Queries"") and TestResults (""Expected"") Document collection formation: tokenizing, POS tagging, indexing, etc. (By Nov 1) (20 total hours expected to aggregate our document collection for Indian Cuisine recipes and associated review/comments ) Associated preprocessing tasks (2 hours expected to tokenize, POS tag, build out the document collection, and build an inverted index ) Recipe Classification (By Nov 20) Classify recipes by cuisine style, meal type, dish type, etc. (20 hours) This process will help provide filters for advanced search functionality, and could also be integrated into our background language models. Develop test queries and test/select optimal scoring function (By Nov 8) Develop test queries (4 hours expected ) Test/select/optimize scoring function (3 hours expected ) Develop user profile and feedback model (By Nov 15) Research, build and test/select optimal user feedback model for recommendation system (15 hours expected ) Working user interface (By Nov 22) Locally hosted Web Interface/Python GUI with search button and results window (20 hours expected ) Trial Phase (Nov 22-Nov 29) Build automated system to pull query results from competing search engines (5 hours expected) Solicit volunteers for our application (4 hours expected to solicit volunteers, explain the tool and the participation requirements ) Evaluate and document/model query relevance judgments (1 hour expected) Evaluate push recommendation relevance judgments and adjust recommendation model (2 hours expected) Progress report completed (By Nov 29) Final software code with documentation and software presentation (By Dec 9) Debug/test source code (2 hours expected) Construct Readme (2 hours expected) Create presentation (4 hours expected) Total Hours expected: 116 hours Total Hours required: 80 hours 4. Which programming language do you plan to use ? Primarily Python Additional Details and Expected Outcome: We expect that our tool will outperform competing search engines within our specified topic/domain of Indian food recipes."
https://github.com/Jon-LaFlamme/CourseProject	README.md	"FOOD RECIPE SEARCH ENGINE USING ELASTICSEARCH, PYTHON AND FLASK Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview/Week 9 Activities. Installation guide for Ubuntu and Mac Users as given below (make sure you are not behind any proxy/firewall to test this project) $git clone https://github.com/Jon-LaFlamme/CourseProject.git Video Tutorial (Ubuntu): Build Docker Image, Generate Index file and running food recipe search engine web app https://www.youtube.com/watch?v=FooQHpTv63I Docker installation Follow instructions from https://docs.docker.com/get-docker/ select download based on your host machine OS Check if docker daemon is running on host machine (Ex: Output from Ubuntu 16.04 host machine): $docker # docker Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files (default ""/home/psakamoori/.docker"") -c, --context string Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with ""docker context use"") -D, --debug Enable debug mode -H, --host list Daemon socket(s) to connect to -l, --log-level string Set the logging level (""debug""|""info""|""warn""|""error""|""fatal"") (default ""info"") --tls Use TLS; implied by --tlsverify --tlscacert string Trust certs signed only by this CA (default ""/home/psakamoori/.docker/ca.pem"") --tlscert string Path to TLS certificate file (default ""/home/psakamoori/.docker/cert.pem"") --tlskey string Path to TLS key file (default ""/home/psakamoori/.docker/key.pem"") --tlsverify Use TLS and verify the remote -v, --version Print version information and quit (Linux) Instruction to Build docker image with ElasticSearch and application After successful installation of Docker, next step is to build the docker container image with ElasticSearch and application Command to build the docker image (makesure you are in same path as Dockerfile.amd64 file) $ docker build -f Dockerfile.amd64 -t food_recipe_se . [you can choose your own name instead of ""food_recipe_se""] It will take some time to build the image. Once done you can check your image on host machine with below command $ docker images Should see something like below REPOSITORY TAG IMAGE ID CREATED SIZE food_recipe_se latest 72ab77c2a9b3 35 minutes ago 2.13GB Troubleshooting ""docker build"" failure: - Change Folder & File access permissions of project recipes-search-engine$ chmod -R 777 * - Check if docker daemon is running https://docs.docker.com/config/daemon/ ### (Linux) Run the docker container to trigger search engine $docker run --net=host -p 5000:5000 food_recipe_se Expected output: Starting Elastic Server * Serving Flask app ""app"" (lazy loading) * Environment: development * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 271-999-248 **IMP NOTE**: Before launching search engine web app and start searching for food recipes user needs to follow below steps to generate index file (Linux) Launch ingest script: Open a new terminal window $docker ps -a (need sudo acccess) Identify and copy <container ID> for food_recipe_se, then run $ docker run -it --net=host -p 5000:5000 <container image name ex: food_recipe_se used to build the docker image> bash Output docker shell will look like below WARNING: Published ports are discarded when using host network mode UIUC@node01:/app$ In the container shell run: UIUC@node01:/app$ python batch/es_data_loader.py NOTE: If you receive connection error, retry again in 30-60 seconds Expected output (may take several minutes to complete ingestion): ****Load begins**** **Loading now: /app/dataset/recipes_raw_nosource_ar.json * 1000 documents successfully indexed in recipes_id index. ... ... * 65000 documents successfully indexed in recipes_idx index. (Linux) On your host machine open: http://0.0.0.0:5000 Paste url into browser to begin testing the application (Mac) ideo Tutorial Build Docker Image, Run Docker Image, Generate Index file https://youtu.be/pEuqzOdScEQ (Mac) Docker Desktop for Mac download instructions and configurations Install the latest edge release from Docker (v2.5.1.0 or later): https://docs.docker.com/docker-for-mac/edge-release-notes/ Install jq. Options: $brew install jq $port install jq Or: https://stedolan.github.io/jq/download/ Setup jq to work with Docker SOCKS server: $cd ~/Library/Group\ Containers/group.com.docker/ $mv settings.json settings.json.backup $cat settings.json.backup | jq '.[""socksProxyPort""]=8888' > settings.json Enable SOCKS proxy: System Preferences -> Network -> Advanced -> Proxies Select the box next to 'SOCKS proxy' Ensure SOCKS Proxy server reads 'local host:8888' Ensure Bypass settings reads '*.local, 169.254.0.0/16, *.io' If Unselected, Select the box next to 'Use Passive FTP Mode' Select ""OK"" then ""APPLY"" NOTE: Internet access may be disabled while you running your proxy server. To restore your settings after testing the application: * Deselect the box next to 'SOCKS Proxy' in your Network settings * Select ""OK"" and ""Apply"" Start Docker Desktop for Mac application: 'commmand' + 'space' and type 'Docker.app' then launch The blue whale Docker icon should appear in the apple menu bar * Select the icon and ensure green status: 'Docker Desktop is Running' Verify Expiramental features in Docker Desktop for Mac: Docker.app -> Preferences -> Expiramental Features * Ensure all features are enabled (Mac) Build the Docker image Navigate to project directory 'recipes-search-engine' in terminal. Enter command to build the Docker image: $ docker build -f Dockerfile.amd64 -t food_recipe_se . Expected output (may take a few minutes to complete): Building ... ... ... naming to docker.io/library/food_recipe_se (Mac) Run the Docker image Run the Docker image: $docker run -p 5000:5000 food_recipe_se Expected output: Starting Elastic Server * Serving Flask app ""app"" (lazy loading) * Environment: development * Debug mode: on * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 544-260-012 (Mac) Launch ingest script: Docker.app -> Dashboard -> Containers/Apps Next to the running food_recipe_se image, select ""CLI"" icon. This will open a terminal window fromn inside the Docker image. Recommend closing unused applications to speed up ingestion In the container shell: $python /app/batch/es_data_loader.py NOTE: If you receive connection error, retry again in 30-60 seconds Expected output (may take several minutes to complete ingestion): ****Load begins**** **Loading now: /app/dataset/recipes_raw_nosource_ar.json * 1000 documents successfully indexed in recipes_id index. ... * 65000 documents indexed. 65125 documents successfully indexed in recipes_idx index. (Mac) On your host machine open: http://0.0.0.0:5000/ Paste url into browser to begin testing the application (Linux/Mac) Exit condition: Kill the (app) docker container How to kill the container: - Open new command terminal - $docker ps -a (need sudo acccess) - Look for docker container with name ""food_recipe_se"" copy CONTAINER ID) - $docker stop <CONTAINER ID> - $docker rm <CONTAINER ID>"
https://github.com/Jon-LaFlamme/CourseProject	Recommender_System.pdf	"Recommender System Food Recipe Search Engine This is a brief overview of the recommender system we developed for our recipe search engine. It was not integrated into the app due to time constraints and some of the difficulties of working in the containerized environment. Overview We implemented a content-based recommender system for our food recipes. Essentially this is a cosine similarity scheme in which unseen recipes that are closest to the recipes in the user's profile would be recommended up to the user. Preprocessing steps and design choices For each recipe, the title and ingredient fields were concatenated into a single string, then preprocessed to lowercase and remove non-alphabetical characters. The rationale here is that title and ingredients would provide the best sense of the similarity between recipes and culinary preferences more generally. Since the ingredients field contains a lot of noise and would also have been much more expensive in terms of compute and space overhead, this was a relatively obvious design choice for us. Following concatenation of the title and ingredients field, we tokenized each word, and performed stop-word removal. The stop-words were adapted from NLTK's stop word list, which covers very frequent English words. We added 32 words that we found commonly in our dataset that were obviously non-informative for distinguishing food recipes. These include words like, ""tablespoon"", ""cups"",""advertisement"", etc. The next step was forming a corpus vocabulary from the preprocessed recipe documents, where every unique word in the vocabulary was stored in an index with the associated number of documents in which each word occurs. Concurrent with this step, each recipe document was converted from a list of words to an index of unique words with an associated word count for each unique word in the document. The average document length of the corpus was also computed during the vocabulary build process. The final step was performing a sub-linear transformation on the document vectors, converting the unique word counts in each document into a score for each term that captured the TF-IDF weighting for each term in each document. The BM25 formula was a natural fit for this transformation. To save on compute resources, this collection preprocessing step was done once with the results written to file in JSON format for easy retrieval during the recommendation step. Recommendation step For the recommendation step, a collection of recipe IDs are provided as the user profile input as well as the number of recommendations that the user would like returned. Then, the corpus is split into ""seen"" and ""unseen"" documents, where the ""seen"" documents are the list of recipes in the user profile. For each seen document, the dot product of the weighted recipe term vectors is computed for that document with each document in the unseen documents collection. The documents that return the overall k-highest scores are stored and returned as the algorithms output. Test Profile 'California Club Turkey Sandwich' ""Slammin' Salmon"" 'Molasses Cookies' 'Asian Chicken Salad' 'Holiday Chicken Salad' 'Jamaican Turkey Sandwich' 'Perfect Pumpkin Pie' 'Best Guacamole' 'American Lasagna' 'Cooky Cookies' Test Recommendations 'Easiest, Amazing Guacamole' 'Party Guacamole' 'Simple Pumpkin Pie' 'California Guacamole Hummus' 'Baked Pumpkin Custard ' Analysis These results are somewhat to be expected, since guacamole and avocado and pumpkin are relatively uncommon ingredients and would score quite highly, particularly in if they were short recipes. We suspect that several performance improvements could be achieved with some BM25 parameter turning, expansion of the stop-words list and perhaps even introducing smoothing into the scoring algorithm. Overall, though, we were pleased to complete a working recommender that returned reasonable results. Further use and testing As we did not design this recommender system to be integrated with our application, this was included as an ""extra'. if you wish to explore this module on your own, you will need to make some minor changes to the code. They are as follows: 1) You need to make copies of the datasets with the ""."" Removed from each recipe ID field. This is because the raw dataset we used is fed into Elasticsearch with the periods removed dynamically during ingest without updating our datasets. Batch -> ""es_data_loader.py"" should give you a nice template for how to complete this step. 2) Update filepaths: batch -> recommend_preprocessor.py ln 27 ln 125 batch -> term_vector_rec.py ln 87 ln 115 ln 116 3) Run recommend_preprocessor.py 4) Use test recipes for identical output 5) For a customized output, find the recipe IDs associated with your search results in the output of your Docker shell window. Just copy and paste the recipe IDs that you want to test into the term_vector_rec.py (lines 69-78) 6) Run term_vector_rec.py 7) Steps 1-3 are one-time activities, repeat steps 5 and 6 to run more tests with different IDs"
https://github.com/Jon-LaFlamme/CourseProject	setup_without_docker.md	Installation guide Go to search-engine-webapp directory Onetime setup-- Begin setup flaskr virtual environment & install it. python3 -m venv venv pip install -e . Download Elasticsearch/Kibana servers https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html https://www.elastic.co/guide/en/kibana/current/install.html Paste elasticsearch.yml from es-setup directory to <ELASTICSEARCH-HOME>/conf Start Elasticsearch and Kibana servers. Fix if there're any errors. Next step is launch the kibana UI and create index. This index will be used to store all documents and provide search features Kibana UI- localhost:5601 Go to management-> Dev Tools Next copy the create index command from /es-setup/es-notes.txt and run from the dev tools Onetime setup-- End Activate virtual environment . venv/bin/activate Launch webapp export FLASK_APP=app export FLASK_ENV=development flask run Fix if there are any missing libraries python -m pip install json2html TODO: set application port from config file Move to docker so same command works for everyone. Automate entire installation process
https://github.com/ShyamShah11/CourseProject	progress_report.pdf	Which tasks have been completed? I have determined the three techniques I wanted to implement (naive bayes, knn, and max entropy classifiers). I have generated my datasets, and have generated the feature vector representation of each URL. I'm still experimenting with different representations but have most of the code ready for me to easily make changes to it. I originally tried using tokens from the URL but have moved to using English words in the URL. I have also created the first draft of my naive bayes and knn classifiers. They both currently classify about 80% of URLs (n=500) correctly. Which tasks are pending? I need to implement the max entropy classifier and tune my models. I also need to prepare the documentation and potentially add some files to be ran specifically for demoing. Are you facing any challenges? Progress has been limited at times because of hardware limitations (I considered moving to cloud but wanted to keep it simple/local for demo purposes). So I haven't been able to use my entire dataset yet which may skew the classifiers performance.
https://github.com/ShyamShah11/CourseProject	proposal.pdf	"Topic Specific Questions: 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will be doing this project by myself so I will be the captain. My name is Shyam Shah and my NetID is shyam3. 2. What system have you chosen? Which subtopic(s) under the system? The system I chose is ExpertSearch and the subtopic I will work on is ""Automatically crawling faculty webpages"" -> ""Identifying webpage faculty URLs"" 3. Briefly describe the datasets, algorithms or techniques you plan to use I will be working primarily with the faculty webpage dataset (from MP2.3) for positive examples. I intend on using the links on the sign-up sheet from MP2.1 (https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVxixU5vvMuU aM/edit#gid=0) to also scrape some negative examples. My plan is to approach this as a classification problem, as suggested. I intend on using different classification algorithms/techniques (such as logistic regression, k-nearest neighbors, and neural networks) and want to treat part of this as a research opportunity to teach myself some different kinds of classification algorithms that are effective with text. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? I am going to treat this as adding a function to classify URLs on a given webpage. I can demonstrate that this works by splitting my dataset into a training and validation dataset and measuring precision/recall on the validation dataset. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly I'll start by forking the current project on Github and adding the functionality to it while developing it. There are instructions in the repo to run the application locally so I would not need to build anything else on my own. Ultimately, I would like to have it merged with the current project but that may be something I look into outside of this course. 6. Which programming language do you plan to use? I will be using Python 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. 3h Research algorithms 2h Prepare data 10h Implement different algorithms 5h Testing and tuning parameters Other General Questions: 1. What is the function of the project? The function is to add the ability for ExpertSearch to automatically identify links within a page as relevant without requiring human intervention. 2. Who will benefit from this project? The users and developers of ExpertSearch 3. What existing resources can you use? I can use existing Python libraries like tensorflow and sklearn to implement the models, most of the work will probably be identifying good features to use for them 4. A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) November 1 Gather information on all of the models I want to implement November 22 Finish implementing first version of models November 29 Finish tuning and testing models December 7 Finish creating documentation and demonstration"
https://github.com/ShyamShah11/CourseProject	README.md	ExpertSearch: Identifying Faculty Webpage URLs Proposal The project proposal is in the file called proposal.pdf. Progress Report The progress report is in the file called progress_report.pdf Demonstration For my reviewers, the setup instructions and demo are documented in the README file of the code folder. Details about my implementation and decisions made are below. Implementation I decided that I did not need to use the ExpertSearch code for this but I did use some of the ExperSearch data. I can isolate the task of classifying URLs from the rest of the functionality provided by ExpertSearch so this project will be me working on it seperately (for now). Everything implementation related is in this folder. Preparing data In the data folder, I have a file for positive training data and negative training data. The positive training data has examples of actual faculty URLs and is taken directly from here. The negative data has examples of non faculty URLs from university pages as well as all of the directory URLs themselves from the MP2.1 signup spreadsheet. The code for generating these is in scraper.py. Note: the scraper uses selenium which requires chromedriver.exe to either be on your PATH, or in the same directory as scraper.py. I have over 15,000 positive and negative URLs but only ended up using 500 of each of them. You won't need to run scraper.py unless you intend on using more than 15,000 URLs to retrain the models. Data as feature vectors For features, I used tokens from the URL (ie. everything seperated by a non alphabetic characters) first. After a few tests, I decided to use only the tokens that were also English words since the tokens that aren't English words are usually people's names or university name acronyms which I felt did not add enough value for the number of features they added. preparedata.py converts the datasets into feature vectors which are used for the different algorithms, and it counts the term frequency over all URLs. I have only uploaded feature vectors for a small subset of my data to avoid file size constraints. Classifiers The three classifiers I experimented with were Naive-Bayes, maximum entropy, and k-nearest neighbors. I also tuned my models based on a small sample size of data (1000 total URLs) because of time and hardware constraints. If the models are retrained with more data, it's likely that they will need to be tuned again. Naive Bayes I experimented with 4 kinds of Naive Bayes classifiers: Gaussian, Bernoulli, complement, and multinomial. Over 400 test points, these were the results (accuracy is the number of correctly labelled URLs). | Classifier | Accuracy | | :-------------|:------------- | | Gaussian | 83% | | Bernoulli | 87% | | Complement | 79% | | Multinomial | 49% | Based on this, I decided to go with a Bernoulli Naive Bayes classifier. One of the benefits of a Bayes classifier is that you get to use a prior. However since I had an even split of positive and negative samples in the data I used, the prior I chose to use was a uniform prior over the two labels. A possible improvement would be to modify the scraper to calculate a more realistic prior by calculating the amount of URLs on a university webpage that are faculty URLs. K-nearest neighbors For the k-nearest neighbors classifier, I experimented with using uniform weights and distance based weights. These were the results for uniform weights. | k | Accuracy | | :----|:------------- | | 2 | 71% | | 3 | 84% | | 4 | 83% | | 5 | 84% | | 6 | 84% | | 7 | 83% | | 8 | 85% | | 9 | 84% | | 10 | 84% | | 11 | 85% | | 12 | 84% | | 13 | 83% | | 14 | 82% | | 15 | 82% | | 16 | 79% | And these were the results for distance based weights. | k | Accuracy | | :----|:------------- | | 2 | 71% | | 3 | 84% | | 4 | 84% | | 5 | 85% | | 6 | 85% | | 7 | 84% | | 8 | 86% | | 9 | 86% | | 10 | 86% | | 11 | 88% | | 12 | 86% | | 13 | 86% | | 14 | 86% | | 15 | 85% | | 16 | 85% | Based on these results I went with k=11 and using distance based weights. Maximum entropy For the maximum entropy classifier, I first had to make changes to how my data was being represented. The nltk MaxEntClassifier requires a dictionary instead of an array to be trained over. So instead of using the same feature vectors, I converted them back to dictionaries of tokens. I experimented with the GIS and the IIS algorithms. With 100 iterations, the GIS algorithm classified 92% of the test points correctly, and the IIS algorithm had 91% accuracy. The accuracy for both algorithms went down when I tried again with 500 iterations so I stuck with the GIS algorithm and 100 iterations. I used this paper to help come up with some of the ideas used for this project.
https://github.com/dvillevald/CourseProject	Project Progress Report dmitryv2.pdf	Using MD&A of company's SEC filing to predict future stock price CS410 Project Progress Report by Dmitry Villevald (dmitryv2@illinois.edu) Project Progress Report: Q: Which tasks have been completed? A: The following tasks have been completed: * I completed the code which pulls the index files from the SEC EDGAR database for each quarter and extracts Q-10 and K-10 filings for the selected companies. * From every filing I built a dictionary including all words references in the document with their frequencies (i.e. bag of words representation.) * Finally, these term frequencies were compared with Loughran and McDonald Sentiment Word Lists and four sentiment scores were calculated for each filing - Positive, Negative, Uncertain and Litigious. Q: Which tasks are pending? A: Pending tasks: * Upload the stock returns data for the selected companies (forward from the date of filing) and estimate performance of several sentiment-based investment strategies. Q: Are you facing any challenges? A: Not really. It turned out that extracting only the MD&A section from the filing is difficult because in many filings this section is a placeholder containing a list of links to the other sections of the document where the related topics are discussed. Because of this I chose to build the bag of words for the entire filing. This, in my opinion, would be a better option (compared to selecting only the filings where MD&A is a well-structured separate section) because it will ensure that the sentiment words are captured regardless where in the document they appear.
https://github.com/dvillevald/CourseProject	Project Proposal dmitryv2.pdf	Using MD&A of company's SEC filing to predict future stock price CS410 Final Project Proposal (Individual project) Project coordinator/Author: Dmitry Villevald (dmitryv2@illinois.edu) For my final course project I would like to choose a free topic (Option 5) and explore the impact of the information extracted from the Management's Discussion and Analysis (MD&A) section of the company's SEC filing on the future stock price. Q: What are the names and NetIDs of all your team members? Who is the captain? A: This is individual project by Dmitry Villevald (dmitryv2) Q: What is your free topic? Please give a detailed description. What is the task? Why is it important or interesting? What is your planned approach? What tools, systems or datasets are involved? What is the expected outcome? How are you going to evaluate your work? A: I would like to explore how the changes in a sentiment of the company's Management's Discussion and Analysis (MD&A) impact the future stock prices. MD&A section is an unaudited section of quarterly and annual SEC filings where the company's management discusses the current status of a company and, more importantly, the future risks and opportunities. I want to explore if investors overreact to changes in management sentiment in the long run. This knowledge would be important for investors trying to decide what to do with a company stock when management sentiment changes. My plan is, first, to pull 10Q and 10K SEC filings of the companies - members of SP&500 or Dow Jones index - over a few recent years from SEC EDGAR database, parse the data and extract MD&A sections. Then, I plan to determine a sentiment score of each MD&A document using the vocabulary of positive and negative words (for example, Loughran and McDonald sentiment word lists). Third, I plan to select the events with significant changes in sentiment scores and see how these changes are correlated with the changes in company's stock price in a quarter following the filing and its announcement (using Yahoo Finance data stock price data or similar sources) adjusted for market or sector/industry returns. If the stock markets are efficient then one would not expect a significant investors' over- or under-reaction to changes in sentiment which I want to confirm. The performance metric could be the confusion matrix showing the relationships between the significant (i.e. higher, in absolute value, than a certain threshold value) changes in MD&A sentiment score - positive or negative - and the significant - positive or negative - next-quarter company's stock returns. Q: Which programming language do you plan to use? A: Python Q: Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. A: I plan to spend about 3 hours to write and test the script which will download, parse and clean MD&A data. Then it would take another 7-10 hours to actually perform these tasks (i.e. downloading, parsing and cleaning the data.) Another 2-5 hours would be spent on extracting the sentiment from MD&A sections and identifying the cases when sentiment score experiences significant changes. About 2 hours will be spent on exploring the relationships between the stock prices and sentiment scores and drawing the conclusions. Finally, I expect to spend about 5-6 hours to document my process/findings and to create a demo/presentation.
https://github.com/dvillevald/CourseProject	README.md	CS 410 Course Final Project Report Using the sentiment analysis of company's SEC filings to predict future stock returns Author: Dmitry Villevald (dmitryv2@illinois.edu) Goal For my final project I chose a free topic (Option 5) and explored the impact of the sentiment extracted from company's 10-Q and 10-k SEC filings on the company's stock price. My expectation was that while there could be a minor impact from the sentiment changes in the filings on the stock price for some companies, this impact is likely too small and statistically insignificant for building stock investment strategy based on the sentiment only. Project Proposal File Project Proposal dmitryv2.pdf contains the project proposal. Project Progress Report File Project Progress Report dmitryv2.pdf is a project progress report. Self-Evaluation I completed most of what I planned. I was able to download from the SEC EDGAR database the index files with the titles and types of the public companies' filings and then extract the quarterly (10-Q) and annual (10-K) report for the selected companies. Originally I planned to extract only Management Analysis and Discussion (MD&A) section of 10-Q and 10-K filings for the sentiment analysis. However, after I learned that in many reports the MD&A is a list of links to the other sections of the document, I decided that a better approach would be to use the entire SEC filing for the sentiment analysis. I managed to parse those filings and build a bad-of-words (unigram) representation of each filing with the term frequencies. Then, by comparing these representations with LoughranMcDonald sentiment lists I was able to calculate four sentiment scores (Positive, Negative, Uncertain and Litigious) for each company's filing. Finally, after I loaded historical stock prices from Yahoo Finance, I calculated the forward returns (which include the dividents and are adjusted for stock splits) and joined them with the sentiment data. Finally, I was able to estimate expected returns for a few sentiment-based investment strategies. Main Results Although I only explored a few years (2016-2020) of filings for about 30 large US companies, I was able to observe some interesting results. For example, I found that Negative, Uncertain and Litigious sentiments are strongly correlated which suggests that if we use one of there three sentiments in out investment strategy, the incremental impact from including the other two will probably be marginal if any. Regarding the correlation between the future returns and changes in the sentiment scores of SEC filings, I observed some positive correlation between 1-month-forward returns from the date of filing and the quarterly changes in Negative, Uncertain and Litigious sentiment scores. For example, as the chart below shows, for MCD and APPL stocks the increase in negative/uncertain/litigious sentiment scores was followed by larger positive stock returns over the following month which is probably contrary to what most would expect. In conclusion, while the changes in the sentiment of company's filings seem to have some impact on the future stock returns, this impact is not large and, given a small data sample, is likely statistically insignificant. Additional research with more companies and longer time period is needed to build a viable stock investment strategy based on a sentiment of companies' SEC filings. Note that the charts above were built from the output of Python script demo.py - file Sentiment scores of SEC filings with forward stock returns.csv - referenced below in subsection Outputs. Demo (Video) The video with demonstration of how to run script demo.py and produce the output can be found here Documented Source Code (folder demo) The Python script demo.py 1) Downloads 10-Q and 10-K SEC filings for selected companies. Ticker, CIK and Company (company name) should be provided (for each companies one is interested in) in the input file /investment_universe/tickers_and_ciks.csv (Note that the input file for the demo contains this data for two companies - McDonalds Corp. and Apple Inc.) Ticker is used to load the historical company's stock prices to backtest investment strategy while CIK (the Central Index Key) is required to download company's filings from SEC's EDGAR database. 2) Build a bag-of-words representation and calculates term frequency for each SEC filing. 3) Calculates sentiment scores (Positive, Negative, Uncertain and Litigious) for each report. 4) Downloads historical company's stock prices and calculates weekly, quarterly and yearly forward returns, starting from the filing date (+ execution_lag_days to mitigate a look-ahead bias via simulation of a more realistic and conservative scenario where the stocks are purchased/sold on the next business day after the filing date). 5) Combines together sentiment scores and foward returns, calculates returns of a few simple investment strategies and saves the results in folder /results. In addition to the instructions below, the Python script demo.py has detailed comments on each step. Also, this video shows how to run the script. Instructions 1) Before running the script demo.py, please assign to variable base_path the location of (path to) the project folder. Example: base_path = '/Users/dmitryvillevald/Documents/UIUC/CS 410 Text Information Systems/Final Project/demo' 2) The following packages have to be installed to run the script: requests, BeautifulSoup, json, urllib, yfinance, pandas, and csv Inputs The script demo.py takes the following inputs: 1) Input file /investment_universe/tickers_and_ciks.csv with a list of companies (stocks) you want to test the investment strategy on. For each company one has to provide: - Ticker - the exchange ticker of a company's stock. Ticker is used to load the historical company's stock prices to backtest the investment strategy - CIK - the Central Index Key of a company in SEC EDGAR database. CIK is used to download the company's filings from SEC's EDGAR database for sentiment analysis - Company - the Company's name which is used for reference only Note that the input file for the demo contains this data for two companies - McDonalds Corp. and Apple Inc. 2) Four files with LoughranMcDonald sentiment lists (positive.csv, negative.csv, uncertain.csv and litigious.csv) which are located in folder /sentiment_word_lists Outputs The script demo.py outputs two files and places them in folder /results: 1) File Sentiment scores of SEC filings with forward stock returns.csv contains (1) a history of the sentiment scores extracted from SEC filings of the selected companies, (2) the changes (quarterly and yearly) of those sentiment scores and (3) one-week, one-month and one-quarter forward total stock returns (including dividends and adjusted for stock splits and spinoffs) starting from the date of SEC filing. 2) File Investment strategies results.csv contains the results of testing simple investment strategy where one takes a long position in a stock (i.e buys it) when the sentiment percent change value exceeds the long threshold and takes a short position in a stock (i.e. sells it short) when the sentiment percent change falls below the short threshold. Credits 1) https://gist.github.com/madewitt/29bceb51c494ef9ea1d34f9474aa4b3c 2) https://github.com/weiwangchun/cs410 3) https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm 4) https://sraf.nd.edu/textual-analysis/resources/#LM%20Sentiment%20Word%20Lists
https://github.com/difangu/CourseProject	Final Report.pdf	"12/10/2020 CS 410 - Text Information System Difan Gu, Yanyue Wang, Wen Long Team APlus Final Documentation Overview: Oftentimes, users found it's hard to fathom the information that they barely learnt before. For example, how are we able to answer the question, ""what is data science?"", especially to an outsider? How are we able to clarify the definition of ""data science"" or ""computer science"" by borrowing more basic or common words or terms to further help our users to understand? Our implementation of a semantic annotation algorithm based on the paper, ""Generating semantic annotations for frequent patterns with context analysis"", can achieve the goal. The final goal is to automatically decipher certain words, terms, and even sentences by providing its highly-associated while distinct frequent patterns in semantic text form. In our case, to be more specific, we used the algorithm to summarize what specialty each college published in major computer science conferences. The Digital Bibliography & Library Project (DBLP) computer science bibliography acts as good study material for our project. It contains the metadata of more than 1.8 million publications in thousands of journals and conferences proceeding series written by over 1 million authors. It first started to be a bibliography on database systems and logic programming but has since expanded to all fields of computer science. From this well-structured dataset, we selected three top U.S.-based universities including Massachusetts Institute of Technology (MIT), Georgia Institute of Technology (GT), and the University of Maryland (UMD) as our use case. By implementing the algorithm, we can extract a series of words or terms to differentiate their academic focus based on thousands of paper titles published throughout the years: some colleges will be more inclined to data analysis, the others will more concentrate on wireless systems. In the real world, the utilization of automatic annotation can also be universal: users can use the algorithm to understand not-well-defined text information such as ""NLP"", ""Machine Learning"" and ""Deep Learning"" that is not defined in the dictionary such as Merriam Webster. Step 1: Load, clean raw data, and tokenization As can be seen in Figure 1, the software first converts the XML file downloaded from DBLP into a string format and output a table. Every publication was saved as a single string. Then all the elements inside the XML schema were saved as an item inside a dictionary, with the key being the name of the element and the value being the content. Another list has been created for every single publication as an element, describing the affiliated university as well as the publication title. We then ranked these universities by their number of publications and printed the top 50. Every title of the publication was then tokenized word by word. Each token was assigned an integer for further mining. Figure 1. Workflow of Step 1. Step 2: Selecting sample titles from target schools Step 2 serves as a pre-processing step for pattern mining. We ranked top 10 universities around the world based on their publication and saved their titles in a long single string, but only those publications from US universities were used for pattern mining simply because English titles are easier to understand. The universities chosen are MIT, GT, and UMD. Step 3: Pattern Mining In this step, we loaded publication titles by the above universities and loaded the frequent patterns mined by CloSpan, a software package of mining closed sequential patterns in a sequence database. Step 4. Feature pattern selection Redundancy removal was performed so that duplicate items are removed from the mined frequent patterns. The resulting patterns from each university were ranked based on mutual information. The integrated patterns were finally converted to words via a built-in dictionary. Technical Details: a) Load, clean raw data, and tokenization -input: -dblp.xml is the source file downloaded from the dblp computer science bibliography Figure 2. DBLP XML File - implementation of the software - script file: dblp_1.ipynb - language: PySpark - Environment: Azure Synapse(Spark) - output - dblp_school.txt is the text documents that contain all school names that are in the same order as dblp_title.txt Figure 3. School Names in DBLP - dblp_title.txt is the text document that contains all titles of papers published by varieties of colleges across the world. It shares the same order as dblp_school.txt. Most importantly, all of the words have been converted into integers where the mapping can be found in dblp_word.txt Figure 4. Integerized Publication Titles. - dblp_word.txt is the dictionary that has the mapping between integers to words. Figure 5. The Word-to-integer Dictionary b) Selecting Sample Titles from Target Schools: - Input: - dblp_school.txt is the text documents that contain all school names that are in the same order as dblp_title.txt - dblp_title.txt is the text document that contains all titles of papers published by varieties of colleges across the world. It shares the same order as dblp_school.txt. Most importantly, all of the words have been converted into integers where the mapping can be found in dblp_word.txt - dblp_word.txt is the dictionary that has the mapping between integers to words. - Process: - school_pattern_creator.ipynb is the pre-processing step that achieves the goal of selecting the sample school that users want to make the comparison. In our case, we select all of the titles published by 3 major U.S colleges. - - Output: The output is a series of titles grouped by each college. For example, in the picture below where you can find 0 is mapped to University of Satilde, therefore output0.txt includes all titles belonging to University of Satilde. However, in order to improve interpretability, we choose output3.txt, output5.txt, and output8.txt as our samples. However, we encourage our users to explore more colleges. - school0.txt - school1.txt - school2.txt - school3.txt - school4.txt - school5.txt - school6.txt - school7.txt - school8.txt - school9.txt c) Pattern Mining: - Input: - school2.txt contains all titles from MIT - school8.txt contains all titles from GT - school9.txt contains all titles from UoM - Process: - CloSpan is a closed pattern mining algorithm that is able to find a series of highly associated patterns in the sequence database. It was proposed by Yan et al. (2003). We use CloSpan to extract ""most common"" word or term patterns in each title grouped by each college. - spmf.jar is required to run CloSpan in the same directory. The following java code is to generate the closed frequent patterns for each college. There is only 1 main parameter, called support. It defines pattern commonality. Users can adjust the parameters to explore. - java -jar spmf.jar run CloSpan school3.txt output3.txt 1% - java -jar spmf.jar run CloSpan school5.txt output5.txt 0.5% - java -jar spmf.jar run CloSpan school8.txt output8.txt 3% - Output: - output2.txt is the frequent pattern from University of Maryland - output8.txt is the frequent pattern from MIT - output9.txt is the frequent pattern from Georgia Institute of Technology d) Feature Pattern Selection: - input: - school2.txt - school8.txt - school9.txt - output2.txt - output8.txt - output9.txt i - dblp_word.txt - Process: - Remove Redundancy is to mitigate redundancy issues in the frequent pattern. For example, in the picture shown below, there are a lot of redundant words in a single transaction due to the way we pre-process the terms. And we found simply taking unique items in the pattern while keeping the word/term orders will be effective. For example, [1,-1,1,1,1, -1, 2, -2] will be reduced to [1, -1, 2, -2] where all positive integers represent a unique word while -1 separates two itemsets and -2 imply the end of the sentence. - Feature Pattern Selection by Mutual Information is to extract the most distinct pattern among all of the schools with high frequency. Two schools might have shared the same focus, for example, both MIT and GT might focus on ""system"" while MIT might pay more attention to ""design"" while GT might give more weight to ""architecture"". Therefore, ""system"" is not the best candidate to define the difference between MIT and GT, while ""design"" and ""architecture"" are. We want to give more weight to ""design"" and ""architecture"" however not losing the importance of ""system"" as well. - Word Conversion is the last step. Till the previous step, we worked on a series of numbers for each transaction that represents a unique word. Now we convert back to words based on the dictionary so that we are able to understand the meaning. - Output: - Each college with its most distinct and representative words or terms to demonstrate their academic focus in computer science conferences. All of the words/terms have been ranked descendingly. We will get the top 5 terms/words for demonstration. Conclusion: We may sense a different focus for each school. For example, MIT focuses a lot on system control and power/energy-related topics; GT focuses more on programming and framework; UoM puts a lot of attention on wireless networks as well as object recognition, which might be in the Computer Vision area. * MIT: ""system control"", ""power, energy"", ""control, analysis"", ""power, applications"", ""systems, large"" * GT: ""programming"", ""framework"", ""management"", ""problems"", ""approach"" * UoM: ""resource, wireless"", ""embedded, systems"", ""networks, resource"", ""recognition, object"", ""social"" Demo Steps: We've shared voiced video here 1. Download file annotation from our Github 2. Open Terminal and cd to the directory annotation 3. python school_pattern_creator.py 4. java -jar spmf.jar run CloSpan ./school_output/school9.txt ./pattern_output/output9.txt 1% 5. java -jar spmf.jar run CloSpan ./school_output/school2.txt ./pattern_output/output2.txt 0.5% 6. java -jar spmf.jar run CloSpan ./school_output/school8.txt ./pattern_output/output8.txt 3% 7. python pattern_decipher.py 8. you can find the final output in ./annotation/outcome Team Responsibilities: * Difan Gu is responsible for the core algorithm development including mutual information, data preprocessing, pattern mining and frequent pattern selection, also contributed to the final report, documentation and presentation. * Wen Long is responsible for data preprocessing from XML, association mining. He also contributed to the final report and documentation. * Yanyue Wang has brainstormed and researched publications concerning frequent pattern mining. She also documented the pattern mining process. Reference: KDD '06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining August 2006 Pages 337-346 https://doi.org/10.1145/1150402.1150441 CloSpan: Mining Closed Sequential Patterns in Large Datasets, by X. Yan, J. Han, and R. Afshar. Proc. of 2003 SIAM Int. Conf. Data Mining (SDM'03), 2003"
https://github.com/difangu/CourseProject	Project Progress Report.pdf	11/28/2020 CS 410 - Text Information System Difan Gu, Yanyue Wang, Wen Long Team APlus Progress Report 1) Which tasks have been completed? - Researched pattern mining and generated ideas on how to demonstrate the significance of analyzing frequent patterns with context units. - Understood the main steps and algorithms (hierarchical and one-step microclustering, etc.) mentioned in the paper. - Loaded and cleaned source data from https://dblp.uni-trier.de/xml - Implemented vector space modeling on the DBLP dataset mentioned in the paper, constructed a set of frequent models. - Selected context units as patterns (minimal units that carry semantic information in a dataset). - In summary, we completed about 30% of the paper. 2) Which tasks are pending? - Redundancy removing - Strength weighting for context units - Extracting strongest context indicator - Extracting representative transactions - Extracting semantically similar patterns 3) Are you facing any challenges? - Some parts of algorithms are too complicated to implement within a short period of time. We may need simplification in order to deliver a reasonable outcome. - dataset is large and we may need some time to perform more detailed data cleaning and manipulation - collaboration of the project development
https://github.com/difangu/CourseProject	README.md	"Generating Semantic Annotations for Frequent Patterns with Context Analysis Click Here for Voiced Presentation & Demo for Grader Oftentimes, users found it's hard to fathom the information that they barely learnt before. For example, how are we able to answer the question, ""what is data science?"", especially to an outsider? How are we able to clarify the definition of ""data science"" or ""computer science"" by borrowing more basic or common words or terms to further help our users to understand? Our implementation of a semantic annotation algorithm based on the paper, ""Generating semantic annotations for frequent patterns with context analysis"", can achieve the goal. The final goal is to automatically decipher certain words, terms, and even sentences by providing its highly-associated while distinct frequent patterns in semantic text form. In our case, to be more specific, we used the algorithm to summarize what specialty each college published in major computer science conferences. The Digital Bibliography & Library Project (DBLP) computer science bibliography acts as good study material for our project. It contains the metadata of more than 1.8 million publications in thousands of journals and conferences proceeding series written by over 1 million authors. It first started to be a bibliography on database systems and logic programming but has since expanded to all fields of computer science. From this well-structured dataset, we selected three top U.S.-based universities including Massachusetts Institute of Technology (MIT), Georgia Institute of Technology (GT), and the University of Maryland as our user case. By implementing the algorithm, we can extract a series of words or terms to differentiate their academic focus based on thousands of paper titles published throughout the years: some colleges will be more inclined to data analysis, the others will more concentrate on wireless systems. In the real world, the utilization of automatic annotation can also be universal: users can use the algorithm to understand not-well-defined text information such as ""NLP"", ""Machine Learning"" and ""Deep Learning"" that is not defined in the dictionary such as Merriam Webster. Reference: KDD '06: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data miningAugust 2006 Pages 337-346https://doi.org/10.1145/1150402.1150441"
https://github.com/zachdick93/CourseProject	CODE DOCUMENTATION.docx	CODE DOCUMENTATION Project Directory Structure: The code implemented for this project is in the LARA directory. In this directory you will find the Makefile that is used to compile and build this code. If the project has been built you will also see the object files (e.g. main.o), and the executable prog.exe. The src directory contains the source files for this project, and the include directory includes the header files. The file src/main.cpp is used to test the classes implemented in this project. The main class in this project is the LRR class. The code for this class can be found in the src/lara directory. In src/optimization is an implementation of the LBFGS optimization algorithm (https://en.wikipedia.org/wiki/Limited-memory_BFGS for more information). The directories src/utilities and src/algebra have some utility like functions used to perform frequently used calculations throughout the rest of the code. To learn more about the theoretical aspects of the implemented functionality in this project I recommend reading the document titled manual.pdf. Building and Running: This code was developed on Windows 10 and compiled using the MINGW64 implementation of the g++ compiler tools. There is no guarantee that this will work on other operating systems, but I would expect that it will work with any g++ version on any OS. To build this code open a command prompt, go into the LARA, and run the command make all. Once the project has built you will see an executable named prog.exe. To run this code, using that same command prompt window, type in the command ./prog.exe. Testing the Code There is a test function for the LRR functionality in the main.cpp folder (see image below). This function initializes the model with the same parameters used in the document manual.pdf which was provided by the original project implementation. Then this function calls the EMEst function passing a file of vectors, also from the original project. At the end of the EM estimation step, this function will save the predictions in LARA/Data/Results/prediction.dat and it will save the model, which can be loaded as a 5th parameter to the constructor, in LARA/Data/Model/model_hotel.dat.
https://github.com/zachdick93/CourseProject	CS410 Project Presentation Slides.pptx	"CS410 Course Project: Implement LRR for MeTA By Zachariah Dick Project Description Implement the aspect segmentation and Latent Rating Regression model in C++ from the paper ""latent aspect rating analysis on review text data: a rating regression approach"". Developed with the intent to be easily ported into the MeTA codebase. I was able to use the original Java project code as a reference for this. So, ultimately the project was rewriting the Java code in C++. Development Environment OS: Windows 10 Build Tools: MINGW64 implementation of g++. IDE: Visual Studio Code In This Repository The C++ code can be found in the LARA directory. The original Java code can be found in the OldJavaLARAProject directory. The project directory structure is the same between the two. Src: contains the source code files. Includes: contains header files. Data: includes the data input and output files used by the project. The next slide has a picture of the directory tree structure. Challenges The MeTA project had too much technical debt for me to build it locally on my machine without many errors. The original LARA Java Code project files were not recognizable by my IntelliJ IDE, so I was not able to use it as a debugging tool for my C++ implementation. The Java code referenced some jar file Java specific libraries that I was not able to replace properly. One was a linear algebra library that I replaced by implementing the functionality. State of the Project I can build and run the program, but the results are not recognizable by the ""topics.py"" script in the data file, which is used to print out the top M topics. With some investigation it is due to the f value and gnorm values being insanely large as they are going through the ""optimizer"" code. Example of the values on next slide displayed in picture of the output from a run of the program (e.g. gnorm printed as ""inf""). Building the Code Ensure you have a version of MINGW64 installed with g++. Open a command prompt and go to the CourseProject/LARA directory and run the command ""make all"" (or whatever make-like command your version of MINGW64 uses for running makefiles). The program will build into the executable ""prog.exe"". See next slide for example picture. Running the Code There is a test function in ./LARA/src/main.cpp call lrr_test(), where you can modify the parameters for this function. You will find a document in this code called ""manual.pdf"" which comes from the original java project and contains more detail about the Lara implementation in these projects. Unfortunately running the C++ project does not currently produce any meaningful results. You can see the saved model, and predictions in the ./LARA/Data directory. See next slide for example picture. Next Steps (if I had more time!) Upgrade the Java project so that I can run it as a reference of comparison to the C++ project. Upgrade the MeTA project so that it can be built with updated tools. Replace the algebra implementation with an optimized library, maybe a Boost library. Replace the LBFGS code with an updated optimizer quasi-newton algorithm. Develop a dataset with expected output for each function using the what has been output by the Java code. References Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903"
https://github.com/zachdick93/CourseProject	CS410-Progress-Report.docx	Progress Report Which tasks have been completed? I currently have set up my development environment using VSCode and by following the setup instructions for the MeTA project. I have reviewed the MeTA project and the LARA project from the paper. Due to being given the location of the data and code from the paper I have decided to convert the Java project from the paper to C++ and to hopefully be able to recreate the results from the paper using the same dataset. I have begun converting the Java code to C++. Which tasks are pending? I need to finish converting the Java code to C++. I also need to setup the C++ repo as a CMake project. I then need to complete the documentation and presentation. Are you facing any challenges? I attempted to build the MeTA package, but it failed. I even debugged it and updated it a bit, but the technical debt of that project seems to have built up to a point where it would not be worth continuing with that. Another challenge that will likely present itself soon is in finding a C++ equivalent for each of the Java libraries used in the paper.
https://github.com/zachdick93/CourseProject	CS410-Project-Proposal-Fall2020.docx	"Project Proposal Team Captain: Zachariah Dick, NetID: zdick2 (One-person team) Topic: Improving a System: MeTA Toolkit Sub-Topic: Add text mining functions to the MeTA toolkit Main Proposal: I propose implementing the aspect segmentation and Latent Rating Regression model (LRR) in C++ from the paper ""latent aspect rating analysis on review text data: a rating regression approach"" mentioned in the project topics document. This functionality will be developed with the intent to be easily ported into the MeTA codebase. Due to time constraints and the fact that I am a one person team I do not intend on fully integrating this functionality into the meta repository, but instead applying an MIT license to this repository and leaving it public to allow for future integration into the MeTA project. Task List Estimate: Setup development environment based on that of the meta-toolkit/meta. Review the structure of the MeTA repository to gather needed namespaces, data structures, testing setup, and other necessary information to ensure compatibility in my setup. Dissect and Analyze the mentioned paper to develop the pseudo code for the implementation and fully understand the algorithm. Gather testing datasets and process them using python tools that have been used in MPs. Use metapy wherever possible to ensure compatibility with MeTA. The link to the dataset used in the paper is no longer valid so I will need to put together my own test data. Write the functions needed to do the implementation. Attempt to maintain the same pattern used for functions in MeTA repository for ease of portability. Write automated tests for the functionality developed. Continuous development of documentation. Create demo presentation. Deliverables: The deliverables for this project are the source code implementing functionality for latent aspect rating analysis, an automated test suite to demonstrate the functionality, usage documentation, and any other project requirements listed in the project overview. Resources to be Used: Hongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. DOI=10.1145/1835804.1835903 meta-toolkit. https://github.com/meta-toolkit/meta"
https://github.com/zachdick93/CourseProject	manual.pdf	"Constraint-free Implementation of LRR Hongning Wang Department of Computer Science University of Illinois at Urbana-Champaign Urbana IL, 61801 USA wang296@illinois.edu 1 Overview In this constraint-free implementation of latent rating regression (LRR) [Wang et al., 2010], auxiliary variables are introduced to get rid of constraints in the original LRR model, where aspect weights a should be positive and sum up to one. The new implementation performs closely as the original LRR model and can be used as a replacement of it. 2 Latent Rating Regression Model In the work of Latent Aspect Rating Analysis (LARA) [Wang et al., 2010], we assume in each review: 1) the overall rating is the weighted sum of the individual aspect ratings; 2) the aspect ratings can be predicted by the words associated with each aspect. Intuitively, we can formalize these assumptions as follows: p(td|u, S, d, s) =  p(ad|u, S)p(rd|aT d Sd, d)dad where rd is the overall rating for review d, Sd = {Sd1, Sd2, . . . , Sdk} are the predicted ratings for each aspect where Sdi = bT i wi, and ad is the corresponding inferred aspect weight; d is the standard deviation of overall rating prediction. In the original LRR model, we require i, adi >= 0 and k i=1 adi = 1. The constraint on ad greatly increases the computational complexity. To avoid solving a constraint optimization problem, we introduce a set of auxiliary variables {^ad1, ^ad2, . . . , ^adk} for each review d, and set adi = exp(^adi) k j=1 exp(^adj) . (1) Furthermore, we will assume ^adi is drawn from N(u, S) rather than adi as in the original LRR model. Similar trick can be applied on the aspect rating Sdi to avoid negative predicted ratings: Sdi = exp(bT i wi). (2) 1 HONGNING WANG IMPLEMENTATION OF LRR 3 EM Updating Formulas The complete-data log-likelihood function for the newly derived problem is very similar as that in the original LRR model: L(rd, ^ad, Sd, u, S, s2, b) = - log s2-(aT d Sd - rd)2 s2 -log S-(^ad-u)TS-1(^ad-u)-lbTb (3) where ad is a function of ^ad as defined in Eq (1). Hard-EM is performed to iteratively optimize the complete-data log-likelihood over the training corpus. Most procedures are the same as we have derived in [Wang et al., 2010], and we will only list the updating formulas for ^a and b in this manual. Besides, in order to ensure numerical stability and provide good initialization of the random variables, we add an additional term in the log-likelihood function for each review document d, Laux(rd, ad, Sd) = p k  i=1 adi(Sdi - rd)2 (4) where p is a predefined confidence parameter to control the influence of the newly introduced term in the log-likelihood function. Intuitive, Laux(rd, ^ad, Sd) guides the optimization proce- dure to estimate a better starting point of ^ad and b from the overall rating rd. Empirically, p should be much smaller than 1/s2. 3.1 Updating ^a In E-step, ^ad is inferred for every review document d by maximizing the following objective function: L(^ad) = (aT d Sd - rd)2 s2 + p k  i=1 adi(Sdi - rd)2 + (^ad - u)TS-1(^ad - u) (5) The gradient with respect to ^adi is, L(^ad)  ^adi = 2(aT d Sd - rd) s2 aT d Sd  ^ai + p  k j=1 adi(Sdj - rd)2  ^ai + 2 k  j=1 S-1 ij (^adj - uj) (6) where aT d Sd  ^ai = adi k  j=1 [ d(i = j)Sdi(1 - adi) - d(i = j)Sdjadj ] (7) and  k j=1 adi(Sdj - rd)2  ^ai = adi k  j=1 [ d(i = j)(Sdi - rd)2(1 - adi) - d(i = j)(Sdj - rd)2adj ] (8) 2 HONGNING WANG IMPLEMENTATION OF LRR 3.2 Updating b In M-step, the optimal b is estimated over the whole corpus by maximizing the following objective function: L(b) = D  d [(aT d Sd - rd)2 s2 + p k  i=1 adi(Sdi - rd)2] + lbTb (9) Taking derivative with respect to bi, we get, L(b) bi = 2 D  d adi [(aT d Sd - rd) s2 + p(Sdi - rd) ]Sdi bi + 2lbi (10) where Sdi bi = Sdiwdi according to Eq (2). 4 Package Details The code implements the keyword-based bootstrapping aspect segmentation and latent rat- ing regression (LRR) model originally published in [Wang et al., 2010]. In particular, the LRR model is slightly changed to avoid solving constrained optimization as described above. 4.1 Dependency There are two external libraries used in the implementation: 1. OpenNLP: it is used to extract and tokenize the sentences in the review content. Both the lib and trained model files are needed. Latest update of this toolkit can be found in http://opennlp.apache.org/. 2. Colt: it is used for the matrix operations in LRR, e.g., matrix inverse. Latest update for this toolkit can be found in http://acs.lbl.gov/software/colt/. In this package, the required files are located at ./libs and ./Data/Model/NLP. 4.2 Aspect Segmentation The keyword-based bootstrapping aspect segmentation is implemented in the same way as in [Wang et al., 2010]. To apply bootstrapping for aspect keyword extraction, call the following methods: Analyzer analyzer = new Analyzer(""Data/Seeds/hotel_bootstrapping.dat"", ""Data/Seeds/stopwords.dat"", ""Data/Model/NLP/en-sent.zip"", ""Data/Model/NLP/en-token.zip"", ""Data/Model/NLP/en-pos-maxent.bin""); analyzer.LoadDirectory(""Data/Reviews/"", "".dat""); analyzer.BootStrapping(""Data/Seeds/hotel_bootstrapping_sel.dat""); 3 HONGNING WANG IMPLEMENTATION OF LRR where the file ""hotel bootstrapping.dat"" is your initial aspect keyword list, one aspect per row and keywords are separated by ""0x09."" ""Data/Reviewss"" is the folder contains your target review files with fixed format. And ""Data/Seeds/hotel bootstrapping sel.dat"" is the result aspect keyword list expanded by bootstrapping. To segment the aspects in review text document and generate the corresponding vec- tor representation of each hotel (i.e., hReviews as defined in [Wang et al., 2010]), call the following methods: Analyzer analyzer = new Analyzer(""Data/Seeds/hotel_bootstrapping_sel.dat"", ""Data/Seeds/stopwords.dat"", ""Data/Model/NLP/en-sent.zip"", ""Data/Model/NLP/en-token.zip"", ""Data/Model/NLP/en-pos-maxent.bin""); analyzer.LoadDirectory(""Data/Reviews/"", "".dat""); analyzer.Save2Vectors(""Data/Vectors/vector_CHI_4000.dat""); Sample codes can be found in ""src/aspectSegmenter/Analyzer.java"". 4.3 Latent Rating Regression Model The LRR model can be simply executed by the following lines: LRR model = new LRR(500, 1e-2, 5000, 1e-2, 2.0); model.EM_est(""Data/Vectors/Vector_CHI_4000.dat"", 10, 1e-4); model.SavePrediction(""Data/Results/prediction.dat""); model.SaveModel(""Data/Model/model_hotel.dat""); In the constructor of LRR model, you need to specify the following parameters: max iteration of a update, a's convergence criterion, max iteration of b update, b's convergence criterion, and b's regularization parameter l. Besides, the LRR model can also be initialized previously trained model: LRR model = new LRR(500, 1e-2, 5000, 1e-2, 2.0, ""Data/Model/model_hotel.dat""); Sample codes can be found in ""src/lara/LRR.java"". 4.4 Baselines In this package, there are also implementations of the baseline methods, SVR-O and SVR-A, as described and compared in [Wang et al., 2010]. The only difference is we used logistic regression rather than SVR in this implementation. These baselines can be executed in a similar way as LRR model: RatingRegression model = new RatingRegression(500, 5e-2, 5000, 1e-4, 1.0); model.EstimateAspectModel(""Data/Vectors/Vector_CHI_4000.dat""); model.SavePrediction(""Data/Results/prediction_baseline.dat""); model.SaveModel(""Data/Model/model_base_hotel.dat""); The parameter ""RatingRegression.BY OVERALL"" specifies the choice of SVR-O (true) or SVR-A (false). And the parameters in the constructor of RatingRegression model are the same as in LRR model. Sample codes can be found in ""src/lara/RatingRegression.java"". 4 HONGNING WANG IMPLEMENTATION OF LRR 4.5 Miscellaneous 1. The trade-off parameter p for controlling the importance of the newly introduced term Laux(rd, ad, Sd) (as defined in Eq (4)) can be set in ""LRR.PI"". Default value is 0.5. 2. The predicted score mapping can also be achieved by square function, i.e., Sdi = (bT i wdi)2. This variation is also implemented in the package and can be enabled by the setting of ""RatingRegression.SCORE SQUARE"": true for square mapping and false for exponential mapping. 3. I have included a parsed vector file for 4000 hotels (i.e., hReviews) for testing purpose. It can be found in the folder of ""Data/Vectors."" 4. The corresponding 4000 selected words for building the hReview vectors are stored in the file ""Data/Seeds/hotel vocabulary CHI.dat,"" scripts are provided to list the learned top ranked opinion words under each aspect (""Data/topics.py"" and ""Data/run.sh""). 5. 21 sample hotel review text documents are included in the folder of ""Data/Reviews"" for illustrating the required format in the aspect segmentation module. 6. NOTE: In the current implementation, the loaded corpus is split into train/test with ratio 0.75, and the performance printed during model training is computed based on the testing corpus. This setting is different from our original setting in [Wang et al., 2010], where we train and test on the same data set for LRR evaluation. References [Wang et al., 2010] Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783-792. ACM. 5"
https://github.com/zachdick93/CourseProject	README.md	"CourseProject ATTENTION: The presentation slides have a voice narration added to it, so make sure to open in powerpoint and click start presentation! The document ""CODE DOCUMENTATION.docx"" has the documentation to go with this project. The document ""manual.pdf"" is the original document provided with the original Java source code from the previous implementation of this LRR model."
https://github.com/mariahuipe/CourseProject	Final Project Proposal Maria Fernandez.pdf	"Final Project Proposal: Student Name: Maria Fernandez 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. I will work individually, my NetIDs is: myf2 2. Which paper have you chosen? Pattern annotation: 1. Qiaozhu Mei, Dong Xin, Hong Cheng, Jiawei Han, and ChengXiang Zhai. 2006. Generating semantic annotations for frequent patterns with context analysis. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD 2006). ACM, New York, NY, USA, 337-346. DOI=10.1145/1150402.1150441 3. Which programming language do you plan to use? Java 4. Can you obtain the datasets used in the paper for evaluation? Yes, I found it here: https://dblp.uni-trier.de/xml/ 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper?"
https://github.com/mariahuipe/CourseProject	Project Progress Report.pdf	Project Progress Report: Student Name: Maria Fernandez My project is to reproduce the paper: Generating Semantic Annotations for Frequent Patterns with Context Analysis The progress I have made: 1. I have read through the paper a few times, making sure I understand the concepts that are used in the paper, like Pattern, Transactions, Context Unit, Closed Frequent Pattern, Semantically Similar Patterns, etc. I tried focusing in understanding the section that explains the One-pass Microclustering algorithm since this is the part that I will have to work from scratch. 2. I found the dataset that I will be using since the original location is not available in the location anymore, but this is the same data set, just a newer version of it. I made sure it has the data we need. 3. I looked into python libraries to parse XM. 4. I looked into python libraries for Frequent Patterns and for Close Patterns. It is allowed to use the libraries to pre-process the data set, and create the input to the microclustering algorithm. I still have to decide what I am using here. Remaining Tasks: I have to decide what libraries I am using for Frequent Pattern and Close Pattern and I need to start coding. Any Challenges: I am working alone because with work and family responsibilities it would be hard to collaborate with others, so it is challenging to take decisions by myself. So far I am not sure they libraries I found are the best ones to use, so I am planning to ask the question in Piazza and see if the other people doing the same paper has found the same libraries or if they have other suggestions.
https://github.com/mariahuipe/CourseProject	README.md	"CourseProject Maria Fernandez - Final Project. PROJECT DEMO VIDEO LINK: https://www.amazon.com/photos/shared/_pfYcGAFShmihAYWeZtZHw.qkEKw7lYgK8usmZzd2AQfI Pre-requirements: You need Java 1.8, I specifically used java version ""1.8.0_271"" If it is not installed in your system, download from: https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html I include the compile versions of the two java programs I developed, but if needed to compile, please do the following: javac -cp mmdb-2019-04-29.jar ParseDblp.java javac DblpCalcDistance.java The result will be the ParseDblp.class & DblpCalcDistance.class files STEP #1 Clone Git repository: https://github.com/mariahuipe/CourseProject/ STEP #2: OPTION A: Run the Data Pre-processing of the source data set. 1. Download data set a. Go here: https://dblp.org/xml/release/ b. Download to the file called: dblp-2020-11-01.xml.gz . Download to same directory where you clone my git repository. c. If in Unix/OS: unzip the gz file: gunzip dblp-2020-11-01.xml.gz This will create the dblp-2020-11-01.xml which is the input of the Pre-processing process. d. Make sure you have the dblp.dtd which was downloaded from my Git repository 2. Run the Pre-Processing process: java -Xmx8G -cp mmdb-2019-04-29.jar:. ParseDblp > inputfp_100K.txt This step will produced the following files: - authors.txt - inputfp_100K.txt - titles.txt NOTE: The xml file is pretty big. I was not able to process this file from my Windows laptop due to lack of memory but I was able to do it from my Mac, it still takes a few minutes. This is why I am putting this step as an optional step and I am including option B which is skipping this step and take the files that have been produced by the pre-processing. OPTION B: Copy pre-processed files from the back up directory: 1. Do the following: cp OUT_FILES_BK/authors.txt . cp OUT_FILES_BK/inputfp_100K.txt . cp OUT_FILES_BK/titles.txt . STEP #3: Run the SPMF library to create Closed Frequent Patterns from the inputfp_100K.txt file: 1. Run the following: java -jar spmf.jar run FPClose inputfp_100K.txt outputfp.txt 0% Results: you should see the file outputfp.txt More information about this library: https://www.philippe-fournier-viger.com/spmf/ STEP #4: Run the One-Pass Micrcocluster algorithm based in the paper description: 1. Do the following: java DblpCalcDistance Results: pattern_annotations.txt will be created."
https://github.com/chenyuzhao98/CourseProject	Progress_Report.pdf	CS 410: Final Project Progress Report Team Abracadabra Chuhan (Vivian Yu), Chenyu Zhao At this point, our group have completely follow the proposed timeline included in our project proposal. We have finished the research on currently available movie recommender systems by doing a competitive analysis. We looked at movielens (https://movielens.org/) and taste (https://www.taste.io/) which we believe share similar concept with the recommendation system we intend to develop. We have completed building up the crawler for gathering information from two target websites: IMDB and Rotten Tomatoes. Another thing we did is we applied tkinter to build a sketchy interface that facilitate the use of our system. To makes the design of our system more human-centered, we have improved our algorithm so users can choose the maximum of three different emotions at the same time. In the following two weeks, we aim to finish two main tasks: first, completing the main script that contains our algorithm and calls the interface and crawlers; second, if time permits, further polish the system by prototyping as a mobile app. The challenge we are facing right now is due to the web structure of Rotten Tomatoes websites: more detailed information about the movie, such as grading and runtime are stored inside the movie profile links. Even though we have successfully enables our program to look into the profile links and capture intended information, it takes a while for Python to actually process it. Our group is still working on figuring out the best way to present the movie information without taking excessive time.
https://github.com/chenyuzhao98/CourseProject	README.md	"MovieMood CS 410 Final Project: A Mood-Based Movie Recommendation System Team Introduction A collaborative work is done by Vivian Chuhan Yu (chuhany2) and Chenyu Zhao (chenyu5). Being a Psych major, Vivian applied her expertise in designing the connection between human emotions and movie genres. She is also responsible for designing the graphical user interface using tkinter, and presenting the information in Treeview. Chenyu is responsible for building the web scraper, designing the logistic regression algorithm, and implement the Cosine Similarity analysis. He is also responsible for writing the documentation and narrating the presentation. Project Overview This is a Python-based movie recommendation system that implemented text-retrieval techniques and Graphical User Interface. One special thing about this system is that its recommendations were tailored around users' emotion of the moment. There are so many existing movie recommender systems available on the market, but only a small number of them were designed based on users' psychological needs. The main objective of this project is to fill this gap by making traditional recommender system more user-driven. Emotion associated with Genre of Movie There are 10 categories of emotion the system presented to users to choose from. These are 5 postive emotions (""Happy"", ""Satisfied"", ""Peaceful"", ""Excited"", ""Content"") and 5 negative emotions (""Sad"", ""Angry"", ""Fearful"", ""Depressed"", ""Sorrowful""). These emotions taken as inputs from the GUI interface we built through tkinter (please refer to interface.py): The correspondence of every emotion with genre of movies are set up as below: - Happy - Horror - Sad - Drama - Satisfied - Animation - Angry - Romance - Peaceful - Fantasy - Fearful - Adventure - Excited - Crime - Depressed - Comedy - Content - Mystery - Sorrowful - Action Based on the inputted emotion, the system is going to be selected from the corresponding genre based on their ratings given by two websites: IMDB and Rotten Tomatoes. The reason why we are collecting movie information from both websites is that we believe the system is able to capture a more full-scaled opinions from movie lovers. Application of Crawling Because we intend to scrape two websites with different web structure, we developed one IMDB crawler and another RT crawler to extract movie information. Check out scraper.py for more details. Here are two example movie pages of IMDB and Rotten Tomatoes: As you can see, comparing to IMDB, Rotten Tomatoes includes the majority of movie information in each movie profile link. Our crawler had to look up each link to capture hidden information, such as movie length, maturity grading, cast, etc. Therefore, it is unavoidable that the program takes more time to scrape RT pages. Ranking and Scoring We would pull user rating scores from both IMDb and Rotten Tomatoes. Due to the different rating scales used by IMDb and Rotten Tomatoes, we would first convert both scores to a 10-point scale for the ease of comparison. We would also take the number of ratings into consideration, as larger number of ratings tends to make the overall rating more credible. Therefore, we would run logistic regression function on the number of ratings, and add it as an additional weightage to the final movie score. Present Movie Information After users indicate their moods, the program is going to look up the corresponding link to the movie page and present movie information as Treeview, which is a module included by the tkinter library displaying a hierarchical collection of items. Here is an example output of the list of recommended movies: Note: Not every movie has all information listed. If the crawler cannot find relevant information, it will automatically fill the space with ""Not Found"". You May Also Like... After users chose their favorite movie from the list, we would run a Cosine Similarity analysis to recommend 3 similar movies based on the summary. Here is an example of movies similar to Toy Story 4: Self-evaluation The work is equally distributed between the two teammates, and we were able to complete our mood-based movie recommender system as intended. We chose not to build a seperate mobile application, but instead spend the time working on additional features like cosine similarity and logistic regression. In the end, we have obtained the expected outcome. Environment Set-up Please check out requirements.txt for information. You can install all packages at once using $ pip install -r requirements.txt. Please use Python 3. Otherwise you will need to import tkinter.ttk separately because it is not a submodule of tkinter in Python2 How to use? After making sure you have all packages installed, activate the program through main.py. The program will start running immediately. The scraping process may take up to 30 seconds. Please do not close the tkinter window when the program is running. Video Presentation YouTube link"
https://github.com/myusername-2/CourseProject	project-progress-report.pdf	Course Project Progress Report Shai Yusov (syusov2@illinois.edu) System: EducationalWeb System Subtopic: Identifying in-demand skills 1. Progress Made Thus Far My project is about automatically identifying in-demand skills. The original plan was to scrape and analyze job postings for software engineers in New York, NY, from the job board Indeed and then extract the top skills from that dataset. So far, I have written, tested, and executed a component to scrape job postings for software engineers in New York, NY, from Indeed, to clean the scraped the data, and to save all processed job postings to a file; I have thoroughly investigated various algorithms and approaches to extracting the top keywords, and thus most in-demand skills, from the dataset; I have written and tested a component to ingest the dataset and extract the top skills using a keyword extraction algorithm; and I have compared and instrumented various approaches to extracting the top skills to achieve the best results. I have taken care to isolate the interfaces of these components from the implementation details as much as possible so that the functionality will be generic and could potentially integrate with other tools and systems in a straightforward manner. 2. Remaining Tasks The primary tasks of this project are complete. 3. Challenges and Issues When scraping job postings from Indeed, the primary issue I have faced is how to successfully extract all the needed pages in a timely manner and without being throttled. Specifically, my use case involves scanning job results pages, then obtaining job post links from that page, and so on. If executed in serial, this process would be rather slow, so I parallelized this IO-intensive process using a pool of threads that fetch and extract information from pages in parallel. This led to a significant speedup. I also had to account for various failures so I implemented retry-able mechanisms to ensure graceful error recovery. Overall, I was able to download all the data. Another issue I am facing is that, so far, the keyword extractions algorithms with the highest subjective quality of results are also the ones taking the longest. Specifically, it seems that an implementation of TextRank produces fairly relevant results but takes a little longer to finish, while other algorithm implementations of RAKE, YAKE, and TF-IDF produce somewhat more diluted results but in a shorter amount of time.
https://github.com/myusername-2/CourseProject	project-proposal.pdf	Course Project Proposal 1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. - Shai Yusov (syusov2@illinois.edu). It's a team of one and I will be the captain. 2. What system have you chosen? Which subtopic(s) under the system? - System: EducationalWeb System - Subtopic: Automatically creating teaching material for in-demand skills - Identifying in- demand skills 3. Briefly describe the datasets, algorithms or techniques you plan to use - I plan to crawl and analyze job postings for software engineers in New York, NY, on the job board Indeed. After obtaining these postings, the dataset will be pre-processed and fed into a component that will be responsible for identifying the emerging keywords/topics. The identification process will likely involve keyword extraction and/or topic mining. 4. If you are adding a function, how will you demonstrate that it works as expected? If you are improving a function, how will you show your implementation actually works better? - I will demonstrate that the function identifies in-demand skills by running it on the input dataset, obtaining the top emerging keywords/topics, and validating the results through human judgement and known industry trends. 5. How will your code communicate with or utilize the system? It is also fine to build your own systems, just please state your plan clearly - I plan to build a standalone system/component for this project. Specifically, the system will ingest a dataset and output the top emerging keywords/topics from that dataset. Although outside the scope of this project, this functionality is generic and can be plugged into other tools so long as the input specification to the system is decoupled from the source or actual content and the outputs are emitted in a standard, easy-to-digest format. 6. Which programming language do you plan to use? - I plan to use Python for this project. 7. Please justify that the workload of your topic is at least 20*N hours, N being the total number of students in your team. You may list the main tasks to be completed, and the estimated time cost for each task. - Writing, testing, and executing a customized web crawler: at least 10 hours - Investigating and defining the right algorithms/techniques: at least 5 hours - Writing and testing the component performing the identification: at least 10 hours
https://github.com/myusername-2/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview. Project Proposal project-proposal.pdf Project Progress Report project-progress-report.pdf Project Presentation Video Presentation Documentation 1. Overview This project contains two main components that together automatically identify in-demand skils through scraping and analysis of job postings from the job board Indeed for positions in New York, NY. indeed_scraper.py: Scrapes job postings in New York, NY, for a user-specified query. Specifically, it runs a user query on Indeed, scans results pages one by one, extracts job post links from each results page, and obtains and saves the content of each job post to a file (one per line). The output file is specified by the user. This component optionally accepts a file containing a list of proxies to use when sending requests. Can be used to run generic, user-specified queries and scrape results from the job board Indeed (For example, scrape job posts for software engineers, scrape job boards for data analysts, etc.). keyword_extractor.py: Extracts the top keywords from a collection of text documents. It accepts an input file of text documents where there is one document per line, extracts a number of top keywords as optionally specified by the user, and writes these keywords to a file that is specified by the user. Can be used to generically extract top keywords from any collection of text (For example, top skills from job postings, top keywords from research papers, top themes in customer complaints, etc.). More generally, the component indeed_scraper.py can be used as a standalone script to scrape job postings at scale from Indeed for any query and keyword_extractor.py can be used both as a standalone script and as a module to extract top keywords (or phrases). 2. Implementation indeed_scraper.py This module does the following: 1. In the main thread, creates a thread pool with a number of workers. 2. Executes the user query on Indeed and iterates through each page of the results one at a time. 3. For each page of results, extracts all job post links from that page and feeds those links to the worker threads, who will process specific job post links in parallel and will fetch the full content of each one. 4. The main thread continues to iterate through pages of results and passes specific job post links to the worker threads, while each worker thread continues to process specific job post links and captures the full content of each one. 5. Once the main thread has iterated through all pages of the search results, it waits for all the worker threads to finish processing the specific job post links that it has continuously passed them thus far. 6. Once all job posts have been processed, the thread pool terminates and the main thread writes all job posts to a file (each job post on its own line). The logic described above is found mainly in the function scrape_multithreaded(url, query, number_of_pages, results_per_page). Other important functions include: job_links_from_search(url, search_params): - Obtains all job post links from the search results page specified by url and search_params. job_from_link(link): - Obtains the content of the job post at the specified link. get_js_soup(url, params): - Obtains a BeautifulSoup object for the page specified by url and params to be used for parsing HTML content. Finally, all HTTP communication is carried out using python's requests module. keyword_extractor.py This module does the following: 1. Ingests a text file where every line is a document of text. 2. Extracts the text from the input file and runs an algorithm on all the text to produce an optionally specified number of top keywords from the text. The algorithms used to produce the keywords is TextRank, as implemented by pytextrank, but other algorithms such as YAKE, as implemented by pke, are included as well unused functions in the codebase. 3. Outputs the top keywords to console or an optionally specified output file. Note that this component can be used both as a standalone script as well as a module. Specifically, the class KeywordExtractor has a generic interface and can easily be reused in other settings: KeywordExtractor(input_file): A KeywordExtractor is a class for extracting keywords from an input file of text where there is a text document on each line. - top_keywords([n]): Extracts and returns a list of the top n keywords from the input file provided at construction. 3. Usage Installation This package requires Python 3.0+. It also requires the following external resources that can be obtained using: pip install bs4 pip install spacy pip install pytextrank python -m spacy download en_core_web_sm To install this package, clone the repository from github: git clone https://github.com/myusername-2/CourseProject.git cd CourseProject Usage Example First, we scrape job posts from Indeed for 'Software Engineer' in New York, NY. The full job descriptions will be stored in jobs.txt, one job on each line. python indeed_scraper.py --query 'Software Engineer' --pages 2 --results-per-page 25 --output-file jobs.txt Then, we want to discover the most relevant skills in all these job descriptions. So we run the keyword extractor script as follows to discover the top 50 keywords: python keyword_extractor.py -n 50 -o keywords.txt jobs.txt The top 50 keywords will be stored in a file keywords.txt. API Usage keyword_extractor.py: ```python Top 25 keywords from the collection in 'input.txt' input_file = 'input.txt' n = 25 keyword_extractor = KeywordExtractor(input_file) top_keywords = keyword_extractor.top_keywords(n=n) ``` Command Line Usage indeed_scraper.py: ``` python indeed_scraper.py --help usage: indeed_scraper.py [-h] --output-file output_file --query query [--pages pages] [--results-per-page results_per_page] Scrape job posts from Indeed. optional arguments: -h, --help Show this help message and exit --pages pages Number of pages of job results to scrape. --results-per-page results_per_page Number of job results per scraped page. required named arguments: --output-file output_file Output file containing job descriptions matching the query and with one job description per line. --query query Query to search for jobs. ``` keyword_extractor.py: ``` python keyword_extractor.py --help usage: keyword_extractor.py [-h] [-n n] [-o output_file] input_file Extract keywords from an input text. positional arguments: input_file Input file containing text. Each text document should be on its own line. optional arguments: -h, --help show this help message and exit -n n Number of top keywords. -o output_file, --output-file output_file Output file containing the top 'n' keywords in the input text. ```
https://github.com/NikhilDIL/CourseProject	ProgressReport.pdf	Team TextMiningMasters Members: ndeena2, ajaw2, pwasal3, jiahuah2 Project: Reproducing a Paper, Mining Casual Topics in Text Data 1) Progress made thus far: We have gathered and prepared the necessary data in order to perform one of the experiments stated in the paper. More specifically, we have parsed the NYT corpus and time series data from Iowa Electronic Markets 2000 Presidential Winner-Takes-All market using Python and stored the data into a data structure. 2) Remaining tasks: Now that we've gathered and prepared the data the next step is to implement the iterative topic modeling with time series feedback algorithm stated in the paper, and feed in the data that we have prepared into that algorithm. Then the next step is to perform some data visualization to understand our results. 3) Any challenges/issues being faced: Some parts of the algorithm are unclear to us. More specifically, steps 2 and 3. We are unsure on how to compute sig (C, X, T) in step 2, and how to apply granger tests to each of the candidate topics for step 3.
https://github.com/NikhilDIL/CourseProject	Project Presentation.pptx	"Project Presentation Team Members: Nikhil Deenamsetty (ndeena2) Peter Wasala (pwasal3) Angela Jaw(ajaw2) Jiahua He (jiahuah2) Step 1: Parsing Data For Time Series Prices data: We converted the data that we found online into a CSV file which we read using pandas. `` df = pd.read_csv(""timeSeriesPrices.csv"") print(df) `` Website link: https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?market_ID=29 For the NYT_Corpus data: we loop through all the folders in order to reach the XML files and search for all the paragraphs that include the words ""Gore"" and ""Bush"" and use that to filter out the non-relevant documents. Libraries used: os, tarfile, pandas, xml.etree. Step 2: Applying Latent Dirichlet Analysis We get the topics by applying the topic modeling method (Latent Dirichlet Analysis) to the set of documents with time stamps, let's call this set D. **The original experiment used PLSA, but according to lecture, PLSA and LDA perform the same. We wanted to use LDA since the gensim library has an LDA function, and our PLSA implementations take a very long time and are possibly incorrect.** Step 3: Get Candidate Causal Topics with lags We use the Granger Tests to find topics with significant values greater than 1 - the output of the Granger Tests. We can get the set of candidate causal topics with lags, let's call this set CT. Step 4: Find the Most Significant Causal Words We apply the Granger Tests for each candidate topic in CT in order to find the most significant causal words. Once we find those values, we record them. Step 5: Define a prior on the Topic Model Parameters We need to separate the positive valued terms from the negative valued terms and ignore terms with values less than 10%. we can assign the prior probability proportions according to the significance levels. Step 6: Apply LDA to D We use the prior from step 5. We use the feedback signals to guide LDA to form topics that better correlate with the time series. Step 7: Repeat step 2 to step 5 We repeat steps 2 through 5 until the stopping criteria. Once we reach the stopping criteria, the process stops and the function outputs CT, which is the output causal topic list. Libraries we used in the project: For parsing, we use the libraries: os, tarfile, pandas, xml.etree. These libraries should already be included. For the Iterative Topic Modeling Framework with Time Series Feedback function, we use the libraries: gensim, nltk, re, pprint, spacy. To install them, the commands are listed as below: conda install gensim conda install nltk conda install nltk conda install re conda install pprint conda install -c conda-forge spacy THANK YOU SO MUCH!"
https://github.com/NikhilDIL/CourseProject	ProjectProposal.pdf	"1. What are the names and NetIDs of all your team members? Who is the captain? The captain will have more administrative duties than team members. a. Nikhil Deenamsetty ndeena2 (captain) b. Peter John Wasala pwasal3 c. Jiahua He jiahuah2 d. Angela Carol Jaw ajaw2 2. Which paper have you chosen? a. Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback 3. Which programming language do you plan to use a. Python 4. Can you obtain the datasets used in the paper for evaluation? a. Yes. 5. If you answer ""no"" to Question 4, can you obtain a similar dataset (e.g. a more recent version of the same dataset, or another dataset that is similar in nature)? a. N/A 6. If you answer ""no"" to Questions 4 & 5, how are you going to demonstrate that you have successfully reproduced the method introduced in the paper? a. N/A * What is the function of the tool? * To analyze textual topics in conjunction with external time series variables. * Who will benefit from such a tool? * Anyone that needs to analyze text along with time series variables. For example, stock traders can benefit from this tool because this tool could help a stock trader decide what stocks to buy, hold, or trade since news articles can tell more information about the future of a stocks' value. * Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? * Yes, this kind of tool already exists. The difference between the similar tool that we found and ours is that the similar tool that we found displays geometric properties and our tool displays prior distributions on parameters. People would probably prefer our tool more since it provides feedback at each iteration. * What existing resources can you use? * https://catalog.ldc.upenn.edu/LDC2008T19 (Dataset) * https://spiderbites.nytimes.com/2000/ (NYT Articles Backup) * https://iemweb.biz.uiowa.edu/pricehistory/pricehistory_SelectContract.cfm?marke t_ID=29 (Iowa Electronic Markets (IEM) Time Series Data) * https://finance.yahoo.com/quote/AAL/history?period1=946684800&period2=9782 20800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true (American Airlines Stock Data) * https://finance.yahoo.com/quote/AAPL/history?period1=946684800&period2=978 220800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true (Apple Stock Data) * MeTA toolkit for topic modeling algorithms * What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) * PLSA or Latent Dirichlet Analysis (LDA) as Topic Model * Granger causality measure testing * How will you demonstrate the usefulness of your tool? * Give a presentation on how you could use this tool to help a user decide what stocks to buy, hold, or trade and show how news articles can influence stock prices. * A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) * Gather data and necessary resources to complete project (10/25) * Finish Iterative Topic Modeling Algorithm (11/15) * Create Documentation (11/22) * Create Presentation (11/29)"
https://github.com/NikhilDIL/CourseProject	README.md	"CS 410 CourseProject Table of Contents 1. Project Video 2. An overview of the function of the code 3. Documentation of how the software is implemented 4. Documentation of the usage of the software 5. Brief description of the contribution of each team member 6. References Project Video Project Video 1. An overview of the function of the code (i.e., what it does and what it can be used for) We chose to reproduce the first experiement of the paper, Mining Causal Topics in Text Data: Iterative Topic Modeling with Time Series Feedback, which is where the authors of the paper examine the 2000 U.S. Presidential Election. We do this by: First, we parse through both the Time Series Prices data, which was from the Iowa Electronic Markets(IRM) Time Series Data, as well as the NYT_Corpus data. Then we start implementing the Iterative Topic Modeling Framework with Time Series Feedback as explained in the paper. Using this data we are able to implement a general text mining framework for discovering causal topics from text by combining the probabilistic topic model with time series causal analysis to discover topics that are semantically coherent as well as correlated with the time series data. By iterating through the data, we can refine the topics which increase the correlation of the topics with the time series. We can use this function in order to analyze textual topics in conjunction with external time series variables such as stocks What we implemented can be used to find the causal relationships between text data and non-text data, between media coverage and public opinion. Thus, our code can potentially be modified in order to identify target paragraphs for topics relating to not just the 2000 U.S. Presidential Election but instead can be used for other things such as measuring the relationship between the public's response to topics such as climate change, corona virus, as well as other issues. 2. Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement For the Time Series Prices data, we converted the data that we found online on this site into a CSV file which we read using pandas. For the NYT_Corpus data, we loop through all the folders in order to reach the XML files and search for all the paragraphs that include the words ""Gore"" and ""Bush"" and use that to filter out the non-relevant documents. The Iterative Topic Modeling Framework with Time Series Feedback function can be broken down into the following six parts First, we get the topics by applying the topic modeling method (Latent Dirichlet Analysis) to the set of documents with time stamps, let's call this set D The original experiment used PLSA, but according to lecture, PLSA and LDA perform the same. We wanted to use LDA since the gensim library has an LDA function, and our PLSA implementations take a very long time and are possibly incorrect. Second, we use the Granger Tests to find topics with significant values greater than 1 - the output of the Granger Tests. Then we can get the set of candidate causal topics with lag, let's call this set CT. Third, we apply the Granger Tests for each candidate topic in CT in order to find the most significant causal words. Once we find those values, we record them. Fourth, we define a prior on the topic model parameters using significant terms and their values First, we need to separate the positive valued terms from the negative valued terms. We can ignore terms with values less than 10% Then, we can assign the prior probability proportions according to the significance levels Fifth, we use the prior from step 4 to apply LDA to D This is the part that uses the feedback signals and guides LDA to form topics that better correlate with the time series Sixth, we repeat steps two through five until the stopping criteria. Once we reach the stopping criteria, the process stops and the function outputs CT, which is the output causal topic list 3. Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run the software, whichever is applicable This project was run using python version 3.8. Make sure you have access to Jupyter Notebook either by installing Jupyter by following the directions at this link or by installing Anaconda by following the directions at this link For parsing, we use the libraries: os, tarfile, xml.dom.minidom, pandas, xml.etree. These libraries should already be included in python. For the Iterative Topic Modeling Framework with Time Series Feedback function, we use the libraries: gensim, nltk, re, pprint, spacy. To use these libraries, you need to install them which you can do by doing the following commands in the command prompt (the one we used was the Anaconda Prompt): - conda install gensim - conda install nltk - conda install re - conda install pprint - conda install -c conda-forage spacy After installing all the libraries, once you launch the notebook, you should be able to run each cell in the notebook by pressting the ""Restart and Run All"" button or you can run each cell one at a time by pressing the ""Run"" button. 4. Brief description of the contribution of each team member in case of a multi-person team In general, we worked together as a group on a call and gave each other advice and helped when possible whether it be by looking at the current issue, clarify what the Iterative Topic Modeling Framework with Time Series Feedback function is doing, or Googling resources such as libraries to use. Down below is what we were tasked with, but we worked on things outside of what we were in charge of. Nikhil Deenamsetty (ndeena2) In charge of the Iterative Topic Modeling Framework with Time Series Feedback function parts 2 and 3 Peter Wasala (pwasal3) In charge of the Iterative Topic Modeling Framework with Time Series Feedback function parts 1, 4, 5 Angela Jaw (ajaw2) In charge of parsing data, wrote up the documentation, helped when asked, did tasks assigned to me Jiahua He (jiahuah2) In charge of parsing data, worked on the presentation, helped when asked, did tasks assigned to me 5. References Topic Modeling with Gensim (Python) Topic Distribution statsmodel documentation"
https://github.com/pwasal3/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview.
https://github.com/mikepigott/TextInformationSystemsCourseProject	Final Project Proposal.pdf	"Project topic: ExpertSearch: Extracting relevant information from faculty bios Group name: Remarkable Scientists Members: * Mike Pigott (mpigott2@illinois.edu), Team Lead * Shuopeng Zhou (sz46@illinois.edu) * Amitha Supragna Sandur (asandur2@illinois.edu) 1. What is the function of the tool? The function of the tool is to make faculty information more accessible to users. Our aim is to improve and expand the existing ExpertSearch system. 2. Who will benefit from such a tool? Users of the ExpertSearch system. 3. Does this kind of tool already exist? If similar tools exist, how is your tool different from them? Would people care about the difference? The ExpertSearch system already leverages Named Entity Extraction, but can be inaccurate. For example, not all professors are listed when searching for ""Data Mining."" In addition, sometimes the existing Named Entity extraction tool retrieves the wrong name from the data set. Users will care about getting more accurate and complete information. 4. What existing resources can you use? * SpaCy allows for building a custom Named Entity Extraction model. * DBPedia Spotlight scrapes Wikipedia pages and collects structured content. * Wikipedia provides an introduction to Named Entity Extraction that we can leverage. 5. What techniques/algorithms will you use to develop the tool? (It's fine if you just mention some vague idea.) We expect to heavily leverage Named Entity Extraction for faculty names. The existing system uses regular expressions to find e-mail addresses; we may inspect the work it already does, or come up with our own. Finally, we will investigate using topic mining to extract topics from the bios. 6. How will you demonstrate the usefulness of your tool? By comparing it with the existing ExpertSearch system and showing all the improvements we make. 7. A very rough timeline to show when you expect to finish what. (The timeline doesn't have to be accurate.) By mid-Nov: * Train Models for Named Entity Extraction. * Run the existing code of ExpertSearch website. * Work on implementing new features for the system. * Submit the progress report. By first week of Dec: * Complete training the models for Topic Mining. * Complete tweaking and verifying the models against the new data sets. * Wire in the resulting model to the existing ExpertSearch system. * Work on software code submission along with documentation."
https://github.com/mikepigott/TextInformationSystemsCourseProject	Progress Report.pdf	"Remarkable Scientists: ExpertSearch Project Status Report For this project, we worked on three different tasks: * Topic Mining of the ExpertSearch professor bios. * Improving Named Entity Recognition & Extraction of professor names from the bios. * Improving recognition and extraction of professor e-mail addresses from the bios. Below is the status report for each. Topic Mining of Professor Bios We were able to combine spaCy, NLTK, and Genism to build an LDA topic model of the professor's bios, using 10 topics. We then used pyLDAvis to visualize the model, and word_cloud to build a word cloud of the 25 highest-weighted terms in each topic. Finally, we wired the word cloud of the highest-ranked topic into the ExpertSearch search results. LDA Visualization The following was created using pyLDAvis, which is visualizing a 10-topic LDA model: Word Clouds The following word clouds show the top 25 words in each of the 10 LDA topics: Topic 1 Word Cloud Topic 2 Word Cloud Topic 3 Word Cloud Topic 4 Word Cloud Topic 5 Word Cloud Topic 6 Word Cloud Topic 7 Word Cloud Topic 8 Word Cloud Topic 9 Word Cloud Topic 10 Word Cloud ExpertSearch Search Results Finally, we were able to wire in the primary topic associated with the bio into the search results: Next Steps We intend to experiment with different topic counts in the LDA model to determine the best results. We are also interested in making the ExpertSearch system browsable by allowing the user to click on a topic word cloud and see the bios ranked in descending order of relevance to that topic. Challenges Metapy does not support the ability to return a list of documents based on its metadata fields, so it is unclear if we would be able to build this additional functionality in the time that we have. Named Entity Extraction of Professor Names We have conducted several experiments on NER(Named Entity Extraction) solutions in the market. The best open source solution would be Standford NER Tagger, which is also provided along with ExpertSearch project for version 2018. The provided tagger and compiled result are incomplete as it eliminates some name information during the tagging phase, part of missing information is critical to extract the main context from the text. We have done two improvements for solving this problem, firstly, we update NER tagger to the newest version which is 2020-11 online, and test on 3-class model and 4-class model separately. Secondly, we implemented a filtered mechanism from tagging results, filtering all names with more than one word, capturing all tokens tagged as 'Person' in a row, and building a two-layer tagging system(3-class model and 4-class model) to cross-validation. We use the 4-class model as the main model to capture 'PERSON' entities, and check each produced token whether it states in the 3-class model. Now the system we created is able to capture most names from context files, filtering names are irrelevant partially. For example, for the first compiled bio text, we are able to retrieve 'Tarek F. Abdelzaher Professor' as full name rather than 'Tarek', second one 'Sarita V. Adve' as correct name rather than 'Sarita V. Adve Richard T. Cheng' in the provided file Next Steps We will continue working on a 2-layer tagging system to cross validation results between two models. Any results show or partial show on both models state they are 'Person/Name' entities with high possibility. Additionally, we will continue on filtering results, with introducing a counting/score system. Any name tokens appearing in high frequency should have a high score indicating its the main context of text. Challenges The biggest challenge we are facing is lots of famous names and confusing information shown in the bio files. For example, 'Ann Arbor' can be a name or location. The tagging system is not able to distinguish between them. Also 'Kennedy' as a famous name, sometimes can occur multiple times in the bio page, the system even with the scoring system is not able to know 'Kennedy' is not the main context. We will introduce another non-standard model can tag 'locations' entity, then do another cross-validation between our model and the third party model result, it should lead us to a potential solution finding the main person of the context, and better filter out non-relevant names/location. Extraction of Professor E-Mail We have worked on improving the existing regex based extraction of email ids from faculty bios. The original code was extracting the usual format of email ids (user@illinois.edu) and also some false positives such as cs@uiuc. Our new code is able to extract many of the alternative email id formats, for example: ""yang.r.yang at yale.edu"" ""denisew (at) uw.edu"" ""moli96 at uw.edu"". It is also able to capture email ids that were originally missing from the first output and remove some false positives. Below is a comparison of the outputs from existing code shown on the left and output from new code shown on the right. As observed from the figure above, the new code is able to catch the ""user at illinois dot edu"" types of format email ids. We have covered cases for the following email formats: * user at illinois.edu * user at illinois dot edu * user ""at"" illinois ""dot"" edu or  user ""at"" illinois.edu * user (at) illinois (dot) edu * We removed some erroneous outputs like website urls/sentences/repeated special characters Next Steps We plan to convert the email ids in ""user at illinois dot edu"" format into the normal ""user@illinois.edu"" format as one of our next steps. Although the new regex based code is a significant improvement over the existing code, we feel that this method is laborious. And even if we spend a lot more time trying to cover all the exhaustive number of cases, the gain in improvement for the tool as a whole wouldn't be much, as the number of cases are a lot. Hence we plan to look into machine learning based approaches to extract email ids. Challenges The challenge here is to convert the various different email formats into the usual email id format. Although we may consider keeping it as is if we discover more new email formats along the way and it becomes too heterogeneous, because we have also seen a few websites having email ids in this alternative format."
https://github.com/mikepigott/TextInformationSystemsCourseProject	README.md	CourseProject Final Presentation: https://uofi.box.com/s/7oaexhrgl33zcnzb034gln9fl97w5fo1
https://github.com/azk0019/CourseProject	Progress Report.pdf	Akash Singla 11/29/2020 CS 410 Progress Report - FALL 2020 1) I have implemented LARA using python. The data set used is Yelp Hotel Review corpus. Latent Aspects are calculated and new aspects from the reviews are generated. The mode was fined tuned empirically to get more than 25 aspects from every review. Every aspect was provided a rating. 2) Submit complete code and instructions (tutorial and documentation) for presentation. 3) The training phase on more than 4000 reviews took time. I would wish to use a GPU for this purpose.
https://github.com/azk0019/CourseProject	Project Instructions and Presentation.pdf	"1. I implemented a Latent Aspect Rating Analysis from Hongning Wang, Yue Lu, Chengxiang Zhai in python. The code is provided in a zip file. The dataset used is Yelp Restaurant Review corpus. Latent Aspects are calculated and new aspects from the reviews is generated. The model was fine tuned empirically to get more than 25 aspects from every review. Every aspect was provided a rating. 2. The training phase on more than 4000 reviews took time. I would wish to use a better GPU for this purpose. 3. The dataset is freely available from https://www.yelp.com/dataset 3. CODE: Dependencies It requires the nltk dataset and additional packages for running. Use nltk.download() and download all the given packages. Also required is the vader package from nltk. It requires python3. Running Code The code can be run manually by using main.py The newly acquired aspects are stored in the ""output"" folder.""final_aspect_words.txt "" text file holds the newly acquired aspects mined. Corresponding ratings are allocated to every aspect mined and stored in review_data.txt. 4. Theory and Observations: In this paper, authors identified and analysed a new problem of opinionated text data analysis called Latent Aspect Rating Analysis (LARA), which aims to analyse opinions expressed about an object in an online review at the level of topical aspects to discover the latent opinion of each individual reviewer on each aspect as well as the relative focus on various aspects when forming the overall judgement To solve this new text mining issue in a general manner, they proposed a novel probabilistic rating regression model. Empirical studies on a data set for a hotel review show that the proposed latent rating regression model can effectively solve the LARA problem and that a thorough analysis of opinions at the level of topical aspects allowed by the proposed model can help a broad range of application tasks, such as overview of aspect opinion, ranking of individuals based on aspect ratings, and reviewer analysis. In the code I went through the following steps to replicate the paper: I. Create Vocabulary from the dataset. II. I used porter stemmer on the dataset to create a vocabulary (stemmed) corpus of the reviews. This stemmed corpus will then be used for aspect mining and rating. III. Initially defined aspects (10 aspects) are read from the file init_aspect_word.txt"" IV. The aspects are then mined using LARAM bootstraping , which uses the following two classes for Restaurant reviews: * class Review: * class Restaurant V. The the w matrix is calculated using ci-square metrics to calculate the ratings per mined aspect. VI. The results are the saved in output folder. NOTE: The original paper used regression to calculate weight for each aspect. But I also used sentiment analysis module from nltk to rate individual aspects. The Regression.py does not work, I just left it there for reference. Use mainpy to run the actual code."
https://github.com/azk0019/CourseProject	Project Proposal Report.pdf	Project Proposal Report 1) Akash Kumar- Net ID: akashk5. I am working this project individually and I will be the captain and have all the administrative duties. 2) I have chosen Latent Aspect Rating Analysis as my research paper to reproduce. 3) I will be using python to reproduce this paper. 4) No. 5) Yes, I can obtain similar dataset.
https://github.com/azk0019/CourseProject	README.md	CourseProject Please fork this repository and paste the github link of your fork on Microsoft CMT. Detailed instructions are on Coursera under Week 1: Course Project Overview.
